diff -Napur linux-linux4microchip-2022.10-orig/drivers/net/ethernet/micrel/hsr_main.h linux-linux4microchip-2022.10/drivers/net/ethernet/micrel/hsr_main.h
--- linux-linux4microchip-2022.10-orig/drivers/net/ethernet/micrel/hsr_main.h	1969-12-31 16:00:00.000000000 -0800
+++ linux-linux4microchip-2022.10/drivers/net/ethernet/micrel/hsr_main.h	2023-03-08 14:40:10.621433764 -0800
@@ -0,0 +1,225 @@
+/* Copyright 2011-2014 Autronica Fire and Security AS
+ *
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms of the GNU General Public License as published by the Free
+ * Software Foundation; either version 2 of the License, or (at your option)
+ * any later version.
+ *
+ * Author(s):
+ *	2011-2014 Arvid Brodin, arvid.brodin@alten.se
+ */
+
+#ifndef __HSR_PRIVATE_H
+#define __HSR_PRIVATE_H
+
+#include <linux/netdevice.h>
+#include <linux/if_vlan.h>
+#include <linux/list.h>
+
+
+#ifdef CONFIG_KSZ_SWITCH
+#define CONFIG_HSRv1
+#endif
+
+
+/* Time constants as specified in the HSR specification (IEC-62439-3 2010)
+ * Table 8.
+ * All values in milliseconds.
+ */
+#define HSR_LIFE_CHECK_INTERVAL		 2000 /* ms */
+#define HSR_NODE_FORGET_TIME		60000 /* ms */
+#define HSR_ANNOUNCE_INTERVAL		  100 /* ms */
+
+#define HSR_ENTRY_FORGET_TIME		  400 /* ms */
+#define HSR_NODE_REBOOT_INTERVAL	  500 /* ms */
+
+
+/* By how much may slave1 and slave2 timestamps of latest received frame from
+ * each node differ before we notify of communication problem?
+ */
+#define MAX_SLAVE_DIFF			 3000 /* ms */
+#define HSR_SEQNR_START			(USHRT_MAX - 1024)
+
+
+/* How often shall we check for broken ring and remove node entries older than
+ * HSR_NODE_FORGET_TIME?
+ */
+#define PRUNE_PERIOD			 3000 /* ms */
+
+
+#ifndef CONFIG_HSRv1
+#define HSR_TLV_ANNOUNCE		   22
+#endif
+#define PRP_TLV_LIFE_CHECK		   20
+#define HSR_TLV_LIFE_CHECK		   23
+#define HSR_TLV_REDBOX			   30
+
+
+/* HSR Tag.
+ * As defined in IEC-62439-3:2010, the HSR tag is really { ethertype = 0x88FB,
+ * path, LSDU_size, sequence Nr }. But we let eth_header() create { h_dest,
+ * h_source, h_proto = 0x88FB }, and add { path, LSDU_size, sequence Nr,
+ * encapsulated protocol } instead.
+ *
+ * Field names as defined in the IEC:2010 standard for HSR.
+ */
+struct hsr_tag {
+	__be16		path_and_LSDU_size;
+	__be16		sequence_nr;
+	__be16		encap_proto;
+} __packed;
+
+#define HSR_HLEN	6
+
+/* The helper functions below assumes that 'path' occupies the 4 most
+ * significant bits of the 16-bit field shared by 'path' and 'LSDU_size' (or
+ * equivalently, the 4 most significant bits of HSR tag byte 14).
+ *
+ * This is unclear in the IEC specification; its definition of MAC addresses
+ * indicates the spec is written with the least significant bit first (to the
+ * left). This, however, would mean that the LSDU field would be split in two
+ * with the path field in-between, which seems strange. I'm guessing the MAC
+ * address definition is in error.
+ */
+static inline u16 get_hsr_tag_path(struct hsr_tag *ht)
+{
+	return ntohs(ht->path_and_LSDU_size) >> 12;
+}
+
+static inline u16 get_hsr_tag_LSDU_size(struct hsr_tag *ht)
+{
+	return ntohs(ht->path_and_LSDU_size) & 0x0FFF;
+}
+
+static inline void set_hsr_tag_path(struct hsr_tag *ht, u16 path)
+{
+	ht->path_and_LSDU_size = htons(
+			(ntohs(ht->path_and_LSDU_size) & 0x0FFF) | (path << 12));
+}
+
+static inline void set_hsr_tag_LSDU_size(struct hsr_tag *ht, u16 LSDU_size)
+{
+	ht->path_and_LSDU_size = htons(
+			(ntohs(ht->path_and_LSDU_size) & 0xF000) |
+			(LSDU_size & 0x0FFF));
+}
+
+struct hsr_ethhdr {
+	struct ethhdr	ethhdr;
+	struct hsr_tag	hsr_tag;
+} __packed;
+
+
+/* HSR Supervision Frame data types.
+ * Field names as defined in the IEC:2010 standard for HSR.
+ */
+#ifdef CONFIG_HSRv1
+struct hsr_sup_tag {
+	__be16		path_and_HSR_Ver;
+	__be16		sequence_nr;
+} __packed;
+
+struct hsr_sup_type {
+	__u8		HSR_TLV_Type;
+	__u8		HSR_TLV_Length;
+} __packed;
+#else
+struct hsr_sup_tag {
+	__be16		path_and_HSR_Ver;
+	__be16		sequence_nr;
+	__u8		HSR_TLV_Type;
+	__u8		HSR_TLV_Length;
+} __packed;
+#endif
+
+struct hsr_sup_payload {
+	unsigned char	MacAddressA[ETH_ALEN];
+} __packed;
+
+static inline u16 get_hsr_stag_path(struct hsr_sup_tag *hst)
+{
+	return get_hsr_tag_path((struct hsr_tag *) hst);
+}
+
+static inline u16 get_hsr_stag_HSR_ver(struct hsr_sup_tag *hst)
+{
+	return get_hsr_tag_LSDU_size((struct hsr_tag *) hst);
+}
+
+static inline void set_hsr_stag_path(struct hsr_sup_tag *hst, u16 path)
+{
+	set_hsr_tag_path((struct hsr_tag *) hst, path);
+}
+
+static inline void set_hsr_stag_HSR_Ver(struct hsr_sup_tag *hst, u16 HSR_Ver)
+{
+	set_hsr_tag_LSDU_size((struct hsr_tag *) hst, HSR_Ver);
+}
+
+struct hsr_ethhdr_sp {
+	struct ethhdr		ethhdr;
+	struct hsr_sup_tag	hsr_sup;
+#ifdef CONFIG_HSRv1
+	struct hsr_sup_type	sup_type;
+#endif
+} __packed;
+
+
+enum hsr_port_type {
+	HSR_PT_NONE = 0,	/* Must be 0, used by framereg */
+	HSR_PT_SLAVE_A,
+	HSR_PT_SLAVE_B,
+	HSR_PT_INTERLINK,
+	HSR_PT_MASTER,
+	HSR_PT_PORTS,	/* This must be the last item in the enum */
+};
+
+struct hsr_port {
+	struct list_head	port_list;
+	struct net_device	*dev;
+	struct hsr_priv		*hsr;
+	enum hsr_port_type	type;
+};
+
+struct hsr_priv {
+	struct rcu_head		rcu_head;
+	struct list_head	ports;
+	struct list_head	node_db;	/* Known HSR nodes */
+	struct list_head	self_node_db;	/* MACs of slaves */
+	struct timer_list	announce_timer;	/* Supervision frame dispatch */
+	struct timer_list	prune_timer;
+	int announce_count;
+	u16 sequence_nr;
+#ifdef CONFIG_HSRv1
+	u16 sup_sequence_nr;
+	u8 net_id;
+#ifdef CONFIG_KSZ_SWITCH
+	u8 redbox_id;
+#endif
+	u16 vid;
+#endif
+	spinlock_t seqnr_lock;			/* locking for sequence_nr */
+	unsigned char		sup_multicast_addr[ETH_ALEN];
+};
+
+#define hsr_for_each_port(hsr, port) \
+	list_for_each_entry_rcu((port), &(hsr)->ports, port_list)
+
+#if 0
+static
+struct hsr_port *hsr_port_get_hsr(struct hsr_priv *hsr, enum hsr_port_type pt);
+#endif
+
+/* Caller must ensure skb is a valid HSR frame */
+static inline u16 hsr_get_skb_sequence_nr(struct sk_buff *skb)
+{
+	struct hsr_ethhdr *hsr_ethhdr;
+
+	hsr_ethhdr = (struct hsr_ethhdr *) skb_mac_header(skb);
+	if (hsr_ethhdr->ethhdr.h_proto == htons(ETH_P_8021Q))
+		hsr_ethhdr = (struct hsr_ethhdr *)((u8 *) hsr_ethhdr +
+			VLAN_HLEN);
+	return ntohs(hsr_ethhdr->hsr_tag.sequence_nr);
+}
+
+#endif /*  __HSR_PRIVATE_H */
diff -Napur linux-linux4microchip-2022.10-orig/drivers/net/ethernet/micrel/i2c-ksz8863.c linux-linux4microchip-2022.10/drivers/net/ethernet/micrel/i2c-ksz8863.c
--- linux-linux4microchip-2022.10-orig/drivers/net/ethernet/micrel/i2c-ksz8863.c	1969-12-31 16:00:00.000000000 -0800
+++ linux-linux4microchip-2022.10/drivers/net/ethernet/micrel/i2c-ksz8863.c	2023-05-08 15:33:42.415997950 -0700
@@ -0,0 +1,473 @@
+/**
+ * Microchip KSZ8863 I2C driver
+ *
+ * Copyright (c) 2015-2023 Microchip Technology Inc.
+ * Copyright (c) 2010-2015 Micrel, Inc.
+ *
+ * Copyright 2009 Simtec Electronics
+ *	http://www.simtec.co.uk/
+ *	Ben Dooks <ben@simtec.co.uk>
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+ */
+
+#if 0
+#define DEBUG
+#define DBG
+#endif
+
+#ifndef CONFIG_KSZ_SWITCH_EMBEDDED
+#define pr_fmt(fmt) KBUILD_MODNAME ": " fmt
+
+#include <linux/kernel.h>
+#include <linux/module.h>
+#include <linux/interrupt.h>
+#include <linux/of_net.h>
+#include <linux/of_mdio.h>
+#include <linux/phy.h>
+#include <linux/phylink.h>
+#include <linux/platform_device.h>
+#include <linux/sched.h>
+#include <linux/netdevice.h>
+#include <linux/etherdevice.h>
+#include <linux/ethtool.h>
+#include <linux/cache.h>
+#include <linux/crc32.h>
+#include <linux/if_vlan.h>
+#include <linux/ip.h>
+#include <net/ip.h>
+#endif
+
+/* -------------------------------------------------------------------------- */
+
+#include <linux/i2c.h>
+#include <linux/debugfs.h>
+#include <linux/seq_file.h>
+
+#include "ksz_cfg_8863.h"
+#ifndef PHY_RESET_NOT
+#define PHY_RESET_NOT			PHY_RESET
+#endif
+
+
+#define SW_DRV_RELDATE			"Mar 25, 2023"
+#define SW_DRV_VERSION			"1.2.3"
+
+/* -------------------------------------------------------------------------- */
+
+#define HW_R(ks, reg)		i2c_rdreg8(ks, reg)
+#define HW_W(ks, reg, val)	i2c_wrreg8(ks, reg, val)
+#define HW_R8(ks, reg)		i2c_rdreg8(ks, reg)
+#define HW_W8(ks, reg, val)	i2c_wrreg8(ks, reg, val)
+#define HW_R16(ks, reg)		i2c_rdreg16(ks, reg)
+#define HW_W16(ks, reg, val)	i2c_wrreg16(ks, reg, val)
+#define HW_R32(ks, reg)		i2c_rdreg32(ks, reg)
+#define HW_W32(ks, reg, val)	i2c_wrreg32(ks, reg, val)
+
+#include "ksz_spi_net.h"
+
+/* -------------------------------------------------------------------------- */
+
+#define I2C_CMD_LEN			1
+
+/*
+ * I2C register read/write calls.
+ *
+ * All these calls issue I2C transactions to access the chip's registers. They
+ * all require that the necessary lock is held to prevent accesses when the
+ * chip is busy transfering packet data (RX/TX FIFO accesses).
+ */
+
+/**
+ * i2c_wrreg - issue write register command
+ * @priv:	The switch device structure.
+ * @reg:	The register address.
+ * @buf:	The data buffer to write.
+ * @txl:	The length of data.
+ *
+ * This is the low level write call that issues the necessary i2c message(s)
+ * to write data to the register specified in @reg.
+ */
+static void i2c_wrreg(struct sw_priv *priv, u8 reg, void *buf, size_t txl)
+{
+	struct i2c_hw_priv *hw_priv = priv->hw_dev;
+	struct i2c_msg msg;
+	struct i2c_client *i2c = hw_priv->i2cdev;
+	struct i2c_adapter *adapter = i2c->adapter;
+
+	hw_priv->txd[0] = reg;
+
+	/* Own transmit buffer is not being used. */
+	if (buf != hw_priv->txd)
+		memcpy(&hw_priv->txd[I2C_CMD_LEN], buf, txl);
+
+	msg.addr = i2c->addr;
+	msg.flags = 0;
+	msg.len = txl + I2C_CMD_LEN;
+	msg.buf = hw_priv->txd;
+
+	if (i2c_transfer(adapter, &msg, 1) != 1)
+		pr_alert("i2c_transfer() failed\n");
+}
+
+static void i2c_wrreg_size(struct sw_priv *priv, u8 reg, unsigned val,
+			   size_t size)
+{
+	struct i2c_hw_priv *hw_priv = priv->hw_dev;
+	u8 *txb = hw_priv->txd;
+	int i = I2C_CMD_LEN;
+	size_t cnt = size;
+
+	do {
+		txb[i++] = (u8)(val >> (8 * (cnt - 1)));
+		cnt--;
+	} while (cnt);
+	i2c_wrreg(priv, reg, txb, size);
+}
+
+/**
+ * i2c_wrreg32 - write 32bit register value to chip
+ * @priv:	The switch device structure.
+ * @reg:	The register address.
+ * @val:	The value to write.
+ *
+ * Issue a write to put the value @val into the register specified in @reg.
+ */
+static void i2c_wrreg32(struct sw_priv *priv, u8 reg, unsigned val)
+{
+	i2c_wrreg_size(priv, reg, val, 4);
+}
+
+/**
+ * i2c_wrreg16 - write 16bit register value to chip
+ * @priv:	The switch device structure.
+ * @reg:	The register address.
+ * @val:	The value to write.
+ *
+ * Issue a write to put the value @val into the register specified in @reg.
+ */
+static void i2c_wrreg16(struct sw_priv *priv, u8 reg, unsigned val)
+{
+	i2c_wrreg_size(priv, reg, val, 2);
+}
+
+/**
+ * i2c_wrreg8 - write 8bit register value to chip
+ * @priv:	The switch device structure.
+ * @reg:	The register address.
+ * @val:	The value to write.
+ *
+ * Issue a write to put the value @val into the register specified in @reg.
+ */
+static void i2c_wrreg8(struct sw_priv *priv, u8 reg, unsigned val)
+{
+	i2c_wrreg_size(priv, reg, val, 1);
+}
+
+/**
+ * i2c_rdreg - issue read register command and return the data
+ * @priv:	The switch device structure.
+ * @reg:	The register address.
+ * @rxb:	The RX buffer to return the result into.
+ * @rxl:	The length of data expected.
+ *
+ * This is the low level read call that issues the necessary i2c message(s)
+ * to read data from the register specified in @reg.
+ */
+static void i2c_rdreg(struct sw_priv *priv, u8 reg, void *rxb, unsigned rxl)
+{
+	struct i2c_hw_priv *hw_priv = priv->hw_dev;
+	struct i2c_msg msg[2];
+	struct i2c_client *i2c = hw_priv->i2cdev;
+	struct i2c_adapter *adapter = i2c->adapter;
+
+	msg[0].addr = i2c->addr;
+	msg[0].flags = 0;
+	msg[0].len = I2C_CMD_LEN;
+	msg[0].buf = &reg;
+
+	msg[1].addr = i2c->addr;
+	msg[1].flags = I2C_M_RD;
+	msg[1].len = rxl;
+	msg[1].buf = rxb;
+
+	if (i2c_transfer(adapter, msg, 2) != 2)
+		pr_alert("i2c_transfer() failed\n");
+}
+
+/**
+ * i2c_rdreg8 - read 8 bit register from device
+ * @priv:	The switch device structure.
+ * @reg:	The register address.
+ *
+ * Read a 8bit register from the chip, returning the result.
+ */
+static u8 i2c_rdreg8(struct sw_priv *priv, u8 reg)
+{
+	u8 rxb[1];
+
+	i2c_rdreg(priv, reg, rxb, 1);
+	return rxb[0];
+}
+
+/**
+ * i2c_rdreg16 - read 16 bit register from device
+ * @priv:	The switch device structure.
+ * @reg:	The register address.
+ *
+ * Read a 16bit register from the chip, returning the result.
+ */
+static u16 i2c_rdreg16(struct sw_priv *priv, u8 reg)
+{
+	__le16 rx = 0;
+
+	i2c_rdreg(priv, reg, &rx, 2);
+	return be16_to_cpu(rx);
+}
+
+/**
+ * i2c_rdreg32 - read 32 bit register from device
+ * @priv:	The switch device structure.
+ * @reg:	The register address.
+ *
+ * Read a 32bit register from the chip.
+ *
+ * Note, this read requires the address be aligned to 4 bytes.
+ */
+static u32 i2c_rdreg32(struct sw_priv *priv, u8 reg)
+{
+	__le32 rx = 0;
+
+	i2c_rdreg(priv, reg, &rx, 4);
+	return be32_to_cpu(rx);
+}
+
+/* -------------------------------------------------------------------------- */
+
+#define USE_SHOW_HELP
+#include "ksz_common.c"
+
+#include "ksz_req.c"
+
+/* -------------------------------------------------------------------------- */
+
+#define MIB_READ_INTERVAL		(HZ / 2)
+
+static u8 sw_r8(struct ksz_sw *sw, unsigned reg)
+{
+	return HW_R8(sw->dev, reg);
+}
+
+static u16 sw_r16(struct ksz_sw *sw, unsigned reg)
+{
+	return HW_R16(sw->dev, reg);
+}
+
+static u32 sw_r32(struct ksz_sw *sw, unsigned reg)
+{
+	return HW_R32(sw->dev, reg);
+}
+
+static void sw_w8(struct ksz_sw *sw, unsigned reg, unsigned val)
+{
+	HW_W8(sw->dev, reg, val);
+}
+
+static void sw_w16(struct ksz_sw *sw, unsigned reg, unsigned val)
+{
+	HW_W16(sw->dev, reg, val);
+}
+
+static void sw_w32(struct ksz_sw *sw, unsigned reg, unsigned val)
+{
+	HW_W32(sw->dev, reg, val);
+}
+
+#include "ksz_sw.c"
+
+static struct ksz_sw_reg_ops sw_reg_ops = {
+	.r8			= sw_r8,
+	.r16			= sw_r16,
+	.r32			= sw_r32,
+	.w8			= sw_w8,
+	.w16			= sw_w16,
+	.w32			= sw_w32,
+
+	.r			= sw_r8,
+	.w			= sw_w8,
+
+	.get			= sw_reg_get,
+	.set			= sw_reg_set,
+};
+
+/* -------------------------------------------------------------------------- */
+
+static int ksz8863_probe(struct i2c_client *i2c,
+	const struct i2c_device_id *i2c_id)
+{
+	struct i2c_hw_priv *hw_priv;
+	struct sw_priv *priv;
+	int err;
+
+	priv = kzalloc(sizeof(struct sw_priv), GFP_KERNEL);
+	if (!priv)
+		return -ENOMEM;
+
+	priv->hw_dev = kzalloc(sizeof(struct i2c_hw_priv), GFP_KERNEL);
+	if (!priv->hw_dev) {
+		kfree(priv);
+		return -ENOMEM;
+	}
+	hw_priv = priv->hw_dev;
+
+	hw_priv->i2cdev = i2c;
+
+	priv->dev = &i2c->dev;
+	priv->of_dev = &i2c->dev;
+
+	priv->sw.reg = &sw_reg_ops;
+
+	priv->irq = i2c->irq;
+
+	err = ksz_probe(priv);
+
+	/* This will load the I2C driver again because of mdiobus_register
+	 * failure, but the MAC driver still misses the connection.
+	 */
+	if (err == -EBUSY)
+		err = -EPROBE_DEFER;
+	return err;
+}
+
+static int ksz8863_remove(struct i2c_client *i2c)
+{
+	struct sw_priv *priv = dev_get_drvdata(&i2c->dev);
+
+	return ksz_remove(priv);
+}
+
+#define I2C_SWITCH_NAME			"ksz8863"
+
+#ifndef CONFIG_OF
+/* Please change the I2C address if necessary. */
+#define I2C_SWITCH_ADDR			0x5F
+
+/* Please provide a system interrupt number here. */
+#define I2C_SWITCH_INTR			-1
+
+static int ksz8863_detect(struct i2c_client *i2c, struct i2c_board_info *info)
+{
+	strncpy(info->type, I2C_SWITCH_NAME, I2C_NAME_SIZE);
+	info->irq = I2C_SWITCH_INTR;
+	return 0;
+}
+
+static unsigned short i2c_address_list[] = {
+	I2C_SWITCH_ADDR,
+
+	I2C_CLIENT_END
+};
+#endif
+
+static const struct i2c_device_id i2c_id[] = {
+	{ I2C_SWITCH_NAME, 0 },
+	{ },
+};
+
+#ifdef CONFIG_OF
+static const struct of_device_id ksz8863_dt_ids[] = {
+	{ .compatible = "microchip,ksz8863" },
+	{ .compatible = "microchip,ksz8873" },
+	{},
+};
+MODULE_DEVICE_TABLE(of, ksz8863_dt_ids);
+#endif
+
+static struct i2c_driver ksz8863_driver = {
+	.driver = {
+		.name	= I2C_SWITCH_NAME,
+		.owner	= THIS_MODULE,
+#ifdef CONFIG_OF
+		.of_match_table = of_match_ptr(ksz8863_dt_ids),
+#endif
+	},
+	.probe		= ksz8863_probe,
+	.remove		= ksz8863_remove,
+	.id_table	= i2c_id,
+
+#ifndef CONFIG_OF
+	/* Big enough to be accepted in all cases. */
+	.class		= 0xffff,
+	.detect		= ksz8863_detect,
+	.address_list	= i2c_address_list,
+#endif
+};
+
+static int __init ksz8863_init(void)
+{
+	int ret;
+
+	ret = i2c_add_driver(&ksz8863_driver);
+#ifndef CONFIG_OF
+	if (ret)
+		return ret;
+
+	/* Probe not called. */
+	if (!sw_device_present) {
+		struct i2c_adapter *adap;
+
+		/* Assume I2C bus starts at 0. */
+		adap = i2c_get_adapter(0);
+
+		/* I2C master may not be created yet. */
+		if (!adap) {
+#if !defined(CONFIG_I2C_KSZ8863_MODULE)
+			struct i2c_board_info info = {
+				.type	= I2C_SWITCH_NAME,
+				.addr	= I2C_SWITCH_ADDR,
+				.irq	= I2C_SWITCH_INTR,
+			};
+
+			ret = i2c_register_board_info(0, &info, 1);
+#else
+			return -ENODEV;
+#endif
+		} else
+			i2c_put_adapter(adap);
+	}
+#endif
+	return ret;
+}
+
+static void
+#ifndef CONFIG_KSZ_SWITCH_EMBEDDED
+__exit
+#endif
+ksz8863_exit(void)
+{
+	i2c_del_driver(&ksz8863_driver);
+}
+
+#ifndef CONFIG_KSZ_SWITCH_EMBEDDED
+subsys_initcall(ksz8863_init);
+module_exit(ksz8863_exit);
+#endif
+
+#ifndef CONFIG_KSZ_SWITCH_EMBEDDED
+MODULE_DESCRIPTION("KSZ8863 I2C switch driver");
+MODULE_AUTHOR("Tristram Ha <Tristram.Ha@microchip.com>");
+MODULE_LICENSE("GPL");
+
+MODULE_ALIAS("i2c:ksz8863");
+#endif
diff -Napur linux-linux4microchip-2022.10-orig/drivers/net/ethernet/micrel/i2c-ksz9897.c linux-linux4microchip-2022.10/drivers/net/ethernet/micrel/i2c-ksz9897.c
--- linux-linux4microchip-2022.10-orig/drivers/net/ethernet/micrel/i2c-ksz9897.c	1969-12-31 16:00:00.000000000 -0800
+++ linux-linux4microchip-2022.10/drivers/net/ethernet/micrel/i2c-ksz9897.c	2023-12-08 15:05:51.000000000 -0800
@@ -0,0 +1,665 @@
+/**
+ * Microchip KSZ9897 I2C driver
+ *
+ * Copyright (c) 2015-2023 Microchip Technology Inc.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+ */
+
+#if 0
+#define DEBUG
+#define DBG
+#define DEBUG_MSG
+#if 0
+#define DEBUG_PHY
+#define DBG_LINK
+#endif
+#endif
+
+#ifndef CONFIG_KSZ_SWITCH_EMBEDDED
+#define pr_fmt(fmt) KBUILD_MODNAME ": " fmt
+
+#include <linux/kernel.h>
+#include <linux/module.h>
+#include <linux/interrupt.h>
+#include <linux/of_net.h>
+#include <linux/of_mdio.h>
+#include <linux/phy.h>
+#include <linux/phylink.h>
+#include <linux/platform_device.h>
+#include <linux/sched.h>
+#include <linux/netdevice.h>
+#include <linux/etherdevice.h>
+#include <linux/ethtool.h>
+#include <linux/cache.h>
+#include <linux/crc32.h>
+#include <linux/if_vlan.h>
+#include <linux/ip.h>
+#include <net/ip.h>
+#include <net/ipv6.h>
+#endif
+
+/* -------------------------------------------------------------------------- */
+
+#include <linux/net_tstamp.h>
+#include <linux/i2c.h>
+#include <linux/debugfs.h>
+#include <linux/seq_file.h>
+
+#undef MAX_REQUEST_SIZE
+#define MAX_REQUEST_SIZE		80
+
+#include "ksz_cfg_9897.h"
+
+
+#ifdef CONFIG_KSZ_DLR
+/* Have ACL to handle beacon timeout. */
+#define CONFIG_HAVE_ACL_HW
+
+/* Have DLR to transmit beacons. */
+#define CONFIG_HAVE_DLR_HW
+#endif
+
+#ifdef CONFIG_KSZ_HSR
+#define CONFIG_HAVE_HSR_HW
+#endif
+
+#if 0
+#define USE_MII_PHY
+#endif
+#if 0
+#define USE_RGMII_PHY
+#endif
+#if 0
+#define USE_MII_MODE
+#endif
+#if 0
+#define USE_GMII_MODE
+#endif
+#if 0
+#define USE_RMII_MODE
+#endif
+#if 0
+#define USE_RGMII_MODE
+#endif
+#if 0
+#define USE_GMII_100_MODE
+#endif
+#if 0
+#define USE_10_MBIT_MODE
+#ifndef USE_GMII_100_MODE
+#define USE_GMII_100_MODE
+#endif
+#endif
+#if 0
+#define USE_HALF_DUPLEX
+#endif
+
+
+#define SW_DRV_RELDATE			"Nov 17, 2023"
+#define SW_DRV_VERSION			"1.2.8"
+
+/* -------------------------------------------------------------------------- */
+
+#define HW_R(ks, reg)		i2c_rdreg8(ks, reg)
+#define HW_W(ks, reg, val)	i2c_wrreg8(ks, reg, val)
+#define HW_R8(ks, reg)		i2c_rdreg8(ks, reg)
+#define HW_W8(ks, reg, val)	i2c_wrreg8(ks, reg, val)
+#define HW_R16(ks, reg)		i2c_rdreg16(ks, reg)
+#define HW_W16(ks, reg, val)	i2c_wrreg16(ks, reg, val)
+#define HW_R24(ks, reg)		i2c_rdreg24(ks, reg)
+#define HW_W24(ks, reg, val)	i2c_wrreg24(ks, reg, val)
+#define HW_R32(ks, reg)		i2c_rdreg32(ks, reg)
+#define HW_W32(ks, reg, val)	i2c_wrreg32(ks, reg, val)
+
+#include "ksz_spi_net.h"
+
+/* -------------------------------------------------------------------------- */
+
+#define I2C_CMD_LEN			2
+
+/*
+ * I2C register read/write calls.
+ *
+ * All these calls issue I2C transactions to access the chip's registers. They
+ * all require that the necessary lock is held to prevent accesses when the
+ * chip is busy transfering packet data (RX/TX FIFO accesses).
+ */
+
+/**
+ * i2c_wrreg - issue write register command
+ * @priv:	The switch device structure.
+ * @addr:	The register address.
+ * @txb:	The data buffer to write.
+ * @txl:	The length of data.
+ *
+ * This is the low level write call that issues the necessary i2c message(s)
+ * to write data to the register specified in @addr.
+ */
+static void i2c_wrreg(struct sw_priv *priv, u32 addr, void *txb, size_t txl)
+{
+	struct i2c_hw_priv *hw_priv = priv->hw_dev;
+	struct i2c_msg msg;
+	struct i2c_client *i2c = hw_priv->i2cdev;
+	struct i2c_adapter *adapter = i2c->adapter;
+
+	if (!mutex_is_locked(&priv->lock))
+		pr_alert("W not locked\n");
+
+	hw_priv->txd[0] = (u8)(addr >> 8);
+	hw_priv->txd[1] = (u8) addr;
+
+	/* Own transmit buffer is not being used. */
+	if (txb != hw_priv->txd)
+		memcpy(&hw_priv->txd[I2C_CMD_LEN], txb, txl);
+
+	msg.addr = i2c->addr;
+	msg.flags = 0;
+	msg.len = txl + I2C_CMD_LEN;
+	msg.buf = hw_priv->txd;
+
+	if (i2c_transfer(adapter, &msg, 1) != 1)
+		pr_alert("i2c_transfer() failed\n");
+}
+
+static void i2c_wrreg_size(struct sw_priv *priv, u32 reg, unsigned val,
+			   size_t size)
+{
+	struct i2c_hw_priv *hw_priv = priv->hw_dev;
+	u8 *txb = hw_priv->txd;
+	int i = I2C_CMD_LEN;
+	size_t cnt = size;
+
+	do {
+		txb[i++] = (u8)(val >> (8 * (cnt - 1)));
+		cnt--;
+	} while (cnt);
+	i2c_wrreg(priv, reg, txb, size);
+}
+
+/**
+ * i2c_wrreg32 - write 32bit register value to chip
+ * @priv:	The switch device structure.
+ * @reg:	The register address.
+ * @val:	The value to write.
+ *
+ * Issue a write to put the value @val into the register specified in @reg.
+ */
+static void i2c_wrreg32(struct sw_priv *priv, u32 reg, u32 val)
+{
+	i2c_wrreg_size(priv, reg, val, 4);
+}
+
+/**
+ * i2c_wrreg24 - write 24bit register value to chip
+ * @priv:	The switch device structure.
+ * @reg:	The register address.
+ * @val:	The value to write.
+ *
+ * Issue a write to put the value @val into the register specified in @reg.
+ */
+static void i2c_wrreg24(struct sw_priv *priv, u32 reg, u32 val)
+{
+	i2c_wrreg_size(priv, reg, val, 3);
+}
+
+/**
+ * i2c_wrreg16 - write 16bit register value to chip
+ * @priv:	The switch device structure.
+ * @reg:	The register address.
+ * @val:	The value to write.
+ *
+ * Issue a write to put the value @val into the register specified in @reg.
+ */
+static void i2c_wrreg16(struct sw_priv *priv, u32 reg, u32 val)
+{
+	i2c_wrreg_size(priv, reg, val, 2);
+}
+
+/**
+ * i2c_wrreg8 - write 8bit register value to chip
+ * @priv:	The switch device structure.
+ * @reg:	The register address.
+ * @val:	The value to write.
+ *
+ * Issue a write to put the value @val into the register specified in @reg.
+ */
+static void i2c_wrreg8(struct sw_priv *priv, u32 reg, u32 val)
+{
+	i2c_wrreg_size(priv, reg, val, 1);
+}
+
+/**
+ * i2c_rdreg - issue read register command and return the data
+ * @priv:	The switch device structure.
+ * @addr:	The register address.
+ * @rxb:	The RX buffer to return the result into.
+ * @rxl:	The length of data expected.
+ *
+ * This is the low level read call that issues the necessary i2c message(s)
+ * to read data from the register specified in @addr.
+ */
+static void i2c_rdreg(struct sw_priv *priv, u32 addr, void *rxb, size_t rxl)
+{
+	struct i2c_hw_priv *hw_priv = priv->hw_dev;
+	struct i2c_msg msg[2];
+	struct i2c_client *i2c = hw_priv->i2cdev;
+	struct i2c_adapter *adapter = i2c->adapter;
+
+	if (!mutex_is_locked(&priv->lock))
+		pr_alert("R not locked\n");
+
+	hw_priv->txd[0] = (u8)(addr >> 8);
+	hw_priv->txd[1] = (u8) addr;
+
+	msg[0].addr = i2c->addr;
+	msg[0].flags = 0;
+	msg[0].len = I2C_CMD_LEN;
+	msg[0].buf = hw_priv->txd;
+
+	msg[1].addr = i2c->addr;
+	msg[1].flags = I2C_M_RD;
+	msg[1].len = rxl;
+	msg[1].buf = rxb;
+
+	if (i2c_transfer(adapter, msg, 2) != 2)
+		pr_alert("i2c_transfer() failed\n");
+}
+
+/**
+ * i2c_rdreg8 - read 8 bit register from device
+ * @priv:	The switch device structure.
+ * @reg:	The register address.
+ *
+ * Read a 8bit register from the chip, returning the result.
+ */
+static u8 i2c_rdreg8(struct sw_priv *priv, u32 reg)
+{
+	u8 rxb[1];
+
+	i2c_rdreg(priv, reg, rxb, 1);
+	return rxb[0];
+}
+
+/**
+ * i2c_rdreg16 - read 16 bit register from device
+ * @priv:	The switch device structure.
+ * @reg:	The register address.
+ *
+ * Read a 16bit register from the chip, returning the result.
+ */
+static u16 i2c_rdreg16(struct sw_priv *priv, u32 reg)
+{
+	__le16 rx = 0;
+
+	i2c_rdreg(priv, reg, &rx, 2);
+	return be16_to_cpu(rx);
+}
+
+/**
+ * i2c_rdreg24 - read 24 bit register from device
+ * @priv:	The switch device structure.
+ * @reg:	The register address.
+ *
+ * Read a 24bit register from the chip.
+ */
+static u32 i2c_rdreg24(struct sw_priv *priv, u32 reg)
+{
+	__le32 rx = 0;
+
+	i2c_rdreg(priv, reg, &rx, 3);
+	return be32_to_cpu(rx);
+}
+
+/**
+ * i2c_rdreg32 - read 32 bit register from device
+ * @priv:	The switch device structure.
+ * @reg:	The register address.
+ *
+ * Read a 32bit register from the chip.
+ */
+static u32 i2c_rdreg32(struct sw_priv *priv, u32 reg)
+{
+	__le32 rx = 0;
+
+	i2c_rdreg(priv, reg, &rx, 4);
+	return be32_to_cpu(rx);
+}
+
+/* -------------------------------------------------------------------------- */
+
+#define USE_SHOW_HELP
+#include "ksz_common.c"
+
+/* For ksz_request used by PTP or MRP driver. */
+#include "ksz_req.c"
+
+/* -------------------------------------------------------------------------- */
+
+#define MIB_READ_INTERVAL		(HZ / 2)
+
+static void sw_lock(struct ksz_sw *sw)
+{
+	const struct ksz_sw_reg_ops *reg_ops = sw->reg;
+
+	mutex_lock(sw->reglock);
+	if (sw->reg != reg_ops) {
+printk(KERN_ALERT "%s changed\n", __func__);
+		mutex_unlock(sw->reglock);
+		sw->reg->lock(sw);
+	}
+}  /* sw_lock */
+
+static void sw_unlock(struct ksz_sw *sw)
+{
+	mutex_unlock(sw->reglock);
+}  /* sw_unlock */
+
+static void sw_r(struct ksz_sw *sw, unsigned reg, void *buf, size_t cnt)
+{
+	i2c_rdreg(sw->dev, reg, buf, cnt);
+}
+
+static void sw_w(struct ksz_sw *sw, unsigned reg, void *buf, size_t cnt)
+{
+	i2c_wrreg(sw->dev, reg, buf, cnt);
+}
+
+static u8 sw_r8(struct ksz_sw *sw, unsigned reg)
+{
+	return HW_R8(sw->dev, reg);
+}
+
+static u16 sw_r16(struct ksz_sw *sw, unsigned reg)
+{
+	return HW_R16(sw->dev, reg);
+}
+
+static u32 sw_r24(struct ksz_sw *sw, unsigned reg)
+{
+	return HW_R24(sw->dev, reg);
+}
+
+static u32 sw_r32(struct ksz_sw *sw, unsigned reg)
+{
+	return HW_R32(sw->dev, reg);
+}
+
+static void sw_w8(struct ksz_sw *sw, unsigned reg, unsigned val)
+{
+	HW_W8(sw->dev, reg, val);
+}
+
+static void sw_w16(struct ksz_sw *sw, unsigned reg, unsigned val)
+{
+/* THa  05/30/2018
+ * Write to PHY register range 0x100-0x11e will trigger an APB timeout
+ * interrupt if the last byte does not end at 32-bit boundary.  Similar code
+ * should be implemented for 8-bit write.  However, as PHY register access is
+ * done with 16-bit it is not implemented.
+ */
+	if (reg & 0x7000) {
+		unsigned base = reg & 0xfff;
+		int shift = reg & 3;
+
+		if (base >= REG_PORT_PHY_CTRL &&
+		    base < REG_PORT_PHY_EXTENDED_STATUS && shift < 2) {
+			u32 val32 = 0;
+
+			/* Cannot read during MMD write.  However, next
+			 * register is read-only.
+			 */
+			if (base != REG_PORT_PHY_MMD_INDEX_DATA)
+				val32 = HW_R32(sw->dev, reg & ~3);
+			shift = 2 - shift;
+			shift *= 8;
+			val32 &= ~(0xffff << shift);
+			val32 |= ((val & 0xffff) << shift);
+			HW_W32(sw->dev, reg & ~3, val32);
+			return;
+		}
+	}
+	HW_W16(sw->dev, reg, val);
+}
+
+static void sw_w24(struct ksz_sw *sw, unsigned reg, unsigned val)
+{
+	HW_W24(sw->dev, reg, val);
+}
+
+static void sw_w32(struct ksz_sw *sw, unsigned reg, unsigned val)
+{
+	HW_W32(sw->dev, reg, val);
+}
+
+#include "ksz_sw_9897.c"
+
+static struct ksz_sw_reg_ops sw_reg_ops = {
+	.lock			= sw_lock,
+	.unlock			= sw_unlock,
+
+	.r8			= sw_r8,
+	.r16			= sw_r16,
+	.r24			= sw_r24,
+	.r32			= sw_r32,
+	.w8			= sw_w8,
+	.w16			= sw_w16,
+	.w24			= sw_w24,
+	.w32			= sw_w32,
+
+	.r			= sw_r,
+	.w			= sw_w,
+
+	.get			= sw_reg_get,
+	.set			= sw_reg_set,
+
+	.r_dyn_mac_hw		= sw_r_dyn_mac_hw,
+	.w_dyn_mac_hw		= sw_w_dyn_mac_hw,
+	.start_dyn_mac_hw	= sw_start_dyn_mac_hw,
+	.g_dyn_mac_hw		= sw_g_dyn_mac_hw,
+	.stop_dyn_mac_hw	= sw_stop_dyn_mac_hw,
+	.r_sta_mac_hw		= sw_r_sta_mac_hw,
+	.w_sta_mac_hw		= sw_w_sta_mac_hw,
+	.r_vlan_hw		= sw_r_vlan_hw,
+	.w_vlan_hw		= sw_w_vlan_hw,
+	.r_mib_cnt_hw		= sw_r_mib_cnt_hw,
+	.r_acl_hw		= sw_r_acl_hw,
+	.w_acl_hw		= sw_w_acl_hw,
+
+#ifdef CONFIG_KSZ_HSR
+	.r_hsr_hw		= sw_r_hsr_hw,
+	.w_hsr_hw		= sw_w_hsr_hw,
+	.start_hsr_hw		= sw_start_hsr_hw,
+	.g_hsr_hw		= sw_g_hsr_hw,
+	.stop_hsr_hw		= sw_stop_hsr_hw,
+#endif
+};
+
+/* -------------------------------------------------------------------------- */
+
+#define MAX_I2C_DEVICES		1
+
+static int ksz9897_probe(struct i2c_client *i2c,
+	const struct i2c_device_id *i2c_id)
+{
+	struct i2c_hw_priv *hw_priv;
+	struct sw_priv *priv;
+	int err;
+
+	priv = kzalloc(sizeof(struct sw_priv), GFP_KERNEL);
+	if (!priv)
+		return -ENOMEM;
+
+	priv->hw_dev = kzalloc(sizeof(struct i2c_hw_priv), GFP_KERNEL);
+	if (!priv->hw_dev) {
+		kfree(priv);
+		return -ENOMEM;
+	}
+	hw_priv = priv->hw_dev;
+
+	hw_priv->i2cdev = i2c;
+
+	priv->dev = &i2c->dev;
+	priv->of_dev = &i2c->dev;
+
+	priv->sw.reg = &sw_reg_ops;
+
+	priv->irq = i2c->irq;
+
+	err = ksz_probe(priv);
+
+	/* This will load the I2C driver again because of mdiobus_register
+	 * failure, but the MAC driver still misses the connection.
+	 */
+	if (err == -EBUSY)
+		err = -EPROBE_DEFER;
+	return err;
+}
+
+static int ksz9897_remove(struct i2c_client *i2c)
+{
+	struct sw_priv *priv = dev_get_drvdata(&i2c->dev);
+
+	return ksz_remove(priv);
+}
+
+#define I2C_SWITCH_NAME			"ksz9897"
+
+#ifndef CONFIG_OF
+/* Please change the I2C address if necessary. */
+#define I2C_SWITCH_ADDR			0x5F
+
+/* Please provide a system interrupt number here. */
+#define I2C_SWITCH_INTR			-1
+
+static int ksz9897_detect(struct i2c_client *i2c, struct i2c_board_info *info)
+{
+	strncpy(info->type, I2C_SWITCH_NAME, I2C_NAME_SIZE);
+	info->irq = I2C_SWITCH_INTR;
+	return 0;
+}
+
+static unsigned short i2c_address_list[] = {
+	I2C_SWITCH_ADDR,
+
+	I2C_CLIENT_END
+};
+#endif
+
+static const struct i2c_device_id i2c_id[] = {
+	{ I2C_SWITCH_NAME, 0 },
+	{ },
+};
+
+#ifdef CONFIG_OF
+static const struct of_device_id ksz9897_dt_ids[] = {
+	{ .compatible = "microchip,ksz9897" },
+	{ .compatible = "microchip,ksz9567" },
+	{ .compatible = "microchip,ksz9477" },
+	{ .compatible = "microchip,ksz9896" },
+	{ .compatible = "microchip,ksz9566" },
+	{ .compatible = "microchip,ksz8565" },
+	{ .compatible = "microchip,ksz9893" },
+	{ .compatible = "microchip,ksz9563" },
+	{ .compatible = "microchip,ksz8563" },
+	{ .compatible = "microchip,lan9646" },
+	{},
+};
+MODULE_DEVICE_TABLE(of, ksz9897_dt_ids);
+#endif
+
+static struct i2c_driver ksz9897_driver = {
+	.driver = {
+		.name	= I2C_SWITCH_NAME,
+		.owner	= THIS_MODULE,
+#ifdef CONFIG_OF
+		.of_match_table = of_match_ptr(ksz9897_dt_ids),
+#endif
+	},
+	.probe		= ksz9897_probe,
+	.remove		= ksz9897_remove,
+	.id_table	= i2c_id,
+
+#ifndef CONFIG_OF
+	/* Big enough to be accepted in all cases. */
+	.class		= 0xffff,
+	.detect		= ksz9897_detect,
+	.address_list	= i2c_address_list,
+#endif
+};
+
+static int __init ksz9897_init(void)
+{
+	int ret;
+
+#ifdef DEBUG_MSG
+	if (init_dbg())
+		return -ENOMEM;
+#endif
+	ret = i2c_add_driver(&ksz9897_driver);
+#ifndef CONFIG_OF
+	if (ret)
+		return ret;
+
+	/* Probe not called. */
+	if (!sw_device_present) {
+		struct i2c_adapter *adap;
+
+		/* Assume I2C bus starts at 0. */
+		adap = i2c_get_adapter(0);
+
+		/* I2C master may not be created yet. */
+		if (!adap) {
+#if !defined(CONFIG_I2C_KSZ9897_MODULE)
+			struct i2c_board_info info = {
+				.type	= I2C_SWITCH_NAME,
+				.addr	= I2C_SWITCH_ADDR,
+				.irq	= I2C_SWITCH_INTR,
+			};
+
+			ret = i2c_register_board_info(0, &info, 1);
+#else
+			return -ENODEV;
+#endif
+		} else
+			i2c_put_adapter(adap);
+	}
+#endif
+	return ret;
+}
+
+static void
+#ifndef CONFIG_KSZ_SWITCH_EMBEDDED
+__exit
+#endif
+ksz9897_exit(void)
+{
+	i2c_del_driver(&ksz9897_driver);
+#ifdef DEBUG_MSG
+	exit_dbg();
+#endif
+}
+
+#ifndef CONFIG_KSZ_SWITCH_EMBEDDED
+subsys_initcall(ksz9897_init);
+module_exit(ksz9897_exit);
+#endif
+
+#ifndef CONFIG_KSZ_SWITCH_EMBEDDED
+MODULE_DESCRIPTION("Microchip KSZ9897 I2C Switch Driver");
+MODULE_AUTHOR("Tristram Ha <Tristram.Ha@microchip.com>");
+MODULE_LICENSE("GPL");
+
+MODULE_ALIAS("i2c:ksz9897");
+#endif
diff -Napur linux-linux4microchip-2022.10-orig/drivers/net/ethernet/micrel/iba-ksz9897.c linux-linux4microchip-2022.10/drivers/net/ethernet/micrel/iba-ksz9897.c
--- linux-linux4microchip-2022.10-orig/drivers/net/ethernet/micrel/iba-ksz9897.c	1969-12-31 16:00:00.000000000 -0800
+++ linux-linux4microchip-2022.10/drivers/net/ethernet/micrel/iba-ksz9897.c	2023-11-20 13:45:47.000000000 -0800
@@ -0,0 +1,109 @@
+/**
+ * Microchip KSZ9897 IBA driver
+ *
+ * Copyright (c) 2015-2023 Microchip Technology Inc.
+ * Copyright (c) 2013-2015 Micrel, Inc.
+ *
+ * Copyright 2009 Simtec Electronics
+ *	http://www.simtec.co.uk/
+ *	Ben Dooks <ben@simtec.co.uk>
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+ */
+
+#if 0
+#define DEBUG
+#define DBG
+#define DEBUG_MSG
+#if 0
+#define DEBUG_PHY
+#define DBG_LINK
+#endif
+#endif
+
+/* -------------------------------------------------------------------------- */
+
+#include <linux/net_tstamp.h>
+
+#undef MAX_REQUEST_SIZE
+#define MAX_REQUEST_SIZE		80
+
+#include "ksz_cfg_9897.h"
+
+
+#ifdef CONFIG_KSZ_DLR
+/* Have ACL to handle beacon timeout. */
+#define CONFIG_HAVE_ACL_HW
+
+/* Have DLR to transmit beacons. */
+#define CONFIG_HAVE_DLR_HW
+#endif
+
+#ifdef CONFIG_KSZ_HSR
+#define CONFIG_HAVE_HSR_HW
+#endif
+
+#if 0
+#define USE_MII_PHY
+#endif
+#if 0
+#define USE_RGMII_PHY
+#endif
+#if 0
+#define USE_MII_MODE
+#endif
+#if 0
+#define USE_GMII_MODE
+#endif
+#if 0
+#define USE_RMII_MODE
+#endif
+#if 0
+#define USE_RGMII_MODE
+#endif
+#if 0
+#define USE_GMII_100_MODE
+#endif
+#if 0
+#define USE_10_MBIT_MODE
+#ifndef USE_GMII_100_MODE
+#define USE_GMII_100_MODE
+#endif
+#endif
+#if 0
+#define USE_HALF_DUPLEX
+#endif
+
+
+#define KS9897MLI_DEV0			"ksz9897"
+
+#define SW_DRV_RELDATE			"Nov 17, 2023"
+#define SW_DRV_VERSION			"1.2.8"
+
+/* -------------------------------------------------------------------------- */
+
+#include "ksz_spi_net.h"
+
+#define USE_SHOW_HELP
+#include "ksz_common.c"
+
+/* For ksz_request used by PTP or MRP driver. */
+#include "ksz_req.c"
+
+/* -------------------------------------------------------------------------- */
+
+#define MIB_READ_INTERVAL		(HZ / 2)
+
+#include "ksz_sw_9897.c"
+
diff -Napur linux-linux4microchip-2022.10-orig/drivers/net/ethernet/micrel/Kconfig linux-linux4microchip-2022.10/drivers/net/ethernet/micrel/Kconfig
--- linux-linux4microchip-2022.10-orig/drivers/net/ethernet/micrel/Kconfig	2022-11-09 04:48:42.000000000 -0800
+++ linux-linux4microchip-2022.10/drivers/net/ethernet/micrel/Kconfig	2023-04-25 16:13:54.860163472 -0700
@@ -60,4 +60,215 @@ config KSZ884X_PCI
 	  To compile this driver as a module, choose M here. The module
 	  will be called ksz884x.
 
+config KSZ8462_HLI
+	tristate "Microchip KSZ8462/KSZ8441/KSZ8852 HLI support"
+	select MII
+	select CRC32
+	select EEPROM_93CX6
+	help
+	  This platform driver is for Microchip KSZ8462/KSZ8441/KSZ8852
+	  address/data bus multiplexed network chip.
+
+config KSZ_PTP
+	bool "1588 PTP support"
+	depends on KSZ8462_HLI || HAVE_KSZ8463 || HAVE_KSZ9897
+	default y
+	help
+	  Enable 1588 PTP support.
+
+config KSZ_STP
+	bool "STP support"
+	depends on KSZ8462_HLI || KSZ_SWITCH
+	default y
+	help
+	  Enable STP support.
+
+menuconfig KSZ_SWITCHES
+	tristate "Drivers for Microchip KSZ switches"
+	help
+	  Supports Microchip KSZ switches.
+
+if KSZ_SWITCHES
+
+comment "Microchip KSZ switch device drivers"
+
+config KSZ_SWITCH
+	bool
+	default n
+
+config KSZ_SMI
+	bool
+	default n
+
+config KSZ_SWITCH_EMBEDDED
+	bool
+	default n
+	select KSZ_SWITCH
+
+config HAVE_KSZ8463
+	bool
+	default n
+
+config SPI_KSZ8463
+	tristate "SPI driver for Microchip KSZ8463 switch"
+	select KSZ_SWITCH
+	select HAVE_KSZ8463
+	help
+	  Supports the Microchip KSZ8463 switch in SPI mode.
+
+config HAVE_KSZ8863
+	bool
+	default n
+
+config I2C_KSZ8863
+	tristate "I2C driver for Microchip KSZ8863 switch"
+	select KSZ_SWITCH
+	select HAVE_KSZ8863
+	help
+	  Supports the Microchip KSZ8863 switch in I2C mode.
+
+config SPI_KSZ8863
+	tristate "SPI driver for Microchip KSZ8863 switch"
+	select KSZ_SWITCH
+	select HAVE_KSZ8863
+	help
+	  Supports the Microchip KSZ8863 switch in SPI mode.
+
+config SMI_KSZ8863
+	bool "SMI driver for Microchip KSZ8863 switch"
+	depends on SPI_KSZ8863 = n && I2C_KSZ8863 = n
+	select KSZ_SWITCH_EMBEDDED
+	select KSZ_SMI
+	select HAVE_KSZ8863
+	help
+	  Supports the Microchip KSZ8863 switch in SMI mode.
+
+config HAVE_KSZ8795
+	bool
+	default n
+
+config SPI_KSZ8795
+	tristate "SPI driver for Microchip KSZ8795 switch"
+	select KSZ_SWITCH
+	select HAVE_KSZ8795
+	help
+	  Supports the Microchip KSZ8795 switch in SPI mode.
+
+config KSZ8795_EMBEDDED
+	bool "Microchip KSZ8795 switch support in network controller"
+	depends on SPI_KSZ8795 = n
+	select KSZ_SWITCH_EMBEDDED
+	select HAVE_KSZ8795
+	help
+	  Supports the Microchip KSZ8795 switch used within a network controller.
+
+config HAVE_KSZ8895
+	bool
+	default n
+
+config SPI_KSZ8895
+	tristate "SPI driver for Microchip KSZ8895 switch"
+	select KSZ_SWITCH
+	select HAVE_KSZ8895
+	help
+	  Supports the Microchip KSZ8895 switch in SPI mode.
+
+config SMI_KSZ8895
+	bool "SMI driver for Microchip KSZ8895 switch"
+	depends on SPI_KSZ8895 = n
+	select KSZ_SWITCH_EMBEDDED
+	select KSZ_SMI
+	select HAVE_KSZ8895
+	help
+	  Supports the Microchip KSZ8895 switch in SMI mode.
+
+config HAVE_KSZ9897
+	bool
+	default n
+
+config I2C_KSZ9897
+	tristate "I2C driver for Microchip KSZ9897 switch"
+	select KSZ_SWITCH
+	select HAVE_KSZ9897
+	help
+	  Supports the Microchip KSZ9897 switch in I2C mode.
+
+config SPI_KSZ9897
+	tristate "SPI driver for Microchip KSZ9897 switch"
+	select KSZ_SWITCH
+	select HAVE_KSZ9897
+	help
+	  Supports the Microchip KSZ9897 switch in SPI mode.
+
+config KSZ9897_EMBEDDED
+	bool "Microchip KSZ9897 switch support in network controller"
+	depends on I2C_KSZ9897 = n && SPI_KSZ9897 = n
+	select KSZ_SWITCH_EMBEDDED
+	select HAVE_KSZ9897
+	help
+	  Supports the Microchip KSZ9897 switch used within a network controller.
+
+config IBA_KSZ9897
+	bool "IBA driver for Microchip KSZ9897 switch"
+	depends on I2C_KSZ9897 = n && SPI_KSZ9897 = n
+	select KSZ_SWITCH_EMBEDDED
+	select KSZ_IBA
+	select HAVE_KSZ9897
+	help
+	  Supports the Microchip KSZ9897 switch in IBA mode.
+
+config KSZ_IBA
+	bool "IBA support"
+	depends on HAVE_KSZ9897
+	default y
+	help
+	  Enable IBA support.
+
+config KSZ_AVB
+	bool "AVB support"
+	depends on HAVE_KSZ9897
+	default y
+	help
+	  Enable AVB support.
+
+config KSZ_MRP
+	bool "MRP support"
+	depends on KSZ_SWITCH && HAVE_KSZ9897
+	default y if (HAVE_KSZ9897)
+	help
+	  Enable MRP support.
+
+config KSZ_MSRP
+	bool "MSRP support"
+	depends on KSZ_MRP && KSZ_AVB
+	default y
+	help
+	  Enable MSRP support.
+
+config KSZ_MSTP
+	bool "MSTP support"
+	depends on HAVE_KSZ9897
+	select KSZ_STP
+	select CRYPTO_HMAC
+	select CRYPTO_MD5
+	default n
+	help
+	  Enable MSTP support.
+
+config KSZ_DLR
+	bool "DLR support"
+	depends on (HAVE_KSZ9897 || HAVE_KSZ8795) && KSZ_SWITCH
+	default y if (HAVE_KSZ9897)
+	help
+	  Enable DLR support.
+
+config KSZ_HSR
+	bool "HSR support"
+	depends on (HAVE_KSZ9897) && KSZ_SWITCH
+	default y if (HAVE_KSZ9897)
+	help
+	  Enable HSR support.
+
+endif
+
 endif # NET_VENDOR_MICREL
diff -Napur linux-linux4microchip-2022.10-orig/drivers/net/ethernet/micrel/ksz8462.h linux-linux4microchip-2022.10/drivers/net/ethernet/micrel/ksz8462.h
--- linux-linux4microchip-2022.10-orig/drivers/net/ethernet/micrel/ksz8462.h	1969-12-31 16:00:00.000000000 -0800
+++ linux-linux4microchip-2022.10/drivers/net/ethernet/micrel/ksz8462.h	2023-04-25 16:13:54.860163472 -0700
@@ -0,0 +1,316 @@
+/**
+ * KSZ8462 register definitions
+ *
+ * Copyright 2015 Microchip Technology Inc.
+ *
+ * Copyright 2011-2014 Micrel, Inc.
+ *
+ * Copyright 2009 Simtec Electronics
+ *      Ben Dooks <ben@simtec.co.uk>
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+*/
+
+
+#ifndef __KSZ8462_H
+#define __KSZ8462_H
+
+
+#define KS_CCR				0x108
+#define CCR_LITTLE_ENDIAN		(1 << 10)
+#define CCR_EEPROM			(1 << 9)
+#define CCR_SPI				(1 << 8)
+#define CCR_8BIT			(1 << 7)
+#define CCR_16BIT			(1 << 6)
+#define CCR_32BIT			(1 << 5)
+#define CCR_SHARED			(1 << 4)
+#define CCR_64PIN			(1 << 1)
+#define CCR_32PIN			(1 << 0)
+
+/* MAC address registers */
+#define KS_MARL				0x110
+#define KS_MARM				0x112
+#define KS_MARH				0x114
+
+#define KS_EEPCR			0x122
+#define EEPCR_EEDW			(1 << 5)
+#define EEPCR_EESA			(1 << 4)
+#define EEPCR_EESB			(1 << 3)
+#define EEPCR_EEDO			(1 << 2)
+#define EEPCR_EESCK			(1 << 1)
+#define EEPCR_EECS			(1 << 0)
+
+#define KS_MBIR				0x124
+#define MBIR_DONE			(1 << 15)
+#define MBIR_TXMBF			(1 << 12)
+#define MBIR_TXMBFA			(1 << 11)
+#define MBIR_TX_SHIFT			(8)
+#define MBIR_RXMBF			(1 << 4)
+#define MBIR_RXMBFA			(1 << 3)
+#define MBIR_RX_SHIFT			(0)
+#define MBIR_FAIL_CNT			(0x7)
+
+#define KS_GRR				0x126
+#define GRR_BIST			(1 << 3)
+#define GRR_PTP				(1 << 2)
+#define GRR_QMU				(1 << 1)
+#define GRR_GSR				(1 << 0)
+
+#define KS_WFCR				0x12A
+#define WFCR_MPRXE			(1 << 7)
+#define WFCR_WF3E			(1 << 3)
+#define WFCR_WF2E			(1 << 2)
+#define WFCR_WF1E			(1 << 1)
+#define WFCR_WF0E			(1 << 0)
+
+#define KS_WF0CRC0			0x130
+#define KS_WF0CRC1			0x132
+#define KS_WF0BM0			0x134
+#define KS_WF0BM1			0x136
+#define KS_WF0BM2			0x138
+#define KS_WF0BM3			0x13A
+
+#define KS_WF1CRC0			0x140
+#define KS_WF1CRC1			0x142
+#define KS_WF1BM0			0x144
+#define KS_WF1BM1			0x146
+#define KS_WF1BM2			0x148
+#define KS_WF1BM3			0x14A
+
+#define KS_WF2CRC0			0x150
+#define KS_WF2CRC1			0x152
+#define KS_WF2BM0			0x154
+#define KS_WF2BM1			0x156
+#define KS_WF2BM2			0x158
+#define KS_WF2BM3			0x15A
+
+#define KS_WF3CRC0			0x160
+#define KS_WF3CRC1			0x162
+#define KS_WF3BM0			0x164
+#define KS_WF3BM1			0x166
+#define KS_WF3BM2			0x168
+#define KS_WF3BM3			0x16A
+
+#define KS_TXCR				0x170
+#define TXCR_TCGICMP			(1 << 8)
+#define TXCR_TCGUDP			(1 << 7)
+#define TXCR_TCGTCP			(1 << 6)
+#define TXCR_TCGIP			(1 << 5)
+#define TXCR_FTXQ			(1 << 4)
+#define TXCR_TXFCE			(1 << 3)
+#define TXCR_TXPE			(1 << 2)
+#define TXCR_TXCRC			(1 << 1)
+#define TXCR_TXE			(1 << 0)
+
+#define KS_TXSR				0x172
+#define TXSR_TXLC			(1 << 13)
+#define TXSR_TXMC			(1 << 12)
+#define TXFID_MASK			(0x3f)
+#define TXFID_SHIFT			(0)
+#define TXSR_TXFID_MASK			(TXFID_MASK << TXFID_SHIFT)
+#define TXSR_TXFID_GET(_v)		(((_v) >> TXFID_SHIFT) & TXFID_MASK)
+
+#define KS_RXCR1			0x174
+#define RXCR1_FRXQ			(1 << 15)
+#define RXCR1_RXUDPFCC			(1 << 14)
+#define RXCR1_RXTCPFCC			(1 << 13)
+#define RXCR1_RXIPFCC			(1 << 12)
+#define RXCR1_RXPAFMA			(1 << 11)
+#define RXCR1_RXFCE			(1 << 10)
+#define RXCR1_RXEFE			(1 << 9)
+#define RXCR1_RXMAFMA			(1 << 8)
+#define RXCR1_RXBE			(1 << 7)
+#define RXCR1_RXME			(1 << 6)
+#define RXCR1_RXUE			(1 << 5)
+#define RXCR1_RXAE			(1 << 4)
+#define RXCR1_RXINVF			(1 << 1)
+#define RXCR1_RXE			(1 << 0)
+#define RXCR1_FILTER_MASK		(RXCR1_RXINVF | RXCR1_RXAE | \
+					 RXCR1_RXMAFMA | RXCR1_RXPAFMA)
+
+#define RX_TYPE_CFG_2			(1 << 11)
+#define RX_TYPE_CFG_1			(1 << 8)
+#define RX_TYPE_ALL			(1 << 4)
+#define RX_TYPE_INVERSE			(1 << 1)
+
+#define RX_PROMISCUOUS			(RX_TYPE_ALL | RX_TYPE_INVERSE)
+#define RX_ALL_HASH			(0)
+#define RX_ALL_HASH_INVERSE		(RX_ALL_HASH | RX_TYPE_INVERSE)
+#define RX_ALL_SAME			(RX_TYPE_CFG_2 | RX_TYPE_CFG_1)
+#define RX_ALL_SAME_INVERSE		(RX_ALL_SAME | RX_TYPE_INVERSE)
+#define RX_UNI_SAME_MULTI_HASH		(RX_TYPE_CFG_2)
+#define RX_UNI_SAME_MULTI_HASH_INVERSE	\
+	(RX_UNI_SAME_MULTI_HASH | RX_TYPE_INVERSE)
+#define RX_UNI_HASH_ALL_MULTI		(RX_TYPE_ALL)
+#define RX_UNI_SAME_ALL_MULTI		\
+	(RX_TYPE_CFG_2 | RX_TYPE_CFG_1 | RX_TYPE_ALL)
+#define RX_MULTI_HASH_ALL_UNI		(RX_TYPE_CFG_2 | RX_TYPE_ALL)
+#define RX_MULTI_SAME_ALL_UNI		(RX_TYPE_CFG_1 | RX_TYPE_ALL)
+
+#define KS_RXCR2			0x176
+#ifdef KSZ846X_SLI
+#define RXCR2_SRDBL_MASK		(0x7 << 5)
+#define RXCR2_SRDBL_SHIFT		(5)
+#define RXCR2_SRDBL_4B			(0x0 << 5)
+#define RXCR2_SRDBL_8B			(0x1 << 5)
+#define RXCR2_SRDBL_16B			(0x2 << 5)
+#define RXCR2_SRDBL_32B			(0x3 << 5)
+#define RXCR2_SRDBL_FRAME		(0x4 << 5)
+#endif
+#define RXCR2_PAUSE_TIMER		(1 << 8)
+#define RXCR2_IUFFP			(1 << 4)
+#define RXCR2_RXIUFCEZ			(1 << 3)
+#define RXCR2_UDPLFE			(1 << 2)
+#define RXCR2_RXICMPFCC			(1 << 1)
+#define RXCR2_RXSAF			(1 << 0)
+
+#define KS_TXMIR			0x178
+
+#define KS_RXBUFSIZE			0x17A
+
+#define KS_RXFHSR			0x17C
+#define RXFSHR_RXFV			(1 << 15)
+#define RXFSHR_RXICMPFCS		(1 << 13)
+#define RXFSHR_RXIPFCS			(1 << 12)
+#define RXFSHR_RXTCPFCS			(1 << 11)
+#define RXFSHR_RXUDPFCS			(1 << 10)
+#define RXFSHR_RXBF			(1 << 7)
+#define RXFSHR_RXMF			(1 << 6)
+#define RXFSHR_RXUF			(1 << 5)
+#define RXFSHR_RXMR			(1 << 4)
+#define RXFSHR_RXFT			(1 << 3)
+#define RXFSHR_RXFTL			(1 << 2)
+#define RXFSHR_RXRF			(1 << 1)
+#define RXFSHR_RXCE			(1 << 0)
+#define	RXFSHR_ERR			(RXFSHR_RXCE | RXFSHR_RXRF | \
+					RXFSHR_RXFTL | RXFSHR_RXMR | \
+					RXFSHR_RXIPFCS | RXFSHR_RXTCPFCS)
+
+#define KS_RXFHBCR			0x17E
+#define RXFHBCR_CNT_MASK		(0x0fff)
+
+#define KS_TXQCR			0x180
+#define TXQCR_TXQMAM			(1 << 1)
+#define TXQCR_METFE			(1 << 0)
+
+#define KS_RXQCR			0x182
+#define RXQCR_RXDTTS			(1 << 12)
+#define RXQCR_RXDBCTS			(1 << 11)
+#define RXQCR_RXFCTS			(1 << 10)
+#define RXQCR_RXIPHTOE			(1 << 9)
+#define RXQCR_RXDTTE			(1 << 7)
+#define RXQCR_RXDBCTE			(1 << 6)
+#define RXQCR_RXFCTE			(1 << 5)
+#define RXQCR_ADRFE			(1 << 4)
+#define RXQCR_SDA			(1 << 3)
+#define RXQCR_RRXEF			(1 << 0)
+#define RXQCR_CMD_CNTL			(RXQCR_RXFCTE | RXQCR_ADRFE)
+#define RXQCR_STATUS			\
+	(RXQCR_RXDTTS | RXQCR_RXDBCTS | RXQCR_RXFCTS)
+
+#define KS_TXFDPR			0x184
+#define TXFDPR_TXFPAI			(1 << 14)
+#define TXFDPR_TXFP_MASK		(0x7ff)
+
+#define KS_RXFDPR			0x186
+#define RXFDPR_RXFPAI			(1 << 14)
+#define RXFDPR_EMS			(1 << 11)
+
+#define KS_RXDTTR			0x18C
+#define RXDTTR_MAX			0xcfff
+
+#define KS_RXDBCTR			0x18E
+
+#define KS_IER				0x190
+#define KS_ISR				0x192
+#define IRQ_LCI				(1 << 15)
+#define IRQ_TXI				(1 << 14)
+#define IRQ_RXI				(1 << 13)
+#define IRQ_TS				(1 << 12)
+#define IRQ_RXOI			(1 << 11)
+#define IRQ_TRIG			(1 << 10)
+#define IRQ_TXPSI			(1 << 9)
+#define IRQ_RXPSI			(1 << 8)
+#define IRQ_TXSAI			(1 << 6)
+#define IRQ_RXWFDI			(1 << 5)
+#define IRQ_RXMPDI			(1 << 4)
+#define IRQ_LDI				(1 << 3)
+#define IRQ_EDI				(1 << 2)
+#define IRQ_SPIBEI			(1 << 1)
+#define IRQ_DEDI			(1 << 0)
+
+#define KS_RXFTR			0x19C
+#define RXFTR_MASK			(0x00ff)
+
+#define KS_TXNTFSR			0x19E
+
+#define KS_MAHTR0			0x1A0
+#define KS_MAHTR1			0x1A2
+#define KS_MAHTR2			0x1A4
+#define KS_MAHTR3			0x1A6
+
+#define KS_FCLWR			0x1B0
+#define KS_FCHWR			0x1B2
+#define KS_FCOWR			0x1B4
+
+#define KS_RXFC				0x1B8
+#define RXFC_MASK			(0xff)
+#define RXFC_SHIFT			(8)
+#define RXFC_GET(_v)			(((_v) >> RXFC_SHIFT) & RXFC_MASK)
+
+#define KS_CIDER			0x0
+#ifdef KSZ846X_HLI
+#define CIDER_ID_8462			(0x8430)
+#define CIDER_ID_8441			(0x8410)
+#endif
+#ifdef KSZ846X_SLI
+#define CIDER_ID_8462			(0x8420)
+#define CIDER_ID_8441			(0x8400)
+#endif
+#define CIDER_ID_MASK			(~0xf)
+#define CIDER_REV_MASK			(0x7)
+#define CIDER_REV_SHIFT			(1)
+#define CIDER_REV_GET(_v)		(((_v) >> CIDER_REV_SHIFT) & \
+	CIDER_REV_MASK)
+
+#define KS_PMCTRL			0x032
+#define PMCTRL_WKEVT_MASK		(0xf << 2)
+#define PMCTRL_WKEVT_FRAME		(1 << 5)
+#define PMCTRL_WKEVT_MAGICPKT		(1 << 4)
+#define PMCTRL_WKEVT_LINK		(1 << 3)
+#define PMCTRL_WKEVT_ENERGY		(1 << 2)
+#define PMCTRL_PM_MASK			(0x3 << 0)
+#define PMCTRL_PM_NORMAL		(0x0 << 0)
+#define PMCTRL_PM_ENERGY		(0x1 << 0)
+#define PMCTRL_PM_SOFTDOWN		(0x2 << 0)
+#define PMCTRL_PM_RESERVED		(0x3 << 0)
+
+#define KS_PME				0x034
+
+#define PME_POLARITY			(1 << 4)
+#define PME_WOL_WAKEUP			(1 << 3)
+#define PME_WOL_MAGICPKT		(1 << 2)
+#define PME_WOL_LINKUP			(1 << 1)
+#define PME_WOL_ENERGY			(1 << 0)
+
+#define KS_GST				0x036
+#define GST_MASK			(0x000f)
+
+#define KS_CTPDC			0x038
+#define CTPDC_PLL_AUTO_POWERDOWN	(1 << 4)
+#define CTPDC_SWITCH_CLK_AUTO_SHUTDOWN	(1 << 3)
+#define CTPDC_CPU_CLK_AUTO_SHUTDOWN	(1 << 2)
+#define CTPDC_WAIT_PERIOD_MASK		(3)
+#define CTPDC_WAIT_5_3_S		0
+#define CTPDC_WAIT_1_6_S		1
+#define CTPDC_WAIT_1_0_mS		2
+#define CTPDC_WAIT_3_2_uS		3
+
+/* TX Frame control */
+
+#define TXFR_TXIC			(1 << 15)
+#define TXFR_TXFID_MASK			(TXFID_MASK << TXFID_SHIFT)
+
+#endif
diff -Napur linux-linux4microchip-2022.10-orig/drivers/net/ethernet/micrel/ksz8462_h.c linux-linux4microchip-2022.10/drivers/net/ethernet/micrel/ksz8462_h.c
--- linux-linux4microchip-2022.10-orig/drivers/net/ethernet/micrel/ksz8462_h.c	1969-12-31 16:00:00.000000000 -0800
+++ linux-linux4microchip-2022.10/drivers/net/ethernet/micrel/ksz8462_h.c	2023-12-06 17:06:31.195507727 -0800
@@ -0,0 +1,3960 @@
+/**
+ * Microchip KSZ8462 HLI Ethernet driver
+ *
+ * Copyright (c) 2015-2023 Microchip Technology Inc.
+ * Copyright (c) 2010-2015 Micrel, Inc.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+ */
+
+/**
+ * Supports:
+ * KSZ8462/KSZ8441 16bit HLI chip from Micrel Inc.
+ */
+
+#if 0
+#define DEBUG
+#endif
+
+#define pr_fmt(fmt) KBUILD_MODNAME ": " fmt
+
+#include <linux/module.h>
+#include <linux/kernel.h>
+#include <linux/netdevice.h>
+#include <linux/etherdevice.h>
+#include <linux/ethtool.h>
+#include <linux/cache.h>
+#include <linux/crc32.h>
+#include <linux/mii.h>
+#include <linux/platform_device.h>
+#include <linux/delay.h>
+#include <linux/slab.h>
+#include <linux/in.h>
+#include <linux/ip.h>
+#include <linux/debugfs.h>
+#include <linux/seq_file.h>
+#include <linux/eeprom_93cx6.h>
+#include <linux/if_vlan.h>
+#include <linux/net_tstamp.h>
+#include <linux/of_net.h>
+#include <linux/of_mdio.h>
+#include <linux/phy.h>
+#include <linux/phylink.h>
+#include <net/ip.h>
+#include <net/ipv6.h>
+#include <net/tcp.h>
+
+#define KSZ846X_HLI
+#include "ksz846x.h"
+#include "ksz8463.h"
+#include "ksz8462.h"
+
+#ifdef CONFIG_KSZ_PTP
+#define CONFIG_1588_PTP
+#endif
+
+#ifdef CONFIG_KSZ_MRP
+/* MRP code not implemented yet. */
+#undef CONFIG_KSZ_MRP
+#undef CONFIG_KSZ_MMRP
+#undef CONFIG_KSZ_MVRP
+#undef CONFIG_KSZ_MSRP
+#endif
+/* Can be defined if KSZ9897 driver is included also. */
+#undef CONFIG_KSZ_DLR
+#undef CONFIG_KSZ_HSR
+
+#include "ksz_common.h"
+#include "ksz_req.h"
+#include "ksz_sw.h"
+#include "ksz_spi_net.h"
+
+#ifdef DEBUG
+#define DEBUG_MSG
+#endif
+
+/*
+ * THa  2012/10/30
+ * There are 4 ways to handle not enough transmit buffer space to transmit a
+ * packet.  First is just simply return busy so the upper layer keeps calling
+ * the driver to send the packet.  This is not desirable as it wastes CPU time.
+ * The preferable way is to stop the transmit queue and restart the queue after
+ * the transmit buffer has enough space to continue transmitting.  Normally
+ * this is done using the interrupt mechanism.
+ * Using the transmit buffer space request interrupt produces the best transmit
+ * throughput.  When the transmit buffer reaches a set threshold an interrupt
+ * will be triggered to let the driver know to wake up the transmit queue.
+ * By experiment it was found using the threshold close to the maximum buffer
+ * size yields the highest transmit throughput.
+ * The transmit done interrupt can be used to simulate the transmit buffer
+ * request interrupt behavior.  When the transmit buffer is less than 1520
+ * bytes the queue is stopped and the packet is sent with interrupt on.  When
+ * the transmit done interrupt is triggered then the transmit buffer should
+ * have more than (1520 * 3) bytes available.
+ * Unfortunately the current chip has problems using the transmit interrupts.
+ * That leaves only the polling way.  A workqueue is scheduled to check the
+ * transmit buffer size is greater than a certain threshold until the condition
+ * is met.  The transmit throughput is the lowest, but the CPU time used is
+ * much less.
+ */
+
+#define TX_BUF_MIN			4
+#define TXFID_INTR_DONE			0x20
+
+#if 0
+#define USE_TX_DONE_INTR
+#define TX_DONE_MIN			(1520 + TX_BUF_MIN)
+#endif
+
+#if 1
+#define USE_TX_BUF_INTR
+#endif
+
+#if 0
+#define USE_2_BYTE_OFFSET
+#endif
+
+#define DRV_NAME			"ksz8462_hli"
+#define DRV_VERSION			"1.1.0"
+#define SW_DRV_VERSION			"1.2.3"
+#define SW_DRV_RELDATE			"Aug 24, 2021"
+
+#define MAX_RECV_FRAMES			180 /* 32 */
+#define MAX_BUF_SIZE			2048
+#define TX_BUF_SIZE			2000
+#define RX_BUF_SIZE			2000
+
+
+#define	ENUM_BUS_NONE			0
+#define	ENUM_BUS_8BIT			1
+#define	ENUM_BUS_16BIT			2
+#define	ENUM_BUS_32BIT			3
+
+#define MAX_MCAST_LST			32
+#define HW_MCAST_SIZE			4
+
+#define MIB_COUNTER_RX_LO_PRIORITY	0x00
+#define MIB_COUNTER_RX_HI_PRIORITY	0x01
+#define MIB_COUNTER_RX_UNDERSIZE	0x02
+#define MIB_COUNTER_RX_FRAGMENT		0x03
+#define MIB_COUNTER_RX_OVERSIZE		0x04
+#define MIB_COUNTER_RX_JABBER		0x05
+#define MIB_COUNTER_RX_SYMBOL_ERR	0x06
+#define MIB_COUNTER_RX_CRC_ERR		0x07
+#define MIB_COUNTER_RX_ALIGNMENT_ERR	0x08
+#define MIB_COUNTER_RX_CTRL_8808	0x09
+#define MIB_COUNTER_RX_PAUSE		0x0A
+#define MIB_COUNTER_RX_BROADCAST	0x0B
+#define MIB_COUNTER_RX_MULTICAST	0x0C
+#define MIB_COUNTER_RX_UNICAST		0x0D
+#define MIB_COUNTER_RX_OCTET_64		0x0E
+#define MIB_COUNTER_RX_OCTET_65_127	0x0F
+#define MIB_COUNTER_RX_OCTET_128_255	0x10
+#define MIB_COUNTER_RX_OCTET_256_511	0x11
+#define MIB_COUNTER_RX_OCTET_512_1023	0x12
+#define MIB_COUNTER_RX_OCTET_1024_1522	0x13
+#define MIB_COUNTER_TX_LO_PRIORITY	0x14
+#define MIB_COUNTER_TX_HI_PRIORITY	0x15
+#define MIB_COUNTER_TX_LATE_COLLISION	0x16
+#define MIB_COUNTER_TX_PAUSE		0x17
+#define MIB_COUNTER_TX_BROADCAST	0x18
+#define MIB_COUNTER_TX_MULTICAST	0x19
+#define MIB_COUNTER_TX_UNICAST		0x1A
+#define MIB_COUNTER_TX_DEFERRED		0x1B
+#define MIB_COUNTER_TX_TOTAL_COLLISION	0x1C
+#define MIB_COUNTER_TX_EXCESS_COLLISION	0x1D
+#define MIB_COUNTER_TX_SINGLE_COLLISION	0x1E
+#define MIB_COUNTER_TX_MULTI_COLLISION	0x1F
+
+#define MIB_COUNTER_RX_DROPPED_PACKET	0x20
+#define MIB_COUNTER_TX_DROPPED_PACKET	0x21
+
+/* -------------------------------------------------------------------------- */
+
+/**
+ * struct ks_rxctrl - KSZ8462SLI driver rx control
+ * @mchash: Multicast hash-table data.
+ * @rxcr1: KS_RXCR1 register setting
+ * @rxcr2: KS_RXCR2 register setting
+ *
+ * Representation of the settings needs to control the receive filtering
+ * such as the multicast hash-filter and the receive register settings. This
+ * is used to make the job of working out if the receive settings change and
+ * then issuing the new settings to the worker that will send the necessary
+ * commands.
+ */
+struct ks_rxctrl {
+	u16	mchash[4];
+	u16	rxcr1;
+	u16	rxcr2;
+};
+
+/**
+ * union ks_tx_hdr - tx header data
+ * @txb: The header as bytes
+ * @txw: The header as 16bit, little-endian words
+ *
+ * A dual representation of the tx header data to allow
+ * access to individual bytes, and to allow 16bit accesses
+ * with 16bit alignment.
+ */
+union ks_tx_hdr {
+	u8	txb[6];
+	__le16	txw[3];
+};
+
+/* Receive multiplex framer header info */
+struct type_frame_head {
+	u16	sts;         /* Frame status */
+	u16	len;         /* Byte count */
+};
+
+/* Hardware features and bug fixes. */
+
+/* Software overrides. */
+#define RX_LOOPBACK			(1 << 3)
+#define RX_ALL				(1 << 4)
+#define DATA_HEADER			(1 << 5)
+
+struct ksz_hw {
+	void __iomem *hw_addr;
+	void __iomem *hw_addr_cmd;
+	union ks_tx_hdr txh ____cacheline_aligned;
+	struct type_frame_head *frame_head_info;
+	int			frame_cnt;
+	int			frame_max;
+	int			bus_width;
+	int			RX_MAX;
+	u32			rx_cont;
+	int			rx_fragment;
+	int			rx_final;
+	int			rx_tcp;
+	int			rx_max;
+	int			rx_num;
+	unsigned long		isr_jiffies;
+	unsigned long		rx_jiffies;
+	u16			rx_cnt;
+	u16			rx_intr;
+	u16			rc_ccr;
+	u16			rc_ier;
+	u16			rc_isr;
+	u16			rc_rxcr1;
+	u16			rc_txqcr;
+	u16			rc_rxqcr;
+	u32			rc_qcr;
+
+	struct ks_rxctrl	rxctrl;
+
+	u16			sharedbus;
+	u16			cmd_reg_cache_int;
+	u16			tx_max;
+	u16			tx_len;
+	u8			fid;
+	u8			extra_byte;
+	u8			promiscuous;
+	u8			all_mcast;
+	u8			mac_addr[ETH_ALEN];
+	u8			enabled;
+	u8			mcast_lst_reserved;
+	u8			mcast_lst_size;
+	u8			mcast_lst[MAX_MCAST_LST][ETH_ALEN];
+	u16			mcast_bits[HW_MCAST_SIZE];
+
+	uint oper_state;
+#define OPER_STATE_TX_PAUSED	(1 << 0)
+#define OPER_STATE_TX_RESET	(1 << 1)
+
+	uint features;
+	uint overrides;
+};
+
+/* -------------------------------------------------------------------------- */
+
+/* For ksz_request used by PTP or MRP driver. */
+#include "ksz_req.c"
+
+/* -------------------------------------------------------------------------- */
+
+/**
+ * struct dev_priv - Network device private data structure
+ * @adapter:		Adapter device information.
+ * @port:		Port information.
+ * @monitor_time_info:	Timer to monitor ports.
+ * @stats:		Network statistics.
+ * @proc_sem:		Semaphore for proc accessing.
+ * @id:			Device ID.
+ * @mii_if:		MII interface information.
+ * @advertising:	Temporary variable to store advertised settings.
+ * @msg_enable:		The message flags controlling driver output.
+ * @media_state:	The connection status of the device.
+ * @multicast:		The all multicast state of the device.
+ * @promiscuous:	The promiscuous state of the device.
+ */
+struct dev_info {
+	struct platform_device *pdev;
+	struct net_device *dev;
+	void *parent;
+	struct napi_struct napi;
+	int use_napi;
+
+	struct ksz_hw hw;
+
+	struct ksz_sw *sw;
+	struct mutex *lock;
+
+	struct eeprom_93cx6 eeprom;
+
+	struct work_struct rxctrl_work;
+
+	struct tasklet_struct rx_tasklet;
+
+	int opened;
+
+	int wol_enable;
+	int wol_support;
+
+	struct work_struct tx_reset;
+};
+
+/**
+ * struct ks_net - KSZ8461/2 HLI driver private data
+ * @net_device	: The network device we're bound to
+ * @stats:		Network statistics.
+ * @hw_addr	: start address of data register.
+ * @hw_addr_cmd	: start address of command register.
+ * @txh		: temporaly buffer to save status/length.
+ * @lock	: Lock to ensure that the device is not accessed when busy.
+ * @pdev	: Pointer to platform device.
+ * @mii		: The MII state information for the mii calls.
+ * @frame_head_info	: frame header information for multi-pkt rx.
+ * @statelock	: Lock on this structure for tx list.
+ * @msg_enable	: The message flags controlling driver output (see ethtool).
+ * @frame_cnt	: number of frames received.
+ * @bus_width	: i/o bus width.
+ * @irq		: irq number assigned to this device.
+ * @rc_rxqcr	: Cached copy of KS_RXQCR.
+ * @rc_txcr	: Cached copy of KS_TXCR.
+ * @rc_ier	: Cached copy of KS_IER.
+ * @sharedbus	: Multipex(addr and data bus) mode indicator.
+ * @cmd_reg_cache_int	: command register cached. Used in the irq handler.
+ * @promiscuous	: promiscuous mode indicator.
+ * @all_mcast	: mutlicast indicator.
+ * @mcast_lst_size	: size of multicast list.
+ * @mcast_lst	: multicast list.
+ * @mcast_bits	: multicast enabed.
+ * @mac_addr	: MAC address assigned to this device.
+ * @fid		: frame id.
+ * @extra_byte	: number of extra byte prepended rx pkt.
+ * @enabled	: indicator this device works.
+ *
+ * The @lock ensures that the chip is protected when certain operations are
+ * in progress. When the read or write packet transfer is in progress, most
+ * of the chip registers are not accessible until the transfer is finished and
+ * the DMA has been de-asserted.
+ *
+ * The @statelock is used to protect information in the structure which may
+ * need to be accessed via several sources, such as the network driver layer
+ * or one of the work queues.
+ *
+ */
+struct ks_net {
+	struct net_device *netdev;
+	struct dev_info *hw_priv;
+	struct sw_priv *sw_priv;
+	struct ksz_port port;
+
+	int id;
+
+	/* spinlock to be interrupt safe */
+	struct mutex lock;
+	spinlock_t statelock;
+
+	struct device *dev;
+
+	struct mii_if_info mii_if;
+	u32 advertising;
+
+	u32 msg_enable;
+	int irq;
+	int media_state;
+	int multicast;
+	int promiscuous;
+	u8 phy_addr;
+	u8 state;
+	u8 multi_list_size;
+	u8 multi_list[MAX_MCAST_LST][ETH_ALEN];
+};
+
+#define dev_priv			ks_net
+
+
+#define BE3             0x8000      /* Byte Enable 3 */
+#define BE2             0x4000      /* Byte Enable 2 */
+#define BE1             0x2000      /* Byte Enable 1 */
+#define BE0             0x1000      /* Byte Enable 0 */
+
+/**
+ * register read/write calls.
+ *
+ * All these calls issue transactions to access the chip's registers. They
+ * all require that the necessary lock is held to prevent accesses when the
+ * chip is busy transfering packet data (RX/TX FIFO accesses).
+ */
+
+/**
+ * ks_rdreg8 - read 8 bit register from device
+ * @hw:		The hardware instance.
+ * @offset:	The register address.
+ *
+ * Read a 8bit register from the chip, returning the result.
+ */
+static u8 ks_rdreg8(struct ksz_hw *hw, int offset)
+{
+	u16 addr;
+	u16 data;
+	u8 shift_bit = offset & 0x03;
+	u8 shift_data = (offset & 1) << 3;
+
+	addr = (u16) offset | (u16)(BE0 << shift_bit);
+	iowrite16(addr, hw->hw_addr_cmd);
+	data = ioread16(hw->hw_addr);
+	return (u8)(data >> shift_data);
+}
+
+/**
+ * ks_rdreg16 - read 16 bit register from device
+ * @hw:		The hardware instance.
+ * @offset:	The register address.
+ *
+ * Read a 16bit register from the chip, returning the result.
+ */
+static u16 ks_rdreg16(struct ksz_hw *hw, int offset)
+{
+	u16 addr;
+
+	addr = (u16) offset | ((BE1 | BE0) << (offset & 0x02));
+	iowrite16(addr, hw->hw_addr_cmd);
+	return ioread16(hw->hw_addr);
+}
+
+/**
+ * ks_rdreg32 - read 32-bit register from device
+ * @hw:		The hardware instance.
+ * @offset:	The register address.
+ *
+ * Read a 32-bit register from the chip, returning the result.
+ */
+static u32 ks_rdreg32(struct ksz_hw *hw, int offset)
+{
+	u32 data;
+
+	data = ks_rdreg16(hw, offset + 2);
+	data <<= 16;
+	data |= ks_rdreg16(hw, offset);
+	return data;
+}
+
+/**
+ * ks_wrreg8 - write 8bit register value to chip
+ * @hw:		The hardware instance.
+ * @offset:	The register address.
+ * @value:	The value to write.
+ *
+ */
+static void ks_wrreg8(struct ksz_hw *hw, int offset, u8 value)
+{
+	u16 addr;
+	u8 shift_bit = (offset & 0x03);
+	u16 value_write = (u16)(value << ((offset & 1) << 3));
+
+	addr = (u16) offset | (BE0 << shift_bit);
+	iowrite16(addr, hw->hw_addr_cmd);
+	iowrite16(value_write, hw->hw_addr);
+}
+
+/**
+ * ks_wrreg16 - write 16bit register value to chip
+ * @hw:		The hardware instance.
+ * @offset:	The register address.
+ * @value:	The value to write.
+ *
+ */
+
+static void ks_wrreg16(struct ksz_hw *hw, int offset, u16 value)
+{
+	u16 addr;
+
+	addr = (u16) offset | ((BE1 | BE0) << (offset & 0x02));
+	iowrite16(addr, hw->hw_addr_cmd);
+	iowrite16(value, hw->hw_addr);
+}
+
+/**
+ * ks_wrreg32 - write 32-bit register value to chip
+ * @hw:		The hardware instance.
+ * @offset:	The register address.
+ * @value:	The value to write.
+ *
+ */
+static void ks_wrreg32(struct ksz_hw *hw, int offset, u32 value)
+{
+	ks_wrreg16(hw, offset, (u16) value);
+	ks_wrreg16(hw, offset + 2, value >> 16);
+}
+
+/**
+ * ks_inblk - read a block of data from QMU.  This is called after pseudo DMA
+ *	mode is enabled.
+ * @hw:		The hardware instance.
+ * @wptr:	buffer address to save data.
+ * @len:	length in byte to read.
+ *
+ */
+static inline void ks_inblk(struct ksz_hw *hw, u16 *wptr, u32 len)
+{
+	len >>= 1;
+	while (len--)
+		*wptr++ = (u16) ioread16(hw->hw_addr);
+}
+
+/**
+ * ks_outblk - write data to QMU.  This is called after pseudo DMA mode is
+ *	enabled.
+ * @hw:		The hardware instance.
+ * @wptr:	buffer address.
+ * @len:	length in byte to write.
+ *
+ */
+static inline void ks_outblk(struct ksz_hw *hw, u16 *wptr, u32 len)
+{
+	len >>= 1;
+	while (len--)
+		iowrite16(*wptr++, hw->hw_addr);
+}
+
+/**
+ * ks_save_cmd_reg - save the command register from the cache.
+ * @hw:		The hardware instance.
+ *
+ */
+static inline void ks_save_cmd_reg(struct ksz_hw *hw)
+{
+	hw->cmd_reg_cache_int = ioread16(hw->hw_addr_cmd);
+}
+
+/**
+ * ks_restore_cmd_reg - restore the command register from the cache and
+ *	write to hardware register.
+ * @hw:		The hardware instance.
+ *
+ */
+static inline void ks_restore_cmd_reg(struct ksz_hw *hw)
+{
+	iowrite16(hw->cmd_reg_cache_int, hw->hw_addr_cmd);
+}
+
+/**
+ * ks_dbg_dumpkkt - dump initial packet contents to debug
+ * @ks:		The device state.
+ * @rxpkt:	The data for the received packet.
+ *
+ * Dump the initial data from the packet to dev_dbg().
+ */
+static void ks_dbg_dumpkkt(struct ks_net *ks, u8 *rxpkt, int len, int flag)
+{
+	int j = 0, k;
+	u8 *data = rxpkt;
+
+	printk(KERN_DEBUG "%s On port\n", (flag == 1) ? "Tx" : "Rx");
+	printk(KERN_DEBUG "Pkt Len=%d\n", len);
+	do {
+		printk(KERN_DEBUG "\n %04x   ", j);
+		for (k = 0; (k < 16 && len); k++, data++, len--)
+			printk(KERN_DEBUG "%02x  ", *data);
+		j += 16;
+	} while (len > 0);
+	printk(KERN_DEBUG "\n");
+}
+
+/* -------------------------------------------------------------------------- */
+
+#define USE_SHOW_HELP
+#include "ksz_common.c"
+
+static inline int sw_is_switch(struct ksz_sw *sw)
+{
+	return sw->info != NULL;
+}
+
+/* -------------------------------------------------------------------------- */
+
+#define MIB_READ_INTERVAL		(HZ / 2)
+
+static void sw_ena_intr(struct ksz_sw *sw)
+{
+}  /* sw_ena_intr */
+
+static int sw_chk_id(struct ksz_sw *sw, u16 *id)
+{
+	struct sw_priv *sw_priv = sw->dev;
+	struct dev_info *hw_priv = sw_priv->hw_dev;
+	struct ksz_hw *hw = &hw_priv->hw;
+	int cnt;
+
+	*id = ks_rdreg16(hw, KS_CIDER);
+	if (((*id & CIDER_ID_MASK) != CIDER_ID_8462) &&
+	    ((*id & CIDER_ID_MASK) != CIDER_ID_8441))
+		return -ENODEV;
+	if ((*id & CIDER_ID_MASK) == CIDER_ID_8462)
+		cnt = 2;
+	else
+		cnt = 1;
+	return cnt;
+}
+
+static u8 sw_r8(struct ksz_sw *sw, unsigned reg)
+{
+	struct sw_priv *sw_priv = sw->dev;
+	struct dev_info *hw_priv = sw_priv->hw_dev;
+	struct ksz_hw *hw = &hw_priv->hw;
+
+	return ks_rdreg8(hw, reg);
+}
+
+static u16 sw_r16(struct ksz_sw *sw, unsigned reg)
+{
+	struct sw_priv *sw_priv = sw->dev;
+	struct dev_info *hw_priv = sw_priv->hw_dev;
+	struct ksz_hw *hw = &hw_priv->hw;
+
+	return ks_rdreg16(hw, reg);
+}
+
+static u32 sw_r32(struct ksz_sw *sw, unsigned reg)
+{
+	struct sw_priv *sw_priv = sw->dev;
+	struct dev_info *hw_priv = sw_priv->hw_dev;
+	struct ksz_hw *hw = &hw_priv->hw;
+
+	return ks_rdreg32(hw, reg);
+}
+
+static void sw_w8(struct ksz_sw *sw, unsigned reg, unsigned val)
+{
+	struct sw_priv *sw_priv = sw->dev;
+	struct dev_info *hw_priv = sw_priv->hw_dev;
+	struct ksz_hw *hw = &hw_priv->hw;
+
+	ks_wrreg8(hw, reg, val);
+}
+
+static void sw_w16(struct ksz_sw *sw, unsigned reg, unsigned val)
+{
+	struct sw_priv *sw_priv = sw->dev;
+	struct dev_info *hw_priv = sw_priv->hw_dev;
+	struct ksz_hw *hw = &hw_priv->hw;
+
+	ks_wrreg16(hw, reg, val);
+}
+
+static void sw_w32(struct ksz_sw *sw, unsigned reg, unsigned val)
+{
+	struct sw_priv *sw_priv = sw->dev;
+	struct dev_info *hw_priv = sw_priv->hw_dev;
+	struct ksz_hw *hw = &hw_priv->hw;
+
+	ks_wrreg32(hw, reg, val);
+}
+
+#ifdef CONFIG_1588_PTP
+/* These are chip dependent 1588 PTP code. */
+
+#ifdef ENABLE_IRIG
+#define IRIG_INTERVAL			10000000
+#define IRIG_GPIO			2
+#endif
+
+static int ks_start_xmit(struct sk_buff *skb, struct net_device *dev);
+#define netdev_tx			ks_start_xmit
+
+#define ADDR_SHIFT			14
+#define ADDR_8				1
+#define ADDR_16				2
+#define ADDR_32				3
+
+#define BANK_SHIFT			12
+#define BANK_SWITCH			0
+#define BANK_TOUT			1
+#define BANK_TEVT			2
+#define BANK_PTP			3
+#endif
+
+static void get_private_data_(struct device *d, struct semaphore **proc_sem,
+	struct ksz_sw **sw, struct ksz_port **port)
+{
+	struct net_device *dev = to_net_dev(d);
+	struct dev_priv *priv = netdev_priv(dev);
+	struct sw_priv *sw_priv = priv->sw_priv;
+
+	*proc_sem = &sw_priv->proc_sem;
+	*sw = &sw_priv->sw;
+	if (port)
+		*port = &priv->port;
+}
+
+#define get_private_data		get_private_data_
+
+#define USE_DIFF_PORT_PRIORITY
+
+/* Do not emulate PHY device in ksz_sw.c code. */
+#define NO_PHYDEV
+
+/* KSZ8463 and KSZ8863 use same ksz_sw.c code. */
+#undef CONFIG_HAVE_KSZ8863
+#include "ksz_sw.c"
+
+static struct ksz_sw_reg_ops sw_reg_ops = {
+	.r8			= sw_r8,
+	.r16			= sw_r16,
+	.r32			= sw_r32,
+	.w8			= sw_w8,
+	.w16			= sw_w16,
+	.w32			= sw_w32,
+
+	.r			= sw_r16,
+	.w			= sw_w16,
+
+	.get			= sw_reg_get,
+	.set			= sw_reg_set,
+};
+
+static void sw_disable(struct ksz_sw *sw)
+{
+	SW_D data;
+
+	if (sw->port_info[0].fiber) {
+		data = SW_R(sw, REG_CFG_CTRL);
+		data &= ~PORT_1_COPPER_MODE;
+		SW_W(sw, REG_CFG_CTRL, data);
+		data = SW_R(sw, REG_DSP_CTRL_6);
+		data &= ~COPPER_RECEIVE_ADJUSTMENT;
+		SW_W(sw, REG_DSP_CTRL_6, data);
+	}
+	sw_set_global_ctrl(sw);
+
+	/*
+	 * Switch actually cannot do auto-negotiation with old 10Mbit hub.
+	 */
+	port_r(sw, 0, P_FORCE_CTRL, &data);
+	if (sw->port_info[0].fiber) {
+		port_cfg_force_flow_ctrl(sw, 0, 1);
+		data &= ~PORT_AUTO_NEG_ENABLE;
+		data |= PORT_FORCE_FULL_DUPLEX;
+	} else {
+		port_cfg_force_flow_ctrl(sw, 0, 0);
+		data &= ~PORT_FORCE_FULL_DUPLEX;
+	}
+	port_w(sw, 0, P_FORCE_CTRL, data);
+
+	sw_setup_mirror(sw);
+
+	port_cfg_rx(sw, 1, 0);
+	port_cfg_tx(sw, 1, 0);
+}
+
+/* -------------------------------------------------------------------------- */
+
+/**
+ * hw_cfg_wol - configure Wake-on-LAN features
+ * @hw:		The hardware instance.
+ * @frame:	The pattern frame bit.
+ * @set:	The flag indicating whether to enable or disable.
+ *
+ * This routine is used to enable or disable certain Wake-on-LAN features.
+ */
+static void hw_cfg_wol(struct ksz_hw *hw, u16 frame, int set)
+{
+	u16 data;
+
+	data = ks_rdreg16(hw, REG_WOL_CTRL_ETH);
+	if (set)
+		data |= frame;
+	else
+		data &= ~frame;
+	ks_wrreg16(hw, REG_WOL_CTRL_ETH, data);
+}
+
+/**
+ * hw_set_wol_frame - program Wake-on-LAN pattern
+ * @hw:		The hardware instance.
+ * @i:		The frame index.
+ * @mask_size:	The size of the mask.
+ * @mask:	Mask to ignore certain bytes in the pattern.
+ * @frame_size:	The size of the frame.
+ * @pattern:	The frame data.
+ *
+ * This routine is used to program Wake-on-LAN pattern.
+ */
+static void hw_set_wol_frame(struct ksz_hw *hw, int i, uint mask_size,
+	u8 *mask, uint frame_size, u8 *pattern)
+{
+	int bits;
+	int from;
+	int len;
+	int to;
+	u32 crc;
+	u8 data[64];
+	u8 val = 0;
+
+	if (frame_size > mask_size * 8)
+		frame_size = mask_size * 8;
+	if (frame_size > 64)
+		frame_size = 64;
+
+	i *= 0x10;
+	ks_wrreg32(hw, REG_WOL_FRAME0_BYTE_MASK0 + i, 0);
+	ks_wrreg32(hw, REG_WOL_FRAME0_BYTE_MASK2 + i, 0);
+
+	bits = len = from = to = 0;
+	do {
+		if (bits) {
+			if ((val & 1))
+				data[to++] = pattern[from];
+			val >>= 1;
+			++from;
+			--bits;
+		} else {
+			val = mask[len];
+			ks_wrreg8(hw, REG_WOL_FRAME0_BYTE_MASK0 + i + len,
+				val);
+			++len;
+			if (val)
+				bits = 8;
+			else
+				from += 8;
+		}
+	} while (from < (int) frame_size);
+	if (val) {
+		bits = mask[len - 1];
+		val <<= (from % 8);
+		bits &= ~val;
+		ks_wrreg8(hw, REG_WOL_FRAME0_BYTE_MASK0 + i + len - 1, bits);
+	}
+	crc = ether_crc(to, data);
+	ks_wrreg32(hw, REG_WOL_FRAME0_CRC0 + i, crc);
+}
+
+/**
+ * hw_add_wol_arp - add ARP pattern
+ * @hw:		The hardware instance.
+ * @ip_addr:	The IPv4 address assigned to the device.
+ *
+ * This routine is used to add ARP pattern for waking up the host.
+ */
+static void hw_add_wol_arp(struct ksz_hw *hw, u8 *ip_addr)
+{
+	u8 mask[6] = { 0x3F, 0xF0, 0x3F, 0x00, 0xC0, 0x03 };
+	u8 pattern[42] = {
+		0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF,
+		0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
+		0x08, 0x06,
+		0x00, 0x01, 0x08, 0x00, 0x06, 0x04, 0x00, 0x01,
+		0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
+		0x00, 0x00, 0x00, 0x00,
+		0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
+		0x00, 0x00, 0x00, 0x00 };
+
+	memcpy(&pattern[38], ip_addr, 4);
+	hw_set_wol_frame(hw, 3, 6, mask, 42, pattern);
+}
+
+/**
+ * hw_add_wol_bcast - add broadcast pattern
+ * @hw:		The hardware instance.
+ *
+ * This routine is used to add broadcast pattern for waking up the host.
+ */
+static void hw_add_wol_bcast(struct ksz_hw *hw)
+{
+	u8 mask[] = { 0x3F };
+	u8 pattern[] = { 0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF };
+
+	hw_set_wol_frame(hw, 2, 1, mask, ETH_ALEN, pattern);
+}
+
+/**
+ * hw_add_wol_mcast - add multicast pattern
+ * @hw:		The hardware instance.
+ *
+ * This routine is used to add multicast pattern for waking up the host.
+ *
+ * It is assumed the multicast packet is the ICMPv6 neighbor solicitation used
+ * by IPv6 ping command.  Note that multicast packets are filtred through the
+ * multicast hash table, so not all multicast packets can wake up the host.
+ */
+static void hw_add_wol_mcast(struct ksz_hw *hw)
+{
+	u8 mask[] = { 0x3F };
+	u8 pattern[] = { 0x33, 0x33, 0xFF, 0x00, 0x00, 0x00 };
+
+	memcpy(&pattern[3], &hw->mac_addr[3], 3);
+	hw_set_wol_frame(hw, 1, 1, mask, 6, pattern);
+}
+
+/**
+ * hw_add_wol_ucast - add unicast pattern
+ * @hw:		The hardware instance.
+ *
+ * This routine is used to add unicast pattern to wakeup the host.
+ *
+ * It is assumed the unicast packet is directed to the device, as the hardware
+ * can only receive them in normal case.
+ */
+static void hw_add_wol_ucast(struct ksz_hw *hw)
+{
+	u8 mask[] = { 0x3F };
+
+	hw_set_wol_frame(hw, 0, 1, mask, ETH_ALEN, hw->mac_addr);
+}
+
+/**
+ * hw_enable_wol - enable Wake-on-LAN
+ * @hw:		The hardware instance.
+ * @wol_enable:	The Wake-on-LAN settings.
+ * @net_addr:	The IPv4 address assigned to the device.
+ *
+ * This routine is used to enable Wake-on-LAN depending on driver settings.
+ */
+static void hw_enable_wol(struct ksz_hw *hw, u32 wol_enable, u8 *net_addr)
+{
+	hw_cfg_wol(hw, WOL_MAGIC_ENABLE_ETH, (wol_enable & WAKE_MAGIC));
+	hw_cfg_wol(hw, WOL_FRAME0_ENABLE_ETH, (wol_enable & WAKE_UCAST));
+	hw_add_wol_ucast(hw);
+	hw_cfg_wol(hw, WOL_FRAME1_ENABLE_ETH, (wol_enable & WAKE_MCAST));
+	hw_add_wol_mcast(hw);
+	hw_cfg_wol(hw, WOL_FRAME2_ENABLE_ETH, (wol_enable & WAKE_BCAST));
+	hw_cfg_wol(hw, WOL_FRAME3_ENABLE_ETH, (wol_enable & WAKE_ARP));
+	hw_add_wol_arp(hw, net_addr);
+}
+
+/* EEPROM support */
+
+static void ks_eeprom_regread(struct eeprom_93cx6 *ee)
+{
+	struct ksz_hw *hw = ee->data;
+	unsigned val;
+
+	val = ks_rdreg16(hw, KS_EEPCR);
+
+	ee->reg_data_out = (val & EEPCR_EESB) ? 1 : 0;
+	ee->reg_data_clock = (val & EEPCR_EESCK) ? 1 : 0;
+	ee->reg_chip_select = (val & EEPCR_EECS) ? 1 : 0;
+}
+
+static void ks_eeprom_regwrite(struct eeprom_93cx6 *ee)
+{
+	struct ksz_hw *hw = ee->data;
+	unsigned val = EEPCR_EESA;	/* default - eeprom access on */
+
+	if (ee->drive_data)
+		val |= EEPCR_EEDW;
+	if (ee->reg_data_in)
+		val |= EEPCR_EEDO;
+	if (ee->reg_data_clock)
+		val |= EEPCR_EESCK;
+	if (ee->reg_chip_select)
+		val |= EEPCR_EECS;
+
+	ks_wrreg16(hw, KS_EEPCR, val);
+}
+
+/**
+ * ks_eeprom_claim - claim device EEPROM and activate the interface
+ * @ks: The network device state.
+ *
+ * Check for the presence of an EEPROM, and then activate software access
+ * to the device.
+ */
+static int ks_eeprom_claim(struct dev_info *ks)
+{
+	struct ksz_hw *hw = &ks->hw;
+
+	if (!(hw->rc_ccr & CCR_EEPROM))
+		return -ENOENT;
+
+	mutex_lock(ks->lock);
+
+	/* start with clock low, cs high */
+	ks_wrreg16(hw, KS_EEPCR, EEPCR_EESA | EEPCR_EECS);
+	return 0;
+}
+
+/**
+ * ks_eeprom_release - release the EEPROM interface
+ * @ks: The device state
+ *
+ * Release the software access to the device EEPROM
+ */
+static void ks_eeprom_release(struct dev_info *ks)
+{
+	struct ksz_hw *hw = &ks->hw;
+	unsigned val = ks_rdreg16(hw, KS_EEPCR);
+
+	ks_wrreg16(hw, KS_EEPCR, val & ~EEPCR_EESA);
+	mutex_unlock(ks->lock);
+}
+
+/* -------------------------------------------------------------------------- */
+
+#ifdef CONFIG_1588_PTP
+#ifdef CONFIG_ARCH_MICREL_PEGASUS
+extern unsigned int ksz_system_bus_clock;
+
+static u32 get_clk_cnt(void)
+{
+	return KS_R(KS8692_TIMER1_COUNTER);
+}
+#endif
+#endif
+
+/* -------------------------------------------------------------------------- */
+
+static void set_media_state(struct net_device *dev, int media_state)
+{
+	struct ks_net *priv = netdev_priv(dev);
+
+	if (media_state == priv->media_state)
+		netif_carrier_on(dev);
+	else
+		netif_carrier_off(dev);
+	netif_info(priv, link, dev, "link %s\n",
+		media_state == priv->media_state ? "on" : "off");
+}
+
+static void update_link(struct net_device *dev, struct ks_net *priv,
+	struct ksz_port *port)
+{
+	if (priv->media_state != port->linked->state) {
+		priv->media_state = port->linked->state;
+		if (netif_running(dev))
+			set_media_state(dev, media_connected);
+	}
+}
+
+static void ksz8462_link_update_work(struct work_struct *work)
+{
+	struct delayed_work *dwork = to_delayed_work(work);
+	struct ksz_port *port =
+		container_of(dwork, struct ksz_port, link_update);
+	struct net_device *dev;
+	struct ksz_sw *sw = port->sw;
+
+	dev = port->netdev;
+	if (dev)
+		update_link(dev, netdev_priv(dev), port);
+
+#ifdef CONFIG_KSZ_STP
+
+	if (sw->features & STP_SUPPORT) {
+		struct ksz_stp_info *stp = &sw->info->rstp;
+
+		stp->ops->link_change(stp, true);
+	}
+#endif
+
+#ifdef CONFIG_KSZ_HSR
+	if (sw->features & HSR_HW) {
+		struct ksz_hsr_info *hsr = &sw->info->hsr;
+
+		p = get_phy_port(sw, port->first_port);
+		if (hsr->ports[0] <= port->first_port &&
+		    port->first_port <= hsr->ports[1])
+			hsr->ops->check_announce(hsr);
+	}
+#endif
+}
+
+/* -------------------------------------------------------------------------- */
+
+static ssize_t display_info(char *buf, ssize_t len)
+{
+	len += sprintf(buf + len, "duplex:\t\t");
+	len += sprintf(buf + len, "0 for auto; ");
+	len += sprintf(buf + len,
+		"set to 1 for half-duplex; 2, full-duplex\n");
+	len += sprintf(buf + len, "speed:\t\t");
+	len += sprintf(buf + len,
+		"0 for auto; set to 10 or 100\n");
+	len += sprintf(buf + len, "force:\t\t");
+	len += sprintf(buf + len,
+		"set to 1 to force link to specific speed setting\n");
+	len += sprintf(buf + len, "flow_ctrl:\t");
+	len += sprintf(buf + len,
+		"set to 0 to disable flow control\n");
+	len += sprintf(buf + len, "mib:\t\t");
+	len += sprintf(buf + len,
+		"display the MIB table\n");
+	return len;
+}
+
+static ssize_t net_show(struct device *d, struct device_attribute *attr,
+	char *buf, unsigned long offset)
+{
+	struct net_device *dev = to_net_dev(d);
+	struct dev_priv *priv = netdev_priv(dev);
+	struct sw_priv *sw_priv = priv->sw_priv;
+	struct dev_info *hw_priv = priv->hw_priv;
+	struct ksz_hw *hw = &hw_priv->hw;
+	ssize_t len = -EINVAL;
+	int proc_num;
+
+	if (down_interruptible(&sw_priv->proc_sem))
+		return -ERESTARTSYS;
+
+	len = 0;
+	proc_num = offset / sizeof(int);
+	switch (proc_num) {
+	case PROC_SW_INFO:
+		len = display_info(buf, len);
+		break;
+	case PROC_SET_SW_FEATURES:
+		len += sprintf(buf + len, "%08x:\n", hw->features);
+		break;
+	case PROC_SET_SW_OVERRIDES:
+		len += sprintf(buf + len, "%08x:\n", hw->overrides);
+		len += sprintf(buf + len, "\t%08x = rx loopback\n",
+			RX_LOOPBACK);
+		len += sprintf(buf + len, "\t%08x = rx all\n",
+			RX_ALL);
+		len += sprintf(buf + len, "\t%08x = data header only\n",
+			DATA_HEADER);
+		break;
+	}
+	up(&sw_priv->proc_sem);
+	return len;
+}
+
+static ssize_t net_store(struct device *d, struct device_attribute *attr,
+	const char *buf, size_t count, unsigned long offset)
+{
+	struct net_device *dev = to_net_dev(d);
+	struct dev_priv *priv = netdev_priv(dev);
+	struct sw_priv *sw_priv = priv->sw_priv;
+	struct dev_info *hw_priv = priv->hw_priv;
+	struct ksz_hw *hw = &hw_priv->hw;
+	ssize_t ret = -EINVAL;
+	int changes;
+	int num;
+	int proc_num;
+
+	if ('0' == buf[0] && 'x' == buf[1])
+		sscanf(&buf[2], "%x", (unsigned int *) &num);
+	else if ('0' == buf[0] && 'b' == buf[1]) {
+		int i = 2;
+
+		num = 0;
+		while (buf[i]) {
+			num <<= 1;
+			num |= buf[i] - '0';
+			i++;
+		}
+	} else if ('0' == buf[0] && 'd' == buf[1])
+		sscanf(&buf[2], "%u", &num);
+	else
+		sscanf(buf, "%d", &num);
+	if (down_interruptible(&sw_priv->proc_sem))
+		return -ERESTARTSYS;
+
+	proc_num = offset / sizeof(int);
+	ret = count;
+	mutex_lock(hw_priv->lock);
+	switch (proc_num) {
+	case PROC_SET_SW_FEATURES:
+		if ('0' != buf[0] || 'x' != buf[1])
+			sscanf(buf, "%x", &num);
+		changes = hw->features ^ num;
+		hw->features = num;
+		break;
+	case PROC_SET_SW_OVERRIDES:
+		if ('0' != buf[0] || 'x' != buf[1])
+			sscanf(buf, "%x", &num);
+		changes = hw->overrides ^ num;
+		if ((changes & (RX_LOOPBACK |
+				0)) &&
+				hw_priv->opened) {
+			printk(KERN_INFO "Stop device first\n");
+			break;
+		}
+		hw->overrides = num;
+		break;
+	}
+	mutex_unlock(hw_priv->lock);
+	up(&sw_priv->proc_sem);
+	return ret;
+}
+
+/* generate a read-only attribute */
+#define NET_RD_ENTRY(name)						\
+static ssize_t show_##name(struct device *d,				\
+	struct device_attribute *attr, char *buf)			\
+{									\
+	return net_show(d, attr, buf,					\
+		offsetof(struct lan_attributes, name));			\
+}									\
+static DEVICE_ATTR(name, S_IRUGO, show_##name, NULL)
+
+/* generate a write-able attribute */
+#define NET_WR_ENTRY(name)						\
+static ssize_t show_##name(struct device *d,				\
+	struct device_attribute *attr, char *buf)			\
+{									\
+	return net_show(d, attr, buf,					\
+		offsetof(struct lan_attributes, name));			\
+}									\
+static ssize_t store_##name(struct device *d,				\
+	struct device_attribute *attr, const char *buf, size_t count)	\
+{									\
+	return net_store(d, attr, buf, count,				\
+		offsetof(struct lan_attributes, name));			\
+}									\
+static DEVICE_ATTR(name, S_IRUGO | S_IWUSR, show_##name, store_##name)
+
+NET_RD_ENTRY(info);
+NET_WR_ENTRY(features);
+NET_WR_ENTRY(overrides);
+
+static struct attribute *wan_attrs[] = {
+	&lan_attr_duplex.attr,
+	&lan_attr_speed.attr,
+	&lan_attr_force.attr,
+	&lan_attr_flow_ctrl.attr,
+	&lan_attr_mib.attr,
+	&dev_attr_info.attr,
+	&dev_attr_features.attr,
+	&dev_attr_overrides.attr,
+	NULL
+};
+
+static struct attribute_group all_group = {
+	.name  = "general",
+	.attrs  = wan_attrs,
+};
+
+static void exit_sysfs(struct net_device *dev)
+{
+	sysfs_remove_group(&dev->dev.kobj, &all_group);
+}
+
+static int init_sysfs(struct net_device *dev)
+{
+	int err;
+
+	err = sysfs_create_group(&dev->dev.kobj, &all_group);
+	return err;
+}
+
+/* MII interface controls */
+
+/**
+ * ks_phy_reg - convert MII register into a KSZ846x register
+ * @reg: MII register number.
+ *
+ * Return the KSZ846x register number for the corresponding MII PHY register
+ * if possible. Return zero if the MII register has no direct mapping to the
+ * KSZ846x register set.
+ */
+static int ks_phy_reg(int reg)
+{
+	switch (reg) {
+	case MII_BMCR:
+		return PHY_REG_CTRL;
+	case MII_BMSR:
+		return PHY_REG_STATUS;
+	case MII_PHYSID1:
+		return PHY_REG_ID_2;
+	case MII_PHYSID2:
+		return PHY_REG_ID_1;
+	case MII_ADVERTISE:
+		return PHY_REG_AUTO_NEGOTIATION;
+	case MII_LPA:
+		return PHY_REG_REMOTE_CAPABILITY;
+	}
+
+	return 0x0;
+}
+
+/**
+ * ks_phy_read - MII interface PHY register read.
+ * @netdev: The network device the PHY is on.
+ * @phy_addr: Address of PHY (ignored as we only have one)
+ * @reg: The register to read.
+ *
+ * This call reads data from the PHY register specified in @reg. Since the
+ * device does not support all the MII registers, the non-existant values
+ * are always returned as zero.
+ *
+ * We return zero for unsupported registers as the MII code does not check
+ * the value returned for any error status, and simply returns it to the
+ * caller. The mii-tool that the driver was tested with takes any -ve error
+ * as real PHY capabilities, thus displaying incorrect data to the user.
+ */
+static int ks_phy_read(struct net_device *netdev, int phy_addr, int reg)
+{
+	struct ks_net *priv = netdev_priv(netdev);
+	struct dev_info *hw_priv = priv->hw_priv;
+	struct ksz_hw *ks = &hw_priv->hw;
+	struct ksz_port *port = &priv->port;
+	int ksreg;
+	int result;
+
+	ksreg = ks_phy_reg(reg);
+	if (!ksreg)
+		return 0x0;	/* no error return allowed, so use zero */
+
+	ksreg += port->linked->port_id * 0xC;
+	mutex_lock(hw_priv->lock);
+	result = ks_rdreg16(ks, ksreg);
+	mutex_unlock(hw_priv->lock);
+
+	return result;
+}
+
+static void ks_phy_write(struct net_device *netdev, int phy, int reg,
+	int value)
+{
+	struct ks_net *priv = netdev_priv(netdev);
+	struct dev_info *hw_priv = priv->hw_priv;
+	struct ksz_hw *ks = &hw_priv->hw;
+	struct ksz_port *port = &priv->port;
+	int ksreg;
+
+	ksreg = ks_phy_reg(reg);
+	if (ksreg) {
+		int i;
+		int first;
+		int last;
+		int reg;
+
+		if (0 == phy) {
+			first = port->first_port;
+			last = port->port_cnt;
+		} else {
+			first = phy - 1;
+			last = phy;
+		}
+		mutex_lock(hw_priv->lock);
+		for (i = first; i < last; i++) {
+			reg = ksreg + i * 0xC;
+			ks_wrreg16(ks, reg, value);
+		}
+		mutex_unlock(hw_priv->lock);
+	}
+}
+
+/**
+ * ks_read_config - read chip configuration of bus width.
+ * @ks: The chip information
+ *
+ */
+static void ks_read_config(struct ksz_hw *ks)
+{
+	u16 reg_data = 0;
+
+	/* Regardless of bus width, 8 bit read should always work.*/
+	reg_data = ks_rdreg8(ks, KS_CCR) & 0x00FF;
+	reg_data |= ks_rdreg8(ks, KS_CCR + 1) << 8;
+
+	/* addr/data bus are multiplexed */
+	ks->sharedbus = (reg_data & CCR_SHARED) == CCR_SHARED;
+
+	/*
+	 * There are garbage data when reading data from QMU,
+	 * depending on bus-width.
+	 */
+	if (reg_data & CCR_8BIT) {
+		ks->bus_width = ENUM_BUS_8BIT;
+		ks->extra_byte = 1;
+	} else if (reg_data & CCR_16BIT) {
+		ks->bus_width = ENUM_BUS_16BIT;
+		ks->extra_byte = 2;
+	} else {
+		ks->bus_width = ENUM_BUS_32BIT;
+		ks->extra_byte = 4;
+	}
+}
+
+/**
+ * ks_soft_reset - issue one of the soft reset to the device
+ * @hw:		The hardware instance.
+ * @op:		The bit(s) to set in the GRR.
+ *
+ * Issue the relevant soft-reset command to the device's GRR register
+ * specified by @op.
+ *
+ * Note, the delays are in there as a caution to ensure that the reset
+ * has time to take effect and then complete. Since the datasheet does
+ * not currently specify the exact sequence, we have chosen something
+ * that seems to work with our device.
+ */
+static void ks_soft_reset(struct ksz_hw *hw, unsigned op)
+{
+	ks_wrreg16(hw, KS_GRR, op);
+	udelay(10);	/* wait a short time to effect reset */
+	ks_wrreg16(hw, KS_GRR, 0);
+	udelay(1);	/* wait for condition to clear */
+}
+
+/**
+ * ks_set_powermode - set power mode of the device
+ * @hw:		The hardware instance.
+ * @pwrmode:	The power mode value to write to KS_PMCTRL.
+ *
+ * Change the power mode of the chip.
+ */
+static void ks_set_powermode(struct ksz_hw *hw, unsigned pwrmode)
+{
+	unsigned pmecr;
+
+	pmecr = ks_rdreg16(hw, KS_PMCTRL);
+	pmecr &= ~PMCTRL_PM_MASK;
+	pmecr |= pwrmode;
+	ks_wrreg16(hw, KS_PMCTRL, pmecr);
+}
+
+static inline void ks_disable_int(struct ksz_hw *hw)
+{
+	ks_wrreg16(hw, KS_IER, 0x0000);
+}  /* ks_disable_int */
+
+static inline void ks_enable_int(struct ksz_hw *hw)
+{
+	ks_wrreg16(hw, KS_IER, hw->rc_ier & ~(IRQ_TS | IRQ_TRIG));
+}  /* ks_enable_int */
+
+#define TX_FIFO_TIMEOUT			8
+#define TX_FIFO_TRIES			2
+
+/**
+ * ks_tx_fifo_space - return the available hardware buffer size.
+ * @hw:		The hardware instance.
+ *
+ */
+static u16 ks_tx_fifo_space(struct ksz_hw *hw)
+{
+	u16 data;
+	int timeout = TX_FIFO_TIMEOUT;
+
+	do {
+		data = ks_rdreg16(hw, KS_TXMIR);
+	} while (!data && --timeout);
+	return data & 0x1fff;
+}
+
+/**
+ * ks_start_rx - ready to serve pkts
+ * @ks		: The chip information
+ *
+ */
+static void ks_start_rx(struct ksz_hw *hw)
+{
+	/* Enable QMU Receive (RXCR1). */
+	hw->rc_rxcr1 |= RXCR1_RXE;
+	ks_wrreg16(hw, KS_RXCR1, hw->rc_rxcr1);
+}  /* ks_start_rx */
+
+/**
+ * ks_stop_rx - stop to serve pkts
+ * @ks		: The chip information
+ *
+ */
+static void ks_stop_rx(struct ksz_hw *hw)
+{
+	/* Disable QMU Receive (RXCR1). */
+	hw->rc_rxcr1 &= ~RXCR1_RXE;
+	ks_wrreg16(hw, KS_RXCR1, hw->rc_rxcr1);
+	udelay(1);
+}  /* ks_stop_rx */
+
+static void ks_read_mac(struct ksz_hw *hw, u8 *data)
+{
+	u16 w;
+	u16 *pw = (u16 *) data;
+
+	w = ks_rdreg16(hw, KS_MARH);
+	*pw++ = htons(w);
+
+	w = ks_rdreg16(hw, KS_MARM);
+	*pw++ = htons(w);
+
+	w = ks_rdreg16(hw, KS_MARL);
+	*pw++ = htons(w);
+}
+
+/**
+ * ks_set_mac - write mac address to device registers
+ * @hw:		The hardware instance.
+ * @data:	The MAC address.
+ *
+ * Update the KSZ8461/2 MAC address registers.
+ */
+static void ks_set_mac(struct ksz_hw *hw, u8 *data)
+{
+	u16 *pw = (u16 *) data;
+	u16 u, w;
+
+	u = *pw++;
+	w = (u16)(((u & 0xFF) << 8) | ((u >> 8) & 0xFF));
+	ks_wrreg16(hw, KS_MARH, w);
+
+	u = *pw++;
+	w = (u16)(((u & 0xFF) << 8) | ((u >> 8) & 0xFF));
+	ks_wrreg16(hw, KS_MARM, w);
+
+	u = *pw;
+	w = (u16)(((u & 0xFF) << 8) | ((u >> 8) & 0xFF));
+	ks_wrreg16(hw, KS_MARL, w);
+
+	memcpy(hw->mac_addr, data, ETH_ALEN);
+}
+
+static void ks_enable_qmu(struct ksz_hw *hw)
+{
+	u16 w;
+
+	/* Enable QMU Transmit (TXCR). */
+	w = ks_rdreg16(hw, KS_TXCR);
+	ks_wrreg16(hw, KS_TXCR, w | TXCR_TXE);
+
+	/* Enable QMU Receive (RXCR1). */
+	hw->rc_rxcr1 = ks_rdreg16(hw, KS_RXCR1);
+	ks_start_rx(hw);
+
+	hw->enabled = true;
+}  /* ks_enable_qmu */
+
+static void ks_disable_qmu(struct ksz_hw *hw)
+{
+	u16 w;
+
+	/* Disable QMU Transmit (TXCR). */
+	w = ks_rdreg16(hw, KS_TXCR);
+	w &= ~TXCR_TXE;
+	ks_wrreg16(hw, KS_TXCR, w);
+
+	/* Disable QMU Receive (RXCR1). */
+	hw->rc_rxcr1 = ks_rdreg16(hw, KS_RXCR1);
+	ks_stop_rx(hw);
+
+	hw->enabled = false;
+}  /* ks_disable_qmu */
+
+/**
+ * ks_read_qmu - read 1 pkt data from the QMU.
+ * @hw:		The hardware instance.
+ * @buf:	Buffer address to save 1 pkt.
+ * @len:	Pkt length.
+ *
+ * Here is the sequence to read 1 pkt:
+ *	1. set pseudo DMA mode
+ *	2. read prepend data
+ *	3. read pkt data
+ *	4. reset pseudo DMA Mode
+ */
+static inline void ks_read_qmu(struct ksz_hw *hw, void *buf, u32 len)
+{
+	u32 r = hw->extra_byte & 0x1;
+	u32 w = hw->extra_byte - r;
+
+	/* 1. set pseudo DMA mode */
+	ks_wrreg16(hw, KS_RXFDPR, RXFDPR_RXFPAI);
+	ks_wrreg8(hw, KS_RXQCR, (hw->rc_rxqcr | RXQCR_SDA) & 0xff);
+
+	/* 2. read prepend data */
+	/**
+	 * read 4 + extra bytes and discard them.
+	 * extra bytes for dummy, 2 for status, 2 for len
+	 */
+
+	/* use likely(r) for 8 bit access for performance */
+	if (unlikely(r))
+		ioread8(hw->hw_addr);
+	ks_inblk(hw, buf, w + 2 + 2);
+
+	/* 3. read pkt data */
+	ks_inblk(hw, buf, ALIGN(len, 4));
+
+	/* 4. reset pseudo DMA Mode */
+	ks_wrreg8(hw, KS_RXQCR, hw->rc_rxqcr);
+}
+
+/**
+ * ks_write_qmu - write 1 pkt data to the QMU.
+ * @hw:		The hardware instance.
+ * @pdata:	Buffer address to save 1 pkt.
+ * @len:	Pkt length in byte.
+ *
+ * Here is the sequence to write 1 pkt:
+ *	1. set pseudo DMA mode
+ *	2. write status/length
+ *	3. write pkt data
+ *	4. reset pseudo DMA Mode
+ *	5. enqueue pkt
+ *	6. wait until pkt is out
+ */
+static void ks_write_qmu(struct ksz_hw *hw, void *pdata, u16 len)
+{
+	u8 fid = ++hw->fid;
+
+	hw->fid &= (TXFID_INTR_DONE - 1);
+
+	/* start header at txb[0] to align txw entries */
+	hw->txh.txw[0] = 0;
+	hw->txh.txw[0] = fid;
+#ifdef USE_TX_DONE_INTR
+	if (hw->tx_len < TX_DONE_MIN) {
+		hw->txh.txw[0] |= TXFR_TXIC;
+		hw->txh.txw[0] |= TXFID_INTR_DONE;
+	}
+#endif
+	hw->txh.txw[1] = cpu_to_le16(len);
+
+	/* 1. set pseudo-DMA mode */
+	ks_wrreg8(hw, KS_RXQCR, (hw->rc_rxqcr | RXQCR_SDA) & 0xff);
+	/* 2. write status/lenth info */
+	ks_outblk(hw, hw->txh.txw, 4);
+	/* 3. write pkt data */
+	ks_outblk(hw, pdata, ALIGN(len, 4));
+	/* 4. reset pseudo-DMA mode */
+	ks_wrreg8(hw, KS_RXQCR, hw->rc_rxqcr);
+	/* 5. Enqueue Tx(move the pkt from TX buffer into TXQ) */
+	ks_wrreg16(hw, KS_TXQCR, TXQCR_METFE);
+	/* 6. wait until TXQCR_METFE is auto-cleared */
+	while (ks_rdreg16(hw, KS_TXQCR) & TXQCR_METFE)
+		;
+}
+
+static int priv_promiscuous(void *ptr)
+{
+	struct ks_net *priv = ptr;
+
+	return priv->promiscuous;
+}  /* priv_promiscuous */
+
+static int priv_match_multi(void *ptr, u8 *data)
+{
+	int i;
+	struct ks_net *priv = ptr;
+	int drop = false;
+
+	if (priv->multi_list_size)
+		drop = true;
+	for (i = 0; i < priv->multi_list_size; i++)
+		if (!memcmp(data, priv->multi_list[i], ETH_ALEN)) {
+			drop = false;
+			break;
+		}
+	return drop;
+}  /* priv_match_multi */
+
+static u16 chk_rcv(struct ksz_hw *hw)
+{
+	u16 status;
+
+	status = ks_rdreg16(hw, KS_ISR);
+	status &= IRQ_RXI;
+	if (status)
+		ks_wrreg16(hw, KS_ISR, IRQ_RXI);
+	return status;
+}
+
+static int chk_tcp(u8 *data)
+{
+	struct ethhdr *eth = (struct ethhdr *) data;
+	struct vlan_ethhdr *vlan = (struct vlan_ethhdr *) data;
+	struct iphdr *iph = NULL;
+	struct ipv6hdr *ip6h = NULL;
+	int ipv6;
+	int tcp = 0;
+
+	if (eth->h_proto == htons(ETH_P_8021Q)) {
+		if (vlan->h_vlan_encapsulated_proto == htons(ETH_P_8021Q)) {
+			unsigned char *ptr = (unsigned char *) vlan;
+
+			ptr += VLAN_HLEN;
+			vlan = (struct vlan_ethhdr *) ptr;
+		}
+		ipv6 = vlan->h_vlan_encapsulated_proto == htons(ETH_P_IPV6);
+		if (vlan->h_vlan_encapsulated_proto != htons(ETH_P_IP) &&
+				!ipv6)
+			return 0;
+		ip6h = (struct ipv6hdr *)(vlan + 1);
+		iph = (struct iphdr *)(vlan + 1);
+	} else {
+		ipv6 = eth->h_proto == htons(ETH_P_IPV6);
+		if (eth->h_proto != htons(ETH_P_IP) && !ipv6)
+			return 0;
+		ip6h = (struct ipv6hdr *)(eth + 1);
+		iph = (struct iphdr *)(eth + 1);
+	}
+	if (ipv6) {
+		if (IPPROTO_TCP == ip6h->nexthdr)
+			tcp = 1;
+		else if (IPPROTO_UDP == ip6h->nexthdr)
+			tcp = 2;
+		else if (IPPROTO_UDP == ip6h->nexthdr)
+			tcp = 4;
+		if (ip6h->nexthdr == NEXTHDR_FRAGMENT)
+			tcp |= 0x8000;
+	} else {
+		int offset = ntohs(iph->frag_off);
+
+		if (IPPROTO_TCP == iph->protocol)
+			tcp = 1;
+		else if (IPPROTO_UDP == iph->protocol)
+			tcp = 2;
+		else if (IPPROTO_ICMP == iph->protocol)
+			tcp = 4;
+
+		/* Not the first IP fragment. */
+		if ((offset & IP_OFFSET))
+			tcp |= 0x8000;
+	}
+	return tcp;
+}
+
+static int det_rcv_cnt(struct ksz_hw *hw, u8 *data, int len)
+{
+	int tcp = hw->rx_tcp;
+
+	/* Using receive interrupt. */
+	if (MAX_RECV_FRAMES == hw->rx_max) {
+		if (hw->isr_jiffies != hw->rx_jiffies) {
+			hw->rx_jiffies = hw->isr_jiffies;
+			hw->rx_cnt = 0;
+			hw->rx_cont = 0;
+			hw->rx_fragment = 0;
+			hw->rx_final = 0;
+		} else
+			++hw->rx_cnt;
+		if (hw->rx_cnt >= 8) {
+
+			/* Try to parse the packet to determine type. */
+			if (tcp != 1 || ((hw->rx_cnt & 7) == 0)) {
+				tcp = chk_tcp(data);
+				hw->rx_tcp = tcp;
+				if (2 == tcp) {
+
+					/* Try to get number of fragments. */
+					if (hw->rx_final < hw->rx_fragment)
+						hw->rx_final = hw->rx_fragment;
+					hw->rx_fragment = 0;
+				} else if (tcp & 0x8000)
+					++hw->rx_fragment;
+			}
+			if (1 == tcp)
+				return tcp;
+
+			hw->rx_cont += len;
+
+			/* A condition of continual packet read is reached. */
+			if (hw->rx_cont >= 1514 * 10) {
+
+				/* Limit the read to this number. */
+				hw->rx_max = hw->RX_MAX;
+
+				if (2 != (tcp & 0xff))
+					return tcp;
+
+				/* Get the final fragment count. */
+				if (hw->rx_final < hw->rx_fragment)
+					hw->rx_final = hw->rx_fragment;
+
+				/* Further reduce it if packet is short. */
+				if (hw->rx_final < 3 && hw->rx_max)
+					hw->rx_max--;
+			}
+		}
+	}
+	return tcp;
+}
+
+static int rx_proc(struct dev_info *hw_priv, struct sk_buff *skb,
+	u16 sts, u32 len)
+{
+	struct net_device *dev = hw_priv->dev;
+	struct ks_net *priv;
+	struct ksz_hw *hw = &hw_priv->hw;
+	struct ksz_sw *sw = hw_priv->sw;
+	unsigned long flags;
+	int forward = 0;
+	int tcp = hw->rx_tcp;
+	int rx_port = 0;
+	int tag = 0;
+	void *ptr = NULL;
+#ifdef CONFIG_1588_PTP
+	struct ptp_info *ptp = &sw->ptp_hw;
+	int ptp_tag = 0;
+#endif
+
+	priv = netdev_priv(dev);
+	spin_lock_irqsave(&priv->statelock, flags);
+#ifdef USE_2_BYTE_OFFSET
+	ks_read_qmu(hw, skb->data, len);
+	len -= 2;
+	skb_reserve(skb, 2);
+#else
+	skb_reserve(skb, 2);
+
+	/* read data block including CRC 4 bytes */
+	ks_read_qmu(hw, skb->data, len);
+#endif
+	spin_unlock_irqrestore(&priv->statelock, flags);
+	len -= 4;
+
+	/* vlan_get_tag requires network device in socket buffer. */
+	skb->dev = dev;
+	if (sw_is_switch(sw)) {
+		dev = sw->net_ops->rx_dev(sw, skb->data, &len, &tag, &rx_port);
+		if (!dev) {
+			dev_kfree_skb_irq(skb);
+			return -ENODEV;
+		}
+	}
+	priv = netdev_priv(dev);
+
+	if (netif_msg_pktdata(priv)) {
+		pr_info("rx: stat=0x%04x, len=0x%04x\n",
+			sts, len);
+		ks_dbg_dumpkkt(priv, skb->data, len, 0);
+	}
+
+	skb_put(skb, len);
+	if (sw_is_switch(sw) && !sw->net_ops->match_pkt(sw, &dev,
+			(void **) &priv, priv_promiscuous, priv_match_multi,
+			skb, hw->promiscuous)) {
+		dev_kfree_skb_irq(skb);
+		return 0;
+	}
+	if (sw_is_switch(sw)) {
+
+		/* Internal packets handled by the switch. */
+		if (!sw->net_ops->drv_rx(sw, skb, rx_port))
+			return 0;
+		forward = sw->info->forward;
+	}
+
+#ifdef CONFIG_1588_PTP
+	ptr = ptp;
+	if ((sw->features & PTP_HW)) {
+		if (ptp->ops->drop_pkt(ptp, skb, sw->vlan_id, &tag, &ptp_tag,
+				       &forward)) {
+			dev_kfree_skb_irq(skb);
+			return 0;
+		}
+	}
+#endif
+	if (sw_is_switch(sw))
+		dev = sw->net_ops->parent_rx(sw, dev, &forward);
+
+	/* Update receive statistics. */
+	priv = netdev_priv(dev);
+	dev->stats.rx_packets++;
+	dev->stats.rx_bytes += len;
+
+	if (sw_is_switch(sw))
+		sw->net_ops->port_vlan_rx(skb, forward, tag);
+	if (!hw_priv->use_napi)
+		tcp = det_rcv_cnt(hw, skb->data, len);
+	skb->protocol = eth_type_trans(skb, dev);
+
+	if (skb) {
+		if (hw_priv->use_napi)
+			netif_receive_skb(skb);
+		else
+			netif_rx(skb);
+	}
+
+	return tcp;
+}
+
+/**
+ * ks_rcv - read multiple pkts data from the QMU.
+ * @hw:		The hardware instance.
+ * @dev:	The network device being opened.
+ *
+ * Read all of header information before reading pkt content.
+ * It is not allowed only port of pkts in QMU after issuing
+ * interrupt ack.
+ */
+static int ks_rcv(struct dev_info *hw_priv)
+{
+	int i;
+	struct ksz_hw *hw = &hw_priv->hw;
+	struct type_frame_head *frame_hdr = hw->frame_head_info;
+	struct sk_buff *skb;
+	int num = 0;
+	int cnt = 0;
+	int tcp = hw->rx_tcp;
+	int rx_max = hw->rx_max;
+
+	/* Indicate this is not a continual read processing if empty. */
+	int cont = hw->frame_cnt;
+
+ks_rcv_begin:
+
+	/* A new read procedure is started. */
+	if (!hw->frame_cnt) {
+		hw->frame_cnt = RXFC_GET(ks_rdreg16(hw, KS_RXFC));
+
+		/* Remember the number of frames available. */
+		hw->frame_max = hw->frame_cnt;
+
+		/* read all header information */
+		for (i = 0; i < hw->frame_cnt; i++) {
+			/* Checking Received packet status */
+			frame_hdr->sts = ks_rdreg16(hw, KS_RXFHSR);
+			/* Get packet len from hardware */
+			frame_hdr->len = ks_rdreg16(hw, KS_RXFHBCR);
+			frame_hdr++;
+		}
+	}
+
+	frame_hdr = hw->frame_head_info;
+	frame_hdr = &hw->frame_head_info[hw->frame_max - hw->frame_cnt];
+	while (hw->frame_cnt) {
+		skb = dev_alloc_skb(frame_hdr->len + 16);
+		if (likely(skb && !(frame_hdr->sts & RXFSHR_ERR) &&
+				(frame_hdr->len < RX_BUF_SIZE) &&
+				frame_hdr->len)) {
+			tcp = rx_proc(hw_priv, skb, frame_hdr->sts,
+				frame_hdr->len);
+			if (tcp != 1)
+				++cnt;
+		} else {
+			pr_err("%s: receive error frame or no skb! "
+				"rx: stat=0x%04x, len=0x%04x, skb=%p\n",
+				__func__, frame_hdr->sts, frame_hdr->len, skb);
+
+			/* drop this error frame */
+			ks_wrreg16(hw, KS_RXQCR, (hw->rc_rxqcr | RXQCR_RRXEF));
+
+			if (skb)
+				dev_kfree_skb_irq(skb);
+		}
+		num++;
+		frame_hdr++;
+		hw->frame_cnt--;
+
+		/* Enough packets are processed. */
+		if (cnt >= rx_max) {
+			hw->rx_intr = IRQ_RXI;
+			cont = 0;
+			break;
+		}
+	}
+
+	/* Try to see if there are new received packets. */
+	if (cont && !hw->frame_cnt) {
+		if (chk_rcv(hw)) {
+			frame_hdr = hw->frame_head_info;
+			goto ks_rcv_begin;
+		}
+	}
+	if (!hw->frame_cnt)
+		hw->rx_intr = 0;
+	hw->rx_num = num;
+	return num;
+}
+
+static void rx_proc_task(unsigned long data)
+{
+	struct dev_info *hw_priv = (struct dev_info *) data;
+	struct ksz_hw *hw = &hw_priv->hw;
+	struct net_device *dev = hw_priv->dev;
+	u16 status = hw->rx_intr;
+
+	if (!status)
+		status = chk_rcv(hw);
+	if (!status || unlikely(!ks_rcv(hw_priv))) {
+		struct ks_net *priv = netdev_priv(dev);
+
+		if (hw_priv->use_napi)
+			napi_complete(&hw_priv->napi);
+		spin_lock_irq(&priv->statelock);
+		hw->rx_max = MAX_RECV_FRAMES;
+		hw->rx_tcp = 0;
+		hw->rc_ier |= IRQ_RXI;
+		hw->rc_isr |= IRQ_RXI;
+		ks_enable_int(hw);
+		spin_unlock_irq(&priv->statelock);
+	} else {
+		if (!hw_priv->use_napi)
+			tasklet_schedule(&hw_priv->rx_tasklet);
+	}
+}  /* rx_proc_task */
+
+static int rx_napi_proc(struct napi_struct *napi, int budget)
+{
+	struct dev_info *ks = container_of(napi, struct dev_info, napi);
+	struct ksz_hw *hw = &ks->hw;
+
+	hw->rx_max = budget;
+	hw->rx_num = 0;
+	rx_proc_task((unsigned long) ks);
+	return hw->rx_num;
+}  /* rx_napi_poll */
+
+static void tx_done(struct dev_info *hw_priv)
+{
+	struct ksz_sw *sw = hw_priv->sw;
+
+	hw_priv->hw.oper_state &= ~OPER_STATE_TX_PAUSED;
+	if (sw_is_switch(sw))
+		sw->net_ops->netdev_wake_queue(sw, hw_priv->dev);
+	else
+		netif_wake_queue(hw_priv->dev);
+}  /* tx_done */
+
+/**
+ * ks_irq - device interrupt handler
+ * @irq: Interrupt number passed from the IRQ hnalder.
+ * @pw: The private word passed to register_irq(), our struct ks_net.
+ *
+ * This is the handler invoked to find out what happened
+ *
+ * Read the interrupt status, work out what needs to be done and then clear
+ * any of the interrupts that are not needed.
+ */
+static irqreturn_t ks_irq(int irq, void *pw)
+{
+	struct dev_info *ks = pw;
+	struct ksz_hw *hw = &ks->hw;
+	u16 status;
+
+	/* this should be the first in IRQ handler */
+	ks_save_cmd_reg(hw);
+	hw->isr_jiffies = jiffies;
+
+	status = ks_rdreg16(hw, KS_ISR);
+	status &= hw->rc_isr;
+	ks_wrreg16(hw, KS_ISR, status);
+	status &= hw->rc_ier;
+	if (unlikely(!status)) {
+		ks_restore_cmd_reg(hw);
+		return IRQ_NONE;
+	}
+
+#ifdef CONFIG_1588_PTP
+	if (likely(status & (IRQ_TS | IRQ_TRIG))) {
+		struct ksz_sw *sw = ks->sw;
+		struct ptp_info *ptp = &sw->ptp_hw;
+
+		ptp->ops->proc_intr(ptp);
+	}
+#endif
+
+	if (likely(status & IRQ_RXI)) {
+		if (ks->use_napi) {
+			if (likely(napi_schedule_prep(&ks->napi))) {
+				hw->rc_ier &= ~IRQ_RXI;
+				hw->rc_isr &= ~IRQ_RXI;
+				ks_enable_int(hw);
+				hw->rx_intr = status;
+				__napi_schedule(&ks->napi);
+			}
+		} else {
+
+			/* Process received packets using receive interrupt. */
+			if (MAX_RECV_FRAMES == hw->rx_max)
+				ks_rcv(ks);
+			else {
+				hw->rc_ier &= ~IRQ_RXI;
+				hw->rc_isr &= ~IRQ_RXI;
+				ks_enable_int(hw);
+				hw->rx_intr = status;
+				hw->rx_cont = 0;
+				hw->rx_tcp = 0;
+				rx_proc_task((unsigned long) ks);
+			}
+		}
+	}
+
+#if defined(USE_TX_DONE_INTR)
+	if (likely(status & IRQ_TXI)) {
+		u16 fifo = ks_tx_fifo_space(hw);
+
+		if (fifo >= 1520 * 3) {
+			hw->tx_max = fifo;
+			tx_done(ks);
+		}
+	}
+#endif
+
+#if defined(USE_TX_BUF_INTR)
+	if (likely(status & IRQ_TXSAI)) {
+		u16 data = ks_rdreg16(hw, KS_RXQCR);
+
+		if ((data & ~RXQCR_STATUS) != hw->rc_rxqcr)
+			ks_wrreg16(hw, KS_RXQCR, hw->rc_rxqcr);
+		hw->rc_txqcr &= ~TXQCR_TXQMAM;
+		hw->rc_qcr = (hw->rc_rxqcr << 16) | hw->rc_txqcr;
+		tx_done(ks);
+	}
+#endif
+
+	if (unlikely(status & IRQ_LCI)) {
+		struct sw_priv *hw_priv = ks->parent;
+
+		schedule_delayed_work(&hw_priv->link_read, 0);
+	}
+
+	if (unlikely(status & (IRQ_RXWFDI | IRQ_RXMPDI | IRQ_LDI | IRQ_EDI))) {
+		u16 pmecr = ks_rdreg16(hw, KS_PMCTRL);
+
+		pmecr &= ~PMCTRL_WKEVT_MASK;
+		if (status & IRQ_RXWFDI)
+			pmecr |= PMCTRL_WKEVT_FRAME;
+		if (status & IRQ_RXMPDI)
+			pmecr |= PMCTRL_WKEVT_MAGICPKT;
+		if (status & IRQ_LDI)
+			pmecr |= PMCTRL_WKEVT_LINK;
+		if (status & IRQ_EDI)
+			pmecr |= PMCTRL_WKEVT_ENERGY;
+		ks_wrreg16(hw, KS_PMCTRL, pmecr);
+		pmecr = ks_rdreg16(hw, KS_PME);
+		pmecr &= ~PME_POLARITY;
+		ks_wrreg16(hw, KS_PME, pmecr);
+	}
+
+	/* this should be the last in IRQ handler */
+	ks_restore_cmd_reg(hw);
+	return IRQ_HANDLED;
+}
+
+static int empty_addr(u8 *addr)
+{
+	u16 *addr1 = (u16 *) addr;
+	u16 *addr2 = (u16 *) &addr[2];
+	u16 *addr3 = (u16 *) &addr[4];
+
+	return (0 == *addr1 && 0 == *addr2 && 0 == *addr3) ||
+		(0xffff == *addr1 && 0xffff == *addr2 && 0xffff == *addr3);
+}  /* empty_addr */
+
+static int ks_hw_init(struct ksz_hw *hw)
+{
+	static u8 KS_DEFAULT_MAC_ADDRESS[] = {
+		0x00, 0x10, 0xA1, 0x84, 0x62, 0x11 };
+
+	/* cache the contents of the CCR register for EEPROM, etc. */
+	hw->rc_ccr = ks_rdreg16(hw, KS_CCR);
+
+	ks_read_mac(hw, hw->mac_addr);
+	if (empty_addr(hw->mac_addr) || !is_valid_ether_addr(hw->mac_addr))
+		ks_set_mac(hw, KS_DEFAULT_MAC_ADDRESS);
+	return true;
+}
+
+static void ks_setup(struct ksz_hw *hw)
+{
+	u16 w;
+
+	/* Setup flow control watermark. */
+	ks_wrreg16(hw, KS_FCLWR, 0x800);
+
+	/**
+	 * Configure QMU Transmit
+	 */
+
+	/* Setup Transmit Frame Data Pointer Auto-Increment (TXFDPR) */
+	ks_wrreg16(hw, KS_TXFDPR, TXFDPR_TXFPAI);
+
+	/* Setup Receive Frame Data Pointer Auto-Increment */
+	ks_wrreg16(hw, KS_RXFDPR, RXFDPR_RXFPAI);
+
+	/* Setup Receive Frame Threshold - 1 frame */
+	ks_wrreg16(hw, KS_RXFTR, 1 & RXFTR_MASK);
+
+	/* Setup RxQ Command Control (RXQCR) */
+	hw->rc_rxqcr = RXQCR_CMD_CNTL;
+#ifdef USE_2_BYTE_OFFSET
+	hw->rc_rxqcr |= RXQCR_RXIPHTOE;
+#endif
+
+	/* Adjust these settings for best performance. */
+	ks_wrreg16(hw, KS_RXFTR, 3 & RXFTR_MASK);
+	ks_wrreg16(hw, KS_RXDTTR, 1000);
+	ks_wrreg16(hw, KS_RXDBCTR, 1518 * 2);
+	hw->rc_rxqcr |= RXQCR_RXDTTE;
+	hw->rc_rxqcr |= RXQCR_RXDBCTE;
+	ks_wrreg16(hw, KS_RXQCR, hw->rc_rxqcr);
+	hw->rc_txqcr = 0;
+	hw->rc_qcr = (hw->rc_rxqcr << 16) | hw->rc_txqcr;
+
+	w = TXCR_TXFCE | TXCR_TXPE | TXCR_TXCRC;
+	ks_wrreg16(hw, KS_TXCR, w);
+
+	w = RXCR1_RXFCE | RXCR1_RXBE | RXCR1_RXUE | RXCR1_RXME | RXCR1_RXIPFCC;
+
+	if (hw->promiscuous)
+		w |= (RXCR1_RXAE | RXCR1_RXINVF);
+	else if (hw->all_mcast)
+		/* Multicast address passed mode */
+		w |= (RXCR1_RXAE | RXCR1_RXMAFMA | RXCR1_RXPAFMA);
+	else
+		/* Normal mode */
+		w |= RXCR1_RXPAFMA;
+
+	ks_wrreg16(hw, KS_RXCR1, w);
+	hw->rc_rxcr1 = w;
+
+	w = ks_rdreg16(hw, KS_RXCR2);
+	w |= RXCR2_IUFFP;
+	ks_wrreg16(hw, KS_RXCR2, w);
+}  /* ks_setup */
+
+static void ks_setup_int(struct ksz_hw *hw)
+{
+	hw->rc_ier = 0x00;
+
+	/* Clear the interrupts status of the hardware. */
+	ks_wrreg16(hw, KS_ISR, 0xffff);
+
+	/* Enable the interrupts of the hardware. */
+	hw->rc_ier = (IRQ_LCI | IRQ_RXI);
+#ifdef CONFIG_1588_PTP
+	/*
+	 * PTP interrupts are always enabled by their own interrupt registers.
+	 * It is not necessary to manually enable them in the main interrupt
+	 * register.
+	 */
+	hw->rc_ier |= (IRQ_TS | IRQ_TRIG);
+#endif
+#ifdef USE_TX_DONE_INTR
+	hw->rc_ier |= IRQ_TXI;
+#endif
+#ifdef USE_TX_BUF_INTR
+	hw->rc_ier |= IRQ_TXSAI;
+#endif
+	hw->rc_ier |= IRQ_RXWFDI | IRQ_RXMPDI | IRQ_LDI | IRQ_EDI;
+	hw->rc_isr = hw->rc_ier;
+	hw->rc_isr |= (IRQ_RXOI | IRQ_RXPSI | IRQ_TXPSI);
+}  /* ks_setup_int */
+
+#ifdef CONFIG_NET_POLL_CONTROLLER
+static void netdev_netpoll(struct net_device *dev)
+{
+	struct ks_net *priv = netdev_priv(dev);
+	struct dev_info *hw_priv = priv->hw_priv;
+	struct ksz_hw *hw = &hw_priv->hw;
+
+	ks_disable_int(hw);
+	ks_irq(dev->irq, hw_priv);
+	ks_enable_int(hw);
+}  /* netdev_netpoll */
+#endif
+
+static void reset_hw(struct dev_info *hw_priv, int complete)
+{
+	struct ksz_hw *hw = &hw_priv->hw;
+	struct ksz_sw *sw = hw_priv->sw;
+	int reset = false;
+
+	if (sw_is_switch(sw))
+		reset = sw->net_ops->stop(sw, complete);
+#ifdef CONFIG_1588_PTP
+	else if (complete && (sw->features & PTP_HW)) {
+		struct ptp_info *ptp = &sw->ptp_hw;
+
+		reset = ptp->ops->stop(ptp, true);
+	}
+#endif
+
+	mutex_lock(hw_priv->lock);
+	if (!reset) {
+		if (complete)
+			ks_soft_reset(hw, GRR_GSR);
+		else
+			ks_soft_reset(hw, GRR_QMU);
+	}
+	mutex_unlock(hw_priv->lock);
+}  /* reset_hw */
+
+static void start_hw(struct dev_info *ks, struct ksz_hw *hw)
+{
+	mutex_lock(ks->lock);
+
+	ks_wrreg16(hw, KS_ISR, 0xffff);
+	ks_enable_int(hw);
+	ks_enable_qmu(hw);
+	hw->tx_max = ks_tx_fifo_space(hw);
+	if (!hw->tx_max)
+		hw->tx_max = 6144;
+	hw->oper_state = 0;
+
+	mutex_unlock(ks->lock);
+
+#ifdef CONFIG_1588_PTP
+	do {
+		struct ksz_sw *sw = ks->sw;
+
+		if ((sw->features & PTP_HW) && !sw_is_switch(sw)) {
+			struct ptp_info *ptp = &sw->ptp_hw;
+
+			ptp->reg->start(ptp, true);
+		}
+	} while (0);
+#endif
+}  /* start_hw */
+
+#define	KS_INT_FLAGS	(IRQF_TRIGGER_LOW)
+
+static int netdev_open_before(struct net_device *dev, struct dev_priv *priv,
+	struct dev_info *hw_priv, struct ksz_hw *hw)
+{
+	int err;
+	struct ksz_sw *sw = hw_priv->sw;
+
+	hw->all_mcast = 0;
+	hw->mcast_lst_size = hw->mcast_lst_reserved;
+
+	mutex_lock(hw_priv->lock);
+
+	/* wake up powermode to normal mode */
+	netif_dbg(priv, hw, dev, "setting power mode %d\n", PMCTRL_PM_NORMAL);
+	ks_set_powermode(hw, PMCTRL_PM_NORMAL);
+
+	/* wait for normal mode to take effect */
+	mdelay(1);
+
+	ks_disable_qmu(hw);
+	if (hw->overrides & RX_LOOPBACK)
+		hw->promiscuous++;
+	ks_setup(hw);
+	ks_setup_int(hw);
+	mutex_unlock(hw_priv->lock);
+
+	/* Remember the network device that requests interrupts. */
+	hw_priv->dev = dev;
+	err = request_irq(dev->irq, ks_irq, KS_INT_FLAGS, DRV_NAME, hw_priv);
+	if (err) {
+		pr_err("Failed to request IRQ: %d: %d\n", dev->irq, err);
+		return err;
+	}
+	tasklet_enable(&hw_priv->rx_tasklet);
+	napi_enable(&hw_priv->napi);
+
+	/* Default is to use receive interrupt. */
+	hw->rx_max = MAX_RECV_FRAMES;
+
+	mutex_lock(hw_priv->lock);
+	hw_add_wol_bcast(hw);
+	mutex_unlock(hw_priv->lock);
+
+	if (sw_is_switch(sw))
+		sw->net_ops->open_dev(sw, dev, &priv->port, hw->mac_addr);
+	else {
+		sw_init_mib(sw);
+		sw_disable(sw);
+	}
+	return 0;
+}  /* netdev_open_before */
+
+/**
+ * ks_net_open - open network device
+ * @dev:	The network device being opened.
+ *
+ * Called when the network device is marked active, such as a user executing
+ * 'ifconfig up' on the device.
+ */
+static int ks_net_open(struct net_device *dev)
+{
+	struct ks_net *priv = netdev_priv(dev);
+	struct dev_info *hw_priv = priv->hw_priv;
+	struct ksz_hw *hw = &hw_priv->hw;
+	struct ksz_sw *sw = hw_priv->sw;
+	struct ksz_port *port = &priv->port;
+	int err;
+
+	netif_dbg(priv, ifup, dev, "opening\n");
+	priv->multicast = 0;
+	priv->promiscuous = 0;
+
+	/* Reset device statistics. */
+	memset(&dev->stats, 0, sizeof(struct net_device_stats));
+
+	if (!(hw_priv->opened)) {
+		err = netdev_open_before(dev, priv, hw_priv, hw);
+		if (err)
+			return err;
+	}
+	sw->net_ops->open_port(sw, dev, &priv->port);
+
+	if (!(hw_priv->opened)) {
+		start_hw(hw_priv, hw);
+		sw->net_ops->open(sw);
+	}
+
+	hw_priv->opened++;
+
+	priv->media_state = port->linked->state;
+	set_media_state(dev, media_connected);
+
+	netif_start_queue(dev);
+
+	netif_dbg(priv, ifup, dev, "network device up\n");
+	netif_info(priv, ifup, dev, "up\n");
+
+	return 0;
+}
+
+/**
+ * ks_net_stop - close network device
+ * @dev:	The device being closed.
+ *
+ * Called to close down a network device which has been active. Cancell any
+ * work, shutdown the RX and TX process and then place the chip into a low
+ * power state whilst it is not being used.
+ */
+static int ks_net_stop(struct net_device *dev)
+{
+	struct ks_net *priv = netdev_priv(dev);
+	struct dev_info *hw_priv = priv->hw_priv;
+	struct ksz_hw *hw = &hw_priv->hw;
+	struct ksz_sw *sw = hw_priv->sw;
+
+	netif_info(priv, ifdown, dev, "shutting down\n");
+
+	netif_stop_queue(dev);
+
+	sw->net_ops->close_port(sw, dev, &priv->port);
+
+	if (priv->multicast)
+		--hw->all_mcast;
+	if (priv->promiscuous)
+		--hw->promiscuous;
+
+	hw_priv->opened--;
+	if (hw_priv->opened)
+		return 0;
+
+	sw->net_ops->close(sw);
+	reset_hw(hw_priv, true);
+
+	mutex_lock(hw_priv->lock);
+
+	/* turn off the IRQs and ack any outstanding */
+	ks_wrreg16(hw, KS_IER, 0x0000);
+	ks_wrreg16(hw, KS_ISR, 0xffff);
+
+	/* shutdown RX/TX QMU */
+	ks_disable_qmu(hw);
+
+	/* set powermode to soft power down to save power */
+	netif_dbg(priv, hw, dev, "setting power mode %d\n", PMCTRL_PM_SOFTDOWN);
+	ks_set_powermode(hw, PMCTRL_PM_SOFTDOWN);
+
+	mutex_unlock(hw_priv->lock);
+
+	napi_disable(&hw_priv->napi);
+	tasklet_disable(&hw_priv->rx_tasklet);
+
+	free_irq(dev->irq, hw_priv);
+	return 0;
+}
+
+/**
+ * Called in interrupt context.
+ */
+static void ks_tx_timeout(struct net_device *dev, unsigned int txqueue)
+{
+	struct ks_net *priv = netdev_priv(dev);
+	struct dev_info *hw_priv = priv->hw_priv;
+
+	if (hw_priv->hw.oper_state & OPER_STATE_TX_RESET)
+		return;
+
+	dbg_msg("tx timeout:%lx\n", jiffies);
+	printk(KERN_DEBUG "tx timeout:%lx\n", jiffies);
+
+	hw_priv->hw.oper_state |= OPER_STATE_TX_RESET;
+	schedule_work(&hw_priv->tx_reset);
+}
+
+static void ks_tx_reset(struct work_struct *work)
+{
+	struct dev_info *ks = container_of(work, struct dev_info, tx_reset);
+	struct ksz_hw *hw = &ks->hw;
+	struct ksz_sw *sw = ks->sw;
+
+	reset_hw(ks, false);
+
+	mutex_lock(ks->lock);
+	ks_setup(hw);
+	mutex_unlock(ks->lock);
+
+	if (sw_is_switch(sw))
+		sw->net_ops->start(sw, hw->mac_addr);
+	else
+		sw_disable(sw);
+
+	sw->net_ops->netdev_open_port(sw, ks->dev);
+
+	start_hw(ks, hw);
+
+	if (sw_is_switch(sw))
+		sw->net_ops->netdev_start_queue(sw, ks->dev);
+	else {
+		netif_start_queue(ks->dev);
+	}
+
+	schedule_work(&ks->rxctrl_work);
+}
+
+/**
+ * ks_start_xmit - transmit packet
+ * @skb:	The buffer to transmit
+ * @dev:	The device used to transmit the packet.
+ *
+ * Called by the network layer to transmit the @skb.
+ * spin_lock_irqsave is required because tx and rx should be mutual exclusive.
+ * So while tx is in-progress, prevent IRQ interrupt from happenning.
+ */
+static int ks_start_xmit(struct sk_buff *skb, struct net_device *dev)
+{
+	unsigned long flags;
+	int retv = NETDEV_TX_OK;
+	struct ks_net *priv = netdev_priv(dev);
+	struct dev_info *hw_priv = priv->hw_priv;
+	struct ksz_hw *hw = &hw_priv->hw;
+	struct ksz_sw *sw = hw_priv->sw;
+	int len;
+
+	if (sw_is_switch(sw)) {
+		skb = sw->net_ops->check_tx(sw, dev, skb, &priv->port);
+		if (!skb)
+			return 0;
+	}
+
+#ifdef CONFIG_1588_PTP
+	if (sw->features & PTP_HW) {
+		struct ptp_info *ptp = &sw->ptp_hw;
+
+		if (skb_shinfo(skb)->tx_flags & SKBTX_HW_TSTAMP)
+			ptp->ops->get_tx_tstamp(ptp, skb);
+	}
+	skb_tx_timestamp(skb);
+#endif
+
+/*
+ * THa  2012/10/24
+ * The formula of the packet length is (MTU + 14 - 2) / 8 * 8 + 2).
+ * The formula of the required buffer is (ALIGN(len + 4), 4).
+ * So the required buffer length can be (len + 4 + 2).
+ *
+ * Keep a minimum of 4 bytes in the buffer to easily detect the zero buffer
+ * problem.
+ */
+	len = ALIGN(skb->len + 4, 4);
+	spin_lock_irqsave(&priv->statelock, flags);
+	ks_save_cmd_reg(hw);
+
+	hw->tx_len = ks_tx_fifo_space(hw);
+	if (likely(hw->tx_len - TX_BUF_MIN >= len)) {
+		hw->tx_len -= len;
+		hw->tx_max = hw->tx_len;
+#ifdef USE_TX_DONE_INTR
+		if (hw->tx_len < TX_DONE_MIN) {
+			if (sw_is_switch(sw))
+				sw->net_ops->netdev_stop_queue(sw, dev);
+			else
+				netif_stop_queue(dev);
+			hw->oper_state |= OPER_STATE_TX_PAUSED;
+		}
+#endif
+		ks_write_qmu(hw, skb->data, skb->len);
+		if (netif_msg_pktdata(priv))
+			ks_dbg_dumpkkt(priv, skb->data, skb->len, 1);
+
+		/* Update transmit statistics. */
+		dev->stats.tx_packets++;
+		dev->stats.tx_bytes += skb->len;
+
+#ifdef CONFIG_1588_PTP_
+		/* Still waiting for tx timestamp. */
+		if (skb_shinfo(skb)->tx_flags &	SKBTX_IN_PROGRESS)
+			skb_shinfo(skb)->tx_flags &= ~SKBTX_HW_TSTAMP;
+		else
+#endif
+		dev_kfree_skb(skb);
+	} else {
+		if (sw_is_switch(sw))
+			sw->net_ops->netdev_stop_queue(sw, dev);
+		else
+			netif_stop_queue(dev);
+		hw->oper_state |= OPER_STATE_TX_PAUSED;
+#ifdef USE_TX_BUF_INTR
+		hw->rc_txqcr = ks_rdreg16(hw, KS_TXQCR);
+
+/*
+ * THa  2012/10/04
+ * The size requested is actually in multiple of 4 bytes.  Setting a larger
+ * size actually has better transmit performance maybe due to not to process
+ * too many interrupts.
+ * In revisions C and older, setting a size more than 1568 will trigger the
+ * 2-byte offset bit setting bug in the receive configuration register.
+ */
+		ks_wrreg16(hw, KS_TXNTFSR, (1520 * 3) / 4);
+		hw->rc_txqcr |= TXQCR_TXQMAM;
+		hw->rc_qcr = (hw->rc_rxqcr << 16) | hw->rc_txqcr;
+		ks_wrreg16(hw, KS_TXQCR, hw->rc_txqcr);
+#endif
+		retv = NETDEV_TX_BUSY;
+	}
+
+	ks_restore_cmd_reg(hw);
+	spin_unlock_irqrestore(&priv->statelock, flags);
+	return retv;
+}
+
+static void set_multi_hash(u8 *addr, u16 *bits)
+{
+	int position;
+	int index;
+	int value;
+
+	position = (ether_crc(6, addr) >> (32 - 6));
+	index = position >> 4;
+	value = 1 << (position & 0xf);
+	bits[index] |= (u16) value;
+}  /* set_multi_hash */
+
+/**
+ * ks_set_grpaddr - set multicast information
+ * @hw:		The hardware instance.
+ *
+ * It is assumed the registers will be programmed in another routine.
+ */
+static void ks_set_grpaddr(struct ksz_hw *hw)
+{
+	u8 i;
+
+	memset(hw->mcast_bits, 0, sizeof(u16) * HW_MCAST_SIZE);
+
+	for (i = 0; i < hw->mcast_lst_size; i++)
+		set_multi_hash(hw->mcast_lst[i], hw->mcast_bits);
+}  /* ks_set_grpaddr */
+
+/**
+ * ks_clear_mcast - clear multicast information
+ * @hw:		The hardware instance.
+ *
+ * This routine removes all mcast addresses set in the hardware.
+ * It is assumed the registers will be programmed in another routine.
+ */
+static void ks_clear_mcast(struct ksz_hw *hw)
+{
+	int i;
+
+	for (i = 0; i < HW_MCAST_SIZE; i++)
+		hw->mcast_bits[i] = 0;
+}
+
+static void ks_set_promis(struct ksz_hw *hw, int promiscuous_mode)
+{
+	hw->rc_rxcr1 &= ~RXCR1_FILTER_MASK;
+	if (promiscuous_mode)
+		/* Enable Promiscuous mode */
+		hw->rc_rxcr1 |= RXCR1_RXAE | RXCR1_RXINVF;
+	else if (hw->all_mcast)
+		/* Enable "Perfect with Multicast address passed mode" */
+		hw->rc_rxcr1 |= (RXCR1_RXAE | RXCR1_RXMAFMA | RXCR1_RXPAFMA);
+	else
+		/* Disable Promiscuous mode (default normal mode) */
+		hw->rc_rxcr1 |= RXCR1_RXPAFMA;
+}  /* ks_set_promis */
+
+static void ks_set_mcast(struct ksz_hw *hw, int mcast)
+{
+	if (hw->promiscuous)
+		return;
+
+	hw->rc_rxcr1 &= ~RXCR1_FILTER_MASK;
+	if (mcast)
+		/* Enable "Perfect with Multicast address passed mode" */
+		hw->rc_rxcr1 |= (RXCR1_RXAE | RXCR1_RXMAFMA | RXCR1_RXPAFMA);
+	else
+		/**
+		 * Disable "Perfect with Multicast address passed
+		 * mode" (normal mode).
+		 */
+		hw->rc_rxcr1 |= RXCR1_RXPAFMA;
+}  /* ks_set_mcast */
+
+static void dev_set_promiscuous(struct net_device *dev, struct ks_net *priv,
+	struct ksz_hw *hw, struct ksz_sw *sw, int promiscuous)
+{
+	if (promiscuous != priv->promiscuous) {
+		u8 prev_state = hw->promiscuous;
+
+		if (promiscuous)
+			++hw->promiscuous;
+		else
+			--hw->promiscuous;
+		priv->promiscuous = promiscuous;
+
+		/* Turn on/off promiscuous mode. */
+		if (hw->promiscuous <= 1 && prev_state <= 1)
+			ks_set_promis(hw, hw->promiscuous);
+	}
+}
+
+static void dev_set_multicast(struct ks_net *priv, struct ksz_hw *hw,
+	int multicast)
+{
+	if (multicast != priv->multicast) {
+		u8 all_mcast = hw->all_mcast;
+
+		if (multicast)
+			++hw->all_mcast;
+		else
+			--hw->all_mcast;
+		priv->multicast = multicast;
+
+		/* Turn on/off all multicast mode. */
+		if (hw->all_mcast <= 1 && all_mcast <= 1)
+			ks_set_mcast(hw, hw->all_mcast);
+	}
+}
+
+/**
+ * ks_rxctrl_work - work handler to change rx mode
+ * @work: The work structure this belongs to.
+ *
+ * Lock the device and issue the necessary changes to the receive mode from
+ * the network device layer. This is done so that we can do this without
+ * having to sleep whilst holding the network device lock.
+ *
+ * Since the recommendation from Micrel is that the RXQ is shutdown whilst the
+ * receive parameters are programmed, we issue a write to disable the RXQ and
+ * then wait for the interrupt handler to be triggered once the RXQ shutdown is
+ * complete. The interrupt handler then writes the new values into the chip.
+ */
+static void ks_rxctrl_work(struct work_struct *work)
+{
+	int i;
+	struct dev_info *ks = container_of(work, struct dev_info, rxctrl_work);
+	struct ksz_hw *hw = &ks->hw;
+
+	mutex_lock(ks->lock);
+
+	/* Stop receiving for reconfiguration */
+	ks_stop_rx(hw);
+
+	for (i = 0; i < HW_MCAST_SIZE; i++)
+		ks_wrreg16(hw, KS_MAHTR0 + i * 2, hw->mcast_bits[i]);
+	ks_wrreg16(hw, KS_RXCR1, hw->rc_rxcr1);
+	memcpy(&hw->rxctrl, hw->mcast_bits, sizeof(hw->mcast_bits));
+	hw->rxctrl.rxcr1 = hw->rc_rxcr1;
+
+	if (hw->enabled)
+		ks_start_rx(hw);
+
+	mutex_unlock(ks->lock);
+}
+
+static void ks_set_rx_mode(struct net_device *dev)
+{
+	struct ks_net *priv = netdev_priv(dev);
+	struct dev_info *hw_priv = priv->hw_priv;
+	struct ksz_hw *ks = &hw_priv->hw;
+	struct ksz_sw *sw = hw_priv->sw;
+	int flags = dev->flags;
+	int multicast = ((dev->flags & IFF_ALLMULTI) == IFF_ALLMULTI);
+
+	/* Turn on/off promiscuous mode. */
+	dev_set_promiscuous(dev, priv, ks, sw,
+		((dev->flags & IFF_PROMISC) == IFF_PROMISC));
+
+	/* Turn on/off all mcast mode. */
+	if (sw->dev_count > 1) {
+		if ((flags & IFF_MULTICAST) && !netdev_mc_empty(dev))
+			sw->net_ops->set_multi(sw, dev, &priv->port);
+		priv->multi_list_size = 0;
+
+		/* Do not update multi_list_size. */
+		if (flags & IFF_ALLMULTI)
+			flags &= ~IFF_MULTICAST;
+
+		/* Turn on all multicast. */
+		multicast |= ((dev->flags & IFF_MULTICAST) == IFF_MULTICAST);
+	}
+	dev_set_multicast(priv, ks, multicast);
+
+	if ((flags & IFF_MULTICAST) && !netdev_mc_empty(dev)) {
+		if (netdev_mc_count(dev) <= MAX_MCAST_LST) {
+			int i = ks->mcast_lst_reserved;
+			struct netdev_hw_addr *ha;
+
+			if (i) {
+				int j;
+
+				for (j = 0; j < i; j++)
+					memcpy(priv->multi_list[j],
+						ks->mcast_lst[j], ETH_ALEN);
+			}
+			netdev_for_each_mc_addr(ha, dev) {
+				if (!(*ha->addr & 1))
+					continue;
+				if (i >= MAX_MCAST_LST)
+					break;
+				memcpy(priv->multi_list[i], ha->addr, ETH_ALEN);
+				memcpy(ks->mcast_lst[i++], ha->addr, ETH_ALEN);
+			}
+			priv->multi_list_size = (u8) i;
+			ks->mcast_lst_size = (u8) i;
+			ks_set_grpaddr(ks);
+		} else {
+			/**
+			 * List too big to support so
+			 * turn on all mcast mode.
+			 */
+			priv->multi_list_size = 0;
+			ks->mcast_lst_size = MAX_MCAST_LST;
+			ks_set_mcast(ks, true);
+		}
+	} else {
+		priv->multi_list_size = ks->mcast_lst_reserved;
+		ks->mcast_lst_size = ks->mcast_lst_reserved;
+		if (!ks->mcast_lst_size)
+			ks_clear_mcast(ks);
+	}
+	schedule_work(&hw_priv->rxctrl_work);
+}  /* ks_set_rx_mode */
+
+static int ks_set_mac_address(struct net_device *dev, void *addr)
+{
+	struct ks_net *priv = netdev_priv(dev);
+	struct dev_info *hw_priv = priv->hw_priv;
+	struct ksz_hw *hw = &hw_priv->hw;
+	struct ksz_sw *sw = hw_priv->sw;
+	struct sockaddr *sa = addr;
+	u8 *da;
+
+	memcpy(dev->dev_addr, sa->sa_data, dev->addr_len);
+
+	da = (u8 *) dev->dev_addr;
+
+	ks_stop_rx(hw);  /* Stop receiving for reconfiguration */
+	ks_set_mac(hw, dev->dev_addr);
+	if (hw->enabled)
+		ks_start_rx(hw);
+
+	if (sw->info) {
+		u8 promiscuous = hw->promiscuous;
+
+		promiscuous = sw->net_ops->set_mac_addr(sw, dev, promiscuous,
+			priv->port.first_port);
+		if (promiscuous != hw->promiscuous) {
+			hw->promiscuous = promiscuous;
+			if (hw->promiscuous <= 1 && promiscuous <= 1)
+				ks_set_promis(hw, hw->promiscuous);
+		}
+	}
+
+#ifdef CONFIG_1588_PTP_
+	if ((sw->features & PTP_HW) && !sw_is_switch(sw)) {
+		struct ptp_info *ptp = &sw->ptp_hw;
+
+		ptp->ops->set_identity(ptp, hw->mac_addr);
+	}
+#endif
+	return 0;
+}
+
+static int netdev_add_vid(struct net_device *dev, __be16 proto, u16 vid)
+{
+	struct ks_net *priv = netdev_priv(dev);
+	struct dev_info *hw_priv = priv->hw_priv;
+	struct ksz_sw *sw = hw_priv->sw;
+
+	if ((sw->features & VLAN_PORT) && vid >= VLAN_PORT_START) {
+		vid -= VLAN_PORT_START;
+		if (vid <= SWITCH_PORT_NUM)
+			sw->vlan_id |= (1 << vid);
+	}
+	return 0;
+}
+
+static int netdev_kill_vid(struct net_device *dev, __be16 proto, u16 vid)
+{
+	struct ks_net *priv = netdev_priv(dev);
+	struct dev_info *hw_priv = priv->hw_priv;
+	struct ksz_sw *sw = hw_priv->sw;
+
+	if ((sw->features & VLAN_PORT) && vid >= VLAN_PORT_START) {
+		vid -= VLAN_PORT_START;
+		if (vid <= SWITCH_PORT_NUM)
+			sw->vlan_id &= ~(1 << vid);
+	}
+	return 0;
+}
+
+/* RX errors = rx_errors */
+/* RX dropped = rx_dropped */
+/* RX overruns = rx_fifo_errors */
+/* RX frame = rx_crc_errors + rx_frame_errors + rx_length_errors */
+/* TX errors = tx_errors */
+/* TX dropped = tx_dropped */
+/* TX overruns = tx_fifo_errors */
+/* TX carrier = tx_aborted_errors + tx_carrier_errors + tx_window_errors */
+/* collisions = collisions */
+
+/**
+ * ks_net_query_statistics - query network device statistics
+ * @dev:	Network device.
+ *
+ * This function returns the statistics of the network device.  The device
+ * needs not be opened.
+ *
+ * Return network device statistics.
+ */
+static struct net_device_stats *ks_net_query_statistics(struct net_device *dev)
+{
+	struct ks_net *priv = netdev_priv(dev);
+	struct ksz_port *port = &priv->port;
+	struct ksz_sw *sw = &priv->sw_priv->sw;
+	struct ksz_port_mib *mib;
+	int i;
+	int p;
+
+	/* Reset to zero to add count later. */
+	dev->stats.multicast = 0;
+	dev->stats.collisions = 0;
+	dev->stats.rx_length_errors = 0;
+	dev->stats.rx_crc_errors = 0;
+	dev->stats.rx_frame_errors = 0;
+	dev->stats.tx_window_errors = 0;
+
+	for (i = 0, p = port->first_port; i < port->mib_port_cnt; i++, p++) {
+		mib = &sw->port_mib[p];
+
+		dev->stats.multicast += (unsigned long)
+			mib->counter[MIB_COUNTER_RX_MULTICAST];
+
+		dev->stats.collisions += (unsigned long)
+			mib->counter[MIB_COUNTER_TX_TOTAL_COLLISION];
+
+		dev->stats.rx_length_errors += (unsigned long)(
+			mib->counter[MIB_COUNTER_RX_UNDERSIZE] +
+			mib->counter[MIB_COUNTER_RX_FRAGMENT] +
+			mib->counter[MIB_COUNTER_RX_OVERSIZE] +
+			mib->counter[MIB_COUNTER_RX_JABBER]);
+		dev->stats.rx_crc_errors += (unsigned long)
+			mib->counter[MIB_COUNTER_RX_CRC_ERR];
+		dev->stats.rx_frame_errors += (unsigned long)(
+			mib->counter[MIB_COUNTER_RX_ALIGNMENT_ERR] +
+			mib->counter[MIB_COUNTER_RX_SYMBOL_ERR]);
+
+		dev->stats.tx_window_errors += (unsigned long)
+			mib->counter[MIB_COUNTER_TX_LATE_COLLISION];
+	}
+
+	return &dev->stats;
+}
+
+static int ks_net_ioctl(struct net_device *dev, struct ifreq *req, int cmd)
+{
+	struct ks_net *priv = netdev_priv(dev);
+	struct sw_priv *sw_priv = priv->sw_priv;
+	struct dev_info *hw_priv = priv->hw_priv;
+#ifdef CONFIG_1588_PTP
+	struct ksz_sw *sw = hw_priv->sw;
+	struct ptp_info *ptp = &sw->ptp_hw;
+#endif
+	int result = 0;
+
+	if (!netif_running(dev))
+		return -EINVAL;
+
+	if (down_interruptible(&sw_priv->proc_sem))
+		return -ERESTARTSYS;
+
+	switch (cmd) {
+#ifdef CONFIG_1588_PTP
+	case SIOCSHWTSTAMP:
+		result = -EOPNOTSUPP;
+		if (sw->features & PTP_HW)
+			result = ptp->ops->hwtstamp_ioctl(ptp, req, 0x3);
+		break;
+	case SIOCDEVPRIVATE + 15:
+		result = -EOPNOTSUPP;
+		if (sw->features & PTP_HW)
+			result = ptp->ops->dev_req(ptp, req->ifr_data,
+				NULL);
+		break;
+#endif
+	default:
+#ifdef CONFIG_1588_PTP
+		if (sw->features & PTP_HW) {
+			result = ptp->ops->ixxat_ioctl(ptp, cmd, req);
+			if (!result)
+				break;
+		}
+#endif
+		result = generic_mii_ioctl(&priv->mii_if, if_mii(req), cmd,
+			NULL);
+	}
+
+	up(&sw_priv->proc_sem);
+
+	return result;
+}
+
+/* ethtool support */
+
+static void ks_get_drvinfo(struct net_device *dev,
+	struct ethtool_drvinfo *di)
+{
+	strlcpy(di->driver, DRV_NAME, sizeof(di->driver));
+	strlcpy(di->version, DRV_VERSION, sizeof(di->version));
+	strlcpy(di->bus_info, dev_name(dev->dev.parent), sizeof(di->bus_info));
+}
+
+static u32 ks_get_msglevel(struct net_device *dev)
+{
+	struct ks_net *ks = netdev_priv(dev);
+	return ks->msg_enable;
+}
+
+static void ks_set_msglevel(struct net_device *dev, u32 to)
+{
+	struct ks_net *ks = netdev_priv(dev);
+	struct sw_priv *hw_priv = ks->sw_priv;
+	struct ksz_sw *sw = &hw_priv->sw;
+
+	sw->msg_enable = to;
+	ks->msg_enable = to;
+}
+
+#if 0
+static int ks_get_settings(struct net_device *dev, struct ethtool_cmd *cmd)
+{
+	struct ks_net *ks = netdev_priv(dev);
+	return mii_ethtool_gset(&ks->mii_if, cmd);
+}
+
+static int ks_set_settings(struct net_device *dev, struct ethtool_cmd *cmd)
+{
+	struct ks_net *ks = netdev_priv(dev);
+	return mii_ethtool_sset(&ks->mii_if, cmd);
+}
+#endif
+
+static u32 ks_get_link(struct net_device *dev)
+{
+	struct ks_net *ks = netdev_priv(dev);
+	return mii_link_ok(&ks->mii_if);
+}
+
+static int ks_nway_reset(struct net_device *dev)
+{
+	struct ks_net *ks = netdev_priv(dev);
+	return mii_nway_restart(&ks->mii_if);
+}
+
+/**
+ * ks_get_regs_len - get length of register dump
+ * @dev:	Network device.
+ *
+ * This function returns the length of the register dump.
+ *
+ * Return length of the register dump.
+ */
+static int ks_get_regs_len(struct net_device *dev)
+{
+	struct sw_regs *range = sw_regs_range;
+	int len;
+	int num;
+	int regs_len;
+
+	regs_len = 0;
+	while (range->end > range->start) {
+		num = (range->end - range->start + 1) / 2;
+		num = (num + 7) / 8;
+		len = (8 * 5 + 6) * num;
+		regs_len += len;
+		range++;
+	}
+	return regs_len;
+}  /* ks_get_regs_len */
+
+/**
+ * ks_get_regs - get register dump
+ * @dev:	Network device.
+ * @regs:	Ethtool registers data structure.
+ * @ptr:	Buffer to store the register values.
+ *
+ * This procedure dumps the register values in the provided buffer.
+ */
+static void ks_get_regs(struct net_device *dev, struct ethtool_regs *regs,
+	void *ptr)
+{
+	struct dev_priv *priv = netdev_priv(dev);
+	struct dev_info *hw_priv = priv->hw_priv;
+	struct ksz_hw *hw = &hw_priv->hw;
+	char *buf = ptr;
+	struct sw_regs *range = sw_regs_range;
+	int i;
+	int len;
+	int val;
+
+	regs->version = 0;
+	while (range->end > range->start) {
+		len = range->start;
+		while (len < range->end) {
+			buf += sprintf(buf, "%04x:", len);
+			for (i = 0; i < 8; i++, len += 2) {
+				mutex_lock(hw_priv->lock);
+				val = ks_rdreg16(hw, len);
+				mutex_unlock(hw_priv->lock);
+				buf += sprintf(buf, " %04x", val);
+			}
+			buf += sprintf(buf, "\n");
+		}
+		range++;
+	}
+}  /* ks_get_regs */
+
+#define WOL_SUPPORT			\
+	(WAKE_PHY | WAKE_MAGIC |	\
+	WAKE_UCAST | WAKE_MCAST |	\
+	WAKE_BCAST | WAKE_ARP)
+
+/**
+ * ks_get_wol - get Wake-on-LAN support
+ * @dev:	Network device.
+ * @wol:	Ethtool Wake-on-LAN data structure.
+ *
+ * This procedure returns Wake-on-LAN support.
+ */
+static void ks_get_wol(struct net_device *dev,
+	struct ethtool_wolinfo *wol)
+{
+	struct ks_net *priv = netdev_priv(dev);
+	struct dev_info *hw_priv = priv->hw_priv;
+
+	wol->supported = hw_priv->wol_support;
+	wol->wolopts = hw_priv->wol_enable;
+	memset(&wol->sopass, 0, sizeof(wol->sopass));
+}
+
+/**
+ * ks_set_wol - set Wake-on-LAN support
+ * @dev:	Network device.
+ * @wol:	Ethtool Wake-on-LAN data structure.
+ *
+ * This function sets Wake-on-LAN support.
+ *
+ * Return 0 if successful; otherwise an error code.
+ */
+static int ks_set_wol(struct net_device *dev,
+	struct ethtool_wolinfo *wol)
+{
+	struct ks_net *priv = netdev_priv(dev);
+	struct dev_info *hw_priv = priv->hw_priv;
+
+	/* Need to find a way to retrieve the device IP address. */
+	u8 net_addr[] = { 192, 168, 1, 1 };
+
+	if (wol->wolopts & ~hw_priv->wol_support)
+		return -EINVAL;
+
+	hw_priv->wol_enable = wol->wolopts;
+
+	/* Link wakeup cannot really be disabled. */
+	if (wol->wolopts)
+		hw_priv->wol_enable |= WAKE_PHY;
+	hw_enable_wol(&hw_priv->hw, hw_priv->wol_enable, net_addr);
+	return 0;
+}
+
+static struct {
+	char string[ETH_GSTRING_LEN];
+} ethtool_stats_keys[TOTAL_SWITCH_COUNTER_NUM] = {
+	{ "rx_lo_priority_octets" },
+	{ "rx_hi_priority_octets" },
+	{ "rx_undersize_packets" },
+	{ "rx_fragments" },
+	{ "rx_oversize_packets" },
+	{ "rx_jabbers" },
+	{ "rx_symbol_errors" },
+	{ "rx_crc_errors" },
+	{ "rx_align_errors" },
+	{ "rx_mac_ctrl_packets" },
+	{ "rx_pause_packets" },
+	{ "rx_bcast_packets" },
+	{ "rx_mcast_packets" },
+	{ "rx_ucast_packets" },
+	{ "rx_64_or_less_octet_packets" },
+	{ "rx_65_to_127_octet_packets" },
+	{ "rx_128_to_255_octet_packets" },
+	{ "rx_256_to_511_octet_packets" },
+	{ "rx_512_to_1023_octet_packets" },
+	{ "rx_1024_to_1522_octet_packets" },
+
+	{ "tx_lo_priority_octets" },
+	{ "tx_hi_priority_octets" },
+	{ "tx_late_collisions" },
+	{ "tx_pause_packets" },
+	{ "tx_bcast_packets" },
+	{ "tx_mcast_packets" },
+	{ "tx_ucast_packets" },
+	{ "tx_deferred" },
+	{ "tx_total_collisions" },
+	{ "tx_excessive_collisions" },
+	{ "tx_single_collisions" },
+	{ "tx_mult_collisions" },
+
+	{ "rx_discards" },
+	{ "tx_discards" },
+};
+
+/**
+ * ks_get_strings - get statistics identity strings
+ * @dev:	Network device.
+ * @stringset:	String set identifier.
+ * @buf:	Buffer to store the strings.
+ *
+ * This procedure returns the strings used to identify the statistics.
+ */
+static void ks_get_strings(struct net_device *dev, u32 stringset, u8 *buf)
+{
+	struct dev_priv *priv = netdev_priv(dev);
+	struct sw_priv *sw_priv = priv->sw_priv;
+	struct ksz_sw *sw = &sw_priv->sw;
+
+	if (ETH_SS_STATS == stringset)
+		memcpy(buf, &ethtool_stats_keys,
+			ETH_GSTRING_LEN * sw->mib_cnt);
+}  /* ks_get_strings */
+
+/**
+ * ks_get_sset_count - get statistics size
+ * @dev:	Network device.
+ * @sset:	The statistics set number.
+ *
+ * This function returns the size of the statistics to be reported.
+ *
+ * Return size of the statistics to be reported.
+ */
+static int ks_get_sset_count(struct net_device *dev, int sset)
+{
+	struct dev_priv *priv = netdev_priv(dev);
+	struct sw_priv *sw_priv = priv->sw_priv;
+	struct ksz_sw *sw = &sw_priv->sw;
+
+	switch (sset) {
+	case ETH_SS_STATS:
+		return sw->mib_cnt;
+	default:
+		return -EOPNOTSUPP;
+	}
+}
+
+/**
+ * ks_get_ethtool_stats - get network device statistics
+ * @dev:	Network device.
+ * @stats:	Ethtool statistics data structure.
+ * @data:	Buffer to store the statistics.
+ *
+ * This procedure returns the statistics.
+ */
+static void ks_get_ethtool_stats(struct net_device *dev,
+	struct ethtool_stats *stats, u64 *data)
+{
+	struct dev_priv *priv = netdev_priv(dev);
+	struct sw_priv *hw_priv = priv->sw_priv;
+	struct ksz_sw *sw = &hw_priv->sw;
+	struct ksz_port *port = &priv->port;
+	int n_stats = stats->n_stats;
+	int i;
+	int n;
+	int p;
+	int rc;
+	u64 counter[TOTAL_SWITCH_COUNTER_NUM];
+
+	n = SWITCH_PORT_NUM;
+	for (i = 0, p = port->first_port; i < port->mib_port_cnt; i++, p++) {
+		if (media_connected == sw->port_state[p].state) {
+			hw_priv->counter[p].read = 1;
+
+			/* Remember first port that requests read. */
+			if (n == SWITCH_PORT_NUM)
+				n = p;
+		}
+	}
+
+	if (n < SWITCH_PORT_NUM)
+		schedule_work(&hw_priv->mib_read);
+
+	if (1 == port->mib_port_cnt && n < SWITCH_PORT_NUM) {
+		p = n;
+		rc = wait_event_interruptible_timeout(
+			hw_priv->counter[p].counter,
+			2 == hw_priv->counter[p].read,
+			HZ * 1);
+	} else
+		for (i = 0, p = n; i < port->mib_port_cnt - n; i++, p++) {
+			if (0 == i) {
+				rc = wait_event_interruptible_timeout(
+					hw_priv->counter[p].counter,
+					2 == hw_priv->counter[p].read,
+					HZ * 2);
+			} else if (sw->port_mib[p].cnt_ptr) {
+				rc = wait_event_interruptible_timeout(
+					hw_priv->counter[p].counter,
+					2 == hw_priv->counter[p].read,
+					HZ * 1);
+			}
+		}
+
+	sw->ops->get_mib_counters(sw, port->first_port, port->mib_port_cnt,
+		counter);
+	n = sw->mib_cnt;
+	if (n > n_stats)
+		n = n_stats;
+	n_stats -= n;
+	for (i = 0; i < n; i++)
+		*data++ = counter[i];
+}  /* ks_get_ethtool_stats */
+
+#define KS_EEPROM_MAGIC (0x00008462)
+
+static int ks_set_eeprom(struct net_device *dev,
+	struct ethtool_eeprom *ee, u8 *data)
+{
+	struct dev_priv *priv = netdev_priv(dev);
+	struct dev_info *ks = priv->hw_priv;
+	int offset = ee->offset;
+	int len = ee->len;
+	u16 tmp;
+
+	/* currently only support byte writing */
+	if (len != 1)
+		return -EINVAL;
+
+	if (ee->magic != KS_EEPROM_MAGIC)
+		return -EINVAL;
+
+	if (ks_eeprom_claim(ks))
+		return -ENOENT;
+
+	eeprom_93cx6_wren(&ks->eeprom, true);
+
+	/* ethtool currently only supports writing bytes, which means
+	 * we have to read/modify/write our 16bit EEPROMs */
+
+	eeprom_93cx6_read(&ks->eeprom, offset/2, &tmp);
+
+	if (offset & 1) {
+		tmp &= 0xff;
+		tmp |= *data << 8;
+	} else {
+		tmp &= 0xff00;
+		tmp |= *data;
+	}
+
+	eeprom_93cx6_write(&ks->eeprom, offset/2, tmp);
+	eeprom_93cx6_wren(&ks->eeprom, false);
+
+	ks_eeprom_release(ks);
+
+	return 0;
+}
+
+static int ks_get_eeprom(struct net_device *dev,
+	struct ethtool_eeprom *ee, u8 *data)
+{
+	struct dev_priv *priv = netdev_priv(dev);
+	struct dev_info *ks = priv->hw_priv;
+	int offset = ee->offset;
+	int len = ee->len;
+
+	/* must be 2 byte aligned */
+	if (len & 1 || offset & 1)
+		return -EINVAL;
+
+	if (ks_eeprom_claim(ks))
+		return -ENOENT;
+
+	ee->magic = KS_EEPROM_MAGIC;
+
+	eeprom_93cx6_multiread(&ks->eeprom, offset/2, (__le16 *)data, len/2);
+	ks_eeprom_release(ks);
+
+	return 0;
+}
+
+static int ks_get_eeprom_len(struct net_device *dev)
+{
+	struct dev_priv *priv = netdev_priv(dev);
+	struct dev_info *hw_priv = priv->hw_priv;
+	struct ksz_hw *hw = &hw_priv->hw;
+
+	/* currently, we assume it is an 93C46 attached, so return 128 */
+	return hw->rc_ccr & CCR_EEPROM ? 128 : 0;
+}
+
+#ifdef CONFIG_1588_PTP
+static int netdev_get_ts_info(struct net_device *dev,
+	struct ethtool_ts_info *info)
+{
+	struct dev_priv *priv = netdev_priv(dev);
+	struct sw_priv *sw_priv = priv->sw_priv;
+	struct ksz_sw *sw = &sw_priv->sw;
+	struct ptp_info *ptp = &sw->ptp_hw;
+
+	return ptp->ops->get_ts_info(ptp, dev, info);
+}  /* netdev_get_ts_info */
+#endif
+
+static const struct ethtool_ops ks_ethtool_ops = {
+	.get_drvinfo		= ks_get_drvinfo,
+	.get_regs_len		= ks_get_regs_len,
+	.get_regs		= ks_get_regs,
+	.get_wol		= ks_get_wol,
+	.set_wol		= ks_set_wol,
+	.get_msglevel		= ks_get_msglevel,
+	.set_msglevel		= ks_set_msglevel,
+#if 0
+	.get_settings		= ks_get_settings,
+	.set_settings		= ks_set_settings,
+#endif
+	.get_link		= ks_get_link,
+	.nway_reset		= ks_nway_reset,
+	.get_strings		= ks_get_strings,
+	.get_sset_count		= ks_get_sset_count,
+	.get_ethtool_stats	= ks_get_ethtool_stats,
+	.get_eeprom_len		= ks_get_eeprom_len,
+	.get_eeprom		= ks_get_eeprom,
+	.set_eeprom		= ks_set_eeprom,
+#ifdef CONFIG_1588_PTP
+	.get_ts_info		= netdev_get_ts_info,
+#endif
+};
+
+/**
+ * ks_read_selftest - read the selftest memory info.
+ * @ks: The device state
+ *
+ * Read and check the TX/RX memory selftest information.
+ */
+static int ks_read_selftest(struct ksz_hw *ks)
+{
+	unsigned both_done = MBIR_TXMBF | MBIR_RXMBF;
+	int ret = 0;
+	int timeout = 10;
+	unsigned rd;
+
+	ks_wrreg16(ks, KS_GRR, GRR_BIST);
+	do {
+		rd = ks_rdreg16(ks, KS_MBIR);
+		mdelay(1);
+		--timeout;
+	} while (!(rd & MBIR_DONE) && timeout);
+	if ((rd & both_done) != both_done) {
+		pr_warn("Memory selftest not finished\n");
+		goto selftest_done;
+	}
+
+	if (rd & MBIR_TXMBFA) {
+		pr_err("TX memory selftest fail: %u\n",
+			(rd >> MBIR_TX_SHIFT) & MBIR_FAIL_CNT);
+		ret |= 1;
+	}
+
+	if (rd & MBIR_RXMBFA) {
+		pr_err("RX memory selftest fail: %u\n",
+			(rd >> MBIR_RX_SHIFT) & MBIR_FAIL_CNT);
+		ret |= 2;
+	}
+
+selftest_done:
+	ks_wrreg16(ks, KS_GRR, 0);
+	return ret;
+}
+
+/*
+ * Linux network device interface functions
+ */
+
+/* Driver exported variables */
+
+static int msg_enable;
+static int napi;
+static int rx_max = 2;
+static int fiber;
+
+static char *macaddr = ":";
+
+static int netdev_init(struct net_device *netdev)
+{
+	struct ks_net *ks = netdev_priv(netdev);
+	struct sw_priv *hw_priv = ks->sw_priv;
+	struct ksz_sw *sw = &hw_priv->sw;
+
+	mutex_init(&ks->lock);
+	spin_lock_init(&ks->statelock);
+
+	netdev->watchdog_timeo = HZ;
+
+	/* setup mii state */
+	ks->mii_if.dev		= netdev;
+	ks->mii_if.phy_id	= ks->phy_addr;
+	ks->mii_if.phy_id_mask	= (1 << SWITCH_PORT_NUM) - 1;
+	ks->mii_if.reg_num_mask	= 0xf;
+	ks->mii_if.mdio_read	= ks_phy_read;
+	ks->mii_if.mdio_write	= ks_phy_write;
+
+	/* set the default message enable */
+	ks->msg_enable = netif_msg_init(msg_enable, (NETIF_MSG_DRV |
+						     NETIF_MSG_PROBE |
+						     NETIF_MSG_LINK));
+	sw->msg_enable = ks->msg_enable;
+	return 0;
+}
+
+static const struct net_device_ops ks_netdev_ops = {
+	.ndo_init		= netdev_init,
+	.ndo_open		= ks_net_open,
+	.ndo_stop		= ks_net_stop,
+	.ndo_get_stats		= ks_net_query_statistics,
+	.ndo_do_ioctl		= ks_net_ioctl,
+	.ndo_start_xmit		= ks_start_xmit,
+	.ndo_tx_timeout		= ks_tx_timeout,
+	.ndo_set_mac_address	= ks_set_mac_address,
+	.ndo_set_rx_mode	= ks_set_rx_mode,
+	.ndo_validate_addr	= eth_validate_addr,
+#ifdef CONFIG_NET_POLL_CONTROLLER
+	.ndo_poll_controller	= netdev_netpoll,
+#endif
+	.ndo_vlan_rx_add_vid	= netdev_add_vid,
+	.ndo_vlan_rx_kill_vid	= netdev_kill_vid,
+};
+
+static void netdev_free(struct net_device *dev)
+{
+	exit_sysfs(dev);
+	if (dev->watchdog_timeo)
+		unregister_netdev(dev);
+
+	free_netdev(dev);
+}
+
+static int net_device_present;
+
+static int ks846x_probe(struct platform_device *pdev)
+{
+	int err = -ENOMEM;
+	int irq;
+	struct resource *io_d, *io_c;
+	struct net_device *netdev;
+	struct ks_net *priv;
+	struct sw_priv *sw_priv;
+	struct dev_info *info;
+	struct ksz_hw *hw;
+	struct ksz_sw *sw;
+	u16 id;
+	int cnt;
+	int i;
+	int dev_count;
+	char dev_name[IFNAMSIZ];
+	int mib_port_count;
+	int port_count;
+
+	io_d = platform_get_resource(pdev, IORESOURCE_MEM, 0);
+	io_c = platform_get_resource(pdev, IORESOURCE_MEM, 1);
+	irq = platform_get_irq(pdev, 0);
+	if (irq < 0)
+		return irq;
+
+	if (!request_mem_region(io_d->start, resource_size(io_d), DRV_NAME))
+		goto err_mem_region_data;
+
+	if (!request_mem_region(io_c->start, resource_size(io_c), DRV_NAME))
+		goto err_mem_region_cmd;
+
+	sw_priv = kzalloc(sizeof(struct sw_priv), GFP_KERNEL);
+	if (!sw_priv)
+		goto err_devinfo;
+
+	info = kzalloc(sizeof(struct dev_info), GFP_KERNEL);
+	if (!info) {
+		kfree(sw_priv);
+		goto err_devinfo;
+	}
+
+	info->parent = sw_priv;
+	hw = &info->hw;
+
+	sw_priv->dev = &pdev->dev;
+	sw_priv->hw_dev = info;
+
+	sw_priv->sw.reg = &sw_reg_ops;
+
+	info->pdev = pdev;
+
+	hw->hw_addr = ioremap(io_d->start, resource_size(io_d));
+	if (!hw->hw_addr)
+		goto err_ioremap_data;
+
+	hw->hw_addr_cmd = ioremap(io_c->start, resource_size(io_c));
+	if (!hw->hw_addr_cmd)
+		goto err_ioremap_cmd;
+
+	ks_read_config(hw);
+
+	err = ksz_probe(sw_priv);
+	if (err < 0) {
+
+		/* These are deleted when probing fails. */
+		info = NULL;
+		sw_priv = NULL;
+		goto err_hw;
+	}
+
+	sw = &sw_priv->sw;
+	info->lock = &sw_priv->lock;
+	info->sw = sw;
+
+	id = ks_rdreg16(hw, KS_CIDER);
+
+#define	MHEADER_SIZE	(sizeof(struct type_frame_head) * MAX_RECV_FRAMES)
+	hw->frame_head_info = (struct type_frame_head *)
+		kmalloc(MHEADER_SIZE, GFP_KERNEL);
+	if (!hw->frame_head_info) {
+		dev_err(&pdev->dev, "Error: Fail to allocate frame memory\n");
+		goto err_hw;
+	}
+
+	ks_read_selftest(hw);
+	ks_soft_reset(hw, GRR_GSR);
+
+	if (macaddr[0] != ':') {
+		int n;
+		unsigned int hex[6];
+		u8 data[6];
+
+		n = sscanf(macaddr, "%x:%x:%x:%x:%x:%x",
+			&hex[0], &hex[1], &hex[2], &hex[3], &hex[4], &hex[5]);
+		if (ETH_ALEN == n) {
+			while (n > 0) {
+				--n;
+				data[n] = (u8) hex[n];
+			}
+			ks_set_mac(hw, data);
+		}
+	}
+
+	ks_hw_init(hw);
+
+	info->eeprom.data = hw;
+	info->eeprom.width = PCI_EEPROM_WIDTH_93C46;
+	info->eeprom.register_read = ks_eeprom_regread;
+	info->eeprom.register_write = ks_eeprom_regwrite;
+
+	dev_info(&pdev->dev,
+		"Found chip, family: 0x%x, id: 0x%x, rev: 0x%x, irq: %d\n",
+		(id >> 8) & 0xff, (id >> 4) & 0xf, (id >> 1) & 0x7, irq);
+
+	dev_count = 1;
+	port_count = 1;
+	mib_port_count = 1;
+	if ((id & CIDER_ID_MASK) == CIDER_ID_8462)
+		cnt = 2;
+	else
+		cnt = 1;
+
+	/* Device has a switch with multiple ports. */
+	if (2 == cnt) {
+		sw->net_ops->setup_special(sw, &port_count, &mib_port_count,
+			&dev_count, NULL);
+	}
+
+	/* tasklet is enabled. */
+	tasklet_init(&info->rx_tasklet, rx_proc_task,
+		(unsigned long) info);
+
+	/* tasklet_enable will decrement the atomic counter. */
+	tasklet_disable(&info->rx_tasklet);
+
+	sw->id = net_device_present;
+
+	info->wol_support = WOL_SUPPORT;
+	info->wol_enable = 0;
+
+	for (i = 0; i < dev_count; i++) {
+		netdev = alloc_etherdev(sizeof(struct ks_net));
+		if (!netdev)
+			goto err_register;
+
+		SET_NETDEV_DEV(netdev, &pdev->dev);
+
+		priv = netdev_priv(netdev);
+		priv->netdev = netdev;
+		priv->dev = &pdev->dev;
+		priv->hw_priv = info;
+		priv->sw_priv = sw_priv;
+		priv->id = net_device_present;
+
+		priv->phy_addr = sw->net_ops->setup_dev(sw, netdev, dev_name,
+			&priv->port, i, port_count, mib_port_count);
+		INIT_DELAYED_WORK(&priv->port.link_update,
+				  ksz8462_link_update_work);
+
+		netdev->mem_start = (unsigned long) hw->hw_addr;
+		netdev->mem_end = netdev->mem_start + 0x20 - 1;
+		netdev->irq = irq;
+		memcpy(netdev->dev_addr, hw->mac_addr, ETH_ALEN);
+
+		netdev->netdev_ops = &ks_netdev_ops;
+		netdev->ethtool_ops = &ks_ethtool_ops;
+
+		err = register_netdev(netdev);
+		if (err)
+			goto err_register;
+		err = init_sysfs(netdev);
+		if (err)
+			goto err_register;
+		net_device_present++;
+
+		/* Save the base device name. */
+		if (!i)
+			strlcpy(dev_name, netdev->name, IFNAMSIZ);
+	}
+
+	netdev = sw->netdev[0];
+	sw_priv->dev = &netdev->dev;
+
+	if (rx_max < 1)
+		rx_max = 1;
+	hw->RX_MAX = rx_max;
+	info->use_napi = napi;
+	netif_napi_add(netdev, &info->napi, rx_napi_proc, rx_max);
+
+	INIT_WORK(&info->rxctrl_work, ks_rxctrl_work);
+	INIT_WORK(&info->tx_reset, ks_tx_reset);
+
+#ifdef CONFIG_ARCH_MICREL_PEGASUS
+#ifdef CONFIG_1588_PTP
+	if (sw->features & PTP_HW) {
+		struct ptp_info *ptp = &sw->ptp_hw;
+
+		ptp->get_clk_cnt = get_clk_cnt;
+		ptp->clk_divider = ksz_system_bus_clock;
+	}
+#endif
+#endif
+
+	if (!sw_is_switch(sw)) {
+		sw->ops->acquire(sw);
+		sw_disable(sw);
+		sw->ops->release(sw);
+	}
+
+	ksz_probe_last(sw_priv);
+
+#ifdef CONFIG_1588_PTP_
+	if (sw->features & PTP_HW) {
+		struct ptp_info *ptp = &sw->ptp_hw;
+
+		ptp->ops->init(ptp, hw->mac_addr);
+		ptp->ports = cnt;
+		if (ptp->version < 1)
+			sw->features &= ~VLAN_PORT_REMOVE_TAG;
+	}
+#endif
+
+	/* Nothing to do for monitor. */
+	ksz_stop_timer(&sw_priv->monitor_timer_info);
+
+	platform_set_drvdata(pdev, sw_priv);
+	return 0;
+
+err_register:
+	for (i = 0; i < sw->dev_count; i++) {
+		if (sw->netdev[i]) {
+			netdev_free(sw->netdev[i]);
+			sw->netdev[i] = NULL;
+		}
+	}
+	kfree(sw->info);
+
+	kfree(hw->frame_head_info);
+
+err_hw:
+	iounmap(hw->hw_addr_cmd);
+
+err_ioremap_cmd:
+	iounmap(hw->hw_addr);
+
+err_ioremap_data:
+	if (info)
+		kfree(info);
+	if (sw_priv)
+		kfree(sw_priv);
+
+err_devinfo:
+	release_mem_region(io_c->start, resource_size(io_c));
+
+err_mem_region_cmd:
+	release_mem_region(io_d->start, resource_size(io_d));
+
+err_mem_region_data:
+	return err;
+}
+
+static int ks846x_remove(struct platform_device *pdev)
+{
+	int i;
+	struct resource *iomem;
+	struct sw_priv *sw_priv = platform_get_drvdata(pdev);
+	struct dev_info *hw_priv = sw_priv->hw_dev;
+	struct ksz_hw *hw = &hw_priv->hw;
+	struct ksz_sw *sw = &sw_priv->sw;
+
+	ksz_remove_first(sw_priv);
+
+	iomem = platform_get_resource(pdev, IORESOURCE_MEM, 0);
+	release_mem_region(iomem->start, resource_size(iomem));
+	iomem = platform_get_resource(pdev, IORESOURCE_MEM, 1);
+	release_mem_region(iomem->start, resource_size(iomem));
+
+	for (i = 0; i < sw->dev_count + sw->dev_offset; i++) {
+		if (sw->netdev[i])
+			netdev_free(sw->netdev[i]);
+	}
+	if (hw->hw_addr_cmd)
+		iounmap(hw->hw_addr_cmd);
+	if (hw->hw_addr)
+		iounmap(hw->hw_addr);
+	kfree(hw->frame_head_info);
+
+	/* Revert to original device. */
+	sw_priv->dev = &pdev->dev;
+
+	platform_set_drvdata(pdev, NULL);
+
+	ksz_remove(sw_priv);
+
+	return 0;
+}
+
+static int netdev_resume(struct platform_device *pdev)
+{
+	int i;
+	struct net_device *dev;
+	struct sw_priv *sw_priv = platform_get_drvdata(pdev);
+	struct ksz_sw *sw = &sw_priv->sw;
+
+	for (i = 0; i < sw->dev_count + sw->dev_offset; i++) {
+		dev = sw->netdev[i];
+		if (dev && netif_running(dev)) {
+			ks_net_open(dev);
+			netif_device_attach(dev);
+		}
+	}
+	return 0;
+}  /* netdev_resume */
+
+static int netdev_suspend(struct platform_device *pdev, pm_message_t state)
+{
+	int i;
+	struct net_device *dev;
+	struct sw_priv *sw_priv = platform_get_drvdata(pdev);
+	struct ksz_sw *sw = &sw_priv->sw;
+
+	for (i = 0; i < sw->dev_count + sw->dev_offset; i++) {
+		dev = sw->netdev[i];
+		if (dev && netif_running(dev)) {
+			netif_device_detach(dev);
+			ks_net_stop(dev);
+		}
+	}
+	return 0;
+}  /* netdev_suspend */
+
+static void netdev_shutdown(struct platform_device *pdev)
+{
+	int i;
+	struct net_device *dev;
+	struct sw_priv *sw_priv = platform_get_drvdata(pdev);
+	struct ksz_sw *sw = &sw_priv->sw;
+
+	for (i = 0; i < sw->dev_count + sw->dev_offset; i++) {
+		dev = sw->netdev[i];
+		if (dev && netif_running(dev)) {
+			netif_device_detach(dev);
+			ks_net_stop(dev);
+		}
+	}
+}  /* netdev_shutdown */
+
+static struct platform_driver ks846x_platform_driver = {
+	.driver = {
+		.name = DRV_NAME,
+		.owner = THIS_MODULE,
+	},
+	.probe = ks846x_probe,
+	.remove = ks846x_remove,
+	.suspend = netdev_suspend,
+	.resume = netdev_resume,
+	.shutdown = netdev_shutdown,
+};
+
+static int __init ksz8462_hli_init(void)
+{
+#ifdef DEBUG_MSG
+	if (init_dbg())
+		return -ENOMEM;
+#endif
+	return platform_driver_register(&ks846x_platform_driver);
+}
+
+static void __exit ksz8462_hli_exit(void)
+{
+	platform_driver_unregister(&ks846x_platform_driver);
+#ifdef DEBUG_MSG
+	exit_dbg();
+#endif
+}
+
+module_init(ksz8462_hli_init);
+module_exit(ksz8462_hli_exit);
+
+MODULE_DESCRIPTION("Microchip KSZ8462 HLI Network Driver");
+MODULE_AUTHOR("Tristram Ha <tristram.ha@microchip.com>");
+MODULE_LICENSE("GPL");
+
+module_param_named(message, msg_enable, int, 0);
+MODULE_PARM_DESC(message, "Message verbosity level (0=none, 31=all)");
+
+module_param(napi, int, 0);
+MODULE_PARM_DESC(napi, "Use NAPI");
+module_param(rx_max, int, 0);
+MODULE_PARM_DESC(rx_max, "Specify maximum non-TCP receive packets");
+
+module_param(macaddr, charp, 0);
+MODULE_PARM_DESC(macaddr, "MAC address");
diff -Napur linux-linux4microchip-2022.10-orig/drivers/net/ethernet/micrel/ksz8463.h linux-linux4microchip-2022.10/drivers/net/ethernet/micrel/ksz8463.h
--- linux-linux4microchip-2022.10-orig/drivers/net/ethernet/micrel/ksz8463.h	1969-12-31 16:00:00.000000000 -0800
+++ linux-linux4microchip-2022.10/drivers/net/ethernet/micrel/ksz8463.h	2023-04-25 16:13:54.868163472 -0700
@@ -0,0 +1,104 @@
+/**
+ * Microchip KSZ8463 definition file
+ *
+ * Copyright (c) 2015-2016 Microchp Technology Inc.
+ *	Tristram Ha <Tristram.Ha@microchip.com>
+ *
+ * Copyright (c) 2012-2013 Micrel, Inc.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ */
+
+
+/* -------------------------------------------------------------------------- */
+
+#ifndef __KSZ8463_H
+#define __KSZ8463_H
+
+
+/* Default values are used in ksz_sw.h if these are not defined. */
+#define PRIO_QUEUES			4
+
+#define KS_PRIO_IN_REG			8
+
+#define TOTAL_PORT_NUM			3
+
+#define KSZ8463_COUNTER_NUM		0x20
+#define TOTAL_KSZ8463_COUNTER_NUM	(KSZ8463_COUNTER_NUM + 2)
+
+#define SWITCH_COUNTER_NUM		KSZ8463_COUNTER_NUM
+#define TOTAL_SWITCH_COUNTER_NUM	TOTAL_KSZ8463_COUNTER_NUM
+
+/* Required for common switch control in ksz_sw.c */
+#define SW_D				u16
+#define SW_R(sw, addr)			(sw)->reg->r16(sw, addr)
+#define SW_W(sw, addr, val)		(sw)->reg->w16(sw, addr, val)
+#define SW_SIZE				(2)
+#define SW_SIZE_STR			"%04x"
+#define port_r				port_r16
+#define port_w				port_w16
+
+#define P_BCAST_STORM_CTRL		REG_PORT_CTRL1
+#define P_PRIO_CTRL			REG_PORT_CTRL1
+#define P_TAG_CTRL			REG_PORT_CTRL1
+#define P_MIRROR_CTRL			REG_PORT_CTRL2
+#define P_STP_CTRL			REG_PORT_CTRL2
+#define P_PHY_CTRL			REG_PORT_CTRL4
+#define P_FORCE_CTRL			REG_PORT_CTRL4
+#define P_NEG_RESTART_CTRL		REG_PORT_CTRL4
+#define P_LINK_STATUS			REG_PORT_STATUS
+#define P_SPEED_STATUS			REG_PORT_STATUS
+#define P_RATE_LIMIT_CTRL		REG_PORT_CTRL3
+#define P_SA_MAC_CTRL			REG_PORT_CTRL1
+#define P_2_QUEUE_CTRL			REG_PORT_CTRL1
+#define P_4_QUEUE_CTRL			REG_PORT_CTRL1
+
+#define S_LINK_AGING_CTRL		REG_SWITCH_CTRL_1
+#define S_MIRROR_CTRL			REG_SWITCH_CTRL_2
+#define S_REPLACE_VID_CTRL		REG_SWITCH_CTRL_3
+#define S_802_1P_PRIO_CTRL		REG_SWITCH_CTRL_6
+#define S_UNKNOWN_DA_CTRL		REG_SWITCH_CTRL_7
+#define S_TOS_PRIO_CTRL			REG_TOS_PRIORITY_CTRL_1
+#define S_FLUSH_TABLE_CTRL		REG_SWITCH_CTRL_8
+#define S_TAIL_TAG_CTRL			REG_SWITCH_CTRL_8
+#define S_FORWARD_INVALID_VID_CTRL	REG_SWITCH_CTRL_9
+#define S_INS_SRC_PVID_CTRL		REG_SWITCH_CTRL_9
+
+#define IND_ACC_TABLE(table)		((table))
+
+#define REG_IND_CTRL_0			REG_IND_IACR
+#define REG_IND_DATA_CHECK		REG_IND_DATA_1
+#define REG_IND_DATA_HI			REG_IND_DATA_2
+#define REG_IND_DATA_LO			REG_IND_DATA_4
+
+#define REG_SWITCH_RESET		REG_RESET_CTRL
+
+#define PORT_DROP_TAG			PORT_DROP_TAG_PKT
+
+#define REG_PORT_0_MAC_ADDR_0		REG_SA_FILTER_MAC1_H
+#define REG_PORT_1_MAC_ADDR_0		REG_SA_FILTER_MAC2_H
+
+#define REG_PORT_IN_RATE_0		REG_PORT_IN_RATE0
+#define REG_PORT_OUT_RATE_0		REG_PORT_OUT_RATE0
+#define REG_PORT_1_IN_RATE_0		REG_PORT_IN_RATE0
+#define REG_PORT_1_OUT_RATE_0		REG_PORT_OUT_RATE0
+
+#define SWITCH_FLUSH_DYN_MAC_TABLE	SWITCH_FLUSH_DYNA_MAC
+#define SWITCH_INS_TAG_1_PORT_2		SWITCH_INS_TAG_0_1
+#define SWITCH_INS_TAG_1_PORT_3		SWITCH_INS_TAG_0_2
+#define SWITCH_INS_TAG_2_PORT_1		SWITCH_INS_TAG_1_0
+#define SWITCH_INS_TAG_2_PORT_3		SWITCH_INS_TAG_1_2
+#define SWITCH_INS_TAG_3_PORT_1		SWITCH_INS_TAG_2_0
+#define SWITCH_INS_TAG_3_PORT_2		SWITCH_INS_TAG_2_1
+
+#define FORWARD_INVALID_PORT_SHIFT	8
+
+
+#endif
diff -Napur linux-linux4microchip-2022.10-orig/drivers/net/ethernet/micrel/ksz846x.h linux-linux4microchip-2022.10/drivers/net/ethernet/micrel/ksz846x.h
--- linux-linux4microchip-2022.10-orig/drivers/net/ethernet/micrel/ksz846x.h	1969-12-31 16:00:00.000000000 -0800
+++ linux-linux4microchip-2022.10/drivers/net/ethernet/micrel/ksz846x.h	2023-04-25 16:13:54.868163472 -0700
@@ -0,0 +1,366 @@
+/**
+ * Microchip KSZ8462 Register Definitions.
+ *
+ * Copyright (C) 2015-2021 Microchip Technology Inc.
+ * Copyright (C) 2010-2015 Micrel, Inc.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation; either version 2 of the License, or
+ * (at your option) any later version.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ *
+  ----------------------------------------------------------------------------
+
+  Author  Date      Version  Description
+  PCD     03/23/10  0.0.1    Register definition for KS8461/2.
+  ----------------------------------------------------------------------------
+ */
+
+#ifndef __KSZ846X_H
+#define __KSZ846X_H
+
+#include "ksz886x.h"
+
+/*
+ * Trigger Output Registers
+ * (Bank 1, Offset 0x200 - 0x398)
+ */
+
+#define MAX_GPIO                    12   /* GPIO from 0 ~ 11 */
+#define MAX_TRIG_UNIT               12   /* Trigger unit from 0 ~ 11 */
+#define MAX_TIMESTAMP_UNIT          12   /* Timestamp unit from 0 ~ 11 */
+#define MAX_TIMESTAMP_EVENT_UNIT    8    /* Timestamp event unit from 0 ~ 7 */
+
+
+#define TRIG_ERROR                 0x200       /* TRIG_ERROR */
+#define TRIG_ACTIVE                0x202       /* TRIG_ACTIVE */
+#define TRIG_DONE                  0x204       /* TRIG_DONE */
+#define TRIG_EN                    0x206       /* TRIG_EN */
+#define   TRIG_UNIT_11               (1 << 11)
+#define   TRIG_UNIT_10               (1 << 10)
+#define   TRIG_UNIT_9                (1 << 9)
+#define   TRIG_UNIT_8                (1 << 8)
+#define   TRIG_UNIT_7                (1 << 7)
+#define   TRIG_UNIT_6                (1 << 6)
+#define   TRIG_UNIT_5                (1 << 5)
+#define   TRIG_UNIT_4                (1 << 4)
+#define   TRIG_UNIT_3                (1 << 3)
+#define   TRIG_UNIT_2                (1 << 2)
+#define   TRIG_UNIT_1                (1 << 1)
+#define   TRIG_UNIT_0                (1 << 0)
+
+#define TRIG_RESET                 0x208       /* TRIG_SR */
+
+#define TRIG_PPS_WS                0x20A       /* TRIG_PPS_WS */
+/* +/- 1ns offset from the internal clock transition */
+#define   TRIG_ADJUST_1NS            (1 << 11)
+/* Trig PULSE_WIDTH multiple mask (only apply to last Trig Unit */
+#define   TRIG_PPS_WS_MASK           (0x00FF)
+
+
+/* TRAG0 */
+#define TRIG0_TARGET_NANOSEC_L     0x220       /* TRIG_TARGET_NS */
+#define TRIG0_TARGET_NANOSEC_H     0x222       /* TRIG_TARGET_NS */
+#define TRIG0_TARGET_SEC_L         0x224       /* TRIG_TARGET_S */
+#define TRIG0_TARGET_SEC_H         0x226       /* TRIG_TARGET_S */
+#define TRIG0_CONF_1               0x228       /* TRIG_CFG_1 */
+/* Enable Trig out cascade mode */
+#define   TRIG_CASCADE_EN            (1 << 15)
+/* Last unit of Trig out cascade mode */
+#define   TRIG_CASCADE_TAIL          (1 << 14)
+/* Trig immediate */
+#define   TRIG_NOW                   (1 << 9)
+/* Enable reporting on TRIG_DONE */
+#define   TRIG_NOTIFY                (1 << 8)
+/* Select +/- 1ns offset from clock */
+#define   TRIG_CLK_OPT               (1 << 7)
+/* Cascade unit link to previous unit */
+#define   TRIG_CASCADE_UPS_MASK      (0x3c00)
+
+/* negative edge trigger pattern */
+#define   TRIG_NEG_EDGE              0
+/* positive edge trigger pattern */
+#define   TRIG_POS_EDGE              1
+/* negative pulse trigger pattern */
+#define   TRIG_NEG_PULSE             2
+/* positive pulse trigger pattern */
+#define   TRIG_POS_PULSE             3
+/* negative period trigger pattern */
+#define   TRIG_NEG_PERIOD            4
+/* positive period trigger pattern */
+#define   TRIG_POS_PERIOD            5
+/* register value output trigger pattern */
+#define   TRIG_REG_OUTPUT            6
+
+
+#define TRIG0_PULSE_WIDTH          0x22A       /* TRIG_CFG_2 */
+#define TRIG0_CYCLE_WIDTH_L        0x22C       /* TRIG_CFG_3 */
+#define TRIG0_CYCLE_WIDTH_H        0x22E       /* TRIG_CFG_4 */
+#define TRIG0_PER_OCCUR            0x230       /* TRIG_CFG_5 */
+#define TRIG0_BIT_PATTERN          0x232       /* TRIG_CFG_6 */
+#define TRIG0_ITERATE_TIME_L       0x234       /* TRIG_CFG_7 */
+#define TRIG0_ITERATE_TIME_H       0x236       /* TRIG_CFG_8 */
+
+/* TRAG0(N=0) to TRIG11(N=11) */
+#define TRIGn_TARGET_NANOSEC_L(N)  (TRIG0_TARGET_NANOSEC_L + (N * 0x20))
+#define TRIGn_TARGET_NANOSEC_H(N)  (TRIG0_TARGET_NANOSEC_H + (N * 0x20))
+#define TRIGn_TARGET_SEC_L(N)      (TRIG0_TARGET_SEC_L + (N * 0x20))
+#define TRIGn_TARGET_SEC_H(N)      (TRIG0_TARGET_SEC_H + (N * 0x20))
+#define TRIGn_CONF_1(N)            (TRIG0_CONF_1 + (N * 0x20))
+#define TRIGn_PULSE_WIDTH(N)       (TRIG0_PULSE_WIDTH + (N * 0x20))
+#define TRIGn_CYCLE_WIDTH_L(N)     (TRIG0_CYCLE_WIDTH_L + (N * 0x20))
+#define TRIGn_CYCLE_WIDTH_H(N)     (TRIG0_CYCLE_WIDTH_H + (N * 0x20))
+#define TRIGn_PER_OCCUR(N)         (TRIG0_PER_OCCUR + (N * 0x20))
+#define TRIGn_BIT_PATTERN(N)       (TRIG0_BIT_PATTERN + (N * 0x20))
+#define TRIGn_ITERATE_TIME_L(N)    (TRIG0_ITERATE_TIME_L + (N * 0x20))
+#define TRIGn_ITERATE_TIME_H(N)    (TRIG0_ITERATE_TIME_H + (N * 0x20))
+
+
+/*
+ * Time Stamping Event Registers
+ * (Bank 2, Offset 0x400 - 0x5FA)
+ */
+
+#define TS_STATUS                  0x400       /* TS_RDY */
+/* Port2 egress timestamp for xDelay available */
+#define   TS_PORT2_OUT_XDELAY        (1 << 15)
+/* Port2 egress timestamp for SYNC available */
+#define   TS_PORT2_OUT_SYNC          (1 << 14)
+/* Port1 egress timestamp for xDelay available */
+#define   TS_PORT1_OUT_XDELAY        (1 << 13)
+/* Port1 egress timestamp for SYNC available */
+#define   TS_PORT1_OUT_SYNC          (1 << 12)
+/* Timestamp is ready for read mask */
+#define   TS_RDY_MASK                (0x0FFF)
+
+#define TS_ENABLE                  0x402       /* TS_EN */
+#define   TS_UNIT_11                 (1 << 11)
+#define   TS_UNIT_10                 (1 << 10)
+#define   TS_UNIT_9                  (1 << 9)
+#define   TS_UNIT_8                  (1 << 8)
+#define   TS_UNIT_7                  (1 << 7)
+#define   TS_UNIT_6                  (1 << 6)
+#define   TS_UNIT_5                  (1 << 5)
+#define   TS_UNIT_4                  (1 << 4)
+#define   TS_UNIT_3                  (1 << 3)
+#define   TS_UNIT_2                  (1 << 2)
+#define   TS_UNIT_1                  (1 << 1)
+#define   TS_UNIT_0                  (1 << 0)
+
+#define TS_RESET                   0x404       /* TS_SR */
+
+/* TS0 */
+#define TS0_EVENT_STATUS           0x420       /* TS_STS */
+/* TS detected events overflow */
+#define   TS_EVENT_OVERFLOW          (1 << 0)
+/* TS has detected number of events */
+#define   TS_NO_EVENT_DET_MASK       (0x001E)
+
+#define TS0_CONF                   0x422       /* TS_CFG */
+/* TS on whose GPIO mask */
+#define   TS_GPIO_MASK               (0x0F00)
+/* Enable TS raising edge detection */
+#define   TS_DETECT_RISE             (1 << 7)
+/* Enable TS falling edge detection */
+#define   TS_DETECT_FALL             (1 << 6)
+/* Last unit of TS cascade mode */
+#define   TS_CASCADE_TAIL            (1 << 5)
+/* Enable TS cascade mode */
+#define   TS_CASCADE_EN              (1 << 0)
+/* Cascade unit link to previous unit */
+#define   TS_CASCADE_UPS_MASK        (0x001E)
+
+/* TS0-0 Event Timestamp */
+#define TS0_0_EVENT_NANOSEC_L      0x424       /* TS_SMPL_NS */
+#define TS0_0_EVENT_NANOSEC_H      0x426       /* TS_SMPL_NS */
+/* Cascade unit link to previous unit */
+#define   TS_DETECT_FALLING          (1 << 30)
+/* 30bits of nanoseconds */
+#define   TS_ENEVT_NANOSEC_MASK      (0x3FFFFFFF)
+
+#define TS0_0_EVENT_SEC_L          0x428       /* TS_SMPL_S */
+#define TS0_0_EVENT_SEC_H          0x42A       /* TS_SMPL_S */
+#define TS0_0_EVENT_SUB_NANOSEC    0x42C       /* TS_SUB    */
+/* 3bits of sub nanoseconds (0 ~ 4) * 8ns */
+#define   TS_ENEVT_SUB_NANOSEC_MASK  (0x0007)
+
+/* TS0-1 Event Timestamp */
+#define TS0_1_EVENT_NANOSEC_L      0x434
+#define TS0_1_EVENT_NANOSEC_H      0x436
+#define TS0_1_EVENT_SEC_L          0x438
+#define TS0_1_EVENT_SEC_H          0x43A
+#define TS0_1_EVENT_SUB_NANOSEC    0x43C
+
+/* TS0(N=0) to TS11(N=11) */
+#define TSn_EVENT_STATUS(N)        (TS0_EVENT_STATUS + (N * 0x20))
+#define TSn_CONF(N)                (TS0_CONF + (N * 0x20))
+
+/* TS0-0(N=0) to TS11-0(N=11) Event Timestamp */
+#define TSn_0_EVENT_NANOSEC_L(N)   (TS0_0_EVENT_NANOSEC_L + (N * 0x20))
+#define TSn_0_EVENT_NANOSEC_H(N)   (TS0_0_EVENT_NANOSEC_H + (N * 0x20))
+#define TSn_0_EVENT_SEC_L(N)       (TS0_0_EVENT_SEC_L + (N * 0x20))
+#define TSn_0_EVENT_SEC_H(N)       (TS0_0_EVENT_SEC_H + (N * 0x20))
+#define TSn_0_EVENT_SUB_NANOSEC(N) (TS0_0_EVENT_SUB_NANOSEC + (N * 0x20))
+
+
+/*
+ * PTP Clock Registers
+ * (Bank 3, Offset 0x600 - 0x690)
+ */
+
+#define PTP_CLK_CTRL               0x600       /* PTP_CLK_CTL */
+/* Enable adjust RTC from PTP delta clock */
+#define   PTP_STEP_TIME              (1 << 6)
+/* 1 add PTP delta clock to RTC; 0: subtract */
+#define   PTP_STEP_DIR               (1 << 5)
+/* Read from RTC */
+#define   PTP_READ_RTC               (1 << 4)
+/* Load RTC to STC */
+#define   PTP_LOAD_TIME              (1 << 3)
+/* Enable continuous RTC adjustment */
+#define   PTP_CLK_ADJ_ENABLE         (1 << 2)
+/* Enable PTP clock */
+#define   PTP_CLK_ENABLE             (1 << 1)
+/* Reset PTP clock */
+#define   PTP_CLK_RESET              (1 << 0)
+
+#define PTP_CLK_CONF               0x602       /* CLK_CFG */
+
+#define PTP_RTC_NANOSEC_L          0x604       /* PTP_RTC_NSL */
+#define PTP_RTC_NANOSEC_H          0x606       /* PTP_RTC_NSH */
+
+#define PTP_RTC_SEC_L              0x608       /* PTP_RTC_SL */
+#define PTP_RTC_SEC_H              0x60A       /* PTP_RTC_SH */
+
+#define PTP_RTC_SUB_NANOSEC        0x60C       /* PTP_RTC_PHASE */
+/* 3bits of sub nanoseconds (0 ~ 4) * 8ns */
+#define   PTP_RTC_SUB_NANOSEC_MASK   0x0007
+
+#define PTP_SUBNANOSEC_RATE_L      0x610       /* PTP_SNC_RATEL */
+#define PTP_SUBNANOSEC_RATE_H      0x612       /* PTP_SNC_RATEH */
+/* 1: Adjust PTP clock by add 4ns; 0:subtract */
+#define   PTP_RATE_DIR               (1 << 15)
+/* enable temporary adjustment PTP clock */
+#define   PTP_TMP_RATE_ENABLE        (1 << 14)
+
+
+#define PTP_RATE_DURATION_L        0x614       /* PTP_DURL */
+#define PTP_RATE_DURATION_H        0x616       /* PTP_DURH */
+
+#define PTP_MSG_CONF1              0x620       /* PTP_MSG_CONF_1 */
+/* all the PTP pkts are forward to Port3 */
+#define   PTP_FORWARD_TO_PORT3       (1 << 7)
+/* Enable PTP function */
+#define   PTP_ENABLE                 (1 << 6)
+/* Enable detection of Ethernet PTP msg */
+#define   PTP_ETH_ENABLE             (1 << 5)
+/* Enable detection of ipv4/UDP PTP msg */
+#define   PTP_IPV4_UDP_ENABLE        (1 << 4)
+/* Enable detection of ipv6/UDP PTP msg */
+#define   PTP_IPV6_UDP_ENABLE        (1 << 3)
+/* 1: P2P TC; 0: E2E TC */
+#define   PTP_TC_P2P                 (1 << 2)
+/* 1: master OC; 0: slave OC */
+#define   PTP_MASTER                 (1 << 1)
+/* 1-step synchronizarion */
+#define   PTP_1STEP                  (1 << 0)
+
+#define PTP_MSG_CONF2              0x622       /* PTP_MSG_CONF_2 */
+#define   PTP_UNICAST_ENABLE         (1 << 12)
+#define   PTP_ALTERNATE_MASTER       (1 << 11)
+#define   PTP_ALL_HIGH_PRIORITY      (1 << 10)
+/* Enable PTP SYNC assciation check */
+#define   PTP_SYNC_CHECK             (1 << 9)
+/* Enable PTP delay_req/delay_resp assciation check */
+#define   PTP_DELAY_CHECK            (1 << 8)
+/* Enable PTP Pdelay_req/Pdelay_resp assciation check */
+#define   PTP_PDELAY_CHECK           (1 << 7)
+/* drop PTP SYNC/delay_req before BMC is selectedi */
+#define   PTP_DROP_SYNC_DELAY_REQ    (1 << 5)
+/* Enavle checing PTP domain field before forward to port3 */
+#define   PTP_DOMAIN_CHECK           (1 << 4)
+/* Enable re-calculate PTP UDP checksum; 0: UDP checksum field is 0 */
+#define   PTP_UDP_CHECKSUM           (1 << 2)
+/* port1 is connected to master port */
+#define   PTP_PORT1_MASTER           (1 << 1)
+/* port2 is connected to master port */
+#define   PTP_PORT2_MASTER           (1 << 0)
+
+#define PTP_DOMAIN_VERSION         0x624       /* PTP_DOMAIN_VERSION */
+/* PTP version number mask */
+#define   PTP_VERSION_MASK           (0xFF00)
+/* PTP domain vlaue mask */
+#define   PTP_DOMAIN_MASK            (0x00FF)
+
+/* PTP_PORT1_RX_LATENCY */
+#define PTP_PORT1_RX_MAC2PHY_DELAY     0x640
+/* PTP_PORT1_TX_LATENCY */
+#define PTP_PORT1_TX_MAC2PHY_DELAY     0x642
+/* PTP_PORT1_ASYMMETRY_CORRECTION */
+#define PTP_PORT1_ASYM_DELAY           0x644
+/* PTP_PORT1_LINK_DELAY */
+#define PTP_PORT1_LINK_DELAY           0x646
+
+#define PTP_PORT1_XDELAY_TIMESTAMP_L   0x648
+#define PTP_PORT1_XDELAY_TIMESTAMP_H   0x64A
+
+#define PTP_PORT1_SYNC_TIMESTAMP_L     0x64C
+#define PTP_PORT1_SYNC_TIMESTAMP_H     0x64E
+
+#define PTP_PORT1_PDRESP_TIMESTAMP_L   0x650
+#define PTP_PORT1_PDRESP_TIMESTAMP_H   0x652
+
+#define PTP_PORT2_RX_MAC2PHY_DELAY     (PTP_PORT1_RX_MAC2PHY_DELAY + 0x20)
+#define PTP_PORT2_TX_MAC2PHY_DELAY     (PTP_PORT1_TX_MAC2PHY_DELAY + 0x20)
+#define PTP_PORT2_ASYM_DELAY           (PTP_PORT1_ASYM_DELAY + 0x20)
+#define PTP_PORT2_LINK_DELAY           (PTP_PORT1_LINK_DELAY + 0x20)
+
+#define PTP_PORT2_XDELAY_TIMESTAMP_L   (PTP_PORT1_XDELAY_TIMESTAMP_L + 0x20)
+#define PTP_PORT2_XDELAY_TIMESTAMP_H   (PTP_PORT1_XDELAY_TIMESTAMP_H + 0x20)
+
+#define PTP_PORT2_SYNC_TIMESTAMP_L     (PTP_PORT1_SYNC_TIMESTAMP_L + 0x20)
+#define PTP_PORT2_SYNC_TIMESTAMP_H     (PTP_PORT1_SYNC_TIMESTAMP_H + 0x20)
+
+#define PTP_PORT2_PDRESP_TIMESTAMP_L   (PTP_PORT1_PDRESP_TIMESTAMP_L + 0x20)
+#define PTP_PORT2_PDRESP_TIMESTAMP_H   (PTP_PORT1_PDRESP_TIMESTAMP_H + 0x20)
+
+#define PTP_PORT_INTERVAL(port)		((port) * 0x20)
+
+#define GPIO_MONITOR               0x680
+#define GPIO_ENABLE                0x682
+/* 0:enable GPIO0 as trigger output; 1:enable GPIO0 as TS input */
+#define   GPIO0_DISABLE              (1 << 0)
+/* GPIO Output and Monitor mask */
+#define   GPIO_MASK                  (0x007F)
+
+#define TRIG_INT_STATUS            0x688       /* TRIG_INTERRUPT_STS */
+#define TRIG_INT_ENABLE            0x68A       /* TRIG_INTERRUPT_IE */
+
+#define TS_INT_STATUS              0x68C       /* TS_INTERRUPT_STS */
+#define TS_INT_ENABLE              0x68E       /* TS_INTERRUPT_IE */
+/* Enable port2 egress xDelay TS interrupt */
+#define   TS_PORT2_INT_XDELAY        (1 << 15)
+/* Enable port2 egress SYNC TS interrupt */
+#define   TS_PORT2_INT_SYNC          (1 << 14)
+/* Enable port1 egress xDelay TS interrupt */
+#define   TS_PORT1_INT_XDELAY        (1 << 13)
+/* Enable port1 egress SYNC TS interrupt */
+#define   TS_PORT1_INT_SYNC          (1 << 12)
+
+#define REG_DSP_CTRL_6			0x734
+#define COPPER_RECEIVE_ADJUSTMENT	(1 << 13)
+
+#define REG_ANA_CTRL_1			0x748
+#define LDO_OFF				(1 << 7)
+
+#define REG_ANA_CTRL_3			0x74C
+
+#endif
+
+/* END */
diff -Napur linux-linux4microchip-2022.10-orig/drivers/net/ethernet/micrel/ksz8795.h linux-linux4microchip-2022.10/drivers/net/ethernet/micrel/ksz8795.h
--- linux-linux4microchip-2022.10-orig/drivers/net/ethernet/micrel/ksz8795.h	1969-12-31 16:00:00.000000000 -0800
+++ linux-linux4microchip-2022.10/drivers/net/ethernet/micrel/ksz8795.h	2023-04-25 16:13:54.868163472 -0700
@@ -0,0 +1,911 @@
+/**
+ * Microchip KSZ8795 definition file
+ *
+ * Copyright (c) 2015-2017 Microchip Technology Inc.
+ *	Tristram Ha <Tristram.Ha@microchip.com>
+ *
+ * Copyright (c) 2014-2015 Micrel, Inc.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ */
+
+
+/* -------------------------------------------------------------------------- */
+
+#ifndef __KSZ8795_H
+#define __KSZ8795_H
+
+
+#define KS_PORT_M			0x1F
+
+#define KS_PRIO_M			0x3
+#define KS_PRIO_S			2
+
+
+#define REG_CHIP_ID0			0x00
+
+#define FAMILY_ID			0x87
+
+#define REG_CHIP_ID1			0x01
+
+#define SW_CHIP_ID_M			0xF0
+#define SW_CHIP_ID_S			4
+#define SW_REVISION_M			0x0E
+#define SW_REVISION_S			1
+#define SW_START			0x01
+
+#define CHIP_ID_94			0x60
+#define CHIP_ID_95			0x90
+
+#define REG_SW_CTRL_0			0x02
+
+#define SW_NEW_BACKOFF			(1 << 7)
+#define SW_GLOBAL_RESET			(1 << 6)
+#define SW_FLUSH_DYN_MAC_TABLE		(1 << 5)
+#define SW_FLUSH_STA_MAC_TABLE		(1 << 4)
+#define SW_LINK_AUTO_AGING		(1 << 0)
+
+#define REG_SW_CTRL_1			0x03
+
+#define SW_HUGE_PACKET			(1 << 6)
+#define SW_TX_FLOW_CTRL_DISABLE		(1 << 5)
+#define SW_RX_FLOW_CTRL_DISABLE		(1 << 4)
+#define SW_CHECK_LENGTH			(1 << 3)
+#define SW_AGING_ENABLE			(1 << 2)
+#define SW_FAST_AGING			(1 << 1)
+#define SW_AGGR_BACKOFF			(1 << 0)
+
+#define REG_SW_CTRL_2			0x04
+
+#define UNICAST_VLAN_BOUNDARY		(1 << 7)
+#define MULTICAST_STORM_DISABLE		(1 << 6)
+#define SW_BACK_PRESSURE		(1 << 5)
+#define FAIR_FLOW_CTRL			(1 << 4)
+#define NO_EXC_COLLISION_DROP		(1 << 3)
+#define SW_LEGAL_PACKET_DISABLE		(1 << 1)
+
+#define REG_SW_CTRL_3			0x05
+ #define WEIGHTED_FAIR_QUEUE_ENABLE	(1 << 3)
+
+#define SW_VLAN_ENABLE			(1 << 7)
+#define SW_IGMP_SNOOP			(1 << 6)
+#define SW_MIRROR_RX_TX			(1 << 0)
+
+#define REG_SW_CTRL_4			0x06
+
+#define SW_HALF_DUPLEX_FLOW_CTRL	(1 << 7)
+#define SW_HALF_DUPLEX			(1 << 6)
+#define SW_FLOW_CTRL			(1 << 5)
+#define SW_10_MBIT			(1 << 4)
+#define SW_REPLACE_VID			(1 << 3)
+#define BROADCAST_STORM_RATE_HI		0x07
+
+#define REG_SW_CTRL_5			0x07
+
+#define BROADCAST_STORM_RATE_LO		0xFF
+#define BROADCAST_STORM_RATE		0x07FF
+
+#define REG_SW_CTRL_6			0x08
+
+#define SW_MIB_COUNTER_FLUSH		(1 << 7)
+#define SW_MIB_COUNTER_FREEZE		(1 << 6)
+#define SW_MIB_COUNTER_CTRL_ENABLE	KS_PORT_M
+
+#define REG_SW_CTRL_9			0x0B
+
+#define SPI_CLK_125_MHZ			0x80
+#define SPI_CLK_62_5_MHZ		0x40
+#define SPI_CLK_31_25_MHZ		0x00
+
+#define SW_LED_MODE_M			0x3
+#define SW_LED_MODE_S			4
+#define SW_LED_LINK_ACT_SPEED		0
+#define SW_LED_LINK_ACT			1
+#define SW_LED_LINK_ACT_DUPLEX		2
+#define SW_LED_LINK_DUPLEX		3
+
+#define REG_SW_CTRL_10			0x0C
+
+#define SW_TAIL_TAG_ENABLE		(1 << 1)
+#define SW_PASS_PAUSE			(1 << 0)
+
+#define REG_SW_CTRL_11			0x0D
+
+#define REG_POWER_MANAGEMENT_1		0x0E
+
+#define SW_PLL_POWER_DOWN		(1 << 5)
+#define SW_POWER_MANAGEMENT_MODE_M	0x3
+#define SW_POWER_MANAGEMENT_MODE_S	3
+#define SW_POWER_NORMAL			0
+#define SW_ENERGY_DETECTION		1
+#define SW_SOFTWARE_POWER_DOWN		2
+
+#define REG_POWER_MANAGEMENT_2		0x0F
+
+
+#define REG_PORT_1_CTRL_0		0x10
+#define REG_PORT_2_CTRL_0		0x20
+#define REG_PORT_3_CTRL_0		0x30
+#define REG_PORT_4_CTRL_0		0x40
+#define REG_PORT_5_CTRL_0		0x50
+
+#define PORT_BROADCAST_STORM		(1 << 7)
+#define PORT_DIFFSERV_ENABLE		(1 << 6)
+#define PORT_802_1P_ENABLE		(1 << 5)
+#define PORT_BASED_PRIO_S		3
+#define PORT_BASED_PRIO_M		KS_PRIO_M
+#define PORT_BASED_PRIO_0		0
+#define PORT_BASED_PRIO_1		1
+#define PORT_BASED_PRIO_2		2
+#define PORT_BASED_PRIO_3		3
+#define PORT_INSERT_TAG			(1 << 2)
+#define PORT_REMOVE_TAG			(1 << 1)
+#define PORT_QUEUE_SPLIT_L		(1 << 0)
+
+#define REG_PORT_1_CTRL_1		0x11
+#define REG_PORT_2_CTRL_1		0x21
+#define REG_PORT_3_CTRL_1		0x31
+#define REG_PORT_4_CTRL_1		0x41
+#define REG_PORT_5_CTRL_1		0x51
+
+#define PORT_MIRROR_SNIFFER		(1 << 7)
+#define PORT_MIRROR_RX			(1 << 6)
+#define PORT_MIRROR_TX			(1 << 5)
+#define PORT_VLAN_MEMBERSHIP		KS_PORT_M
+
+#define REG_PORT_1_CTRL_2		0x12
+#define REG_PORT_2_CTRL_2		0x22
+#define REG_PORT_3_CTRL_2		0x32
+#define REG_PORT_4_CTRL_2		0x42
+#define REG_PORT_5_CTRL_2		0x52
+
+#define PORT_802_1P_REMAPPING		(1 << 7)
+#define PORT_INGRESS_FILTER		(1 << 6)
+#define PORT_DISCARD_NON_VID		(1 << 5)
+#define PORT_FORCE_FLOW_CTRL		(1 << 4)
+#define PORT_BACK_PRESSURE		(1 << 3)
+#define PORT_TX_ENABLE			(1 << 2)
+#define PORT_RX_ENABLE			(1 << 1)
+#define PORT_LEARN_DISABLE		(1 << 0)
+
+#define REG_PORT_1_CTRL_3		0x13
+#define REG_PORT_2_CTRL_3		0x23
+#define REG_PORT_3_CTRL_3		0x33
+#define REG_PORT_4_CTRL_3		0x43
+#define REG_PORT_5_CTRL_3		0x53
+#define REG_PORT_1_CTRL_4		0x14
+#define REG_PORT_2_CTRL_4		0x24
+#define REG_PORT_3_CTRL_4		0x34
+#define REG_PORT_4_CTRL_4		0x44
+#define REG_PORT_5_CTRL_4		0x54
+
+#define PORT_DEFAULT_VID		0x0001
+
+#define REG_PORT_1_CTRL_5		0x15
+#define REG_PORT_2_CTRL_5		0x25
+#define REG_PORT_3_CTRL_5		0x35
+#define REG_PORT_4_CTRL_5		0x45
+#define REG_PORT_5_CTRL_5		0x55
+
+#define PORT_ACL_ENABLE			(1 << 2)
+#define PORT_AUTHEN_MODE		0x3
+#define PORT_AUTHEN_PASS		0
+#define PORT_AUTHEN_BLOCK		1
+#define PORT_AUTHEN_TRAP		2
+
+#define REG_PORT_5_CTRL_6		0x56
+
+#define PORT_MII_INTERNAL_CLOCK		(1 << 7)
+#define PORT_GMII_1GPS_MODE		(1 << 6)
+#define PORT_RGMII_ID_IN_ENABLE		(1 << 4)
+#define PORT_RGMII_ID_OUT_ENABLE	(1 << 3)
+#define PORT_GMII_MAC_MODE		(1 << 2)
+#define PORT_INTERFACE_TYPE		0x3
+#define PORT_INTERFACE_MII		0
+#define PORT_INTERFACE_RMII		1
+#define PORT_INTERFACE_GMII		2
+#define PORT_INTERFACE_RGMII		3
+
+#define REG_PORT_1_CTRL_7		0x17
+#define REG_PORT_2_CTRL_7		0x27
+#define REG_PORT_3_CTRL_7		0x37
+#define REG_PORT_4_CTRL_7		0x47
+
+#define PORT_AUTO_NEG_ASYM_PAUSE	(1 << 5)
+#define PORT_AUTO_NEG_SYM_PAUSE		(1 << 4)
+#define PORT_AUTO_NEG_100BTX_FD		(1 << 3)
+#define PORT_AUTO_NEG_100BTX		(1 << 2)
+#define PORT_AUTO_NEG_10BT_FD		(1 << 1)
+#define PORT_AUTO_NEG_10BT		(1 << 0)
+
+#define REG_PORT_1_STATUS_0		0x18
+#define REG_PORT_2_STATUS_0		0x28
+#define REG_PORT_3_STATUS_0		0x38
+#define REG_PORT_4_STATUS_0		0x48
+
+/* For KSZ8765. */
+#define PORT_FIBER_MODE			(1 << 7)
+
+#define PORT_REMOTE_ASYM_PAUSE		(1 << 5)
+#define PORT_REMOTE_SYM_PAUSE		(1 << 4)
+#define PORT_REMOTE_100BTX_FD		(1 << 3)
+#define PORT_REMOTE_100BTX		(1 << 2)
+#define PORT_REMOTE_10BT_FD		(1 << 1)
+#define PORT_REMOTE_10BT		(1 << 0)
+
+#define REG_PORT_1_STATUS_1		0x19
+#define REG_PORT_2_STATUS_1		0x29
+#define REG_PORT_3_STATUS_1		0x39
+#define REG_PORT_4_STATUS_1		0x49
+
+#define PORT_HP_MDIX			(1 << 7)
+#define PORT_REVERSED_POLARITY		(1 << 5)
+#define PORT_TX_FLOW_CTRL		(1 << 4)
+#define PORT_RX_FLOW_CTRL		(1 << 3)
+#define PORT_STAT_SPEED_100MBIT		(1 << 2)
+#define PORT_STAT_FULL_DUPLEX		(1 << 1)
+
+#define PORT_REMOTE_FAULT		(1 << 0)
+
+#define REG_PORT_1_LINK_MD_CTRL		0x1A
+#define REG_PORT_2_LINK_MD_CTRL		0x2A
+#define REG_PORT_3_LINK_MD_CTRL		0x3A
+#define REG_PORT_4_LINK_MD_CTRL		0x4A
+
+#define PORT_CABLE_10M_SHORT		(1 << 7)
+#define PORT_CABLE_DIAG_RESULT_M	0x3
+#define PORT_CABLE_DIAG_RESULT_S	5
+#define PORT_CABLE_STAT_NORMAL		0
+#define PORT_CABLE_STAT_OPEN		1
+#define PORT_CABLE_STAT_SHORT		2
+#define PORT_CABLE_STAT_FAILED		3
+#define PORT_START_CABLE_DIAG		(1 << 4)
+#define PORT_FORCE_LINK			(1 << 3)
+#define PORT_POWER_SAVING		(1 << 2)
+#define PORT_PHY_REMOTE_LOOPBACK	(1 << 1)
+#define PORT_CABLE_FAULT_COUNTER_H	0x01
+
+#define REG_PORT_1_LINK_MD_RESULT	0x1B
+#define REG_PORT_2_LINK_MD_RESULT	0x2B
+#define REG_PORT_3_LINK_MD_RESULT	0x3B
+#define REG_PORT_4_LINK_MD_RESULT	0x4B
+
+#define PORT_CABLE_FAULT_COUNTER_L	0xFF
+#define PORT_CABLE_FAULT_COUNTER	0x1FF
+
+#define REG_PORT_1_CTRL_9		0x1C
+#define REG_PORT_2_CTRL_9		0x2C
+#define REG_PORT_3_CTRL_9		0x3C
+#define REG_PORT_4_CTRL_9		0x4C
+
+#define PORT_AUTO_NEG_DISABLE		(1 << 7)
+#define PORT_FORCE_100_MBIT		(1 << 6)
+#define PORT_FORCE_FULL_DUPLEX		(1 << 5)
+
+#define REG_PORT_1_CTRL_10		0x1D
+#define REG_PORT_2_CTRL_10		0x2D
+#define REG_PORT_3_CTRL_10		0x3D
+#define REG_PORT_4_CTRL_10		0x4D
+
+#define PORT_LED_OFF			(1 << 7)
+#define PORT_TX_DISABLE			(1 << 6)
+#define PORT_AUTO_NEG_RESTART		(1 << 5)
+#define PORT_POWER_DOWN			(1 << 3)
+#define PORT_AUTO_MDIX_DISABLE		(1 << 2)
+#define PORT_FORCE_MDIX			(1 << 1)
+#define PORT_MAC_LOOPBACK		(1 << 0)
+
+#define REG_PORT_1_STATUS_2		0x1E
+#define REG_PORT_2_STATUS_2		0x2E
+#define REG_PORT_3_STATUS_2		0x3E
+#define REG_PORT_4_STATUS_2		0x4E
+
+#define PORT_MDIX_STATUS		(1 << 7)
+#define PORT_AUTO_NEG_COMPLETE		(1 << 6)
+#define PORT_STAT_LINK_GOOD		(1 << 5)
+
+#define REG_PORT_1_STATUS_3		0x1F
+#define REG_PORT_2_STATUS_3		0x2F
+#define REG_PORT_3_STATUS_3		0x3F
+#define REG_PORT_4_STATUS_3		0x4F
+
+#define PORT_PHY_LOOPBACK		(1 << 7)
+#define PORT_PHY_ISOLATE		(1 << 5)
+#define PORT_PHY_SOFT_RESET		(1 << 4)
+#define PORT_PHY_FORCE_LINK		(1 << 3)
+#define PORT_PHY_MODE_M			0x7
+#define PHY_MODE_IN_AUTO_NEG		1
+#define PHY_MODE_10BT_HALF		2
+#define PHY_MODE_100BT_HALF		3
+#define PHY_MODE_10BT_FULL		5
+#define PHY_MODE_100BT_FULL		6
+#define PHY_MODE_ISOLDATE		7
+
+#define REG_PORT_CTRL_0			0x00
+#define REG_PORT_CTRL_1			0x01
+#define REG_PORT_CTRL_2			0x02
+#define REG_PORT_CTRL_VID		0x03
+
+#define REG_PORT_CTRL_5			0x05
+
+#define REG_PORT_CTRL_7			0x07
+#define REG_PORT_STATUS_0		0x08
+#define REG_PORT_STATUS_1		0x09
+#define REG_PORT_LINK_MD_CTRL		0x0A
+#define REG_PORT_LINK_MD_RESULT		0x0B
+#define REG_PORT_CTRL_9			0x0C
+#define REG_PORT_CTRL_10		0x0D
+#define REG_PORT_STATUS_2		0x0E
+#define REG_PORT_STATUS_3		0x0F
+
+#define REG_PORT_CTRL_12		0xA0
+#define REG_PORT_CTRL_13		0xA1
+#define REG_PORT_RATE_CTRL_3		0xA2
+#define REG_PORT_RATE_CTRL_2		0xA3
+#define REG_PORT_RATE_CTRL_1		0xA4
+#define REG_PORT_RATE_CTRL_0		0xA5
+#define REG_PORT_RATE_LIMIT		0xA6
+#define REG_PORT_IN_RATE_0		0xA7
+#define REG_PORT_IN_RATE_1		0xA8
+#define REG_PORT_IN_RATE_2		0xA9
+#define REG_PORT_IN_RATE_3		0xAA
+#define REG_PORT_OUT_RATE_0		0xAB
+#define REG_PORT_OUT_RATE_1		0xAC
+#define REG_PORT_OUT_RATE_2		0xAD
+#define REG_PORT_OUT_RATE_3		0xAE
+
+#define PORT_CTRL_ADDR(port, addr)		\
+	(addr = REG_PORT_1_CTRL_0 + (port) *	\
+		(REG_PORT_2_CTRL_0 - REG_PORT_1_CTRL_0))
+
+
+#define REG_SW_MAC_ADDR_0		0x68
+#define REG_SW_MAC_ADDR_1		0x69
+#define REG_SW_MAC_ADDR_2		0x6A
+#define REG_SW_MAC_ADDR_3		0x6B
+#define REG_SW_MAC_ADDR_4		0x6C
+#define REG_SW_MAC_ADDR_5		0x6D
+
+#define REG_IND_CTRL_0			0x6E
+
+#define TABLE_EXT_SELECT_S		5
+#define TABLE_EEE			(1 << TABLE_EXT_SELECT_S)
+#define TABLE_ACL			(2 << TABLE_EXT_SELECT_S)
+#define TABLE_PME			(4 << TABLE_EXT_SELECT_S)
+#define TABLE_LINK_MD			(5 << TABLE_EXT_SELECT_S)
+#define TABLE_READ			(1 << 4)
+#define TABLE_SELECT_S			2
+#define TABLE_STATIC_MAC		(0 << TABLE_SELECT_S)
+#define TABLE_VLAN			(1 << TABLE_SELECT_S)
+#define TABLE_DYNAMIC_MAC		(2 << TABLE_SELECT_S)
+#define TABLE_MIB			(3 << TABLE_SELECT_S)
+
+#define REG_IND_CTRL_1			0x6F
+
+#define TABLE_ENTRY_MASK		0x03FF
+#define TABLE_EXT_ENTRY_MASK		0x0FFF
+
+#define REG_IND_DATA_8			0x70
+#define REG_IND_DATA_7			0x71
+#define REG_IND_DATA_6			0x72
+#define REG_IND_DATA_5			0x73
+#define REG_IND_DATA_4			0x74
+#define REG_IND_DATA_3			0x75
+#define REG_IND_DATA_2			0x76
+#define REG_IND_DATA_1			0x77
+#define REG_IND_DATA_0			0x78
+
+#define REG_IND_DATA_PME_EEE_ACL	0xA0
+
+#define REG_IND_DATA_CHECK		REG_IND_DATA_6
+#define REG_IND_MIB_CHECK		REG_IND_DATA_4
+#define REG_IND_DATA_HI			REG_IND_DATA_7
+#define REG_IND_DATA_LO			REG_IND_DATA_3
+
+#define REG_INT_STATUS			0x7C
+#define REG_INT_ENABLE			0x7D
+
+#define INT_PME				(1 << 4)
+
+#define REG_ACL_INT_STATUS		0x7E
+#define REG_ACL_INT_ENABLE		0x7F
+
+#define INT_PORT_5			(1 << 4)
+#define INT_PORT_4			(1 << 3)
+#define INT_PORT_3			(1 << 2)
+#define INT_PORT_2			(1 << 1)
+#define INT_PORT_1			(1 << 0)
+
+#define INT_PORT_ALL			\
+	(INT_PORT_5 | INT_PORT_4 | INT_PORT_3 | INT_PORT_2 | INT_PORT_1)
+
+#define REG_SW_CTRL_12			0x80
+#define REG_SW_CTRL_13			0x81
+
+#define SWITCH_802_1P_MASK		3
+#define SWITCH_802_1P_BASE		3
+#define SWITCH_802_1P_SHIFT		2
+
+#define SW_802_1P_MAP_M			KS_PRIO_M
+#define SW_802_1P_MAP_S			KS_PRIO_S
+
+#define REG_SWITCH_CTRL_14		0x82
+
+#define SW_PRIO_MAPPING_M		KS_PRIO_M
+#define SW_PRIO_MAPPING_S		6
+#define SW_PRIO_MAP_3_HI		0
+#define SW_PRIO_MAP_2_HI		2
+#define SW_PRIO_MAP_0_LO		3
+
+#define REG_SW_CTRL_15			0x83
+#define REG_SW_CTRL_16			0x84
+#define REG_SW_CTRL_17			0x85
+#define REG_SW_CTRL_18			0x86
+
+#define SW_SELF_ADDR_FILTER_ENABLE	(1 << 6)
+
+#define REG_SW_UNK_UCAST_CTRL		0x83
+#define REG_SW_UNK_MCAST_CTRL		0x84
+#define REG_SW_UNK_VID_CTRL		0x85
+#define REG_SW_UNK_IP_MCAST_CTRL	0x86
+
+#define SW_UNK_FWD_ENABLE		(1 << 5)
+#define SW_UNK_FWD_MAP			KS_PORT_M
+
+#define REG_SW_CTRL_19			0x87
+
+#define SW_IN_RATE_LIMIT_PERIOD_M	0x3
+#define SW_IN_RATE_LIMIT_PERIOD_S	4
+#define SW_IN_RATE_LIMIT_16_MS		0
+#define SW_IN_RATE_LIMIT_64_MS		1
+#define SW_IN_RATE_LIMIT_256_MS		2
+#define SW_OUT_RATE_LIMIT_QUEUE_BASED	(1 << 3)
+#define SW_INS_TAG_ENABLE		(1 << 2)
+
+#define REG_TOS_PRIO_CTRL_0		0x90
+#define REG_TOS_PRIO_CTRL_1		0x91
+#define REG_TOS_PRIO_CTRL_2		0x92
+#define REG_TOS_PRIO_CTRL_3		0x93
+#define REG_TOS_PRIO_CTRL_4		0x94
+#define REG_TOS_PRIO_CTRL_5		0x95
+#define REG_TOS_PRIO_CTRL_6		0x96
+#define REG_TOS_PRIO_CTRL_7		0x97
+#define REG_TOS_PRIO_CTRL_8		0x98
+#define REG_TOS_PRIO_CTRL_9		0x99
+#define REG_TOS_PRIO_CTRL_10		0x9A
+#define REG_TOS_PRIO_CTRL_11		0x9B
+#define REG_TOS_PRIO_CTRL_12		0x9C
+#define REG_TOS_PRIO_CTRL_13		0x9D
+#define REG_TOS_PRIO_CTRL_14		0x9E
+#define REG_TOS_PRIO_CTRL_15		0x9F
+
+#define TOS_PRIO_M			KS_PRIO_M
+#define TOS_PRIO_S			KS_PRIO_S
+
+#define REG_SW_CTRL_20			0xA3
+
+#define SW_GMII_DRIVE_STRENGTH_S	4
+#define SW_DRIVE_STRENGTH_M		0x7
+#define SW_DRIVE_STRENGTH_2MA		0
+#define SW_DRIVE_STRENGTH_4MA		1
+#define SW_DRIVE_STRENGTH_8MA		2
+#define SW_DRIVE_STRENGTH_12MA		3
+#define SW_DRIVE_STRENGTH_16MA		4
+#define SW_DRIVE_STRENGTH_20MA		5
+#define SW_DRIVE_STRENGTH_24MA		6
+#define SW_DRIVE_STRENGTH_28MA		7
+#define SW_MII_DRIVE_STRENGTH_S		0
+
+#define REG_SW_CTRL_21			0xA4
+
+#define SW_IPV6_MLD_OPTION		(1 << 3)
+#define SW_IPV6_MLD_SNOOP		(1 << 2)
+
+
+#define REG_PORT_1_CTRL_12		0xB0
+#define REG_PORT_2_CTRL_12		0xC0
+#define REG_PORT_3_CTRL_12		0xD0
+#define REG_PORT_4_CTRL_12		0xE0
+#define REG_PORT_5_CTRL_12		0xF0
+
+#define PORT_PASS_ALL			(1 << 6)
+#define PORT_INS_TAG_FOR_PORT_5_S	3
+#define PORT_INS_TAG_FOR_PORT_5		(1 << 3)
+#define PORT_INS_TAG_FOR_PORT_4		(1 << 2)
+#define PORT_INS_TAG_FOR_PORT_3		(1 << 1)
+#define PORT_INS_TAG_FOR_PORT_2		(1 << 0)
+
+#define REG_PORT_1_CTRL_13		0xB1
+#define REG_PORT_2_CTRL_13		0xC1
+#define REG_PORT_3_CTRL_13		0xD1
+#define REG_PORT_4_CTRL_13		0xE1
+#define REG_PORT_5_CTRL_13		0xF1
+
+#define PORT_QUEUE_SPLIT_H		(1 << 1)
+#define PORT_QUEUE_SPLIT_1		0
+#define PORT_QUEUE_SPLIT_2		1
+#define PORT_QUEUE_SPLIT_4		2
+#define PORT_DROP_TAG			(1 << 0)
+
+#define REG_PORT_1_CTRL_14		0xB2
+#define REG_PORT_2_CTRL_14		0xC2
+#define REG_PORT_3_CTRL_14		0xD2
+#define REG_PORT_4_CTRL_14		0xE2
+#define REG_PORT_5_CTRL_14		0xF2
+#define REG_PORT_1_CTRL_15		0xB3
+#define REG_PORT_2_CTRL_15		0xC3
+#define REG_PORT_3_CTRL_15		0xD3
+#define REG_PORT_4_CTRL_15		0xE3
+#define REG_PORT_5_CTRL_15		0xF3
+#define REG_PORT_1_CTRL_16		0xB4
+#define REG_PORT_2_CTRL_16		0xC4
+#define REG_PORT_3_CTRL_16		0xD4
+#define REG_PORT_4_CTRL_16		0xE4
+#define REG_PORT_5_CTRL_16		0xF4
+#define REG_PORT_1_CTRL_17		0xB5
+#define REG_PORT_2_CTRL_17		0xC5
+#define REG_PORT_3_CTRL_17		0xD5
+#define REG_PORT_4_CTRL_17		0xE5
+#define REG_PORT_5_CTRL_17		0xF5
+
+#define REG_PORT_1_RATE_CTRL_3		0xB2
+#define REG_PORT_1_RATE_CTRL_2		0xB3
+#define REG_PORT_1_RATE_CTRL_1		0xB4
+#define REG_PORT_1_RATE_CTRL_0		0xB5
+#define REG_PORT_2_RATE_CTRL_3		0xC2
+#define REG_PORT_2_RATE_CTRL_2		0xC3
+#define REG_PORT_2_RATE_CTRL_1		0xC4
+#define REG_PORT_2_RATE_CTRL_0		0xC5
+#define REG_PORT_3_RATE_CTRL_3		0xD2
+#define REG_PORT_3_RATE_CTRL_2		0xD3
+#define REG_PORT_3_RATE_CTRL_1		0xD4
+#define REG_PORT_3_RATE_CTRL_0		0xD5
+#define REG_PORT_4_RATE_CTRL_3		0xE2
+#define REG_PORT_4_RATE_CTRL_2		0xE3
+#define REG_PORT_4_RATE_CTRL_1		0xE4
+#define REG_PORT_4_RATE_CTRL_0		0xE5
+#define REG_PORT_5_RATE_CTRL_3		0xF2
+#define REG_PORT_5_RATE_CTRL_2		0xF3
+#define REG_PORT_5_RATE_CTRL_1		0xF4
+#define REG_PORT_5_RATE_CTRL_0		0xF5
+
+#define RATE_CTRL_ENABLE		(1 << 7)
+#define RATE_RATIO_M			((1 << 7) - 1)
+
+#define PORT_OUT_RATE_ENABLE		(1 << 7)
+
+#define REG_PORT_1_RATE_LIMIT		0xB6
+#define REG_PORT_2_RATE_LIMIT		0xC6
+#define REG_PORT_3_RATE_LIMIT		0xD6
+#define REG_PORT_4_RATE_LIMIT		0xE6
+#define REG_PORT_5_RATE_LIMIT		0xF6
+
+#define PORT_IN_PORT_BASED_S		6
+#define PORT_RATE_PACKET_BASED_S	5
+#define PORT_IN_FLOW_CTRL_S		4
+#define PORT_IN_LIMIT_MODE_M		0x3
+#define PORT_IN_LIMIT_MODE_S		2
+#define PORT_COUNT_IFG_S		1
+#define PORT_COUNT_PREAMBLE_S		0
+#define PORT_IN_PORT_BASED		(1 << PORT_IN_PORT_BASED_S)
+#define PORT_RATE_PACKET_BASED		(1 << PORT_RATE_PACKET_BASED_S)
+#define PORT_IN_FLOW_CTRL		(1 << PORT_IN_FLOW_CTRL_S)
+#define PORT_IN_ALL			0
+#define PORT_IN_UNICAST			1
+#define PORT_IN_MULTICAST		2
+#define PORT_IN_BROADCAST		3
+#define PORT_COUNT_IFG			(1 << PORT_COUNT_IFG_S)
+#define PORT_COUNT_PREAMBLE		(1 << PORT_COUNT_PREAMBLE_S)
+
+#define REG_PORT_1_IN_RATE_0		0xB7
+#define REG_PORT_2_IN_RATE_0		0xC7
+#define REG_PORT_3_IN_RATE_0		0xD7
+#define REG_PORT_4_IN_RATE_0		0xE7
+#define REG_PORT_5_IN_RATE_0		0xF7
+#define REG_PORT_1_IN_RATE_1		0xB8
+#define REG_PORT_2_IN_RATE_1		0xC8
+#define REG_PORT_3_IN_RATE_1		0xD8
+#define REG_PORT_4_IN_RATE_1		0xE8
+#define REG_PORT_5_IN_RATE_1		0xF8
+#define REG_PORT_1_IN_RATE_2		0xB9
+#define REG_PORT_2_IN_RATE_2		0xC9
+#define REG_PORT_3_IN_RATE_2		0xD9
+#define REG_PORT_4_IN_RATE_2		0xE9
+#define REG_PORT_5_IN_RATE_2		0xF9
+#define REG_PORT_1_IN_RATE_3		0xBA
+#define REG_PORT_2_IN_RATE_3		0xCA
+#define REG_PORT_3_IN_RATE_3		0xDA
+#define REG_PORT_4_IN_RATE_3		0xEA
+#define REG_PORT_5_IN_RATE_3		0xFA
+
+#define PORT_IN_RATE_ENABLE		(1 << 7)
+#define PORT_RATE_LIMIT_M		((1 << 7) - 1)
+
+#define REG_PORT_1_OUT_RATE_0		0xBB
+#define REG_PORT_2_OUT_RATE_0		0xCB
+#define REG_PORT_3_OUT_RATE_0		0xDB
+#define REG_PORT_4_OUT_RATE_0		0xEB
+#define REG_PORT_5_OUT_RATE_0		0xFB
+#define REG_PORT_1_OUT_RATE_1		0xBC
+#define REG_PORT_2_OUT_RATE_1		0xCC
+#define REG_PORT_3_OUT_RATE_1		0xDC
+#define REG_PORT_4_OUT_RATE_1		0xEC
+#define REG_PORT_5_OUT_RATE_1		0xFC
+#define REG_PORT_1_OUT_RATE_2		0xBD
+#define REG_PORT_2_OUT_RATE_2		0xCD
+#define REG_PORT_3_OUT_RATE_2		0xDD
+#define REG_PORT_4_OUT_RATE_2		0xED
+#define REG_PORT_5_OUT_RATE_2		0xFD
+#define REG_PORT_1_OUT_RATE_3		0xBE
+#define REG_PORT_2_OUT_RATE_3		0xCE
+#define REG_PORT_3_OUT_RATE_3		0xDE
+#define REG_PORT_4_OUT_RATE_3		0xEE
+#define REG_PORT_5_OUT_RATE_3		0xFE
+
+/* PME */
+
+#define SW_PME_OUTPUT_ENABLE		(1 << 1)
+#define SW_PME_ACTIVE_HIGH		(1 << 0)
+
+#define PORT_MAGIC_PACKET_DETECT	(1 << 2)
+#define PORT_LINK_UP_DETECT		(1 << 1)
+#define PORT_ENERGY_DETECT		(1 << 0)
+
+/* ACL */
+
+#define ACL_FIRST_RULE_M		0xF
+
+#define ACL_MODE_M			0x3
+#define ACL_MODE_S			4
+#define ACL_MODE_DISABLE		0
+#define ACL_MODE_LAYER_2		1
+#define ACL_MODE_LAYER_3		2
+#define ACL_MODE_LAYER_4		3
+#define ACL_ENABLE_M			0x3
+#define ACL_ENABLE_S			2
+#define ACL_ENABLE_2_COUNT		0
+#define ACL_ENABLE_2_TYPE		1
+#define ACL_ENABLE_2_MAC		2
+#define ACL_ENABLE_2_BOTH		3
+#define ACL_ENABLE_3_IP			1
+#define ACL_ENABLE_3_SRC_DST_COMP	2
+#define ACL_ENABLE_4_PROTOCOL		0
+#define ACL_ENABLE_4_TCP_PORT_COMP	1
+#define ACL_ENABLE_4_UDP_PORT_COMP	2
+#define ACL_ENABLE_4_TCP_SEQN_COMP	3
+#define ACL_SRC				(1 << 1)
+#define ACL_EQUAL			(1 << 0)
+
+#define ACL_MAX_PORT			0xFFFF
+
+#define ACL_MIN_PORT			0xFFFF
+#define ACL_IP_ADDR			0xFFFFFFFF
+#define ACL_TCP_SEQNUM			0xFFFFFFFF
+
+#define ACL_RESERVED			0xF8
+#define ACL_PORT_MODE_M			0x3
+#define ACL_PORT_MODE_S			1
+#define ACL_PORT_MODE_DISABLE		0
+#define ACL_PORT_MODE_EITHER		1
+#define ACL_PORT_MODE_IN_RANGE		2
+#define ACL_PORT_MODE_OUT_OF_RANGE	3
+
+#define ACL_TCP_FLAG_ENABLE		(1 << 0)
+
+#define ACL_TCP_FLAG_M			0xFF
+
+#define ACL_TCP_FLAG			0xFF
+#define ACL_ETH_TYPE			0xFFFF
+#define ACL_IP_M			0xFFFFFFFF
+
+#define ACL_PRIO_MODE_M			0x3
+#define ACL_PRIO_MODE_S			6
+#define ACL_PRIO_MODE_DISABLE		0
+#define ACL_PRIO_MODE_HIGHER		1
+#define ACL_PRIO_MODE_LOWER		2
+#define ACL_PRIO_MODE_REPLACE		3
+#define ACL_PRIO_M			0x7
+#define ACL_PRIO_S			3
+#define ACL_VLAN_PRIO_REPLACE		(1 << 2)
+#define ACL_VLAN_PRIO_M			0x7
+#define ACL_VLAN_PRIO_HI_M		0x3
+
+#define ACL_VLAN_PRIO_LO_M		0x8
+#define ACL_VLAN_PRIO_S			7
+#define ACL_MAP_MODE_M			0x3
+#define ACL_MAP_MODE_S			5
+#define ACL_MAP_MODE_DISABLE		0
+#define ACL_MAP_MODE_OR			1
+#define ACL_MAP_MODE_AND		2
+#define ACL_MAP_MODE_REPLACE		3
+#define ACL_MAP_PORT_M			0x1F
+
+#define ACL_CNT_M			((1 << 11) - 1)
+#define ACL_CNT_S			5
+#define ACL_MSEC_UNIT			(1 << 4)
+#define ACL_INTR_MODE			(1 << 3)
+
+#define REG_PORT_ACL_BYTE_EN_MSB	0x10
+
+#define ACL_BYTE_EN_MSB_M		0x3F
+
+#define REG_PORT_ACL_BYTE_EN_LSB	0x11
+
+#define ACL_ACTION_START		0xA
+#define ACL_ACTION_LEN			2
+#define ACL_INTR_CNT_START		0xB
+#define ACL_RULESET_START		0xC
+#define ACL_RULESET_LEN			2
+#define ACL_TABLE_LEN			14
+
+#define ACL_ACTION_ENABLE		0x000C
+#define ACL_MATCH_ENABLE		0x1FF0
+#define ACL_RULESET_ENABLE		0x2003
+#define ACL_BYTE_ENABLE			((ACL_BYTE_EN_MSB_M << 8) | 0xFF)
+#define ACL_MODE_ENABLE			(0x10 << 8)
+
+#define REG_PORT_ACL_CTRL_0		0x12
+
+#define PORT_ACL_WRITE_DONE		(1 << 6)
+#define PORT_ACL_READ_DONE		(1 << 5)
+#define PORT_ACL_WRITE			(1 << 4)
+#define PORT_ACL_INDEX_M		0xF
+
+#define REG_PORT_ACL_CTRL_1		0x13
+
+#define PORT_ACL_FORCE_DLR_MISS		(1 << 0)
+
+#ifndef PHY_REG_CTRL
+#define PHY_REG_CTRL			0
+
+#define PHY_RESET			(1 << 15)
+#define PHY_LOOPBACK			(1 << 14)
+#define PHY_SPEED_100MBIT		(1 << 13)
+#define PHY_AUTO_NEG_ENABLE		(1 << 12)
+#define PHY_POWER_DOWN			(1 << 11)
+#define PHY_MII_DISABLE			(1 << 10)
+#define PHY_AUTO_NEG_RESTART		(1 << 9)
+#define PHY_FULL_DUPLEX			(1 << 8)
+#define PHY_COLLISION_TEST_NOT		(1 << 7)
+#define PHY_HP_MDIX			(1 << 5)
+#define PHY_FORCE_MDIX			(1 << 4)
+#define PHY_AUTO_MDIX_DISABLE		(1 << 3)
+#define PHY_REMOTE_FAULT_DISABLE	(1 << 2)
+#define PHY_TRANSMIT_DISABLE		(1 << 1)
+#define PHY_LED_DISABLE			(1 << 0)
+
+#define PHY_REG_STATUS			1
+
+#define PHY_100BT4_CAPABLE		(1 << 15)
+#define PHY_100BTX_FD_CAPABLE		(1 << 14)
+#define PHY_100BTX_CAPABLE		(1 << 13)
+#define PHY_10BT_FD_CAPABLE		(1 << 12)
+#define PHY_10BT_CAPABLE		(1 << 11)
+#define PHY_MII_SUPPRESS_CAPABLE_NOT	(1 << 6)
+#define PHY_AUTO_NEG_ACKNOWLEDGE	(1 << 5)
+#define PHY_REMOTE_FAULT		(1 << 4)
+#define PHY_AUTO_NEG_CAPABLE		(1 << 3)
+#define PHY_LINK_STATUS			(1 << 2)
+#define PHY_JABBER_DETECT_NOT		(1 << 1)
+#define PHY_EXTENDED_CAPABILITY		(1 << 0)
+
+#define PHY_REG_ID_1			2
+#define PHY_REG_ID_2			3
+
+#define PHY_REG_AUTO_NEGOTIATION	4
+
+#define PHY_AUTO_NEG_NEXT_PAGE_NOT	(1 << 15)
+#define PHY_AUTO_NEG_REMOTE_FAULT_NOT	(1 << 13)
+#define PHY_AUTO_NEG_SYM_PAUSE		(1 << 10)
+#define PHY_AUTO_NEG_100BT4		(1 << 9)
+#define PHY_AUTO_NEG_100BTX_FD		(1 << 8)
+#define PHY_AUTO_NEG_100BTX		(1 << 7)
+#define PHY_AUTO_NEG_10BT_FD		(1 << 6)
+#define PHY_AUTO_NEG_10BT		(1 << 5)
+#define PHY_AUTO_NEG_SELECTOR		0x001F
+#define PHY_AUTO_NEG_802_3		0x0001
+
+#define PHY_REG_REMOTE_CAPABILITY	5
+
+#define PHY_REMOTE_NEXT_PAGE_NOT	(1 << 15)
+#define PHY_REMOTE_ACKNOWLEDGE_NOT	(1 << 14)
+#define PHY_REMOTE_REMOTE_FAULT_NOT	(1 << 13)
+#define PHY_REMOTE_SYM_PAUSE		(1 << 10)
+#define PHY_REMOTE_100BTX_FD		(1 << 8)
+#define PHY_REMOTE_100BTX		(1 << 7)
+#define PHY_REMOTE_10BT_FD		(1 << 6)
+#define PHY_REMOTE_10BT			(1 << 5)
+#endif
+
+#define KSZ8795_ID_HI			0x0022
+#define KSZ8795_ID_LO			0x1550
+
+#define PHY_REG_LINK_MD			0x1D
+
+#define PHY_START_CABLE_DIAG		(1 << 15)
+#define PHY_CABLE_DIAG_RESULT		0x6000
+#define PHY_CABLE_STAT_NORMAL		0x0000
+#define PHY_CABLE_STAT_OPEN		0x2000
+#define PHY_CABLE_STAT_SHORT		0x4000
+#define PHY_CABLE_STAT_FAILED		0x6000
+#define PHY_CABLE_10M_SHORT		(1 << 12)
+#define PHY_CABLE_FAULT_COUNTER		0x01FF
+
+#define PHY_REG_PHY_CTRL		0x1F
+
+#define PHY_MODE_M			0x7
+#define PHY_MODE_S			8
+#define PHY_STAT_REVERSED_POLARITY	(1 << 5)
+#define PHY_STAT_MDIX			(1 << 4)
+#define PHY_FORCE_LINK			(1 << 3)
+#define PHY_POWER_SAVING_ENABLE		(1 << 2)
+#define PHY_REMOTE_LOOPBACK		(1 << 1)
+
+
+/* Default values are used in ksz_sw.h if these are not defined. */
+#define PRIO_QUEUES			4
+
+#define KS_PRIO_IN_REG			4
+
+#define TOTAL_PORT_NUM			5
+
+#define KSZ8795_COUNTER_NUM		0x20
+#define TOTAL_KSZ8795_COUNTER_NUM	(KSZ8795_COUNTER_NUM + 4)
+
+#define SWITCH_COUNTER_NUM		KSZ8795_COUNTER_NUM
+#define TOTAL_SWITCH_COUNTER_NUM	TOTAL_KSZ8795_COUNTER_NUM
+
+/* Required for common switch control in ksz_sw.c */
+#define SW_D				u8
+#define SW_R(sw, addr)			(sw)->reg->r8(sw, addr)
+#define SW_W(sw, addr, val)		(sw)->reg->w8(sw, addr, val)
+#define SW_SIZE				(1)
+#define SW_SIZE_STR			"%02x"
+#define port_r				port_r8
+#define port_w				port_w8
+
+
+#define P_BCAST_STORM_CTRL		REG_PORT_CTRL_0
+#define P_PRIO_CTRL			REG_PORT_CTRL_0
+#define P_TAG_CTRL			REG_PORT_CTRL_0
+#define P_MIRROR_CTRL			REG_PORT_CTRL_1
+#define P_802_1P_CTRL			REG_PORT_CTRL_2
+#define P_STP_CTRL			REG_PORT_CTRL_2
+#define P_LOCAL_CTRL			REG_PORT_CTRL_7
+#define P_REMOTE_STATUS			REG_PORT_STATUS_0
+#define P_FORCE_CTRL			REG_PORT_CTRL_9
+#define P_NEG_RESTART_CTRL		REG_PORT_CTRL_10
+#define P_SPEED_STATUS			REG_PORT_STATUS_1
+#define P_LINK_STATUS			REG_PORT_STATUS_2
+#define P_PASS_ALL_CTRL			REG_PORT_CTRL_12
+#define P_INS_SRC_PVID_CTRL		REG_PORT_CTRL_12
+#define P_DROP_TAG_CTRL			REG_PORT_CTRL_13
+#define P_RATE_LIMIT_CTRL		REG_PORT_RATE_LIMIT
+
+#define S_UNKNOWN_DA_CTRL		REG_SWITCH_CTRL_12
+#define S_FORWARD_INVALID_VID_CTRL	REG_FORWARD_INVALID_VID
+
+#define S_FLUSH_TABLE_CTRL		REG_SW_CTRL_0
+#define S_LINK_AGING_CTRL		REG_SW_CTRL_0
+#define S_HUGE_PACKET_CTRL		REG_SW_CTRL_1
+#define S_MIRROR_CTRL			REG_SW_CTRL_3
+#define S_REPLACE_VID_CTRL		REG_SW_CTRL_4
+#define S_PASS_PAUSE_CTRL		REG_SW_CTRL_10
+#define S_TAIL_TAG_CTRL			REG_SW_CTRL_10
+#define S_802_1P_PRIO_CTRL		REG_SW_CTRL_12
+#define S_TOS_PRIO_CTRL			REG_TOS_PRIO_CTRL_0
+#define S_IPV6_MLD_CTRL			REG_SW_CTRL_21
+
+#define IND_ACC_TABLE(table)		((table) << 8)
+
+#define TAIL_TAG_OVERRIDE		(1 << 6)
+#define TAIL_TAG_LOOKUP			(1 << 7)
+
+#endif
diff -Napur linux-linux4microchip-2022.10-orig/drivers/net/ethernet/micrel/ksz8863.h linux-linux4microchip-2022.10/drivers/net/ethernet/micrel/ksz8863.h
--- linux-linux4microchip-2022.10-orig/drivers/net/ethernet/micrel/ksz8863.h	1969-12-31 16:00:00.000000000 -0800
+++ linux-linux4microchip-2022.10/drivers/net/ethernet/micrel/ksz8863.h	2023-04-25 16:13:54.868163472 -0700
@@ -0,0 +1,628 @@
+/**
+ * Microchip KSZ8863 definition file
+ *
+ * Copyright (c) 2015-2018 Microchip Technology Inc.
+ *	Tristram Ha <Tristram.Ha@microchip.com>
+ *
+ * Copyright (c) 2012-2014 Micrel, Inc.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ */
+
+
+/* -------------------------------------------------------------------------- */
+
+#ifndef __KSZ8863_H
+#define __KSZ8863_H
+
+
+#define REG_CHIP_ID0			0x00
+
+#define FAMILY_ID			0x88
+
+#define REG_CHIP_ID1			0x01
+
+#define SWITCH_CHIP_ID_MASK		0xF0
+#define SWITCH_CHIP_ID_SHIFT		4
+#define SWITCH_REVISION_MASK		0x0E
+#define SWITCH_REVISION_SHIFT		1
+#define SWITCH_START			0x01
+
+#define CHIP_ID_63			0x30
+
+#define REG_SWITCH_CTRL_0		0x02
+
+#define SWITCH_NEW_BACKOFF		(1 << 7)
+#define SWITCH_FLUSH_DYN_MAC_TABLE	(1 << 5)
+#define SWITCH_FLUSH_STA_MAC_TABLE	(1 << 4)
+#define SWITCH_PASS_PAUSE		(1 << 3)
+#define SWITCH_LINK_AUTO_AGING		(1 << 0)
+
+#define REG_SWITCH_CTRL_1		0x03
+
+#define SWITCH_PASS_ALL			(1 << 7)
+#define SWITCH_TAIL_TAG_ENABLE		(1 << 6)
+#define SWITCH_TX_FLOW_CTRL		(1 << 5)
+#define SWITCH_RX_FLOW_CTRL		(1 << 4)
+#define SWITCH_CHECK_LENGTH		(1 << 3)
+#define SWITCH_AGING_ENABLE		(1 << 2)
+#define SWITCH_FAST_AGING		(1 << 1)
+#define SWITCH_AGGR_BACKOFF		(1 << 0)
+
+#define REG_SWITCH_CTRL_2		0x04
+
+#define UNICAST_VLAN_BOUNDARY		(1 << 7)
+#define MULTICAST_STORM_DISABLE		(1 << 6)
+#define SWITCH_BACK_PRESSURE		(1 << 5)
+#define FAIR_FLOW_CTRL			(1 << 4)
+#define NO_EXC_COLLISION_DROP		(1 << 3)
+#define SWITCH_HUGE_PACKET		(1 << 2)
+#define SWITCH_LEGAL_PACKET		(1 << 1)
+
+#define REG_SWITCH_CTRL_3		0x05
+
+#define SWITCH_VLAN_ENABLE		(1 << 7)
+#define SWITCH_IGMP_SNOOP		(1 << 6)
+#define WEIGHTED_FAIR_QUEUE_ENABLE	(1 << 3)
+#define SWITCH_MIRROR_RX_TX		(1 << 0)
+
+#define REG_SWITCH_CTRL_4		0x06
+
+#define SWITCH_HALF_DUPLEX		(1 << 6)
+#define SWITCH_FLOW_CTRL		(1 << 5)
+#define SWITCH_10_MBIT			(1 << 4)
+#define SWITCH_REPLACE_VID		(1 << 3)
+#define BROADCAST_STORM_RATE_HI		0x07
+
+#define REG_SWITCH_CTRL_5		0x07
+
+#define BROADCAST_STORM_RATE_LO		0xFF
+#define BROADCAST_STORM_RATE		0x07FF
+
+#define REG_SWITCH_CTRL_9		0x0B
+
+#define SPI_CLK_125_MHZ			0x80
+#define SPI_CLK_62_5_MHZ		0x40
+#define SPI_CLK_31_25_MHZ		0x00
+
+#define REG_SWITCH_CTRL_10		0x0C
+#define REG_SWITCH_CTRL_11		0x0D
+
+#define SWITCH_802_1P_MASK		3
+#define SWITCH_802_1P_BASE		3
+#define SWITCH_802_1P_SHIFT		2
+
+#define SWITCH_802_1P_MAP_MASK		3
+#define SWITCH_802_1P_MAP_SHIFT		2
+
+#define REG_SWITCH_CTRL_12		0x0E
+
+#define SWITCH_UNKNOWN_DA_ENABLE	(1 << 7)
+#define SWITCH_DRIVER_16MA		(1 << 6)
+#define SWITCH_UNKNOWN_DA_2_PORT3	(1 << 2)
+#define SWITCH_UNKNOWN_DA_2_PORT2	(1 << 1)
+#define SWITCH_UNKNOWN_DA_2_PORT1	(1 << 0)
+
+#define REG_SWITCH_CTRL_13		0x0F
+
+#define SWITCH_PORT_PHY_ADDR_MASK	0x1F
+#define SWITCH_PORT_PHY_ADDR_SHIFT	3
+
+
+#define REG_PORT_1_CTRL_0		0x10
+#define REG_PORT_2_CTRL_0		0x20
+#define REG_PORT_3_CTRL_0		0x30
+
+#define PORT_BROADCAST_STORM		(1 << 7)
+#define PORT_DIFFSERV_ENABLE		(1 << 6)
+#define PORT_802_1P_ENABLE		(1 << 5)
+#define PORT_BASED_PRIORITY_MASK	0x18
+#define PORT_BASED_PRIORITY_BASE	0x03
+#define PORT_BASED_PRIORITY_SHIFT	3
+#define PORT_PORT_PRIO_0		0x00
+#define PORT_PORT_PRIO_1		0x08
+#define PORT_PORT_PRIO_2		0x10
+#define PORT_PORT_PRIO_3		0x18
+#define PORT_INSERT_TAG			(1 << 2)
+#define PORT_REMOVE_TAG			(1 << 1)
+#define PORT_4_QUEUES_ENABLE		(1 << 0)
+
+#define REG_PORT_1_CTRL_1		0x11
+#define REG_PORT_2_CTRL_1		0x21
+#define REG_PORT_3_CTRL_1		0x31
+
+#define PORT_MIRROR_SNIFFER		(1 << 7)
+#define PORT_MIRROR_RX			(1 << 6)
+#define PORT_MIRROR_TX			(1 << 5)
+#define PORT_DOUBLE_TAG			(1 << 4)
+#define PORT_802_1P_REMAPPING		(1 << 3)
+#define PORT_VLAN_MEMBERSHIP		0x07
+
+#define REG_PORT_1_CTRL_2		0x12
+#define REG_PORT_2_CTRL_2		0x22
+#define REG_PORT_3_CTRL_2		0x32
+
+#define PORT_2_QUEUES_ENABLE		(1 << 7)
+#define PORT_INGRESS_FILTER		(1 << 6)
+#define PORT_DISCARD_NON_VID		(1 << 5)
+#define PORT_FORCE_FLOW_CTRL		(1 << 4)
+#define PORT_BACK_PRESSURE		(1 << 3)
+#define PORT_TX_ENABLE			(1 << 2)
+#define PORT_RX_ENABLE			(1 << 1)
+#define PORT_LEARN_DISABLE		(1 << 0)
+
+#define REG_PORT_1_CTRL_3		0x13
+#define REG_PORT_2_CTRL_3		0x23
+#define REG_PORT_3_CTRL_3		0x33
+#define REG_PORT_1_CTRL_4		0x14
+#define REG_PORT_2_CTRL_4		0x24
+#define REG_PORT_3_CTRL_4		0x34
+
+#define PORT_DEFAULT_VID		0x0001
+
+#define REG_PORT_1_CTRL_5		0x15
+#define REG_PORT_2_CTRL_5		0x25
+#define REG_PORT_3_CTRL_5		0x35
+
+#define PORT_3_MII_MAC_MODE		(1 << 7)
+#define PORT_SA_MAC2			(1 << 6)
+#define PORT_SA_MAC1			(1 << 5)
+#define PORT_DROP_TAG			(1 << 4)
+#define PORT_INGRESS_LIMIT_MODE		0x0C
+#define PORT_INGRESS_ALL		0x00
+#define PORT_INGRESS_UNICAST		0x04
+#define PORT_INGRESS_MULTICAST		0x08
+#define PORT_INGRESS_BROADCAST		0x0C
+#define PORT_COUNT_IFG			(1 << 1)
+#define PORT_COUNT_PREAMBLE		(1 << 0)
+
+#define REG_PORT_1_IN_RATE_0		0x16
+#define REG_PORT_2_IN_RATE_0		0x26
+#define REG_PORT_3_IN_RATE_0		0x36
+
+#define PORT_3_INVERTED_REFCLK		(1 << 7)
+
+#define REG_PORT_1_IN_RATE_1		0x17
+#define REG_PORT_2_IN_RATE_1		0x27
+#define REG_PORT_3_IN_RATE_1		0x37
+#define REG_PORT_1_IN_RATE_2		0x18
+#define REG_PORT_2_IN_RATE_2		0x28
+#define REG_PORT_3_IN_RATE_2		0x38
+#define REG_PORT_1_IN_RATE_3		0x19
+#define REG_PORT_2_IN_RATE_3		0x29
+#define REG_PORT_3_IN_RATE_3		0x39
+
+#define PORT_PRIO_RATE			0x0F
+#define PORT_PRIO_RATE_SHIFT		4
+
+
+#define REG_PORT_1_LINK_MD_CTRL		0x1A
+#define REG_PORT_2_LINK_MD_CTRL		0x2A
+
+#define PORT_CABLE_10M_SHORT		(1 << 7)
+#define PORT_CABLE_DIAG_RESULT		0x60
+#define PORT_CABLE_STAT_NORMAL		0x00
+#define PORT_CABLE_STAT_OPEN		0x20
+#define PORT_CABLE_STAT_SHORT		0x40
+#define PORT_CABLE_STAT_FAILED		0x60
+#define PORT_START_CABLE_DIAG		(1 << 4)
+#define PORT_FORCE_LINK			(1 << 3)
+#define PORT_POWER_SAVING		0x04
+#define PORT_PHY_REMOTE_LOOPBACK	(1 << 1)
+#define PORT_CABLE_FAULT_COUNTER_H	0x01
+
+#define REG_PORT_1_LINK_MD_RESULT	0x1B
+#define REG_PORT_2_LINK_MD_RESULT	0x2B
+
+#define PORT_CABLE_FAULT_COUNTER_L	0xFF
+#define PORT_CABLE_FAULT_COUNTER	0x1FF
+
+#define REG_PORT_1_CTRL_12		0x1C
+#define REG_PORT_2_CTRL_12		0x2C
+
+#define PORT_AUTO_NEG_ENABLE		(1 << 7)
+#define PORT_FORCE_100_MBIT		(1 << 6)
+#define PORT_FORCE_FULL_DUPLEX		(1 << 5)
+#define PORT_AUTO_NEG_SYM_PAUSE		(1 << 4)
+#define PORT_AUTO_NEG_100BTX_FD		(1 << 3)
+#define PORT_AUTO_NEG_100BTX		(1 << 2)
+#define PORT_AUTO_NEG_10BT_FD		(1 << 1)
+#define PORT_AUTO_NEG_10BT		(1 << 0)
+
+#define REG_PORT_1_CTRL_13		0x1D
+#define REG_PORT_2_CTRL_13		0x2D
+
+#define PORT_LED_OFF			(1 << 7)
+#define PORT_TX_DISABLE			(1 << 6)
+#define PORT_AUTO_NEG_RESTART		(1 << 5)
+#define PORT_REMOTE_FAULT_DISABLE	(1 << 4)
+#define PORT_POWER_DOWN			(1 << 3)
+#define PORT_AUTO_MDIX_DISABLE		(1 << 2)
+#define PORT_FORCE_MDIX			(1 << 1)
+#define PORT_LOOPBACK			(1 << 0)
+
+#define REG_PORT_1_STATUS_0		0x1E
+#define REG_PORT_2_STATUS_0		0x2E
+
+#define PORT_MDIX_STATUS		(1 << 7)
+#define PORT_AUTO_NEG_COMPLETE		(1 << 6)
+#define PORT_STATUS_LINK_GOOD		(1 << 5)
+#define PORT_REMOTE_SYM_PAUSE		(1 << 4)
+#define PORT_REMOTE_100BTX_FD		(1 << 3)
+#define PORT_REMOTE_100BTX		(1 << 2)
+#define PORT_REMOTE_10BT_FD		(1 << 1)
+#define PORT_REMOTE_10BT		(1 << 0)
+
+#define REG_PORT_1_STATUS_1		0x1F
+#define REG_PORT_2_STATUS_1		0x2F
+#define REG_PORT_3_STATUS_1		0x3F
+
+#define PORT_HP_MDIX			(1 << 7)
+#define PORT_REVERSED_POLARITY		(1 << 5)
+#define PORT_TX_FLOW_CTRL		(1 << 4)
+#define PORT_RX_FLOW_CTRL		(1 << 3)
+#define PORT_STAT_SPEED_100MBIT		(1 << 2)
+#define PORT_STAT_FULL_DUPLEX		(1 << 1)
+#define PORT_REMOTE_FAULT		(1 << 0)
+
+#define REG_PORT_CTRL_0			0x00
+#define REG_PORT_CTRL_1			0x01
+#define REG_PORT_CTRL_2			0x02
+#define REG_PORT_CTRL_VID		0x03
+#define REG_PORT_CTRL_5			0x05
+#define REG_PORT_IN_RATE_0		0x06
+#define REG_PORT_IN_RATE_1		0x07
+#define REG_PORT_IN_RATE_2		0x08
+#define REG_PORT_IN_RATE_3		0x09
+#define REG_PORT_LINK_MD_CTRL		0x0A
+#define REG_PORT_LINK_MD_RESULT		0x0B
+#define REG_PORT_CTRL_12		0x0C
+#define REG_PORT_CTRL_13		0x0D
+#define REG_PORT_STATUS_0		0x0E
+#define REG_PORT_STATUS_1		0x0F
+
+#define PORT_CTRL_ADDR(port, addr)		\
+	(addr = REG_PORT_1_CTRL_0 + (port) *	\
+		(REG_PORT_2_CTRL_0 - REG_PORT_1_CTRL_0))
+
+
+#define REG_SWITCH_RESET		0x43
+
+#define GLOBAL_SOFTWARE_RESET		(1 << 4)
+#define PCS_RESET			(1 << 0)
+
+
+#define REG_TOS_PRIO_CTRL_0		0x60
+#define REG_TOS_PRIO_CTRL_1		0x61
+#define REG_TOS_PRIO_CTRL_2		0x62
+#define REG_TOS_PRIO_CTRL_3		0x63
+#define REG_TOS_PRIO_CTRL_4		0x64
+#define REG_TOS_PRIO_CTRL_5		0x65
+#define REG_TOS_PRIO_CTRL_6		0x66
+#define REG_TOS_PRIO_CTRL_7		0x67
+#define REG_TOS_PRIO_CTRL_8		0x68
+#define REG_TOS_PRIO_CTRL_9		0x69
+#define REG_TOS_PRIO_CTRL_10		0x6A
+#define REG_TOS_PRIO_CTRL_11		0x6B
+#define REG_TOS_PRIO_CTRL_12		0x6C
+#define REG_TOS_PRIO_CTRL_13		0x6D
+#define REG_TOS_PRIO_CTRL_14		0x6E
+#define REG_TOS_PRIO_CTRL_15		0x6F
+
+#define TOS_PRIO_M			3
+#define TOS_PRIO_S			2
+
+
+#define REG_SWITCH_MAC_ADDR_0		0x70
+#define REG_SWITCH_MAC_ADDR_1		0x71
+#define REG_SWITCH_MAC_ADDR_2		0x72
+#define REG_SWITCH_MAC_ADDR_3		0x73
+#define REG_SWITCH_MAC_ADDR_4		0x74
+#define REG_SWITCH_MAC_ADDR_5		0x75
+
+
+#define REG_USER_DEFINED_1		0x76
+#define REG_USER_DEFINED_2		0x77
+#define REG_USER_DEFINED_3		0x78
+
+
+#define REG_IND_CTRL_0			0x79
+
+#define TABLE_READ			(1 << 4)
+#define TABLE_STATIC_MAC		(0 << 2)
+#define TABLE_VLAN			(1 << 2)
+#define TABLE_DYNAMIC_MAC		(2 << 2)
+#define TABLE_MIB			(3 << 2)
+
+#define REG_IND_CTRL_1			0x7A
+
+#define TABLE_ENTRY_MASK		0x03FF
+
+#define REG_IND_DATA_8			0x7B
+#define REG_IND_DATA_7			0x7C
+#define REG_IND_DATA_6			0x7D
+#define REG_IND_DATA_5			0x7E
+#define REG_IND_DATA_4			0x7F
+#define REG_IND_DATA_3			0x80
+#define REG_IND_DATA_2			0x81
+#define REG_IND_DATA_1			0x82
+#define REG_IND_DATA_0			0x83
+
+#define REG_IND_DATA_CHECK		REG_IND_DATA_8
+#define REG_IND_DATA_HI			REG_IND_DATA_7
+#define REG_IND_DATA_LO			REG_IND_DATA_3
+
+#define REG_PORT_0_MAC_ADDR_0		0x8E
+#define REG_PORT_0_MAC_ADDR_1		0x8F
+#define REG_PORT_0_MAC_ADDR_2		0x90
+#define REG_PORT_0_MAC_ADDR_3		0x91
+#define REG_PORT_0_MAC_ADDR_4		0x92
+#define REG_PORT_0_MAC_ADDR_5		0x93
+#define REG_PORT_1_MAC_ADDR_0		0x94
+#define REG_PORT_1_MAC_ADDR_1		0x95
+#define REG_PORT_1_MAC_ADDR_2		0x96
+#define REG_PORT_1_MAC_ADDR_3		0x97
+#define REG_PORT_1_MAC_ADDR_4		0x98
+#define REG_PORT_1_MAC_ADDR_5		0x99
+
+#define REG_PORT_1_OUT_RATE_0		0x9A
+#define REG_PORT_2_OUT_RATE_0		0x9E
+#define REG_PORT_3_OUT_RATE_0		0xA2
+
+#define SWITCH_OUT_RATE_ENABLE		(1 << 7)
+
+#define REG_PORT_1_OUT_RATE_1		0x9B
+#define REG_PORT_2_OUT_RATE_1		0x9F
+#define REG_PORT_3_OUT_RATE_1		0xA3
+#define REG_PORT_1_OUT_RATE_2		0x9C
+#define REG_PORT_2_OUT_RATE_2		0xA0
+#define REG_PORT_3_OUT_RATE_2		0xA4
+#define REG_PORT_1_OUT_RATE_3		0x9D
+#define REG_PORT_2_OUT_RATE_3		0xA1
+#define REG_PORT_3_OUT_RATE_3		0xA5
+
+#define REG_PORT_OUT_RATE_0		0x00
+#define REG_PORT_OUT_RATE_1		0x01
+#define REG_PORT_OUT_RATE_2		0x02
+#define REG_PORT_OUT_RATE_3		0x03
+
+#define PORT_OUT_RATE_ADDR(port, addr)			\
+	(addr = REG_PORT_1_OUT_RATE_0 + (port) *	\
+		(REG_PORT_2_OUT_RATE_0 - REG_PORT_1_OUT_RATE_0))
+
+
+#define REG_MODE_INDICATOR		0xA6
+
+#define MODE_2_MII			(1 << 7)
+#define MODE_2_PHY			(1 << 6)
+#define PORT_1_RMII			(1 << 5)
+#define PORT_3_RMII			(1 << 4)
+#define PORT_1_MAC_MII			(1 << 3)
+#define PORT_3_MAC_MII			(1 << 2)
+#define PORT_1_COPPER			(1 << 1)
+#define PORT_2_COPPER			(1 << 0)
+
+#define MODE_MLL			0x03
+#define MODE_RLL			0x13
+#define MODE_FLL			0x01
+
+#define REG_BUF_RESERVED_Q3		0xA7
+#define REG_BUF_RESERVED_Q2		0xA8
+#define REG_BUF_RESERVED_Q1		0xA9
+#define REG_BUF_RESERVED_Q0		0xAA
+#define REG_PM_FLOW_CTRL_SELECT_1	0xAB
+#define REG_PM_FLOW_CTRL_SELECT_2	0xAC
+#define REG_PM_FLOW_CTRL_SELECT_3	0xAD
+#define REG_PM_FLOW_CTRL_SELECT_4	0xAE
+
+#define REG_PORT1_TXQ3_RATE_CTRL	0xAF
+#define REG_PORT1_TXQ2_RATE_CTRL	0xB0
+#define REG_PORT1_TXQ1_RATE_CTRL	0xB1
+#define REG_PORT1_TXQ0_RATE_CTRL	0xB2
+#define REG_PORT2_TXQ3_RATE_CTRL	0xB3
+#define REG_PORT2_TXQ2_RATE_CTRL	0xB4
+#define REG_PORT2_TXQ1_RATE_CTRL	0xB5
+#define REG_PORT2_TXQ0_RATE_CTRL	0xB6
+#define REG_PORT3_TXQ3_RATE_CTRL	0xB7
+#define REG_PORT3_TXQ2_RATE_CTRL	0xB8
+#define REG_PORT3_TXQ1_RATE_CTRL	0xB9
+#define REG_PORT3_TXQ0_RATE_CTRL	0xBA
+
+#define RATE_CTRL_ENABLE		(1 << 7)
+
+
+#define REG_INT_ENABLE			0xBB
+
+#define REG_INT_STATUS			0xBC
+
+#define INT_PORT_1_2_LINK_CHANGE	(1 << 7)
+#define INT_PORT_3_LINK_CHANGE		(1 << 2)
+#define INT_PORT_2_LINK_CHANGE		(1 << 1)
+#define INT_PORT_1_LINK_CHANGE		(1 << 0)
+
+#define REG_PAUSE_ITERATION_LIMIT	0xBD
+
+#define REG_INSERT_SRC_PVID		0xC2
+
+#define SWITCH_INS_TAG_1_PORT_2		(1 << 5)
+#define SWITCH_INS_TAG_1_PORT_3		(1 << 4)
+#define SWITCH_INS_TAG_2_PORT_1		(1 << 3)
+#define SWITCH_INS_TAG_2_PORT_3		(1 << 2)
+#define SWITCH_INS_TAG_3_PORT_1		(1 << 1)
+#define SWITCH_INS_TAG_3_PORT_2		(1 << 0)
+
+
+#define REG_POWER_MANAGEMENT		0xC3
+
+#define SWITCH_CPU_CLK_POWER_DOWN	(1 << 7)
+#define SWITCH_CLK_POWER_DOWN		(1 << 6)
+#define SWITCH_LED_SELECTION		0x30
+#define SWITCH_LED_LINK_ACT_SPEED	0x00
+#define SWITCH_LED_LINK_ACT		0x20
+#define SWITCH_LED_LINK_ACT_DUPLEX	0x10
+#define SWITCH_LED_LINK_DUPLEX		0x30
+#define SWITCH_LED_OUTPUT_MODE		(1 << 3)
+#define SWITCH_PLL_POWER_DOWN		(1 << 2)
+#define SWITCH_POWER_MANAGEMENT_MODE	0x03
+#define SWITCH_NORMAL			0x00
+#define SWITCH_ENERGY_DETECTION		0x01
+#define SWITCH_SOFTWARE_POWER_DOWN	0x02
+#define SWITCH_POWER_SAVING		0x03
+
+
+#define REG_FORWARD_INVALID_VID		0xC6
+
+#define SWITCH_FORWARD_INVALID_PORTS	0x70
+#define FORWARD_INVALID_PORT_SHIFT	4
+#define PORT_3_RMII_CLOCK_SELECTION	(1 << 3)
+#define PORT_1_RMII_CLOCK_SELECTION	(1 << 2)
+#define SWITCH_HOST_INTERFACE_MODE	0x03
+#define SWITCH_I2C_MASTER		0x00
+#define SWITCH_I2C_SLAVE		0x01
+#define SWITCH_SPI_SLAVE		0x02
+#define SWITCH_SMII			0x03
+
+
+#ifndef PHY_REG_CTRL
+#define PHY_REG_CTRL			0
+
+#define PHY_RESET_NOT			(1 << 15)
+#define PHY_LOOPBACK			(1 << 14)
+#define PHY_SPEED_100MBIT		(1 << 13)
+#define PHY_AUTO_NEG_ENABLE		(1 << 12)
+#define PHY_POWER_DOWN			(1 << 11)
+#define PHY_MII_DISABLE_NOT		(1 << 10)
+#define PHY_AUTO_NEG_RESTART		(1 << 9)
+#define PHY_FULL_DUPLEX			(1 << 8)
+#define PHY_COLLISION_TEST_NOT		(1 << 7)
+#define PHY_HP_MDIX			(1 << 5)
+#define PHY_FORCE_MDIX			(1 << 4)
+#define PHY_AUTO_MDIX_DISABLE		(1 << 3)
+#define PHY_REMOTE_FAULT_DISABLE	(1 << 2)
+#define PHY_TRANSMIT_DISABLE		(1 << 1)
+#define PHY_LED_DISABLE			(1 << 0)
+
+#define PHY_REG_STATUS			1
+
+#define PHY_100BT4_CAPABLE		(1 << 15)
+#define PHY_100BTX_FD_CAPABLE		(1 << 14)
+#define PHY_100BTX_CAPABLE		(1 << 13)
+#define PHY_10BT_FD_CAPABLE		(1 << 12)
+#define PHY_10BT_CAPABLE		(1 << 11)
+#define PHY_MII_SUPPRESS_CAPABLE_NOT	(1 << 6)
+#define PHY_AUTO_NEG_ACKNOWLEDGE	(1 << 5)
+#define PHY_REMOTE_FAULT		(1 << 4)
+#define PHY_AUTO_NEG_CAPABLE		(1 << 3)
+#define PHY_LINK_STATUS			(1 << 2)
+#define PHY_JABBER_DETECT_NOT		(1 << 1)
+#define PHY_EXTENDED_CAPABILITY		(1 << 0)
+
+#define PHY_REG_ID_1			2
+#define PHY_REG_ID_2			3
+
+#define PHY_REG_AUTO_NEGOTIATION	4
+
+#define PHY_AUTO_NEG_NEXT_PAGE_NOT	(1 << 15)
+#define PHY_AUTO_NEG_REMOTE_FAULT_NOT	(1 << 13)
+#define PHY_AUTO_NEG_SYM_PAUSE		(1 << 10)
+#define PHY_AUTO_NEG_100BT4		(1 << 9)
+#define PHY_AUTO_NEG_100BTX_FD		(1 << 8)
+#define PHY_AUTO_NEG_100BTX		(1 << 7)
+#define PHY_AUTO_NEG_10BT_FD		(1 << 6)
+#define PHY_AUTO_NEG_10BT		(1 << 5)
+#define PHY_AUTO_NEG_SELECTOR		0x001F
+#define PHY_AUTO_NEG_802_3		0x0001
+
+#define PHY_REG_REMOTE_CAPABILITY	5
+
+#define PHY_REMOTE_NEXT_PAGE_NOT	(1 << 15)
+#define PHY_REMOTE_ACKNOWLEDGE_NOT	(1 << 14)
+#define PHY_REMOTE_REMOTE_FAULT_NOT	(1 << 13)
+#define PHY_REMOTE_SYM_PAUSE		(1 << 10)
+#define PHY_REMOTE_100BTX_FD		(1 << 8)
+#define PHY_REMOTE_100BTX		(1 << 7)
+#define PHY_REMOTE_10BT_FD		(1 << 6)
+#define PHY_REMOTE_10BT			(1 << 5)
+#endif
+
+#define KSZ8863_ID_HI			0x0022
+#define KSZ8863_ID_LO			0x1430
+
+#define PHY_REG_LINK_MD			29
+
+#define PHY_START_CABLE_DIAG		(1 << 15)
+#define PHY_CABLE_DIAG_RESULT		0x6000
+#define PHY_CABLE_STAT_NORMAL		0x0000
+#define PHY_CABLE_STAT_OPEN		0x2000
+#define PHY_CABLE_STAT_SHORT		0x4000
+#define PHY_CABLE_STAT_FAILED		0x6000
+#define PHY_CABLE_10M_SHORT		(1 << 12)
+#define PHY_CABLE_FAULT_COUNTER		0x01FF
+
+#define PHY_REG_PHY_CTRL		30
+
+#define PHY_STAT_REVERSED_POLARITY	(1 << 5)
+#define PHY_STAT_MDIX			(1 << 4)
+#define PHY_FORCE_LINK			(1 << 3)
+#define PHY_POWER_SAVING_DISABLE	(1 << 2)
+#define PHY_REMOTE_LOOPBACK		(1 << 1)
+
+
+/* Default values are used in ksz_sw.h if these are not defined. */
+#define PRIO_QUEUES			4
+
+#define KS_PRIO_IN_REG			4
+
+#define TOTAL_PORT_NUM			3
+
+#define KSZ8863_COUNTER_NUM		0x20
+#define TOTAL_KSZ8863_COUNTER_NUM	(KSZ8863_COUNTER_NUM + 2)
+
+#define SWITCH_COUNTER_NUM		KSZ8863_COUNTER_NUM
+#define TOTAL_SWITCH_COUNTER_NUM	TOTAL_KSZ8863_COUNTER_NUM
+
+/* Required for common switch control in ksz_sw.c */
+#define SW_D				u8
+#define SW_R(sw, addr)			(sw)->reg->r8(sw, addr)
+#define SW_W(sw, addr, val)		(sw)->reg->w8(sw, addr, val)
+#define SW_SIZE				(1)
+#define SW_SIZE_STR			"%02x"
+#define port_r				port_r8
+#define port_w				port_w8
+
+#define P_BCAST_STORM_CTRL		REG_PORT_CTRL_0
+#define P_PRIO_CTRL			REG_PORT_CTRL_0
+#define P_TAG_CTRL			REG_PORT_CTRL_0
+#define P_MIRROR_CTRL			REG_PORT_CTRL_1
+#define P_STP_CTRL			REG_PORT_CTRL_2
+#define P_PHY_CTRL			REG_PORT_CTRL_12
+#define P_FORCE_CTRL			REG_PORT_CTRL_12
+#define P_NEG_RESTART_CTRL		REG_PORT_CTRL_13
+#define P_LINK_STATUS			REG_PORT_STATUS_0
+#define P_SPEED_STATUS			REG_PORT_STATUS_1
+#define P_RATE_LIMIT_CTRL		REG_PORT_CTRL_5
+#define P_SA_MAC_CTRL			REG_PORT_CTRL_5
+#define P_4_QUEUE_CTRL			REG_PORT_CTRL_0
+#define P_2_QUEUE_CTRL			REG_PORT_CTRL_2
+
+#define S_LINK_AGING_CTRL		REG_SWITCH_CTRL_0
+#define S_MIRROR_CTRL			REG_SWITCH_CTRL_3
+#define S_REPLACE_VID_CTRL		REG_SWITCH_CTRL_4
+#define S_802_1P_PRIO_CTRL		REG_SWITCH_CTRL_10
+#define S_UNKNOWN_DA_CTRL		REG_SWITCH_CTRL_12
+#define S_TOS_PRIO_CTRL			REG_TOS_PRIO_CTRL_0
+#define S_FLUSH_TABLE_CTRL		REG_SWITCH_CTRL_0
+#define S_TAIL_TAG_CTRL			REG_SWITCH_CTRL_1
+#define S_FORWARD_INVALID_VID_CTRL	REG_FORWARD_INVALID_VID
+#define S_INS_SRC_PVID_CTRL		REG_INSERT_SRC_PVID
+
+#define IND_ACC_TABLE(table)		((table) << 8)
+
+#endif
diff -Napur linux-linux4microchip-2022.10-orig/drivers/net/ethernet/micrel/ksz886x.h linux-linux4microchip-2022.10/drivers/net/ethernet/micrel/ksz886x.h
--- linux-linux4microchip-2022.10-orig/drivers/net/ethernet/micrel/ksz886x.h	1969-12-31 16:00:00.000000000 -0800
+++ linux-linux4microchip-2022.10/drivers/net/ethernet/micrel/ksz886x.h	2023-04-25 16:13:54.872163472 -0700
@@ -0,0 +1,1061 @@
+/**
+ * Microchip KSZ8862 Register Definitions.
+ *
+ * Copyright (C) 2015-2021 Microchip Technology Inc.
+ * Copyright (C) 2010-2015 Micrel, Inc.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation; either version 2 of the License, or
+ * (at your option) any later version.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ *
+  ----------------------------------------------------------------------------
+
+  Author  Date      Version  Description
+  PCD     04/08/10  0.0.1    Register definition for KS8861/2.
+  ----------------------------------------------------------------------------
+ */
+
+#ifndef __KSZ886X_H
+#define __KSZ886X_H
+
+
+/*
+ * QMU/BIC Registers
+ * (Bank 0, Offset 0x100 - 0x200)
+ */
+
+#define REG_CHIP_CFG_STATUS        0x108       /* CCFG */
+/* Bus in little endian mode */
+#define   LITTLE_ENDIAN_BUS_MODE     (1 << 10)
+/* External EEPROM is used */
+#define   EEPROM_PRESENCE            (1 << 9)
+/* In SPI bus mode */
+#define   SPI_BUS_MODE               (1 << 8)
+/* In 8-bit bus mode operation */
+#define   DATA_BUS_8BIT              (1 << 7)
+/* In 16-bit bus mode operation */
+#define   DATA_BUS_16BIT             (1 << 6)
+/* Data and address bus are shared */
+#define   MULTIPLEX_MODE             (1 << 4)
+/* 48-pin package */
+#define   CHIP_PACKAGE_48PIN         (1 << 1)
+/* 32-pin package for SPI host interface only */
+#define   CHIP_PACKAGE_32PIN         (1 << 0)
+
+#define REG_MAC_ADDR_0             0x110       /* MARL */
+#define REG_MAC_ADDR_1             0x111       /* MARL */
+#define REG_MAC_ADDR_2             0x112       /* MARM */
+#define REG_MAC_ADDR_3             0x113       /* MARM */
+#define REG_MAC_ADDR_4             0x114       /* MARH */
+#define REG_MAC_ADDR_5             0x115       /* MARH */
+
+#define REG_EEPROM_CTRL            0x122       /* EEPCR */
+/* 0 set EDD pad tri-state during read and set to 1 during write. */
+#define   EEPROM_DATA_WRITE          (1 << 5)
+/* Enable software to access EEPROM through bit 3 to bit 0 */
+#define   EEPROM_ACCESS_ENABLE       (1 << 4)
+/* Data receive from EEPROM (EEDI pin) */
+#define   EEPROM_DATA_IN             (1 << 3)
+/* Data transmit to EEPROM (EEDO pin) */
+#define   EEPROM_DATA_OUT            (1 << 2)
+/* Serial clock (EESK pin) */
+#define   EEPROM_SERIAL_CLOCK        (1 << 1)
+/* EEPROM chip select (EECS pin) */
+#define   EEPROM_CHIP_SELECT         (1 << 0)
+
+
+#define REG_RESET_CTRL             0x126       /* GRR */
+#define   PTP_SOFTWARE_RESET         (1 << 2)
+/* QMU soft reset (clear TxQ, RxQ) */
+#define   QMU_SOFTWARE_RESET         (1 << 1)
+/* Global soft reset (PHY, MAC, QMU) */
+#define   GLOBAL_SOFTWARE_RESET      (1 << 0)
+
+
+/* Wake On Lan Control Registers */
+
+#define REG_WOL_CTRL_ETH           0x12A       /* WFCR */
+/* Enable the magic packet pattern detection */
+#define   WOL_MAGIC_ENABLE_ETH       (1 << 7)
+/* Enable the wake up frame 3 pattern detection */
+#define   WOL_FRAME3_ENABLE_ETH      (1 << 3)
+/* Enable the wake up frame 2 pattern detection */
+#define   WOL_FRAME2_ENABLE_ETH      (1 << 2)
+/* Enable the wake up frame 1 pattern detection */
+#define   WOL_FRAME1_ENABLE_ETH      (1 << 1)
+/* Enable the wake up frame 0 pattern detection */
+#define   WOL_FRAME0_ENABLE_ETH      (1 << 0)
+
+#define REG_WOL_FRAME0_CRC0        0x130       /* WF0CRC0 */
+#define REG_WOL_FRAME0_CRC1        0x132       /* WF0CRC1 */
+#define REG_WOL_FRAME0_BYTE_MASK0  0x134       /* WF0BM0 */
+#define REG_WOL_FRAME0_BYTE_MASK1  0x136       /* WF0BM1 */
+#define REG_WOL_FRAME0_BYTE_MASK2  0x138       /* WF0BM2 */
+#define REG_WOL_FRAME0_BYTE_MASK3  0x13A       /* WF0BM3 */
+
+#define REG_WOL_FRAME1_CRC0        0x140       /* WF1CRC0 */
+#define REG_WOL_FRAME1_CRC1        0x142       /* WF1CRC1 */
+#define REG_WOL_FRAME1_BYTE_MASK0  0x144       /* WF1BM0 */
+#define REG_WOL_FRAME1_BYTE_MASK1  0x146       /* WF1BM1 */
+#define REG_WOL_FRAME1_BYTE_MASK2  0x148       /* WF1BM2 */
+#define REG_WOL_FRAME1_BYTE_MASK3  0x14A       /* WF1BM3 */
+
+#define REG_WOL_FRAME2_CRC0        0x150       /* WF2CRC0 */
+#define REG_WOL_FRAME2_CRC1        0x152       /* WF2CRC1 */
+#define REG_WOL_FRAME2_BYTE_MASK0  0x154       /* WF2BM0 */
+#define REG_WOL_FRAME2_BYTE_MASK1  0x156       /* WF2BM1 */
+#define REG_WOL_FRAME2_BYTE_MASK2  0x158       /* WF2BM2 */
+#define REG_WOL_FRAME2_BYTE_MASK3  0x15A       /* WF2BM3 */
+
+#define REG_WOL_FRAME3_CRC0        0x160       /* WF3CRC0 */
+#define REG_WOL_FRAME3_CRC1        0x162       /* WF3CRC1 */
+#define REG_WOL_FRAME3_BYTE_MASK0  0x164       /* WF3BM0 */
+#define REG_WOL_FRAME3_BYTE_MASK1  0x166       /* WF3BM1 */
+#define REG_WOL_FRAME3_BYTE_MASK2  0x168       /* WF3BM2 */
+#define REG_WOL_FRAME3_BYTE_MASK3  0x16A       /* WF3BM3 */
+
+
+/* Transmit/Receive Control Registers */
+
+/* Transmit Frame Header */
+/* Dummy address to access QMU RxQ, TxQ */
+#define REG_QDR_DUMMY              0x00
+/* Transmit Interrupt on Completion */
+#define   TX_CTRL_INTERRUPT_ON       (0x8000)
+
+#define REG_TX_CTRL                0x170       /* TXCR */
+/* Enable ICMP frame checksum generation */
+#define   TX_CTRL_ICMP_CHECKSUM      (1 << 8)
+/* Enable UDP frame checksum generation */
+#define   TX_CTRL_UDP_CHECKSUM       (1 << 7)
+/* Enable TCP frame checksum generation */
+#define   TX_CTRL_TCP_CHECKSUM       (1 << 6)
+/* Enable IP frame checksum generation */
+#define   TX_CTRL_IP_CHECKSUM        (1 << 5)
+/* Clear transmit queue, reset tx frame pointer */
+#define   TX_CTRL_FLUSH_QUEUE        (1 << 4)
+/* Enable transmit flow control */
+#define   TX_CTRL_FLOW_ENABLE        (1 << 3)
+/* Eanble adding a padding to a packet shorter than 64 bytes */
+#define   TX_CTRL_PAD_ENABLE         (1 << 2)
+/* Enable adding a CRC to the end of transmit frame */
+#define   TX_CTRL_CRC_ENABLE         (1 << 1)
+/* Enable tranmsit */
+#define   TX_CTRL_ENABLE             (1 << 0)
+#if (0)
+#define   DEFAULT_TX_CTRL	\
+	(TX_CTRL_ICMP_CHECKSUM | TX_CTRL_UDP_CHECKSUM | \
+	TX_CTRL_TCP_CHECKSUM | TX_CTRL_IP_CHECKSUM |  \
+	TX_CTRL_FLOW_ENABLE | TX_CTRL_PAD_ENABLE |   \
+	TX_CTRL_CRC_ENABLE)
+#else
+#define   DEFAULT_TX_CTRL	\
+	(TX_CTRL_FLOW_ENABLE | TX_CTRL_PAD_ENABLE |   \
+	TX_CTRL_CRC_ENABLE)
+#endif
+
+#define REG_TX_STATUS              0x172       /* TXSR */
+/* Trnasmit late collision occurs */
+#define   TX_STAT_LATE_COL           (1 << 13)
+/* Tranmsit maximum collision is reached */
+#define   TX_STAT_MAX_COL            (1 << 12)
+/* Transmit frame ID mask */
+#define   TX_FRAME_ID_MASK           (0x003F)
+#define   TX_STAT_ERRORS             (TX_STAT_MAX_COL | TX_STAT_LATE_COL)
+
+#define REG_RX_CTRL1               0x174       /* RXCR1 */
+/* Clear receive queue, reset rx frame pointer */
+#define   RX_CTRL_FLUSH_QUEUE        (1 << 15)
+/* Enable UDP frame checksum verification */
+#define   RX_CTRL_UDP_CHECKSUM       (1 << 14)
+/* Enable TCP frame checksum verification */
+#define   RX_CTRL_TCP_CHECKSUM       (1 << 13)
+/* Enable IP frame checksum verification */
+#define   RX_CTRL_IP_CHECKSUM        (1 << 12)
+/* Receive with address that pass MAC address filtering */
+#define   RX_CTRL_MAC_FILTER         (1 << 11)
+/* Enable receive flow control */
+#define   RX_CTRL_FLOW_ENABLE        (1 << 10)
+/* Eanble receive CRC error frames */
+#define   RX_CTRL_BAD_PACKET         (1 << 9)
+/* Receive multicast frames that pass the CRC hash filtering */
+#define   RX_CTRL_MULTICAST          (1 << 8)
+/* Receive all the broadcast frames */
+#define   RX_CTRL_BROADCAST          (1 << 7)
+/* Receive all the multicast frames (including broadcast frames) */
+#define   RX_CTRL_ALL_MULTICAST      (1 << 6)
+/* Receive unicast frames that match the device MAC address */
+#define   RX_CTRL_UNICAST            (1 << 5)
+/* Receive all incoming frames, regardless of frame's DA */
+#define   RX_CTRL_PROMISCUOUS        (1 << 4)
+/* Receive with address check in inverse filtering mode */
+#define   RX_CTRL_INVERSE_FILTER     (1 << 1)
+/* Enable receive */
+#define   RX_CTRL_ENABLE             (1 << 0)
+#if (0) /* Promiscuous mode */
+#define   DEFAULT_RX_CTRL1	\
+	(RX_CTRL_UDP_CHECKSUM | RX_CTRL_TCP_CHECKSUM | \
+	RX_CTRL_IP_CHECKSUM | RX_CTRL_PROMISCUOUS | \
+	RX_CTRL_FLOW_ENABLE | RX_CTRL_BROADCAST | \
+	RX_CTRL_ALL_MULTICAST | RX_CTRL_UNICAST)
+#endif
+#if (0)
+#define   DEFAULT_RX_CTRL1	\
+	(RX_CTRL_UDP_CHECKSUM | RX_CTRL_TCP_CHECKSUM | \
+	RX_CTRL_IP_CHECKSUM | RX_CTRL_MAC_FILTER | \
+	RX_CTRL_FLOW_ENABLE | RX_CTRL_BROADCAST | \
+	RX_CTRL_ALL_MULTICAST | RX_CTRL_UNICAST)
+#else
+#define   DEFAULT_RX_CTRL1	\
+	(RX_CTRL_MAC_FILTER | RX_CTRL_MULTICAST | RX_CTRL_PROMISCUOUS | \
+	RX_CTRL_FLOW_ENABLE | RX_CTRL_BROADCAST | \
+	RX_CTRL_ALL_MULTICAST | RX_CTRL_UNICAST)
+#endif
+
+
+#define REG_RX_CTRL2               0x176       /* RXCR2 */
+/* SPI Receive data burst length - single frame */
+#define   RX_SPI_BURST_FRAME         (0x0080)
+/* SPI Receive data burst length - 32-byte */
+#define   RX_SPI_BURST_32            (0x0060)
+/* SPI Receive data burst length - 16-byte */
+#define   RX_SPI_BURST_16            (0x0040)
+/* SPI Receive data burst length - 8-byte */
+#define   RX_SPI_BURST_8             (0x0020)
+/* SPI Receive data burst length - 4-byte */
+#define   RX_SPI_BURST_4             (0x0000)
+/* Enable pause frame timer */
+#define   RX_CTRL_PAUSE_TIMER        (1 << 8)
+/* No checksum generation and verification if IPv6 UDP is fragment */
+#define   RX_CTRL_IPV6_UDP_FRAG_PASS (1 << 4)
+/* Receive pass IPv6 UDP frame with UDP checksum is zero */
+#define   RX_CTRL_IPV6_UDP_ZERO_PASS (1 << 3)
+/* Enable UDP Lite frame checksum generation and verification */
+#define   RX_CTRL_UDP_LITE_CHECKSUM  (1 << 2)
+/* Enable ICMP frame checksum verification */
+#define   RX_CTRL_ICMP_CHECKSUM      (1 << 1)
+/* Receive drop frame if the SA is same as device MAC address */
+#define   RX_CTRL_BLOCK_MAC          (1 << 0)
+#define   DEFAULT_RX_CTRL2	\
+	(RX_CTRL_IPV6_UDP_FRAG_PASS | RX_CTRL_UDP_LITE_CHECKSUM | \
+	RX_CTRL_ICMP_CHECKSUM | RX_CTRL_PAUSE_TIMER)
+
+
+#define REG_TX_MEM_INFO            0x178       /* TXMIR */
+/* The amount of memory available in TXQ */
+#define   TX_MEM_AVAILABLE_MASK      (0x1FFF)
+
+#define REG_RX_FHR_STATUS          0x17C       /* RXFHSR */
+/* Frame in the receive packet memory is valid */
+#define   RX_VALID                   (1 << 15)
+/* ICMP checksum field doesn't match */
+#define   RX_ICMP_ERROR              (1 << 13)
+/* IP checksum field doesn't match */
+#define   RX_IP_ERROR                (1 << 12)
+/* TCP checksum field doesn't match */
+#define   RX_TCP_ERROR               (1 << 11)
+/* UDP checksum field doesn't match */
+#define   RX_UDP_ERROR               (1 << 10)
+/* Received frame is a broadcast frame */
+#define   RX_BROADCAST               (1 << 7)
+/* Received frame is a multicast frame */
+#define   RX_MULTICAST               (1 << 6)
+/* Received frame is a unicast frame */
+#define   RX_UNICAST                 (1 << 5)
+/* Received frame has runt error */
+#define   RX_PHY_ERROR               (1 << 4)
+/* Received frame is an Ethernet-type frame */
+#define   RX_FRAME_ETHER             (1 << 3)
+/* Received frame length exceeds max size 0f 2048 bytes */
+#define   RX_TOO_LONG                (1 << 2)
+/* Received frame was demaged by a collision */
+#define   RX_RUNT_ERROR              (1 << 1)
+/* Received frame has a CRC error */
+#define   RX_BAD_CRC                 (1 << 0)
+#define   RX_ERRORS		\
+	(RX_BAD_CRC | RX_TOO_LONG | RX_RUNT_ERROR | RX_PHY_ERROR | \
+	RX_ICMP_ERROR | RX_IP_ERROR | RX_TCP_ERROR | RX_UDP_ERROR)
+
+#define REG_RX_FHR_BYTE_CNT        0x17E       /* RXFHBCR */
+/* Received frame byte size mask */
+#define   RX_BYTE_CNT_MASK           (0x0FFF)
+
+#define REG_TXQ_CMD                0x180       /* TXQCR */
+/* Enable enqueue tx frames from tx buffer automatically */
+#define   TXQ_AUTO_ENQUEUE           (1 << 2)
+/* Enable generate interrupt when tx memory is available */
+#define   TXQ_MEM_AVAILABLE_INT      (1 << 1)
+/* Enable enqueue tx frames one frame at a time */
+#define   TXQ_ENQUEUE                (1 << 0)
+
+#define REG_RXQ_CMD                0x182       /* RXQCR */
+/* RX interrupt is occured by timer duration */
+#define   RXQ_STAT_TIME_INT          (1 << 12)
+/* RX interrupt is occured by byte count threshold */
+#define   RXQ_STAT_BYTE_CNT_INT      (1 << 11)
+/* RX interrupt is occured by frame count threshold */
+#define   RXQ_STAT_FRAME_CNT_INT     (1 << 10)
+/* Enable adding 2-byte before frame header for IP aligned with DWORD */
+#define   RXQ_TWOBYTE_OFFSET         (1 << 9)
+/* Enable RX interrupt by timer duration */
+#define   RXQ_TIME_INT               (1 << 7)
+/* Enable RX interrupt by byte count threshold */
+#define   RXQ_BYTE_CNT_INT           (1 << 6)
+/* Enable RX interrupt by frame count threshold */
+#define   RXQ_FRAME_CNT_INT          (1 << 5)
+/* Enable release rx frames from rx buffer automatically */
+#define   RXQ_AUTO_DEQUEUE           (1 << 4)
+/* Start QMU transfer operation */
+#define   RXQ_START                  (1 << 3)
+/* Manual dequeue (release the current frame from RxQ) */
+#define   RXQ_CMD_FREE_PACKET        (1 << 0)
+#if (0)
+#define   RXQ_CMD_CNTL		(RXQ_FRAME_CNT_INT|RXQ_AUTO_DEQUEUE)
+#endif
+#if (1)
+#define   RXQ_CMD_CNTL		\
+	(RXQ_FRAME_CNT_INT|RXQ_TWOBYTE_OFFSET|RXQ_AUTO_DEQUEUE)
+#endif
+
+#define REG_TX_ADDR_PTR            0x184       /* TXFDPR */
+#define REG_RX_ADDR_PTR            0x186       /* RXFDPR */
+/* Enable Frame data pointer increments automatically */
+#define   ADDR_PTR_AUTO_INC          (1 << 14)
+/* 1: WRN active to write data valid sample time is 4ns */
+#define   WRN_WRITE_4NS              (1 << 12)
+/* Set device in Big Endian mode */
+#define   BIG_ENDIAN_MODE            (1 << 11)
+
+#define REG_RX_TIME_THRES          0x18C       /* RXDTTR */
+/* Set receive timer duration threshold */
+#define   RX_TIME_THRESHOLD_MASK     (0xFFFF)
+
+#define REG_RX_BYTE_CNT_THRES      0x18E       /* RXDBCTR */
+/* Set receive byte count threshold */
+#define   RX_BYTE_THRESHOLD_MASK     (0xFFFF)
+
+#define REG_INT_MASK               0x190       /* IER */
+/* Enable link change interrupt */
+#define   INT_PHY                    (1 << 15)
+/* Enable transmit done interrupt */
+#define   INT_TX_ETH                 (1 << 14)
+/* Enable receive interrupt */
+#define   INT_RX_ETH                 (1 << 13)
+/* Enable timestamp event detection interrupt */
+#define   INT_TIMESTAMP              (1 << 12)
+/* Enable receive overrun interrupt */
+#define   INT_RX_OVERRUN_ETH         (1 << 11)
+/* Enable trigger outout interrupt */
+#define   INT_TRIG_OUTPUT            (1 << 10)
+/* Enable transmit process stopped interrupt */
+#define   INT_TX_STOPPED_ETH         (1 << 9)
+/* Enable receive process stopped interrupt */
+#define   INT_RX_STOPPED_ETH         (1 << 8)
+/* Enable transmit space available interrupt */
+#define   INT_TX_SPACE               (1 << 6)
+/* Enable WOL on receive wake-up frame detect interrupt */
+#define   INT_RX_WOL_FRAME           (1 << 5)
+/* Enable WOL on receive magic packet detect interrupt */
+#define   INT_RX_WOL_MAGIC           (1 << 4)
+/* Enable WOL on link up detect interrupt */
+#define   INT_RX_WOL_LINKUP          (1 << 3)
+/* Enable WOL on energy detect interrupt */
+#define   INT_RX_WOL_ENERGY          (1 << 2)
+/* Enable receive SPI bus error interrupt */
+#define   INT_RX_SPI_ERROR           (1 << 1)
+/* Enable delay generate WOL on energy detect */
+#define   RX_WOL_DELAY_ENERGY        (1 << 0)
+#define   INT_MASK                    (INT_RX | INT_TX | INT_PHY)
+
+#define REG_INT_STATUS             0x192       /* ISR */
+
+#define REG_RX_FRAME_CNT_THRES     0x19C       /* RXFCTFC */
+/* Set receive frame count threshold mask */
+#define   RX_FRAME_THRESHOLD_MASK    (0x00FF)
+
+#define REG_TX_TOTAL_FRAME_SIZE    0x19E       /* TXNTFSR */
+/* Set next total tx frame size mask */
+#define   TX_TOTAL_FRAME_SIZE_MASK   (0xFFFF)
+
+
+/* QMU MAC Address Hash Table Control Registers */
+
+#define REG_MAC_HASH_0             0x1A0       /* MAHTR0 */
+#define REG_MAC_HASH_1             0x1A1
+
+#define REG_MAC_HASH_2             0x1A2       /* MAHTR1 */
+#define REG_MAC_HASH_3             0x1A3
+
+#define REG_MAC_HASH_4             0x1A4       /* MAHTR2 */
+#define REG_MAC_HASH_5             0x1A5
+
+#define REG_MAC_HASH_6             0x1A6       /* MAHTR3 */
+#define REG_MAC_HASH_7             0x1A7
+
+
+/* QMU Receive Queue Watermark Control Registers */
+
+#define REG_RX_LOW_WATERMARK       0x1B0       /* FCLWR */
+/* Set QMU RxQ low watermark mask */
+#define   RX_LOW_WATERMARK_MASK      (0x0FFF)
+
+#define REG_RX_HIGH_WATERMARK      0x1B2       /* FCHWR */
+/* Set QMU RxQ high watermark mask */
+#define   RX_HIGH_WATERMARK_MASK     (0x0FFF)
+
+#define REG_RX_OVERRUN_WATERMARK   0x1B4       /* FCOWR */
+/* Set QMU RxQ overrun watermark mask */
+#define   RX_OVERRUN_WATERMARK_MASK  (0x0FFF)
+
+
+/*
+ * Switch Registers
+ * (Bank 0, Offset 0 - 0x0ff)
+ */
+
+/* Switch Global Control registers  */
+
+#define REG_SWITCH_SIDER           0x00        /* SIDER */
+/* Enable switch */
+#define   SWITCH_START               (1 << 0)
+#define	  CIDER_ID_8463			(0x8440)
+#define	  CIDER_ID_8463_RLI		(0x8450)
+#define   CIDER_ID_MASK			(~0xf)
+
+#define REG_SWITCH_CTRL_1          0x02        /* SGCR1 */
+/* pass all frames */
+#define   SWITCH_PASS_ALL            (1 << 15)
+/* IEEE 802.3x tx flow control enable */
+#define   SWITCH_TX_FLOW_CTRL        (1 << 13)
+/* IEEE 802.3x rx flow control enable */
+#define   SWITCH_RX_FLOW_CTRL        (1 << 12)
+/* check frame length field in the IEEE pkts */
+#define   SWITCH_CHECK_LENGTH        (1 << 11)
+/* enable Aging */
+#define   SWITCH_AGING_ENABLE        (1 << 10)
+/* enable fast aging */
+#define   SWITCH_FAST_AGING          (1 << 9)
+/* enable aggressive back off */
+#define   SWITCH_AGGR_BACKOFF        (1 << 8)
+/* pass flow control pkts */
+#define   SWITCH_PASS_PAUSE          (1 << 3)
+/* port 3 in turbo-mii mode */
+#define   SWITCH_PORT3_TURBO_MII     (1 << 2)
+/* fast aging when from link up to down */
+#define   SWITCH_LINK_AUTO_AGING     (1 << 0)
+
+#define REG_SWITCH_CTRL_2          0x04        /* SGCR2 */
+/* enable 802.1Q */
+#define   SWITCH_VLAN_ENABLE         (1 << 15)
+/* enable IGMP */
+#define   SWITCH_IGMP_SNOOP          (1 << 14)
+/* enable ipv6 MLD */
+#define   SWITCH_IPV6_MLD_SNOOP      (1 << 13)
+/* enable ipv6 MLD option */
+#define   SWITCH_IPV6_MLD_OPTION     (1 << 12)
+#if 0
+/* always tx higher priority first */
+#define   PRIORITY_SCHEME_SELECT     (1 << 11)
+#endif
+/* select rx AND tx sniff mode */
+#define   SWITCH_MIRROR_RX_TX        (1 << 8)
+/* all pkts can't cross VLAN boundary */
+#define   UNICAST_VLAN_BOUNDARY      (1 << 7)
+/* broadcast storm protection not include multicast pkt */
+#define   MULTICAST_STORM_DISABLE    (1 << 6)
+/* select carrier sense based backpressure */
+#define   SWITCH_BACK_PRESSURE       (1 << 5)
+/* select fair mode */
+#define   FAIR_FLOW_CTRL             (1 << 4)
+/* switch not drop pkts when more than 16 collision happen */
+#define   NO_EXC_COLLISION_DROP      (1 << 3)
+/* accept huge pkts - upto 1916 bytes */
+#define   SWITCH_HUGE_PACKET         (1 << 2)
+/* accept legal max pkt - uptp 1536 bytes */
+#define   SWITCH_LEGAL_PACKET        (1 << 1)
+/* each ports pre-allocated 48 buffers */
+#define   SWITCH_BUF_RESERVE         (1 << 0)
+
+#define REG_SWITCH_CTRL_3          0x06        /* SGCR3 */
+/* enable switch host port in half duplex */
+#define   SWITCH_HALF_DUPLEX         (1 << 6)
+/* enable switch host port flow control */
+#define   SWITCH_FLOW_CTRL           (1 << 5)
+/* replace NULL VID with port VID */
+#define   SWITCH_REPLACE_VID         (1 << 3)
+/* Broadcast storm protection rate bit[7:0] mask */
+#define   BROADCAST_STORM_RATE_LO    (0xFF00)
+/* Broadcast storm protection rate bit[10:8] mask */
+#define   BROADCAST_STORM_RATE_HI    (0x0007)
+
+#define BROADCAST_STORM_RATE		0x07FF
+
+#define REG_SWITCH_CTRL_4          0x08        /* SGCR4 */
+
+#define REG_SWITCH_CTRL_6          0x0C        /* SGCR6 */
+#define   SWITCH_802_1P_MAP_MASK      3
+#define   SWITCH_802_1P_MAP_SHIFT     2
+
+#define REG_SWITCH_CTRL_7          0x0E        /* SGCR7 */
+/* enable tx unknow DA pkts to specified port */
+#define   SWITCH_UNKNOWN_DA_ENABLE   (1 << 7)
+/* Set driver strength to 16mA */
+#define   SWITCH_DRIVER_16MA         (1 << 6)
+/* Invert port 3 txc */
+#define   SWITCH_PORT3_INVERT_TXC    (1 << 4)
+/* tx unknow DA pkts to port3 */
+#define   SWITCH_UNKNOWN_DA_2_PORT3  (1 << 2)
+/* tx unknow DA pkts to port3 */
+#define   SWITCH_UNKNOWN_DA_2_PORT2  (1 << 1)
+/* tx unknow DA pkts to port3 */
+#define   SWITCH_UNKNOWN_DA_2_PORT1  (1 << 0)
+
+#define REG_SWITCH_MAC_ADDR_0      0x10        /* MACAR1 */
+#define REG_SWITCH_MAC_ADDR_1      0x11
+#define REG_SWITCH_MAC_ADDR_2      0x12        /* MACAR2 */
+#define REG_SWITCH_MAC_ADDR_3      0x13
+#define REG_SWITCH_MAC_ADDR_4      0x14        /* MACAR3 */
+#define REG_SWITCH_MAC_ADDR_5      0x15
+
+
+#define REG_TOS_PRIORITY_CTRL_1    0x16       /* TOSR1 */
+#define REG_TOS_PRIORITY_CTRL_2    0x18       /* TOSR2 */
+#define REG_TOS_PRIORITY_CTRL_3    0x1A       /* TOSR3 */
+#define REG_TOS_PRIORITY_CTRL_4    0x1C       /* TOSR4 */
+#define REG_TOS_PRIORITY_CTRL_5    0x1E       /* TOSR5 */
+#define REG_TOS_PRIORITY_CTRL_6    0x20       /* TOSR6 */
+#define REG_TOS_PRIORITY_CTRL_7    0x22       /* TOSR7 */
+#define REG_TOS_PRIORITY_CTRL_8    0x24       /* TOSR8 */
+
+#define REG_IND_DATA_1             0x26        /* IADR1 */
+/* read is still in progress */
+#define   TABLE_READ_NOT_COMPLETE    (1 << 7)
+
+#define REG_IND_DATA_2             0x28        /* IADR2 */
+#define REG_IND_DATA_3             0x2A        /* IADR3 */
+#define REG_IND_DATA_4             0x2C        /* IADR4 */
+#define REG_IND_DATA_5             0x2E        /* IADR5 */
+/* MIB count overflow */
+#define   MIB_COUNTER_OVERFLOW       (1 << 31)
+/* MIB count value is valid */
+#define   MIB_COUNTER_VALID          (1 << 30)
+/* MIB count value mask */
+#define   MIB_COUNTER_VALUE          (0x3FFFFFFF)
+#ifndef MIB_COUNTER_PACKET_DROPPED
+/* for pkt drop overflow */
+#define   MIB_COUNTER_PACKET_DROPPED (0xFFFF)
+#endif
+
+/* Port 1 TX drop pkt MIB count address */
+#define   MIB_COUNTER_PACKET_DROPPED_TX_0  0x100
+/* Port 1 RX drop pkt MIB count address */
+#define   MIB_COUNTER_PACKET_DROPPED_RX_0  0x103
+
+#define REG_IND_IACR               0x30        /* IACR */
+/* read operation */
+#define   TABLE_READ                 (1 << 12)
+/* access static MAC address table */
+#define   TABLE_STATIC_MAC           (0 << 10)
+/* access VLAN table */
+#define   TABLE_VLAN                 (1 << 10)
+/* access dynamic MAC address table */
+#define   TABLE_DYNAMIC_MAC          (2 << 10)
+/* access MIB count table */
+#define   TABLE_MIB                  (3 << 10)
+#define   TABLE_ENTRY_MASK           (0x03ff)
+
+
+/* PHY 1 registers  */
+
+#define PHY1_REG_CTRL               0x4C        /* P1MBCR */
+#define PHY1_REG_STATUS             0x4E        /* P1MBSR */
+#define PHY1_REG_ID_1               0x50        /* PHY1ILR */
+#define PHY1_REG_ID_2               0x52        /* PHY1IHR */
+#define PHY1_REG_AUTO_NEGOTIATION   0x54        /* P1ANAR */
+#define PHY1_REG_REMOTE_CAPABILITY  0x56        /* P1ANLPR */
+
+#ifndef PHY_REG_CTRL
+#define PHY_REG_CTRL               0x4C        /* P1MBCR */
+/* PHY soft reset */
+#define   PHY_RESET                  (1 << 15)
+/* phy loopback */
+#define   PHY_LOOPBACK               (1 << 14)
+/* force link speed to 100bt */
+#define   PHY_SPEED_100MBIT          (1 << 13)
+/* enable auto-nego */
+#define   PHY_AUTO_NEG_ENABLE        (1 << 12)
+/* phy power down */
+#define   PHY_POWER_DOWN             (1 << 11)
+/* isolate phy */
+#define   PHY_MII_DISABLE            (1 << 10)
+/* restart auto-nego */
+#define   PHY_AUTO_NEG_RESTART       (1 << 9)
+/* force link in full duplex mode */
+#define   PHY_FULL_DUPLEX            (1 << 8)
+/* test collision */
+#define   PHY_COLLISION_TEST         (1 << 7)
+/* enable HP auto MDIX mode */
+#define   PHY_HP_MDIX                (1 << 5)
+/* force MDIX mode */
+#define   PHY_FORCE_MDIX             (1 << 4)
+/* disable auto MDIX mode */
+#define   PHY_AUTO_MDIX_DISABLE      (1 << 3)
+/* disable far end fail detection */
+#define   PHY_REMOTE_FAULT_DISABLE   (1 << 2)
+/* disable tx */
+#define   PHY_TRANSMIT_DISABLE       (1 << 1)
+/* disable LED */
+#define   PHY_LED_DISABLE            (1 << 0)
+
+#define PHY_REG_STATUS             0x4E        /* P1MBSR */
+/* T4 capable */
+#define   PHY_100BT4_CAPABLE         (1 << 15)
+/* 100BT full duplex capable */
+#define   PHY_100BTX_FD_CAPABLE      (1 << 14)
+/* 100BT half duplex capable */
+#define   PHY_100BTX_CAPABLE         (1 << 13)
+/* 10BT full duplex capable */
+#define   PHY_10BT_FD_CAPABLE        (1 << 12)
+/* 10BT half duplex capable */
+#define   PHY_10BT_CAPABLE           (1 << 11)
+/* preamble suppressed */
+#define   PHY_MII_SUPPRESS_CAPABLE   (1 << 6)
+/* auto-nego complete */
+#define   PHY_AUTO_NEG_ACKNOWLEDGE   (1 << 5)
+/* far end fault detected */
+#define   PHY_REMOTE_FAULT           (1 << 4)
+/* auto-nego capable */
+#define   PHY_AUTO_NEG_CAPABLE       (1 << 3)
+/* link is up */
+#define   PHY_LINK_STATUS            (1 << 2)
+/* Jabber test */
+#define   PHY_JABBER_DETECT          (1 << 1)
+/* extended capable */
+#define   PHY_EXTENDED_CAPABILITY    (1 << 0)
+
+#define PHY_REG_ID_1               0x50        /* PHY1ILR */
+#define PHY_REG_ID_2               0x52        /* PHY1IHR */
+
+#define PHY_REG_AUTO_NEGOTIATION   0x54        /* P1ANAR */
+/* next page */
+#define   PHY_AUTO_NEG_NEXT_PAGE     (1 << 15)
+/* remote fault */
+#define   PHY_AUTO_NEG_REMOTE_FAULT  (1 << 13)
+/* advertise pause ability */
+#define   PHY_AUTO_NEG_SYM_PAUSE     (1 << 10)
+/* advertise 100 full */
+#define   PHY_AUTO_NEG_100BTX_FD     (1 << 8)
+/* advertise 100 half */
+#define   PHY_AUTO_NEG_100BTX        (1 << 7)
+/* advertise 10 full */
+#define   PHY_AUTO_NEG_10BT_FD       (1 << 6)
+/* advertise 10 half */
+#define   PHY_AUTO_NEG_10BT          (1 << 5)
+#define   PHY_AUTO_NEG_SELECTOR      (0x001F)
+#define   PHY_AUTO_NEG_802_3         (0x0001)
+
+#define PHY_REG_REMOTE_CAPABILITY  0x56        /* P1ANLPR */
+/* remote next page */
+#define   PHY_REMOTE_NEXT_PAGE       (1 << 15)
+/* link partner pause capability */
+#define   PHY_REMOTE_SYM_PAUSE       (1 << 10)
+/* link partner 100 full capability */
+#define   PHY_REMOTE_100BTX_FD       (1 << 8)
+/* link partner 100 half capability */
+#define   PHY_REMOTE_100BTX          (1 << 7)
+/* link partner 10 full capability */
+#define   PHY_REMOTE_10BT_FD         (1 << 6)
+/* link partner 10 hal capability */
+#define   PHY_REMOTE_10BT            (1 << 5)
+#endif
+
+/* PHY 2 registers  */
+
+#define PHY2_REG_CTRL              (PHY1_REG_CTRL+0x0C)    /* P2MBCR */
+#define PHY2_REG_STATUS            (PHY1_REG_STATUS+0x0C)  /* P2MBSR */
+#define PHY2_REG_ID_1              (PHY1_REG_ID_1+0x0C)    /* PHY2ILR */
+#define PHY2_REG_ID_2              (PHY1_REG_ID_2+0x0C)    /* PHY2IHR */
+/* P2ANAR */
+#define PHY2_REG_AUTO_NEGOTIATION  (PHY1_REG_AUTO_NEGOTIATION+0x0C)
+/* P2ANLPR */
+#define PHY2_REG_REMOTE_CAPABILITY (PHY1_REG_REMOTE_CAPABILITY+0x0C)
+
+/* PHY 1 LinkMD registers  */
+
+#define PHY_REG_LINK_MD            0x64        /* P1VCT */
+/* start cable diag test */
+#define   PHY_START_CABLE_DIAG       (1 << 15)
+/* cable test result mask */
+#define   PHY_CABLE_DIAG_RESULT      (0x6000)
+/* cable is normal */
+#define   PHY_CABLE_STAT_NORMAL      (0x0000)
+/* cable is open */
+#define   PHY_CABLE_STAT_OPEN        (0x2000)
+/* cable is short */
+#define   PHY_CABLE_STAT_SHORT       (0x4000)
+/* cable test fail */
+#define   PHY_CABLE_STAT_FAILED      (0x6000)
+/* cable is less than 10 meter */
+#define   PHY_CABLE_10M_SHORT        (1 << 12)
+/* cable fail in distance count mask */
+#define   PHY_CABLE_FAULT_COUNTER    (0x01FF)
+
+#define PHY_REG_PHY_CTRL           0x66        /* P1PHYCTRL */
+/* phy polarity reverse */
+#define   PHY_STAT_REVERSED_POLARITY (1 << 5)
+/* phy is in MDI mode */
+#define   PHY_STAT_MDIX              (1 << 4)
+/* force link pass */
+#define   PHY_FORCE_LINK             (1 << 3)
+/* enable phy power saving */
+#define   PHY_POWER_SAVING_DISABLE   (1 << 2)
+/* enable pyh remote loopback */
+#define   PHY_REMOTE_LOOPBACK        (1 << 1)
+
+/* PHY 2 LinkMD registers  */
+
+#define PHY2_REG_LINK_MD           (PHY_REG_LINK_MD+0x04)    /* P2VCT */
+#define PHY2_REG_PHY_CTRL          (PHY_REG_PHY_CTRL+0x04)   /* P2PHYCTRL */
+
+
+/* Port 1 registers  */
+
+#define REG_PORT1_CTRL1            0x6C        /* P1CR1 */
+/* enable SA MAC2 */
+#define   PORT_SA_MAC2               (1 << 11)
+/* enable SA MAC1 */
+#define   PORT_SA_MAC1               (1 << 10)
+/* enable drop tagged ingress pkts */
+#define   PORT_DROP_TAG_PKT          (1 << 9)
+/* enable split port tx queue to two queues */
+#define   PORT_SPLIT_2QUEUE          (1 << 8)
+#define   PORT_2_QUEUES_ENABLE       (1 << 8)
+/* enable broadcast storm protection */
+#define   PORT_BROADCAST_STORM       (1 << 7)
+/* enable Diffserv QoS */
+#define   PORT_DIFFSERV_ENABLE       (1 << 6)
+/* enable 802.1p QoS */
+#define   PORT_802_1P_ENABLE         (1 << 5)
+/* port-base QoS mask */
+#define   PORT_BASED_PRIORITY_MASK   (0x0018)
+#define   PORT_BASED_PRIORITY_BASE   (0x03)
+#define   PORT_BASED_PRIORITY_SHIFT  3
+/* port-base priority queue 0 QoS */
+#define   PORT_PORT_PRIORITY_0       (0x0000)
+/* port-base priority queue 1 QoS */
+#define   PORT_PORT_PRIORITY_1       (0x0008)
+/* port-base priority queue 2 QoS */
+#define   PORT_PORT_PRIORITY_2       (0x0010)
+/* port-base priority queue 3 QoS */
+#define   PORT_PORT_PRIORITY_3       (0x0018)
+/* enable vlan tag insertion */
+#define   PORT_INSERT_TAG            (1 << 2)
+/* enable vlan tag removal */
+#define   PORT_REMOVE_TAG            (1 << 1)
+/* enable split port tx queue to four queues */
+#define   PORT_PRIORITY_ENABLE       (1 << 0)
+#define   PORT_4_QUEUES_ENABLE       (1 << 0)
+
+#define REG_PORT1_CTRL2            0x6E        /* P1CR2 */
+/* remote loopback */
+#define   PORT_REMOTE_LOOPBACK       (1 << 15)
+/* VLAN ingress filtering */
+#define   PORT_INGRESS_FILTER        (1 << 14)
+/* discard pkts whose VID not match port default VID */
+#define   PORT_DISCARD_NON_VID       (1 << 13)
+/* force flow control */
+#define   PORT_FORCE_FLOW_CTRL       (1 << 12)
+/* enable back pressure */
+#define   PORT_BACK_PRESSURE         (1 << 11)
+/* enable tx */
+#define   PORT_TX_ENABLE             (1 << 10)
+/* enable rx */
+#define   PORT_RX_ENABLE             (1 << 9)
+/* disable learning */
+#define   PORT_LEARN_DISABLE         (1 << 8)
+/* select this port as sniffer port */
+#define   PORT_MIRROR_SNIFFER        (1 << 7)
+/* select this port as monitored rx pkts */
+#define   PORT_MIRROR_RX             (1 << 6)
+/* select this port as monitored tx pkts */
+#define   PORT_MIRROR_TX             (1 << 5)
+/* enable replace 802.1p priority field */
+#define   PORT_802_1P_REMAPPING      (1 << 3)
+/* Port VLAN membership mask */
+#define   PORT_VLAN_MEMBERSHIP       (0x07)
+
+#define REG_PORT_1_CTRL_VID        0x70        /* P1VIDCR */
+
+#define REG_PORT1_CTRL3            0x72        /* P1CR3 */
+/* Ingress rate limit mode mask */
+#define   PORT_INGRESS_LIMIT_MODE    (0x0C)
+/*  - limit all frames */
+#define   PORT_INGRESS_ALL           (0x00)
+/*  - limit broadcast, multicast, flooded unicast pkts */
+#define   PORT_INGRESS_UNICAST       (0x04)
+/*  - limit broadcast, multicast pkts only */
+#define   PORT_INGRESS_MULTICAST     (0x08)
+/*  - limit broadcast pkts only */
+#define   PORT_INGRESS_BROADCAST     (0x0C)
+/* rate limiting include IFG bytes */
+#define   PORT_COUNT_IFG             (1 << 1)
+/* rate limiting include preamble bytes */
+#define   PORT_COUNT_PREAMBLE        (1 << 0)
+
+#define REG_PORT_1_IN_RATE0        0x74        /* P1IRCR0 */
+#define REG_PORT_1_IN_RATE1        0x76        /* P1IRCR1 */
+
+#define REG_PORT_1_OUT_RATE0       0x78        /* P1ERCR0 */
+/* enable port egress rate limit flow control */
+#define   PORT_EGRESS_LIMIT_FLOW_EN  (1 << 7)
+
+#define REG_PORT_1_OUT_RATE1       0x7A        /* P1ERCR1 */
+
+#define REG_PORT_1_LINK_MD_CTRL    0x7C        /* P1SCSLMD */
+/* cable less than 10 meter short */
+#define   PORT_CABLE_10M_SHORT       (1 << 15)
+/* cable test result mask */
+#define   PORT_CABLE_DIAG_RESULT     (0x6000)
+/* cable is normal */
+#define   PORT_CABLE_STAT_NORMAL     (0x0000)
+/* cable is open */
+#define   PORT_CABLE_STAT_OPEN       (0x2000)
+/* cable is short */
+#define   PORT_CABLE_STAT_SHORT      (0x4000)
+/* cable test fail */
+#define   PORT_CABLE_STAT_FAILED     (0x6000)
+/* enable cable diag test */
+#define   PORT_START_CABLE_DIAG      (1 << 12)
+/* force link pass */
+#define   PORT_FORCE_LINK            (1 << 11)
+/* enable phy power saving */
+#define   PORT_POWER_SAVING          (1 << 10)
+/* phy remote loopback */
+#define   PORT_PHY_REMOTE_LOOPBACK   (1 << 9)
+/* cable fail in distance count mask */
+#define   PORT_CABLE_FAULT_COUNTER   (0x01FF)
+
+#define REG_PORT_1_CTRL_4          0x7E        /* P1CR4 */
+/* turn off all port's LEDs */
+#define   PORT_LED_OFF               (1 << 15)
+/* disable tx */
+#define   PORT_TX_DISABLE            (1 << 14)
+/* restart auto-nego */
+#define   PORT_AUTO_NEG_RESTART      (1 << 13)
+/* disable far end fail detection */
+#define   PORT_REMOTE_FAULT_DISABLE  (1 << 12)
+/* phy power down */
+#define   PORT_POWER_DOWN            (1 << 11)
+/* disable auto MDIX mode */
+#define   PORT_AUTO_MDIX_DISABLE     (1 << 10)
+/* force MDIX mode */
+#define   PORT_FORCE_MDIX            (1 << 9)
+/* phy loopback */
+#define   PORT_LOOPBACK              (1 << 8)
+/* enable auto-nego */
+#define   PORT_AUTO_NEG_ENABLE       (1 << 7)
+/* force link speed to 100bt */
+#define   PORT_FORCE_100_MBIT        (1 << 6)
+/* force link in full duplex mode */
+#define   PORT_FORCE_FULL_DUPLEX     (1 << 5)
+/* advertise pause ability */
+#define   PORT_AUTO_NEG_SYM_PAUSE    (1 << 4)
+/* advertise 100 full */
+#define   PORT_AUTO_NEG_100BTX_FD    (1 << 3)
+/* advertise 100 half */
+#define   PORT_AUTO_NEG_100BTX       (1 << 2)
+/* advertise 10 full */
+#define   PORT_AUTO_NEG_10BT_FD      (1 << 1)
+/* advertise 10 half */
+#define   PORT_AUTO_NEG_10BT         (1 << 0)
+
+#define REG_PORT_1_STATUS          0x80        /* P1SR */
+/* enable HP auto MDIX mode */
+#define   PORT_HP_MDIX               (1 << 15)
+/* phy polarity reverse */
+#define   PORT_REVERSED_POLARITY     (1 << 13)
+/* tx flow control is active */
+#define   PORT_RX_FLOW_CTRL          (1 << 12)
+/* rx flow control is active */
+#define   PORT_TX_FLOW_CTRL          (1 << 11)
+/* link speed is in 100BT */
+#define   PORT_STAT_SPEED_100MBIT    (1 << 10)
+/* Link is in full duplex */
+#define   PORT_STAT_FULL_DUPLEX      (1 << 9)
+/* far end fault detected */
+#define   PORT_REMOTE_FAULT          (1 << 8)
+/* phy is in MDI mode */
+#define   PORT_MDIX_STATUS           (1 << 7)
+/* auto-nego complete */
+#define   PORT_AUTO_NEG_COMPLETE     (1 << 6)
+/* link is up */
+#define   PORT_STATUS_LINK_GOOD      (1 << 5)
+/* link partner pause capability */
+#define   PORT_REMOTE_SYM_PAUSE      (1 << 4)
+/* link partner 100 full capability */
+#define   PORT_REMOTE_100BTX_FD      (1 << 3)
+/* link partner 100 half capability */
+#define   PORT_REMOTE_100BTX         (1 << 2)
+/* link partner 10 full capability */
+#define   PORT_REMOTE_10BT_FD        (1 << 1)
+/* link partner 10 hal capability */
+#define   PORT_REMOTE_10BT           (1 << 0)
+
+/* Port 2 registers  */
+
+#define REG_PORT2_CTRL1            (REG_PORT1_CTRL1 + 0x18)   /* P2CR1 */
+#define REG_PORT2_CTRL2            (REG_PORT1_CTRL2 + 0x18)   /* P2CR2 */
+#define REG_PORT_2_CTRL_VID        (REG_PORT_1_CTRL_VID+0x18) /* P2VIDCR */
+#define REG_PORT2_CTRL3            (REG_PORT1_CTRL3 + 0x18)   /* P2CR3 */
+#define REG_PORT_2_IN_RATE0        (REG_PORT_1_IN_RATE0+0x18) /* P2IRCR0 */
+#define REG_PORT_2_IN_RATE1        (REG_PORT_1_IN_RATE1+0x18) /* P2IRCR1 */
+#define REG_PORT_2_OUT_RATE0       (REG_PORT_1_OUT_RATE0+0x18)/* P2ERCR0 */
+#define REG_PORT_2_OUT_RATE1       (REG_PORT_1_OUT_RATE1+0x18)/* P2ERCR1 */
+#define REG_PORT_2_LINK_MD_CTRL    (REG_PORT_1_LINK_MD_CTRL+0x18) /* P2SCSLMD */
+#define REG_PORT_2_CTRL_4          (REG_PORT_1_CTRL_4 + 0x18) /* P2CR4 */
+#define REG_PORT_2_STATUS          (REG_PORT_1_STATUS + 0x18) /* P2SR */
+
+/* Port 3 registers  */
+
+#define REG_PORT3_CTRL1            (REG_PORT2_CTRL1 + 0x18)  /* P3CR1 */
+#define REG_PORT3_CTRL2            (REG_PORT2_CTRL2 + 0x18)  /* P3CR2 */
+#define REG_PORT3_CTRL3            (REG_PORT2_CTRL3 + 0x18)  /* P3CR3 */
+#define REG_PORT_3_CTRL_VID        (REG_PORT_2_CTRL_VID+0x18) /* P3VIDCR */
+#define REG_PORT3_CTRL3            (REG_PORT2_CTRL3 + 0x18)   /* P3CR3 */
+#define REG_PORT_3_IN_RATE0        (REG_PORT_2_IN_RATE0+0x18) /* P3IRCR0 */
+#define REG_PORT_3_IN_RATE1        (REG_PORT_2_IN_RATE1+0x18) /* P3IRCR1 */
+#define REG_PORT_3_OUT_RATE0       (REG_PORT_2_OUT_RATE0+0x18)/* P3ERCR0 */
+#define REG_PORT_3_OUT_RATE1       (REG_PORT_2_OUT_RATE1+0x18)/* P3ERCR1 */
+
+#define REG_PORT_CTRL1			0x00
+#define REG_PORT_CTRL2			0x02
+#define REG_PORT_CTRL_VID		0x04
+#define REG_PORT_CTRL3			0x06
+#define REG_PORT_IN_RATE0		0x08
+#define REG_PORT_IN_RATE1		0x0A
+#define REG_PORT_OUT_RATE0		0x0C
+#define REG_PORT_OUT_RATE1		0x0E
+#define REG_PORT_LINK_MD_CTRL		0x10
+#define REG_PORT_CTRL4			0x12
+#define REG_PORT_STATUS			0x14
+
+#define PORT_CTRL_ADDR(port, addr)		\
+	(addr = REG_PORT1_CTRL1 + (port) *	\
+		(REG_PORT2_CTRL1 - REG_PORT1_CTRL1))
+
+/* More Switch Global Control registers  */
+
+#define REG_SWITCH_CTRL_8          0xAC        /* SGCR8 */
+#define   SWITCH_PRIORITY_MAP_MASK   3
+#define   SWITCH_PRIORITY_MAP_SHIFT  14
+#define   SWITCH_PRIORITY_3_HI       (0 << SWITCH_PRIORITY_MAP_SHIFT)
+#define   SWITCH_PRIORITY_0_LO       (1 << SWITCH_PRIORITY_MAP_SHIFT)
+#define   SWITCH_PRIORITY_SPLIT      (2 << SWITCH_PRIORITY_MAP_SHIFT)
+/* Flush dynamic mac table */
+#define   SWITCH_FLUSH_DYNA_MAC      (1 << 10)
+/* Flush static mac table */
+#define   SWITCH_FLUSH_STATIC_MAC    (1 << 9)
+/* enable port 3 tail tag mode */
+#define   SWITCH_TAIL_TAG_ENABLE     (1 << 8)
+
+#define REG_SWITCH_CTRL_9          0xAE        /* SGCR9 */
+/* forwarding invalid VID frames to port 3 */
+#define   SWITCH_INVALID_VID_2_PORT3 (1 << 10)
+/* forwarding invalid VID frames to port 2 */
+#define   SWITCH_INVALID_VID_2_PORT2 (1 << 9)
+/* forwarding invalid VID frames to port 1 */
+#define   SWITCH_INVALID_VID_2_PORT1 (1 << 8)
+/* enable insert vlan tag when untagged pkts from port 3 to port 2 */
+#define   SWITCH_INS_TAG_2_1         (1 << 5)
+/* enable insert vlan tag when untagged pkts from port 3 to port 1 */
+#define   SWITCH_INS_TAG_2_0         (1 << 4)
+/* enable insert vlan tag when untagged pkts from port 2 to port 3 */
+#define   SWITCH_INS_TAG_1_2         (1 << 3)
+/* enable insert vlan tag when untagged pkts from port 2 to port 1 */
+#define   SWITCH_INS_TAG_1_0         (1 << 2)
+/* enable insert vlan tag when untagged pkts from port 2 to port 3 */
+#define   SWITCH_INS_TAG_0_2         (1 << 1)
+/* enable insert vlan tag when untagged pkts from port 1 to port 2 */
+#define   SWITCH_INS_TAG_0_1         (1 << 0)
+
+
+#define REG_SA_FILTER_MAC1_L       0xB0        /* SAFMACA1L */
+#define REG_SA_FILTER_MAC1_M       0xB2        /* SAFMACA1M */
+#define REG_SA_FILTER_MAC1_H       0xB4        /* SAFMACA1H */
+
+#define REG_SA_FILTER_MAC2_L       0xB6        /* SAFMACA2L */
+#define REG_SA_FILTER_MAC2_M       0xB8        /* SAFMACA2M */
+#define REG_SA_FILTER_MAC2_H       0xBA        /* SAFMACA2H */
+
+
+/* more Ports registers  */
+
+#define REG_PORT1_TXQ_RATE_CTRL1   0xC8        /* P1TXQRCR1 */
+/* tx highest priority before lower priority pkts for TX queue 2 */
+#define   PORT_TXQ2_HIGH_PRIORITY    (1 << 15)
+/* tx highest priority before lower priority pkts for TX queue 1 */
+#define   PORT_TXQ1_HIGH_PRIORITY    (1 << 7)
+
+#define REG_PORT1_TXQ_RATE_CTRL2   0xCA        /* P1TXQRCR2 */
+/* tx highest priority before lower priority pkts for TX queue 4 */
+#define   PORT_TXQ4_HIGH_PRIORITY    (1 << 15)
+/* tx highest priority before lower priority pkts for TX queue 3 */
+#define   PORT_TXQ3_HIGH_PRIORITY    (1 << 7)
+
+/* P2TXQRCR1 */
+#define REG_PORT2_TXQ_RATE_CTRL1   (REG_PORT1_TXQ_RATE_CTRL1 + 0x04)
+/* P2TXQRCR2 */
+#define REG_PORT2_TXQ_RATE_CTRL2   (REG_PORT1_TXQ_RATE_CTRL2 + 0x04)
+
+/* P3TXQRCR1 */
+#define REG_PORT3_TXQ_RATE_CTRL1   (REG_PORT2_TXQ_RATE_CTRL1 + 0x04)
+/* P3TXQRCR2 */
+#define REG_PORT3_TXQ_RATE_CTRL2   (REG_PORT2_TXQ_RATE_CTRL2 + 0x04)
+
+#define RATE_CTRL_ENABLE	(1 << 7)
+#define RATE_RATIO_MASK		0x7f
+
+
+#define REG_IO_MUX_CTRL            0xD6        /* IOMXSEL */
+/* 1 as P2LED1; 0 as GPIO9 for KS8463MLL/KS8441HLL */
+#define   IO_SEL_P2LED1             (1 << 10)
+/* 1 as P2LED0; 0 as GPIO10 for KS8463MLL */
+#define   IO_SEL_P2LED0             (1 << 9)
+/* 1 as P1LED1; 0 as GPIO7  for KS8463MLL */
+#define   IO_SEL_P1LED1             (1 << 8)
+/* 1 as EESK; 0 as GPIO5 for KS8462SNL */
+#define   IO_SEL_EESK_SNL_2PORT     (1 << 7)
+/* 1 as EESK; 0 as GPIO0 for KS8441SNL */
+#define   IO_SEL_EESK_SNL_1PORT     (1 << 6)
+/* 1 as EESK; 0 as GPIO3 for KS8462HLL/KS8441HLL */
+#define   IO_SEL_EESK_HLL           (1 << 5)
+/* 1 as EEDIO; 0 as GPIO6 for KS8462SNL */
+#define   IO_SEL_EEDIO_SNL_2PORT    (1 << 4)
+/* 1 as EEDIO; 0 as GPIO1 for KS8441SNL */
+#define   IO_SEL_EEDIO_SNL_1PORT    (1 << 3)
+/* 1 as EEDIO; 0 as GPIO4 for KS8462HLL/KS8441HLL */
+#define   IO_SEL_EEDIO_HLL          (1 << 2)
+/* 1 as EECS; 0 as GPIO5 for KS8462HLL/KS8441HLL */
+#define   IO_SEL_EECS_HLL           (1 << 1)
+/* 1 as EECS; 0 as GPIO4 for KS8462SNL/KS8441SNL */
+#define   IO_SEL_EECS_SNL           (1 << 0)
+
+#define REG_CFG_CTRL			0xD8
+#define PORT_2_COPPER_MODE		(1 << 7)
+#define PORT_1_COPPER_MODE		(1 << 6)
+#define PORT_COPPER_MODE_S		6
+
+#define REG_PCS_EEE_CTRL		0xF3
+#define PORT_2_NEXT_PAGE_ENABLE		(1 << 1)
+#define PORT_1_NEXT_PAGE_ENABLE		(1 << 0)
+
+#endif
+
+/* END */
diff -Napur linux-linux4microchip-2022.10-orig/drivers/net/ethernet/micrel/ksz8895.h linux-linux4microchip-2022.10/drivers/net/ethernet/micrel/ksz8895.h
--- linux-linux4microchip-2022.10-orig/drivers/net/ethernet/micrel/ksz8895.h	1969-12-31 16:00:00.000000000 -0800
+++ linux-linux4microchip-2022.10/drivers/net/ethernet/micrel/ksz8895.h	2023-04-25 16:13:54.872163472 -0700
@@ -0,0 +1,733 @@
+/**
+ * Microchip KSZ8895 definition file
+ *
+ * Copyright (c) 2015-2016 Microchip Technology Inc.
+ *	Tristram Ha <Tristram.Ha@microchip.com>
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ */
+
+
+/* -------------------------------------------------------------------------- */
+
+#ifndef __KSZ8895_H
+#define __KSZ8895_H
+
+
+#define KS_PORT_M			0x1F
+
+#define KS_PRIO_M			0x3
+#define KS_PRIO_S			2
+
+
+#define REG_CHIP_ID0			0x00
+
+#define FAMILY_ID			0x95
+
+#define REG_CHIP_ID1			0x01
+
+#define SW_CHIP_ID_M			0xF0
+#define SW_CHIP_ID_S			4
+#define SW_REVISION_M			0x0E
+#define SW_REVISION_S			1
+#define SW_START			0x01
+
+#define CHIP_ID_95			0x40
+#define CHIP_ID_95R			0x60
+
+#define REG_SW_CTRL_0			0x02
+
+#define SW_NEW_BACKOFF			(1 << 7)
+#define SW_FLUSH_DYN_MAC_TABLE		(1 << 5)
+#define SW_FLUSH_STA_MAC_TABLE		(1 << 4)
+#define SW_UNH_MODE			(1 << 1)
+#define SW_LINK_AUTO_AGING		(1 << 0)
+
+#define REG_SW_CTRL_1			0x03
+
+#define SW_PASS_ALL			(1 << 7)
+#define SW_2K_PACKET			(1 << 6)
+#define SW_TX_FLOW_CTRL_DISABLE		(1 << 5)
+#define SW_RX_FLOW_CTRL_DISABLE		(1 << 4)
+#define SW_CHECK_LENGTH			(1 << 3)
+#define SW_AGING_ENABLE			(1 << 2)
+#define SW_FAST_AGING			(1 << 1)
+#define SW_AGGR_BACKOFF			(1 << 0)
+
+#define REG_SW_CTRL_2			0x04
+
+#define UNICAST_VLAN_BOUNDARY		(1 << 7)
+#define MULTICAST_STORM_DISABLE		(1 << 6)
+#define SW_BACK_PRESSURE		(1 << 5)
+#define FAIR_FLOW_CTRL			(1 << 4)
+#define NO_EXC_COLLISION_DROP		(1 << 3)
+#define SW_HUGE_PACKET			(1 << 2)
+#define SW_LEGAL_PACKET			(1 << 1)
+
+#define REG_SW_CTRL_3			0x05
+#define SW_VLAN_ENABLE			(1 << 7)
+#define SW_IGMP_SNOOP			(1 << 6)
+#define SW_DIRECT			(1 << 5)
+#define SW_PRE_TAG			(1 << 4)
+#define SW_VLAN_TAG			(1 << 1)
+#define SW_MIRROR_RX_TX			(1 << 0)
+
+#define REG_SW_CTRL_4			0x06
+
+#define SW_HALF_DUPLEX_FLOW_CTRL	(1 << 7)
+#define SW_HALF_DUPLEX			(1 << 6)
+#define SW_FLOW_CTRL			(1 << 5)
+#define SW_10_MBIT			(1 << 4)
+#define SW_REPLACE_VID			(1 << 3)
+#define BROADCAST_STORM_RATE_HI		0x07
+
+#define REG_SW_CTRL_5			0x07
+
+#define BROADCAST_STORM_RATE_LO		0xFF
+#define BROADCAST_STORM_RATE		0x07FF
+
+#define REG_SW_CTRL_9			0x0B
+
+#define SW_DATA_SAMPLING_NEG		(1 << 6)
+#define SW_PHY_POWER_SAVE_DISABLE	(1 << 3)
+#define SW_LED_MODE_1			(1 << 1)
+#define SW_SPI_SAMPLING_RISING		(1 << 0)
+
+#define REG_SW_CTRL_10			0x0C
+
+#define SPI_CLK_125_MHZ			0x20
+#define SPI_CLK_83_33_MHZ		0x10
+#define SPI_CLK_41_67_MHZ		0x00
+#define SW_TAIL_TAG_ENABLE		(1 << 1)
+#define SW_PASS_PAUSE			(1 << 0)
+
+#define REG_SW_CTRL_11			0x0D
+
+#define REG_POWER_MANAGEMENT_1		0x0E
+
+#define SW_PLL_POWER_DOWN		(1 << 5)
+#define SW_POWER_MANAGEMENT_MODE_M	0x3
+#define SW_POWER_MANAGEMENT_MODE_S	3
+#define SW_POWER_NORMAL			0
+#define SW_ENERGY_DETECTION		1
+#define SW_SOFTWARE_POWER_DOWN		2
+#define SW_POWER_SAVING			3
+
+#define REG_POWER_MANAGEMENT_2		0x0F
+
+
+#define REG_PORT_1_CTRL_0		0x10
+#define REG_PORT_2_CTRL_0		0x20
+#define REG_PORT_3_CTRL_0		0x30
+#define REG_PORT_4_CTRL_0		0x40
+#define REG_PORT_5_CTRL_0		0x50
+
+#define PORT_BROADCAST_STORM		(1 << 7)
+#define PORT_DIFFSERV_ENABLE		(1 << 6)
+#define PORT_802_1P_ENABLE		(1 << 5)
+#define PORT_BASED_PRIO_S		3
+#define PORT_BASED_PRIO_M		KS_PRIO_M
+#define PORT_PORT_PRIO_0		0
+#define PORT_PORT_PRIO_1		1
+#define PORT_PORT_PRIO_2		2
+#define PORT_PORT_PRIO_3		3
+#define PORT_INSERT_TAG			(1 << 2)
+#define PORT_REMOVE_TAG			(1 << 1)
+#define PORT_QUEUE_SPLIT_L		(1 << 0)
+
+#define REG_PORT_1_CTRL_1		0x11
+#define REG_PORT_2_CTRL_1		0x21
+#define REG_PORT_3_CTRL_1		0x31
+#define REG_PORT_4_CTRL_1		0x41
+#define REG_PORT_5_CTRL_1		0x51
+
+#define PORT_MIRROR_SNIFFER		(1 << 7)
+#define PORT_MIRROR_RX			(1 << 6)
+#define PORT_MIRROR_TX			(1 << 5)
+#define PORT_VLAN_MEMBERSHIP		KS_PORT_M
+
+#define REG_PORT_1_CTRL_2		0x12
+#define REG_PORT_2_CTRL_2		0x22
+#define REG_PORT_3_CTRL_2		0x32
+#define REG_PORT_4_CTRL_2		0x42
+#define REG_PORT_5_CTRL_2		0x52
+
+#define PORT_802_1P_REMAPPING		(1 << 7)
+#define PORT_INGRESS_FILTER		(1 << 6)
+#define PORT_DISCARD_NON_VID		(1 << 5)
+#define PORT_FORCE_FLOW_CTRL		(1 << 4)
+#define PORT_BACK_PRESSURE		(1 << 3)
+#define PORT_TX_ENABLE			(1 << 2)
+#define PORT_RX_ENABLE			(1 << 1)
+#define PORT_LEARN_DISABLE		(1 << 0)
+
+#define REG_PORT_1_CTRL_3		0x13
+#define REG_PORT_2_CTRL_3		0x23
+#define REG_PORT_3_CTRL_3		0x33
+#define REG_PORT_4_CTRL_3		0x43
+#define REG_PORT_5_CTRL_3		0x53
+#define REG_PORT_1_CTRL_4		0x14
+#define REG_PORT_2_CTRL_4		0x24
+#define REG_PORT_3_CTRL_4		0x34
+#define REG_PORT_4_CTRL_4		0x44
+#define REG_PORT_5_CTRL_4		0x54
+
+#define PORT_DEFAULT_VID		0x0001
+
+#define REG_PORT_1_STATUS_0		0x19
+#define REG_PORT_2_STATUS_0		0x29
+#define REG_PORT_3_STATUS_0		0x39
+#define REG_PORT_4_STATUS_0		0x49
+#define REG_PORT_5_STATUS_0		0x59
+
+#define PORT_HP_MDIX			(1 << 7)
+#define PORT_REVERSED_POLARITY		(1 << 5)
+#define PORT_TX_FLOW_CTRL		(1 << 4)
+#define PORT_RX_FLOW_CTRL		(1 << 3)
+#define PORT_STAT_SPEED_100MBIT		(1 << 2)
+#define PORT_STAT_FULL_DUPLEX		(1 << 1)
+
+#define REG_PORT_1_LINK_MD_CTRL		0x1A
+#define REG_PORT_2_LINK_MD_CTRL		0x2A
+#define REG_PORT_3_LINK_MD_CTRL		0x3A
+#define REG_PORT_4_LINK_MD_CTRL		0x4A
+#define REG_PORT_5_LINK_MD_CTRL		0x5A
+
+#define PORT_CABLE_10M_SHORT		(1 << 7)
+#define PORT_CABLE_DIAG_RESULT_M	0x3
+#define PORT_CABLE_DIAG_RESULT_S	5
+#define PORT_CABLE_STAT_NORMAL		0
+#define PORT_CABLE_STAT_OPEN		1
+#define PORT_CABLE_STAT_SHORT		2
+#define PORT_CABLE_STAT_FAILED		3
+#define PORT_START_CABLE_DIAG		(1 << 4)
+#define PORT_FORCE_LINK			(1 << 3)
+#define PORT_POWER_SAVING		(1 << 2)
+#define PORT_PHY_REMOTE_LOOPBACK	(1 << 1)
+#define PORT_CABLE_FAULT_COUNTER_H	0x01
+
+#define REG_PORT_1_LINK_MD_RESULT	0x1B
+#define REG_PORT_2_LINK_MD_RESULT	0x2B
+#define REG_PORT_3_LINK_MD_RESULT	0x3B
+#define REG_PORT_4_LINK_MD_RESULT	0x4B
+#define REG_PORT_5_LINK_MD_RESULT	0x5B
+
+#define PORT_CABLE_FAULT_COUNTER_L	0xFF
+#define PORT_CABLE_FAULT_COUNTER	0x1FF
+
+#define REG_PORT_1_CTRL_5		0x1C
+#define REG_PORT_2_CTRL_5		0x2C
+#define REG_PORT_3_CTRL_5		0x3C
+#define REG_PORT_4_CTRL_5		0x4C
+#define REG_PORT_5_CTRL_5		0x5C
+
+#define PORT_AUTO_NEG_DISABLE		(1 << 7)
+#define PORT_FORCE_100_MBIT		(1 << 6)
+#define PORT_FORCE_FULL_DUPLEX		(1 << 5)
+#define PORT_AUTO_NEG_SYM_PAUSE		(1 << 4)
+#define PORT_AUTO_NEG_100BTX_FD		(1 << 3)
+#define PORT_AUTO_NEG_100BTX		(1 << 2)
+#define PORT_AUTO_NEG_10BT_FD		(1 << 1)
+#define PORT_AUTO_NEG_10BT		(1 << 0)
+
+#define REG_PORT_1_CTRL_6		0x1D
+#define REG_PORT_2_CTRL_6		0x2D
+#define REG_PORT_3_CTRL_6		0x3D
+#define REG_PORT_4_CTRL_6		0x4D
+#define REG_PORT_5_CTRL_6		0x5D
+
+#define PORT_LED_OFF			(1 << 7)
+#define PORT_TX_DISABLE			(1 << 6)
+#define PORT_AUTO_NEG_RESTART		(1 << 5)
+#define PORT_POWER_DOWN			(1 << 3)
+#define PORT_AUTO_MDIX_DISABLE		(1 << 2)
+#define PORT_FORCE_MDIX			(1 << 1)
+#define PORT_MAC_LOOPBACK		(1 << 0)
+
+#define REG_PORT_1_STATUS_1		0x1E
+#define REG_PORT_2_STATUS_1		0x2E
+#define REG_PORT_3_STATUS_1		0x3E
+#define REG_PORT_4_STATUS_1		0x4E
+#define REG_PORT_5_STATUS_1		0x5E
+
+#define PORT_MDIX_STATUS		(1 << 7)
+#define PORT_AUTO_NEG_COMPLETE		(1 << 6)
+#define PORT_STAT_LINK_GOOD		(1 << 5)
+#define PORT_REMOTE_SYM_PAUSE		(1 << 4)
+#define PORT_REMOTE_100BTX_FD		(1 << 3)
+#define PORT_REMOTE_100BTX		(1 << 2)
+#define PORT_REMOTE_10BT_FD		(1 << 1)
+#define PORT_REMOTE_10BT		(1 << 0)
+
+#define REG_PORT_1_STATUS_2		0x1F
+#define REG_PORT_2_STATUS_2		0x2F
+#define REG_PORT_3_STATUS_2		0x3F
+#define REG_PORT_4_STATUS_2		0x4F
+#define REG_PORT_5_STATUS_2		0x5F
+
+#define PORT_PHY_LOOPBACK		(1 << 7)
+#define PORT_PHY_ISOLATE		(1 << 5)
+#define PORT_PHY_SOFT_RESET		(1 << 4)
+#define PORT_PHY_FORCE_LINK		(1 << 3)
+#define PORT_PHY_MODE_M			0x7
+#define PHY_MODE_IN_AUTO_NEG		1
+#define PHY_MODE_10BT_HALF		2
+#define PHY_MODE_100BT_HALF		3
+#define PHY_MODE_10BT_FULL		5
+#define PHY_MODE_100BT_FULL		6
+#define PHY_MODE_ISOLDATE		7
+
+#define REG_PORT_CTRL_0			0x00
+#define REG_PORT_CTRL_1			0x01
+#define REG_PORT_CTRL_2			0x02
+#define REG_PORT_CTRL_VID		0x03
+
+#define REG_PORT_STATUS_0		0x09
+#define REG_PORT_LINK_MD_CTRL		0x0A
+#define REG_PORT_LINK_MD_RESULT		0x0B
+#define REG_PORT_CTRL_5			0x0C
+#define REG_PORT_CTRL_6			0x0D
+#define REG_PORT_STATUS_1		0x0E
+#define REG_PORT_STATUS_2		0x0F
+
+#define REG_PORT_CTRL_8			0xA0
+#define REG_PORT_CTRL_9			0xA1
+#define REG_PORT_RATE_CTRL_3		0xA2
+#define REG_PORT_RATE_CTRL_2		0xA3
+#define REG_PORT_RATE_CTRL_1		0xA4
+#define REG_PORT_RATE_CTRL_0		0xA5
+#define REG_PORT_RATE_LIMIT		0xA6
+#define REG_PORT_IN_RATE_0		0xA7
+#define REG_PORT_IN_RATE_1		0xA8
+#define REG_PORT_IN_RATE_2		0xA9
+#define REG_PORT_IN_RATE_3		0xAA
+#define REG_PORT_OUT_RATE_0		0xAB
+#define REG_PORT_OUT_RATE_1		0xAC
+#define REG_PORT_OUT_RATE_2		0xAD
+#define REG_PORT_OUT_RATE_3		0xAE
+
+#define PORT_CTRL_ADDR(port, addr)		\
+	(addr = REG_PORT_1_CTRL_0 + (port) *	\
+		(REG_PORT_2_CTRL_0 - REG_PORT_1_CTRL_0))
+
+
+#define REG_SW_MAC_ADDR_0		0x68
+#define REG_SW_MAC_ADDR_1		0x69
+#define REG_SW_MAC_ADDR_2		0x6A
+#define REG_SW_MAC_ADDR_3		0x6B
+#define REG_SW_MAC_ADDR_4		0x6C
+#define REG_SW_MAC_ADDR_5		0x6D
+
+#define REG_IND_CTRL_0			0x6E
+
+#define TABLE_READ			(1 << 4)
+#define TABLE_SELECT_S			2
+#define TABLE_STATIC_MAC		(0 << TABLE_SELECT_S)
+#define TABLE_VLAN			(1 << TABLE_SELECT_S)
+#define TABLE_DYNAMIC_MAC		(2 << TABLE_SELECT_S)
+#define TABLE_MIB			(3 << TABLE_SELECT_S)
+
+#define REG_IND_CTRL_1			0x6F
+
+#define TABLE_ENTRY_MASK		0x03FF
+
+#define REG_IND_DATA_8			0x70
+#define REG_IND_DATA_7			0x71
+#define REG_IND_DATA_6			0x72
+#define REG_IND_DATA_5			0x73
+#define REG_IND_DATA_4			0x74
+#define REG_IND_DATA_3			0x75
+#define REG_IND_DATA_2			0x76
+#define REG_IND_DATA_1			0x77
+#define REG_IND_DATA_0			0x78
+
+#define REG_IND_DATA_CHECK		REG_IND_DATA_6
+#define REG_IND_MIB_CHECK		REG_IND_DATA_3
+#define REG_IND_DATA_HI			REG_IND_DATA_7
+#define REG_IND_DATA_LO			REG_IND_DATA_3
+
+#define REG_INT_STATUS			0x7C
+#define REG_INT_ENABLE			0x7D
+
+#define INT_PORT_5			(1 << 4)
+#define INT_PORT_4			(1 << 3)
+#define INT_PORT_3			(1 << 2)
+#define INT_PORT_2			(1 << 1)
+#define INT_PORT_1			(1 << 0)
+
+#define REG_SW_CTRL_12			0x80
+#define REG_SW_CTRL_13			0x81
+
+#define SWITCH_802_1P_MASK		3
+#define SWITCH_802_1P_BASE		3
+#define SWITCH_802_1P_SHIFT		2
+
+#define SW_802_1P_MAP_M			KS_PRIO_M
+#define SW_802_1P_MAP_S			KS_PRIO_S
+
+#define REG_SWITCH_CTRL_14		0x82
+
+#define SW_PRIO_MAPPING_M		KS_PRIO_M
+#define SW_PRIO_MAPPING_S		6
+#define SW_PRIO_MAP_3_HI		0
+#define SW_PRIO_MAP_2_HI		2
+#define SW_PRIO_MAP_0_LO		3
+
+#define REG_SW_CTRL_15			0x83
+#define REG_SW_CTRL_16			0x84
+
+#define SW_DRIVE_STRENGTH_M		0x3
+#define SW_DRIVE_STRENGTH_4MA		0
+#define SW_DRIVE_STRENGTH_8MA		1
+#define SW_DRIVE_STRENGTH_10MA		2
+#define SW_DRIVE_STRENGTH_14MA		3
+#define SW_MII_DRIVE_STRENGTH_S		6
+
+#define REG_SW_CTRL_17			0x85
+#define REG_SW_CTRL_18			0x86
+
+#define SW_SELF_ADDR_FILTER_ENABLE	(1 << 6)
+
+#define REG_SW_UNK_UCAST_CTRL		0x83
+#define REG_SW_UNK_MCAST_CTRL		0x84
+#define REG_SW_UNK_VID_CTRL		0x85
+#define REG_SW_UNK_IP_MCAST_CTRL	0x86
+
+#define SW_UNK_FWD_ENABLE		(1 << 5)
+#define SW_UNK_FWD_MAP			KS_PORT_M
+
+#define REG_SW_CTRL_19			0x87
+
+#define SW_IN_RATE_LIMIT_PERIOD_M	0x3
+#define SW_IN_RATE_LIMIT_PERIOD_S	4
+#define SW_IN_RATE_LIMIT_16_MS		0
+#define SW_IN_RATE_LIMIT_64_MS		1
+#define SW_IN_RATE_LIMIT_256_MS		2
+#define SW_OUT_RATE_LIMIT_QUEUE_BASED	(1 << 3)
+#define SW_INS_TAG_ENABLE		(1 << 2)
+
+#define REG_TOS_PRIO_CTRL_0		0x90
+#define REG_TOS_PRIO_CTRL_1		0x91
+#define REG_TOS_PRIO_CTRL_2		0x92
+#define REG_TOS_PRIO_CTRL_3		0x93
+#define REG_TOS_PRIO_CTRL_4		0x94
+#define REG_TOS_PRIO_CTRL_5		0x95
+#define REG_TOS_PRIO_CTRL_6		0x96
+#define REG_TOS_PRIO_CTRL_7		0x97
+#define REG_TOS_PRIO_CTRL_8		0x98
+#define REG_TOS_PRIO_CTRL_9		0x99
+#define REG_TOS_PRIO_CTRL_10		0x9A
+#define REG_TOS_PRIO_CTRL_11		0x9B
+#define REG_TOS_PRIO_CTRL_12		0x9C
+#define REG_TOS_PRIO_CTRL_13		0x9D
+#define REG_TOS_PRIO_CTRL_14		0x9E
+#define REG_TOS_PRIO_CTRL_15		0x9F
+
+#define TOS_PRIO_M			KS_PRIO_M
+#define TOS_PRIO_S			KS_PRIO_S
+
+
+#define REG_PORT_1_CTRL_8		0xB0
+#define REG_PORT_2_CTRL_8		0xC0
+#define REG_PORT_3_CTRL_8		0xD0
+#define REG_PORT_4_CTRL_8		0xE0
+#define REG_PORT_5_CTRL_8		0xF0
+
+#define PORT_INS_TAG_FOR_PORT_5_S	3
+#define PORT_INS_TAG_FOR_PORT_5		(1 << 3)
+#define PORT_INS_TAG_FOR_PORT_4		(1 << 2)
+#define PORT_INS_TAG_FOR_PORT_3		(1 << 1)
+#define PORT_INS_TAG_FOR_PORT_2		(1 << 0)
+
+#define REG_PORT_1_CTRL_9		0xB1
+#define REG_PORT_2_CTRL_9		0xC1
+#define REG_PORT_3_CTRL_9		0xD1
+#define REG_PORT_4_CTRL_9		0xE1
+#define REG_PORT_5_CTRL_9		0xF1
+
+#define PORT_QUEUE_SPLIT_H		(1 << 1)
+#define PORT_QUEUE_SPLIT_1		0
+#define PORT_QUEUE_SPLIT_2		1
+#define PORT_QUEUE_SPLIT_4		2
+#define PORT_DROP_TAG			(1 << 0)
+
+#define REG_PORT_1_CTRL_10		0xB2
+#define REG_PORT_2_CTRL_10		0xC2
+#define REG_PORT_3_CTRL_10		0xD2
+#define REG_PORT_4_CTRL_10		0xE2
+#define REG_PORT_5_CTRL_10		0xF2
+#define REG_PORT_1_CTRL_11		0xB3
+#define REG_PORT_2_CTRL_11		0xC3
+#define REG_PORT_3_CTRL_11		0xD3
+#define REG_PORT_4_CTRL_11		0xE3
+#define REG_PORT_5_CTRL_11		0xF3
+#define REG_PORT_1_CTRL_12		0xB4
+#define REG_PORT_2_CTRL_12		0xC4
+#define REG_PORT_3_CTRL_12		0xD4
+#define REG_PORT_4_CTRL_12		0xE4
+#define REG_PORT_5_CTRL_12		0xF4
+#define REG_PORT_1_CTRL_13		0xB5
+#define REG_PORT_2_CTRL_13		0xC5
+#define REG_PORT_3_CTRL_13		0xD5
+#define REG_PORT_4_CTRL_13		0xE5
+#define REG_PORT_5_CTRL_13		0xF5
+
+#define REG_PORT_1_RATE_CTRL_3		0xB2
+#define REG_PORT_1_RATE_CTRL_2		0xB3
+#define REG_PORT_1_RATE_CTRL_1		0xB4
+#define REG_PORT_1_RATE_CTRL_0		0xB5
+#define REG_PORT_2_RATE_CTRL_3		0xC2
+#define REG_PORT_2_RATE_CTRL_2		0xC3
+#define REG_PORT_2_RATE_CTRL_1		0xC4
+#define REG_PORT_2_RATE_CTRL_0		0xC5
+#define REG_PORT_3_RATE_CTRL_3		0xD2
+#define REG_PORT_3_RATE_CTRL_2		0xD3
+#define REG_PORT_3_RATE_CTRL_1		0xD4
+#define REG_PORT_3_RATE_CTRL_0		0xD5
+#define REG_PORT_4_RATE_CTRL_3		0xE2
+#define REG_PORT_4_RATE_CTRL_2		0xE3
+#define REG_PORT_4_RATE_CTRL_1		0xE4
+#define REG_PORT_4_RATE_CTRL_0		0xE5
+#define REG_PORT_5_RATE_CTRL_3		0xF2
+#define REG_PORT_5_RATE_CTRL_2		0xF3
+#define REG_PORT_5_RATE_CTRL_1		0xF4
+#define REG_PORT_5_RATE_CTRL_0		0xF5
+
+#define RATE_CTRL_ENABLE		(1 << 7)
+#define RATE_RATIO_M			((1 << 7) - 1)
+
+#define REG_PORT_1_RATE_LIMIT		0xB6
+#define REG_PORT_2_RATE_LIMIT		0xC6
+#define REG_PORT_3_RATE_LIMIT		0xD6
+#define REG_PORT_4_RATE_LIMIT		0xE6
+#define REG_PORT_5_RATE_LIMIT		0xF6
+
+#define PORT_IN_FLOW_CTRL_S		4
+#define PORT_IN_LIMIT_MODE_M		0x3
+#define PORT_IN_LIMIT_MODE_S		2
+#define PORT_COUNT_IFG_S		1
+#define PORT_COUNT_PREAMBLE_S		0
+#define PORT_IN_FLOW_CTRL		(1 << PORT_IN_FLOW_CTRL_S)
+#define PORT_IN_ALL			0
+#define PORT_IN_UNICAST			1
+#define PORT_IN_MULTICAST		2
+#define PORT_IN_BROADCAST		3
+#define PORT_COUNT_IFG			(1 << PORT_COUNT_IFG_S)
+#define PORT_COUNT_PREAMBLE		(1 << PORT_COUNT_PREAMBLE_S)
+
+#define REG_PORT_1_IN_RATE_0		0xB7
+#define REG_PORT_2_IN_RATE_0		0xC7
+#define REG_PORT_3_IN_RATE_0		0xD7
+#define REG_PORT_4_IN_RATE_0		0xE7
+#define REG_PORT_5_IN_RATE_0		0xF7
+#define REG_PORT_1_IN_RATE_1		0xB8
+#define REG_PORT_2_IN_RATE_1		0xC8
+#define REG_PORT_3_IN_RATE_1		0xD8
+#define REG_PORT_4_IN_RATE_1		0xE8
+#define REG_PORT_5_IN_RATE_1		0xF8
+#define REG_PORT_1_IN_RATE_2		0xB9
+#define REG_PORT_2_IN_RATE_2		0xC9
+#define REG_PORT_3_IN_RATE_2		0xD9
+#define REG_PORT_4_IN_RATE_2		0xE9
+#define REG_PORT_5_IN_RATE_2		0xF9
+#define REG_PORT_1_IN_RATE_3		0xBA
+#define REG_PORT_2_IN_RATE_3		0xCA
+#define REG_PORT_3_IN_RATE_3		0xDA
+#define REG_PORT_4_IN_RATE_3		0xEA
+#define REG_PORT_5_IN_RATE_3		0xFA
+
+#define PORT_RATE_LIMIT_M		((1 << 7) - 1)
+
+#define REG_PORT_1_OUT_RATE_0		0xBB
+#define REG_PORT_2_OUT_RATE_0		0xCB
+#define REG_PORT_3_OUT_RATE_0		0xDB
+#define REG_PORT_4_OUT_RATE_0		0xEB
+#define REG_PORT_5_OUT_RATE_0		0xFB
+#define REG_PORT_1_OUT_RATE_1		0xBC
+#define REG_PORT_2_OUT_RATE_1		0xCC
+#define REG_PORT_3_OUT_RATE_1		0xDC
+#define REG_PORT_4_OUT_RATE_1		0xEC
+#define REG_PORT_5_OUT_RATE_1		0xFC
+#define REG_PORT_1_OUT_RATE_2		0xBD
+#define REG_PORT_2_OUT_RATE_2		0xCD
+#define REG_PORT_3_OUT_RATE_2		0xDD
+#define REG_PORT_4_OUT_RATE_2		0xED
+#define REG_PORT_5_OUT_RATE_2		0xFD
+#define REG_PORT_1_OUT_RATE_3		0xBE
+#define REG_PORT_2_OUT_RATE_3		0xCE
+#define REG_PORT_3_OUT_RATE_3		0xDE
+#define REG_PORT_4_OUT_RATE_3		0xEE
+#define REG_PORT_5_OUT_RATE_3		0xFE
+
+
+#define REG_SW_CFG			0xEF
+
+#define SW_PORT_3_FIBER			(1 << 7)
+
+/* KSZ8864 */
+
+#define REG_PHY_PORT_CTRL_1		0xCF
+
+#define PORT_HALF_DUPLEX		(1 << 7)
+#define PORT_FLOW_CTRL			(1 << 6)
+#define PORT_10_MBIT			(1 << 5)
+
+#define REG_PHY_PORT_CTRL_2		0xDF
+
+#define PORT_MII_MAC_MODE		(1 << 6)
+
+#define REG_KSZ8864_CHIP_ID		0xFE
+
+#define SW_KSZ8864			(1 << 7)
+
+
+#ifndef PHY_REG_CTRL
+#define PHY_REG_CTRL			0
+
+#define PHY_RESET			(1 << 15)
+#define PHY_LOOPBACK			(1 << 14)
+#define PHY_SPEED_100MBIT		(1 << 13)
+#define PHY_AUTO_NEG_ENABLE		(1 << 12)
+#define PHY_POWER_DOWN			(1 << 11)
+#define PHY_MII_DISABLE			(1 << 10)
+#define PHY_AUTO_NEG_RESTART		(1 << 9)
+#define PHY_FULL_DUPLEX			(1 << 8)
+#define PHY_COLLISION_TEST_NOT		(1 << 7)
+#define PHY_HP_MDIX			(1 << 5)
+#define PHY_FORCE_MDIX			(1 << 4)
+#define PHY_AUTO_MDIX_DISABLE		(1 << 3)
+#define PHY_REMOTE_FAULT_DISABLE	(1 << 2)
+#define PHY_TRANSMIT_DISABLE		(1 << 1)
+#define PHY_LED_DISABLE			(1 << 0)
+
+#define PHY_REG_STATUS			1
+
+#define PHY_100BT4_CAPABLE		(1 << 15)
+#define PHY_100BTX_FD_CAPABLE		(1 << 14)
+#define PHY_100BTX_CAPABLE		(1 << 13)
+#define PHY_10BT_FD_CAPABLE		(1 << 12)
+#define PHY_10BT_CAPABLE		(1 << 11)
+#define PHY_MII_SUPPRESS_CAPABLE_NOT	(1 << 6)
+#define PHY_AUTO_NEG_ACKNOWLEDGE	(1 << 5)
+#define PHY_REMOTE_FAULT		(1 << 4)
+#define PHY_AUTO_NEG_CAPABLE		(1 << 3)
+#define PHY_LINK_STATUS			(1 << 2)
+#define PHY_JABBER_DETECT_NOT		(1 << 1)
+#define PHY_EXTENDED_CAPABILITY		(1 << 0)
+
+#define PHY_REG_ID_1			2
+#define PHY_REG_ID_2			3
+
+#define PHY_REG_AUTO_NEGOTIATION	4
+
+#define PHY_AUTO_NEG_NEXT_PAGE_NOT	(1 << 15)
+#define PHY_AUTO_NEG_REMOTE_FAULT_NOT	(1 << 13)
+#define PHY_AUTO_NEG_SYM_PAUSE		(1 << 10)
+#define PHY_AUTO_NEG_100BT4		(1 << 9)
+#define PHY_AUTO_NEG_100BTX_FD		(1 << 8)
+#define PHY_AUTO_NEG_100BTX		(1 << 7)
+#define PHY_AUTO_NEG_10BT_FD		(1 << 6)
+#define PHY_AUTO_NEG_10BT		(1 << 5)
+#define PHY_AUTO_NEG_SELECTOR		0x001F
+#define PHY_AUTO_NEG_802_3		0x0001
+
+#define PHY_REG_REMOTE_CAPABILITY	5
+
+#define PHY_REMOTE_NEXT_PAGE_NOT	(1 << 15)
+#define PHY_REMOTE_ACKNOWLEDGE_NOT	(1 << 14)
+#define PHY_REMOTE_REMOTE_FAULT_NOT	(1 << 13)
+#define PHY_REMOTE_SYM_PAUSE		(1 << 10)
+#define PHY_REMOTE_100BTX_FD		(1 << 8)
+#define PHY_REMOTE_100BTX		(1 << 7)
+#define PHY_REMOTE_10BT_FD		(1 << 6)
+#define PHY_REMOTE_10BT			(1 << 5)
+#endif
+
+#define KSZ8895_ID_HI			0x0022
+#define KSZ8895_ID_LO			0x1450
+
+#define PHY_REG_LINK_MD			0x1D
+
+#define PHY_START_CABLE_DIAG		(1 << 15)
+#define PHY_CABLE_DIAG_RESULT		0x6000
+#define PHY_CABLE_STAT_NORMAL		0x0000
+#define PHY_CABLE_STAT_OPEN		0x2000
+#define PHY_CABLE_STAT_SHORT		0x4000
+#define PHY_CABLE_STAT_FAILED		0x6000
+#define PHY_CABLE_10M_SHORT		(1 << 12)
+#define PHY_CABLE_FAULT_COUNTER		0x01FF
+
+#define PHY_REG_PHY_CTRL		0x1F
+
+#define PHY_MODE_M			0x7
+#define PHY_MODE_S			8
+#define PHY_STAT_REVERSED_POLARITY	(1 << 5)
+#define PHY_STAT_MDIX			(1 << 4)
+#define PHY_FORCE_LINK			(1 << 3)
+#define PHY_POWER_SAVING_ENABLE		(1 << 2)
+#define PHY_REMOTE_LOOPBACK		(1 << 1)
+
+
+/* Default values are used in ksz_sw.h if these are not defined. */
+#define PRIO_QUEUES			4
+
+#define KS_PRIO_IN_REG			4
+
+#define TOTAL_PORT_NUM			5
+
+#define KSZ8895_COUNTER_NUM		0x20
+#define TOTAL_KSZ8895_COUNTER_NUM	(KSZ8895_COUNTER_NUM + 2)
+
+#define SWITCH_COUNTER_NUM		KSZ8895_COUNTER_NUM
+#define TOTAL_SWITCH_COUNTER_NUM	TOTAL_KSZ8895_COUNTER_NUM
+
+/* Required for common switch control in ksz_sw.c */
+#define SW_D				u8
+#define SW_R(sw, addr)			(sw)->reg->r8(sw, addr)
+#define SW_W(sw, addr, val)		(sw)->reg->w8(sw, addr, val)
+#define SW_SIZE				(1)
+#define SW_SIZE_STR			"%02x"
+#define port_r				port_r8
+#define port_w				port_w8
+
+
+#define P_BCAST_STORM_CTRL		REG_PORT_CTRL_0
+#define P_PRIO_CTRL			REG_PORT_CTRL_0
+#define P_TAG_CTRL			REG_PORT_CTRL_0
+#define P_MIRROR_CTRL			REG_PORT_CTRL_1
+#define P_802_1P_CTRL			REG_PORT_CTRL_2
+#define P_STP_CTRL			REG_PORT_CTRL_2
+#define P_LOCAL_CTRL			REG_PORT_CTRL_5
+#define P_REMOTE_STATUS			REG_PORT_STATUS_1
+#define P_FORCE_CTRL			REG_PORT_CTRL_5
+#define P_NEG_RESTART_CTRL		REG_PORT_CTRL_6
+#define P_SPEED_STATUS			REG_PORT_STATUS_0
+#define P_LINK_STATUS			REG_PORT_STATUS_1
+#define P_INS_SRC_PVID_CTRL		REG_PORT_CTRL_8
+#define P_DROP_TAG_CTRL			REG_PORT_CTRL_9
+#define P_RATE_LIMIT_CTRL		REG_PORT_RATE_LIMIT
+
+#define S_FLUSH_TABLE_CTRL		REG_SW_CTRL_0
+#define S_LINK_AGING_CTRL		REG_SW_CTRL_0
+#define S_HUGE_PACKET_CTRL		REG_SW_CTRL_2
+#define S_MIRROR_CTRL			REG_SW_CTRL_3
+#define S_REPLACE_VID_CTRL		REG_SW_CTRL_4
+#define S_PASS_PAUSE_CTRL		REG_SW_CTRL_10
+#define S_TAIL_TAG_CTRL			REG_SW_CTRL_10
+#define S_802_1P_PRIO_CTRL		REG_SW_CTRL_12
+#define S_TOS_PRIO_CTRL			REG_TOS_PRIO_CTRL_0
+#define S_IPV6_MLD_CTRL			REG_SW_CTRL_21
+
+#define IND_ACC_TABLE(table)		((table) << 8)
+
+#define TAIL_TAG_OVERRIDE		(1 << 6)
+#define TAIL_TAG_LOOKUP			(1 << 7)
+
+#endif
diff -Napur linux-linux4microchip-2022.10-orig/drivers/net/ethernet/micrel/ksz9897.h linux-linux4microchip-2022.10/drivers/net/ethernet/micrel/ksz9897.h
--- linux-linux4microchip-2022.10-orig/drivers/net/ethernet/micrel/ksz9897.h	1969-12-31 16:00:00.000000000 -0800
+++ linux-linux4microchip-2022.10/drivers/net/ethernet/micrel/ksz9897.h	2024-04-29 16:09:18.813324020 -0700
@@ -0,0 +1,1735 @@
+/**
+ * Microchip KSZ9897 definition file
+ *
+ * Copyright (c) 2015-2024 Microchip Technology Inc.
+ *	Tristram Ha <Tristram.Ha@microchip.com>
+ *
+ * Copyright (c) 2013-2015 Micrel, Inc.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ */
+
+
+/* -------------------------------------------------------------------------- */
+
+#ifndef __KSZ9897_H
+#define __KSZ9897_H
+
+
+#define KS_PRIO_M			0x7
+#define KS_PRIO_S			4
+
+
+/* 0 - Operation */
+#define REG_CHIP_ID0__1			0x0000
+
+#define REG_CHIP_ID1__1			0x0001
+
+#define FAMILY_ID			0x95
+#define FAMILY_ID_94			0x94
+#define FAMILY_ID_95			0x95
+#define FAMILY_ID_85			0x85
+#define FAMILY_ID_98			0x98
+#define FAMILY_ID_88			0x88
+
+#define REG_CHIP_ID2__1			0x0002
+
+#define CHIP_ID_63			0x63
+#define CHIP_ID_66			0x66
+#define CHIP_ID_67			0x67
+#define CHIP_ID_77			0x77
+#define CHIP_ID_93			0x93
+#define CHIP_ID_96			0x96
+#define CHIP_ID_97			0x97
+
+#define REG_CHIP_ID3__1			0x0003
+
+#define SWITCH_REVISION_M		0x0F
+#define SWITCH_REVISION_S		4
+#define SWITCH_RESET			0x01
+
+#define REG_SW_PME_CTRL			0x0006
+
+#define PME_ENABLE			(1 << 1)
+#define PME_POLARITY			(1 << 0)
+
+#define REG_GLOBAL_OPTIONS		0x000F
+
+/* For KSZ9897 */
+#define SW_GIGABIT_ABLE			(1 << 6)
+#define SW_REDUNDANCY_ABLE		(1 << 5)
+#define SW_AVB_ABLE			(1 << 4)
+#define SW_9567_RL_5_2			0xC
+#define SW_9477_SL_5_2			0xD
+
+#define SW_9896_GL_5_1			0xB
+#define SW_9896_RL_5_1			0x8
+#define SW_9896_SL_5_1			0x9
+
+#define SW_9895_GL_4_1			0x7
+#define SW_9895_RL_4_1			0x4
+#define SW_9895_SL_4_1			0x5
+
+#define SW_9896_RL_4_2			0x6
+
+#define SW_9893_RL_2_1			0x0
+#define SW_9893_SL_2_1			0x1
+#define SW_9893_GL_2_1			0x3
+
+/* For KSZ9893 */
+#define SW_QW_ABLE			(1 << 5)
+#define SW_9893_RN_2_1			0xC
+
+#define REG_SW_INT_STATUS__4		0x0010
+#define REG_SW_INT_MASK__4		0x0014
+
+#define LUE_INT				(1 << 31)
+#define TRIG_TS_INT			(1 << 30)
+#define APB_TIMEOUT_INT			(1 << 29)
+
+#define SWITCH_INT_MASK			\
+	(LUE_INT | TRIG_TS_INT | APB_TIMEOUT_INT)
+
+#define REG_SW_PORT_INT_STATUS__4	0x0018
+#define REG_SW_PORT_INT_MASK__4		0x001C
+#define REG_SW_PHY_INT_STATUS		0x0020
+#define REG_SW_PHY_INT_ENABLE		0x0024
+
+/* 1 - Global */
+#define REG_SW_GLOBAL_SERIAL_CTRL_0	0x0100
+#define SW_SPARE_REG_2			(1 << 7)
+#define SW_SPARE_REG_1			(1 << 6)
+#define SW_SPARE_REG_0			(1 << 5)
+#define SW_BIG_ENDIAN			(1 << 4)
+#define SPI_AUTO_EDGE_DETECTION		(1 << 1)
+#define SPI_CLOCK_OUT_RISING_EDGE	(1 << 0)
+
+#define REG_SW_GLOBAL_OUTPUT_CTRL__1	0x0103
+#define SW_ENABLE_REFCLKO		(1 << 1)
+#define SW_REFCLKO_IS_125MHZ		(1 << 0)
+
+#define REG_SW_IBA__4			0x0104
+
+#define SW_IBA_ENABLE			(1 << 31)
+#define SW_IBA_DA_MATCH			(1 << 30)
+#define SW_IBA_INIT			(1 << 29)
+#define SW_IBA_QID_M			0x3
+#define SW_IBA_QID_S			22
+#define SW_IBA_PORT_M			0x7
+#define SW_IBA_PORT_S			16
+#define SW_IBA_FRAME_TPID_M		0xFFFF
+
+#define REG_SW_APB_TIMEOUT_ADDR__4	0x0108
+
+#define APB_TIMEOUT_ACKNOWLEDGE		(1 << 31)
+
+#define REG_SW_IBA_SYNC__1		0x010C
+
+#define REG_SW_IO_STRENGTH__1		0x010D
+#define SW_DRIVE_STRENGTH_M		0x7
+#define SW_DRIVE_STRENGTH_2MA		0
+#define SW_DRIVE_STRENGTH_4MA		1
+#define SW_DRIVE_STRENGTH_8MA		2
+#define SW_DRIVE_STRENGTH_12MA		3
+#define SW_DRIVE_STRENGTH_16MA		4
+#define SW_DRIVE_STRENGTH_20MA		5
+#define SW_DRIVE_STRENGTH_24MA		6
+#define SW_DRIVE_STRENGTH_28MA		7
+#define SW_HI_SPEED_DRIVE_STRENGTH_S	4
+#define SW_LO_SPEED_DRIVE_STRENGTH_S	0
+
+#define REG_SW_IBA_STATUS__4		0x0110
+
+#define SW_IBA_REQ			(1 << 31)
+#define SW_IBA_RESP			(1 << 30)
+#define SW_IBA_DA_MISMATCH		(1 << 14)
+#define SW_IBA_FMT_MISMATCH		(1 << 13)
+#define SW_IBA_CODE_ERROR		(1 << 12)
+#define SW_IBA_CMD_ERROR		(1 << 11)
+#define SW_IBA_CMD_LOC_M		((1 << 6) - 1)
+
+#define REG_SW_IBA_STATES__4		0x0114
+
+#define SW_IBA_BUF_STATE_S		30
+#define SW_IBA_CMD_STATE_S		28
+#define SW_IBA_RESP_STATE_S		26
+#define SW_IBA_STATE_M			0x3
+#define SW_IBA_PACKET_SIZE_M		0x7F
+#define SW_IBA_PACKET_SIZE_S		16
+#define SW_IBA_FMT_ID_M			0xFFFF
+
+#define REG_SW_IBA_RESULT__4		0x0118
+
+#define SW_IBA_SIZE_S			24
+
+#define SW_IBA_RETRY_CNT_M		((1 << 5) - 1)
+
+
+/* 2 - PHY */
+#define REG_SW_POWER_MANAGEMENT_CTRL	0x0201
+
+#define SW_PLL_POWER_DOWN		(1 << 5)
+#define SW_POWER_DOWN_MODE		0x3
+#define SW_POWER_DOWN_S			3
+#define SW_ENERGY_DETECTION		1
+#define SW_SOFT_POWER_DOWN		2
+
+/* 3 - Operation Control */
+#define REG_SW_OPERATION		0x0300
+
+#define SW_DOUBLE_TAG			(1 << 7)
+#define SW_RESET			(1 << 1)
+#define SW_START			(1 << 0)
+
+#define REG_SW_MAC_ADDR_0		0x0302
+#define REG_SW_MAC_ADDR_1		0x0303
+#define REG_SW_MAC_ADDR_2		0x0304
+#define REG_SW_MAC_ADDR_3		0x0305
+#define REG_SW_MAC_ADDR_4		0x0306
+#define REG_SW_MAC_ADDR_5		0x0307
+
+#define REG_SW_MTU__2			0x0308
+
+#define REG_SW_ISP_TPID__2		0x030A
+
+#define REG_SW_HSR_TPID__2		0x030C
+
+#define REG_AVB_STRATEGY__2		0x030E
+
+#define SW_SHAPING_CREDIT_ACCT		(1 << 1)
+#define SW_POLICING_CREDIT_ACCT		(1 << 0)
+
+#define REG_SW_LUE_CTRL_0		0x0310
+
+#define SW_VLAN_ENABLE			(1 << 7)
+#define SW_DROP_INVALID_VID		(1 << 6)
+#define SW_AGE_CNT_M			0x7
+#define SW_AGE_CNT_S			3
+#define SW_RESV_MCAST_ENABLE		(1 << 2)
+#define SW_HASH_OPTION_M		0x03
+#define SW_HASH_OPTION_CRC		1
+#define SW_HASH_OPTION_XOR		2
+#define SW_HASH_OPTION_DIRECT		3
+
+#define REG_SW_LUE_CTRL_1		0x0311
+
+#define UNICAST_LEARN_DISABLE		(1 << 7)
+#define SW_SRC_ADDR_FILTER		(1 << 6)
+#define SW_FLUSH_STP_TABLE		(1 << 5)
+#define SW_FLUSH_MSTP_TABLE		(1 << 4)
+#define SW_FWD_MCAST_SRC_ADDR		(1 << 3)
+#define SW_AGING_ENABLE			(1 << 2)
+#define SW_FAST_AGING			(1 << 1)
+#define SW_LINK_AUTO_AGING		(1 << 0)
+
+#define REG_SW_LUE_CTRL_2		0x0312
+
+#define SW_TRAP_DOUBLE_TAG		(1 << 6)
+#define SW_EGRESS_VLAN_FILTER_DYN	(1 << 5)
+#define SW_EGRESS_VLAN_FILTER_STA	(1 << 4)
+#define SW_FLUSH_OPTION_M		0x3
+#define SW_FLUSH_OPTION_S		2
+#define SW_FLUSH_OPTION_DYN_MAC		1
+#define SW_FLUSH_OPTION_STA_MAC		2
+#define SW_FLUSH_OPTION_BOTH		3
+#define SW_PRIO_M			0x3
+#define SW_PRIO_DA			0
+#define SW_PRIO_SA			1
+#define SW_PRIO_HIGHEST_DA_SA		2
+#define SW_PRIO_LOWEST_DA_SA		3
+
+#define REG_SW_LUE_CTRL_3		0x0313
+
+#define REG_SW_LUE_INT_STATUS__1	0x0314
+#define REG_SW_LUE_INT_MASK__1		0x0315
+
+#define LEARN_FAIL_INT			(1 << 2)
+#define ALMOST_FULL_INT			(1 << 1)
+#define WRITE_FAIL_INT			(1 << 0)
+
+#define LUE_INT_MASK			(LEARN_FAIL_INT | WRITE_FAIL_INT)
+
+#define REG_SW_LUE_INDEX_0__2		0x0316
+
+#define ENTRY_INDEX_M			0x0FFF
+
+#define REG_SW_LUE_INDEX_1__2		0x0318
+
+#define FAIL_INDEX_M			0x03FF
+
+#define REG_SW_LUE_INDEX_2__2		0x031A
+
+#define REG_SW_LUE_UNK_UCAST_CTRL__4	0x0320
+
+#define SW_UNK_UCAST_ENABLE		(1 << 31)
+
+#define REG_SW_LUE_UNK_MCAST_CTRL__4	0x0324
+
+#define SW_UNK_MCAST_ENABLE		(1 << 31)
+
+#define REG_SW_LUE_UNK_VID_CTRL__4	0x0328
+
+#define SW_UNK_VID_ENABLE		(1 << 31)
+
+#define REG_SW_MAC_CTRL_0		0x0330
+
+#define SW_NEW_BACKOFF			(1 << 7)
+#define SW_CHECK_LENGTH			(1 << 3)
+#define SW_PAUSE_UNH_MODE		(1 << 1)
+#define SW_AGGR_BACKOFF			(1 << 0)
+
+#define REG_SW_MAC_CTRL_1		0x0331
+
+#define MULTICAST_STORM_DISABLE		(1 << 6)
+#define SW_BACK_PRESSURE		(1 << 5)
+#define FAIR_FLOW_CTRL			(1 << 4)
+#define NO_EXC_COLLISION_DROP		(1 << 3)
+#define SW_JUMBO_PACKET			(1 << 2)
+#define SW_LEGAL_PACKET_DISABLE		(1 << 1)
+#define SW_PASS_SHORT_FRAME		(1 << 0)
+
+#define REG_SW_MAC_CTRL_2		0x0332
+
+#define SW_REPLACE_VID			(1 << 3)
+#define BROADCAST_STORM_RATE_HI		0x07
+
+#define REG_SW_MAC_CTRL_3		0x0333
+
+#define BROADCAST_STORM_RATE_LO		0xFF
+#define BROADCAST_STORM_RATE		0x07FF
+
+#define REG_SW_MAC_CTRL_4		0x0334
+
+#define SW_PASS_PAUSE			(1 << 3)
+
+#define REG_SW_MAC_CTRL_5		0x0335
+
+#define SW_OUT_RATE_LIMIT_QUEUE_BASED	(1 << 3)
+
+#define REG_SW_MAC_CTRL_6		0x0336
+
+#define SW_MIB_COUNTER_FLUSH		(1 << 7)
+#define SW_MIB_COUNTER_FREEZE		(1 << 6)
+
+#define REG_SW_MAC_802_1P_MAP_0		0x0338
+#define REG_SW_MAC_802_1P_MAP_1		0x0339
+#define REG_SW_MAC_802_1P_MAP_2		0x033A
+#define REG_SW_MAC_802_1P_MAP_3		0x033B
+
+#define SW_802_1P_MAP_M			KS_PRIO_M
+#define SW_802_1P_MAP_S			KS_PRIO_S
+
+#define REG_SW_MAC_ISP_CTRL		0x033C
+
+#define REG_SW_MAC_TOS_CTRL		0x033E
+
+#define SW_TOS_DSCP_REMARK		(1 << 1)
+#define SW_TOS_DSCP_REMAP		(1 << 0)
+
+#define REG_SW_MAC_TOS_PRIO_0		0x0340
+#define REG_SW_MAC_TOS_PRIO_1		0x0341
+#define REG_SW_MAC_TOS_PRIO_2		0x0342
+#define REG_SW_MAC_TOS_PRIO_3		0x0343
+#define REG_SW_MAC_TOS_PRIO_4		0x0344
+#define REG_SW_MAC_TOS_PRIO_5		0x0345
+#define REG_SW_MAC_TOS_PRIO_6		0x0346
+#define REG_SW_MAC_TOS_PRIO_7		0x0347
+#define REG_SW_MAC_TOS_PRIO_8		0x0348
+#define REG_SW_MAC_TOS_PRIO_9		0x0349
+#define REG_SW_MAC_TOS_PRIO_10		0x034A
+#define REG_SW_MAC_TOS_PRIO_11		0x034B
+#define REG_SW_MAC_TOS_PRIO_12		0x034C
+#define REG_SW_MAC_TOS_PRIO_13		0x034D
+#define REG_SW_MAC_TOS_PRIO_14		0x034E
+#define REG_SW_MAC_TOS_PRIO_15		0x034F
+#define REG_SW_MAC_TOS_PRIO_16		0x0350
+#define REG_SW_MAC_TOS_PRIO_17		0x0351
+#define REG_SW_MAC_TOS_PRIO_18		0x0352
+#define REG_SW_MAC_TOS_PRIO_19		0x0353
+#define REG_SW_MAC_TOS_PRIO_20		0x0354
+#define REG_SW_MAC_TOS_PRIO_21		0x0355
+#define REG_SW_MAC_TOS_PRIO_22		0x0356
+#define REG_SW_MAC_TOS_PRIO_23		0x0357
+#define REG_SW_MAC_TOS_PRIO_24		0x0358
+#define REG_SW_MAC_TOS_PRIO_25		0x0359
+#define REG_SW_MAC_TOS_PRIO_26		0x035A
+#define REG_SW_MAC_TOS_PRIO_27		0x035B
+#define REG_SW_MAC_TOS_PRIO_28		0x035C
+#define REG_SW_MAC_TOS_PRIO_29		0x035D
+#define REG_SW_MAC_TOS_PRIO_30		0x035E
+#define REG_SW_MAC_TOS_PRIO_31		0x035F
+
+#define REG_SW_MRI_CTRL_0		0x0370
+
+#define SW_IGMP_SNOOP			(1 << 6)
+#define SW_IPV6_MLD_OPTION		(1 << 3)
+#define SW_IPV6_MLD_SNOOP		(1 << 2)
+#define SW_MIRROR_RX_TX			(1 << 0)
+
+#define REG_SW_CLASS_D_IP_CTRL__4	0x0374
+
+#define SW_CLASS_D_IP_ENABLE		(1 << 31)
+
+#define REG_SW_MRI_CTRL_8		0x0378
+
+#define SW_NO_COLOR_S			6
+#define SW_RED_COLOR_S			4
+#define SW_YELLOW_COLOR_S		2
+#define SW_GREEN_COLOR_S		0
+#define SW_COLOR_M			0x3
+
+#define REG_PTP_EVENT_PRIO_CTRL		0x037C
+#define REG_PTP_GENERAL_PRIO_CTRL	0x037D
+
+#define PTP_PRIO_ENABLE			BIT(7)
+
+#define REG_SW_QM_CTRL__4		0x0390
+
+#define PRIO_SCHEME_SELECT_M		KS_PRIO_M
+#define PRIO_SCHEME_SELECT_S		6
+#define PRIO_MAP_3_HI			0
+#define PRIO_MAP_2_HI			2
+#define PRIO_MAP_0_LO			3
+#define UNICAST_VLAN_BOUNDARY		(1 << 1)
+
+#define REG_SW_EEE_QM_CTRL__2		0x03C0
+
+#define REG_SW_EEE_TXQ_WAIT_TIME__2	0x03C2
+
+/* 4 - */
+#define REG_SW_VLAN_ENTRY__4		0x0400
+
+#define VLAN_VALID			(1 << 31)
+#define VLAN_FORWARD_OPTION		(1 << 27)
+#define VLAN_PRIO_M			KS_PRIO_M
+#define VLAN_PRIO_S			24
+#define VLAN_MSTP_M			0x7
+#define VLAN_MSTP_S			12
+#define VLAN_FID_M			0x7F
+
+#define REG_SW_VLAN_ENTRY_UNTAG__4	0x0404
+#define REG_SW_VLAN_ENTRY_PORTS__4	0x0408
+
+#define REG_SW_VLAN_ENTRY_INDEX__2	0x040C
+
+#define VLAN_INDEX_M			0x0FFF
+
+#define REG_SW_VLAN_CTRL		0x040E
+
+#define VLAN_START			(1 << 7)
+#define VLAN_ACTION			0x3
+#define VLAN_WRITE			1
+#define VLAN_READ			2
+#define VLAN_CLEAR			3
+
+#define REG_SW_ALU_INDEX_0		0x0410
+
+#define ALU_FID_INDEX_S			16
+#define ALU_MAC_ADDR_HI			0xFFFF
+
+#define REG_SW_ALU_INDEX_1		0x0414
+
+#define ALU_DIRECT_INDEX_M		((1 << 12) - 1)
+
+#define REG_SW_ALU_CTRL__4		0x0418
+
+#define ALU_VALID_CNT_M			((1 << 14) - 1)
+#define ALU_VALID_CNT_S			16
+#define ALU_START			(1 << 7)
+#define ALU_VALID			(1 << 6)
+#define ALU_VALID_OR_STOP		(1 << 5)
+#define ALU_DIRECT			(1 << 2)
+#define ALU_ACTION			0x3
+#define ALU_WRITE			1
+#define ALU_READ			2
+#define ALU_SEARCH			3
+
+#define REG_SW_ALU_STAT_CTRL__4		0x041C
+
+#define ALU_STAT_INDEX_M		((1 << 4) - 1)
+#define ALU_STAT_INDEX_S		16
+#define ALU_RESV_MCAST_INDEX_M		((1 << 6) - 1)
+#define ALU_STAT_START			(1 << 7)
+#define ALU_RESV_MCAST_ADDR		(1 << 1)
+#define ALU_STAT_READ			(1 << 0)
+
+#define REG_SW_ALU_VAL_A		0x0420
+
+#define ALU_V_STATIC_VALID		(1 << 31)
+#define ALU_V_SRC_FILTER		(1 << 30)
+#define ALU_V_DST_FILTER		(1 << 29)
+#define ALU_V_PRIO_AGE_CNT_M		((1 << 3) - 1)
+#define ALU_V_PRIO_AGE_CNT_S		26
+#define ALU_V_MSTP_M			0x7
+
+#define REG_SW_ALU_VAL_B		0x0424
+
+#define ALU_V_OVERRIDE			(1 << 31)
+#define ALU_V_USE_FID			(1 << 30)
+#define ALU_V_PORT_MAP			((1 << 24) - 1)
+
+#define REG_SW_ALU_VAL_C		0x0428
+
+#define ALU_V_FID_M			((1 << 16) - 1)
+#define ALU_V_FID_S			16
+#define ALU_V_MAC_ADDR_HI		0xFFFF
+
+#define REG_SW_ALU_VAL_D		0x042C
+
+#define REG_HSR_ALU_INDEX_0		0x0440
+
+#define REG_HSR_ALU_INDEX_1		0x0444
+
+#define HSR_DST_MAC_INDEX_LO_S		16
+#define HSR_SRC_MAC_INDEX_HI		0xFFFF
+
+#define REG_HSR_ALU_INDEX_2		0x0448
+
+#define HSR_INDEX_MAX			(1 << 9)
+#define HSR_DIRECT_INDEX_M		(HSR_INDEX_MAX - 1)
+
+#define REG_HSR_ALU_INDEX_3		0x044C
+
+#define HSR_PATH_INDEX_M		((1 << 4) - 1)
+
+#define REG_HSR_ALU_CTRL__4		0x0450
+
+#define HSR_VALID_CNT_M			((1 << 14) - 1)
+#define HSR_VALID_CNT_S			16
+#define HSR_START			(1 << 7)
+#define HSR_VALID			(1 << 6)
+#define HSR_SEARCH_END			(1 << 5)
+#define HSR_DIRECT			(1 << 2)
+#define HSR_ACTION			0x3
+#define HSR_WRITE			1
+#define HSR_READ			2
+#define HSR_SEARCH			3
+
+#define REG_HSR_ALU_VAL_A		0x0454
+
+#define HSR_V_STATIC_VALID		(1 << 31)
+#define HSR_V_AGE_CNT_M			((1 << 3) - 1)
+#define HSR_V_AGE_CNT_S			26
+#define HSR_V_PATH_ID_M			((1 << 4) - 1)
+
+#define REG_HSR_ALU_VAL_B		0x0458
+
+#define REG_HSR_ALU_VAL_C		0x045C
+
+#define HSR_V_DST_MAC_ADDR_LO_S		16
+#define HSR_V_SRC_MAC_ADDR_HI		0xFFFF
+
+#define REG_HSR_ALU_VAL_D		0x0460
+
+#define REG_HSR_ALU_VAL_E		0x0464
+
+#define HSR_V_START_SEQ_1_S		16
+#define HSR_V_START_SEQ_2_S		0
+
+#define REG_HSR_ALU_VAL_F		0x0468
+
+#define HSR_V_EXP_SEQ_1_S		16
+#define HSR_V_EXP_SEQ_2_S		0
+
+#define REG_HSR_ALU_VAL_G		0x046C
+
+#define HSR_V_SEQ_CNT_1_S		16
+#define HSR_V_SEQ_CNT_2_S		0
+
+#define HSR_V_SEQ_M			((1 << 16) - 1)
+
+
+/* 5 - PTP Clock */
+#define REG_PTP_CLK_CTRL		0x0500
+
+#define PTP_STEP_ADJ			(1 << 6)
+#define PTP_STEP_DIR			(1 << 5)
+#define PTP_READ_TIME			(1 << 4)
+#define PTP_LOAD_TIME			(1 << 3)
+#define PTP_CLK_ADJ_ENABLE		(1 << 2)
+#define PTP_CLK_ENABLE			(1 << 1)
+#define PTP_CLK_RESET			(1 << 0)
+
+#define REG_PTP_RTC_SUB_NANOSEC__2	0x0502
+
+#define PTP_RTC_SUB_NANOSEC_M		0x0007
+
+#define REG_PTP_RTC_NANOSEC		0x0504
+#define REG_PTP_RTC_NANOSEC_H		0x0504
+#define REG_PTP_RTC_NANOSEC_L		0x0506
+
+#define REG_PTP_RTC_SEC			0x0508
+#define REG_PTP_RTC_SEC_H		0x0508
+#define REG_PTP_RTC_SEC_L		0x050A
+
+#define REG_PTP_SUBNANOSEC_RATE		0x050C
+#define REG_PTP_SUBNANOSEC_RATE_H	0x050C
+
+#define PTP_RATE_DIR			(1 << 31)
+#define PTP_TMP_RATE_ENABLE		(1 << 30)
+
+#define REG_PTP_SUBNANOSEC_RATE_L	0x050E
+
+#define REG_PTP_RATE_DURATION		0x0510
+#define REG_PTP_RATE_DURATION_H		0x0510
+#define REG_PTP_RATE_DURATION_L		0x0512
+
+#define REG_PTP_MSG_CONF1		0x0514
+
+#define PTP_802_1AS			(1 << 7)
+#define PTP_ENABLE			(1 << 6)
+#define PTP_ETH_ENABLE			(1 << 5)
+#define PTP_IPV4_UDP_ENABLE		(1 << 4)
+#define PTP_IPV6_UDP_ENABLE		(1 << 3)
+#define PTP_TC_P2P			(1 << 2)
+#define PTP_MASTER			(1 << 1)
+#define PTP_1STEP			(1 << 0)
+
+#define REG_PTP_MSG_CONF2		0x0516
+
+#define PTP_UNICAST_ENABLE		(1 << 12)
+#define PTP_ALTERNATE_MASTER		(1 << 11)
+#define PTP_SYNC_CHECK			(1 << 9)
+#define PTP_DELAY_CHECK			(1 << 8)
+#define PTP_PDELAY_CHECK		(1 << 7)
+#define PTP_DROP_SYNC_DELAY_REQ		(1 << 5)
+#define PTP_DOMAIN_CHECK		(1 << 4)
+#define PTP_UDP_CHECKSUM		(1 << 2)
+
+#define REG_PTP_DOMAIN_VERSION		0x0518
+#define PTP_VERSION_M			0xFF00
+#define PTP_DOMAIN_M			0x00FF
+
+#define REG_PTP_UNIT_INDEX__4		0x0520
+
+#define PTP_UNIT_M			0xF
+
+/* 2013-09-10 */
+#define PTP_GPIO_INDEX_S		16
+#define PTP_TSI_INDEX_S			8
+#define PTP_TOU_INDEX_S			0
+
+#define REG_PTP_TRIG_STATUS__4		0x0524
+
+#define TRIG_ERROR_S			16
+#define TRIG_DONE_S			0
+
+#define REG_PTP_INT_STATUS__4		0x0528
+
+#define TRIG_INT_S			16
+#define TS_INT_S			0
+
+#define TRIG_UNIT_M			0x7
+#define TS_UNIT_M			0x3
+
+#define REG_PTP_CTRL_STAT__4		0x052C
+
+#define GPIO_IN				(1 << 7)
+#define GPIO_MODE_IN			(1 << 6)
+#define TS_INT_ENABLE			(1 << 5)
+#define TRIG_ACTIVE			(1 << 4)
+#define TRIG_ENABLE			(1 << 3)
+#define TRIG_RESET			(1 << 2)
+#define TS_ENABLE			(1 << 1)
+#define TS_RESET			(1 << 0)
+
+#define TRIG_CTRL_M			\
+	(TRIG_ACTIVE | TRIG_ENABLE | TRIG_RESET)
+
+#define TS_CTRL_M			\
+	(TS_INT_ENABLE | TS_ENABLE | TS_RESET)
+
+#define REG_TRIG_TARGET_NANOSEC		0x0530
+#define REG_TRIG_TARGET_SEC		0x0534
+
+#define REG_TRIG_CTRL__4		0x0538
+
+#define TRIG_CASCADE_ENABLE		(1 << 31)
+#define TRIG_CASCADE_TAIL		(1 << 30)
+#define TRIG_CASCADE_UPS_M		0xF
+#define TRIG_CASCADE_UPS_S		26
+#define TRIG_NOW			(1 << 25)
+#define TRIG_NOTIFY			(1 << 24)
+#define TRIG_EDGE			(1 << 23)
+#define TRIG_PATTERN_S			20
+#define TRIG_PATTERN_M			0x7
+#define TRIG_NEG_EDGE			0
+#define TRIG_POS_EDGE			1
+#define TRIG_NEG_PULSE			2
+#define TRIG_POS_PULSE			3
+#define TRIG_NEG_PERIOD			4
+#define TRIG_POS_PERIOD			5
+#define TRIG_REG_OUTPUT			6
+#define TRIG_GPO_S			16
+#define TRIG_GPO_M			0xF
+#define TRIG_CASCADE_ITERATE_CNT_M	0xFFFF
+
+#define REG_TRIG_CYCLE_WIDTH		0x053C
+
+#define REG_TRIG_CYCLE_CNT		0x0540
+
+#define TRIG_CYCLE_CNT_M		0xFFFF
+#define TRIG_CYCLE_CNT_S		16
+#define TRIG_BIT_PATTERN_M		0xFFFF
+
+#define REG_TRIG_ITERATE_TIME		0x0544
+
+#define REG_TRIG_PULSE_WIDTH__4		0x0548
+
+#define TRIG_PULSE_WIDTH_M		0x00FFFFFF
+
+#define REG_TS_CTRL_STAT__4		0x0550
+
+#define TS_EVENT_DETECT_M		0xF
+#define TS_EVENT_DETECT_S		17
+#define TS_EVENT_OVERFLOW		(1 << 16)
+#define TS_GPI_M			0xF
+#define TS_GPI_S			8
+#define TS_DETECT_RISE			(1 << 7)
+#define TS_DETECT_FALL			(1 << 6)
+#define TS_DETECT_S			6
+#define TS_CASCADE_TAIL			(1 << 5)
+#define TS_CASCADE_UPS_M		0xF
+#define TS_CASCADE_UPS_S		1
+#define TS_CASCADE_ENABLE		(1 << 0)
+
+#define DETECT_RISE			(TS_DETECT_RISE >> TS_DETECT_S)
+#define DETECT_FALL			(TS_DETECT_FALL >> TS_DETECT_S)
+
+#define REG_TS_EVENT_0_NANOSEC		0x0554
+#define REG_TS_EVENT_0_SEC		0x0558
+#define REG_TS_EVENT_0_SUB_NANOSEC	0x055C
+
+#define REG_TS_EVENT_1_NANOSEC		0x0560
+#define REG_TS_EVENT_1_SEC		0x0564
+#define REG_TS_EVENT_1_SUB_NANOSEC	0x0568
+
+#define REG_TS_EVENT_2_NANOSEC		0x056C
+#define REG_TS_EVENT_2_SEC		0x0570
+#define REG_TS_EVENT_2_SUB_NANOSEC	0x0574
+
+#define REG_TS_EVENT_3_NANOSEC		0x0578
+#define REG_TS_EVENT_3_SEC		0x057C
+#define REG_TS_EVENT_3_SUB_NANOSEC	0x0580
+
+#define REG_TS_EVENT_4_NANOSEC		0x0584
+#define REG_TS_EVENT_4_SEC		0x0588
+#define REG_TS_EVENT_4_SUB_NANOSEC	0x058C
+
+#define REG_TS_EVENT_5_NANOSEC		0x0590
+#define REG_TS_EVENT_5_SEC		0x0594
+#define REG_TS_EVENT_5_SUB_NANOSEC	0x0598
+
+#define REG_TS_EVENT_6_NANOSEC		0x059C
+#define REG_TS_EVENT_6_SEC		0x05A0
+#define REG_TS_EVENT_6_SUB_NANOSEC	0x05A4
+
+#define REG_TS_EVENT_7_NANOSEC		0x05A8
+#define REG_TS_EVENT_7_SEC		0x05AC
+#define REG_TS_EVENT_7_SUB_NANOSEC	0x05B0
+
+#define TS_EVENT_EDGE_M			0x1
+#define TS_EVENT_EDGE_S			30
+#define TS_EVENT_NANOSEC_M		((1 << 30) - 1)
+
+#define TS_EVENT_SUB_NANOSEC_M		0x7
+
+#define TS_EVENT_SAMPLE			\
+	(REG_TS_EVENT_1_NANOSEC - REG_TS_EVENT_0_NANOSEC)
+
+
+#define PORT_CTRL_ADDR(port, addr)	((addr) | (((port) + 1) << 12))
+
+#define REG_GLOBAL_RR_INDEX__1		0x0600
+
+/* DLR */
+#define REG_DLR_SRC_PORT__4		0x0604
+
+#define DLR_SRC_PORT_UNICAST		(1 << 31)
+#define DLR_SRC_PORT_M			0x3
+#define DLR_SRC_PORT_BOTH		0
+#define DLR_SRC_PORT_EACH		1
+
+#define REG_DLR_IP_ADDR__4		0x0608
+
+#define REG_DLR_CTRL__1			0x0610
+
+#define DLR_RESET_SEQ_ID		(1 << 3)
+#define DLR_BACKUP_AUTO_ON		(1 << 2)
+#define DLR_BEACON_TX_ENABLE		(1 << 1)
+#define DLR_ASSIST_ENABLE		(1 << 0)
+
+#define REG_DLR_STATE__1		0x0611
+
+#define DLR_NODE_STATE_M		0x3
+#define DLR_NODE_STATE_S		1
+#define DLR_NODE_STATE_IDLE		0
+#define DLR_NODE_STATE_FAULT		1
+#define DLR_NODE_STATE_NORMAL		2
+#define DLR_RING_STATE_FAULT		0
+#define DLR_RING_STATE_NORMAL		1
+
+#define REG_DLR_PRECEDENCE__1		0x0612
+
+#define REG_DLR_BEACON_INTERVAL__4	0x0614
+
+#define REG_DLR_BEACON_TIMEOUT__4	0x0618
+
+#define REG_DLR_TIMEOUT_WINDOW__4	0x061C
+
+#define DLR_TIMEOUT_WINDOW_M		((1 << 22) - 1)
+
+#define REG_DLR_VLAN_ID__2		0x0620
+
+#define DLR_VLAN_ID_M			((1 << 12) - 1)
+
+#define REG_DLR_DEST_ADDR_0		0x0622
+#define REG_DLR_DEST_ADDR_1		0x0623
+#define REG_DLR_DEST_ADDR_2		0x0624
+#define REG_DLR_DEST_ADDR_3		0x0625
+#define REG_DLR_DEST_ADDR_4		0x0626
+#define REG_DLR_DEST_ADDR_5		0x0627
+
+#define REG_DLR_PORT_MAP__4		0x0628
+
+#define REG_DLR_CLASS__1		0x062C
+
+#define DLR_FRAME_QID_M			0x3
+
+/* HSR */
+#define REG_HSR_PORT_MAP__4		0x0640
+
+#define REG_HSR_ALU_CTRL_0__1		0x0644
+
+#define HSR_DUPLICATE_DISCARD		(1 << 7)
+#define HSR_NODE_UNICAST		(1 << 6)
+#define HSR_AGE_CNT_DEFAULT_M		0x7
+#define HSR_AGE_CNT_DEFAULT_S		3
+#define HSR_LEARN_MCAST_DISABLE		(1 << 2)
+#define HSR_HASH_OPTION_M		0x3
+#define HSR_HASH_DISABLE		0
+#define HSR_HASH_UPPER_BITS		1
+#define HSR_HASH_LOWER_BITS		2
+#define HSR_HASH_XOR_BOTH_BITS		3
+
+#define REG_HSR_ALU_CTRL_1__1		0x0645
+
+#define HSR_LEARN_UCAST_DISABLE		(1 << 7)
+#define HSR_FLUSH_TABLE			(1 << 5)
+#define HSR_PROC_MCAST_SRC		(1 << 3)
+#define HSR_AGING_ENABLE		(1 << 2)
+
+#define REG_HSR_ALU_CTRL_2__2		0x0646
+
+#define REG_HSR_ALU_AGE_PERIOD__4	0x0648
+
+#define REG_HSR_ALU_INT_STATUS__1	0x064C
+#define REG_HSR_ALU_INT_MASK__1		0x064D
+
+#define HSR_WINDOW_OVERFLOW_INT		(1 << 3)
+#define HSR_LEARN_FAIL_INT		(1 << 2)
+#define HSR_ALMOST_FULL_INT		(1 << 1)
+#define HSR_WRITE_FAIL_INT		(1 << 0)
+
+#define REG_HSR_ALU_ENTRY_0__2		0x0650
+
+#define HSR_ENTRY_INDEX_M		((1 << 10) - 1)
+#define HSR_FAIL_INDEX_M		((1 << 8) - 1)
+
+#define REG_HSR_ALU_ENTRY_1__2		0x0652
+
+#define HSR_FAIL_LEARN_INDEX_M		((1 << 8) - 1)
+
+#define REG_HSR_ALU_ENTRY_3__2		0x0654
+
+#define HSR_CPU_ACCESS_ENTRY_INDEX_M	((1 << 8) - 1)
+
+
+/* 0 - Operation */
+#define REG_PORT_DEFAULT_VID		0x0000
+
+#define REG_PORT_CUSTOM_VID		0x0002
+#define REG_PORT_AVB_SR_1_VID		0x0004
+#define REG_PORT_AVB_SR_2_VID		0x0006
+
+#define REG_PORT_AVB_SR_1_TYPE		0x0008
+#define REG_PORT_AVB_SR_2_TYPE		0x000A
+
+#define REG_PORT_PME_STATUS		0x0013
+#define REG_PORT_PME_CTRL		0x0017
+
+#define PME_WOL_MAGICPKT		(1 << 2)
+#define PME_WOL_LINKUP			(1 << 1)
+#define PME_WOL_ENERGY			(1 << 0)
+
+#define REG_PORT_INT_STATUS		0x001B
+#define REG_PORT_INT_MASK		0x001F
+
+#define PORT_SGMII_INT			(1 << 3)
+#define PORT_PTP_INT			(1 << 2)
+#define PORT_PHY_INT			(1 << 1)
+#define PORT_ACL_INT			(1 << 0)
+
+#define PORT_INT_MASK			\
+	(PORT_SGMII_INT | PORT_PTP_INT | PORT_PHY_INT | PORT_ACL_INT)
+
+#define REG_PORT_CTRL_0			0x0020
+
+#define PORT_MAC_LOOPBACK		(1 << 7)
+#define PORT_FORCE_TX_FLOW_CTRL		(1 << 4)
+#define PORT_FORCE_RX_FLOW_CTRL		(1 << 3)
+#define PORT_TAIL_TAG_ENABLE		(1 << 2)
+#define PORT_QUEUE_SPLIT_ENABLE		0x3
+
+#define REG_PORT_CTRL_1			0x0021
+
+#define PORT_SRP_ENABLE			0x3
+
+#define REG_PORT_STATUS_0		0x0030
+
+#define PORT_INTF_SPEED_M		0x3
+#define PORT_INTF_SPEED_S		3
+#define PORT_INTF_FULL_DUPLEX		(1 << 2)
+#define PORT_TX_FLOW_CTRL		(1 << 1)
+#define PORT_RX_FLOW_CTRL		(1 << 0)
+
+#define REG_PORT_STATUS_1		0x0034
+
+/* 1 - PHY */
+#define REG_PORT_PHY_CTRL		0x0100
+
+#define PORT_PHY_RESET			(1 << 15)
+#define PORT_PHY_LOOPBACK		(1 << 14)
+#define PORT_SPEED_100MBIT		(1 << 13)
+#define PORT_AUTO_NEG_ENABLE		(1 << 12)
+#define PORT_POWER_DOWN			(1 << 11)
+#define PORT_ISOLATE			(1 << 10)
+#define PORT_AUTO_NEG_RESTART		(1 << 9)
+#define PORT_FULL_DUPLEX		(1 << 8)
+#define PORT_COLLISION_TEST		(1 << 7)
+#define PORT_SPEED_1000MBIT		(1 << 6)
+#if 0
+#define PHY_HP_MDIX			(1 << 5)
+#define PHY_REMOTE_FAULT_DISABLE	(1 << 2)
+#define PHY_LED_DISABLE			(1 << 0)
+#endif
+
+#define REG_PORT_PHY_STATUS		0x0102
+
+#define PORT_100BT4_CAPABLE		(1 << 15)
+#define PORT_100BTX_FD_CAPABLE		(1 << 14)
+#define PORT_100BTX_CAPABLE		(1 << 13)
+#define PORT_10BT_FD_CAPABLE		(1 << 12)
+#define PORT_10BT_CAPABLE		(1 << 11)
+#define PORT_EXTENDED_STATUS		(1 << 8)
+#define PORT_MII_SUPPRESS_CAPABLE	(1 << 6)
+#define PORT_AUTO_NEG_ACKNOWLEDGE	(1 << 5)
+#define PORT_REMOTE_FAULT		(1 << 4)
+#define PORT_AUTO_NEG_CAPABLE		(1 << 3)
+#define PORT_LINK_STATUS		(1 << 2)
+#define PORT_JABBER_DETECT		(1 << 1)
+#define PORT_EXTENDED_CAPABILITY	(1 << 0)
+
+#define REG_PORT_PHY_ID_HI		0x0104
+#define REG_PORT_PHY_ID_LO		0x0106
+
+#define KSZ9897_ID_HI			0x0022
+#define KSZ9897_ID_LO			0x1622
+
+#define REG_PORT_PHY_AUTO_NEGOTIATION	0x0108
+
+#define PORT_AUTO_NEG_NEXT_PAGE		(1 << 15)
+#define PORT_AUTO_NEG_REMOTE_FAULT	(1 << 13)
+#define PORT_AUTO_NEG_ASYM_PAUSE	(1 << 11)
+#define PORT_AUTO_NEG_SYM_PAUSE		(1 << 10)
+#define PORT_AUTO_NEG_100BT4		(1 << 9)
+#define PORT_AUTO_NEG_100BTX_FD		(1 << 8)
+#define PORT_AUTO_NEG_100BTX		(1 << 7)
+#define PORT_AUTO_NEG_10BT_FD		(1 << 6)
+#define PORT_AUTO_NEG_10BT		(1 << 5)
+#define PORT_AUTO_NEG_SELECTOR		0x001F
+#define PORT_AUTO_NEG_802_3		0x0001
+
+#define PORT_AUTO_NEG_PAUSE		\
+	(PORT_AUTO_NEG_ASYM_PAUSE | PORT_AUTO_NEG_SYM_PAUSE)
+
+#define REG_PORT_PHY_REMOTE_CAPABILITY	0x010A
+
+#define PORT_REMOTE_NEXT_PAGE		(1 << 15)
+#define PORT_REMOTE_ACKNOWLEDGE		(1 << 14)
+#define PORT_REMOTE_REMOTE_FAULT	(1 << 13)
+#define PORT_REMOTE_ASYM_PAUSE		(1 << 11)
+#define PORT_REMOTE_SYM_PAUSE		(1 << 10)
+#define PORT_REMOTE_100BTX_FD		(1 << 8)
+#define PORT_REMOTE_100BTX		(1 << 7)
+#define PORT_REMOTE_10BT_FD		(1 << 6)
+#define PORT_REMOTE_10BT		(1 << 5)
+
+#define REG_PORT_PHY_1000_CTRL		0x0112
+
+#define PORT_AUTO_NEG_MANUAL		(1 << 12)
+#define PORT_AUTO_NEG_MASTER		(1 << 11)
+#define PORT_AUTO_NEG_MASTER_PREFERRED	(1 << 10)
+#define PORT_AUTO_NEG_1000BT_FD		(1 << 9)
+#define PORT_AUTO_NEG_1000BT		(1 << 8)
+
+#define REG_PORT_PHY_1000_STATUS	0x0114
+
+#define PORT_MASTER_FAULT		(1 << 15)
+#define PORT_LOCAL_MASTER		(1 << 14)
+#define PORT_LOCAL_RX_OK		(1 << 13)
+#define PORT_REMOTE_RX_OK		(1 << 12)
+#define PORT_REMOTE_1000BT_FD		(1 << 11)
+#define PORT_REMOTE_1000BT		(1 << 10)
+#define PORT_REMOTE_IDLE_CNT_M		0x0F
+
+#define PORT_PHY_1000_STATIC_STATUS	\
+	(PORT_LOCAL_RX_OK |		\
+	PORT_REMOTE_RX_OK |		\
+	PORT_REMOTE_1000BT_FD |		\
+	PORT_REMOTE_1000BT)
+
+#define REG_PORT_PHY_MMD_SETUP		0x011A
+
+#define PORT_MMD_OP_MODE_M		0x3
+#define PORT_MMD_OP_MODE_S		14
+#define PORT_MMD_OP_INDEX		0
+#define PORT_MMD_OP_DATA_NO_INCR	1
+#define PORT_MMD_OP_DATA_INCR_RW	2
+#define PORT_MMD_OP_DATA_INCR_W		3
+#define PORT_MMD_DEVICE_ID_M		0x1F
+
+#define MMD_SETUP(mode, dev)		\
+	(((u16) mode << PORT_MMD_OP_MODE_S) | dev)
+
+#define REG_PORT_PHY_MMD_INDEX_DATA	0x011C
+
+#define MMD_DEVICE_ID_DSP		1
+
+#define MMD_DSP_SQI_CHAN_A		0xAC
+#define MMD_DSP_SQI_CHAN_B		0xAD
+#define MMD_DSP_SQI_CHAN_C		0xAE
+#define MMD_DSP_SQI_CHAN_D		0xAF
+
+#define DSP_SQI_ERR_DETECTED		(1 << 15)
+#define DSP_SQI_AVG_ERR			0x7FFF
+
+#define MMD_DEVICE_ID_COMMON		2
+
+#define MMD_COMMON_LED_MODE		0
+
+#define MMD_COMMON_SINGLE_LED		(1 << 4)
+
+#define MMD_DEVICE_ID_EEE_ADV		7
+
+#define MMD_EEE_ADV			0x3C
+#define EEE_ADV_100MBIT			(1 << 1)
+#define EEE_ADV_1GBIT			(1 << 2)
+
+#define MMD_EEE_LP_ADV			0x3D
+#define MMD_EEE_MSG_CODE		0x3F
+
+#define MMD_DEVICE_ID_AFED		0x1C
+
+#define REG_PORT_PHY_EXTENDED_STATUS	0x011E
+
+#define PORT_100BTX_FD_ABLE		(1 << 15)
+#define PORT_100BTX_ABLE		(1 << 14)
+#define PORT_10BT_FD_ABLE		(1 << 13)
+#define PORT_10BT_ABLE			(1 << 12)
+
+#define REG_PORT_SGMII_ADDR__4		0x0200
+#define PORT_SGMII_AUTO_INCR		(1 << 23)
+#define PORT_SGMII_DEVICE_ID_M		0x1F
+#define PORT_SGMII_DEVICE_ID_S		16
+#define PORT_SGMII_ADDR_M		((1 << 21) - 1)
+
+#define REG_PORT_SGMII_DATA__4		0x0204
+#define PORT_SGMII_DATA_M		((1 << 16) - 1)
+
+#define MMD_DEVICE_ID_PMA		0x01
+#define MMD_DEVICE_ID_PCS		0x03
+#define MMD_DEVICE_ID_PHY_XS		0x04
+#define MMD_DEVICE_ID_DTE_XS		0x05
+#define MMD_DEVICE_ID_AN		0x07
+#define MMD_DEVICE_ID_VENDOR_CTRL	0x1E
+#define MMD_DEVICE_ID_VENDOR_MII	0x1F
+
+#define SR_MII				MMD_DEVICE_ID_VENDOR_MII
+
+#define MMD_SR_MII_CTRL			0x0000
+
+#define SR_MII_RESET			(1 << 15)
+#define SR_MII_LOOPBACK			(1 << 14)
+#define SR_MII_SPEED_100MBIT		(1 << 13)
+#define SR_MII_AUTO_NEG_ENABLE		(1 << 12)
+#define SR_MII_POWER_DOWN		(1 << 11)
+#define SR_MII_AUTO_NEG_RESTART		(1 << 9)
+#define SR_MII_FULL_DUPLEX		(1 << 8)
+#define SR_MII_SPEED_1000MBIT		(1 << 6)
+
+#define MMD_SR_MII_STATUS		0x0001
+#define MMD_SR_MII_ID_1			0x0002
+#define MMD_SR_MII_ID_2			0x0003
+#define MMD_SR_MII_AUTO_NEGOTIATION	0x0004
+
+#define SR_MII_AUTO_NEG_NEXT_PAGE	(1 << 15)
+#define SR_MII_AUTO_NEG_REMOTE_FAULT_M	0x3
+#define SR_MII_AUTO_NEG_REMOTE_FAULT_S	12
+#define SR_MII_AUTO_NEG_NO_ERROR	0
+#define SR_MII_AUTO_NEG_OFFLINE		1
+#define SR_MII_AUTO_NEG_LINK_FAILURE	2
+#define SR_MII_AUTO_NEG_ERROR		3
+#define SR_MII_AUTO_NEG_PAUSE_M		0x3
+#define SR_MII_AUTO_NEG_PAUSE_S		7
+#define SR_MII_AUTO_NEG_NO_PAUSE	0
+#define SR_MII_AUTO_NEG_ASYM_PAUSE_TX	1
+#define SR_MII_AUTO_NEG_SYM_PAUSE	2
+#define SR_MII_AUTO_NEG_ASYM_PAUSE_RX	3
+#define SR_MII_AUTO_NEG_HALF_DUPLEX	(1 << 6)
+#define SR_MII_AUTO_NEG_FULL_DUPLEX	(1 << 5)
+
+#define MMD_SR_MII_REMOTE_CAPABILITY	0x0005
+
+#define SR_MII_REMOTE_ACK		BIT(14)
+#define SR_MII_REMOTE_HALF_DUPLEX	BIT(6)
+#define SR_MII_REMOTE_FULL_DUPLEX	BIT(5)
+
+#define MMD_SR_MII_AUTO_NEG_EXP		0x0006
+#define MMD_SR_MII_AUTO_NEG_EXT		0x000F
+
+#define MMD_SR_MII_DIGITAL_CTRL_1	0x8000
+
+
+#define MMD_SR_MII_AUTO_NEG_CTRL	0x8001
+
+#define SR_MII_8_BIT			(1 << 8)
+#define SR_MII_SGMII_LINK_UP		(1 << 4)
+#define SR_MII_TX_CFG_PHY_MASTER	(1 << 3)
+#define SR_MII_PCS_MODE_M		0x3
+#define SR_MII_PCS_MODE_S		1
+#define SR_MII_PCS_SGMII		2
+#define SR_MII_AUTO_NEG_COMPLETE_INTR	(1 << 0)
+
+#define MMD_SR_MII_AUTO_NEG_STATUS	0x8002
+
+#define SR_MII_STAT_LINK_UP		(1 << 4)
+#define SR_MII_STAT_M			0x3
+#define SR_MII_STAT_S			2
+#define SR_MII_STAT_10_MBPS		0
+#define SR_MII_STAT_100_MBPS		1
+#define SR_MII_STAT_1000_MBPS		2
+#define SR_MII_STAT_FULL_DUPLEX		(1 << 1)
+
+#define MMD_SR_MII_PHY_CTRL		0x80A0
+
+#define SR_MII_PHY_LANE_SEL_M		0xF
+#define SR_MII_PHY_LANE_SEL_S		8
+#define SR_MII_PHY_WRITE		(1 << 1)
+#define SR_MII_PHY_START_BUSY		(1 << 0)
+
+#define MMD_SR_MII_PHY_ADDR		0x80A1
+
+#define SR_MII_PHY_ADDR_M		((1 << 16) - 1)
+
+#define MMD_SR_MII_PHY_DATA		0x80A2
+
+#define SR_MII_PHY_DATA_M		((1 << 16) - 1)
+
+#define SR_MII_PHY_JTAG_CHIP_ID_HI	0x000C
+#define SR_MII_PHY_JTAG_CHIP_ID_LO	0x000D
+
+
+/*
+ * 2016-05-31
+ * PHY registers greater than 15 needs to be written in 32-bit.
+ */
+#define REG_PORT_PHY_REMOTE_LB_LED	0x0122
+
+#define PORT_REMOTE_LOOPBACK		(1 << 8)
+#define PORT_LED_SELECT			(3 << 6)
+#define PORT_LED_CTRL			(3 << 4)
+#define PORT_LED_CTRL_TEST		(1 << 3)
+#define PORT_10BT_PREAMBLE		(1 << 2)
+#define PORT_LINK_MD_10BT_ENABLE	(1 << 1)
+#define PORT_LINK_MD_PASS		(1 << 0)
+
+#define REG_PORT_PHY_LINK_MD		0x0124
+
+#define PORT_START_CABLE_DIAG		(1 << 15)
+#define PORT_TX_DISABLE			(1 << 14)
+#define PORT_CABLE_DIAG_PAIR_M		0x3
+#define PORT_CABLE_DIAG_PAIR_S		12
+#define PORT_CABLE_DIAG_SELECT_M	0x3
+#define PORT_CABLE_DIAG_SELECT_S	10
+#define PORT_CABLE_DIAG_RESULT_M	0x3
+#define PORT_CABLE_DIAG_RESULT_S	8
+#define PORT_CABLE_STAT_NORMAL		0
+#define PORT_CABLE_STAT_OPEN		1
+#define PORT_CABLE_STAT_SHORT		2
+#define PORT_CABLE_STAT_FAILED		3
+#if 0
+#define PORT_CABLE_10M_SHORT		(1 << 12)
+#endif
+#define PORT_CABLE_FAULT_COUNTER	0x00FF
+
+#define REG_PORT_PHY_PMA_STATUS		0x0126
+
+#define PORT_1000_LINK_GOOD		(1 << 1)
+#define PORT_100_LINK_GOOD		(1 << 0)
+
+#define REG_PORT_PHY_DIGITAL_STATUS	0x0128
+
+#define PORT_LINK_DETECT		(1 << 14)
+#define PORT_SIGNAL_DETECT		(1 << 13)
+#define PORT_PHY_STAT_MDI		(1 << 12)
+#define PORT_PHY_STAT_MASTER		(1 << 11)
+
+#define REG_PORT_PHY_RXER_COUNTER	0x012A
+
+#define REG_PORT_PHY_INT_ENABLE		0x0136
+#define REG_PORT_PHY_INT_STATUS		0x0137
+
+#define JABBER_INT			(1 << 7)
+#define RX_ERR_INT			(1 << 6)
+#define PAGE_RX_INT			(1 << 5)
+#define PARALLEL_DETECT_FAULT_INT	(1 << 4)
+#define LINK_PARTNER_ACK_INT		(1 << 3)
+#define LINK_DOWN_INT			(1 << 2)
+#define REMOTE_FAULT_INT		(1 << 1)
+#define LINK_UP_INT			(1 << 0)
+
+#define REG_PORT_PHY_DIGITAL_DEBUG_1	0x0138
+
+#define PORT_REG_CLK_SPEED_25_MHZ	(1 << 14)
+#define PORT_PHY_FORCE_MDI		(1 << 7)
+#define PORT_PHY_AUTO_MDIX_DISABLE	(1 << 6)
+
+/* Same as PORT_PHY_LOOPBACK */
+#define PORT_PHY_PCS_LOOPBACK		(1 << 0)
+
+#define REG_PORT_PHY_DIGITAL_DEBUG_2	0x013A
+
+#define REG_PORT_PHY_DIGITAL_DEBUG_3	0x013C
+
+#define PORT_100BT_FIXED_LATENCY	(1 << 15)
+#define PORT_SINGLE_LED_MODE		(1 << 9)
+
+#define REG_PORT_PHY_PHY_CTRL		0x013E
+
+#define PORT_INT_PIN_HIGH		(1 << 14)
+#define PORT_ENABLE_JABBER		(1 << 9)
+#define PORT_STAT_SPEED_1000MBIT	(1 << 6)
+#define PORT_STAT_SPEED_100MBIT		(1 << 5)
+#define PORT_STAT_SPEED_10MBIT		(1 << 4)
+#define PORT_STAT_FULL_DUPLEX		(1 << 3)
+
+/* Same as PORT_PHY_STAT_MASTER */
+#define PORT_STAT_MASTER		(1 << 2)
+#define PORT_RESET			(1 << 1)
+#define PORT_LINK_STATUS_FAIL		(1 << 0)
+
+/* 3 - xMII */
+#define REG_PORT_XMII_CTRL_0		0x0300
+
+#define PORT_SGMII_SEL			(1 << 7)
+#define PORT_MII_FULL_DUPLEX		(1 << 6)
+#define PORT_MII_TX_FLOW_CTRL		(1 << 5)
+#define PORT_MII_100MBIT		(1 << 4)
+#define PORT_MII_RX_FLOW_CTRL		(1 << 3)
+#define PORT_GRXC_ENABLE		(1 << 0)
+
+#define REG_PORT_XMII_CTRL_1		0x0301
+
+#define PORT_RMII_CLK_SEL		(1 << 7)
+/* S1 */
+#define PORT_MII_1000MBIT_S1		(1 << 6)
+/* S2 */
+#define PORT_MII_NOT_1GBIT		(1 << 6)
+#define PORT_MII_SEL_EDGE		(1 << 5)
+#define PORT_RGMII_ID_IG_ENABLE		(1 << 4)
+#define PORT_RGMII_ID_EG_ENABLE		(1 << 3)
+#define PORT_MII_MAC_MODE		(1 << 2)
+#define PORT_MII_SEL_M			0x3
+/* S1 */
+#define PORT_MII_SEL_S1			0x0
+#define PORT_RMII_SEL_S1		0x1
+#define PORT_GMII_SEL_S1		0x2
+#define PORT_RGMII_SEL_S1		0x3
+/* S2 */
+#define PORT_RGMII_SEL			0x0
+#define PORT_RMII_SEL			0x1
+#define PORT_GMII_SEL			0x2
+#define PORT_MII_SEL			0x3
+
+/* 4 - MAC */
+#define REG_PORT_MAC_CTRL_0		0x0400
+
+#define PORT_BROADCAST_STORM		(1 << 1)
+#define PORT_JUMBO_FRAME		(1 << 0)
+
+#define REG_PORT_MAC_CTRL_1		0x0401
+
+#define PORT_BACK_PRESSURE		(1 << 3)
+#define PORT_PASS_ALL			(1 << 0)
+
+#define REG_PORT_MAC_CTRL_2		0x0402
+
+#define PORT_100BT_EEE_DISABLE		(1 << 7)
+#define PORT_1000BT_EEE_DISABLE		(1 << 6)
+
+#define REG_PORT_MAC_IN_RATE_LIMIT	0x0403
+
+#define PORT_IN_PORT_BASED_S		6
+#define PORT_RATE_PACKET_BASED_S	5
+#define PORT_IN_FLOW_CTRL_S		4
+#define PORT_COUNT_IFG_S		1
+#define PORT_COUNT_PREAMBLE_S		0
+#define PORT_IN_PORT_BASED		(1 << 6)
+#define PORT_IN_PACKET_BASED		(1 << 5)
+#define PORT_IN_FLOW_CTRL		(1 << 4)
+#define PORT_IN_LIMIT_MODE_M		0x3
+#define PORT_IN_LIMIT_MODE_S		2
+#define PORT_IN_ALL			0
+#define PORT_IN_UNICAST			1
+#define PORT_IN_MULTICAST		2
+#define PORT_IN_BROADCAST		3
+#define PORT_COUNT_IFG			(1 << 1)
+#define PORT_COUNT_PREAMBLE		(1 << 0)
+
+#define REG_PORT_IN_RATE_0		0x0410
+#define REG_PORT_IN_RATE_1		0x0411
+#define REG_PORT_IN_RATE_2		0x0412
+#define REG_PORT_IN_RATE_3		0x0413
+#define REG_PORT_IN_RATE_4		0x0414
+#define REG_PORT_IN_RATE_5		0x0415
+#define REG_PORT_IN_RATE_6		0x0416
+#define REG_PORT_IN_RATE_7		0x0417
+
+#define REG_PORT_OUT_RATE_0		0x0420
+#define REG_PORT_OUT_RATE_1		0x0421
+#define REG_PORT_OUT_RATE_2		0x0422
+#define REG_PORT_OUT_RATE_3		0x0423
+
+#define PORT_RATE_LIMIT_M		((1 << 7) - 1)
+
+/* 5 - MIB Counters */
+#define REG_PORT_MIB_CTRL_STAT__4	0x0500
+
+#define MIB_COUNTER_OVERFLOW		(1 << 31)
+#define MIB_COUNTER_VALID		(1 << 30)
+#define MIB_COUNTER_READ		(1 << 25)
+#define MIB_COUNTER_FLUSH_FREEZE	(1 << 24)
+#define MIB_COUNTER_INDEX_M		((1 << 8) - 1)
+#define MIB_COUNTER_INDEX_S		16
+#define MIB_COUNTER_DATA_HI_M		0xF
+
+#define REG_PORT_MIB_DATA		0x0504
+
+/* 6 - ACL */
+#define REG_PORT_ACL_0			0x0600
+
+#define ACL_FIRST_RULE_M		0xF
+
+#define REG_PORT_ACL_1			0x0601
+
+#define ACL_MODE_M			0x3
+#define ACL_MODE_S			4
+#define ACL_MODE_DISABLE		0
+#define ACL_MODE_LAYER_2		1
+#define ACL_MODE_LAYER_3		2
+#define ACL_MODE_LAYER_4		3
+#define ACL_ENABLE_M			0x3
+#define ACL_ENABLE_S			2
+#define ACL_ENABLE_2_COUNT		0
+#define ACL_ENABLE_2_TYPE		1
+#define ACL_ENABLE_2_MAC		2
+#define ACL_ENABLE_2_BOTH		3
+#define ACL_ENABLE_3_IP			1
+#define ACL_ENABLE_3_SRC_DST_COMP	2
+#define ACL_ENABLE_4_PROTOCOL		0
+#define ACL_ENABLE_4_TCP_PORT_COMP	1
+#define ACL_ENABLE_4_UDP_PORT_COMP	2
+#define ACL_ENABLE_4_TCP_SEQN_COMP	3
+#define ACL_SRC				(1 << 1)
+#define ACL_EQUAL			(1 << 0)
+
+#define REG_PORT_ACL_2			0x0602
+#define REG_PORT_ACL_3			0x0603
+
+#define ACL_MAX_PORT			0xFFFF
+
+#define REG_PORT_ACL_4			0x0604
+#define REG_PORT_ACL_5			0x0605
+
+#define ACL_MIN_PORT			0xFFFF
+#define ACL_IP_ADDR			0xFFFFFFFF
+#define ACL_TCP_SEQNUM			0xFFFFFFFF
+
+#define REG_PORT_ACL_6			0x0606
+
+#define ACL_RESERVED			0xF8
+#define ACL_PORT_MODE_M			0x3
+#define ACL_PORT_MODE_S			1
+#define ACL_PORT_MODE_DISABLE		0
+#define ACL_PORT_MODE_EITHER		1
+#define ACL_PORT_MODE_IN_RANGE		2
+#define ACL_PORT_MODE_OUT_OF_RANGE	3
+
+#define REG_PORT_ACL_7			0x0607
+
+#define ACL_TCP_FLAG_ENABLE		(1 << 0)
+
+#define REG_PORT_ACL_8			0x0608
+
+#define ACL_TCP_FLAG_M			0xFF
+
+#define REG_PORT_ACL_9			0x0609
+
+#define ACL_TCP_FLAG			0xFF
+#define ACL_ETH_TYPE			0xFFFF
+#define ACL_IP_M			0xFFFFFFFF
+
+#define REG_PORT_ACL_A			0x060A
+
+#define ACL_PRIO_MODE_M			0x3
+#define ACL_PRIO_MODE_S			6
+#define ACL_PRIO_MODE_DISABLE		0
+#define ACL_PRIO_MODE_HIGHER		1
+#define ACL_PRIO_MODE_LOWER		2
+#define ACL_PRIO_MODE_REPLACE		3
+#define ACL_PRIO_M			KS_PRIO_M
+#define ACL_PRIO_S			3
+#define ACL_VLAN_PRIO_REPLACE		(1 << 2)
+#define ACL_VLAN_PRIO_M			KS_PRIO_M
+#define ACL_VLAN_PRIO_HI_M		0x3
+
+#define REG_PORT_ACL_B			0x060B
+
+#define ACL_VLAN_PRIO_LO_M		0x8
+#define ACL_VLAN_PRIO_S			7
+#define ACL_MAP_MODE_M			0x3
+#define ACL_MAP_MODE_S			5
+#define ACL_MAP_MODE_DISABLE		0
+#define ACL_MAP_MODE_OR			1
+#define ACL_MAP_MODE_AND		2
+#define ACL_MAP_MODE_REPLACE		3
+
+#define ACL_CNT_M			((1 << 11) - 1)
+#define ACL_CNT_S			5
+
+#define REG_PORT_ACL_C			0x060C
+
+#define REG_PORT_ACL_D			0x060D
+#define ACL_MSEC_UNIT			(1 << 6)
+#define ACL_INTR_MODE			(1 << 5)
+#define ACL_PORT_MAP			0x7F
+
+#define REG_PORT_ACL_E			0x060E
+#define REG_PORT_ACL_F			0x060F
+
+#define REG_PORT_ACL_BYTE_EN_MSB	0x0610
+#define REG_PORT_ACL_BYTE_EN_LSB	0x0611
+
+#define ACL_ACTION_START		0xA
+#define ACL_ACTION_LEN			4
+#define ACL_INTR_CNT_START		0xD
+#define ACL_RULESET_START		0xE
+#define ACL_RULESET_LEN			2
+#define ACL_TABLE_LEN			16
+
+#define ACL_ACTION_ENABLE		0x003C
+#define ACL_MATCH_ENABLE		0x7FC0
+#define ACL_RULESET_ENABLE		0x8003
+#define ACL_BYTE_ENABLE			0xFFFF
+
+#define REG_PORT_ACL_CTRL_0		0x0612
+
+#define PORT_ACL_WRITE_DONE		(1 << 6)
+#define PORT_ACL_READ_DONE		(1 << 5)
+#define PORT_ACL_WRITE			(1 << 4)
+#define PORT_ACL_INDEX_M		0xF
+
+#define REG_PORT_ACL_CTRL_1		0x0613
+
+/* 8 - Classification and Policing */
+#define REG_PORT_MRI_MIRROR_CTRL	0x0800
+
+#define PORT_MIRROR_RX			(1 << 6)
+#define PORT_MIRROR_TX			(1 << 5)
+#define PORT_MIRROR_SNIFFER		(1 << 1)
+
+#define REG_PORT_MRI_PRIO_CTRL		0x0801
+
+#define PORT_HIGHEST_PRIO		(1 << 7)
+#define PORT_OR_PRIO			(1 << 6)
+#define PORT_MAC_PRIO_ENABLE		(1 << 4)
+#define PORT_VLAN_PRIO_ENABLE		(1 << 3)
+#define PORT_802_1P_PRIO_ENABLE		(1 << 2)
+#define PORT_DIFFSERV_PRIO_ENABLE	(1 << 1)
+#define PORT_ACL_PRIO_ENABLE		(1 << 0)
+
+#define REG_PORT_MRI_MAC_CTRL		0x0802
+
+#define PORT_USER_PRIO_CEILING		(1 << 7)
+#define PORT_DROP_NON_VLAN		(1 << 4)
+#define PORT_DROP_TAG			(1 << 3)
+#define PORT_BASED_PRIO_M		KS_PRIO_M
+#define PORT_BASED_PRIO_S		0
+
+#define REG_PORT_MRI_AUTHEN_CTRL	0x0803
+
+#define PORT_ACL_ENABLE			(1 << 2)
+#define PORT_AUTHEN_MODE		0x3
+#define PORT_AUTHEN_NORMAL		0
+#define PORT_AUTHEN_BLOCK		1
+#define PORT_AUTHEN_PASS		2
+#define PORT_AUTHEN_TRAP		3
+
+#define REG_PORT_MRI_INDEX__4		0x0804
+
+#define MRI_INDEX_P_M			0x7
+#define MRI_INDEX_P_S			16
+#define MRI_INDEX_Q_M			0x3
+#define MRI_INDEX_Q_S			0
+
+#define REG_PORT_MRI_TC_MAP__4		0x0808
+
+#define PORT_TC_MAP_M			0xf
+#define PORT_TC_MAP_S			4
+
+#define REG_PORT_MRI_POLICE_CTRL__4	0x080C
+
+#define POLICE_DROP_ALL			(1 << 10)
+#define POLICE_PACKET_TYPE_M		0x3
+#define POLICE_PACKET_TYPE_S		8
+#define POLICE_PACKET_DROPPED		0
+#define POLICE_PACKET_GREEN		1
+#define POLICE_PACKET_YELLOW		2
+#define POLICE_PACKET_RED		3
+#define PORT_BASED_POLICING		(1 << 7)
+#define NON_DSCP_COLOR_M		0x3
+#define NON_DSCP_COLOR_S		5
+#define COLOR_MARK_ENABLE		(1 << 4)
+#define COLOR_REMAP_ENABLE		(1 << 3)
+#define POLICE_DROP_SRP			(1 << 2)
+#define POLICE_COLOR_NOT_AWARE		(1 << 1)
+#define POLICE_ENABLE			(1 << 0)
+
+#define REG_PORT_POLICE_COLOR_0__4	0x0810
+#define REG_PORT_POLICE_COLOR_1__4	0x0814
+#define REG_PORT_POLICE_COLOR_2__4	0x0818
+#define REG_PORT_POLICE_COLOR_3__4	0x081C
+
+#define POLICE_COLOR_MAP_S		2
+#define POLICE_COLOR_MAP_M		((1 << POLICE_COLOR_MAP_S) - 1)
+
+#define REG_PORT_POLICE_RATE__4		0x0820
+
+#define POLICE_CIR_S			16
+#define POLICE_PIR_S			0
+
+#define REG_PORT_POLICE_BURST_SIZE__4	0x0824
+
+#define POLICE_BURST_SIZE_M		0x3FFF
+#define POLICE_CBS_S			16
+#define POLICE_PBS_S			0
+
+#define REG_PORT_WRED_PM_CTRL_0__4	0x0830
+
+#define WRED_PM_CTRL_M			((1 << 11) - 1)
+
+#define WRED_PM_MAX_THRESHOLD_S		16
+#define WRED_PM_MIN_THRESHOLD_S		0
+
+#define REG_PORT_WRED_PM_CTRL_1__4	0x0834
+
+#define WRED_PM_MULTIPLIER_S		16
+#define WRED_PM_AVG_QUEUE_SIZE_S	0
+
+#define REG_PORT_WRED_QUEUE_CTRL_0__4	0x0840
+#define REG_PORT_WRED_QUEUE_CTRL_1__4	0x0844
+
+#define REG_PORT_WRED_QUEUE_PMON__4	0x0848
+
+#define WRED_RANDOM_DROP_ENABLE		(1 << 31)
+#define WRED_PMON_FLUSH			(1 << 30)
+#define WRED_DROP_GYR_DISABLE		(1 << 29)
+#define WRED_DROP_YR_DISABLE		(1 << 28)
+#define WRED_DROP_R_DISABLE		(1 << 27)
+#define WRED_DROP_ALL			(1 << 26)
+#define WRED_PMON_M			((1 << 24) - 1)
+
+/* 9 - Shaping */
+
+#define REG_PORT_MTI_QUEUE_INDEX__4	0x0900
+
+#define REG_PORT_MTI_QUEUE_CTRL_0__4	0x0904
+
+#define MTI_PVID_REPLACE		(1 << 0)
+
+#define REG_PORT_MTI_QUEUE_CTRL_0	0x0914
+
+#define MTI_SCHEDULE_MODE_M		0x3
+#define MTI_SCHEDULE_MODE_S		6
+#define MTI_SCHEDULE_STRICT_PRIO	0
+#define MTI_SCHEDULE_WRR		2
+#define MTI_SHAPING_M			0x3
+#define MTI_SHAPING_S			4
+#define MTI_SHAPING_OFF			0
+#define MTI_SHAPING_SRP			1
+#define MTI_SHAPING_TIME_AWARE		2
+#if 0
+#define MTI_PREEMPT_ENABLE		(1 << 3)
+#endif
+
+#define REG_PORT_MTI_QUEUE_CTRL_1	0x0915
+
+#define MTI_TX_RATIO_M			((1 << 7) - 1)
+
+#define REG_PORT_MTI_QUEUE_CTRL_2__2	0x0916
+#define REG_PORT_MTI_HI_WATER_MARK	0x0916
+#define REG_PORT_MTI_QUEUE_CTRL_3__2	0x0918
+#define REG_PORT_MTI_LO_WATER_MARK	0x0918
+#define REG_PORT_MTI_QUEUE_CTRL_4__2	0x091A
+#define REG_PORT_MTI_CREDIT_INCREMENT	0x091A
+
+/* A - QM */
+
+#define REG_PORT_QM_CTRL__4		0x0A00
+
+#define PORT_QM_DROP_PRIO_M		0x3
+
+#define REG_PORT_VLAN_MEMBERSHIP__4	0x0A04
+
+#define REG_PORT_QM_QUEUE_INDEX__4	0x0A08
+
+#define PORT_QM_QUEUE_INDEX_S		24
+#define PORT_QM_BURST_SIZE_S		16
+#define PORT_QM_MIN_RESV_SPACE_M	((1 << 11) - 1)
+
+#define REG_PORT_QM_WATER_MARK__4	0x0A0C
+
+#define PORT_QM_HI_WATER_MARK_S		16
+#define PORT_QM_LO_WATER_MARK_S		0
+#define PORT_QM_WATER_MARK_M		((1 << 11) - 1)
+
+#define REG_PORT_QM_TX_CNT_0__4		0x0A10
+
+#define PORT_QM_TX_CNT_USED_S		0
+#define PORT_QM_TX_CNT_M		((1 << 11) - 1)
+
+#define REG_PORT_QM_TX_CNT_1__4		0x0A14
+
+#define PORT_QM_TX_CNT_CALCULATED_S	16
+#define PORT_QM_TX_CNT_AVAIL_S		0
+
+/* B - LUE */
+#define REG_PORT_LUE_CTRL		0x0B00
+
+#define PORT_VLAN_LOOKUP_VID_0		(1 << 7)
+#define PORT_INGRESS_FILTER		(1 << 6)
+#define PORT_DISCARD_NON_VID		(1 << 5)
+#define PORT_MAC_BASED_802_1X		(1 << 4)
+#define PORT_SRC_ADDR_FILTER		(1 << 3)
+
+#define REG_PORT_LUE_MSTP_INDEX		0x0B01
+
+#define REG_PORT_LUE_MSTP_STATE		0x0B04
+
+#define PORT_TX_ENABLE			(1 << 2)
+#define PORT_RX_ENABLE			(1 << 1)
+#define PORT_LEARN_DISABLE		(1 << 0)
+
+/* C - PTP */
+
+#define REG_PTP_PORT_RX_DELAY__2	0x0C00
+#define REG_PTP_PORT_TX_DELAY__2	0x0C02
+#define REG_PTP_PORT_ASYM_DELAY__2	0x0C04
+
+#define REG_PTP_PORT_XDELAY_TS		0x0C08
+#define REG_PTP_PORT_XDELAY_TS_H	0x0C08
+#define REG_PTP_PORT_XDELAY_TS_L	0x0C0A
+
+#define REG_PTP_PORT_SYNC_TS		0x0C0C
+#define REG_PTP_PORT_SYNC_TS_H		0x0C0C
+#define REG_PTP_PORT_SYNC_TS_L		0x0C0E
+
+#define REG_PTP_PORT_PDRESP_TS		0x0C10
+#define REG_PTP_PORT_PDRESP_TS_H	0x0C10
+#define REG_PTP_PORT_PDRESP_TS_L	0x0C12
+
+#define REG_PTP_PORT_TX_INT_STATUS__2	0x0C14
+#define REG_PTP_PORT_TX_INT_ENABLE__2	0x0C16
+
+#define PTP_PORT_SYNC_INT		(1 << 15)
+#define PTP_PORT_XDELAY_REQ_INT		(1 << 14)
+#define PTP_PORT_PDELAY_RESP_INT	(1 << 13)
+
+#define REG_PTP_PORT_LINK_DELAY__4	0x0C18
+
+
+/* Default values are used in ksz_sw_9897.h if these are not defined. */
+#define PRIO_QUEUES			4
+#define RX_PRIO_QUEUES			8
+
+#define KS_PRIO_IN_REG			2
+
+#define TOTAL_PORT_NUM			7
+
+#define KSZ9897_COUNTER_NUM		0x20
+#define TOTAL_KSZ9897_COUNTER_NUM	(KSZ9897_COUNTER_NUM + 2 + 2)
+
+#define SWITCH_COUNTER_NUM		KSZ9897_COUNTER_NUM
+#define TOTAL_SWITCH_COUNTER_NUM	TOTAL_KSZ9897_COUNTER_NUM
+
+/* Required for common switch control in ksz_sw_9897.c */
+#define SW_D				u8
+#define SW_R(sw, addr)			(sw)->reg->r8(sw, addr)
+#define SW_W(sw, addr, val)		(sw)->reg->w8(sw, addr, val)
+#define SW_SIZE				(1)
+#define SW_SIZE_STR			"%02x"
+#define port_r				port_r8
+#define port_w				port_w8
+
+#define P_BCAST_STORM_CTRL		REG_PORT_MAC_CTRL_0
+#define P_PRIO_CTRL			REG_PORT_MRI_PRIO_CTRL
+#define P_MIRROR_CTRL			REG_PORT_MRI_MIRROR_CTRL
+#define P_STP_CTRL			REG_PORT_LUE_MSTP_STATE
+#define P_PHY_CTRL			REG_PORT_PHY_CTRL
+#define P_NEG_RESTART_CTRL		REG_PORT_PHY_CTRL
+#define P_LINK_STATUS			REG_PORT_PHY_STATUS
+#define P_SPEED_STATUS			REG_PORT_PHY_PHY_CTRL
+#define P_RATE_LIMIT_CTRL		REG_PORT_MAC_IN_RATE_LIMIT
+
+#define S_LINK_AGING_CTRL		REG_SW_LUE_CTRL_1
+#define S_MIRROR_CTRL			REG_SW_MRI_CTRL_0
+#define S_REPLACE_VID_CTRL		REG_SW_MAC_CTRL_2
+#define S_802_1P_PRIO_CTRL		REG_SW_MAC_802_1P_MAP_0
+#define S_TOS_PRIO_CTRL			REG_SW_MAC_TOS_PRIO_0
+#define S_FLUSH_TABLE_CTRL		REG_SW_LUE_CTRL_1
+
+#define REG_SWITCH_RESET		REG_RESET_CTRL
+
+#define SW_FLUSH_DYN_MAC_TABLE		SW_FLUSH_MSTP_TABLE
+
+
+#define MAX_TIMESTAMP_UNIT		2
+#define MAX_TRIG_UNIT			3
+#define MAX_TIMESTAMP_EVENT_UNIT	8
+#define MAX_GPIO			2
+
+#define PTP_TRIG_UNIT_M			((1 << MAX_TRIG_UNIT) - 1)
+#define PTP_TS_UNIT_M			((1 << MAX_TIMESTAMP_UNIT) - 1)
+
+#define TAIL_TAG_PTP			BIT(7)
+#define TAIL_TAG_RX_PORTS_M		0x7
+
+#endif
diff -Napur linux-linux4microchip-2022.10-orig/drivers/net/ethernet/micrel/ksz_cfg_8463.h linux-linux4microchip-2022.10/drivers/net/ethernet/micrel/ksz_cfg_8463.h
--- linux-linux4microchip-2022.10-orig/drivers/net/ethernet/micrel/ksz_cfg_8463.h	1969-12-31 16:00:00.000000000 -0800
+++ linux-linux4microchip-2022.10/drivers/net/ethernet/micrel/ksz_cfg_8463.h	2023-04-25 16:13:54.872163472 -0700
@@ -0,0 +1,39 @@
+/**
+ * This file contains shared configurations between the network and switch
+ * drivers.
+ */
+
+
+#ifndef KSZ_CFG_8463_H
+#define KSZ_CFG_8463_H
+
+#if defined(CONFIG_KSZ_PTP)
+/* Support 1588 PTP. */
+#define CONFIG_1588_PTP
+#define PTP_SPI
+#endif
+
+#ifdef CONFIG_KSZ_MRP
+/* MRP code not implemented yet. */
+#undef CONFIG_KSZ_MRP
+#undef CONFIG_KSZ_MMRP
+#undef CONFIG_KSZ_MVRP
+#undef CONFIG_KSZ_MSRP
+#endif
+/* Can be defined if KSZ9897 driver is included also. */
+#undef CONFIG_KSZ_DLR
+#undef CONFIG_KSZ_HSR
+
+/* KSZ8463 and KSZ8863 use same ksz_sw.c code. */
+#undef CONFIG_HAVE_KSZ8863
+
+#include "ksz_common.h"
+
+#include "ksz_req.h"
+
+#include "ksz846x.h"
+#include "ksz8463.h"
+#include "ksz_sw.h"
+
+#endif
+
diff -Napur linux-linux4microchip-2022.10-orig/drivers/net/ethernet/micrel/ksz_cfg_8795.h linux-linux4microchip-2022.10/drivers/net/ethernet/micrel/ksz_cfg_8795.h
--- linux-linux4microchip-2022.10-orig/drivers/net/ethernet/micrel/ksz_cfg_8795.h	1969-12-31 16:00:00.000000000 -0800
+++ linux-linux4microchip-2022.10/drivers/net/ethernet/micrel/ksz_cfg_8795.h	2023-04-25 16:13:54.872163472 -0700
@@ -0,0 +1,29 @@
+/**
+ * This file contains shared configurations between the network and switch
+ * drivers.
+ */
+
+
+#ifndef KSZ_CFG_8795_H
+#define KSZ_CFG_8795_H
+
+#ifdef CONFIG_KSZ_MRP
+/* MRP code not implemented yet. */
+#undef CONFIG_KSZ_MRP
+#undef CONFIG_KSZ_MMRP
+#undef CONFIG_KSZ_MVRP
+#undef CONFIG_KSZ_MSRP
+#endif
+/* Can be defined if KSZ9897 driver is included also. */
+#undef CONFIG_KSZ_HSR
+
+
+#include "ksz_common.h"
+
+#include "ksz_req.h"
+
+#include "ksz8795.h"
+#include "ksz_sw_8795.h"
+
+#endif
+
diff -Napur linux-linux4microchip-2022.10-orig/drivers/net/ethernet/micrel/ksz_cfg_8863.h linux-linux4microchip-2022.10/drivers/net/ethernet/micrel/ksz_cfg_8863.h
--- linux-linux4microchip-2022.10-orig/drivers/net/ethernet/micrel/ksz_cfg_8863.h	1969-12-31 16:00:00.000000000 -0800
+++ linux-linux4microchip-2022.10/drivers/net/ethernet/micrel/ksz_cfg_8863.h	2023-04-25 16:13:54.872163472 -0700
@@ -0,0 +1,33 @@
+/**
+ * This file contains shared configurations between the network and switch
+ * drivers.
+ */
+
+
+#ifndef KSZ_CFG_8863_H
+#define KSZ_CFG_8863_H
+
+#ifdef CONFIG_KSZ_MRP
+/* MRP code not implemented yet. */
+#undef CONFIG_KSZ_MRP
+#undef CONFIG_KSZ_MMRP
+#undef CONFIG_KSZ_MVRP
+#undef CONFIG_KSZ_MSRP
+#endif
+/* Can be defined if KSZ9897 driver is included also. */
+#undef CONFIG_KSZ_DLR
+#undef CONFIG_KSZ_HSR
+
+/* KSZ8463 and KSZ8863 use same ksz_sw.c code. */
+#undef CONFIG_HAVE_KSZ8463
+
+#include "ksz_common.h"
+
+/* For ksz_request used by PTP or MRP driver. */
+#include "ksz_req.h"
+
+#include "ksz8863.h"
+#include "ksz_sw.h"
+
+#endif
+
diff -Napur linux-linux4microchip-2022.10-orig/drivers/net/ethernet/micrel/ksz_cfg_8895.h linux-linux4microchip-2022.10/drivers/net/ethernet/micrel/ksz_cfg_8895.h
--- linux-linux4microchip-2022.10-orig/drivers/net/ethernet/micrel/ksz_cfg_8895.h	1969-12-31 16:00:00.000000000 -0800
+++ linux-linux4microchip-2022.10/drivers/net/ethernet/micrel/ksz_cfg_8895.h	2023-04-25 16:13:54.872163472 -0700
@@ -0,0 +1,30 @@
+/**
+ * This file contains shared configurations between the network and switch
+ * drivers.
+ */
+
+
+#ifndef KSZ_CFG_8895_H
+#define KSZ_CFG_8895_H
+
+#ifdef CONFIG_KSZ_MRP
+/* MRP code not implemented yet. */
+#undef CONFIG_KSZ_MRP
+#undef CONFIG_KSZ_MMRP
+#undef CONFIG_KSZ_MVRP
+#undef CONFIG_KSZ_MSRP
+#endif
+/* Can be defined if KSZ9897 driver is included also. */
+#undef CONFIG_KSZ_DLR
+#undef CONFIG_KSZ_HSR
+
+
+#include "ksz_common.h"
+
+#include "ksz_req.h"
+
+#include "ksz8895.h"
+#include "ksz_sw_8895.h"
+
+#endif
+
diff -Napur linux-linux4microchip-2022.10-orig/drivers/net/ethernet/micrel/ksz_cfg_9897.h linux-linux4microchip-2022.10/drivers/net/ethernet/micrel/ksz_cfg_9897.h
--- linux-linux4microchip-2022.10-orig/drivers/net/ethernet/micrel/ksz_cfg_9897.h	1969-12-31 16:00:00.000000000 -0800
+++ linux-linux4microchip-2022.10/drivers/net/ethernet/micrel/ksz_cfg_9897.h	2023-04-25 16:13:54.872163472 -0700
@@ -0,0 +1,29 @@
+/**
+ * This file contains shared configurations between the network and switch
+ * drivers.
+ */
+
+
+#ifndef KSZ_CFG_9897_H
+#define KSZ_CFG_9897_H
+
+#if defined(CONFIG_KSZ_PTP)
+/* Support 1588 PTP. */
+#define CONFIG_1588_PTP
+#endif
+
+#if defined(CONFIG_IBA_KSZ9897)
+#define CONFIG_KSZ_IBA_ONLY
+#endif
+
+
+#include "ksz_common.h"
+
+/* For ksz_request used by PTP or MRP driver. */
+#include "ksz_req.h"
+
+#include "ksz9897.h"
+#include "ksz_sw_9897.h"
+
+#endif
+
diff -Napur linux-linux4microchip-2022.10-orig/drivers/net/ethernet/micrel/ksz_common.c linux-linux4microchip-2022.10/drivers/net/ethernet/micrel/ksz_common.c
--- linux-linux4microchip-2022.10-orig/drivers/net/ethernet/micrel/ksz_common.c	1969-12-31 16:00:00.000000000 -0800
+++ linux-linux4microchip-2022.10/drivers/net/ethernet/micrel/ksz_common.c	2023-04-25 16:13:54.872163472 -0700
@@ -0,0 +1,500 @@
+/**
+ * Microchip Ethernet driver common code
+ *
+ * Copyright (c) 2015-2020 Microchip Technology Inc.
+ *	Tristram Ha <Tristram.Ha@microchip.com>
+ *
+ * Copyright (c) 2009-2011 Micrel, Inc.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ */
+
+
+/**
+ * ksz_start_timer - start kernel timer
+ * @info:	Kernel timer information.
+ * @time:	The time tick.
+ *
+ * This routine starts the kernel timer after the specified time tick.
+ */
+static void ksz_start_timer(struct ksz_timer_info *info, int time)
+{
+	info->cnt = 0;
+	info->timer.expires = jiffies + time;
+	add_timer(&info->timer);
+
+	/* infinity */
+	info->max = -1;
+}  /* ksz_start_timer */
+
+/**
+ * ksz_stop_timer - stop kernel timer
+ * @info:	Kernel timer information.
+ *
+ * This routine stops the kernel timer.
+ */
+static void ksz_stop_timer(struct ksz_timer_info *info)
+{
+	if (info->max) {
+		info->max = 0;
+		del_timer_sync(&info->timer);
+	}
+}  /* ksz_stop_timer */
+
+static void ksz_init_timer(struct ksz_timer_info *info, int period,
+	void (*function)(struct timer_list *t), void *ptr)
+{
+	info->dev = ptr;
+	info->max = 0;
+	info->period = period;
+	timer_setup(&info->timer, function, 0);
+}  /* ksz_init_timer */
+
+static void ksz_update_timer(struct ksz_timer_info *info)
+{
+	++info->cnt;
+	if (info->max > 0) {
+		if (info->cnt < info->max) {
+			info->timer.expires = jiffies + info->period;
+			add_timer(&info->timer);
+		} else
+			info->max = 0;
+	} else if (info->max < 0) {
+		info->timer.expires = jiffies + info->period;
+		add_timer(&info->timer);
+	}
+}  /* ksz_update_timer */
+
+/* -------------------------------------------------------------------------- */
+
+#ifndef NO_FILE_DEV
+static void file_gen_dev_release(struct file_dev_info *info,
+	struct file_dev_info **n)
+{
+	struct file_dev_info *prev = *n;
+
+	if (prev == info) {
+		*n = info->next;
+	} else {
+		while (prev && prev->next != info)
+			prev = prev->next;
+		if (prev)
+		prev->next = info->next;
+	}
+	kfree(info->read_buf);
+	kfree(info->write_buf);
+	kfree(info);
+}  /* file_gen_dev_release */
+
+static void file_dev_setup_msg(struct file_dev_info *info, void *data, int len,
+	void (*func)(void *data, void *param), void *param)
+{
+	u8 *buf = info->read_in;
+	int in_intr = in_interrupt();
+
+	if (!buf)
+		return;
+	if (len > info->read_tmp)
+		len = info->read_tmp;
+	if (!in_intr)
+		mutex_lock(&info->lock);
+	memcpy(buf, data, len);
+	if (func)
+		func(buf, param);
+	len += 2;
+	if (info->read_len + len <= info->read_max) {
+		u16 *msg_len = (u16 *) &info->read_buf[info->read_len];
+
+		*msg_len = len;
+		msg_len++;
+		memcpy(msg_len, buf, len - 2);
+		info->read_len += len;
+	}
+	if (!in_intr)
+		mutex_unlock(&info->lock);
+	wake_up_interruptible(&info->wait_msg);
+}  /* file_dev_setup_msg */
+
+static void file_dev_clear_notify(struct file_dev_info *list,
+	struct file_dev_info *info, u16 mod, uint *notifications)
+{
+	struct file_dev_info *dev_info;
+	uint notify = 0;
+
+	if (!info->notifications)
+		return;
+	dev_info = list;
+	while (dev_info) {
+		if (dev_info != info)
+			notify |= dev_info->notifications[mod];
+		dev_info = dev_info->next;
+	}
+	*notifications = notify;
+	info->notifications[mod] = 0;
+}  /* file_dev_clear_notify */
+#endif
+
+/* -------------------------------------------------------------------------- */
+
+static inline s64 div_s64_s32_rem(s64 val, u32 divisor, s32 *rem)
+{
+	val = div_s64_rem(val, divisor, rem);
+	return val;
+}
+
+static inline u64 div_u64_u32_rem(u64 val, u32 divisor, u32 *rem)
+{
+	val = div_u64_rem(val, divisor, rem);
+	return val;
+}
+
+static inline s64 div_s64_u32(u64 val, u32 divisor)
+{
+	s32 rem;
+
+	val = div_s64_rem(val, divisor, &rem);
+	return val;
+}
+
+static inline u64 div_u64_u32(u64 val, u32 divisor)
+{
+	u32 rem;
+
+	val = div_u64_rem(val, divisor, &rem);
+	return val;
+}
+
+static inline u64 div_s64_s64(s64 val, s64 divisor)
+{
+	val = div64_s64(val, divisor);
+	return val;
+}
+
+static inline u64 div_u64_u64(u64 val, u64 divisor)
+{
+	val = div64_u64(val, divisor);
+	return val;
+}
+
+/* -------------------------------------------------------------------------- */
+
+#define DBG_CH  '-'
+
+#ifdef DEBUG_MSG
+
+/* 2 lines buffer. */
+#define DEBUG_MSG_BUF			(80 * 2)
+
+#define PRINT_MSG_SIZE			(80 * 150)
+#define PRINT_INT_SIZE			(80 * 10)
+
+#define DEBUG_MSG_SIZE			(PRINT_MSG_SIZE + PRINT_INT_SIZE + \
+	DEBUG_MSG_BUF * 2)
+
+struct dbg_print {
+	char *dbg_buf;
+	char *int_buf;
+	char *msg;
+	char *int_msg;
+	int msg_cnt;
+	int int_cnt;
+	int last_msg_line;
+	int last_int_line;
+	unsigned long lock;
+
+	struct work_struct dbg_print;
+	struct ksz_timer_info dbg_timer_info;
+};
+
+static struct dbg_print db;
+
+static void print_buf(char *buf, char **msg, int *cnt, int *last)
+{
+	char ch;
+	char *start;
+
+	if (*last)
+		printk(KERN_INFO "%c\n", DBG_CH);
+	*last = 0;
+	if ('\n' == buf[*cnt - 2] && DBG_CH == buf[*cnt - 1]) {
+		buf[*cnt - 1] = '\0';
+		*last = 1;
+	}
+	*msg = buf;
+
+	/* Kernel seems to limit printk buffer to 1024 bytes. */
+	while (strlen(buf) >= 1024) {
+		start = &buf[1020];
+		while (start != buf && *start != '\n')
+			start--;
+		if (start != buf) {
+			start++;
+			ch = *start;
+			*start = '\0';
+			printk(KERN_INFO "%s", buf);
+			*start = ch;
+			buf = start;
+		}
+	}
+	*cnt = 0;
+	printk(KERN_INFO "%s", buf);
+}  /* print_buf */
+
+static void dbg_print_work(struct work_struct *work)
+{
+	if (db.msg != db.dbg_buf)
+		print_buf(db.dbg_buf, &db.msg, &db.msg_cnt,
+			&db.last_msg_line);
+	if (db.int_msg != db.int_buf) {
+		printk(KERN_INFO "---\n");
+		print_buf(db.int_buf, &db.int_msg, &db.int_cnt,
+			&db.last_int_line);
+		printk(KERN_INFO "+++\n");
+	}
+}  /* dbg_print_work */
+
+static void dbg_monitor(struct timer_list *t)
+{
+	struct ksz_timer_info *info = from_timer(info, t, timer);
+	struct dbg_print *dbp = info->dev;
+
+	dbg_print_work(&dbp->dbg_print);
+	ksz_update_timer(&dbp->dbg_timer_info);
+}  /* dbg_monitor */
+
+static int init_dbg(void)
+{
+	if (db.dbg_buf)
+		return 0;
+	db.dbg_buf = kmalloc(DEBUG_MSG_SIZE, GFP_KERNEL);
+	if (!db.dbg_buf)
+		return -ENOMEM;
+
+	db.msg = db.dbg_buf;
+	*db.msg = '\0';
+	db.int_buf = db.dbg_buf + PRINT_MSG_SIZE + DEBUG_MSG_BUF;
+	db.int_msg = db.int_buf;
+	*db.int_msg = '\0';
+	db.msg_cnt = db.int_cnt = 0;
+	db.last_msg_line = 1;
+	db.last_int_line = 1;
+	db.lock = 0;
+
+	INIT_WORK(&db.dbg_print, dbg_print_work);
+
+	/* 100 ms timeout */
+	ksz_init_timer(&db.dbg_timer_info, 100 * HZ / 1000, dbg_monitor, &db);
+	ksz_start_timer(&db.dbg_timer_info, db.dbg_timer_info.period);
+
+	return 0;
+}  /* init_dbg */
+
+static void exit_dbg(void)
+{
+	if (db.dbg_buf) {
+		ksz_stop_timer(&db.dbg_timer_info);
+		flush_work(&db.dbg_print);
+
+		if (db.msg != db.dbg_buf)
+			printk(KERN_DEBUG "%s\n", db.dbg_buf);
+		if (db.int_msg != db.int_buf)
+			printk(KERN_DEBUG "%s\n", db.int_buf);
+		kfree(db.dbg_buf);
+		db.dbg_buf = NULL;
+	}
+}  /* exit_dbg */
+#endif
+
+static void dbg_msg(char *fmt, ...)
+{
+#ifdef DEBUG_MSG
+	va_list args;
+	char **msg;
+	int *dbg_cnt;
+	int left;
+	int in_intr = in_interrupt();
+	int n;
+
+	dbg_cnt = &db.msg_cnt;
+	msg = &db.msg;
+	left = PRINT_MSG_SIZE - db.msg_cnt - 1;
+	if (left <= 0) {
+		db.last_msg_line = 1;
+		return;
+	}
+
+	/* Called within interrupt routines. */
+	if (in_intr) {
+		/*
+		 * If not able to get lock then put in the interrupt message
+		 * buffer.
+		 */
+		if (test_bit(1, &db.lock)) {
+			dbg_cnt = &db.int_cnt;
+			msg = &db.int_msg;
+			left = PRINT_INT_SIZE - db.int_cnt - 1;
+			in_intr = 0;
+		}
+	} else
+		set_bit(1, &db.lock);
+	va_start(args, fmt);
+	n = vsnprintf(*msg, left + 1, fmt, args);
+	va_end(args);
+	if (n > 0) {
+		if (left > n)
+			left = n;
+		*dbg_cnt += left;
+		*msg += left;
+	}
+	if (!in_intr)
+		clear_bit(1, &db.lock);
+#endif
+}  /* dbg_msg */
+
+/* -------------------------------------------------------------------------- */
+
+static inline void dbp_mac_addr(u8 *addr)
+{
+	dbg_msg("%02x:%02x:%02x:%02x:%02x:%02x",
+		addr[0], addr[1], addr[2],
+		addr[3], addr[4], addr[5]);
+}  /* dbp_mac_addr */
+
+static inline void dbp_pkt(struct sk_buff *skb, char first, char *msg, int hdr)
+{
+	int i;
+	int len = skb->len;
+	u8 *data = (u8 *) skb->data;
+
+	if (!first || first != data[0]) {
+		if (msg)
+			dbg_msg(msg);
+		if (hdr && len > 0x50)
+			len = 0x50;
+		for (i = 0; i < len; i++) {
+			dbg_msg("%02x ", data[i]);
+			if ((i % 16) == 15)
+				dbg_msg("\n");
+		}
+		if ((i % 16))
+			dbg_msg("\n");
+	}
+}  /* dbp_pkt */
+
+/* -------------------------------------------------------------------------- */
+
+/**
+ * delay_micro - delay in microsecond
+ * @microsec:	Number of microseconds to delay.
+ *
+ * This routine delays in microseconds.
+ */
+static inline void delay_micro(uint microsec)
+{
+	uint millisec = microsec / 1000;
+
+	microsec %= 1000;
+	if (millisec)
+		mdelay(millisec);
+	if (microsec)
+		udelay(microsec);
+}
+
+/**
+ * delay_milli - delay in millisecond
+ * @millisec:	Number of milliseconds to delay.
+ *
+ * This routine delays in milliseconds.
+ */
+static inline void delay_milli(uint millisec)
+{
+	unsigned long ticks = millisec * HZ / 1000;
+
+	if (!ticks || in_interrupt())
+		mdelay(millisec);
+	else {
+		set_current_state(TASK_INTERRUPTIBLE);
+		schedule_timeout(ticks);
+	}
+}
+
+/* -------------------------------------------------------------------------- */
+
+#ifndef DO_NOT_USE_COPY_SKB
+static inline void copy_old_skb(struct sk_buff *old, struct sk_buff *skb)
+{
+	if (old->ip_summed) {
+		int offset = old->head - old->data;
+
+		skb->head = skb->data + offset;
+	}
+	skb->dev = old->dev;
+	skb->sk = old->sk;
+	skb->protocol = old->protocol;
+	skb->ip_summed = old->ip_summed;
+	skb->csum = old->csum;
+	skb_shinfo(skb)->tx_flags = skb_shinfo(old)->tx_flags;
+	skb_set_network_header(skb, ETH_HLEN);
+
+	dev_kfree_skb(old);
+}  /* copy_old_skb */
+#endif
+
+/* -------------------------------------------------------------------------- */
+
+#ifdef USE_SHOW_HELP
+enum {
+	SHOW_HELP_NONE,
+	SHOW_HELP_ON_OFF,
+	SHOW_HELP_NUM,
+	SHOW_HELP_HEX,
+	SHOW_HELP_HEX_2,
+	SHOW_HELP_HEX_4,
+	SHOW_HELP_HEX_8,
+	SHOW_HELP_SPECIAL,
+};
+
+static char *help_formats[] = {
+	"",
+	"%d%s\n",
+	"%u%s\n",
+	"0x%x%s\n",
+	"0x%02x%s\n",
+	"0x%04x%s\n",
+	"0x%08x%s\n",
+	"%d%s\n",
+};
+
+static char *display_strs[] = {
+	" (off)",
+	" (on)",
+};
+
+static char *show_on_off(uint on)
+{
+	if (on <= 1)
+		return display_strs[on];
+	return NULL;
+}  /* show_on_off */
+
+static ssize_t sysfs_show(ssize_t len, char *buf, int type, int chk, char *ptr,
+	int verbose)
+{
+	if (type) {
+		if (verbose) {
+			if (SHOW_HELP_ON_OFF == type)
+				ptr = show_on_off(chk);
+		}
+		len += sprintf(buf + len, help_formats[type], chk, ptr);
+	}
+	return len;
+}  /* sysfs_show */
+#endif
+
diff -Napur linux-linux4microchip-2022.10-orig/drivers/net/ethernet/micrel/ksz_common.h linux-linux4microchip-2022.10/drivers/net/ethernet/micrel/ksz_common.h
--- linux-linux4microchip-2022.10-orig/drivers/net/ethernet/micrel/ksz_common.h	1969-12-31 16:00:00.000000000 -0800
+++ linux-linux4microchip-2022.10/drivers/net/ethernet/micrel/ksz_common.h	2023-04-25 16:13:54.872163472 -0700
@@ -0,0 +1,117 @@
+/**
+ * Microchip Ethernet driver common header
+ *
+ * Copyright (c) 2015-2020 Microchip Technology Inc.
+ *	Tristram Ha <Tristram.Ha@microchip.com>
+ *
+ * Copyright (c) 2009-2011 Micrel, Inc.
+ *
+ * This file contains shared structure definitions to be used between network
+ * and switch drivers.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ */
+
+
+#ifndef KSZ_COMMON_H
+#define KSZ_COMMON_H
+
+
+#define NL  "\n"
+#define PER_CHAR  "%%"
+
+
+/* Used to indicate type of flow control support. */
+enum {
+	PHY_NO_FLOW_CTRL,
+	PHY_FLOW_CTRL,
+	PHY_TX_ONLY,
+	PHY_RX_ONLY
+};
+
+/* Used to indicate link connection state. */
+enum {
+	media_connected,
+	media_disconnected,
+	media_unknown
+};
+
+/* -------------------------------------------------------------------------- */
+
+/**
+ * struct ksz_timer_info - Timer information data structure
+ * @timer:	Kernel timer.
+ * @cnt:	Running timer counter.
+ * @max:	Number of times to run timer; -1 for infinity.
+ * @period:	Timer period in jiffies.
+ */
+struct ksz_timer_info {
+	struct timer_list timer;
+	int cnt;
+	int max;
+	int period;
+	void *dev;
+};
+
+/**
+ * struct ksz_counter_info - OS dependent counter information data structure
+ * @counter:	Wait queue to wakeup after counters are read.
+ * @time:	Next time in jiffies to read counter.
+ * @read:	Indication of counters read in full or not.
+ */
+struct ksz_counter_info {
+	wait_queue_head_t counter;
+	unsigned long time;
+	int read;
+};
+
+#define DEV_NAME_SIZE			32
+
+/**
+ * struct ksz_dev_attr - Sysfs data structure
+ * @dev_attr:	Device attribute.
+ * @dev_name:	Attribute name.
+ */
+struct ksz_dev_attr {
+	struct device_attribute dev_attr;
+	char dev_name[DEV_NAME_SIZE];
+};
+
+struct ksz_dev_major {
+	void *dev;
+	int major;
+};
+
+struct file_dev_info {
+	void *dev;
+	uint minor;
+	u8 *read_buf;
+	u8 *read_in;
+	u8 *write_buf;
+	size_t read_max;
+	size_t read_tmp;
+	size_t read_len;
+	size_t write_len;
+	struct semaphore sem;
+	struct mutex lock;
+	wait_queue_head_t wait_msg;
+	uint notifications[8];
+	struct file_dev_info *next;
+
+	void (*dev_free)(struct file_dev_info *info);
+	int (*dev_ioctl)(struct file_dev_info *info, void *arg);
+	ssize_t (*dev_read)(struct file_dev_info *info, u8 *buf, size_t count,
+			    size_t *offp);
+	ssize_t (*dev_write)(struct file_dev_info *info, u8 *buf, size_t count,
+			     size_t *offp);
+};
+
+#endif
+
diff -Napur linux-linux4microchip-2022.10-orig/drivers/net/ethernet/micrel/ksz_dlr_api.h linux-linux4microchip-2022.10/drivers/net/ethernet/micrel/ksz_dlr_api.h
--- linux-linux4microchip-2022.10-orig/drivers/net/ethernet/micrel/ksz_dlr_api.h	1969-12-31 16:00:00.000000000 -0800
+++ linux-linux4microchip-2022.10/drivers/net/ethernet/micrel/ksz_dlr_api.h	2023-04-25 16:13:54.872163472 -0700
@@ -0,0 +1,202 @@
+/**
+ * Microchip DLR driver API header
+ *
+ * Copyright (c) 2015-2016 Microchip Technology Inc.
+ *	Tristram Ha <Tristram.Ha@microchip.com>
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ */
+
+
+#ifndef KSZ_DLR_API_H
+#define KSZ_DLR_API_H
+
+#ifndef ETH_ALEN
+#define ETH_ALEN				6
+#endif
+
+
+enum {
+	DEV_INFO_DLR_LINK = DEV_INFO_LAST,
+	DEV_INFO_DLR_CFG,
+};
+
+#define DLR_INFO_LINK_LOST			(1 << 0)
+#define DLR_INFO_CFG_CHANGE			(1 << 1)
+
+
+#define STATUS_INVALID_ATTRIB_VALUE		0x09
+#define STATUS_OBJECT_STATE_CONFLICT		0x0C
+#define STATUS_REPLY_DATA_TOO_LARGE		0x11
+
+#define SVC_GET_ATTRIBUTES_ALL			0x01
+#define SVC_GET_ATTRIBUTE_SINGLE		0x0E
+#define SVC_SET_ATTRIBUTE_SINGLE		0x10
+#define SVC_GET_MEMBER				0x18
+
+
+#define CIP_CLASS_ATTRIBUTES			0
+#define CIP_INSTANCE_ATTRIBUTES			1
+
+
+#define CLASS_DLR_OBJECT			0x47
+
+
+#define CIP_SVC_S				24
+#define CIP_CLASS_S				16
+#define CIP_ATTR_S				8
+
+
+#define DLR_GET_REVISION			1
+
+#define DLR_REVISION				3
+
+
+#define DLR_GET_NETWORK_TOPOLOGY		1
+#define DLR_GET_NETWORK_STATUS			2
+#define DLR_GET_RING_SUPERVISOR_STATUS		3
+#define DLR_SET_RING_SUPERVISOR_CONFIG		4
+#define DLR_SET_RING_FAULT_COUNT		5
+#define DLR_GET_LAST_ACTIVE_NODE_ON_PORT_1	6
+#define DLR_GET_LAST_ACTIVE_NODE_ON_PORT_2	7
+#define DLR_GET_RING_PARTICIPANTS_COUNT		8
+#define DLR_GET_RING_PARTICIPANTS_LIST		9
+#define DLR_GET_ACTIVE_SUPERVISOR_ADDRESS	10
+#define DLR_GET_ACTIVE_SUPERVISOR_PRECEDENCE	11
+#define DLR_GET_CAPABILITY_FLAGS		12
+#define DLR_SET_REDUNDANT_GATEWAY_CONFIG	13
+#define DLR_GET_REDUNDANT_GATEWAY_STATUS	14
+#define DLR_GET_ACTIVE_GATEWAY_ADDRESS		15
+#define DLR_GET_ACTIVE_GATEWAY_PRECEDENCE	16
+#define DLR_SET_IP_ADDRESS			17
+
+#define SVC_DLR_VERIFY_FAULT_LOCATION		0x4B
+#define SVC_DLR_CLEAR_RAPID_FAULTS		0x4C
+#define SVC_DLR_RESTART_SIGN_ON			0x4D
+#define SVC_DLR_CLEAR_GATEWAY_PARTIAL_FAULT	0x4E
+
+
+#define DLR_TOPOLOGY_LINEAR			0
+#define DLR_TOPOLOGY_RING			1
+
+#define DLR_NET_NORMAL				0
+#define DLR_NET_RING_FAULT			1
+#define DLR_NET_UNEXPECTED_LOOP_DETECTED	2
+#define DLR_NET_PARTIAL_FAULT			3
+#define DLR_NET_RAPID_FAULT			4
+
+#define DLR_STAT_BACKUP_SUPERVISOR		0
+#define DLR_STAT_ACTIVE_SUPERVISOR		1
+#define DLR_STAT_RING_NODE			2
+#define DLR_STAT_NO_SUPERVISOR			3
+#define DLR_STAT_NODE_NOT_SUPPORTED		4
+
+#define DLR_CAP_ANNOUNCE_BASED			(1 << 0)
+#define DLR_CAP_BEACON_BASED			(1 << 1)
+#define DLR_CAP_SUPERVISOR_CAPABLE		(1 << 5)
+#define DLR_CAP_GATEWAY_CAPABLE			(1 << 6)
+#define DLR_CAP_FLUSH_TABLE_CAPABLE		(1 << 7)
+
+#define DLR_STAT_NON_GATEWAY			0
+#define DLR_STAT_BACKUP_GATEWAY			1
+#define DLR_STAT_ACTIVE_GATEWAY			2
+#define DLR_STAT_GATEWAY_FAULT_TO_UPLINK	3
+#define DLR_STAT_GATEWAY_NOT_SUPPORTED		4
+#define DLR_STAT_GATEWAY_FAULT_TO_NETWORK	5
+
+
+struct ksz_dlr_active_node {
+	u32 ip_addr;
+	u8 addr[ETH_ALEN];
+} __packed;
+
+struct ksz_dlr_super_cfg {
+	u8 enable;
+	u8 prec;
+	u32 beacon_interval;
+	u32 beacon_timeout;
+	u16 vid;
+}  __packed;
+
+struct ksz_dlr_gateway_cfg {
+	u8 enable;
+	u8 prec;
+	u32 advertise_interval;
+	u32 advertise_timeout;
+	u8 learning_enable;
+}  __packed;
+
+struct ksz_dlr_non_super_capable_1 {
+	u8 net_topology;
+	u8 net_status;
+} __packed;
+
+struct ksz_dlr_super_capable_1 {
+	u8 net_topology;
+	u8 net_status;
+	u8 super_status;
+	struct ksz_dlr_super_cfg super_cfg;
+	u16 fault_cnt;
+	struct ksz_dlr_active_node last_active[2];
+	u16 participants_cnt;
+	struct ksz_dlr_active_node active_super_addr;
+	u8 active_super_prec;
+} __packed;
+
+struct ksz_dlr_non_super_capable_2 {
+	u8 net_topology;
+	u8 net_status;
+	struct ksz_dlr_active_node active_super_addr;
+	u32 cap;
+} __packed;
+
+struct ksz_dlr_super_capable_2 {
+	u8 net_topology;
+	u8 net_status;
+	u8 super_status;
+	struct ksz_dlr_super_cfg super_cfg;
+	u16 fault_cnt;
+	struct ksz_dlr_active_node last_active[2];
+	u16 participants_cnt;
+	struct ksz_dlr_active_node active_super_addr;
+	u8 active_super_prec;
+	u32 cap;
+} __packed;
+
+struct ksz_dlr_gateway_capable {
+	u8 net_topology;
+	u8 net_status;
+	u8 super_status;
+	struct ksz_dlr_super_cfg super_cfg;
+	u16 fault_cnt;
+	struct ksz_dlr_active_node last_active[2];
+	u16 participants_cnt;
+	struct ksz_dlr_active_node active_super_addr;
+	u8 active_super_prec;
+	u32 cap;
+	struct ksz_dlr_gateway_cfg gateway_cfg;
+	u8 gateway_status;
+	struct ksz_dlr_active_node active_gateway_addr;
+	u8 active_gateway_prec;
+} __packed;
+
+union dlr_data {
+	struct ksz_dlr_gateway_capable gateway;
+	struct ksz_dlr_super_capable_2 super;
+	struct ksz_dlr_non_super_capable_2 non_super;
+	struct ksz_dlr_super_cfg super_cfg;
+	struct ksz_dlr_gateway_cfg gateway_cfg;
+	struct ksz_dlr_active_node active;
+	u32 dword;
+	u16 word;
+	u8 byte;
+} __packed;
+
+#endif
diff -Napur linux-linux4microchip-2022.10-orig/drivers/net/ethernet/micrel/ksz_dlr.c linux-linux4microchip-2022.10/drivers/net/ethernet/micrel/ksz_dlr.c
--- linux-linux4microchip-2022.10-orig/drivers/net/ethernet/micrel/ksz_dlr.c	1969-12-31 16:00:00.000000000 -0800
+++ linux-linux4microchip-2022.10/drivers/net/ethernet/micrel/ksz_dlr.c	2023-10-11 12:21:52.000000000 -0700
@@ -0,0 +1,5765 @@
+/**
+ * Microchip DLR code
+ *
+ * Copyright (c) 2015-2019 Microchip Technology Inc.
+ *	Tristram Ha <Tristram.Ha@microchip.com>
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ */
+
+
+#if 0
+#define DBG_DLR_STATE
+#endif
+
+#if 0
+#define DBG_DLR_BEACON
+#endif
+#if 0
+#define DBG_DLR_OPER
+#endif
+#if 0
+#define DBG_DLR_HW_OPER
+#endif
+#if 0
+#define DBG_DLR_SUPERVISOR
+#endif
+#if 0
+#define DBG_DLR_ANN_SIGNON
+#endif
+#if 0
+#define DBG_DLR_SIGNON
+#endif
+
+
+static u8 MAC_ADDR_BEACON[] = { 0x01, 0x21, 0x6C, 0x00, 0x00, 0x01 };
+static u8 MAC_ADDR_SIGNON[] = { 0x01, 0x21, 0x6C, 0x00, 0x00, 0x02 };
+static u8 MAC_ADDR_ANNOUNCE[] = { 0x01, 0x21, 0x6C, 0x00, 0x00, 0x03 };
+static u8 MAC_ADDR_ADVERTISE[] = { 0x01, 0x21, 0x6C, 0x00, 0x00, 0x04 };
+static u8 MAC_ADDR_LEARNING_UPDATE[] = { 0x01, 0x21, 0x6C, 0x00, 0x00, 0x05 };
+
+
+#define BEACON_TICK			5
+#define BEACON_INTERVAL			(BEACON_TICK * 10)
+
+enum {
+	DLR_ANNOUNCE_NODE,
+	DLR_BEACON_NODE,
+	DLR_SUPERVISOR,
+	DLR_ACTIVE_SUPERVISOR,
+};
+
+enum {
+	DLR_BEGIN,
+
+	DLR_IDLE_STATE,
+	DLR_FAULT_STATE,
+	DLR_NORMAL_STATE,
+	DLR_ACTIVE_STATE,
+	DLR_ACTIVE_FAULT_STATE,
+	DLR_ACTIVE_NORMAL_STATE,
+	DLR_BACKUP_STATE,
+	DLR_PREPARE_STATE,
+	DLR_RESTART_STATE,
+};
+
+static int dbg_leak = 5;
+
+#ifdef DBG_DLR_BEACON
+static int dbg_bcn;
+#endif
+
+#ifdef DBG_DLR_ANN_SIGNON
+static int dbg_ann;
+#endif
+
+#ifdef CONFIG_HAVE_ACL_HW
+
+/* Lower action index has higher precedence. */
+#define DLR_BEACON_DROP_ACL_ACTION	12
+#define DLR_NO_ACL_ACTION		13
+#define DLR_DROP_ACL_ACTION		15
+
+#define DLR_BEACON_DROP_ACL_RULE	12
+#define DLR_BEACON_ACL_RULE		13
+#define DLR_DROP_SELF_ACL_RULE		15
+#define DLR_BEACON_TIMEOUT_ACL_RULE	14
+
+#define DLR_BEACON_DROP_ACL_ENTRY	12
+#define DLR_BEACON_ACL_ENTRY		13
+#define DLR_DROP_SELF_ACL_ENTRY		15
+#define DLR_TIMEOUT_ACL_ENTRY		14
+
+static void setup_acl_beacon(struct ksz_dlr_info *info, int port)
+{
+	struct ksz_sw *sw = info->sw_dev;
+	struct ksz_port_cfg *cfg = &sw->info->port_cfg[port];
+	struct ksz_acl_table *acl;
+	int i = DLR_BEACON_DROP_ACL_RULE;
+	int first_rule = i;
+	int ruleset = (3 << i);
+
+	acl = &cfg->acl_info[i];
+	mutex_lock(&sw->acllock);
+	if (!memcmp(acl->mac, info->attrib.active_super_addr.addr, ETH_ALEN) &&
+	    acl->first_rule == first_rule)
+		goto done;
+
+	acl->first_rule = first_rule;
+	acl->ruleset = ruleset;
+	sw_w_acl_ruleset(sw, port, i, acl);
+
+	acl->mode = ACL_MODE_LAYER_2;
+	acl->enable = ACL_ENABLE_2_BOTH;
+	acl->equal = 1;
+	acl->src = 1;
+	memcpy(acl->mac, info->attrib.active_super_addr.addr, ETH_ALEN);
+	memcpy(info->beacon_addr, acl->mac, ETH_ALEN);
+	acl->eth_type = DLR_TAG_TYPE;
+	sw_w_acl_rule(sw, port, i, acl);
+
+done:
+	mutex_unlock(&sw->acllock);
+}  /* setup_acl_beacon */
+
+static void setup_acl_beacon_timeout(struct ksz_dlr_info *info, int port)
+{
+	struct ksz_sw *sw = info->sw_dev;
+	struct ksz_port_cfg *cfg = &sw->info->port_cfg[port];
+	struct ksz_acl_table *acl;
+	int i;
+	u8 *addr = MAC_ADDR_BEACON;
+
+	i = DLR_TIMEOUT_ACL_ENTRY;
+	mutex_lock(&sw->acllock);
+	acl = &cfg->acl_info[i];
+	acl->first_rule = DLR_NO_ACL_ACTION;
+	acl->ruleset = (1 << i);
+	acl->mode = ACL_MODE_LAYER_2;
+	acl->enable = ACL_ENABLE_2_COUNT;
+	acl->equal = 1;
+	acl->src = 0;
+	memcpy(acl->mac, addr, ETH_ALEN);
+	acl->eth_type = DLR_TAG_TYPE;
+	if (info->beacon_timeout > ACL_CNT_M) {
+		int cnt = info->beacon_timeout + 500;
+
+		cnt /= 1000;
+		acl->cnt = cnt;
+		acl->msec = 1;
+	} else {
+		acl->cnt = info->beacon_timeout;
+		acl->msec = 0;
+	}
+	acl->intr_mode = 0;
+	sw_w_acl_table(sw, port, i, acl);
+	mutex_unlock(&sw->acllock);
+}  /* setup_acl_beacon_timeout */
+
+static void disable_acl_beacon_timeout(struct ksz_dlr_info *info, int port)
+{
+	struct ksz_sw *sw = info->sw_dev;
+	struct ksz_port_cfg *cfg = &sw->info->port_cfg[port];
+	struct ksz_acl_table *acl;
+	int i = DLR_BEACON_TIMEOUT_ACL_RULE;
+
+	mutex_lock(&sw->acllock);
+	acl = &cfg->acl_info[i];
+	acl->ruleset = 0;
+	sw_w_acl_table(sw, port, i, acl);
+	mutex_unlock(&sw->acllock);
+}  /* disable_acl_beacon_timeout */
+
+static void setup_acl_beacon_drop(struct ksz_dlr_info *info, int drop)
+{
+	struct ksz_sw *sw = info->sw_dev;
+	struct ksz_port_cfg *cfg;
+	struct ksz_acl_table *acl;
+	int i = DLR_BEACON_DROP_ACL_ACTION;
+	int p;
+
+	for (p = 0; p < 2; p++)
+		setup_acl_beacon(info, info->ports[p]);
+	mutex_lock(&sw->acllock);
+	for (p = 0; p < 2; p++) {
+		cfg = &sw->info->port_cfg[info->ports[p]];
+
+		acl = &cfg->acl_info[i];
+		acl->map_mode = ACL_MAP_MODE_REPLACE;
+		acl->ports = drop & info->member;
+		if (!drop)
+			acl->map_mode = ACL_MAP_MODE_DISABLE;
+		sw_w_acl_action(sw, info->ports[p], i, acl);
+	}
+	mutex_unlock(&sw->acllock);
+
+	if (drop)
+		return;
+
+	/* Ring state may be changed in the beacon. */
+	memset(&info->beacon_info[0].last, 0, sizeof(struct ksz_dlr_beacon));
+	memset(&info->beacon_info[1].last, 0, sizeof(struct ksz_dlr_beacon));
+	info->beacon_info[0].timeout =
+	info->beacon_info[1].timeout = 0;
+}  /* setup_acl_beacon_drop */
+
+static void setup_acl_self(struct ksz_dlr_info *info, int port)
+{
+	struct ksz_sw *sw = info->sw_dev;
+	struct ksz_port_cfg *cfg = &sw->info->port_cfg[port];
+	struct ksz_acl_table *acl;
+	int i;
+
+	/* Drop packets sent by self. */
+	i = DLR_DROP_SELF_ACL_ENTRY;
+	mutex_lock(&sw->acllock);
+	acl = &cfg->acl_info[i];
+	acl->data[0] = 0xff;
+	acl->mode = ACL_MODE_LAYER_2;
+	acl->enable = ACL_ENABLE_2_MAC;
+	acl->equal = 1;
+	acl->src = 1;
+	memcpy(acl->mac, info->src_addr, ETH_ALEN);
+	acl->eth_type = 0;
+	acl->first_rule = DLR_DROP_ACL_ACTION;
+	acl->ruleset = (1 << i);
+	acl->map_mode = ACL_MAP_MODE_REPLACE;
+	acl->ports = sw->HOST_MASK;
+	acl->ports = 0;
+	sw_w_acl_table(sw, port, i, acl);
+	mutex_unlock(&sw->acllock);
+}  /* setup_acl_self */
+
+static void dlr_setup_acl(struct ksz_dlr_info *info, int port)
+{
+	struct ksz_sw *sw = info->sw_dev;
+	struct ksz_port_cfg *cfg = &sw->info->port_cfg[port];
+	struct ksz_acl_table *acl;
+	int i;
+
+	mutex_lock(&sw->acllock);
+
+	/* Use MAC table for beacon forwarding. */
+	i = DLR_BEACON_ACL_ENTRY;
+	acl = &cfg->acl_info[i];
+	acl->mode = ACL_MODE_LAYER_2;
+	acl->enable = ACL_ENABLE_2_BOTH;
+	acl->equal = 1;
+	acl->src = 0;
+	memcpy(acl->mac, MAC_ADDR_BEACON, ETH_ALEN);
+	acl->eth_type = DLR_TAG_TYPE;
+	sw_w_acl_rule(sw, port, i, acl);
+
+	i = DLR_BEACON_ACL_RULE;
+	acl = &cfg->acl_info[i];
+	acl->first_rule = DLR_NO_ACL_ACTION;
+	acl->ruleset = (1 << i);
+	sw_w_acl_ruleset(sw, port, i, acl);
+
+	i = DLR_NO_ACL_ACTION;
+	acl = &cfg->acl_info[i];
+	acl->map_mode = ACL_MAP_MODE_DISABLE;
+	acl->ports = 0;
+	sw_w_acl_action(sw, port, i, acl);
+	mutex_unlock(&sw->acllock);
+}  /* dlr_setup_acl */
+#endif
+
+static void setup_vlan_table(struct ksz_dlr_info *info, u16 vid, int set)
+{
+	struct ksz_sw *sw = info->sw_dev;
+	u32 ports = 0;
+
+#ifdef DBG_DLR_OPER
+	dbg_msg(" %s %d %d\n", __func__, vid, set);
+#endif
+
+	/* Do not do anything for VID 0, which is priority tagged frame. */
+	if (1 >= vid)
+		return;
+	if (set)
+		ports = sw->HOST_MASK | info->member;
+	sw->ops->cfg_vlan(sw, 0, vid, 0, ports);
+}  /* setup_vlan_table */
+
+#ifdef CONFIG_HAVE_DLR_HW
+static void dlr_hw_set_state(struct ksz_sw *sw, u8 node, u8 ring)
+{
+	u8 data;
+
+	if (!(sw->features & REDUNDANCY_SUPPORT))
+		return;
+	data = SW_R(sw, REG_DLR_STATE__1);
+	data &= ~DLR_RING_STATE_NORMAL;
+	data &= ~(DLR_NODE_STATE_M << DLR_NODE_STATE_S);
+	node &= DLR_NODE_STATE_M;
+	node <<= DLR_NODE_STATE_S;
+	ring &= DLR_RING_STATE_NORMAL;
+	data |= node | ring;
+	SW_W(sw, REG_DLR_STATE__1, data);
+}
+
+static void dlr_hw_set_supervisor(struct ksz_sw *sw, u8 super)
+{
+	u8 data;
+	u8 saved;
+
+	if (!(sw->features & REDUNDANCY_SUPPORT))
+		return;
+	data = SW_R(sw, REG_DLR_CTRL__1);
+	saved = data;
+	data &= ~DLR_BEACON_TX_ENABLE;
+	data &= ~DLR_BACKUP_AUTO_ON;
+	if (DLR_ACTIVE_SUPERVISOR == super)
+		data |= DLR_BEACON_TX_ENABLE;
+#if 0
+/*
+ * THa  2016/12/30
+ * Occassionally the ACL beacon timeout no longer is triggered after the
+ * active supervisor is turned off.  The backup supervisor feature in the
+ * switch starts sending beacons, but software has no way to become the
+ * active supervisor.  Turning on/off supervisor does not recover from this
+ * problem.
+ */
+	if (DLR_SUPERVISOR == super) {
+
+		/* Turn off previous automatic start first. */
+		if (saved & DLR_BACKUP_AUTO_ON) {
+			SW_W(sw, REG_DLR_CTRL__1, data);
+#ifdef DBG_DLR_HW_OPER
+			dbg_msg("  reset backup: %x\n", saved);
+#endif
+		}
+		data |= DLR_BACKUP_AUTO_ON;
+	}
+#endif
+	data |= DLR_ASSIST_ENABLE;
+	SW_W(sw, REG_DLR_CTRL__1, data);
+#ifdef DBG_DLR_HW_OPER
+	dbg_msg("  %s %x\n", __func__, data);
+#endif
+}
+
+static void dlr_hw_set_dest_addr(struct ksz_sw *sw, u8 *addr)
+{
+	if (!(sw->features & REDUNDANCY_SUPPORT))
+		return;
+	sw->reg->w(sw, REG_DLR_DEST_ADDR_0, addr, ETH_ALEN);
+}
+
+static void dlr_hw_set_ip_addr(struct ksz_sw *sw, u32 ip_addr)
+{
+	if (!(sw->features & REDUNDANCY_SUPPORT))
+		return;
+	sw->reg->w32(sw, REG_DLR_IP_ADDR__4, ip_addr);
+}
+
+static void dlr_hw_reset_seq_id(struct ksz_sw *sw)
+{
+	u8 data;
+
+	if (!(sw->features & REDUNDANCY_SUPPORT))
+		return;
+	data = SW_R(sw, REG_DLR_CTRL__1);
+	SW_W(sw, REG_DLR_CTRL__1, data | DLR_RESET_SEQ_ID);
+	SW_W(sw, REG_DLR_CTRL__1, data & ~DLR_RESET_SEQ_ID);
+}
+
+static void dlr_hw_set_port_map(struct ksz_sw *sw, u32 ports)
+{
+	if (!(sw->features & REDUNDANCY_SUPPORT))
+		return;
+	sw->reg->w32(sw, REG_DLR_PORT_MAP__4, ports);
+}
+
+static void dlr_hw_set_precedence(struct ksz_sw *sw, u8 precedence)
+{
+	if (!(sw->features & REDUNDANCY_SUPPORT))
+		return;
+	sw->reg->w8(sw, REG_DLR_PRECEDENCE__1, precedence);
+}
+
+static void dlr_hw_set_interval(struct ksz_sw *sw, u16 interval)
+{
+	if (!(sw->features & REDUNDANCY_SUPPORT))
+		return;
+	sw->reg->w32(sw, REG_DLR_BEACON_INTERVAL__4, interval);
+}
+
+static void dlr_hw_set_timeout(struct ksz_sw *sw, u32 timeout)
+{
+	if (!(sw->features & REDUNDANCY_SUPPORT))
+		return;
+	sw->reg->w32(sw, REG_DLR_BEACON_TIMEOUT__4, timeout);
+}
+
+static void dlr_hw_set_vlan_id(struct ksz_sw *sw, u16 vid)
+{
+	if (!(sw->features & REDUNDANCY_SUPPORT))
+		return;
+	sw->reg->w16(sw, REG_DLR_VLAN_ID__2, vid);
+}
+#endif
+
+
+#define DLR_LEARNING_ENTRY		3
+#define DLR_BEACON_ENTRY		4
+#define DLR_ANNOUNCE_ENTRY		5
+#define DLR_SIGNON_ENTRY		6
+#define DLR_SUPERVISOR_ENTRY		7
+#define DLR_SUPERVISOR_DEV_0_ENTRY	8
+
+static void sw_setup_dlr(struct ksz_sw *sw)
+{
+	struct ksz_dlr_info *dlr = &sw->info->dlr;
+
+	/* Enable 802.1p priority to give highest priority to beacon frame. */
+	sw_ena_802_1p(sw, dlr->ports[0]);
+	sw_ena_802_1p(sw, dlr->ports[1]);
+	sw_ena_802_1p(sw, sw->HOST_PORT);
+
+#ifdef CONFIG_HAVE_ACL_HW
+	/* Need to receive beacon frame with changed VID. */
+	sw->ops->fwd_unk_vid(sw);
+#endif
+
+#ifdef CONFIG_HAVE_DLR_HW
+	dlr_hw_set_dest_addr(sw, MAC_ADDR_BEACON);
+	dlr_hw_set_port_map(sw, dlr->member);
+#endif
+	sw->ops->release(sw);
+
+	/* SignOn is not forwarded automatically. */
+	sw->ops->cfg_mac(sw, DLR_SIGNON_ENTRY, MAC_ADDR_SIGNON, sw->HOST_MASK,
+		false, false, 0);
+
+	/* Not 3-port switch and other ports are used. */
+	if (sw->overrides & HAVE_MORE_THAN_2_PORTS) {
+		u32 member = dlr->member | sw->HOST_MASK;
+
+		sw->ops->cfg_mac(sw, DLR_BEACON_ENTRY, MAC_ADDR_BEACON,
+			member, false, false, 0);
+		sw->ops->cfg_mac(sw, DLR_ANNOUNCE_ENTRY, MAC_ADDR_ANNOUNCE,
+			member, false, false, 0);
+		sw->ops->cfg_mac(sw, DLR_LEARNING_ENTRY,
+			MAC_ADDR_LEARNING_UPDATE, member, false, false, 0);
+		if (1 == sw->eth_cnt) {
+			sw->ops->acquire(sw);
+			sw_cfg_port_base_vlan(sw, dlr->ports[0], member);
+			sw_cfg_port_base_vlan(sw, dlr->ports[1], member);
+			sw_cfg_port_base_vlan(sw, sw->HOST_PORT, member);
+			sw->ops->release(sw);
+		}
+	}
+	dlr_setup_acl(dlr, dlr->ports[0]);
+	dlr_setup_acl(dlr, dlr->ports[1]);
+
+	/* Do not need to process beacons. */
+	if (DLR_ANNOUNCE_NODE == sw->info->dlr.node)
+		sw->ops->cfg_mac(sw, DLR_BEACON_ENTRY, MAC_ADDR_BEACON,
+			dlr->member, false, false, 0);
+	sw->ops->acquire(sw);
+}  /* sw_setup_dlr */
+
+enum {
+	DEV_DLR_CHK_HW,
+	DEV_DLR_CLR_SUPER,
+	DEV_DLR_SETUP_DIR,
+	DEV_DLR_SETUP_DROP,
+	DEV_DLR_SETUP_TIMEOUT,
+	DEV_DLR_SETUP_VID,
+	DEV_DLR_FLUSH,
+	DEV_DLR_BLOCK,
+	DEV_DLR_STOP,
+	DEV_DLR_UPDATE,
+	DEV_DLR_RX,
+	DEV_DLR_START_ANNOUNCE,
+	DEV_DLR_TX_ANNOUNCE,
+	DEV_DLR_TX_LOCATE,
+	DEV_DLR_TX_SIGNON,
+	DEV_DLR_TX_REQ,
+	DEV_DLR_TX_RESP,
+	DEV_DLR_TX_STATUS,
+	DEV_DLR_TX_ADVERTISE,
+	DEV_DLR_TX_FLUSH_TABLES,
+	DEV_DLR_TX_LEARNING_UPDATE,
+};
+
+static void wait_for_timeout(u32 microsec)
+{
+	if (microsec >= 20000) {
+		microsec /= 1000;
+		delay_milli(microsec);
+	} else
+		delay_micro(microsec);
+}  /* wait_for_timeout */
+
+static void dlr_set_addr(struct ksz_dlr_active_node *node, u32 ip_addr,
+	u8 *addr)
+{
+	node->ip_addr = ip_addr;
+	memcpy(node->addr, addr, ETH_ALEN);
+}  /* dlr_set_addr */
+
+static int dlr_dev_xmit(struct ksz_dlr_info *info, u8 *data, int len,
+	int ports, u16 vid)
+{
+	int rc;
+	struct sk_buff *skb;
+	const struct net_device_ops *ops = info->dev->netdev_ops;
+	struct ksz_sw *sw = info->sw_dev;
+
+	/* Add extra for tail tagging. */
+	skb = dev_alloc_skb(len + VLAN_HLEN + 4 + 8);
+	if (!skb)
+		return -ENOMEM;
+
+	/* Regular transmit from DLR node. */
+	if (vid == info->vid)
+		memcpy(skb->data, data, len);
+
+	/* Need to replace source MAC address for self-address filtering. */
+	if (ports & info->member)
+		memcpy(&skb->data[6], info->src_addr, ETH_ALEN);
+
+	skb_put(skb, len);
+	sw->net_ops->add_tail_tag(sw, skb, ports);
+	skb->protocol = htons(DLR_TAG_TYPE);
+	skb->dev = info->dev;
+	do {
+		struct ksz_sw *sw = info->sw_dev;
+
+		rc = ops->ndo_start_xmit(skb, skb->dev);
+		if (NETDEV_TX_BUSY == rc) {
+			rc = wait_event_interruptible_timeout(sw->queue,
+				!netif_queue_stopped(info->dev),
+				50 * HZ / 1000);
+
+			rc = NETDEV_TX_BUSY;
+		}
+	} while (NETDEV_TX_BUSY == rc);
+	return rc;
+}  /* dlr_dev_xmit */
+
+static int dlr_xmit(struct ksz_dlr_info *info, u16 ports)
+{
+	u8 *frame = info->tx_frame;
+	int len = info->len;
+
+	/* Do not send if network device is not ready. */
+	if (!netif_running(info->dev) || !netif_carrier_ok(info->dev))
+		return 0;
+
+	/* Do not send to port if its link is lost. */
+	if ((ports & (1 << info->ports[0])) && info->p1_down)
+		ports &= ~(1 << info->ports[0]);
+	if ((ports & (1 << info->ports[1])) && info->p2_down)
+		ports &= ~(1 << info->ports[1]);
+
+	if (len < 60) {
+		memset(&frame[len], 0, 60 - len);
+		len = 60;
+	}
+
+	return dlr_dev_xmit(info, info->tx_frame, len, ports, info->vid);
+}  /* dlr_xmit */
+
+static int prep_dlr_beacon(struct ksz_dlr_info *dlr)
+{
+	struct ksz_dlr_frame *frame = &dlr->frame.body;
+
+	memcpy(dlr->frame.vlan.h_dest, MAC_ADDR_BEACON, ETH_ALEN);
+	frame->hdr.frame_type = DLR_BEACON;
+	frame->hdr.src_port = DLR_PORT_NONE;
+	frame->hdr.seqid = htonl(dlr->seqid_beacon++);
+
+	memset(frame->data.reserved, 0, 30);
+	frame->data.beacon.ring_state = dlr->ring_state;
+	frame->data.beacon.precedence = dlr->precedence;
+	frame->data.beacon.interval = htonl(dlr->beacon_interval);
+	frame->data.beacon.timeout = htonl(dlr->beacon_timeout);
+	return sizeof(struct ksz_dlr_beacon);
+}  /* prep_dlr_beacon */
+
+static int prep_dlr_announce(struct ksz_dlr_info *dlr)
+{
+	struct ksz_dlr_frame *frame = &dlr->frame.body;
+
+	memcpy(dlr->frame.vlan.h_dest, MAC_ADDR_ANNOUNCE, ETH_ALEN);
+	frame->hdr.frame_type = DLR_ANNOUNCE;
+	frame->hdr.src_port = DLR_PORT_NONE;
+	frame->hdr.seqid = htonl(dlr->seqid++);
+
+	memset(frame->data.reserved, 0, 30);
+	frame->data.announce.ring_state = dlr->ring_state;
+	return sizeof(struct ksz_dlr_announce);
+}  /* prep_dlr_announce */
+
+static int prep_dlr_link_status(struct ksz_dlr_info *dlr, int neigh)
+{
+	struct ksz_dlr_frame *frame = &dlr->frame.body;
+
+	memcpy(dlr->frame.vlan.h_dest, dlr->attrib.active_super_addr.addr,
+		ETH_ALEN);
+	frame->hdr.frame_type = DLR_LINK_STATUS;
+	frame->hdr.src_port = DLR_PORT_NONE;
+	frame->hdr.seqid = htonl(dlr->seqid++);
+
+	memset(frame->data.reserved, 0, 30);
+	frame->data.status.port1_active = 1;
+	frame->data.status.port2_active = 1;
+
+	if (neigh) {
+		frame->data.status.neighbor = 1;
+		if (dlr->p1_lost)
+			frame->data.status.port1_active = 0;
+		if (dlr->p2_lost)
+			frame->data.status.port2_active = 0;
+	} else {
+		frame->data.status.neighbor = 0;
+		if (dlr->p1_down)
+			frame->data.status.port1_active = 0;
+		if (dlr->p2_down)
+			frame->data.status.port2_active = 0;
+	}
+	return sizeof(struct ksz_dlr_status);
+}  /* prep_dlr_link_status */
+
+static int prep_dlr_locate_fault(struct ksz_dlr_info *dlr)
+{
+	struct ksz_dlr_frame *frame = &dlr->frame.body;
+
+	memcpy(dlr->frame.vlan.h_dest, MAC_ADDR_ANNOUNCE, ETH_ALEN);
+	frame->hdr.frame_type = DLR_LOCATE_FAULT;
+	frame->hdr.src_port = DLR_PORT_NONE;
+	frame->hdr.seqid = htonl(dlr->seqid++);
+
+	memset(frame->data.reserved, 0, 30);
+	return 0;
+}  /* prep_dlr_locate_fault */
+
+static int prep_dlr_neigh_chk_req(struct ksz_dlr_info *dlr, int port)
+{
+	struct ksz_dlr_frame *frame = &dlr->frame.body;
+
+	memcpy(dlr->frame.vlan.h_dest, MAC_ADDR_SIGNON, ETH_ALEN);
+	frame->hdr.frame_type = DLR_NEIGH_CHK_REQ;
+	frame->hdr.src_port = port ? DLR_PORT_2 : DLR_PORT_1;
+	dlr->seqid_chk[port] = dlr->seqid++;
+	if (1 == dlr->port_chk[port])
+		dlr->seqid_first[port] = dlr->seqid_chk[port];
+	frame->hdr.seqid = htonl(dlr->seqid_chk[port]);
+
+	memset(frame->data.reserved, 0, 30);
+	return 0;
+}  /* prep_dlr_neigh_chk_req */
+
+static int prep_dlr_neigh_chk_resp(struct ksz_dlr_info *dlr, u32 seqid, u8 in,
+	int out)
+{
+	struct ksz_dlr_frame *frame = &dlr->frame.body;
+
+	memcpy(dlr->frame.vlan.h_dest, MAC_ADDR_SIGNON, ETH_ALEN);
+	frame->hdr.frame_type = DLR_NEIGH_CHK_RESP;
+	frame->hdr.src_port = out ? DLR_PORT_2 : DLR_PORT_1;
+	frame->hdr.seqid = htonl(seqid);
+
+	memset(frame->data.reserved, 0, 30);
+	frame->data.neigh_chk_resp.src_port = in;
+	return sizeof(struct ksz_dlr_neigh_chk_resp);
+}  /* prep_dlr_neigh_chk_resp */
+
+static int prep_dlr_signon(struct ksz_dlr_info *dlr, int len)
+{
+	int i;
+	struct ksz_dlr_tx_frame *base = (struct ksz_dlr_tx_frame *)
+		dlr->signon_frame;
+	struct ksz_dlr_frame *frame = &base->body;
+	u16 num = ntohs(frame->data.signon.num);
+	struct ksz_dlr_node *node = frame->data.signon.node;
+	int space = 1000;
+
+	/* The very first signon frame. */
+	if (!len) {
+		memcpy(base->vlan.h_dest, dlr->signon_addr, ETH_ALEN);
+		frame->hdr.frame_type = DLR_SIGN_ON;
+		frame->hdr.src_port = dlr->tx_port;
+		dlr->seqid_signon = dlr->seqid;
+		frame->hdr.seqid = htonl(dlr->seqid++);
+		num = 0;
+		len = sizeof(struct vlan_ethhdr) +
+			sizeof(struct ksz_dlr_hdr) +
+			sizeof(struct ksz_dlr_signon) -
+			sizeof(struct ksz_dlr_node);
+		if ((dlr->overrides & DLR_TEST) && dlr->signon_space)
+			space = dlr->signon_space + 1;
+	}
+	memcpy(base->vlan.h_source, dlr->src_addr, ETH_ALEN);
+	if (len + sizeof(struct ksz_dlr_node) > 1500) {
+		memcpy(base->vlan.h_dest, dlr->attrib.active_super_addr.addr,
+			ETH_ALEN);
+		frame->hdr.src_port = dlr->rx_port;
+		dlr->signon_port = dlr->rx_port;
+		return len;
+	}
+	for (i = 0; i < num; i++)
+		node++;
+	do {
+		num++;
+		memcpy(node->addr, dlr->src_addr, ETH_ALEN);
+		node->ip_addr = dlr->ip_addr;
+		len += sizeof(struct ksz_dlr_node);
+		node++;
+	} while (len + sizeof(struct ksz_dlr_node) * space <= 1500);
+	frame->data.signon.num = htons(num);
+
+	dlr->signon_port = dlr->tx_port;
+	return len;
+}  /* prep_dlr_signon */
+
+static int prep_dlr_advertise(struct ksz_dlr_info *dlr)
+{
+	struct ksz_dlr_frame *frame = &dlr->frame.body;
+
+	memcpy(dlr->frame.vlan.h_dest, MAC_ADDR_ADVERTISE, ETH_ALEN);
+	frame->hdr.frame_type = DLR_ADVERTISE;
+	frame->hdr.src_port = DLR_PORT_NONE;
+	frame->hdr.seqid = htonl(dlr->seqid++);
+
+	memset(frame->data.reserved, 0, 30);
+	frame->data.advertise.state = DLR_GW_ACTIVE_LISTEN_STATE;
+	frame->data.advertise.precedence = dlr->precedence;
+	frame->data.advertise.interval = htonl(dlr->beacon_interval);
+	frame->data.advertise.timeout = htonl(dlr->beacon_timeout);
+	frame->data.advertise.learning_update_enable = 1;
+	return sizeof(struct ksz_dlr_advertise);
+}  /* prep_dlr_advertise */
+
+static int prep_dlr_flush_tables(struct ksz_dlr_info *dlr)
+{
+	struct ksz_dlr_frame *frame = &dlr->frame.body;
+
+	memcpy(dlr->frame.vlan.h_dest, MAC_ADDR_ANNOUNCE, ETH_ALEN);
+	frame->hdr.frame_type = DLR_FLUSH_TABLES;
+	frame->hdr.src_port = DLR_PORT_NONE;
+	frame->hdr.seqid = htonl(dlr->seqid++);
+
+	memset(frame->data.reserved, 0, 30);
+	frame->data.flush.learning_update_enable = 1;
+	return sizeof(struct ksz_dlr_flush_tables);
+}  /* prep_dlr_flush_tables */
+
+static int prep_dlr_learning_update(struct ksz_dlr_info *dlr)
+{
+	dlr->update_frame.hdr.seqid = htonl(dlr->seqid++);
+
+	return sizeof(struct ksz_dlr_update_frame);
+}  /* prep_dlr_learning_update */
+
+static void dlr_tx_beacon(struct ksz_dlr_info *dlr)
+{
+	int rc;
+
+	dlr->len = prep_dlr_beacon(dlr);
+	dlr->len += sizeof(struct vlan_ethhdr) + sizeof(struct ksz_dlr_hdr);
+	rc = dlr_xmit(dlr, dlr->member);
+}  /* dlr_tx_beacon */
+
+static void dlr_tx_announce(struct ksz_dlr_info *dlr)
+{
+	int rc;
+	u16 ports = dlr->member;
+
+#ifdef DBG_DLR_ANN_SIGNON
+	if (dbg_ann > 0) {
+		dbg_msg(" tx ann: S=%d R=%d %x %lx\n",
+			dlr->state, dlr->ring_state,
+			dlr->seqid, jiffies);
+		dbg_ann--;
+	}
+#endif
+
+	if (RING_NORMAL_STATE == dlr->ring_state) {
+		if (dlr->ann_first)
+			dlr->ann_jiffies = jiffies;
+		else if (dlr->ann_jiffies == jiffies) {
+			dlr->ann_jiffies = 0;
+			return;
+		}
+	}
+
+	if (RING_NORMAL_STATE == dlr->ring_state)
+		ports = 1 << dlr->ports[dlr->port];
+
+	dlr->len = prep_dlr_announce(dlr);
+	dlr->len += sizeof(struct vlan_ethhdr) + sizeof(struct ksz_dlr_hdr);
+	rc = dlr_xmit(dlr, ports);
+
+	/* First delayed announce from initial fault state sent. */
+	if (dlr->ann_first)
+		dlr->ann_first = 0;
+}  /* dlr_tx_announce */
+
+static void dlr_tx_chk_req(struct ksz_dlr_info *dlr, int port)
+{
+	int rc;
+
+	dlr->len = prep_dlr_neigh_chk_req(dlr, port);
+	dlr->len += sizeof(struct vlan_ethhdr) + sizeof(struct ksz_dlr_hdr);
+	rc = dlr_xmit(dlr, (1 << dlr->ports[port]));
+}  /* dlr_tx_chk_req */
+
+static void dlr_tx_chk_resp(struct ksz_dlr_info *dlr, int port)
+{
+	int rc;
+
+	dlr->len = prep_dlr_neigh_chk_resp(dlr, dlr->seqid_rcv[port],
+		dlr->port_rcv[port], port);
+	dlr->len += sizeof(struct vlan_ethhdr) + sizeof(struct ksz_dlr_hdr);
+	rc = dlr_xmit(dlr, (1 << dlr->ports[port]));
+}  /* dlr_tx_chk_resp */
+
+static void dlr_tx_status(struct ksz_dlr_info *dlr, int neigh)
+{
+	int rc;
+
+	dlr->len = prep_dlr_link_status(dlr, neigh);
+	dlr->len += sizeof(struct vlan_ethhdr) + sizeof(struct ksz_dlr_hdr);
+	rc = dlr_xmit(dlr, 0);
+}  /* dlr_tx_status */
+
+static void dlr_tx_signon(struct ksz_dlr_info *dlr, int len)
+{
+	int rc;
+
+	dlr->len = prep_dlr_signon(dlr, len);
+	dlr->tx_frame = dlr->signon_frame;
+	rc = dlr_xmit(dlr, (1 << dlr->ports[dlr->signon_port]));
+	dlr->tx_frame = (u8 *) &dlr->frame;
+}  /* dlr_tx_signon */
+
+static void dlr_tx_locate_fault(struct ksz_dlr_info *dlr)
+{
+	int rc;
+
+	dlr->len = prep_dlr_locate_fault(dlr);
+	dlr->len += sizeof(struct vlan_ethhdr) + sizeof(struct ksz_dlr_hdr);
+	rc = dlr_xmit(dlr, dlr->member);
+}  /* dlr_tx_locate_fault */
+
+static void dlr_tx_learning_update(struct ksz_dlr_info *dlr)
+{
+	int rc;
+	u16 ports = dlr->member;
+
+	dlr->len = prep_dlr_learning_update(dlr);
+	dlr->tx_frame = (u8 *) &dlr->update_frame;
+
+	/* Attempt to notify the other supervisor about incoming beacons. */
+	if (dlr->rogue_super) {
+		memcpy(dlr->update_frame.eth.h_dest,
+			&dlr->rogue_super->prec_addr[1], ETH_ALEN);
+		ports = (1 << dlr->ports[dlr->rogue_super->port]);
+		ports |= TAIL_TAG_SET_OVERRIDE;
+	}
+	rc = dlr_xmit(dlr, ports);
+
+	/* Reset default Learning_Update address. */
+	if (dlr->rogue_super) {
+		memcpy(dlr->update_frame.eth.h_dest, MAC_ADDR_LEARNING_UPDATE,
+			ETH_ALEN);
+		dlr->rogue_super = NULL;
+	}
+	dlr->tx_frame = (u8 *) &dlr->frame;
+}  /* dlr_tx_learning_update */
+
+static void dlr_tx_advertise(struct ksz_dlr_info *dlr)
+{
+	int rc;
+
+	dlr->len = prep_dlr_advertise(dlr);
+	dlr->len += sizeof(struct vlan_ethhdr) + sizeof(struct ksz_dlr_hdr);
+	rc = dlr_xmit(dlr, dlr->member);
+}  /* dlr_tx_advertise */
+
+static void dlr_tx_flush_tables(struct ksz_dlr_info *dlr)
+{
+	int rc;
+
+	dlr->len = prep_dlr_flush_tables(dlr);
+	dlr->len += sizeof(struct vlan_ethhdr) + sizeof(struct ksz_dlr_hdr);
+	rc = dlr_xmit(dlr, dlr->member);
+}  /* dlr_tx_flush_tables */
+
+static void flushMacTable(struct ksz_dlr_info *info)
+{
+	struct ksz_sw *sw = info->sw_dev;
+
+	sw->ops->acquire(sw);
+	sw->ops->flush_table(sw, sw->port_cnt);
+	sw->ops->release(sw);
+}
+
+static void enableOnePort(struct ksz_dlr_info *info)
+{
+	struct ksz_sw *sw = info->sw_dev;
+	int block_port = (info->port + 1) & 1;
+
+	sw->ops->acquire(sw);
+	port_set_stp_state(sw, info->ports[block_port], STP_STATE_BLOCKED);
+	sw->ops->release(sw);
+}
+
+static void enableBothPorts(struct ksz_dlr_info *info)
+{
+	struct ksz_sw *sw = info->sw_dev;
+
+	sw->ops->acquire(sw);
+	port_set_stp_state(sw, info->ports[0], STP_STATE_FORWARDING);
+	port_set_stp_state(sw, info->ports[1], STP_STATE_FORWARDING);
+	sw->ops->release(sw);
+}
+
+static void disableLearn(struct ksz_dlr_info *info, int disable)
+{
+	struct ksz_sw *sw = info->sw_dev;
+
+	if (disable != info->disable_learn) {
+		sw->ops->acquire(sw);
+		port_cfg_dis_learn(sw, info->ports[0], disable);
+		port_cfg_dis_learn(sw, info->ports[1], disable);
+		sw->ops->release(sw);
+		info->disable_learn = disable;
+	}
+}
+
+static void dlr_set_state(struct ksz_dlr_info *info)
+{
+#ifdef CONFIG_HAVE_DLR_HW
+	u8 node;
+	u8 ring;
+	struct ksz_sw *sw = info->sw_dev;
+
+	if (DLR_IDLE_STATE == info->state)
+		node = DLR_NODE_STATE_IDLE;
+	else if (DLR_NORMAL_STATE == info->state ||
+		 DLR_ACTIVE_NORMAL_STATE == info->state)
+		node = DLR_NODE_STATE_NORMAL;
+	else
+		node = DLR_NODE_STATE_FAULT;
+	ring = RING_FAULT_STATE == info->ring_state ?
+		DLR_RING_STATE_FAULT : DLR_RING_STATE_NORMAL;
+
+	/* Backup supervisor needs to send ring fault with the first beacon. */
+	if (DLR_ACTIVE_SUPERVISOR != info->node ||
+	    DLR_RESTART_STATE == info->state)
+		ring = DLR_RING_STATE_FAULT;
+	sw->ops->acquire(sw);
+	dlr_hw_set_state(sw, node, ring);
+	sw->ops->release(sw);
+
+	/* Make sure beacon is sent with new ring state. */
+	if (DLR_ACTIVE_SUPERVISOR == info->node)
+		wait_for_timeout(info->beacon_interval * 2);
+#endif
+}  /* dlr_set_state */
+
+static void dlr_chk_supervisor(struct ksz_dlr_info *info)
+{
+#ifdef CONFIG_HAVE_DLR_HW
+	u32 data;
+	struct ksz_sw *sw = info->sw_dev;
+
+	if (!(sw->features & REDUNDANCY_SUPPORT))
+		return;
+	sw->ops->acquire(sw);
+	data = SW_R(sw, REG_DLR_CTRL__1);
+	if (data & DLR_BEACON_TX_ENABLE) {
+#ifdef DBG_DLR_HW_OPER
+		dbg_msg("  %s %x\n", __func__, data);
+#endif
+		if (info->node != DLR_ACTIVE_SUPERVISOR) {
+			data &= ~DLR_BEACON_TX_ENABLE;
+			SW_W(sw, REG_DLR_CTRL__1, data);
+#ifdef DBG_DLR_HW_OPER
+			dbg_msg(" tx off\n");
+#endif
+		}
+	} else if (DLR_ACTIVE_SUPERVISOR == info->node) {
+		data |= DLR_BEACON_TX_ENABLE;
+		SW_W(sw, REG_DLR_CTRL__1, data);
+#ifdef DBG_DLR_HW_OPER
+		dbg_msg(" tx on\n");
+#endif
+	}
+	sw->ops->release(sw);
+#endif
+	info->chk_hw = 0;
+}  /* dlr_chk_supervisor */
+
+static void dlr_set_supervisor(struct ksz_dlr_info *info)
+{
+#ifdef CONFIG_HAVE_DLR_HW
+	u8 node;
+	struct ksz_sw *sw = info->sw_dev;
+
+	if (info->node < DLR_SUPERVISOR)
+		node = DLR_BEACON_NODE;
+	else if (DLR_ACTIVE_SUPERVISOR == info->node)
+		node = DLR_ACTIVE_SUPERVISOR;
+	else
+		node = DLR_SUPERVISOR;
+
+	/* In case the backup supervisor starts sending beacons. */
+	if (DLR_SUPERVISOR == node && 7 == info->beacon_timeout_ports)
+		node = DLR_BEACON_NODE;
+	sw->ops->acquire(sw);
+	dlr_hw_set_supervisor(sw, node);
+	sw->ops->release(sw);
+#ifdef DBG_DLR_HW_OPER
+	dbg_msg("  %s %d %d\n", __func__, node, info->node);
+#endif
+#endif
+}  /* dlr_set_supervisor */
+
+static void setupBeacons(struct ksz_dlr_info *info)
+{
+	int use_hw = false;
+
+#ifdef CONFIG_HAVE_DLR_HW
+	struct ksz_sw *sw = info->sw_dev;
+
+	if (sw->features & REDUNDANCY_SUPPORT) {
+		use_hw = true;
+	}
+#endif
+	if (use_hw) {
+		dlr_set_state(info);
+	} else {
+		info->interval = 0;
+		dlr_tx_beacon(info);
+	}
+}  /* setupBeacons */
+
+static void disableSupervisor(struct ksz_dlr_info *info)
+{
+	struct ksz_sw *sw = info->sw_dev;
+	u32 member = 0;
+
+#ifdef DBG_DLR_OPER
+	dbg_msg("%s\n", __func__);
+#endif
+	if (sw->overrides & HAVE_MORE_THAN_2_PORTS)
+		member = info->member | sw->HOST_MASK;
+	dlr_set_supervisor(info);
+	info->start = 0;
+
+	/* Allow beacons to be forwarded. */
+	sw->ops->cfg_mac(sw, DLR_BEACON_ENTRY, MAC_ADDR_BEACON, member, false,
+		false, 0);
+	sw->ops->cfg_mac(sw, DLR_ANNOUNCE_ENTRY, MAC_ADDR_ANNOUNCE, member,
+		false, false, 0);
+	sw->ops->cfg_mac(sw, DLR_SIGNON_ENTRY, MAC_ADDR_SIGNON, sw->HOST_MASK,
+		false, false, 0);
+	sw->ops->cfg_mac(sw, DLR_LEARNING_ENTRY, MAC_ADDR_LEARNING_UPDATE,
+		member, false, false, 0);
+#ifdef CONFIG_HAVE_DLR_HW
+	sw->ops->acquire(sw);
+	dlr_hw_reset_seq_id(sw);
+	sw->ops->release(sw);
+#endif
+}  /* disableSupervisor */
+
+static void enableSupervisor(struct ksz_dlr_info *info)
+{
+	struct ksz_sw *sw = info->sw_dev;
+
+	/* Do not need to process Announce. */
+	sw->ops->cfg_mac(sw, DLR_ANNOUNCE_ENTRY, MAC_ADDR_ANNOUNCE, 0, true,
+		false, 0);
+
+	/* Force to receive messages as port will be closed. */
+	sw->ops->cfg_mac(sw, DLR_SIGNON_ENTRY, MAC_ADDR_SIGNON, sw->HOST_MASK,
+		true, false, 0);
+#if 0
+/* For case where tag is removed and new tag is added. */
+	sw->ops->cfg_mac(sw, DLR_SIGNON_ENTRY, MAC_ADDR_SIGNON, sw->HOST_MASK,
+		true, true, 2);
+#endif
+	sw->ops->cfg_mac(sw, DLR_LEARNING_ENTRY, MAC_ADDR_LEARNING_UPDATE, 0,
+		true, false, 0);
+	sw->ops->acquire(sw);
+#ifdef CONFIG_HAVE_DLR_HW
+	dlr_hw_set_precedence(sw, info->precedence);
+	dlr_hw_set_interval(sw, info->beacon_interval);
+	dlr_hw_set_timeout(sw, info->beacon_timeout);
+	dlr_hw_set_vlan_id(sw, info->vid);
+	dlr_hw_set_ip_addr(sw, ntohl(info->ip_addr));
+#endif
+	sw->ops->release(sw);
+
+	/* Beacons will only be forwarded to self. */
+	sw->ops->cfg_mac(sw, DLR_BEACON_ENTRY, MAC_ADDR_BEACON, sw->HOST_MASK,
+		false, false, 0);
+
+	info->seqid_last[0] = info->seqid_last[1] = 0xdeadbeef;
+	info->start = 1;
+	dlr_set_supervisor(info);
+}  /* enableSupervisor */
+
+static void disableAnnounce(struct ksz_dlr_info *info)
+{
+#ifdef DBG_DLR_OPER
+	dbg_msg("%s\n", __func__);
+#endif
+	cancel_delayed_work_sync(&info->announce_tx);
+}
+
+static void startAnnounce(struct ksz_dlr_info *info)
+{
+#ifdef DBG_DLR_OPER
+	dbg_msg("%s\n", __func__);
+#endif
+	cancel_delayed_work_sync(&info->announce_tx);
+	schedule_delayed_work(&info->announce_tx, msecs_to_jiffies(1000));
+	if (!info->ann_first)
+		dlr_tx_announce(info);
+	else
+		info->tx_announce = true;
+}  /* startAnnounce */
+
+static void enableAnnounce(struct ksz_dlr_info *info, int delay)
+{
+#ifdef DBG_DLR_OPER
+	dbg_msg("%s %d %d %lx\n", __func__, delay, info->ann_delay,
+		jiffies);
+#endif
+	if (1 == delay) {
+
+		/* Wait for 2 * beacon timeout before sending announce. */
+		if (!info->ann_delay) {
+			info->ann_jiffies = jiffies;
+			info->ann_delay = 1;
+			info->ann_first = 1;
+			cancel_delayed_work_sync(&info->announce_tx);
+			schedule_delayed_work(&info->announce_tx, 0);
+		}
+
+	/* Wait until first announce is sent. */
+	} else if (!info->ann_delay)
+		startAnnounce(info);
+}
+
+static void disableAnnounceTimeout(struct ksz_dlr_info *info)
+{
+#ifdef DBG_DLR_OPER
+	dbg_msg("%s\n", __func__);
+#endif
+	ksz_stop_timer(&info->announce_timeout_timer_info);
+}
+
+static void disableNeighChkTimers(struct ksz_dlr_info *info)
+{
+	info->p1_lost = info->p2_lost = 0;
+	info->port_chk[0] = info->port_chk[1] = 0;
+	ksz_stop_timer(&info->neigh_chk_timer_info);
+	info->neigh_chk = 0;
+}
+
+static void disableSignOnTimer(struct ksz_dlr_info *info)
+{
+	ksz_stop_timer(&info->signon_timer_info);
+	info->signon_start = 0;
+}  /* disableSignOnTimer */
+
+static int updateValues(struct ksz_dlr_info *info)
+{
+	struct ksz_dlr_gateway_capable *attrib = &info->attrib;
+	struct ksz_sw *sw = info->sw_dev;
+	int can_change = false;
+	int vid_change = false;
+	int prepare = false;
+	int cmp = memcmp(info->src_addr, attrib->active_super_addr.addr,
+		ETH_ALEN);
+
+#ifdef DBG_DLR_OPER
+	dbg_msg("%s\n", __func__);
+#endif
+	sw->ops->acquire(sw);
+	if (info->precedence != attrib->super_cfg.prec) {
+		info->precedence = attrib->super_cfg.prec;
+#ifdef CONFIG_HAVE_DLR_HW
+		dlr_hw_set_precedence(sw, info->precedence);
+#endif
+	}
+	sw->ops->release(sw);
+
+	/*
+	 * Changing precedence or MAC address can cause backup supervisor to
+	 * become active.
+	 */
+	if (info->precedence > attrib->active_super_prec ||
+	    (info->precedence == attrib->active_super_prec &&
+	    cmp > 0)) {
+		can_change = true;
+		prepare = true;
+	} else if (!cmp)
+		can_change = true;
+	if (can_change && info->vid != attrib->super_cfg.vid) {
+		vid_change = true;
+		setup_vlan_table(info, info->vid, false);
+	}
+	sw->ops->acquire(sw);
+	if (can_change &&
+	    info->beacon_interval != attrib->super_cfg.beacon_interval) {
+#ifdef DBG_DLR_OPER
+		dbg_msg("interval %u %u\n",
+			info->beacon_interval,
+			attrib->super_cfg.beacon_interval);
+#endif
+		info->beacon_interval = attrib->super_cfg.beacon_interval;
+#ifdef CONFIG_HAVE_DLR_HW
+		dlr_hw_set_interval(sw, info->beacon_interval);
+#endif
+	}
+	if (can_change &&
+	    info->beacon_timeout != attrib->super_cfg.beacon_timeout) {
+#ifdef DBG_DLR_OPER
+		dbg_msg("timeout %u %u\n",
+			info->beacon_timeout,
+			attrib->super_cfg.beacon_timeout);
+#endif
+		info->beacon_timeout = attrib->super_cfg.beacon_timeout;
+#ifdef CONFIG_HAVE_DLR_HW
+		dlr_hw_set_timeout(sw, info->beacon_timeout);
+#endif
+	}
+	if (can_change && info->vid != attrib->super_cfg.vid) {
+		info->vid = attrib->super_cfg.vid;
+#ifdef CONFIG_HAVE_DLR_HW
+		dlr_hw_set_vlan_id(sw, info->vid);
+#endif
+	}
+	sw->ops->release(sw);
+	if (vid_change)
+		setup_vlan_table(info, info->vid, true);
+	info->new_val = 0;
+	info->frame.vlan.h_vlan_TCI = htons((7 << VLAN_PRIO_SHIFT) | info->vid);
+	memcpy(info->signon_frame, &info->frame, sizeof(struct vlan_ethhdr));
+	return prepare;
+}  /* updateValues */
+
+static void dlr_flush(struct ksz_dlr_info *info)
+{
+	flushMacTable(info);
+}
+
+static void setupDir(struct ksz_dlr_info *info, int port)
+{
+	struct ksz_sw *sw = info->sw_dev;
+	struct ksz_dlr_active_node *active;
+	int cmp;
+
+	if (port >= 0) {
+		active = &info->attrib.active_super_addr;
+		memcpy(info->last_sup.addr, active->addr, ETH_ALEN);
+		port = 1 << info->ports[port];
+	} else {
+		active = &info->last_sup;
+		port = 0;
+	}
+
+	/* Do not change entry of own address. */
+	cmp = memcmp(active->addr, info->src_addr, ETH_ALEN);
+	if (!cmp)
+		return;
+	info->active_port = port;
+	sw->ops->cfg_mac(sw, DLR_SUPERVISOR_ENTRY,
+		active->addr, port, false, false, 0);
+#if 0
+	if (sw->eth_cnt > 1)
+		sw->ops->cfg_mac(sw, DLR_SUPERVISOR_DEV_0_ENTRY, active->addr,
+			port, false, true, sw->eth_maps[0].vlan);
+#endif
+#ifdef DBG_DLR_HW_OPER
+	dbg_msg("%s x%x %02x:%02x:%02x\n", __func__, port,
+		active->addr[3],
+		active->addr[4],
+		active->addr[5]);
+#endif
+}
+
+
+#define announcedState		(info->ring_state)
+#define fromRingState		(RING_FAULT_STATE == info->ring_state ?	\
+	DLR_FAULT_STATE : DLR_NORMAL_STATE)
+#define announceRcvd		(info->ann_rcvd)
+#define announceTimeout		(info->ann_timeout)
+#define oneBeaconRcvd		(info->one_rcvd)
+#define twoBeaconsRcvd		(info->both_rcvd)
+#define oneBeaconTimeout	(info->one_timeout)
+#define twoBeaconsTimeout	(info->both_timeout)
+#define newSupervisor		(info->new_supervisor)
+#define newValue		(info->new_val)
+#define backupSupervisor	(DLR_SUPERVISOR == info->node)
+#define faultState		(RING_FAULT_STATE == info->ring_state)
+#define linkDown		(info->both_down)
+#define linkLoss		(info->one_down)
+#define linkStatus		(info->p1_lost || info->p2_lost)
+
+
+static void acceptBeacons(struct ksz_dlr_info *dlr)
+{
+#ifdef DBG_DLR_BEACON
+	dbg_msg("%s\n", __func__);
+#endif
+#ifdef DBG_DLR_BEACON
+	dbg_bcn += 4;
+#endif
+	dlr->skip_beacon = false;
+
+	/* Indicate in process of accepting beacons. */
+	dlr->drop_beacon++;
+#ifdef CONFIG_HAVE_ACL_HW
+	setup_acl_beacon_drop(dlr, 0);
+#endif
+	dlr->drop_beacon = false;
+	memset(&dlr->last_beacon[0], 0, sizeof(struct ksz_dlr_tx_frame));
+	memset(&dlr->last_beacon[1], 0, sizeof(struct ksz_dlr_tx_frame));
+}  /* acceptBeacons */
+
+static void dropBeacons(struct ksz_dlr_info *info)
+{
+	int drop = info->member;
+
+#ifdef DBG_DLR_BEACON
+	dbg_msg("%s\n", __func__);
+#endif
+	if (info->node == DLR_ACTIVE_SUPERVISOR)
+		drop = 0x8000;
+#ifdef CONFIG_HAVE_ACL_HW
+	setup_acl_beacon_drop(info, drop);
+#endif
+	info->skip_beacon = !!drop;
+}  /* dropBeacons */
+
+static int seq_ahead(u32 start, u32 num)
+{
+	u32 ahead;
+	u32 behind;
+
+	ahead = num - start;
+	behind = start - num;
+	return ahead < behind;
+}
+
+static int seq_in(u32 start, u32 end, u32 num)
+{
+	end -= start;
+	num -= start;
+	return 0 <= num && num <= end;
+}
+
+static struct ksz_dlr_node_info *find_dlr_node(struct ksz_dlr_info *info,
+	u8 *addr)
+{
+	int i;
+
+	for (i = 0; i < info->attrib.participants_cnt; i++)
+		if (!memcmp(info->nodes[i].signon.addr, addr, ETH_ALEN))
+			return &info->nodes[i];
+	return NULL;
+}  /* find_dlr_node */
+
+static void dbg_dlr(struct ksz_dlr_info *info, char *msg)
+{
+	dbg_msg(" %s: S=%d R=%d L=%d ", msg, info->state, info->ring_state,
+		info->LastBcnRcvPort);
+	dbg_msg("d=%d:%d D=%d:%d l=%d:%d r=%d:%d R=%d:%d t=%d:%d T=%d:%d\n",
+		info->p1_down, info->p2_down,
+		info->one_down, info->both_down,
+		info->p1_lost, info->p2_lost,
+		info->p1_rcvd, info->p2_rcvd,
+		info->one_rcvd, info->both_rcvd,
+		info->p1_timeout, info->p2_timeout,
+		info->one_timeout, info->both_timeout);
+}
+
+#ifdef DBG_DLR_STATE
+static void dlr_print(struct ksz_dlr_info *info, char *msg)
+{
+	if (info->overrides & DLR_TEST)
+		printk(KERN_INFO "%s\n", msg);
+}
+#endif
+
+static int dlr_chk_beacon_timeout(struct ksz_dlr_info *info, int p,
+	struct ksz_dlr_frame *beacon,
+	struct ksz_dlr_super_info *active,
+	struct ksz_dlr_super_info *super)
+{
+	u32 crc;
+	int i;
+	struct ksz_dlr_super_info *next;
+	struct ksz_dlr_super_info *first = NULL;
+	struct ksz_dlr_super_info *found = NULL;
+	int accept = false;
+
+	if (DLR_ACTIVE_SUPERVISOR != info->node)
+		return accept;
+	crc = ether_crc(ETH_ALEN + 1, super->prec_addr);
+	for (i = 0; i < DLR_SUPERVISOR_NUM; i++) {
+		next = &info->supers[i];
+		if (!next->crc && !first)
+			first = next;
+		if (next->crc == crc) {
+			found = next;
+			break;
+		}
+	}
+	if (!found)
+		found = first;
+	if (found) {
+		u32 interval = ntohl(beacon->data.beacon.interval);
+
+		found->cnt++;
+		found->port = p;
+
+		/* First time. */
+		if (!found->crc) {
+#ifdef DBG_DLR_SUPERVISOR
+			dbg_msg("  A %x=%02x:%02x:%02x:%02x:%02x:%02x  "
+				"S %x=%02x:%02x:%02x:%02x:%02x:%02x  ",
+				active->prec_addr[0],
+				active->prec_addr[1],
+				active->prec_addr[2],
+				active->prec_addr[3],
+				active->prec_addr[4],
+				active->prec_addr[5],
+				active->prec_addr[6],
+				super->prec_addr[0],
+				super->prec_addr[1],
+				super->prec_addr[2],
+				super->prec_addr[3],
+				super->prec_addr[4],
+				super->prec_addr[5],
+				super->prec_addr[6]);
+			dbg_msg("cnt: %d %x\n", found->cnt, crc);
+#endif
+			found->crc = crc;
+			memcpy(found->prec_addr, super->prec_addr,
+				ETH_ALEN + 1);
+		}
+		found->timeout[p] += interval;
+		if (found->timeout[p] > (10000 + 4 * interval) &&
+		    !found->sent) {
+dbg_msg("rogue: %u\n", found->timeout[p]);
+			found->sent = 1;
+			accept = true;
+		}
+	}
+	return accept;
+}  /* dlr_chk_beacon_timeout */
+
+static void dbg_supervisor(struct ksz_dlr_info *info)
+{
+#ifdef DBG_DLR_SUPERVISOR
+	struct ksz_dlr_super_info *next;
+	int i;
+
+	for (i = 0; i < DLR_SUPERVISOR_NUM; i++) {
+		next = &info->supers[i];
+		if (next->crc) {
+			dbg_msg("  super %d %x=%02x:%02x:%02x\n",
+				next->cnt,
+				next->prec_addr[0],
+				next->prec_addr[4],
+				next->prec_addr[5],
+				next->prec_addr[6]);
+		}
+	}
+#endif
+}  /* dbg_supervisor */
+
+static void dlr_clr_supervisor(struct ksz_dlr_info *info)
+{
+	struct ksz_dlr_super_info *next;
+	int i;
+
+	for (i = 0; i < DLR_SUPERVISOR_NUM; i++) {
+		next = &info->supers[i];
+		if (next->crc) {
+			if (next->last_cnt == next->cnt) {
+#ifdef DBG_DLR_SUPERVISOR
+				dbg_msg("  clr %d %u:%u %x=%02x:%02x:%02x\n",
+					next->cnt,
+					next->timeout[0], next->timeout[1],
+					next->prec_addr[0],
+					next->prec_addr[4],
+					next->prec_addr[5],
+					next->prec_addr[6]);
+#endif
+				memset(next, 0,
+					sizeof(struct ksz_dlr_super_info));
+			} else
+				next->last_cnt = next->cnt;
+		}
+	}
+}  /* dlr_clr_supervisor */
+
+static void update_vlan(struct ksz_dlr_info *info, u16 vid)
+{
+	if (info->vid == vid)
+		return;
+	setup_vlan_table(info, info->vid, false);
+	info->vid = vid;
+	setup_vlan_table(info, info->vid, true);
+	info->frame.vlan.h_vlan_TCI = htons((7 << VLAN_PRIO_SHIFT) | info->vid);
+	memcpy(info->signon_frame, &info->frame, sizeof(struct vlan_ethhdr));
+}  /* update_vlan */
+
+static void setup_dir(struct ksz_dlr_info *info, int port)
+{
+	info->rx_port = port;
+	info->tx_port = (port + 1) & 1;
+	setupDir(info, port);
+}  /* setup_dir */
+
+static void dlr_notify_link_lost(struct ksz_dlr_info *dlr)
+{
+	static u8 lost_buf[sizeof(struct ksz_resp_msg) +
+		sizeof(struct ksz_dlr_active_node) * 2];
+
+	if ((dlr->notifications & DLR_INFO_LINK_LOST)) {
+		struct ksz_resp_msg *msg = (struct ksz_resp_msg *) lost_buf;
+		struct ksz_sw *sw = dlr->sw_dev;
+		struct file_dev_info *dev_info;
+		size_t n = sizeof(struct ksz_resp_msg) +
+			sizeof(struct ksz_dlr_active_node) * 2;
+
+		msg->module = DEV_MOD_DLR;
+		msg->cmd = DEV_INFO_DLR_LINK;
+		msg->resp.data[0] = 0;
+		if (dlr->p1_down)
+			msg->resp.data[0] |= 0x01;
+		if (dlr->p2_down)
+			msg->resp.data[0] |= 0x02;
+		if (dlr->p1_lost)
+			msg->resp.data[0] |= 0x04;
+		if (dlr->p2_lost)
+			msg->resp.data[0] |= 0x08;
+		memcpy(&msg->resp.data[1], dlr->attrib.last_active,
+			sizeof(struct ksz_dlr_active_node) * 2);
+		dev_info = sw->dev_list[0];
+		while (dev_info) {
+			if ((dev_info->notifications[DEV_MOD_DLR] &
+			    DLR_INFO_LINK_LOST))
+				file_dev_setup_msg(dev_info, msg, n, NULL,
+						   NULL);
+			dev_info = dev_info->next;
+		}
+	}
+}  /* dlr_notify_link_lost */
+
+static void dlr_notify_cfg_change(struct ksz_dlr_info *dlr, int notify)
+{
+	if ((dlr->notifications & DLR_INFO_CFG_CHANGE)) {
+		struct ksz_resp_msg msg;
+		struct ksz_sw *sw = dlr->sw_dev;
+		struct file_dev_info *dev_info;
+		size_t n = sizeof(msg);
+
+		msg.module = DEV_MOD_DLR;
+		msg.cmd = DEV_INFO_DLR_CFG;
+		msg.resp.data[0] = notify;
+		dev_info = sw->dev_list[0];
+		while (dev_info) {
+			if ((dev_info->notifications[DEV_MOD_DLR] &
+			    DLR_INFO_CFG_CHANGE))
+				file_dev_setup_msg(dev_info, &msg, n, NULL,
+						   NULL);
+			dev_info = dev_info->next;
+		}
+	}
+}  /* dlr_notify_cfg_change */
+
+static int checkBeacon(struct ksz_dlr_info *info,
+	struct ksz_dlr_rx_frame *frame, int port)
+{
+	struct ksz_dlr_super_info active;
+	struct ksz_dlr_super_info super;
+	u32 interval;
+	u32 timeout;
+	int cmp = 0;
+	int accept = false;
+	struct ksz_dlr_tx_frame beacon_frame;
+	struct vlan_ethhdr *vlan = frame->vlan;
+	struct ksz_dlr_frame *beacon = frame->body;
+	u32 seqid = ntohl(beacon->hdr.seqid);
+	struct ksz_dlr_gateway_capable *attrib = &info->attrib;
+	struct ksz_dlr_beacon_info *beacon_info = &info->beacon_info[port];
+
+	if (info->state && (info->overrides & DLR_TEST_SEQ) &&
+	    (info->seqid_last[port] + 1 != seqid) && !info->seqid_cnt) {
+		info->seqid_cnt = 1000;
+		dbg_msg("bcn seq %d: %d %d\n", port,
+			info->seqid_last[port], seqid);
+	}
+	if (info->seqid_cnt > 0)
+		info->seqid_cnt--;
+	info->seqid_last[port] = seqid;
+
+	/* Announce node does not accept beacons. */
+	if (DLR_ANNOUNCE_NODE == info->node)
+		return accept;
+
+#if 1
+/*
+ * THa  2015/11/03
+ * Hardware generates wrong ring state.
+ */
+	if (0 == beacon->data.beacon.ring_state) {
+		beacon->data.beacon.ring_state = 2;
+		if (!(info->overrides & DLR_BEACON_STATE_HACK)) {
+			info->overrides |= DLR_BEACON_STATE_HACK;
+			printk(KERN_INFO "beacon fault state value wrong\n");
+		}
+	}
+#endif
+
+	/* Ignore own beacon if stopped. */
+	/*
+	 * Only active supervisor can receive its own beacons because of
+	 * self-address filtering.
+	 */
+	if (!memcmp(info->src_addr, vlan->h_source, ETH_ALEN)) {
+		if (info->skip_beacon && info->drop_beacon && dbg_leak)
+			dbg_msg(" ^ ");
+		if (!info->start)
+			return accept;
+	}
+
+	/* Determine precedence level. */
+	super.prec_addr[0] = beacon->data.beacon.precedence;
+	memcpy(&super.prec_addr[1], vlan->h_source, ETH_ALEN);
+
+	/* Compare own address first if supervisor capable. */
+	if (info->node >= DLR_SUPERVISOR) {
+		active.prec_addr[0] = attrib->super_cfg.prec;
+		memcpy(&active.prec_addr[1], info->src_addr, ETH_ALEN);
+		cmp = memcmp(&super, &active, ETH_ALEN + 1);
+	}
+
+	if (cmp >= 0) {
+		active.prec_addr[0] = attrib->active_super_prec;
+		memcpy(&active.prec_addr[1], attrib->active_super_addr.addr,
+			ETH_ALEN);
+		cmp = memcmp(&super, &active, ETH_ALEN + 1);
+	}
+
+	/* Ignore lower precedence beacon. */
+	if (cmp < 0) {
+		/* Simulate beacon timeout as hardware cannot catch that. */
+		accept = dlr_chk_beacon_timeout(info, port, beacon, &active,
+			&super);
+
+		return accept;
+	} else if (cmp > 0) {
+#ifdef DBG_DLR_SUPERVISOR
+		dbg_msg("new %d p:%d %08x A=%02x:%02x:%02x:%02x:%02x:%02x  "
+			"S=%02x:%02x:%02x:%02x:%02x:%02x\n",
+			cmp,
+			beacon->data.beacon.precedence,
+			beacon->hdr.ip_addr,
+			attrib->active_super_addr.addr[0],
+			attrib->active_super_addr.addr[1],
+			attrib->active_super_addr.addr[2],
+			attrib->active_super_addr.addr[3],
+			attrib->active_super_addr.addr[4],
+			attrib->active_super_addr.addr[5],
+			vlan->h_source[0],
+			vlan->h_source[1],
+			vlan->h_source[2],
+			vlan->h_source[3],
+			vlan->h_source[4],
+			vlan->h_source[5]);
+		dbg_msg("  new prec: %d >= %d %d %d s:%d\n",
+			beacon->data.beacon.precedence,
+			attrib->active_super_prec, info->precedence,
+			attrib->super_cfg.prec,
+			beacon->data.beacon.ring_state);
+#endif
+		accept = true;
+		dlr_set_addr(&attrib->active_super_addr,
+			beacon->hdr.ip_addr, vlan->h_source);
+		attrib->active_super_prec = beacon->data.beacon.precedence;
+		info->new_supervisor = 1;
+		info->p1_set = info->p2_set = 1;
+		info->p1_rcvd = info->p2_rcvd =
+		info->one_rcvd = info->both_rcvd = 0;
+		info->p1_timeout = info->p2_timeout =
+		info->one_timeout = info->both_timeout = 0;
+
+		/* Set in following code. */
+		info->LastBcnRcvPort = 0;
+
+#ifdef DBG_DLR_STATE
+		if (DLR_ACTIVE_SUPERVISOR == info->node) {
+			dbg_dlr(info, "stop being active");
+#ifdef DBG_DLR_BEACON
+			dbg_bcn = 2;
+#endif
+		}
+#endif
+		memset(&info->last_beacon[0], 0,
+			sizeof(struct ksz_dlr_tx_frame));
+		memset(&info->last_beacon[1], 0,
+			sizeof(struct ksz_dlr_tx_frame));
+		memset(&info->beacon_info[0].last, 0,
+			sizeof(struct ksz_dlr_beacon));
+		memset(&info->beacon_info[1].last, 0,
+			sizeof(struct ksz_dlr_beacon));
+		info->beacon_info[0].timeout =
+		info->beacon_info[1].timeout = 0;
+	}
+
+	/* Process accepted beacon. */
+	info->LastBcnRcvPort |= port ? 2 : 1;
+
+	interval = ntohl(beacon->data.beacon.interval);
+	timeout = ntohl(beacon->data.beacon.timeout);
+
+#ifdef CONFIG_HAVE_DLR_HW
+	if (info->skip_beacon &&
+	    !memcmp(vlan->h_source, info->beacon_addr, ETH_ALEN)) {
+		if (dbg_leak > 0) {
+			dbg_msg(" ??: p=%d %d n=%d %x "
+				"%02x:%02x:%02x:%02x:%02x:%02x "
+				"%02x:%02x:%02x:%02x:%02x:%02x\n",
+				port, info->skip_beacon, info->node, seqid,
+				vlan->h_dest[0],
+				vlan->h_dest[1],
+				vlan->h_dest[2],
+				vlan->h_dest[3],
+				vlan->h_dest[4],
+				vlan->h_dest[5],
+				vlan->h_source[0],
+				vlan->h_source[1],
+				vlan->h_source[2],
+				vlan->h_source[3],
+				vlan->h_source[4],
+				vlan->h_source[5]);
+			--dbg_leak;
+		}
+	}
+#endif
+	if (!accept && info->skip_beacon)
+		return accept;
+	else if (beacon_info->timeout && !info->drop_beacon) {
+		beacon_info->timeout += interval;
+		if (beacon_info->timeout > timeout) {
+			info->drop_beacon = true;
+			accept = true;
+		}
+	}
+
+	/* Start the beacon drop process. */
+	if (!beacon_info->timeout &&
+	    !info->skip_beacon && !info->drop_beacon &&
+	    !newSupervisor &&
+	    RING_NORMAL_STATE == beacon->data.beacon.ring_state &&
+	    RING_NORMAL_STATE == info->ring_state &&
+	    (DLR_NORMAL_STATE == info->state ||
+	    DLR_ACTIVE_NORMAL_STATE == info->state)) {
+		beacon_info->timeout = 1;
+#ifdef DBG_DLR_BEACON
+		dbg_msg("  ready to drop: %d=r:%d\n",
+			port, beacon->data.beacon.ring_state);
+		dbg_bcn = 3;
+#endif
+	}
+
+	/* Zero out the sequence id for comparison. */
+	memcpy(&beacon_frame.vlan, frame->vlan, sizeof(struct vlan_ethhdr));
+	memcpy(&beacon_frame.body, frame->body, sizeof(struct ksz_dlr_frame));
+	beacon_frame.body.hdr.seqid = 0;
+	if (!accept)
+		accept = memcmp(&beacon_frame, &info->last_beacon[port],
+			sizeof(struct ksz_dlr_tx_frame));
+	if (accept)
+		memcpy(&info->last_beacon[port], &beacon_frame,
+			sizeof(struct ksz_dlr_tx_frame));
+
+	/* Try to process as few beacons as possible. */
+	if (memcmp(&beacon_info->last, &beacon->data.beacon,
+	    sizeof(struct ksz_dlr_beacon))) {
+		memcpy(&beacon_info->last, &beacon->data.beacon,
+			sizeof(struct ksz_dlr_beacon));
+		info->seqid_accept[port] = seqid;
+	}
+
+#ifdef DBG_DLR_BEACON
+	if (accept || dbg_bcn > 0) {
+		dbg_msg("b: %d=%d:%d %04x; %02x %02x:%02x:%02x %08lx; "
+			"%d %d %lx %d\n",
+			port,
+			beacon->data.beacon.ring_state, info->ring_state,
+			ntohs(vlan->h_vlan_TCI),
+			beacon->data.beacon.precedence,
+			vlan->h_source[3], vlan->h_source[4], vlan->h_source[5],
+			seqid, info->new_supervisor,
+			beacon_info->timeout,
+			jiffies, dbg_bcn);
+	}
+	if (dbg_bcn)
+		--dbg_bcn;
+#endif
+	return accept;
+}  /* checkBeacon */
+
+static int handleBeacon(struct ksz_dlr_info *info,
+	struct ksz_dlr_rx_frame *frame, int port)
+{
+	u32 interval;
+	u32 timeout;
+	u16 vid;
+	int cmp = 0;
+	int update = false;
+	struct vlan_ethhdr *vlan = frame->vlan;
+	struct ksz_dlr_frame *beacon = frame->body;
+	struct ksz_dlr_gateway_capable *attrib = &info->attrib;
+	struct ksz_dlr_beacon_info *beacon_info = &info->beacon_info[port];
+#ifdef CONFIG_HAVE_DLR_HW
+	struct ksz_sw *sw = info->sw_dev;
+#endif
+
+	/* Only accepted beacon from active supervisor is processed. */
+	cmp = memcmp(vlan->h_source, attrib->active_super_addr.addr,
+		ETH_ALEN);
+	if (cmp) {
+#if 0
+		struct ksz_dlr_super_info found;
+
+		memcpy(&found.prec_addr[1], vlan->h_source, ETH_ALEN);
+		found.port = port;
+		info->rogue_super = &found;
+		dlr_tx_learning_update(info);
+#endif
+		return false;
+	}
+
+	/* Process accepted beacon. */
+	if (info->new_supervisor) {
+		if (info->notifications & DLR_INFO_CFG_CHANGE)
+			dlr_notify_cfg_change(info, 1);
+		update = true;
+	}
+
+	interval = ntohl(beacon->data.beacon.interval);
+	timeout = ntohl(beacon->data.beacon.timeout);
+
+	/* Used to determine beacon timeout in software simulation. */
+	beacon_info->rcv_once = 1;
+	beacon_info->timeout_start = 0;
+
+	if (!info->skip_beacon && info->drop_beacon) {
+		dropBeacons(info);
+		info->beacon_info[0].timeout =
+		info->beacon_info[1].timeout = 0;
+		return update;
+	}
+	if (update && info->skip_beacon)
+		acceptBeacons(info);
+
+	/* Not running as supervisor. */
+	if ((DLR_ACTIVE_SUPERVISOR != info->node &&
+	    info->ring_state != beacon->data.beacon.ring_state) ||
+	    update) {
+		if ((info->notifications & DLR_INFO_CFG_CHANGE) &&
+		    info->ring_state != beacon->data.beacon.ring_state)
+			dlr_notify_cfg_change(info, 2);
+		info->ring_state = beacon->data.beacon.ring_state;
+
+		/* Set in following code. */
+		info->p1_rcvd = info->p2_rcvd =
+		info->one_rcvd = info->both_rcvd = 0;
+		beacon_info->timeout = 0;
+
+#ifdef DBG_DLR_STATE
+		if (RING_FAULT_STATE == info->ring_state) {
+			dbg_dlr(info, "ring fault");
+		}
+#endif
+		/* Get back into backup mode. */
+		if (7 == info->beacon_timeout_ports)
+			info->beacon_timeout_ports = 0;
+	}
+	if (1 == port) {
+		info->p2_rcvd = 1;
+		info->p2_timeout = 0;
+		info->p2_down = 0;
+	} else {
+		info->p1_rcvd = 1;
+		info->p1_timeout = 0;
+		info->p1_down = 0;
+	}
+	if (!info->p1_timeout && !info->p2_timeout)
+		info->one_timeout = 0;
+	info->both_timeout = 0;
+
+	/* Change down state as beacon can be received before link status. */
+	if (!info->p1_down && !info->p2_down)
+		info->one_down = 0;
+	info->both_down = 0;
+
+	if (info->p1_rcvd && info->p2_rcvd) {
+
+		/* Running as supervisor. */
+		if (DLR_ACTIVE_SUPERVISOR == info->node ||
+		    RING_NORMAL_STATE == info->ring_state) {
+			if (!info->both_rcvd)
+				update = true;
+			info->both_rcvd = 1;
+			info->one_rcvd = 0;
+		}
+	} else {
+		if (!info->one_rcvd)
+			update = true;
+		info->one_rcvd = 1;
+		beacon_info->timeout = 0;
+	}
+	if (attrib->active_super_prec != beacon->data.beacon.precedence) {
+		attrib->active_super_prec = beacon->data.beacon.precedence;
+	}
+	vid = info->vid;
+	if (vlan->h_vlan_proto == htons(ETH_P_8021Q))
+		vid = ntohs(vlan->h_vlan_TCI) & ((1 << VLAN_PRIO_SHIFT) - 1);
+	if (info->vid != vid) {
+
+		/* Use current VID. */
+		update_vlan(info, vid);
+		if (attrib->cap & DLR_CAP_SUPERVISOR_CAPABLE)
+			attrib->super_cfg.vid = vid;
+		if (attrib->super_cfg.enable) {
+			attrib->super_cfg.vid = vid;
+#ifdef CONFIG_HAVE_DLR_HW
+			sw->ops->acquire(sw);
+			dlr_hw_set_vlan_id(sw, info->vid);
+			sw->ops->release(sw);
+#endif
+		}
+	}
+	if (info->beacon_interval != interval) {
+#ifdef DBG_DLR_OPER
+		dbg_msg("interval %u %u\n", info->beacon_interval, interval);
+#endif
+
+		/* Use current beacon interval. */
+		info->beacon_interval = interval;
+		if (attrib->cap & DLR_CAP_SUPERVISOR_CAPABLE)
+			attrib->super_cfg.beacon_interval = interval;
+		if (attrib->super_cfg.enable) {
+			attrib->super_cfg.beacon_interval = interval;
+#ifdef CONFIG_HAVE_DLR_HW
+			sw->ops->acquire(sw);
+			dlr_hw_set_interval(sw, info->beacon_interval);
+			sw->ops->release(sw);
+#endif
+		}
+	}
+	if (info->beacon_timeout != timeout) {
+#ifdef DBG_DLR_OPER
+		dbg_msg("timeout %u %u\n", info->beacon_timeout, timeout);
+#endif
+
+		/* Use current beacon timeout. */
+		info->beacon_timeout = timeout;
+		if (attrib->cap & DLR_CAP_SUPERVISOR_CAPABLE)
+			attrib->super_cfg.beacon_timeout = timeout;
+		if (attrib->super_cfg.enable) {
+			attrib->super_cfg.beacon_timeout = timeout;
+#ifdef CONFIG_HAVE_DLR_HW
+			sw->ops->acquire(sw);
+			dlr_hw_set_timeout(sw, info->beacon_timeout);
+			sw->ops->release(sw);
+#endif
+		}
+		info->p1_set = info->p2_set = 1;
+	}
+#ifdef DBG_DLR_BEACON
+	if (update || dbg_bcn > 0) {
+		u32 seqid = ntohl(beacon->hdr.seqid);
+		dbg_msg("B: %d=%d:%d; r=%d:%d R=%d:%d; %08lx %d %lx\n",
+			port,
+			beacon->data.beacon.ring_state, info->ring_state,
+			info->p1_rcvd, info->p2_rcvd,
+			info->one_rcvd, info->both_rcvd,
+			seqid, info->new_supervisor,
+			jiffies);
+	}
+#endif
+
+	beacon_info->interval = 0;
+	if (info->p1_rcvd && info->p1_set) {
+		info->p1_set = 0;
+#ifdef CONFIG_HAVE_ACL_HW
+		setup_acl_beacon_timeout(info, info->ports[0]);
+#endif
+	}
+	if (info->p2_rcvd && info->p2_set) {
+		info->p2_set = 0;
+#ifdef CONFIG_HAVE_ACL_HW
+		setup_acl_beacon_timeout(info, info->ports[1]);
+#endif
+	}
+	return update;
+}  /* handleBeacon */
+
+static int handleSignOn(struct ksz_dlr_info *info,
+	struct ksz_dlr_rx_frame *frame, int port)
+{
+	int i;
+	struct vlan_ethhdr *vlan = frame->vlan;
+	struct ksz_dlr_frame *signon = frame->body;
+	u16 num = ntohs(signon->data.signon.num);
+	struct ksz_dlr_node *node = signon->data.signon.node;
+
+	/* Ignore if not NORMAL_STATE. */
+	if (info->state != DLR_ACTIVE_NORMAL_STATE &&
+	    info->state != DLR_NORMAL_STATE &&
+	    !(info->overrides & DLR_TEST)) {
+#ifdef DBG_DLR_ANN_SIGNON
+		dbg_dlr(info, " ?signon");
+#endif
+#ifdef DBG_DLR_BEACON
+		dbg_bcn = 10;
+#endif
+		return false;
+	}
+#ifdef DBG_DLR_BEACON
+	dbg_bcn = 2;
+#endif
+	if (DLR_ACTIVE_SUPERVISOR == info->node &&
+	    !memcmp(node->addr, info->src_addr, ETH_ALEN)) {
+		struct ksz_dlr_node_info *cur;
+
+#ifdef DBG_DLR_SIGNON
+		dbg_msg("signon: %d; %d; %02x\n", num, port, vlan->h_dest[0]);
+		dbg_msg("%02x:%02x:%02x:%02x:%02x:%02x\n",
+			node->addr[0],
+			node->addr[1],
+			node->addr[2],
+			node->addr[3],
+			node->addr[4],
+			node->addr[5]);
+#endif
+		for (i = 0; i < num; i++, node++) {
+			if (i && !memcmp(node->addr, info->src_addr, ETH_ALEN))
+				continue;
+#ifdef DBG_DLR_SIGNON
+			dbg_msg("%d %02x:%02x:%02x:%02x:%02x:%02x\n", i,
+				node->addr[0],
+				node->addr[1],
+				node->addr[2],
+				node->addr[3],
+				node->addr[4],
+				node->addr[5]);
+#endif
+			cur = &info->nodes[info->attrib.participants_cnt];
+			memcpy(&cur->signon, node,
+				sizeof(struct ksz_dlr_node));
+			cur->p1_down = cur->p2_down =
+			cur->p1_lost = cur->p2_lost = 0;
+			info->attrib.participants_cnt++;
+		}
+
+		/* Addressed to the supervisor instead of multicast address. */
+		if (!memcmp(info->src_addr, vlan->h_dest, ETH_ALEN)) {
+			memcpy(info->signon_addr, vlan->h_source,
+				ETH_ALEN);
+			dlr_tx_signon(info, 0);
+		} else
+			disableSignOnTimer(info);
+	} else {
+		int len;
+		struct ksz_dlr_tx_frame *tx = (struct ksz_dlr_tx_frame *)
+			info->signon_frame;
+
+		if ((info->overrides & DLR_TEST) && info->ignore_req) {
+			++info->req_cnt[0];
+			if (info->req_cnt[0] <= info->ignore_req)
+				printk(KERN_INFO
+					"ignore SignOn: %d\n",
+					info->req_cnt[0]);
+			if (info->req_cnt[0] <= info->ignore_req)
+				return false;
+			info->req_cnt[0] = 0;
+		}
+		len = sizeof(struct ksz_dlr_hdr) +
+			sizeof(struct ksz_dlr_signon) +
+			(num - 1) * sizeof(struct ksz_dlr_node);
+		memcpy(info->signon_frame, vlan, ETH_ALEN * 2);
+		memcpy(&tx->body, signon, len);
+		len += sizeof(struct vlan_ethhdr);
+		info->rx_port = port;
+		info->tx_port = (port + 1) & 1;
+		dlr_tx_signon(info, len);
+		if (info->active_port != (1 << info->ports[info->rx_port]))
+			setupDir(info, port);
+	}
+	return false;
+}  /* handleSignOn */
+
+static int handleLocateFault(struct ksz_dlr_info *info,
+	struct ksz_dlr_rx_frame *frame, int port)
+{
+	struct vlan_ethhdr *vlan = frame->vlan;
+
+	/* Ignore if not FAULT_STATE. */
+	if (info->state != DLR_FAULT_STATE &&
+	    !(info->overrides & DLR_TEST)) {
+		dbg_msg(" ?%s %d\n", __func__, port);
+		return false;
+	}
+	if (info->node != DLR_ACTIVE_SUPERVISOR &&
+	    !memcmp(vlan->h_source, info->attrib.active_super_addr.addr,
+	    ETH_ALEN)) {
+		info->fault_jiffies = jiffies;
+		if (info->p1_down || info->p2_down)
+			dlr_tx_status(info, 0);
+		else if (!info->neigh_chk) {
+			info->neigh_chk = 1;
+			ksz_start_timer(&info->neigh_chk_timer_info,
+				info->neigh_chk_timer_info.period);
+			info->neigh_chk_timer_info.max = 3;
+			info->p1_lost = info->p2_lost = 0;
+			info->port_chk[0] = info->port_chk[1] = 1;
+			dlr_tx_chk_req(info, 0);
+			dlr_tx_chk_req(info, 1);
+		}
+	} else {
+		dbg_msg("%s ignored\n", __func__);
+	}
+	return false;
+}  /* handleLocateFault */
+
+static int handleAnnounce(struct ksz_dlr_info *info,
+	struct ksz_dlr_rx_frame *frame, int port)
+{
+	u32 seqid;
+	u16 vid;
+	struct vlan_ethhdr *vlan = frame->vlan;
+	struct ksz_dlr_frame *announce = frame->body;
+	int new = 0;
+	struct ksz_dlr_gateway_capable *attrib = &info->attrib;
+
+	/* Supervisor direction is not set yet. */
+	if (RING_NORMAL_STATE == announce->data.announce.ring_state &&
+	    DLR_NORMAL_STATE == info->state &&
+	    (!info->active_port || info->rx_port != port)) {
+#ifdef DBG_DLR_OPER
+		dbg_msg("%s p=%d r=%d x%x\n", __func__, port, info->rx_port,
+			info->active_port);
+#endif
+		setup_dir(info, port);
+	}
+
+#ifdef DBG_DLR_STATE
+	if (DLR_ANNOUNCE_NODE != info->node) {
+		if (announce->data.announce.ring_state != info->ring_state)
+			dbg_msg("  ann: p=%d r=%d != %d\n",
+				port, info->ring_state,
+				announce->data.announce.ring_state);
+		else if (RING_NORMAL_STATE == info->ring_state &&
+			 info->state != DLR_NORMAL_STATE)
+			dbg_msg("  ann ring normal: p=%d s=%d\n",
+				port, info->state);
+	}
+#endif
+
+	/* Rely on Announce frame to determine ring state. */
+	if (info->skip_beacon) {
+		if (memcmp(vlan->h_source, attrib->active_super_addr.addr,
+		    ETH_ALEN)) {
+dbg_msg(" diff super\n");
+			dlr_set_addr(&attrib->active_super_addr,
+				announce->hdr.ip_addr, vlan->h_source);
+		}
+		if (!memcmp(vlan->h_source, attrib->active_super_addr.addr,
+		    ETH_ALEN) && info->ring_state !=
+		    announce->data.announce.ring_state) {
+			if ((info->notifications & DLR_INFO_CFG_CHANGE) &&
+			    info->ring_state !=
+			    announce->data.announce.ring_state)
+				dlr_notify_cfg_change(info, 2);
+			info->ring_state = announce->data.announce.ring_state;
+			acceptBeacons(info);
+#ifdef DBG_DLR_BEACON
+			dbg_bcn += 4;
+#endif
+			return true;
+		}
+	}
+
+	if (DLR_ACTIVE_SUPERVISOR == info->node) {
+#if 0
+		dbg_msg("%s ignored %d %d %d\n", __func__, port,
+			announce->data.announce.ring_state, info->skip_beacon);
+		if (vlan)
+			dbg_msg("src: %02x:%02x:%02x:%02x:%02x:%02x\n",
+				vlan->h_source[0], vlan->h_source[1],
+				vlan->h_source[2], vlan->h_source[3],
+				vlan->h_source[4], vlan->h_source[5]);
+#endif
+		return false;
+	}
+	if (DLR_SUPERVISOR == info->node)
+		goto done;
+	if (DLR_ANNOUNCE_NODE != info->node)
+		return false;
+
+	info->rx_port = port;
+	info->tx_port = (port + 1) & 1;
+	seqid = ntohl(announce->hdr.seqid);
+
+	if (memcmp(vlan->h_source, attrib->active_super_addr.addr, ETH_ALEN) ||
+	    info->ann_timeout) {
+		memcpy(info->last_sup.addr, attrib->active_super_addr.addr,
+			ETH_ALEN);
+		dlr_set_addr(&attrib->active_super_addr,
+			announce->hdr.ip_addr, vlan->h_source);
+		info->new_supervisor = 1;
+		new = 1;
+		if (info->notifications & DLR_INFO_CFG_CHANGE)
+			dlr_notify_cfg_change(info, 1);
+	} else {
+		/* Check valid sequence number. */
+		if (!seq_ahead(info->seqid_announce, seqid))
+			return false;
+	}
+	info->seqid_announce = seqid;
+	if (announce->data.announce.ring_state != info->ring_state || new) {
+		if ((info->notifications & DLR_INFO_CFG_CHANGE) &&
+		    info->ring_state != announce->data.announce.ring_state)
+			dlr_notify_cfg_change(info, 2);
+		info->ring_state = announce->data.announce.ring_state;
+		info->ann_rcvd = 1;
+		new = 1;
+	}
+	if (vlan->h_vlan_proto == htons(ETH_P_8021Q)) {
+		vid = ntohs(vlan->h_vlan_TCI) & ((1 << VLAN_PRIO_SHIFT) - 1);
+		if (vid != info->vid) {
+			update_vlan(info, vid);
+			new = 1;
+		}
+	}
+
+done:
+	info->ann_timeout = 0;
+
+	ksz_stop_timer(&info->announce_timeout_timer_info);
+	ksz_start_timer(&info->announce_timeout_timer_info,
+		info->announce_timeout_timer_info.period);
+	info->announce_timeout_timer_info.max = 1;
+	return new;
+}  /* handleAnnounce */
+
+static int handleNeighChkReq(struct ksz_dlr_info *info,
+	struct ksz_dlr_rx_frame *frame, int port)
+{
+	struct ksz_dlr_frame *req = frame->body;
+
+	/* Ignore if not FAULT_STATE. */
+	if (info->state != DLR_FAULT_STATE &&
+	    info->state != DLR_ACTIVE_FAULT_STATE &&
+	    !(info->overrides & DLR_TEST))
+		return false;
+	if (info->overrides & DLR_TEST) {
+		printk(KERN_INFO "req: p=%d s=%08x %d; %lx\n",
+			port, ntohl(req->hdr.seqid), req->hdr.src_port,
+			jiffies);
+	}
+	if ((info->overrides & DLR_TEST) && info->ignore_req) {
+		++info->req_cnt[port];
+		if (info->req_cnt[port] <= info->ignore_req)
+			return false;
+		info->req_cnt[port] = 0;
+	}
+	info->port_rcv[port] = req->hdr.src_port;
+	info->seqid_rcv[port] = ntohl(req->hdr.seqid);
+	dlr_tx_chk_resp(info, port);
+	return false;
+}  /* handleNeighChkReq */
+
+static int handleNeighChkResp(struct ksz_dlr_info *info,
+	struct ksz_dlr_rx_frame *frame, int port)
+{
+	struct ksz_dlr_frame *resp = frame->body;
+	int src_port = port ? DLR_PORT_2 : DLR_PORT_1;
+
+	/* Ignore if not FAULT_STATE. */
+	if (info->state != DLR_FAULT_STATE &&
+	    info->state != DLR_ACTIVE_FAULT_STATE &&
+	    !(info->overrides & DLR_TEST))
+		return false;
+	if (info->overrides & DLR_TEST) {
+		printk(KERN_INFO "resp: p=%d s=%08x %d; %08x - %08x; %lx\n",
+			port, ntohl(resp->hdr.seqid),
+			resp->data.neigh_chk_resp.src_port,
+			info->seqid_first[port],
+			info->seqid_chk[port], jiffies);
+	}
+	if (src_port == resp->data.neigh_chk_resp.src_port) {
+		u32 seqid = ntohl(resp->hdr.seqid);
+
+		if (seq_in(info->seqid_first[port], info->seqid_chk[port],
+		    seqid)) {
+			if (port)
+				info->p2_down = info->p2_lost = 0;
+			else
+				info->p1_down = info->p1_lost = 0;
+			info->port_chk[port] = 0;
+			if (!info->port_chk[(port + 1) & 1]) {
+				ksz_stop_timer(&info->neigh_chk_timer_info);
+				info->neigh_chk = 0;
+			}
+		}
+	}
+	return false;
+}  /* handleNeighChkResp */
+
+static int handleFlushTables(struct ksz_dlr_info *info,
+	struct ksz_dlr_rx_frame *frame, int port)
+{
+	struct ksz_dlr_frame *flush = frame->body;
+
+	/* Ignore if not in NORMAL_STATE or FAULT_STATE. */
+	if (info->state <= DLR_IDLE_STATE &&
+	    !(info->overrides & DLR_TEST))
+		return false;
+	dlr_flush(info);
+	if (flush->data.flush.learning_update_enable)
+		dlr_tx_learning_update(info);
+
+	return false;
+}  /* handleFlushTables */
+
+static int handleLinkStatus(struct ksz_dlr_info *info,
+	struct ksz_dlr_rx_frame *frame, int port)
+{
+	struct vlan_ethhdr *vlan = frame->vlan;
+	struct ksz_dlr_frame *status = frame->body;
+
+	if (info->overrides & DLR_TEST) {
+		printk(KERN_INFO
+			"link: %02x:%02x:%02x:%02x:%02x:%02x %d:%d %d\n",
+			vlan->h_source[0], vlan->h_source[1], vlan->h_source[2],
+			vlan->h_source[3], vlan->h_source[4], vlan->h_source[5],
+			status->data.status.port1_active,
+			status->data.status.port2_active,
+			status->data.status.neighbor);
+	}
+	if (DLR_ACTIVE_SUPERVISOR == info->node) {
+		int i;
+		struct ksz_dlr_node_info *node;
+
+#ifdef DBG_DLR_STATE
+		dbg_msg("link: %02x:%02x:%02x:%02x:%02x:%02x %x %d:%d %d\n",
+			vlan->h_source[0],
+			vlan->h_source[1],
+			vlan->h_source[2],
+			vlan->h_source[3],
+			vlan->h_source[4],
+			vlan->h_source[5],
+			ntohl(status->hdr.seqid),
+			status->data.status.port1_active,
+			status->data.status.port2_active,
+			status->data.status.neighbor);
+#endif
+		for (i = 1; i < info->attrib.participants_cnt; i++) {
+			node = &info->nodes[i];
+			if (!memcmp(node->signon.addr, vlan->h_source,
+			    ETH_ALEN)) {
+				if (status->data.status.neighbor) {
+					if (status->data.status.port1_active) {
+						node->p1_down = 0;
+						node->p1_lost = 0;
+					} else {
+						node->p1_lost = 1;
+					}
+					if (status->data.status.port2_active) {
+						node->p2_down = 0;
+						node->p2_lost = 0;
+					} else {
+						node->p2_lost = 1;
+					}
+				} else {
+					if (status->data.status.port1_active) {
+						node->p1_down = 0;
+					} else {
+						node->p1_down = 1;
+					}
+					if (status->data.status.port2_active) {
+						node->p2_down = 0;
+					} else {
+						node->p2_down = 1;
+					}
+				}
+				break;
+			}
+		}
+		if (!status->data.status.port1_active ||
+		    !status->data.status.port2_active) {
+			dlr_set_addr(&info->attrib.last_active[port],
+				status->hdr.ip_addr, vlan->h_source);
+			if (info->notifications & DLR_INFO_LINK_LOST)
+				dlr_notify_link_lost(info);
+			return true;
+		}
+	} else {
+		dbg_msg("%s ignored\n", __func__);
+	}
+	return false;
+}  /* handleLinkStatus */
+
+static int handleLearningUpdate(struct ksz_dlr_info *info,
+	struct ksz_dlr_rx_frame *frame, int port)
+{
+	struct vlan_ethhdr *vlan = frame->vlan;
+
+#ifdef DBG_DLR_SUPERVISOR
+	dbg_msg("%s %d %d\n", __func__, info->node, port);
+#endif
+	if (!memcmp(info->src_addr, vlan->h_dest, ETH_ALEN)) {
+		if (!newSupervisor) {
+dbg_msg(" %d %d %d  ", info->state, info->node, info->precedence);
+dbg_msg(" to self!\n");
+			dlr_chk_supervisor(info);
+		}
+	}
+	if (DLR_SUPERVISOR <= info->node) {
+		if (!memcmp(info->src_addr, vlan->h_source, ETH_ALEN)) {
+dbg_msg("%s %d\n", __func__, info->node);
+		}
+	}
+	return false;
+}  /* handleLearningUpdate */
+
+static void dlr_clr_last_beacon(struct ksz_dlr_info *info, int missed)
+{
+	int last = info->LastBcnRcvPort;
+
+	info->LastBcnRcvPort &= ~missed;
+	if (last != info->LastBcnRcvPort &&
+	    0 == info->LastBcnRcvPort) {
+#ifdef DBG_DLR_STATE
+		if (DLR_SUPERVISOR == info->node)
+			dbg_msg("become active\n");
+		else if (DLR_ACTIVE_SUPERVISOR != info->node)
+			dbg_msg("become idle\n");
+#endif
+	}
+}  /* dlr_clr_last_beacon */
+
+static int handleBeaconTimeout(struct ksz_dlr_info *info, int port)
+{
+	struct ksz_dlr_beacon_info *beacon_info = &info->beacon_info[port];
+
+	if (1 == port) {
+		info->p2_rcvd = 0;
+		info->p2_timeout = 1;
+	} else {
+		info->p1_rcvd = 0;
+		info->p1_timeout = 1;
+	}
+	memset(&beacon_info->last, 0, sizeof(struct ksz_dlr_beacon));
+	beacon_info->timeout = 0;
+	memset(&info->last_beacon[port], 0, sizeof(struct ksz_dlr_tx_frame));
+	if (info->p1_rcvd || info->p2_rcvd)
+		info->one_rcvd = 1;
+	else
+		info->one_rcvd = 0;
+	info->both_rcvd = 0;
+	if ((info->p1_timeout && info->p2_timeout)) {
+		info->both_timeout = 1;
+		info->one_timeout = 0;
+		info->chk_hw = 1;
+	} else
+		info->one_timeout = 1;
+
+#ifdef DBG_DLR_STATE
+	dbg_dlr(info, "timeout");
+#endif
+	if (info->both_timeout)
+		info->p1_timeout = info->p2_timeout = 0;
+	dlr_clr_last_beacon(info, port ? 2 : 1);
+
+	/* Automatic backup supervisor will start. */
+	if (info->both_timeout && DLR_SUPERVISOR == info->node)
+		info->start = 1;
+	return true;
+}  /* handleBeaconTimeout */
+
+static int handleLinkChange(struct ksz_dlr_info *info, int link1, int link2)
+{
+	int change[2];
+	int down[2];
+	int update = false;
+	int going_up = false;
+	int missed = 0;
+
+	/* State machine not ready yet. */
+	if (!info->state)
+		return update;
+	down[0] = down[1] = 0;
+	change[0] = !link1 ^ info->p1_down;
+	change[1] = !link2 ^ info->p2_down;
+	if (link1)
+		info->p1_down = 0;
+	else
+		info->p1_down = 1;
+	if (link2)
+		info->p2_down = 0;
+	else
+		info->p2_down = 1;
+	if (DLR_ACTIVE_SUPERVISOR == info->node) {
+		struct ksz_dlr_node_info *node;
+
+		node = find_dlr_node(info, info->src_addr);
+		if (node) {
+			node->p1_down = info->p1_down;
+			node->p2_down = info->p2_down;
+		}
+	}
+	if ((!change[0] || link1) && (!change[1] || link2)) {
+		going_up = true;
+		info->fault_jiffies = 0;
+		return update;
+	}
+	if (info->p1_down && info->p2_down) {
+		if (!info->both_down)
+			update = true;
+		info->both_down = 1;
+		info->one_down = 0;
+		info->one_rcvd = 0;
+	} else if (info->p1_down || info->p2_down) {
+		if (!info->one_down)
+			update = true;
+		info->both_down = 0;
+		info->one_down = 1;
+		if (info->both_rcvd)
+			info->one_rcvd = 1;
+	} else {
+		info->one_down = 0;
+		info->both_down = 0;
+	}
+
+	/* Reset beacon timeout if the link also goes down. */
+	if (info->one_timeout) {
+		if ((info->p1_timeout && info->p1_down) ||
+		    (info->p2_timeout && info->p2_down)) {
+			info->one_timeout = 0;
+			info->p1_timeout = info->p2_timeout = 0;
+		}
+	}
+	if (info->p1_down) {
+		info->p1_rcvd = 0;
+		info->both_rcvd = 0;
+		missed |= 1;
+
+		/* Stop neighbor check if link is down (unlikely). */
+		if (info->port_chk[0] && info->port_chk[0] < 3)
+			info->port_chk[0] = 3;
+	}
+	if (info->p2_down) {
+		info->p2_rcvd = 0;
+		info->both_rcvd = 0;
+		missed |= 2;
+
+		/* Stop neighbor check if link is down (unlikely). */
+		if (info->port_chk[1] && info->port_chk[1] < 3)
+			info->port_chk[1] = 3;
+	}
+	dlr_clr_last_beacon(info, missed);
+	down[0] = info->p1_down;
+	down[1] = info->p2_down;
+	if (info->node != DLR_ANNOUNCE_NODE) {
+		info->beacon_info[0].timer = !down[0];
+		info->beacon_info[1].timer = !down[1];
+	}
+	if (going_up)
+		update = false;
+	if ((info->p1_down || info->p2_down) && !going_up) {
+		int p;
+
+#ifdef DBG_DLR_STATE
+		dbg_msg("Lo: %d:%d %x\n", down[0], down[1],
+			info->LastBcnRcvPort);
+#endif
+		if (info->fault_jiffies)
+			dbg_msg("time from fault: %lu\n",
+				jiffies - info->fault_jiffies);
+		if (DLR_ACTIVE_SUPERVISOR == info->node) {
+			struct ksz_dlr_gateway_capable *attrib = &info->attrib;
+
+			for (p = 0; p < 2; p++) {
+				if (down[p]) {
+					dlr_set_addr(&attrib->last_active[p],
+						info->ip_addr, info->src_addr);
+				}
+			}
+		}
+
+		/* Reset last beacon in case timeout is not processed. */
+		for (p = 0; p < 2; p++) {
+			if (down[p]) {
+				memset(&info->beacon_info[p].last, 0,
+					sizeof(struct ksz_dlr_beacon));
+				info->beacon_info[p].timeout = 0;
+				memset(&info->last_beacon[p], 0,
+					sizeof(struct ksz_dlr_tx_frame));
+			}
+		}
+	}
+	return update;
+}  /* handleLinkChange */
+
+static void LocateFault(struct ksz_dlr_info *info)
+{
+	if (info->node != DLR_ACTIVE_SUPERVISOR)
+		return;
+	dlr_tx_locate_fault(info);
+}  /* LocateFault */
+
+static void NeighborCheck(struct ksz_dlr_info *info)
+{
+	struct ksz_dlr_gateway_capable *attrib = &info->attrib;
+
+	if (!info->p1_down || !info->p2_down) {
+		info->neigh_chk = 1;
+		ksz_start_timer(&info->neigh_chk_timer_info,
+			info->neigh_chk_timer_info.period);
+		info->neigh_chk_timer_info.max = 3;
+	}
+	if (info->p1_down) {
+		dlr_set_addr(&attrib->last_active[0],
+			info->ip_addr, info->src_addr);
+	} else {
+		info->p1_lost = 0;
+		info->port_chk[0] = 1;
+		dlr_tx_chk_req(info, 0);
+	}
+	if (info->p2_down) {
+		dlr_set_addr(&attrib->last_active[1],
+			info->ip_addr, info->src_addr);
+	} else {
+		info->p2_lost = 0;
+		info->port_chk[1] = 1;
+		dlr_tx_chk_req(info, 1);
+	}
+}  /* NeighborCheck */
+
+static void startSignOn(struct ksz_dlr_info *info, int now)
+{
+	info->attrib.participants_cnt = 0;
+	memcpy(info->signon_addr, MAC_ADDR_SIGNON, ETH_ALEN);
+	ksz_start_timer(&info->signon_timer_info,
+		info->signon_timer_info.period);
+	if (!info->signon_delay || !info->ann_delay) {
+		info->signon_delay = 0;
+		if (now && !info->ann_first)
+			dlr_tx_signon(info, 0);
+		else
+			info->tx_signon = true;
+	}
+#ifdef DBG_DLR_OPER
+	else
+		dbg_msg("%s %d %d %lx\n", __func__,
+			info->signon_delay, info->ann_delay,
+			jiffies);
+#endif
+}  /* startSignOn */
+
+static void sendLinkStatus(struct ksz_dlr_info *info)
+{
+	/* Supervisor is known. */
+	if (info->attrib.active_super_addr.addr[0] ||
+	    info->attrib.active_super_addr.addr[1])
+		dlr_tx_status(info, false);
+}  /* sendLinkStatus */
+
+#if 1
+static int getting_last_active;
+#endif
+
+static void dlr_clear(struct ksz_dlr_info *info)
+{
+	struct ksz_dlr_beacon_info *beacon_info;
+	int p;
+
+	info->p1_rcvd = info->p2_rcvd =
+	info->one_rcvd = info->both_rcvd =
+	info->p1_timeout = info->p2_timeout =
+	info->one_timeout = info->both_timeout = 0;
+	for (p = 0; p < 2; p++ ) {
+		beacon_info = &info->beacon_info[p];
+		beacon_info->timer =
+		beacon_info->rcv_once =
+		beacon_info->timeout_start =
+		beacon_info->timeout_stop = 0;
+		beacon_info->timeout = 0;
+		memset(&beacon_info->last, 0, sizeof(struct ksz_dlr_beacon));
+		memset(&info->last_beacon[p], 0,
+			sizeof(struct ksz_dlr_tx_frame));
+	}
+	memset(info->supers, 0, sizeof(struct ksz_dlr_super_info) *
+		DLR_SUPERVISOR_NUM);
+#if 1
+	getting_last_active = 0;
+#endif
+}  /* dlr_clear */
+
+struct dlr_state {
+	int change;
+	int delay_ann;
+	int new_state;
+};
+
+static void dlr_idle_init(struct ksz_dlr_info *info, struct dlr_state *state)
+{
+	struct ksz_dlr_gateway_capable *attrib = &info->attrib;
+
+#ifdef DBG_DLR_STATE
+	dbg_dlr(info, "idle");
+	dlr_print(info, "idle");
+#endif
+
+	oneBeaconRcvd = twoBeaconsRcvd = 0;
+	oneBeaconTimeout = twoBeaconsTimeout = 0;
+	announcedState = RING_FAULT_STATE;
+
+	attrib->net_topology = DLR_TOPOLOGY_LINEAR;
+	attrib->net_status = DLR_NET_NORMAL;
+	attrib->super_status = DLR_STAT_NO_SUPERVISOR;
+	memset(&attrib->active_super_addr, 0,
+		sizeof(struct ksz_dlr_active_node));
+	attrib->active_super_prec = 0;
+
+	if (info->skip_beacon)
+		acceptBeacons(info);
+	disableLearn(info, 1);
+	flushMacTable(info);
+	dlr_set_state(info);
+	if (info->notifications & DLR_INFO_CFG_CHANGE)
+		dlr_notify_cfg_change(info, 2);
+}  /* dlr_idle_init */
+
+static void dlr_idle_next(struct ksz_dlr_info *info, struct dlr_state *state)
+{
+	if (twoBeaconsRcvd && !faultState) {
+		state->new_state = DLR_NORMAL_STATE;
+	}
+	if (oneBeaconRcvd || (twoBeaconsRcvd && faultState)) {
+		state->new_state = DLR_FAULT_STATE;
+	}
+}  /* dlr_idle_next */
+
+static void dlr_ann_idle_init(struct ksz_dlr_info *info,
+	struct dlr_state *state)
+{
+	struct ksz_dlr_gateway_capable *attrib = &info->attrib;
+
+#ifdef DBG_DLR_STATE
+	dbg_dlr(info, "ann idle");
+	dlr_print(info, "idle");
+#endif
+
+	announceRcvd = 0;
+	announcedState = 0;
+
+	attrib->net_topology = DLR_TOPOLOGY_LINEAR;
+	attrib->net_status = DLR_NET_NORMAL;
+	attrib->super_status = DLR_STAT_NO_SUPERVISOR;
+	memset(&attrib->active_super_addr, 0,
+		sizeof(struct ksz_dlr_active_node));
+	attrib->active_super_prec = 0;
+}  /* dlr_ann_idle_init */
+
+static void dlr_ann_idle_next(struct ksz_dlr_info *info,
+	struct dlr_state *state)
+{
+	if (announceRcvd) {
+		state->new_state = fromRingState;
+	}
+}  /* dlr_ann_idle_next */
+
+static void dlr_fault_init(struct ksz_dlr_info *info, struct dlr_state *state)
+{
+	struct ksz_dlr_gateway_capable *attrib = &info->attrib;
+
+#ifdef DBG_DLR_BEACON
+	dbg_msg("  %08x - %08x; %08x - %08x;\n",
+		info->seqid_accept[0], info->seqid_last[0],
+		info->seqid_accept[1], info->seqid_last[1]);
+#endif
+#ifdef DBG_DLR_STATE
+	dbg_dlr(info, "fault");
+	dlr_print(info, "fault");
+#endif
+
+	newSupervisor = 0;
+	if (DLR_ACTIVE_SUPERVISOR == info->node)
+		announcedState = RING_FAULT_STATE;
+
+	attrib->net_topology = DLR_TOPOLOGY_RING;
+	attrib->net_status = DLR_NET_RING_FAULT;
+	if (DLR_BEACON_NODE >= info->node)
+		attrib->super_status = DLR_STAT_RING_NODE;
+	else
+		attrib->super_status = DLR_STAT_BACKUP_SUPERVISOR;
+
+#if 1
+	disableLearn(info, 1);
+#endif
+	flushMacTable(info);
+#if 0
+	disableLearn(info, 0);
+#endif
+	dlr_set_state(info);
+	if (linkLoss) {
+		linkLoss = 0;
+		sendLinkStatus(info);
+	}
+}  /* dlr_fault_init */
+
+static void dlr_fault_next(struct ksz_dlr_info *info, struct dlr_state *state)
+{
+	if (twoBeaconsRcvd && !faultState) {
+		newSupervisor = 0;
+		disableNeighChkTimers(info);
+		state->new_state = DLR_NORMAL_STATE;
+	}
+	if ((twoBeaconsTimeout || (oneBeaconTimeout && !oneBeaconRcvd)) &&
+	    info->LastBcnRcvPort)
+		dbg_dlr(info, "  ! last beacon");
+	if (!info->LastBcnRcvPort) {
+		if (linkDown)
+dbg_msg(" f linkDown\n");
+		disableNeighChkTimers(info);
+		if (DLR_BEACON_NODE >= info->node)
+			state->new_state = DLR_IDLE_STATE;
+		else
+			state->new_state = DLR_PREPARE_STATE;
+	}
+	if (linkLoss) {
+		linkLoss = 0;
+		sendLinkStatus(info);
+	}
+	if (newSupervisor) {
+		newSupervisor = 0;
+		flushMacTable(info);
+	}
+
+	/* Apply only to supervisor. */
+	if (newValue) {
+		if (updateValues(info))
+			state->new_state = DLR_PREPARE_STATE;
+	}
+	if (announceTimeout) {
+		announceTimeout = false;
+		dbg_msg("  ??? announce timeout\n");
+		disableAnnounceTimeout(info);
+		if (DLR_SUPERVISOR == info->node)
+			state->new_state = DLR_PREPARE_STATE;
+	}
+}  /* dlr_fault_next */
+
+static void dlr_ann_fault_init(struct ksz_dlr_info *info,
+	struct dlr_state *state)
+{
+	struct ksz_dlr_gateway_capable *attrib = &info->attrib;
+
+#ifdef DBG_DLR_STATE
+	dbg_dlr(info, "fault");
+	dlr_print(info, "fault");
+#endif
+
+	announceRcvd = 0;
+
+	attrib->net_topology = DLR_TOPOLOGY_RING;
+	attrib->net_status = DLR_NET_RING_FAULT;
+	attrib->super_status = DLR_STAT_RING_NODE;
+
+	flushMacTable(info);
+	if (linkLoss) {
+		linkLoss = 0;
+		sendLinkStatus(info);
+	}
+}  /* dlr_ann_fault_init */
+
+static void dlr_ann_fault_next(struct ksz_dlr_info *info,
+	struct dlr_state *state)
+{
+	if (announceTimeout) {
+		disableNeighChkTimers(info);
+		state->new_state = DLR_IDLE_STATE;
+	}
+	if (linkDown) {
+		disableAnnounceTimeout(info);
+		disableNeighChkTimers(info);
+		state->new_state = DLR_IDLE_STATE;
+	}
+	if (linkLoss) {
+		linkLoss = 0;
+		sendLinkStatus(info);
+	}
+	if (announceRcvd) {
+		state->new_state = fromRingState;
+	}
+	if (newSupervisor) {
+		newSupervisor = 0;
+		disableNeighChkTimers(info);
+	}
+}  /* dlr_ann_fault_next */
+
+static void dlr_normal_init(struct ksz_dlr_info *info, struct dlr_state *state)
+{
+	struct ksz_dlr_gateway_capable *attrib = &info->attrib;
+
+#ifdef DBG_DLR_BEACON
+	dbg_msg("  %08x - %08x; %08x - %08x;\n",
+		info->seqid_accept[0], info->seqid_last[0],
+		info->seqid_accept[1], info->seqid_last[1]);
+#endif
+#ifdef DBG_DLR_STATE
+	dbg_dlr(info, "normal");
+	dlr_print(info, "normal");
+#endif
+
+	newSupervisor = 0;
+
+	attrib->net_topology = DLR_TOPOLOGY_RING;
+	attrib->net_status = DLR_NET_NORMAL;
+	if (DLR_BEACON_NODE >= info->node)
+		attrib->super_status = DLR_STAT_RING_NODE;
+	else
+		attrib->super_status = DLR_STAT_BACKUP_SUPERVISOR;
+	info->ok_ports = info->member;
+
+	flushMacTable(info);
+	disableLearn(info, 0);
+	dlr_set_state(info);
+}  /* dlr_normal_init */
+
+static void dlr_normal_next(struct ksz_dlr_info *info, struct dlr_state *state)
+{
+	if (twoBeaconsTimeout || linkDown) {
+		if (linkDown)
+dbg_msg(" n linkDown\n");
+		disableNeighChkTimers(info);
+		if (DLR_BEACON_NODE >= info->node)
+			state->new_state = DLR_IDLE_STATE;
+		else
+			state->new_state = DLR_FAULT_STATE;
+	}
+	if (newSupervisor || faultState || oneBeaconTimeout) {
+		state->new_state = DLR_FAULT_STATE;
+	}
+	if (linkLoss) {
+		state->new_state = DLR_FAULT_STATE;
+	}
+	if (announceTimeout) {
+		announceTimeout = false;
+		dbg_msg("  ??? announce timeout\n");
+		disableAnnounceTimeout(info);
+		if (DLR_SUPERVISOR == info->node)
+			state->new_state = DLR_PREPARE_STATE;
+	}
+
+	/* Apply only to supervisor. */
+	if (newValue) {
+		if (updateValues(info))
+			state->new_state = DLR_PREPARE_STATE;
+	}
+	if (state->new_state) {
+		setupDir(info, -1);
+		if (info->skip_beacon)
+			acceptBeacons(info);
+		info->beacon_info[0].timeout =
+		info->beacon_info[1].timeout = 0;
+	}
+	if (DLR_FAULT_STATE == state->new_state) {
+		struct ksz_dlr_gateway_capable *attrib = &info->attrib;
+
+		attrib->fault_cnt++;
+	}
+}  /* dlr_normal_next */
+
+static void dlr_ann_normal_init(struct ksz_dlr_info *info,
+	struct dlr_state *state)
+{
+	struct ksz_dlr_gateway_capable *attrib = &info->attrib;
+
+#ifdef DBG_DLR_STATE
+	dbg_dlr(info, "normal");
+	dlr_print(info, "normal");
+#endif
+
+	announceRcvd = 0;
+
+	attrib->net_topology = DLR_TOPOLOGY_RING;
+	attrib->net_status = DLR_NET_NORMAL;
+	attrib->super_status = DLR_STAT_RING_NODE;
+	info->ok_ports = info->member;
+
+	flushMacTable(info);
+	disableNeighChkTimers(info);
+}  /* dlr_ann_normal_init */
+
+static void dlr_ann_normal_next(struct ksz_dlr_info *info,
+	struct dlr_state *state)
+{
+	if (announceTimeout) {
+		state->new_state = DLR_IDLE_STATE;
+	}
+	if (announceRcvd) {
+		state->new_state = fromRingState;
+	}
+	if (linkLoss) {
+		state->new_state = DLR_FAULT_STATE;
+	}
+	if (state->new_state)
+		setupDir(info, -1);
+	if (DLR_FAULT_STATE == state->new_state) {
+		struct ksz_dlr_gateway_capable *attrib = &info->attrib;
+
+		attrib->fault_cnt++;
+	}
+}  /* dlr_ann_normal_next */
+
+static void dlr_active_init(struct ksz_dlr_info *info, struct dlr_state *state)
+{
+	struct ksz_dlr_gateway_capable *attrib = &info->attrib;
+
+#ifdef DBG_DLR_STATE
+	dbg_dlr(info, "active");
+#endif
+	dlr_clear(info);
+	info->beacon_info[0].timer =
+	info->beacon_info[1].timer = 1;
+#if 1
+	getting_last_active = 0;
+#endif
+
+	/* Reset incoming and outgoing ports. */
+	info->tx_port = info->port;
+	info->rx_port = (info->tx_port + 1) & 1;
+	info->LastBcnRcvPort = 0;
+	info->node = DLR_ACTIVE_SUPERVISOR;
+	info->interval = info->beacon_interval;
+
+	attrib->net_topology = DLR_TOPOLOGY_RING;
+	attrib->super_status = DLR_STAT_ACTIVE_SUPERVISOR;
+	dlr_set_addr(&attrib->active_super_addr,
+		info->ip_addr, info->src_addr);
+	attrib->active_super_prec = attrib->super_cfg.prec;
+
+	/* Supervisor source address may change. */
+	info->p1_set = info->p2_set = 1;
+
+	if (!newSupervisor) {
+		enableSupervisor(info);
+		disableAnnounceTimeout(info);
+	}
+	if (state->change > 0)
+		state->change--;
+}  /* dlr_active_init */
+
+static void dlr_active_next(struct ksz_dlr_info *info, struct dlr_state *state)
+{
+	state->delay_ann = 1;
+	state->new_state = DLR_ACTIVE_FAULT_STATE;
+	if (newSupervisor)
+		state->new_state = DLR_BACKUP_STATE;
+	else if (info->notifications & DLR_INFO_CFG_CHANGE)
+		dlr_notify_cfg_change(info, 1);
+#ifdef DBG_DLR_BEACON_
+	if (!dbg_bcn)
+		dbg_bcn = 4;
+#endif
+}  /* dlr_active_next */
+
+static void dlr_active_fault_init(struct ksz_dlr_info *info,
+	struct dlr_state *state)
+{
+	struct ksz_dlr_gateway_capable *attrib = &info->attrib;
+
+#ifdef DBG_DLR_SUPERVISOR
+	dbg_msg("  %d %d;\n",
+		info->beacon_info[0].timeout, info->beacon_info[1].timeout);
+#endif
+#ifdef DBG_DLR_STATE
+	dbg_dlr(info, "active fault");
+	dlr_print(info, "active fault");
+#endif
+	dbg_supervisor(info);
+#ifdef DBG_DLR_BEACON
+	dbg_bcn = 3;
+#endif
+
+	linkLoss = 0;
+	if (!memcmp(info->attrib.active_super_addr.addr, info->src_addr,
+	    ETH_ALEN))
+		newSupervisor = 0;
+	announcedState = RING_FAULT_STATE;
+
+	attrib->net_status = DLR_NET_RING_FAULT;
+
+	/* Reset timeout notification. */
+	info->beacon_timeout_ports = 0;
+
+#ifdef DBG_DLR_ANN_SIGNON
+	dbg_ann = 3;
+#endif
+	flushMacTable(info);
+	enableBothPorts(info);
+	setupBeacons(info);
+	enableAnnounce(info, state->delay_ann);
+
+	/* Coming here from active normal state. */
+	if (!state->delay_ann && !linkDown) {
+#if 1
+		getting_last_active = 1;
+#endif
+		LocateFault(info);
+		NeighborCheck(info);
+	}
+	state->delay_ann = 0;
+#if 0
+	if (info->notifications & DLR_INFO_CFG_CHANGE)
+		dlr_notify_cfg_change(info, 2);
+#endif
+}  /* dlr_active_fault_init */
+
+static void dlr_active_fault_next(struct ksz_dlr_info *info,
+	struct dlr_state *state)
+{
+	if (oneBeaconTimeout || twoBeaconsTimeout) {
+		oneBeaconTimeout = twoBeaconsTimeout = 0;
+		if (info->start && info->chk_hw)
+			dlr_chk_supervisor(info);
+	}
+	if (twoBeaconsRcvd) {
+		disableNeighChkTimers(info);
+		state->new_state = DLR_ACTIVE_NORMAL_STATE;
+	}
+	if (newSupervisor) {
+		disableNeighChkTimers(info);
+		state->new_state = DLR_BACKUP_STATE;
+	}
+	if (!newSupervisor && newValue) {
+		state->new_state = DLR_RESTART_STATE;
+	}
+
+	if (!state->new_state && DLR_ACTIVE_SUPERVISOR == info->node &&
+	    memcmp(info->attrib.active_super_addr.addr, info->src_addr,
+	    ETH_ALEN)) {
+		printk(KERN_INFO "still active fault\n");
+dbg_msg("prec: %02x %d\n", info->attrib.active_super_addr.addr[5],
+info->precedence);
+		state->new_state = DLR_BACKUP_STATE;
+	}
+}  /* dlr_active_fault_next */
+
+static void dlr_active_normal_init(struct ksz_dlr_info *info,
+	struct dlr_state *state)
+{
+	struct ksz_dlr_gateway_capable *attrib = &info->attrib;
+
+#ifdef DBG_DLR_STATE
+	dbg_dlr(info, "active normal");
+	dlr_print(info, "active normal");
+#endif
+#ifdef DBG_DLR_BEACON
+	dbg_bcn += 4;
+#endif
+
+	announcedState = RING_NORMAL_STATE;
+
+	attrib->net_status = DLR_NET_NORMAL;
+	memset(&attrib->last_active[0], 0,
+		sizeof(struct ksz_dlr_active_node));
+	memset(&attrib->last_active[1], 0,
+		sizeof(struct ksz_dlr_active_node));
+#if 1
+	getting_last_active = 0;
+#endif
+
+	/* Allow timeout notification. */
+	info->beacon_timeout_ports = 0;
+	info->ok_ports = info->member;
+
+#ifdef DBG_DLR_ANN_SIGNON
+	dbg_ann = 3;
+#endif
+	enableOnePort(info);
+	flushMacTable(info);
+	setupBeacons(info);
+	enableAnnounce(info, 0);
+
+	/*
+	 * Need to wait until the normal beacons are
+	 * sent.
+	 */
+	info->signon_delay = 1;
+	if (!info->signon_start) {
+		info->signon_start = 1;
+		startSignOn(info, true);
+	}
+	if (info->notifications & DLR_INFO_CFG_CHANGE)
+		dlr_notify_cfg_change(info, 2);
+}  /* dlr_active_normal_init */
+
+static void dlr_active_normal_next(struct ksz_dlr_info *info,
+	struct dlr_state *state)
+{
+	struct ksz_dlr_gateway_capable *attrib = &info->attrib;
+
+	if ((oneBeaconTimeout || twoBeaconsTimeout) && !linkLoss) {
+#ifdef DBG_DLR_STATE
+		dbg_dlr(info, "active timeout");
+#endif
+		disableSignOnTimer(info);
+		if (DLR_ACTIVE_SUPERVISOR == info->node) {
+			if (twoBeaconsTimeout && info->start)
+				dlr_chk_supervisor(info);
+			state->new_state = DLR_ACTIVE_FAULT_STATE;
+		} else
+			state->new_state = DLR_FAULT_STATE;
+	}
+	if (linkDown) {
+		disableNeighChkTimers(info);
+		state->new_state = DLR_ACTIVE_FAULT_STATE;
+	}
+	if (linkLoss || linkStatus) {
+#ifdef DBG_DLR_STATE
+		dbg_dlr(info, "active loss");
+#endif
+		twoBeaconsRcvd = 0;
+		disableSignOnTimer(info);
+		if (DLR_ACTIVE_SUPERVISOR == info->node)
+			state->new_state = DLR_ACTIVE_FAULT_STATE;
+		else
+			state->new_state = DLR_FAULT_STATE;
+	}
+	if (newSupervisor) {
+		twoBeaconsRcvd = 0;
+#ifdef DBG_DLR_BEACON
+		dbg_bcn += 4;
+#endif
+		disableSignOnTimer(info);
+		state->new_state = DLR_BACKUP_STATE;
+	}
+	if (!newSupervisor && newValue) {
+		state->new_state = DLR_RESTART_STATE;
+
+		/* Ignore own beacons until they are changed. */
+		info->start = 0;
+	}
+	if (DLR_ACTIVE_FAULT_STATE == state->new_state)
+		attrib->fault_cnt++;
+	if (state->new_state) {
+		if (info->skip_beacon)
+			acceptBeacons(info);
+		info->beacon_info[0].timeout =
+		info->beacon_info[1].timeout = 0;
+	}
+
+	if (!state->new_state && DLR_ACTIVE_SUPERVISOR == info->node &&
+	    memcmp(info->attrib.active_super_addr.addr, info->src_addr,
+	    ETH_ALEN)) {
+		printk(KERN_INFO "still active normal\n");
+		state->new_state = DLR_BACKUP_STATE;
+	}
+}  /* dlr_active_normal_next */
+
+static void dlr_backup_init(struct ksz_dlr_info *info, struct dlr_state *state)
+{
+#ifdef DBG_DLR_STATE
+	dbg_dlr(info, "backup");
+#endif
+
+	info->node = DLR_SUPERVISOR;
+	info->attrib.participants_cnt = 0;
+
+	disableSupervisor(info);
+	enableBothPorts(info);
+	state->new_state = DLR_FAULT_STATE;
+
+	/* Delay resetting so that node knows it is becoming backup. */
+	newSupervisor = 0;
+}  /* dlr_backup_init */
+
+static void dlr_backup_next(struct ksz_dlr_info *info, struct dlr_state *state)
+{
+}
+
+static void dlr_prepare_init(struct ksz_dlr_info *info,
+	struct dlr_state *state)
+{
+#ifdef DBG_DLR_STATE
+	dbg_dlr(info, "prepare");
+#endif
+	wait_for_timeout(info->beacon_timeout);
+	info->wait_done = 1;
+}  /* dlr_prepare_init */
+
+static void dlr_prepare_next(struct ksz_dlr_info *info,
+	struct dlr_state *state)
+{
+	if (info->wait_done) {
+		info->wait_done = 0;
+		state->new_state = DLR_ACTIVE_STATE;
+	}
+	if (newSupervisor)
+		state->new_state = DLR_BACKUP_STATE;
+}  /* dlr_prepare_init */
+
+static void dlr_restart_init(struct ksz_dlr_info *info,
+	struct dlr_state *state)
+{
+#ifdef DBG_DLR_STATE
+	dbg_dlr(info,"restart");
+#endif
+#ifdef DBG_DLR_BEACON
+	dbg_bcn += 5;
+#endif
+
+	/* Disable timeout notification. */
+	info->beacon_timeout_ports = 7;
+	info->wait_done = 0;
+	info->node = DLR_SUPERVISOR;
+
+	disableSupervisor(info);
+	disableAnnounce(info);
+	dlr_clear(info);
+
+	/* Reset to ring fault state. */
+	dlr_set_state(info);
+	wait_for_timeout(info->beacon_timeout * 2);
+	info->wait_done = 1;
+}  /* dlr_restart_init */
+
+static void dlr_restart_next(struct ksz_dlr_info *info,
+	struct dlr_state *state)
+{
+	if (info->wait_done) {
+		info->wait_done = 0;
+		updateValues(info);
+		info->beacon_info[0].timer =
+		info->beacon_info[1].timer = 1;
+		state->new_state = DLR_ACTIVE_STATE;
+	}
+	if (oneBeaconRcvd || twoBeaconsRcvd) {
+		dbg_msg("may never get here\n");
+		enableBothPorts(info);
+
+		/* Reset timeout notification. */
+		info->beacon_timeout_ports = 0;
+		state->new_state = DLR_FAULT_STATE;
+	}
+}  /* dlr_restart_next */
+
+static int dlr_proc_state(struct ksz_dlr_info *info, struct dlr_state *state,
+	void (*state_init)(struct ksz_dlr_info *info, struct dlr_state *state),
+	void (*state_next)(struct ksz_dlr_info *info, struct dlr_state *state))
+{
+	if (state->new_state) {
+		state->new_state = 0;
+		state_init(info, state);
+		if (1 == state->change)
+			return 1;
+	}
+	state_next(info, state);
+	return 0;
+}  /* dlr_proc_state */
+
+static void RingSupervisor_state(struct ksz_dlr_info *info)
+{
+	struct dlr_state state_info;
+	struct ksz_dlr_gateway_capable *attrib = &info->attrib;
+
+	state_info.change = 1;
+	state_info.delay_ann = 0;
+	state_info.new_state = 0;
+	dbg_leak = 5;
+	do {
+		state_info.change--;
+		switch (info->state) {
+		case DLR_BEGIN:
+			dlr_clear(info);
+			info->ok_ports = info->member;
+			info->beacon_info[0].timer =
+			info->beacon_info[1].timer = 1;
+			info->LastBcnRcvPort = 0;
+			memset(&attrib->last_active[0], 0,
+				sizeof(struct ksz_dlr_active_node));
+			memset(&attrib->last_active[1], 0,
+				sizeof(struct ksz_dlr_active_node));
+			attrib->participants_cnt = 0;
+			if (info->skip_beacon)
+				acceptBeacons(info);
+			if (DLR_BEACON_NODE >= info->node)
+				state_info.new_state = DLR_IDLE_STATE;
+			else
+				state_info.new_state = DLR_ACTIVE_STATE;
+			state_info.change = 1;
+			break;
+		case DLR_IDLE_STATE:
+			if (dlr_proc_state(info, &state_info,
+			    dlr_idle_init, dlr_idle_next))
+				goto done;
+			break;
+		case DLR_FAULT_STATE:
+			if (dlr_proc_state(info, &state_info,
+			    dlr_fault_init, dlr_fault_next))
+				goto done;
+			break;
+		case DLR_NORMAL_STATE:
+			if (dlr_proc_state(info, &state_info,
+			    dlr_normal_init, dlr_normal_next))
+				goto done;
+			break;
+		case DLR_ACTIVE_STATE:
+			if (dlr_proc_state(info, &state_info,
+			    dlr_active_init, dlr_active_next))
+				goto done;
+			break;
+		case DLR_ACTIVE_FAULT_STATE:
+			if (dlr_proc_state(info, &state_info,
+			    dlr_active_fault_init, dlr_active_fault_next))
+				goto done;
+			break;
+		case DLR_ACTIVE_NORMAL_STATE:
+			if (dlr_proc_state(info, &state_info,
+			    dlr_active_normal_init, dlr_active_normal_next))
+				goto done;
+			break;
+		case DLR_BACKUP_STATE:
+			if (dlr_proc_state(info, &state_info,
+			    dlr_backup_init, dlr_backup_next))
+				goto done;
+			break;
+		case DLR_PREPARE_STATE:
+			if (dlr_proc_state(info, &state_info,
+			    dlr_prepare_init, dlr_prepare_next))
+				goto done;
+			break;
+		case DLR_RESTART_STATE:
+			if (dlr_proc_state(info, &state_info,
+			    dlr_restart_init, dlr_restart_next))
+				goto done;
+			break;
+		}
+		if (info->reset) {
+			info->reset = 0;
+			if (DLR_NORMAL_STATE == info->state)
+				setupDir(info, -1);
+			state_info.new_state = 0;
+			info->state = DLR_BEGIN;
+			state_info.change++;
+		}
+
+		/* There is a new state. */
+		if (state_info.new_state) {
+			info->state = state_info.new_state;
+			state_info.change++;
+		}
+	} while (state_info.change);
+
+done:
+	return;
+}  /* RingSupervisor_state */
+
+static void AnnounceRingNode_state(struct ksz_dlr_info *info)
+{
+	struct dlr_state state_info;
+	struct ksz_dlr_gateway_capable *attrib = &info->attrib;
+
+	state_info.change = 1;
+	state_info.delay_ann = 0;
+	state_info.new_state = 0;
+	do {
+		state_info.change--;
+		switch (info->state) {
+		case DLR_BEGIN:
+			dlr_clear(info);
+			info->ok_ports = info->member;
+			info->beacon_info[0].timer =
+			info->beacon_info[1].timer = 0;
+			announceRcvd = 0;
+			announcedState = 0;
+			memset(&attrib->last_active[0], 0,
+				sizeof(struct ksz_dlr_active_node));
+			memset(&attrib->last_active[1], 0,
+				sizeof(struct ksz_dlr_active_node));
+			attrib->participants_cnt = 0;
+			if (info->skip_beacon)
+				acceptBeacons(info);
+			state_info.new_state = DLR_IDLE_STATE;
+			state_info.change = 1;
+			break;
+		case DLR_IDLE_STATE:
+			if (dlr_proc_state(info, &state_info,
+			    dlr_ann_idle_init, dlr_ann_idle_next))
+				goto done;
+			break;
+		case DLR_FAULT_STATE:
+			if (dlr_proc_state(info, &state_info,
+			    dlr_ann_fault_init, dlr_ann_fault_next))
+				goto done;
+			break;
+		case DLR_NORMAL_STATE:
+			if (dlr_proc_state(info, &state_info,
+			    dlr_ann_normal_init, dlr_ann_normal_next))
+				goto done;
+			break;
+		}
+		if (info->reset) {
+			info->reset = 0;
+			if (DLR_NORMAL_STATE == info->state)
+				setupDir(info, -1);
+			state_info.new_state = 0;
+			info->state = DLR_BEGIN;
+			state_info.change++;
+		}
+
+		/* There is a new state. */
+		if (state_info.new_state) {
+			info->state = state_info.new_state;
+			state_info.change++;
+		}
+	} while (state_info.change);
+
+done:
+	return;
+}  /* AnnounceRingNode_state */
+
+static void *check_dlr_frame(u8 *data)
+{
+	struct vlan_ethhdr *vlan = (struct vlan_ethhdr *) data;
+
+	if (vlan->h_vlan_proto == htons(ETH_P_8021Q)) {
+		if (vlan->h_vlan_encapsulated_proto == htons(DLR_TAG_TYPE))
+			return vlan + 1;
+	}
+
+	/* VLAN tag can be removed by the switch. */
+	if (vlan->h_vlan_proto == htons(DLR_TAG_TYPE)) {
+		struct ethhdr *eth = (struct ethhdr *) data;
+
+		return eth + 1;
+	}
+	return NULL;
+}  /* check_dlr_frame */
+
+static void dlr_proc_rx(struct ksz_dlr_info *info, struct sk_buff *skb,
+	int port)
+{
+	struct ksz_dlr_rx_frame frame;
+	struct ksz_dlr_frame *body;
+	int update = false;
+
+	body = check_dlr_frame(skb->data);
+	frame.vlan = (struct vlan_ethhdr *) skb->data;
+	frame.body = body;
+	switch (body->hdr.frame_type) {
+	case DLR_SIGN_ON:
+		update = handleSignOn(info, &frame, port);
+		break;
+	case DLR_NEIGH_CHK_REQ:
+		update = handleNeighChkReq(info, &frame, port);
+		break;
+	case DLR_NEIGH_CHK_RESP:
+		update = handleNeighChkResp(info, &frame, port);
+		break;
+	case DLR_LOCATE_FAULT:
+		update = handleLocateFault(info, &frame, port);
+		break;
+	case DLR_FLUSH_TABLES:
+		update = handleFlushTables(info, &frame, port);
+		break;
+	case DLR_BEACON:
+		update = handleBeacon(info, &frame, port);
+		break;
+	case DLR_ANNOUNCE:
+		update = handleAnnounce(info, &frame, port);
+		break;
+	case DLR_LINK_STATUS:
+		update = handleLinkStatus(info, &frame, port);
+		break;
+	case DLR_LEARNING_UPDATE:
+		update = handleLearningUpdate(info, &frame, port);
+		break;
+	}
+	if (update)
+		info->state_machine(info);
+}  /* dlr_proc_rx */
+
+static void dlr_stop(struct ksz_dlr_info *info)
+{
+	if (DLR_SUPERVISOR == info->node)
+		disableAnnounceTimeout(info);
+
+	/* Avoid trying to drop the beacons again. */
+	info->beacon_info[0].timeout =
+	info->beacon_info[1].timeout = 1;
+	info->node = DLR_BEACON_NODE;
+
+	disableSupervisor(info);
+	enableBothPorts(info);
+	disableAnnounce(info);
+	disableSignOnTimer(info);
+	info->beacon_info[0].timeout =
+	info->beacon_info[1].timeout = 0;
+}  /* dlr_stop */
+
+static void dlr_delay_proc(struct work_struct *work)
+{
+	struct ksz_dlr_info *dlr =
+		container_of(work, struct ksz_dlr_info, delay_proc);
+	bool empty;
+	bool last;
+	struct sk_buff *skb;
+	bool notify_link_lost = false;
+
+	if (dlr->stop) {
+		dlr->stop = false;
+		dlr_stop(dlr);
+	}
+	if (dlr->timeout_beacon) {
+		dlr->timeout_beacon = false;
+		if (dlr->beacon_timeout_ports) {
+			int p;
+
+			for (p = 0; p < 2; p++) {
+				if (dlr->beacon_timeout_ports & (1 << p))
+					handleBeaconTimeout(dlr, p);
+			}
+			dlr->beacon_timeout_ports = 0;
+		}
+	}
+	if (dlr->link_change) {
+		dlr->link_change = false;
+		if (dlr->notifications & DLR_INFO_LINK_LOST)
+			notify_link_lost = true;
+	}
+
+	last = skb_queue_empty(&dlr->rxq);
+	empty = last;
+	while (!last) {
+		skb = skb_dequeue(&dlr->rxq);
+		last = skb_queue_empty(&dlr->rxq);
+		if (skb) {
+			int port;
+
+			port = skb->cb[0];
+			dlr_proc_rx(dlr, skb, port);
+			dev_kfree_skb_irq(skb);
+		}
+	}
+	dlr->state_machine(dlr);
+	if (dlr->tx_announce) {
+		dlr->tx_announce = false;
+		dlr_tx_announce(dlr);
+	}
+	if (dlr->tx_signon) {
+		dlr->tx_signon = false;
+		dlr_tx_signon(dlr, 0);
+	}
+	if (dlr->tx_advertise) {
+		dlr->tx_advertise = false;
+		dlr_tx_advertise(dlr);
+	}
+	if (dlr->tx_flush_tables) {
+		dlr->tx_flush_tables = false;
+		dlr_tx_flush_tables(dlr);
+	}
+	if (dlr->clr_supervisor) {
+		dlr->clr_supervisor = false;
+		dlr_clr_supervisor(dlr);
+	}
+	if (notify_link_lost)
+		dlr_notify_link_lost(dlr);
+}  /* dlr_delay_proc */
+
+static int dlr_rcv(struct ksz_dlr_info *info, struct sk_buff *skb, int port)
+{
+	struct ksz_dlr_frame *body;
+	int dlr_port = port;
+
+	/* Accept only from port 1 or 2. */
+	if (port == info->ports[0])
+		dlr_port = 0;
+	else if (port == info->ports[1])
+		dlr_port = 1;
+	body = check_dlr_frame(skb->data);
+	if (body) {
+		struct ksz_dlr_rx_frame frame;
+		int accept = true;
+
+		if (dlr_port < 2) {
+			frame.vlan = (struct vlan_ethhdr *) skb->data;
+			frame.body = body;
+			if (DLR_BEACON == body->hdr.frame_type)
+				accept = checkBeacon(info, &frame, dlr_port);
+		} else
+			accept = false;
+		if (accept) {
+
+			/* Use control buffer to save port information. */
+			skb->cb[0] = (char) dlr_port;
+			skb_queue_tail(&info->rxq, skb);
+			schedule_work(&info->delay_proc);
+		} else
+			dev_kfree_skb_irq(skb);
+		return 0;
+	}
+	return 1;
+}  /* dlr_rcv */
+
+static void dlr_link_change(struct ksz_dlr_info *info, int link1, int link2)
+{
+	if (handleLinkChange(info, link1, link2)) {
+		info->link_change = true;
+		schedule_work(&info->delay_proc);
+	}
+}  /* dlr_link_change */
+
+static void dlr_timeout(struct ksz_dlr_info *info, int port)
+{
+	/* Accept only from port 1 or 2. */
+	if (port == info->ports[0])
+		port = 0;
+	else if (port == info->ports[1])
+		port = 1;
+	if (port > 1)
+		return;
+	if (!(info->beacon_timeout_ports & (1 << port))) {
+		info->beacon_timeout_ports |= (1 << port);
+		info->timeout_beacon = true;
+		schedule_work(&info->delay_proc);
+	}
+}  /* dlr_timeout */
+
+static void prep_dlr_addr(struct ksz_dlr_info *dlr, u8 *src)
+{
+	memcpy(dlr->src_addr, src, ETH_ALEN);
+	memcpy(dlr->frame.vlan.h_source, src, ETH_ALEN);
+	memcpy(dlr->update_frame.eth.h_source, src, ETH_ALEN);
+	if (DLR_ACTIVE_SUPERVISOR == dlr->node)
+		memcpy(dlr->attrib.active_super_addr.addr, src, ETH_ALEN);
+}  /* prep_dlr_addr */
+
+static void dlr_change_addr(struct ksz_dlr_info *dlr, u8 *addr)
+{
+	struct ksz_dlr_gateway_capable *attrib = &dlr->attrib;
+	struct ksz_sw *sw = dlr->sw_dev;
+
+	/* Do not do anything if device is not ready. */
+	if (!dlr->dev || !netif_running(dlr->dev))
+		return;
+	if (!memcmp(dlr->src_addr, addr, ETH_ALEN))
+		return;
+
+	sw->ops->cfg_mac(sw, BRIDGE_ADDR_ENTRY, dlr->src_addr, 0,
+		false, false, 0);
+	if (sw->eth_cnt > 1) {
+		sw->ops->cfg_mac(sw, DEV_0_ADDR_ENTRY, dlr->src_addr, 0,
+			false, true, sw->eth_maps[0].vlan);
+		sw->ops->cfg_mac(sw, DEV_1_ADDR_ENTRY, dlr->src_addr, 0,
+			false, true, sw->eth_maps[1].vlan);
+	}
+	prep_dlr_addr(dlr, addr);
+	memcpy(dlr->signon_frame, &dlr->frame, sizeof(struct vlan_ethhdr) +
+		sizeof(struct ksz_dlr_hdr));
+	sw->ops->cfg_mac(sw, BRIDGE_ADDR_ENTRY, dlr->src_addr, sw->HOST_MASK,
+		false, false, 0);
+	if (sw->eth_cnt > 1) {
+		sw->ops->cfg_mac(sw, DEV_0_ADDR_ENTRY, dlr->src_addr,
+			sw->HOST_MASK, false, true, sw->eth_maps[0].vlan);
+		sw->ops->cfg_mac(sw, DEV_1_ADDR_ENTRY, dlr->src_addr,
+			sw->HOST_MASK, false, true, sw->eth_maps[1].vlan);
+	}
+#ifdef CONFIG_HAVE_ACL_HW
+	setup_acl_self(dlr, dlr->ports[0]);
+	setup_acl_self(dlr, dlr->ports[1]);
+#endif
+	if (DLR_SUPERVISOR == dlr->node &&
+	    dlr->attrib.super_cfg.prec == attrib->active_super_prec) {
+		int cmp = memcmp(dlr->src_addr, attrib->active_super_addr.addr,
+			ETH_ALEN);
+
+		if (cmp > 0) {
+			dlr->new_val = 1;
+			schedule_work(&dlr->delay_proc);
+		}
+	} else if (DLR_ACTIVE_SUPERVISOR == dlr->node) {
+		if (dlr->skip_beacon)
+			acceptBeacons(dlr);
+		dlr->new_val = 1;
+		schedule_work(&dlr->delay_proc);
+	}
+}  /* dlr_change_addr */
+
+static void prep_dlr_mcast(struct net_device *dev)
+{
+	char addr[MAX_ADDR_LEN];
+
+	memcpy(addr, MAC_ADDR_BEACON, ETH_ALEN);
+	dev_mc_add(dev, addr);
+	memcpy(addr, MAC_ADDR_SIGNON, ETH_ALEN);
+	dev_mc_add(dev, addr);
+	memcpy(addr, MAC_ADDR_ANNOUNCE, ETH_ALEN);
+	dev_mc_add(dev, addr);
+	memcpy(addr, MAC_ADDR_ADVERTISE, ETH_ALEN);
+	dev_mc_add(dev, addr);
+	memcpy(addr, MAC_ADDR_LEARNING_UPDATE, ETH_ALEN);
+	dev_mc_add(dev, addr);
+}  /* prep_dlr_mcast */
+
+static void prep_dlr(struct ksz_dlr_info *dlr, struct net_device *dev, u8 *src)
+{
+	dlr->dev = dev;
+	prep_dlr_addr(dlr, src);
+	dlr->frame.vlan.h_vlan_TCI = htons((7 << VLAN_PRIO_SHIFT) | dlr->vid);
+	memcpy(dlr->signon_frame, &dlr->frame, sizeof(struct vlan_ethhdr) +
+		sizeof(struct ksz_dlr_hdr));
+
+	dlr->active_port = 0;
+	dlr->seqid = 0;
+#if 1
+	dlr->seqid = 0xffffffe0;
+#endif
+	dlr->seqid_beacon = 0;
+	dlr->state = DLR_BEGIN;
+	do {
+		struct ksz_sw *sw = dlr->sw_dev;
+
+		sw->ops->cfg_mac(sw, BRIDGE_ADDR_ENTRY, dlr->src_addr,
+			sw->HOST_MASK, false, false, 0);
+		if (sw->eth_cnt > 1) {
+			sw->ops->cfg_mac(sw, DEV_0_ADDR_ENTRY, dlr->src_addr,
+				sw->HOST_MASK, false, true,
+				sw->eth_maps[0].vlan);
+			sw->ops->cfg_mac(sw, DEV_1_ADDR_ENTRY, dlr->src_addr,
+				sw->HOST_MASK, false, true,
+				sw->eth_maps[1].vlan);
+		}
+		dlr->p1_down = sw->port_info[dlr->ports[0]].state !=
+			media_connected;
+		dlr->p2_down = sw->port_info[dlr->ports[1]].state !=
+			media_connected;
+		if (DLR_ACTIVE_SUPERVISOR == dlr->node) {
+			if (dlr->p1_down)
+				dlr_set_addr(&dlr->attrib.last_active[0],
+					dlr->ip_addr, dlr->src_addr);
+			if (dlr->p2_down)
+				dlr_set_addr(&dlr->attrib.last_active[1],
+					dlr->ip_addr, dlr->src_addr);
+		}
+#if defined(REG_PORT_LUE_CTRL) && defined(PORT_SRC_ADDR_FILTER)
+		sw->ops->acquire(sw);
+		port_cfg(sw, dlr->ports[0], REG_PORT_LUE_CTRL,
+			PORT_SRC_ADDR_FILTER, false);
+		port_cfg(sw, dlr->ports[1], REG_PORT_LUE_CTRL,
+			PORT_SRC_ADDR_FILTER, false);
+		sw->ops->release(sw);
+#endif
+	} while (0);
+#ifdef CONFIG_HAVE_ACL_HW
+	setup_acl_self(dlr, dlr->ports[0]);
+	setup_acl_self(dlr, dlr->ports[1]);
+#endif
+	setup_vlan_table(dlr, dlr->vid, true);
+	schedule_work(&dlr->delay_proc);
+}  /* prep_dlr */
+
+static void announce_monitor(struct work_struct *work)
+{
+	struct delayed_work *dwork = to_delayed_work(work);
+	struct ksz_dlr_info *dlr =
+		container_of(dwork, struct ksz_dlr_info, announce_tx);
+
+	dlr->clr_supervisor = true;
+
+	/* No longer being active supervisor after this scheduling. */
+	if (DLR_ACTIVE_SUPERVISOR != dlr->node) {
+#if 0
+if (dlr->ann_delay)
+dbg_msg(" ! %s %d %d\n", __func__, dlr->ann_delay, dlr->signon_delay);
+#endif
+		dlr->ann_delay = 0;
+		dlr->signon_delay = 0;
+		goto done;
+	}
+
+	if (dlr->ann_delay) {
+		u32 microsec;
+		unsigned long diff = jiffies - dlr->ann_jiffies;
+
+		microsec = dlr->beacon_timeout * 2;
+		if (diff >= 2)
+			diff = diff * 1000 * (1000 / HZ);
+		else
+			diff = 0;
+		if (diff < microsec) {
+			microsec -= diff;
+			wait_for_timeout(microsec);
+		}
+		dlr->ann_delay = 0;
+		dlr->ann_jiffies = 0;
+	}
+
+	/* No longer being active supervisor after the wait. */
+	if (DLR_ACTIVE_SUPERVISOR != dlr->node) {
+		dlr->signon_delay = 0;
+		goto done;
+	}
+
+	dlr->tx_announce = true;
+	if (dlr->signon_delay) {
+#ifdef DBG_DLR_ANN_SIGNON
+		dbg_msg("delay signon: %lx\n", jiffies);
+#endif
+		dlr->tx_signon = true;
+		dlr->signon_delay = 0;
+	}
+	schedule_delayed_work(&dlr->announce_tx, msecs_to_jiffies(1000));
+
+done:
+	schedule_work(&dlr->delay_proc);
+}  /* announce_monitor */
+
+static void announce_timeout_monitor(struct timer_list *t)
+{
+	struct ksz_timer_info *timer = from_timer(timer, t, timer);
+	struct ksz_dlr_info *info = timer->dev;
+
+dbg_msg("ann timeout\n");
+	info->ann_timeout = 1;
+	if (DLR_ANNOUNCE_NODE == info->node ||
+	    DLR_SUPERVISOR == info->node)
+		schedule_work(&info->delay_proc);
+	ksz_update_timer(&info->announce_timeout_timer_info);
+}  /* announce_timeout_monitor */
+
+static void neigh_chk_proc(struct work_struct *work)
+{
+	int p;
+	int checking = 0;
+	int lost = false;
+	struct ksz_dlr_info *dlr =
+		container_of(work, struct ksz_dlr_info, neigh_chk_proc);
+
+	/* Neighbor_Check_Request timeout. */
+	for (p = 0; p < 2; p++) {
+
+		/* This port has sent a Neighbor_Check_Request frame. */
+		if (dlr->port_chk[p]) {
+			++checking;
+			++dlr->port_chk[p];
+			if (dlr->port_chk[p] > 3) {
+				if (p)
+					dlr->p2_lost = 1;
+				else
+					dlr->p1_lost = 1;
+				lost = true;
+				--checking;
+			} else
+				dlr_tx_chk_req(dlr, p);
+		}
+	}
+	if (lost) {
+		if (DLR_ACTIVE_SUPERVISOR == dlr->node) {
+			struct ksz_dlr_node_info *node;
+
+			if (dlr->p1_lost)
+				dlr->ok_ports &= ~(1 << dlr->ports[0]);
+			if (dlr->p2_lost)
+				dlr->ok_ports &= ~(1 << dlr->ports[1]);
+			node = find_dlr_node(dlr, dlr->src_addr);
+			if (node) {
+				if (dlr->p1_down)
+					node->p1_down = 1;
+				if (dlr->p2_down)
+					node->p2_down = 1;
+				if (dlr->p1_lost)
+					node->p1_lost = 1;
+				if (dlr->p2_lost)
+					node->p2_lost = 1;
+			}
+			if (dlr->p1_lost)
+				dlr_set_addr(&dlr->attrib.last_active[0],
+					dlr->ip_addr, dlr->src_addr);
+			if (dlr->p2_lost)
+				dlr_set_addr(&dlr->attrib.last_active[1],
+					dlr->ip_addr, dlr->src_addr);
+#if 1
+			if (dlr->notifications & DLR_INFO_CFG_CHANGE)
+				dlr_notify_cfg_change(dlr, 2);
+#endif
+			dlr_notify_link_lost(dlr);
+		} else
+			dlr_tx_status(dlr, 1);
+	}
+}  /* neigh_chk_proc */
+
+static void neigh_chk_monitor(struct timer_list *t)
+{
+	struct ksz_timer_info *timer = from_timer(timer, t, timer);
+	struct ksz_dlr_info *dlr = timer->dev;
+
+	schedule_work(&dlr->neigh_chk_proc);
+	ksz_update_timer(&dlr->neigh_chk_timer_info);
+	dlr->neigh_chk = !!dlr->neigh_chk_timer_info.max;
+}  /* neigh_chk_monitor */
+
+static void signon_monitor(struct timer_list *t)
+{
+	struct ksz_timer_info *timer = from_timer(timer, t, timer);
+	struct ksz_dlr_info *info = timer->dev;
+
+#ifdef DBG_DLR_ANN_SIGNON
+	dbg_msg("%s\n", __func__);
+#endif
+	if (DLR_ACTIVE_SUPERVISOR == info->node) {
+		info->tx_signon = true;
+		schedule_work(&info->delay_proc);
+	} else
+		info->signon_timer_info.max = 1;
+	ksz_update_timer(&info->signon_timer_info);
+}  /* signon_monitor */
+
+static void dlr_reset_attrib(struct ksz_dlr_info *dlr)
+{
+#ifdef CONFIG_HAVE_DLR_HW
+	struct ksz_sw *sw = dlr->sw_dev;
+#endif
+
+	memset(&dlr->attrib, 0, sizeof(struct ksz_dlr_gateway_capable));
+	switch (dlr->node) {
+	case DLR_ANNOUNCE_NODE:
+		dlr->attrib.cap = DLR_CAP_ANNOUNCE_BASED;
+		dlr->attrib.super_status = DLR_STAT_RING_NODE;
+		break;
+	case DLR_BEACON_NODE:
+		dlr->attrib.cap = DLR_CAP_BEACON_BASED;
+
+#ifdef CONFIG_HAVE_DLR_HW
+		if (sw->features & REDUNDANCY_SUPPORT)
+			dlr->attrib.cap |= DLR_CAP_SUPERVISOR_CAPABLE;
+#endif
+		dlr->attrib.super_status = DLR_STAT_RING_NODE;
+		dlr->attrib.super_cfg.beacon_interval = 400;
+		dlr->attrib.super_cfg.beacon_timeout = 1960;
+		dlr->attrib.super_cfg.beacon_interval *= 1;
+		dlr->attrib.super_cfg.beacon_timeout *= 1;
+		break;
+	default:
+
+#ifdef CONFIG_HAVE_DLR_HW
+		if (!(sw->features & REDUNDANCY_SUPPORT))
+			break;
+#endif
+		dlr->attrib.cap = DLR_CAP_BEACON_BASED;
+		dlr->attrib.cap |= DLR_CAP_SUPERVISOR_CAPABLE;
+		dlr->attrib.super_status = DLR_STAT_ACTIVE_SUPERVISOR;
+		dlr->attrib.super_cfg.enable = true;
+		dlr->attrib.super_cfg.beacon_interval = 400;
+		dlr->attrib.super_cfg.beacon_timeout = 1960;
+		dlr->attrib.super_cfg.beacon_interval *= 1;
+		dlr->attrib.super_cfg.beacon_timeout *= 1;
+	}
+}  /* dlr_reset_attrib */
+
+static void setup_dlr(struct ksz_dlr_info *dlr)
+{
+	struct ksz_dlr_frame *frame = &dlr->frame.body;
+
+	dlr_reset_attrib(dlr);
+
+	dlr->ip_addr = 0;
+	dlr->vid = 0;
+	dlr->precedence = 0;
+	dlr->beacon_interval = 400;
+	dlr->beacon_timeout = 1960;
+	dlr->beacon_interval *= 1;
+	dlr->beacon_timeout *= 1;
+
+	dlr->frame.vlan.h_vlan_proto = htons(ETH_P_8021Q);
+	dlr->frame.vlan.h_vlan_encapsulated_proto = htons(DLR_TAG_TYPE);
+	frame->hdr.ring_subtype = DLR_RING_SUBTYPE;
+	frame->hdr.ring_protocol_version = 1;
+	frame->hdr.ip_addr = dlr->ip_addr;
+	dlr->tx_frame = (u8 *) &dlr->frame;
+
+	memcpy(dlr->update_frame.eth.h_dest, MAC_ADDR_LEARNING_UPDATE,
+		ETH_ALEN);
+	dlr->update_frame.eth.h_proto = htons(DLR_TAG_TYPE);
+	dlr->update_frame.hdr.ring_subtype = DLR_RING_SUBTYPE;
+	dlr->update_frame.hdr.ring_protocol_version = 1;
+	dlr->update_frame.hdr.frame_type = DLR_LEARNING_UPDATE;
+	dlr->update_frame.hdr.src_port = DLR_PORT_NONE;
+	memset(dlr->update_frame.reserved, 0, 34);
+
+	dlr->p1_set = dlr->p2_set = 1;
+
+	dlr->port = 0;
+	dlr->tx_port = dlr->port;
+	dlr->rx_port = (dlr->tx_port + 1) & 1;
+
+	ksz_init_timer(&dlr->announce_timeout_timer_info, 1200 * HZ / 1000,
+		announce_timeout_monitor, dlr);
+	ksz_init_timer(&dlr->neigh_chk_timer_info, 100 * HZ / 1000,
+		neigh_chk_monitor, dlr);
+	ksz_init_timer(&dlr->signon_timer_info, 60000 * HZ / 1000,
+		signon_monitor, dlr);
+
+}  /* setup_dlr */
+
+static int dlr_get_attrib(struct ksz_dlr_info *dlr, int subcmd, int size,
+	int *req_size, size_t *len, u8 *data, int *output)
+{
+	struct ksz_dlr_node_info *cur;
+	struct ksz_dlr_active_node *node;
+	int i;
+	struct ksz_dlr_gateway_capable *attr = &dlr->attrib;
+	u8 svc = (u8)(subcmd >> CIP_SVC_S);
+	u8 class = (u8)(subcmd >> CIP_CLASS_S);
+	u8 code = (u8)(subcmd >> CIP_ATTR_S);
+	u8 id = (u8) subcmd;
+
+	*len = 0;
+	*output = 0;
+	if (class != CLASS_DLR_OBJECT)
+		return DEV_IOC_INVALID_CMD;
+	if (svc != SVC_GET_ATTRIBUTES_ALL &&
+	    svc != SVC_GET_ATTRIBUTE_SINGLE &&
+	    svc != SVC_GET_MEMBER)
+		return DEV_IOC_INVALID_CMD;
+	if (CIP_INSTANCE_ATTRIBUTES == code) {
+		if (SVC_GET_ATTRIBUTES_ALL == svc) {
+			*len = sizeof(struct ksz_dlr_super_capable_2);
+			if (attr->cap & DLR_CAP_GATEWAY_CAPABLE)
+				*len = sizeof(struct ksz_dlr_gateway_capable);
+			memcpy(data, attr, *len);
+		} else if (SVC_GET_ATTRIBUTE_SINGLE == svc) {
+			union dlr_data *attrib = (union dlr_data *) data;
+
+			switch (id) {
+			case DLR_GET_NETWORK_TOPOLOGY:
+				*len = 1;
+				attrib->byte = attr->net_topology;
+				break;
+			case DLR_GET_NETWORK_STATUS:
+				*len = 1;
+				attrib->byte = attr->net_status;
+#if 1
+if (attrib->byte == DLR_NET_RING_FAULT && getting_last_active &&
+((!attr->last_active[0].addr[4] && !attr->last_active[0].addr[5]) ||
+(!attr->last_active[1].addr[4] && !attr->last_active[1].addr[5])))
+attrib->byte = DLR_NET_NORMAL;
+#endif
+				break;
+			case DLR_GET_RING_SUPERVISOR_STATUS:
+				*len = 1;
+				attrib->byte = attr->super_status;
+				break;
+			case DLR_SET_RING_SUPERVISOR_CONFIG:
+				*len = sizeof(struct ksz_dlr_super_cfg);
+				memcpy(&attrib->super_cfg, &attr->super_cfg,
+					*len);
+				break;
+			case DLR_SET_RING_FAULT_COUNT:
+				*len = 2;
+				attrib->word = attr->fault_cnt;
+				break;
+			case DLR_GET_LAST_ACTIVE_NODE_ON_PORT_1:
+dbg_msg("+");
+				*len = sizeof(struct ksz_dlr_active_node);
+				memcpy(&attrib->active,	&attr->last_active[0],
+					*len);
+				break;
+			case DLR_GET_LAST_ACTIVE_NODE_ON_PORT_2:
+				*len = sizeof(struct ksz_dlr_active_node);
+				memcpy(&attrib->active,	&attr->last_active[1],
+					*len);
+				break;
+			case DLR_GET_RING_PARTICIPANTS_COUNT:
+				*len = 2;
+				attrib->word = attr->participants_cnt;
+				break;
+			case DLR_GET_ACTIVE_SUPERVISOR_ADDRESS:
+				*len = sizeof(struct ksz_dlr_active_node);
+				memcpy(&attrib->active,
+					&attr->active_super_addr, *len);
+				break;
+			case DLR_GET_ACTIVE_SUPERVISOR_PRECEDENCE:
+				*len = 1;
+				attrib->byte = attr->active_super_prec;
+				break;
+			case DLR_GET_CAPABILITY_FLAGS:
+				*len = 4;
+				attrib->dword = attr->cap;
+				break;
+			case DLR_SET_REDUNDANT_GATEWAY_CONFIG:
+				if (!(attr->cap & DLR_CAP_GATEWAY_CAPABLE))
+					break;
+				*len = sizeof(struct ksz_dlr_gateway_cfg);
+				memcpy(&attrib->gateway_cfg, &attr->gateway_cfg,
+					*len);
+				break;
+			case DLR_GET_REDUNDANT_GATEWAY_STATUS:
+				if (!(attr->cap & DLR_CAP_GATEWAY_CAPABLE))
+					break;
+				*len = 1;
+				attrib->byte = attr->gateway_status;
+				break;
+			case DLR_GET_ACTIVE_GATEWAY_ADDRESS:
+				if (!(attr->cap & DLR_CAP_GATEWAY_CAPABLE))
+					break;
+				*len = sizeof(struct ksz_dlr_active_node);
+				memcpy(&attrib->active,
+					&attr->active_gateway_addr, *len);
+				break;
+			case DLR_GET_ACTIVE_GATEWAY_PRECEDENCE:
+				if (!(attr->cap & DLR_CAP_GATEWAY_CAPABLE))
+					break;
+				*len = 1;
+				attrib->byte = attr->active_gateway_prec;
+				break;
+			case DLR_GET_RING_PARTICIPANTS_LIST:
+				*len = sizeof(struct ksz_dlr_active_node);
+				if (attr->super_status !=
+				    DLR_STAT_ACTIVE_SUPERVISOR) {
+					*output = STATUS_OBJECT_STATE_CONFLICT;
+					break;
+				}
+				*len *= attr->participants_cnt;
+				if (size < *len) {
+					*output = STATUS_REPLY_DATA_TOO_LARGE;
+					break;
+				}
+				node = (struct ksz_dlr_active_node *) data;
+				for (i = 0; i < attr->participants_cnt; i++) {
+					cur = &dlr->nodes[i];
+					node->ip_addr = cur->signon.ip_addr;
+					memcpy(&node->addr, cur->signon.addr,
+						ETH_ALEN);
+					node++;
+				}
+				break;
+			}
+		} else if (SVC_GET_MEMBER == svc) {
+			switch (id) {
+			case DLR_GET_RING_PARTICIPANTS_LIST:
+				*len = sizeof(struct ksz_dlr_active_node);
+				if (attr->super_status !=
+				    DLR_STAT_ACTIVE_SUPERVISOR) {
+					*output = STATUS_OBJECT_STATE_CONFLICT;
+					break;
+				}
+				node = (struct ksz_dlr_active_node *) data;
+				i = 0;
+				cur = &dlr->nodes[i];
+				node->ip_addr = cur->signon.ip_addr;
+				memcpy(&node->addr, cur->signon.addr,
+					ETH_ALEN);
+				break;
+			}
+		}
+	} else if (CIP_CLASS_ATTRIBUTES == code) {
+		union dlr_data *attrib = (union dlr_data *) data;
+
+		if (DLR_GET_REVISION == id) {
+			*len = 2;
+			attrib->word = DLR_REVISION;
+		}
+	}
+	if (!*len)
+		return DEV_IOC_INVALID_CMD;
+	if (size < *len) {
+		*req_size = *len + SIZEOF_ksz_request;
+		return DEV_IOC_INVALID_LEN;
+	}
+	return DEV_IOC_OK;
+}  /* dlr_get_attrib */
+
+static int dlr_change_cfg(struct ksz_dlr_info *dlr,
+	struct ksz_dlr_super_cfg *cfg)
+{
+	struct ksz_dlr_gateway_capable *attrib = &dlr->attrib;
+	struct ksz_dlr_super_cfg *super = &attrib->super_cfg;
+
+	if (cfg->beacon_interval < 100 || cfg->beacon_interval > 100000 ||
+	    cfg->beacon_timeout < 1000 || cfg->beacon_timeout > 2000000)
+		return STATUS_INVALID_ATTRIB_VALUE;
+	if (cfg->enable && !(dlr->attrib.cap & DLR_CAP_SUPERVISOR_CAPABLE))
+		return STATUS_INVALID_ATTRIB_VALUE;
+	if (cfg->enable != super->enable) {
+		if (super->enable) {
+			dlr->stop = true;
+		} else
+			dlr->node = DLR_SUPERVISOR;
+		super->enable = cfg->enable;
+		if (super->enable) {
+			super->prec = cfg->prec;
+			super->beacon_interval = cfg->beacon_interval;
+			super->beacon_timeout = cfg->beacon_timeout;
+			super->vid = cfg->vid;
+			dlr->precedence = super->prec;
+			dlr->beacon_interval = super->beacon_interval;
+			dlr->beacon_timeout = super->beacon_timeout;
+			dlr->vid = super->vid;
+		}
+		dlr->reset = true;
+		schedule_work(&dlr->delay_proc);
+	} else if (super->enable) {
+		if (cfg->prec != super->prec) {
+			super->prec = cfg->prec;
+			dlr->new_val = 1;
+		}
+		if (DLR_ACTIVE_SUPERVISOR == dlr->node || dlr->new_val) {
+			if (cfg->beacon_interval != super->beacon_interval) {
+				super->beacon_interval = cfg->beacon_interval;
+				dlr->new_val = 1;
+			}
+			if (cfg->beacon_timeout != super->beacon_timeout) {
+				super->beacon_timeout = cfg->beacon_timeout;
+				dlr->new_val = 1;
+			}
+			if (cfg->vid != super->vid) {
+				super->vid = cfg->vid;
+				dlr->new_val = 1;
+			}
+		}
+		if (dlr->new_val)
+			schedule_work(&dlr->delay_proc);
+	}
+	return 0;
+}  /* dlr_change_cfg */
+
+static int dlr_set_attrib(struct ksz_dlr_info *dlr, int subcmd, int size,
+	int *req_size, u8 *data, int *output)
+{
+	struct ksz_dlr_gateway_capable *attr = &dlr->attrib;
+	int len = 0;
+	u8 svc = (u8)(subcmd >> CIP_SVC_S);
+	u8 class = (u8)(subcmd >> CIP_CLASS_S);
+	u8 code = (u8)(subcmd >> CIP_ATTR_S);
+	u8 id = (u8) subcmd;
+	union dlr_data *attrib = (union dlr_data *) data;
+
+	*output = 0;
+	if (class != CLASS_DLR_OBJECT)
+		return DEV_IOC_INVALID_CMD;
+	switch (svc) {
+	case SVC_SET_ATTRIBUTE_SINGLE:
+		if (CIP_INSTANCE_ATTRIBUTES != code)
+			return DEV_IOC_INVALID_CMD;
+		switch (id) {
+		case DLR_SET_RING_SUPERVISOR_CONFIG:
+			len = sizeof(struct ksz_dlr_super_cfg);
+			break;
+		case DLR_SET_RING_FAULT_COUNT:
+			len = 2;
+			break;
+		case DLR_SET_REDUNDANT_GATEWAY_CONFIG:
+			if (!(attr->cap & DLR_CAP_GATEWAY_CAPABLE))
+				break;
+			len = sizeof(struct ksz_dlr_gateway_cfg);
+			break;
+		case DLR_SET_IP_ADDRESS:
+			len = sizeof(struct ksz_dlr_active_node);
+			break;
+		}
+		if (!len)
+			return DEV_IOC_INVALID_CMD;
+		if (size < len) {
+			*req_size = len + SIZEOF_ksz_request;
+			return DEV_IOC_INVALID_LEN;
+		}
+		switch (id) {
+		case DLR_SET_RING_SUPERVISOR_CONFIG:
+			*output = dlr_change_cfg(dlr, &attrib->super_cfg);
+			break;
+		case DLR_SET_RING_FAULT_COUNT:
+			if (attrib->word) {
+				*output = STATUS_INVALID_ATTRIB_VALUE;
+				break;
+			}
+			attr->fault_cnt = attrib->word;
+			break;
+		case DLR_SET_REDUNDANT_GATEWAY_CONFIG:
+			if (memcmp(&attr->gateway_cfg, &attrib->gateway_cfg,
+			    len)) {
+				memcpy(&attr->gateway_cfg, &attrib->gateway_cfg,
+					len);
+			}
+			break;
+		case DLR_SET_IP_ADDRESS:
+		{
+			struct ksz_dlr_frame *frame = &dlr->frame.body;
+
+			dlr->ip_addr = attrib->active.ip_addr;
+			frame->hdr.ip_addr = dlr->ip_addr;
+			break;
+		}
+		}
+		break;
+	case SVC_DLR_VERIFY_FAULT_LOCATION:
+	{
+		struct ksz_dlr_node_info *node;
+		int i;
+
+		if (attr->super_status !=
+		    DLR_STAT_ACTIVE_SUPERVISOR ||
+		    dlr->state != DLR_ACTIVE_FAULT_STATE) {
+			*output = STATUS_OBJECT_STATE_CONFLICT;
+			memset(&attr->last_active[0], 0,
+				sizeof(struct ksz_dlr_active_node));
+			memset(&attr->last_active[1], 0,
+				sizeof(struct ksz_dlr_active_node));
+			break;
+		}
+		for (i = 1; i < attr->participants_cnt; i++) {
+			node = &dlr->nodes[i];
+			node->p1_down = 0;
+			node->p1_lost = 0;
+			node->p2_down = 0;
+			node->p2_lost = 0;
+		}
+		dlr_tx_locate_fault(dlr);
+		break;
+	}
+	case SVC_DLR_CLEAR_RAPID_FAULTS:
+		break;
+	case SVC_DLR_RESTART_SIGN_ON:
+		if (attr->super_status !=
+		    DLR_STAT_ACTIVE_SUPERVISOR ||
+		    dlr->state != DLR_ACTIVE_NORMAL_STATE) {
+			*output = STATUS_OBJECT_STATE_CONFLICT;
+			break;
+		}
+
+		/* SignOn timer not started. */
+		if (!dlr->signon_start) {
+			dlr->signon_start = 1;
+			startSignOn(dlr, false);
+		}
+		break;
+	case SVC_DLR_CLEAR_GATEWAY_PARTIAL_FAULT:
+		break;
+	default:
+		return DEV_IOC_INVALID_CMD;
+	}
+	return DEV_IOC_OK;
+}  /* dlr_set_attrib */
+
+static int dlr_dev_req(struct ksz_dlr_info *dlr, char *arg, void *info)
+{
+	struct ksz_request *req = (struct ksz_request *) arg;
+	int len;
+	int maincmd;
+	int req_size;
+	int subcmd;
+	int output;
+	size_t param_size;
+	u8 data[PARAM_DATA_SIZE];
+	struct ksz_resp_msg *msg = (struct ksz_resp_msg *) data;
+	int err = 0;
+	int result = 0;
+	struct ksz_sw *sw = dlr->sw_dev;
+
+	get_user_data(&req_size, &req->size, info);
+	get_user_data(&maincmd, &req->cmd, info);
+	get_user_data(&subcmd, &req->subcmd, info);
+	get_user_data(&output, &req->output, info);
+	len = req_size - SIZEOF_ksz_request;
+
+	maincmd &= 0xffff;
+	switch (maincmd) {
+	case DEV_CMD_INFO:
+		switch (subcmd) {
+		case DEV_INFO_INIT:
+			req_size = SIZEOF_ksz_request + 4;
+			if (len >= 4) {
+				data[0] = 'M';
+				data[1] = 'i';
+				data[2] = 'c';
+				data[3] = 'r';
+				data[4] = 0;
+				data[5] = dlr->member;
+				err = write_user_data(data, req->param.data,
+					6, info);
+				if (err)
+					goto dev_ioctl_done;
+			} else
+				result = DEV_IOC_INVALID_LEN;
+			break;
+		case DEV_INFO_EXIT:
+
+		/* fall through */
+		case DEV_INFO_QUIT:
+
+			/* Not called through char device. */
+			if (!info)
+				break;
+			file_dev_clear_notify(sw->dev_list[0], info,
+					      DEV_MOD_DLR,
+					      &dlr->notifications);
+			msg->module = DEV_MOD_DLR;
+			msg->cmd = DEV_INFO_QUIT;
+			msg->resp.data[0] = 0;
+			file_dev_setup_msg(info, msg, 8, NULL, NULL);
+			break;
+		case DEV_INFO_NOTIFY:
+			if (len >= 4) {
+				struct file_dev_info *dev_info = info;
+				uint *notify = (uint *) data;
+
+				_chk_ioctl_size(len, 4, 0, &req_size, &result,
+					&req->param, data, info);
+				dev_info->notifications[DEV_MOD_DLR] =
+					*notify;
+				dlr->notifications |= *notify;
+			}
+			break;
+		default:
+			result = DEV_IOC_INVALID_CMD;
+			break;
+		}
+		break;
+	case DEV_CMD_PUT:
+		if (_chk_ioctl_size(len, len, 0, &req_size, &result,
+		    &req->param, data, info))
+			goto dev_ioctl_resp;
+		result = dlr_set_attrib(dlr, subcmd, len, &req_size, data,
+			&output);
+		if (result)
+			goto dev_ioctl_resp;
+		put_user_data(&output, &req->output, info);
+		break;
+	case DEV_CMD_GET:
+		result = dlr_get_attrib(dlr, subcmd, len, &req_size,
+			&param_size, data, &output);
+		if (result)
+			goto dev_ioctl_resp;
+		err = write_user_data(data, req->param.data, param_size, info);
+		if (err)
+			goto dev_ioctl_done;
+		req_size = param_size + SIZEOF_ksz_request;
+		put_user_data(&output, &req->output, info);
+		break;
+	default:
+		result = DEV_IOC_INVALID_CMD;
+		break;
+	}
+
+dev_ioctl_resp:
+	put_user_data(&req_size, &req->size, info);
+	put_user_data(&result, &req->result, info);
+
+	/* Return ERESTARTSYS so that the system call is called again. */
+	if (result < 0)
+		err = result;
+
+dev_ioctl_done:
+	return err;
+}  /* dlr_dev_req */
+
+#define _dlr_dev_req(a, b)		dlr_dev_req(a, b, NULL)
+
+static void set_dlr_req(void *ptr, int cmd,
+	u8 svc, u8 class, u8 code, u8 id, void *dlr, size_t dlr_size)
+{
+	struct ksz_request *req = ptr;
+
+	req->size = SIZEOF_ksz_request;
+	req->size += dlr_size;
+	req->cmd = cmd;
+	req->subcmd = (svc << CIP_SVC_S) | (class << CIP_CLASS_S) |
+		(code << CIP_ATTR_S) | id;
+	req->output = 0;
+	if (dlr)
+		memcpy(&req->param, dlr, dlr_size);
+}  /* set_dlr_req */
+
+static int get_dlr_revision(void *fd,
+	u16 *rev)
+{
+	struct ksz_request_actual req;
+	int rc;
+	struct ksz_dlr_info *dlr = fd;
+
+	set_dlr_req(&req, DEV_CMD_GET, SVC_GET_ATTRIBUTE_SINGLE,
+		CLASS_DLR_OBJECT, CIP_CLASS_ATTRIBUTES, DLR_GET_REVISION,
+		NULL, sizeof(u16));
+	rc = _dlr_dev_req(dlr, (char *) &req);
+	if (!rc)
+		rc = req.result;
+	if (!rc) {
+		union dlr_data *data = (union dlr_data *) &req.param;
+
+		*rev = data->word;
+	}
+	return rc;
+}  /* get_dlr_revision */
+
+static int get_dlr_all(void *fd,
+	struct ksz_dlr_gateway_capable *capable)
+{
+	struct ksz_request_actual *req;
+	int rc;
+	struct ksz_dlr_info *dlr = fd;
+
+	req = kzalloc(sizeof(struct ksz_request_actual), GFP_KERNEL);
+	if (!req)
+		return -1;
+	set_dlr_req(req, DEV_CMD_GET, SVC_GET_ATTRIBUTES_ALL,
+		CLASS_DLR_OBJECT, CIP_INSTANCE_ATTRIBUTES, 0,
+		NULL, sizeof(struct ksz_dlr_gateway_capable));
+	rc = _dlr_dev_req(dlr, (char *) req);
+	if (!rc)
+		rc = req->result;
+	if (!rc) {
+		memcpy(capable, &req->param, req->size - SIZEOF_ksz_request);
+	}
+	kfree(req);
+	return rc;
+}  /* get_dlr_all */
+
+static int get_dlr_topology(void *fd,
+	u8 *topology)
+{
+	struct ksz_request_actual req;
+	int rc;
+	struct ksz_dlr_info *dlr = fd;
+
+	set_dlr_req(&req, DEV_CMD_GET, SVC_GET_ATTRIBUTE_SINGLE,
+		CLASS_DLR_OBJECT, CIP_INSTANCE_ATTRIBUTES,
+		DLR_GET_NETWORK_TOPOLOGY,
+		NULL, sizeof(u8));
+	rc = _dlr_dev_req(dlr, (char *) &req);
+	if (!rc)
+		rc = req.result;
+	if (!rc) {
+		union dlr_data *data = (union dlr_data *) &req.param;
+
+		*topology = data->byte;
+	}
+	return rc;
+}  /* get_dlr_topology */
+
+static int get_dlr_network(void *fd,
+	u8 *network)
+{
+	struct ksz_request_actual req;
+	int rc;
+	struct ksz_dlr_info *dlr = fd;
+
+	set_dlr_req(&req, DEV_CMD_GET, SVC_GET_ATTRIBUTE_SINGLE,
+		CLASS_DLR_OBJECT, CIP_INSTANCE_ATTRIBUTES,
+		DLR_GET_NETWORK_STATUS,
+		NULL, sizeof(u8));
+	rc = _dlr_dev_req(dlr, (char *) &req);
+	if (!rc)
+		rc = req.result;
+	if (!rc) {
+		union dlr_data *data = (union dlr_data *) &req.param;
+
+		*network = data->byte;
+	}
+	return rc;
+}  /* get_dlr_network */
+
+static int get_dlr_super_status(void *fd,
+	u8 *status)
+{
+	struct ksz_request_actual req;
+	int rc;
+	struct ksz_dlr_info *dlr = fd;
+
+	set_dlr_req(&req, DEV_CMD_GET, SVC_GET_ATTRIBUTE_SINGLE,
+		CLASS_DLR_OBJECT, CIP_INSTANCE_ATTRIBUTES,
+		DLR_GET_RING_SUPERVISOR_STATUS,
+		NULL, sizeof(u8));
+	rc = _dlr_dev_req(dlr, (char *) &req);
+	if (!rc)
+		rc = req.result;
+	if (!rc) {
+		union dlr_data *data = (union dlr_data *) &req.param;
+
+		*status = data->byte;
+	}
+	return rc;
+}  /* get_dlr_super_status */
+
+static int get_dlr_super_cfg(void *fd,
+	struct ksz_dlr_super_cfg *cfg)
+{
+	struct ksz_request_actual req;
+	int rc;
+	struct ksz_dlr_info *dlr = fd;
+
+	set_dlr_req(&req, DEV_CMD_GET, SVC_GET_ATTRIBUTE_SINGLE,
+		CLASS_DLR_OBJECT, CIP_INSTANCE_ATTRIBUTES,
+		DLR_SET_RING_SUPERVISOR_CONFIG,
+		NULL, sizeof(struct ksz_dlr_super_cfg));
+	rc = _dlr_dev_req(dlr, (char *) &req);
+	if (!rc)
+		rc = req.result;
+	if (!rc) {
+		memcpy(cfg, &req.param, req.size - SIZEOF_ksz_request);
+	}
+	return rc;
+}  /* get_dlr_super_cfg */
+
+static int set_dlr_super_cfg(void *fd,
+	struct ksz_dlr_super_cfg *cfg, u8 *err)
+{
+	struct ksz_request_actual req;
+	int rc;
+	struct ksz_dlr_info *dlr = fd;
+
+	set_dlr_req(&req, DEV_CMD_PUT, SVC_SET_ATTRIBUTE_SINGLE,
+		CLASS_DLR_OBJECT, CIP_INSTANCE_ATTRIBUTES,
+		DLR_SET_RING_SUPERVISOR_CONFIG,
+		cfg, sizeof(struct ksz_dlr_super_cfg));
+	rc = _dlr_dev_req(dlr, (char *) &req);
+	if (!rc)
+		rc = req.result;
+	if (!rc) {
+		*err = (u8) req.output;
+	}
+	return rc;
+}  /* set_dlr_super_cfg */
+
+static int get_dlr_ring_fault_cnt(void *fd,
+	u16 *cnt)
+{
+	struct ksz_request_actual req;
+	int rc;
+	struct ksz_dlr_info *dlr = fd;
+
+	set_dlr_req(&req, DEV_CMD_GET, SVC_GET_ATTRIBUTE_SINGLE,
+		CLASS_DLR_OBJECT, CIP_INSTANCE_ATTRIBUTES,
+		DLR_SET_RING_FAULT_COUNT,
+		NULL, sizeof(u16));
+	rc = _dlr_dev_req(dlr, (char *) &req);
+	if (!rc)
+		rc = req.result;
+	if (!rc) {
+		union dlr_data *data = (union dlr_data *) &req.param;
+
+		*cnt = data->word;
+	}
+	return rc;
+}  /* get_dlr_ring_fault_cnt */
+
+static int set_dlr_ring_fault_cnt(void *fd,
+	u16 cnt, u8 *err)
+{
+	struct ksz_request_actual req;
+	int rc;
+	struct ksz_dlr_info *dlr = fd;
+
+	set_dlr_req(&req, DEV_CMD_PUT, SVC_SET_ATTRIBUTE_SINGLE,
+		CLASS_DLR_OBJECT, CIP_INSTANCE_ATTRIBUTES,
+		DLR_SET_RING_FAULT_COUNT,
+		&cnt, sizeof(u16));
+	rc = _dlr_dev_req(dlr, (char *) &req);
+	if (!rc)
+		rc = req.result;
+	if (!rc) {
+		*err = (u8) req.output;
+	}
+	return rc;
+}  /* set_dlr_ring_fault_cnt */
+
+static int get_dlr_active_node(void *fd,
+	u8 port, struct ksz_dlr_active_node *node)
+{
+	struct ksz_request_actual req;
+	int rc;
+	struct ksz_dlr_info *dlr = fd;
+	u8 id;
+
+	if (1 == port)
+		id = DLR_GET_LAST_ACTIVE_NODE_ON_PORT_2;
+	else
+		id = DLR_GET_LAST_ACTIVE_NODE_ON_PORT_1;
+	set_dlr_req(&req, DEV_CMD_GET, SVC_GET_ATTRIBUTE_SINGLE,
+		CLASS_DLR_OBJECT, CIP_INSTANCE_ATTRIBUTES,
+		id,
+		NULL, sizeof(struct ksz_dlr_active_node));
+	rc = _dlr_dev_req(dlr, (char *) &req);
+	if (!rc)
+		rc = req.result;
+	if (!rc) {
+		memcpy(node, &req.param, req.size - SIZEOF_ksz_request);
+	}
+	return rc;
+}  /* get_dlr_active_node */
+
+static int get_dlr_ring_part_cnt(void *fd,
+	u16 *cnt)
+{
+	struct ksz_request_actual req;
+	int rc;
+	struct ksz_dlr_info *dlr = fd;
+
+	set_dlr_req(&req, DEV_CMD_GET, SVC_GET_ATTRIBUTE_SINGLE,
+		CLASS_DLR_OBJECT, CIP_INSTANCE_ATTRIBUTES,
+		DLR_GET_RING_PARTICIPANTS_COUNT,
+		NULL, sizeof(u16));
+	rc = _dlr_dev_req(dlr, (char *) &req);
+	if (!rc)
+		rc = req.result;
+	if (!rc) {
+		union dlr_data *data = (union dlr_data *) &req.param;
+
+		*cnt = data->word;
+	}
+	return rc;
+}  /* get_dlr_ring_part_cnt */
+
+static int get_dlr_ring_part_list(void *fd,
+	struct ksz_dlr_active_node *node, u16 *size, u8 *err)
+{
+	struct ksz_request_actual req;
+	int rc;
+	struct ksz_dlr_info *dlr = fd;
+
+	set_dlr_req(&req, DEV_CMD_GET, SVC_GET_ATTRIBUTE_SINGLE,
+		CLASS_DLR_OBJECT, CIP_INSTANCE_ATTRIBUTES,
+		DLR_GET_RING_PARTICIPANTS_LIST,
+		NULL, *size);
+	rc = _dlr_dev_req(dlr, (char *) &req);
+	if (!rc)
+		rc = req.result;
+	if (!rc) {
+		*err = (u8) req.output;
+		*size = req.size - SIZEOF_ksz_request;
+		memcpy(node, &req.param, *size);
+	}
+	return rc;
+}  /* get_dlr_ring_part_list */
+
+static int get_dlr_active_super_addr(void *fd,
+	struct ksz_dlr_active_node *node)
+{
+	struct ksz_request_actual req;
+	int rc;
+	struct ksz_dlr_info *dlr = fd;
+
+	set_dlr_req(&req, DEV_CMD_GET, SVC_GET_ATTRIBUTE_SINGLE,
+		CLASS_DLR_OBJECT, CIP_INSTANCE_ATTRIBUTES,
+		DLR_GET_ACTIVE_SUPERVISOR_ADDRESS,
+		NULL, sizeof(struct ksz_dlr_active_node));
+	rc = _dlr_dev_req(dlr, (char *) &req);
+	if (!rc)
+		rc = req.result;
+	if (!rc) {
+		memcpy(node, &req.param, req.size - SIZEOF_ksz_request);
+	}
+	return rc;
+}  /* get_dlr_active_super_addr */
+
+static int get_dlr_active_super_prec(void *fd,
+	u8 *prec)
+{
+	struct ksz_request_actual req;
+	int rc;
+	struct ksz_dlr_info *dlr = fd;
+
+	set_dlr_req(&req, DEV_CMD_GET, SVC_GET_ATTRIBUTE_SINGLE,
+		CLASS_DLR_OBJECT, CIP_INSTANCE_ATTRIBUTES,
+		DLR_GET_ACTIVE_SUPERVISOR_PRECEDENCE,
+		NULL, sizeof(u8));
+	rc = _dlr_dev_req(dlr, (char *) &req);
+	if (!rc)
+		rc = req.result;
+	if (!rc) {
+		union dlr_data *data = (union dlr_data *) &req.param;
+
+		*prec = data->byte;
+	}
+	return rc;
+}  /* get_dlr_active_super_prec */
+
+static int get_dlr_cap(void *fd,
+	u32 *flags)
+{
+	struct ksz_request_actual req;
+	int rc;
+	struct ksz_dlr_info *dlr = fd;
+
+	set_dlr_req(&req, DEV_CMD_GET, SVC_GET_ATTRIBUTE_SINGLE,
+		CLASS_DLR_OBJECT, CIP_INSTANCE_ATTRIBUTES,
+		DLR_GET_CAPABILITY_FLAGS,
+		NULL, sizeof(u32));
+	rc = _dlr_dev_req(dlr, (char *) &req);
+	if (!rc)
+		rc = req.result;
+	if (!rc) {
+		union dlr_data *data = (union dlr_data *) &req.param;
+
+		*flags = data->dword;
+	}
+	return rc;
+}  /* get_dlr_cap */
+
+static int set_dlr_verify_fault(void *fd,
+	u8 *err)
+{
+	struct ksz_request_actual req;
+	int rc;
+	struct ksz_dlr_info *dlr = fd;
+
+	set_dlr_req(&req, DEV_CMD_PUT, SVC_DLR_VERIFY_FAULT_LOCATION,
+		CLASS_DLR_OBJECT, 0, 0,
+		NULL, 0);
+	rc = _dlr_dev_req(dlr, (char *) &req);
+	if (!rc)
+		rc = req.result;
+	if (!rc) {
+		*err = (u8) req.output;
+	}
+	return rc;
+}  /* set_dlr_verify_fault */
+
+static int set_dlr_clear_rapid_fault(void *fd,
+	u8 *err)
+{
+	struct ksz_request_actual req;
+	int rc;
+	struct ksz_dlr_info *dlr = fd;
+
+	set_dlr_req(&req, DEV_CMD_PUT, SVC_DLR_CLEAR_RAPID_FAULTS,
+		CLASS_DLR_OBJECT, 0, 0,
+		NULL, 0);
+	rc = _dlr_dev_req(dlr, (char *) &req);
+	if (!rc)
+		rc = req.result;
+	if (!rc) {
+		*err = (u8) req.output;
+	}
+	return rc;
+}  /* set_dlr_clear_rapid_fault */
+
+static int set_dlr_restart_sign_on(void *fd,
+	u8 *err)
+{
+	struct ksz_request_actual req;
+	int rc;
+	struct ksz_dlr_info *dlr = fd;
+
+	set_dlr_req(&req, DEV_CMD_PUT, SVC_DLR_RESTART_SIGN_ON,
+		CLASS_DLR_OBJECT, 0, 0,
+		NULL, 0);
+	rc = _dlr_dev_req(dlr, (char *) &req);
+	if (!rc)
+		rc = req.result;
+	if (!rc) {
+		*err = (u8) req.output;
+	}
+	return rc;
+}  /* set_dlr_restart_sign_on */
+
+static int set_dlr_clear_gateway_fault(void *fd,
+	u8 *err)
+{
+	struct ksz_request_actual req;
+	int rc;
+	struct ksz_dlr_info *dlr = fd;
+
+	set_dlr_req(&req, DEV_CMD_PUT, SVC_DLR_CLEAR_GATEWAY_PARTIAL_FAULT,
+		CLASS_DLR_OBJECT, 0, 0,
+		NULL, 0);
+	rc = _dlr_dev_req(dlr, (char *) &req);
+	if (!rc)
+		rc = req.result;
+	if (!rc) {
+		*err = (u8) req.output;
+	}
+	return rc;
+}  /* set_dlr_clear_gateway_fault */
+
+static void test_monitor(struct timer_list *t)
+{
+	struct ksz_timer_info *timer = from_timer(timer, t, timer);
+	struct ksz_dlr_info *info = timer->dev;
+
+	if (DLR_SUPERVISOR <= info->node) {
+		u8 prec;
+
+		if (1 == info->precedence)
+			prec = 9;
+		else if (9 == info->precedence)
+			prec = 1;
+		else
+			prec = 0;
+		if (prec) {
+			int rc;
+			u8 err;
+			struct ksz_dlr_super_cfg super;
+
+			memcpy(&super, &info->attrib.super_cfg, sizeof(super));
+			super.prec = prec;
+			rc = set_dlr_super_cfg(info, &super, &err);
+		}
+	}
+	ksz_update_timer(&info->test_timer_info);
+}  /* test_monitor */
+
+enum {
+	PROC_GET_DLR_INFO,
+	PROC_SET_DLR_NODE,
+	PROC_SET_DLR_PRECEDENCE,
+	PROC_SET_DLR_INTERVAL,
+	PROC_SET_DLR_TIMEOUT,
+	PROC_SET_DLR_VID,
+	PROC_SET_DLR_CFG,
+	PROC_SET_DLR_STATE,
+	PROC_SET_DLR_PORT,
+	PROC_SET_DLR_TEST,
+	PROC_SET_DLR_NEIGH_CHK_REQ,
+	PROC_SET_DLR_NEIGH_CHK_RESP,
+	PROC_SET_DLR_LINK_BREAK,
+	PROC_SET_DLR_LEARNING_UPDATE,
+	PROC_SET_DLR_LOCATE_FAULT,
+	PROC_SET_DLR_SIGN_ON,
+	PROC_SET_DLR_CLEAR_FAULT,
+	PROC_SET_DLR_CLEAR_GATEWAY_FAULT,
+
+	PROC_GET_DLR_ALL,
+	PROC_GET_DLR_REVISION,
+	PROC_GET_DLR_NETWORK_TOPOLOGY,
+	PROC_GET_DLR_NETWORK_STATUS,
+	PROC_GET_DLR_RING_SUPERVISOR_STATUS,
+	PROC_SET_DLR_RING_SUPERVISOR_CONFIG,
+	PROC_SET_DLR_RING_FAULT_COUNT,
+	PROC_GET_DLR_LAST_ACTIVE_NODE_1,
+	PROC_GET_DLR_LAST_ACTIVE_NODE_2,
+	PROC_GET_DLR_RING_PARTICIPANTS_COUNT,
+	PROC_GET_DLR_RING_PARTICIPANTS_LIST,
+	PROC_GET_DLR_ACTIVE_SUPERVISOR_ADDRESS,
+	PROC_GET_DLR_ACTIVE_SUPERVISOR_PRECEDENCE,
+	PROC_GET_DLR_CAPABILITIES,
+	PROC_SET_DLR_PORT_1,
+	PROC_SET_DLR_PORT_2,
+};
+
+static ssize_t display_faults(struct ksz_dlr_info *dlr, char *buf, ssize_t len)
+{
+	int i;
+	struct ksz_dlr_node_info *node;
+	struct ksz_dlr_node *signon;
+
+	for (i = 0; i < dlr->attrib.participants_cnt; i++) {
+		node = &dlr->nodes[i];
+		signon = &node->signon;
+		if (!node->p1_down && !node->p2_down &&
+		    !node->p1_lost && !node->p2_lost)
+			continue;
+		len += sprintf(buf + len,
+			"%02x:%02x:%02x:%02x:%02x:%02x  %3u.%3u.%3u.%3u  ",
+			signon->addr[0], signon->addr[1], signon->addr[2],
+			signon->addr[3], signon->addr[4], signon->addr[5],
+			(u8) signon->ip_addr,
+			(u8)(signon->ip_addr >> 8),
+			(u8)(signon->ip_addr >> 24),
+			(u8)(signon->ip_addr >> 16));
+		len += sprintf(buf + len, "%u:%u %u:%u\n",
+			node->p1_down, node->p2_down,
+			node->p1_lost, node->p2_lost);
+		if (len >= 2048 - 80) {
+			len += sprintf(buf + len, "...\n");
+			break;
+		}
+	}
+	return len;
+}  /* display_faults */
+
+static ssize_t display_nodes(struct ksz_dlr_info *dlr, char *buf, ssize_t len)
+{
+	int i;
+	struct ksz_dlr_node_info *node;
+	struct ksz_dlr_node *signon;
+
+	for (i = 0; i < dlr->attrib.participants_cnt; i++) {
+		node = &dlr->nodes[i];
+		signon = &node->signon;
+		len += sprintf(buf + len,
+			"%02x:%02x:%02x:%02x:%02x:%02x  %3u.%3u.%3u.%3u  ",
+			signon->addr[0], signon->addr[1], signon->addr[2],
+			signon->addr[3], signon->addr[4], signon->addr[5],
+			(u8) signon->ip_addr,
+			(u8)(signon->ip_addr >> 8),
+			(u8)(signon->ip_addr >> 16),
+			(u8)(signon->ip_addr >> 24));
+		len += sprintf(buf + len, "%u:%u %u:%u\n",
+			node->p1_down, node->p2_down,
+			node->p1_lost, node->p2_lost);
+		if (len >= 2048 - 80) {
+			len += sprintf(buf + len, "...\n");
+			break;
+		}
+	}
+	return len;
+}  /* display_nodes */
+
+static ssize_t show_dlr_err(ssize_t len, char *buf, u8 err)
+{
+	if (buf)
+		len += sprintf(buf + len, "!0x%02x\n", err);
+	else
+		printk(KERN_INFO "!0x%02x\n", err);
+	return len;
+}  /* show_dlr_err */
+
+static ssize_t show_dlr_attrib(ssize_t len, char *buf, u8 err, char *format,
+	u32 data)
+{
+	if (err)
+		len = show_dlr_err(len, buf, err);
+	else
+		len += sprintf(buf + len, format, data);
+	return len;
+}  /* show_dlr_attrib */
+
+static ssize_t show_dlr_attrib_super(ssize_t len, char *buf, u8 err,
+	struct ksz_dlr_super_cfg *cfg)
+{
+	if (err)
+		len = show_dlr_err(len, buf, err);
+	else
+		len += sprintf(buf + len, "E:%u  P:%u  I:%u  T:%u  V:%u\n",
+			cfg->enable,
+			cfg->prec,
+			cfg->beacon_interval,
+			cfg->beacon_timeout,
+			cfg->vid);
+	return len;
+}  /* show_dlr_attrib_super */
+
+static ssize_t show_dlr_attrib_node(ssize_t len, char *buf, u8 err,
+	struct ksz_dlr_active_node *node)
+{
+	if (err)
+		len = show_dlr_err(len, buf, err);
+	else
+		len += sprintf(buf + len,
+			"%02x:%02x:%02x:%02x:%02x:%02x  %3u.%3u.%3u.%3u\n",
+			node->addr[0],
+			node->addr[1],
+			node->addr[2],
+			node->addr[3],
+			node->addr[4],
+			node->addr[5],
+			(u8) node->ip_addr,
+			(u8)(node->ip_addr >> 8),
+			(u8)(node->ip_addr >> 16),
+			(u8)(node->ip_addr >> 24));
+	return len;
+}  /* show_dlr_attrib_node */
+
+static ssize_t show_dlr_attrib_all(ssize_t len, char *buf, u8 err,
+	struct ksz_dlr_gateway_capable *capable)
+{
+	if (err)
+		len = show_dlr_err(len, buf, err);
+	else {
+		len = show_dlr_attrib(len, buf, 0,
+			"top:%u  ", capable->net_topology);
+		len = show_dlr_attrib(len, buf, 0,
+			"net:%u  ", capable->net_status);
+		len = show_dlr_attrib(len, buf, 0,
+			"sup:%u  ", capable->super_status);
+		len = show_dlr_attrib(len, buf, 0,
+			"fault:%u\n", capable->fault_cnt);
+		len = show_dlr_attrib_super(len, buf, 0,
+			&capable->super_cfg);
+		len = show_dlr_attrib_node(len, buf, 0,
+			&capable->last_active[0]);
+		len = show_dlr_attrib_node(len, buf, 0,
+			&capable->last_active[1]);
+		len = show_dlr_attrib_node(len, buf, 0,
+			&capable->active_super_addr);
+		len = show_dlr_attrib(len, buf, 0,
+			"cnt:%u  ", capable->participants_cnt);
+		len = show_dlr_attrib(len, buf, 0,
+			"prec:%u  ", capable->active_super_prec);
+		len = show_dlr_attrib(len, buf, 0,
+			"cap:%08x\n", capable->cap);
+	}
+	return len;
+}  /* show_dlr_attrib_all */
+
+static ssize_t sysfs_dlr_read(struct ksz_dlr_info *dlr, int proc_num,
+	ssize_t len, char *buf)
+{
+	struct ksz_dlr_active_node node;
+	struct ksz_dlr_super_cfg super;
+	int rc;
+	u32 dword;
+	u16 word;
+	u8 byte;
+	u8 err = 0;
+
+	switch (proc_num) {
+	case PROC_GET_DLR_INFO:
+		break;
+	case PROC_SET_DLR_NODE:
+		len += sprintf(buf + len, "%u\n", dlr->node);
+		break;
+	case PROC_SET_DLR_PRECEDENCE:
+		len += sprintf(buf + len, "%u\n", dlr->precedence);
+		break;
+	case PROC_SET_DLR_INTERVAL:
+		len += sprintf(buf + len, "%u\n", dlr->beacon_interval);
+		break;
+	case PROC_SET_DLR_TIMEOUT:
+		len += sprintf(buf + len, "%u\n", dlr->beacon_timeout);
+		break;
+	case PROC_SET_DLR_VID:
+		len += sprintf(buf + len, "0x%03x\n", dlr->vid);
+		break;
+	case PROC_SET_DLR_STATE:
+		len += sprintf(buf + len, "%u\n", dlr->ring_state);
+		break;
+	case PROC_SET_DLR_PORT:
+		len += sprintf(buf + len, "%u\n", dlr->port);
+		break;
+	case PROC_SET_DLR_TEST:
+		len += sprintf(buf + len, "%u\n",
+			(dlr->overrides & DLR_TEST) ? 1 : 0);
+		break;
+	case PROC_SET_DLR_LOCATE_FAULT:
+		if (DLR_ACTIVE_SUPERVISOR == dlr->node)
+			len = display_faults(dlr, buf, len);
+		break;
+	case PROC_SET_DLR_SIGN_ON:
+		if (DLR_ACTIVE_SUPERVISOR == dlr->node)
+			len = display_nodes(dlr, buf, len);
+		else
+			len += sprintf(buf + len, "%u\n", dlr->ignore_req);
+		break;
+	case PROC_SET_DLR_NEIGH_CHK_REQ:
+		break;
+	case PROC_SET_DLR_NEIGH_CHK_RESP:
+		len += sprintf(buf + len, "%u\n", dlr->ignore_req);
+		break;
+	case PROC_SET_DLR_LINK_BREAK:
+		len += sprintf(buf + len, "0x%x\n", dlr->link_break);
+		break;
+	case PROC_GET_DLR_REVISION:
+		rc = get_dlr_revision(dlr, &word);
+		if (!rc)
+			len = show_dlr_attrib(len, buf, err, "%u\n", word);
+		break;
+	case PROC_GET_DLR_ALL:
+	{
+		struct ksz_dlr_gateway_capable *capable;
+
+		capable = kzalloc(sizeof(struct ksz_dlr_gateway_capable),
+			GFP_KERNEL);
+		if (!capable)
+			break;
+		rc = get_dlr_all(dlr, capable);
+		if (!rc)
+			len = show_dlr_attrib_all(len, buf, err, capable);
+		kfree(capable);
+		break;
+	}
+	case PROC_GET_DLR_NETWORK_TOPOLOGY:
+		rc = get_dlr_topology(dlr, &byte);
+		if (!rc)
+			len = show_dlr_attrib(len, buf, err, "%u\n", byte);
+		break;
+	case PROC_GET_DLR_NETWORK_STATUS:
+		rc = get_dlr_network(dlr, &byte);
+		if (!rc)
+			len = show_dlr_attrib(len, buf, err, "%u\n", byte);
+		break;
+	case PROC_GET_DLR_RING_SUPERVISOR_STATUS:
+		rc = get_dlr_super_status(dlr, &byte);
+		if (!rc)
+			len = show_dlr_attrib(len, buf, err, "%u\n", byte);
+		break;
+	case PROC_SET_DLR_RING_SUPERVISOR_CONFIG:
+		rc = get_dlr_super_cfg(dlr, &super);
+		if (!rc)
+			len = show_dlr_attrib_super(len, buf, err, &super);
+		break;
+	case PROC_SET_DLR_RING_FAULT_COUNT:
+		rc = get_dlr_ring_fault_cnt(dlr, &word);
+		if (!rc)
+			len = show_dlr_attrib(len, buf, err, "%u\n", word);
+		break;
+	case PROC_GET_DLR_LAST_ACTIVE_NODE_1:
+		rc = get_dlr_active_node(dlr, 0, &node);
+		if (!rc)
+			len = show_dlr_attrib_node(len, buf, err, &node);
+		break;
+	case PROC_GET_DLR_LAST_ACTIVE_NODE_2:
+		rc = get_dlr_active_node(dlr, 1, &node);
+		if (!rc)
+			len = show_dlr_attrib_node(len, buf, err, &node);
+		break;
+	case PROC_GET_DLR_RING_PARTICIPANTS_COUNT:
+		rc = get_dlr_ring_part_cnt(dlr, &word);
+		if (!rc)
+			len = show_dlr_attrib(len, buf, err, "%u\n", word);
+		break;
+	case PROC_GET_DLR_RING_PARTICIPANTS_LIST:
+	{
+		struct ksz_dlr_active_node *nodes;
+
+		rc = get_dlr_ring_part_cnt(dlr, &word);
+		if (rc || !word)
+			break;
+
+		word *= sizeof(struct ksz_dlr_active_node);
+		nodes = kzalloc(word, GFP_KERNEL);
+		if (!nodes)
+			break;
+
+		rc = get_dlr_ring_part_list(dlr, nodes, &word, &err);
+		if (!rc) {
+			int i;
+
+			if (err) {
+				len = show_dlr_err(len, buf, err);
+				break;
+			}
+			word /= sizeof(struct ksz_dlr_active_node);
+			for (i = 0; i < word; i++)
+				len = show_dlr_attrib_node(len, buf, 0,
+					&nodes[i]);
+		}
+		kfree(nodes);
+		break;
+	}
+	case PROC_GET_DLR_ACTIVE_SUPERVISOR_ADDRESS:
+		rc = get_dlr_active_super_addr(dlr, &node);
+		if (!rc)
+			len = show_dlr_attrib_node(len, buf, err, &node);
+		break;
+	case PROC_GET_DLR_ACTIVE_SUPERVISOR_PRECEDENCE:
+		rc = get_dlr_active_super_prec(dlr, &byte);
+		if (!rc)
+			len = show_dlr_attrib(len, buf, err, "%u\n", byte);
+		break;
+	case PROC_GET_DLR_CAPABILITIES:
+		rc = get_dlr_cap(dlr, &dword);
+		if (!rc)
+			len = show_dlr_attrib(len, buf, err, "%08x\n", dword);
+		break;
+	case PROC_SET_DLR_PORT_1:
+		len += sprintf(buf + len, "%u\n", dlr->ports[0]);
+		break;
+	case PROC_SET_DLR_PORT_2:
+		len += sprintf(buf + len, "%u\n", dlr->ports[1]);
+		break;
+	}
+	return len;
+}  /* sysfs_dlr_read */
+
+static void sysfs_dlr_write(struct ksz_dlr_info *info, int proc_num, int num,
+	const char *buf)
+{
+	int rc;
+	u8 err;
+	struct ksz_dlr_super_cfg super;
+	struct ksz_dlr_gateway_capable *attrib = &info->attrib;
+	struct ksz_sw *sw = info->sw_dev;
+	int node = DLR_ACTIVE_SUPERVISOR;
+	u32 member = 0;
+
+	if (sw->overrides & HAVE_MORE_THAN_2_PORTS)
+		member = info->member | sw->HOST_MASK;
+	switch (proc_num) {
+	case PROC_SET_DLR_NODE:
+#ifdef CONFIG_HAVE_DLR_HW
+		/* Cannot be a supervisor without beacon generation. */
+		if (!(sw->features & REDUNDANCY_SUPPORT))
+			node = DLR_BEACON_NODE;
+#endif
+		if (!(DLR_ANNOUNCE_NODE <= num && num <= node))
+			break;
+		if (num == info->node)
+			break;
+		disableAnnounceTimeout(info);
+		if (DLR_SUPERVISOR <= info->node) {
+			int prev_node = info->node;
+
+			info->node = DLR_BEACON_NODE;
+			disableSupervisor(info);
+			if (DLR_ACTIVE_SUPERVISOR == prev_node) {
+				enableBothPorts(info);
+				disableAnnounce(info);
+				disableSignOnTimer(info);
+			}
+		} else if (DLR_NORMAL_STATE == info->state) {
+			setupDir(info, -1);
+		}
+		if (info->skip_beacon)
+			acceptBeacons(info);
+		if (DLR_ANNOUNCE_NODE == info->node)
+			sw->ops->cfg_mac(sw, DLR_BEACON_ENTRY,
+				MAC_ADDR_BEACON, member, false, false, 0);
+		if (num == info->node)
+			break;
+		info->node = (u8) num;
+		dlr_reset_attrib(info);
+		info->attrib.super_cfg.beacon_interval = info->beacon_interval;
+		info->attrib.super_cfg.beacon_timeout = info->beacon_timeout;
+#ifdef DBG_DLR_BEACON
+		dbg_bcn = 4;
+#endif
+		do {
+			struct ksz_sw *sw = info->sw_dev;
+
+			if (DLR_ANNOUNCE_NODE == info->node) {
+#ifdef CONFIG_HAVE_ACL_HW
+				disable_acl_beacon_timeout(info,
+					info->ports[0]);
+				disable_acl_beacon_timeout(info,
+					info->ports[1]);
+#endif
+				sw->ops->cfg_mac(sw, DLR_BEACON_ENTRY,
+					MAC_ADDR_BEACON, info->member,
+					false, false, 0);
+				info->state_machine = AnnounceRingNode_state;
+			} else
+				info->state_machine = RingSupervisor_state;
+			info->reset = true;
+			schedule_work(&info->delay_proc);
+		} while (0);
+		break;
+	case PROC_SET_DLR_PRECEDENCE:
+		/* Value can only be set through supervisor. */
+		if (info->node < DLR_SUPERVISOR)
+			break;
+		memcpy(&super, &info->attrib.super_cfg, sizeof(super));
+		super.prec = (u8) num;
+		rc = set_dlr_super_cfg(info, &super, &err);
+		if (!rc && err)
+			show_dlr_err(0, NULL, err);
+		break;
+	case PROC_SET_DLR_INTERVAL:
+		/* Value can only be set through active supervisor. */
+		if (DLR_ACTIVE_SUPERVISOR != info->node)
+			break;
+		if (num < 200) {
+			printk("too small\n");
+			break;
+		} else if (num * 4 > info->beacon_timeout) {
+			printk("too large\n");
+			break;
+		}
+		memcpy(&super, &info->attrib.super_cfg, sizeof(super));
+		super.beacon_interval = num;
+		rc = set_dlr_super_cfg(info, &super, &err);
+		if (!rc && err)
+			show_dlr_err(0, NULL, err);
+		break;
+	case PROC_SET_DLR_TIMEOUT:
+		/* Value can only be set through active supervisor. */
+		if (DLR_ACTIVE_SUPERVISOR != info->node)
+			break;
+		if (num < info->beacon_interval * 4) {
+			printk("too small\n");
+			break;
+
+#ifdef CONFIG_HAVE_ACL_HW
+		} else if (num >= ACL_CNT_M * 1000) {
+			printk("too large\n");
+			break;
+#endif
+		}
+		memcpy(&super, &info->attrib.super_cfg, sizeof(super));
+		super.beacon_timeout = num;
+		rc = set_dlr_super_cfg(info, &super, &err);
+		if (!rc && err)
+			show_dlr_err(0, NULL, err);
+		break;
+	case PROC_SET_DLR_VID:
+		/* Value can only be set through active supervisor. */
+		if (DLR_ACTIVE_SUPERVISOR != info->node)
+			break;
+		if (num >= 0x400)
+			break;
+		memcpy(&super, &info->attrib.super_cfg, sizeof(super));
+		super.vid = (u16) num;
+		rc = set_dlr_super_cfg(info, &super, &err);
+		if (!rc && err)
+			show_dlr_err(0, NULL, err);
+		break;
+	case PROC_SET_DLR_CFG:
+	{
+		int enable;
+		int prec;
+		int interval;
+		int timeout;
+		int vid;
+
+		sscanf(buf, "%u %u %u %u %u",
+			&enable, &prec, &interval, &timeout, &vid);
+		if (interval < 200) {
+			printk("too small\n");
+			break;
+		} else if (interval * 4 > timeout) {
+			printk("too large\n");
+			break;
+		}
+		if (timeout < interval * 4) {
+			printk("too small\n");
+			break;
+
+#ifdef CONFIG_HAVE_ACL_HW
+		} else if (timeout >= ACL_CNT_M * 1000) {
+			printk("too large\n");
+			break;
+#endif
+		}
+		super.enable = (u8) enable;
+		super.prec = (u8) prec;
+		super.beacon_interval = interval;
+		super.beacon_timeout = timeout;
+		super.vid = (u16) vid;
+		rc = set_dlr_super_cfg(info, &super, &err);
+		if (!rc && err)
+			show_dlr_err(0, NULL, err);
+		break;
+	}
+	case PROC_SET_DLR_STATE:
+		if (1 <= num && num <= 2) {
+			info->ring_state = (u8) num;
+			dlr_set_state(info);
+		}
+		break;
+	case PROC_SET_DLR_PORT:
+		if (0 <= num && num <= 1) {
+			info->port = (u8) num;
+			info->tx_port = (u8) num;
+			info->rx_port = (info->tx_port + 1) & 1;
+		}
+		break;
+	case PROC_SET_DLR_TEST:
+		if (num & 1)
+			info->overrides |= DLR_TEST;
+		else
+			info->overrides &= ~DLR_TEST;
+		if (num & 2)
+			info->overrides |= DLR_TEST_SEQ;
+		else
+			info->overrides &= ~DLR_TEST_SEQ;
+		if (num & 0x80)
+			ksz_start_timer(&info->test_timer_info,
+				info->test_timer_info.period);
+		else
+			ksz_stop_timer(&info->test_timer_info);
+		break;
+	case PROC_SET_DLR_LEARNING_UPDATE:
+		dlr_tx_learning_update(info);
+		break;
+	case PROC_SET_DLR_LOCATE_FAULT:
+		rc = set_dlr_verify_fault(info, &err);
+		if (!rc && err)
+			show_dlr_err(0, NULL, err);
+		break;
+	case PROC_SET_DLR_SIGN_ON:
+		if (DLR_ACTIVE_SUPERVISOR == info->node) {
+			info->signon_space = num;
+		} else {
+			info->ignore_req = (u8) num;
+			info->req_cnt[0] = info->req_cnt[1] = 0;
+			if (info->overrides & DLR_TEST)
+				break;
+		}
+		rc = set_dlr_restart_sign_on(info, &err);
+		if (!rc && err)
+			show_dlr_err(0, NULL, err);
+		break;
+	case PROC_SET_DLR_NEIGH_CHK_REQ:
+		if (num & 3) {
+			if (!info->neigh_chk) {
+				info->neigh_chk = 1;
+				ksz_start_timer(&info->neigh_chk_timer_info,
+					info->neigh_chk_timer_info.period);
+			}
+			info->neigh_chk_timer_info.max = 3;
+		}
+		if (num & 1) {
+			info->port_chk[0] = 1;
+			dlr_tx_chk_req(info, 0);
+		}
+		if (num & 2) {
+			info->port_chk[1] = 1;
+			dlr_tx_chk_req(info, 1);
+		}
+		break;
+	case PROC_SET_DLR_NEIGH_CHK_RESP:
+		info->ignore_req = (u8) num;
+		info->req_cnt[0] = info->req_cnt[1] = 0;
+		break;
+	case PROC_SET_DLR_LINK_BREAK:
+		info->link_break = num;
+		if (info->link_break & 1)
+			info->p1_down = 1;
+		else
+			info->p1_down = 0;
+		if (info->link_break & 2)
+			info->p2_down = 1;
+		else
+			info->p2_down = 0;
+		if (info->link_break & 4)
+			info->p1_lost = 1;
+		else
+			info->p1_lost = 0;
+		if (info->link_break & 8)
+			info->p2_lost = 1;
+		else
+			info->p2_lost = 0;
+		if (info->node != DLR_ACTIVE_SUPERVISOR)
+			dlr_tx_status(info, 0);
+		else if (attrib->participants_cnt > 0) {
+			struct ksz_dlr_node_info *node;
+
+			node = &info->nodes[0];
+			if (attrib->participants_cnt > 1) {
+				struct ksz_dlr_node_info *prev;
+				struct ksz_dlr_node_info *next;
+
+				prev = &info->nodes[attrib->participants_cnt -
+					1];
+				next = &info->nodes[1];
+				if (info->p1_lost)
+					prev->p2_lost = 1;
+				if (info->p2_lost)
+					next->p1_lost = 1;
+			}
+			node->p1_down = info->p1_down;
+			node->p2_down = info->p2_down;
+			node->p1_lost = info->p1_lost;
+			node->p2_lost = info->p2_lost;
+		}
+		break;
+	case PROC_SET_DLR_RING_SUPERVISOR_CONFIG:
+	{
+		struct ksz_dlr_super_cfg super;
+
+		/* Value can only be set through supervisor. */
+		if (!(info->attrib.cap & DLR_CAP_SUPERVISOR_CAPABLE))
+			break;
+		memcpy(&super, &info->attrib.super_cfg, sizeof(super));
+		if (num) {
+			super.enable = true;
+			if (!super.beacon_interval)
+				super.beacon_interval = 400;
+			if (!super.beacon_timeout)
+				super.beacon_timeout = 1960;
+		} else
+			super.enable = false;
+		rc = set_dlr_super_cfg(info, &super, &err);
+		if (!rc && err)
+			show_dlr_err(0, NULL, err);
+		break;
+	}
+	case PROC_SET_DLR_RING_FAULT_COUNT:
+		rc = set_dlr_ring_fault_cnt(info, num, &err);
+		if (!rc && err)
+			show_dlr_err(0, NULL, err);
+		break;
+	case PROC_SET_DLR_CLEAR_FAULT:
+		rc = set_dlr_clear_rapid_fault(info, &err);
+		if (!rc && err)
+			show_dlr_err(0, NULL, err);
+		break;
+	case PROC_SET_DLR_CLEAR_GATEWAY_FAULT:
+		rc = set_dlr_clear_gateway_fault(info, &err);
+		if (!rc && err)
+			show_dlr_err(0, NULL, err);
+		break;
+	case PROC_SET_DLR_PORT_1:
+#ifdef CONFIG_HAVE_DLR_HW
+		if (netif_running(info->dev))
+			printk(KERN_ALERT "stop %s first", info->dev->name);
+		if (0 <= num && num < sw->port_cnt &&
+		    num != sw->HOST_PORT && num != info->ports[1])
+			info->ports[0] = (u8) num;
+		info->member = (1 << info->ports[0]) | (1 << info->ports[1]);
+#endif
+		break;
+	case PROC_SET_DLR_PORT_2:
+#ifdef CONFIG_HAVE_DLR_HW
+		if (netif_running(info->dev))
+			printk(KERN_ALERT "stop %s first", info->dev->name);
+		if (0 <= num && num < sw->port_cnt &&
+		    num != sw->HOST_PORT && num != info->ports[0])
+			info->ports[1] = (u8) num;
+		info->member = (1 << info->ports[0]) | (1 << info->ports[1]);
+#endif
+		break;
+	}
+}  /* sysfs_dlr_write */
+
+static struct dlr_ops dlr_ops = {
+	.change_addr		= dlr_change_addr,
+	.link_change		= dlr_link_change,
+	.timeout		= dlr_timeout,
+
+	.dev_req		= dlr_dev_req,
+
+	.sysfs_read		= sysfs_dlr_read,
+	.sysfs_write		= sysfs_dlr_write,
+};
+
+static void ksz_dlr_exit(struct ksz_dlr_info *dlr)
+{
+	ksz_stop_timer(&dlr->announce_timeout_timer_info);
+	ksz_stop_timer(&dlr->neigh_chk_timer_info);
+	ksz_stop_timer(&dlr->signon_timer_info);
+	ksz_stop_timer(&dlr->test_timer_info);
+	flush_work(&dlr->delay_proc);
+	flush_work(&dlr->neigh_chk_proc);
+	cancel_delayed_work_sync(&dlr->announce_tx);
+}  /* ksz_dlr_exit */
+
+static void ksz_dlr_init(struct ksz_dlr_info *dlr, struct ksz_sw *sw)
+{
+	dlr->ok_ports = dlr->member;
+	dlr->sw_dev = sw;
+	dlr->node = DLR_BEACON_NODE;
+	dlr->state_machine = RingSupervisor_state;
+	setup_dlr(dlr);
+	INIT_DELAYED_WORK(&dlr->announce_tx, announce_monitor);
+	INIT_WORK(&dlr->delay_proc, dlr_delay_proc);
+	INIT_WORK(&dlr->neigh_chk_proc, neigh_chk_proc);
+	skb_queue_head_init(&dlr->rxq);
+	dlr->ops = &dlr_ops;
+	ksz_init_timer(&dlr->test_timer_info, 1000 * HZ / 1000,
+		test_monitor, dlr);
+}  /* ksz_dlr_init */
+
diff -Napur linux-linux4microchip-2022.10-orig/drivers/net/ethernet/micrel/ksz_dlr.h linux-linux4microchip-2022.10/drivers/net/ethernet/micrel/ksz_dlr.h
--- linux-linux4microchip-2022.10-orig/drivers/net/ethernet/micrel/ksz_dlr.h	1969-12-31 16:00:00.000000000 -0800
+++ linux-linux4microchip-2022.10/drivers/net/ethernet/micrel/ksz_dlr.h	2023-04-25 16:13:54.876163472 -0700
@@ -0,0 +1,346 @@
+/**
+ * Microchip DLR driver header
+ *
+ * Copyright (c) 2015-2019 Microchip Technology Inc.
+ *	Tristram Ha <Tristram.Ha@microchip.com>
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ */
+
+
+#ifndef KSZ_DLR_H
+#define KSZ_DLR_H
+
+#include <linux/if_vlan.h>
+#include "ksz_dlr_api.h"
+
+
+#define DLR_TAG_TYPE			0x80E1
+
+#define DLR_RING_SUBTYPE		2
+
+#define DLR_BEACON			0x1
+#define DLR_NEIGH_CHK_REQ		0x2
+#define DLR_NEIGH_CHK_RESP		0x3
+#define DLR_LINK_STATUS			0x4
+#define DLR_LOCATE_FAULT		0x5
+#define DLR_ANNOUNCE			0x6
+#define DLR_SIGN_ON			0x7
+#define DLR_ADVERTISE			0x8
+#define DLR_FLUSH_TABLES		0x9
+#define DLR_LEARNING_UPDATE		0xA
+
+#define DLR_PORT_NONE			0
+#define DLR_PORT_1			1
+#define DLR_PORT_2			2
+
+#define RING_NORMAL_STATE		1
+#define RING_FAULT_STATE		2
+
+#define DLR_GW_ACTIVE_LISTEN_STATE	1
+#define DLR_GW_ACTIVE_NORMAL_STATE	2
+#define DLR_GW_FAULT_STATE		3
+
+
+struct ksz_dlr_hdr {
+	u8 ring_subtype;
+	u8 ring_protocol_version;
+	u8 frame_type;
+	u8 src_port;
+	u32 ip_addr;
+	u32 seqid;
+} __packed;
+
+struct ksz_dlr_advertise {
+	u8 state;
+	u8 precedence;
+	u32 interval;
+	u32 timeout;
+	u8 learning_update_enable;
+} __packed;
+
+struct ksz_dlr_announce {
+	u8 ring_state;
+} __packed;
+
+struct ksz_dlr_beacon {
+	u8 ring_state;
+	u8 precedence;
+	u32 interval;
+	u32 timeout;
+	u8 reserved[20];
+} __packed;
+
+struct ksz_dlr_flush_tables {
+	u8 learning_update_enable;
+} __packed;
+
+struct ksz_dlr_status {
+	u8 port1_active:1;
+	u8 port2_active:1;
+	u8 reserved:5;
+	u8 neighbor:1;
+} __packed;
+
+struct ksz_dlr_node {
+	u32 ip_addr;
+	u8 addr[ETH_ALEN];
+} __packed;
+
+struct ksz_dlr_signon {
+	u16 num;
+	struct ksz_dlr_node node[1];
+} __packed;
+
+struct ksz_dlr_neigh_chk_resp {
+	u8 src_port;
+} __packed;
+
+struct ksz_dlr_frame {
+	struct ksz_dlr_hdr hdr;
+	union {
+		struct ksz_dlr_advertise advertise;
+		struct ksz_dlr_announce announce;
+		struct ksz_dlr_beacon beacon;
+		struct ksz_dlr_flush_tables flush;
+		struct ksz_dlr_status status;
+		struct ksz_dlr_neigh_chk_resp neigh_chk_resp;
+		struct ksz_dlr_signon signon;
+		u8 reserved[30];
+	} data;
+} __packed;
+
+struct ksz_dlr_tx_frame {
+	struct vlan_ethhdr vlan;
+	struct ksz_dlr_frame body;
+} __packed;
+
+struct ksz_dlr_update_frame {
+	struct ethhdr eth;
+	struct ksz_dlr_hdr hdr;
+	u8 reserved[34];
+} __packed;
+
+struct ksz_dlr_rx_frame {
+	struct vlan_ethhdr *vlan;
+	struct ksz_dlr_frame *body;
+};
+
+#define DLR_SUPERVISOR_NUM	10
+
+struct ksz_dlr_super_info {
+	u8 prec_addr[ETH_ALEN + 1];
+	u32 crc;
+	int port;
+	u32 timeout[2];
+	u32 cnt;
+	u32 last_cnt;
+	u32 sent:1;
+};
+
+struct ksz_dlr_info;
+
+struct dlr_ops {
+	void (*change_addr)(struct ksz_dlr_info *dlr, u8 *addr);
+	void (*link_change)(struct ksz_dlr_info *dlr, int link1, int link2);
+	void (*timeout)(struct ksz_dlr_info *dir, int port);
+
+	int (*dev_req)(struct ksz_dlr_info *dlr, char *arg, void *info);
+
+	ssize_t (*sysfs_read)(struct ksz_dlr_info *dlr, int proc_num,
+		ssize_t len, char *buf);
+	void (*sysfs_write)(struct ksz_dlr_info *dlr, int proc_num, int num,
+		const char *buf);
+
+};
+
+struct ksz_dlr_node_info {
+	struct ksz_dlr_node signon;
+	u32 p1_down:1;
+	u32 p2_down:1;
+	u32 p1_lost:1;
+	u32 p2_lost:1;
+};
+
+struct ksz_dlr_beacon_info {
+	u32 timer:1;
+	u32 rcv_once:1;
+	u32 timeout_start:1;
+	u32 timeout_stop:1;
+	u32 interval;
+	u32 timeout;
+	struct ksz_dlr_beacon last;
+};
+
+#define DLR_BEACON_STATE_HACK		(1 << 1)
+#define DLR_TEST_SEQ			(1 << 30)
+#define DLR_TEST			(1 << 31)
+
+struct ksz_dlr_info {
+	u32 p1_down:1;
+	u32 p2_down:1;
+	u32 p1_lost:1;
+	u32 p2_lost:1;
+	u32 p1_rcvd:1;
+	u32 p2_rcvd:1;
+	u32 p1_set:1;
+	u32 p2_set:1;
+	u32 p1_timeout:1;
+	u32 p2_timeout:1;
+	u32 one_down:1;
+	u32 both_down:1;
+	u32 one_rcvd:1;
+	u32 both_rcvd:1;
+	u32 one_timeout:1;
+	u32 both_timeout:1;
+	u32 new_supervisor:1;
+	u32 ann_rcvd:1;
+	u32 ann_timeout:1;
+	u32 ann_delay:1;
+	u32 ann_first:1;
+	u32 signon_delay:1;
+	u32 signon_start:1;
+	u32 new_val:1;
+	u32 neigh_chk:1;
+	u32 wait_done:1;
+	u32 reset:1;
+	u32 start:1;
+	u32 chk_hw:1;
+
+	struct ksz_dlr_gateway_capable attrib;
+	struct ksz_dlr_active_node last_sup;
+
+	u32 beacon_interval;
+	u32 beacon_timeout;
+	u32 ip_addr;
+	u16 vid;
+	u8 src_addr[ETH_ALEN];
+	u8 next_node;
+	u8 node;
+	u8 port;
+	u8 precedence;
+	u8 ring_state;
+	u8 drop_beacon;
+	u32 skip_beacon:1;
+	u32 disable_learn:1;
+	u8 LastBcnRcvPort;
+	struct ksz_dlr_beacon_info beacon_info[2];
+	u32 interval;
+	unsigned long ann_jiffies;
+
+	void *sw_dev;
+	struct net_device *dev;
+	struct ksz_timer_info announce_timer_info;
+	struct ksz_timer_info announce_timeout_timer_info;
+	struct ksz_timer_info neigh_chk_timer_info;
+	struct ksz_timer_info signon_timer_info;
+	struct ksz_timer_info test_timer_info;
+	struct delayed_work announce_tx;
+	u32 beacon_timeout_ports;
+	struct work_struct delay_proc;
+	struct work_struct neigh_chk_proc;
+	struct sk_buff_head rxq;
+	void (*state_machine)(struct ksz_dlr_info *info);
+	struct ksz_dlr_tx_frame last_beacon[2];
+	u8 beacon_addr[ETH_ALEN];
+	u32 stop:1;
+	u32 tx_signon:1;
+	u32 tx_announce:1;
+	u32 tx_advertise:1;
+	u32 tx_flush_tables:1;
+	u32 link_change:1;
+	u32 clr_supervisor:1;
+	u32 timeout_beacon:1;
+
+	struct ksz_dlr_tx_frame frame;
+	struct ksz_dlr_update_frame update_frame;
+	u8 signon_frame[2000];
+	u8 signon_addr[ETH_ALEN];
+	struct ksz_dlr_node_info nodes[500];
+	u8 *tx_frame;
+	int signon_port;
+	int len;
+	int cur_state;
+	int state;
+	int rx_port;
+	int tx_port;
+	int active_port;
+	u32 seqid;
+	u32 seqid_announce;
+	u32 seqid_beacon;
+	u32 seqid_signon;
+	u32 seqid_chk[2];
+	u32 seqid_first[2];
+	u32 seqid_last[2];
+	u32 seqid_rcv[2];
+	u8 port_chk[2];
+	u8 port_rcv[2];
+	u32 seqid_accept[2];
+	u8 ports[2];
+	u16 member;
+	u16 ok_ports;
+
+	struct ksz_dlr_super_info supers[DLR_SUPERVISOR_NUM];
+	struct ksz_dlr_super_info *rogue_super;
+
+	int seqid_cnt;
+	int signon_space;
+	u8 ignore_req;
+	u8 req_cnt[2];
+	u8 link_break;
+	unsigned long fault_jiffies;
+
+	uint notifications;
+
+	uint overrides;
+
+	const struct dlr_ops *ops;
+};
+
+struct dlr_attributes {
+	int info;
+	int node;
+	int prec;
+	int interval;
+	int timeout;
+	int vid;
+	int cfg;
+	int state;
+	int port;
+
+	int test;
+	int req;
+	int resp;
+	int link;
+	int learn;
+	int fault;
+	int signon;
+	int clear_rapid;
+	int clear_partial;
+
+	int all;
+	int rev;
+	int topology;
+	int network;
+	int super;
+	int super_cfg;
+	int fault_cnt;
+	int last_active_1;
+	int last_active_2;
+	int part_cnt;
+	int part_list;
+	int active_super_addr;
+	int active_super_prec;
+	int cap;
+	int port_1;
+	int port_2;
+};
+
+#endif
diff -Napur linux-linux4microchip-2022.10-orig/drivers/net/ethernet/micrel/ksz_dlr_sysfs.c linux-linux4microchip-2022.10/drivers/net/ethernet/micrel/ksz_dlr_sysfs.c
--- linux-linux4microchip-2022.10-orig/drivers/net/ethernet/micrel/ksz_dlr_sysfs.c	1969-12-31 16:00:00.000000000 -0800
+++ linux-linux4microchip-2022.10/drivers/net/ethernet/micrel/ksz_dlr_sysfs.c	2023-04-25 16:13:54.876163472 -0700
@@ -0,0 +1,186 @@
+/**
+ * Microchip DLR common sysfs code
+ *
+ * Copyright (c) 2015-2017 Microchip Technology Inc.
+ *	Tristram Ha <Tristram.Ha@microchip.com>
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ */
+
+
+static ssize_t dlr_show(struct device *d, struct device_attribute *attr,
+	char *buf, unsigned long offset)
+{
+	struct ksz_sw *sw;
+	struct semaphore *proc_sem;
+	struct ksz_dlr_info *dlr;
+	ssize_t len = -EINVAL;
+	int proc_num;
+
+	get_private_data(d, &proc_sem, &sw, NULL);
+	if (down_interruptible(proc_sem))
+		return -ERESTARTSYS;
+
+	dlr = &sw->info->dlr;
+	len = 0;
+	proc_num = offset / sizeof(int);
+	len = dlr->ops->sysfs_read(dlr, proc_num, len, buf);
+	up(proc_sem);
+	return len;
+}
+
+static ssize_t dlr_store(struct device *d, struct device_attribute *attr,
+	const char *buf, size_t count, unsigned long offset)
+{
+	struct ksz_sw *sw;
+	struct semaphore *proc_sem;
+	struct ksz_dlr_info *dlr;
+	ssize_t ret = -EINVAL;
+	int num;
+	int proc_num;
+
+	num = get_num_val(buf);
+	get_private_data(d, &proc_sem, &sw, NULL);
+	if (down_interruptible(proc_sem))
+		return -ERESTARTSYS;
+
+	dlr = &sw->info->dlr;
+	proc_num = offset / sizeof(int);
+	ret = count;
+	dlr->ops->sysfs_write(dlr, proc_num, num, buf);
+	up(proc_sem);
+	return ret;
+}
+
+#define DLR_ATTR(_name, _mode, _show, _store) \
+struct device_attribute dlr_attr_##_name = \
+	__ATTR(_name, _mode, _show, _store)
+
+/* generate a read-only attribute */
+#define DLR_RD_ENTRY(name)						\
+static ssize_t show_dlr_##name(struct device *d,			\
+	struct device_attribute *attr, char *buf)			\
+{									\
+	return dlr_show(d, attr, buf,					\
+		offsetof(struct dlr_attributes, name));			\
+}									\
+static DLR_ATTR(name, S_IRUGO, show_dlr_##name, NULL)
+
+/* generate a write-able attribute */
+#define DLR_WR_ENTRY(name)						\
+static ssize_t show_dlr_##name(struct device *d,			\
+	struct device_attribute *attr, char *buf)			\
+{									\
+	return dlr_show(d, attr, buf,					\
+		offsetof(struct dlr_attributes, name));			\
+}									\
+static ssize_t store_dlr_##name(struct device *d,			\
+	struct device_attribute *attr, const char *buf, size_t count)	\
+{									\
+	return dlr_store(d, attr, buf, count,				\
+		offsetof(struct dlr_attributes, name));			\
+}									\
+static DLR_ATTR(name, S_IRUGO | S_IWUSR, show_dlr_##name, store_dlr_##name)
+
+DLR_RD_ENTRY(info);
+DLR_WR_ENTRY(node);
+DLR_WR_ENTRY(prec);
+DLR_WR_ENTRY(interval);
+DLR_WR_ENTRY(timeout);
+DLR_WR_ENTRY(vid);
+DLR_WR_ENTRY(cfg);
+DLR_WR_ENTRY(state);
+DLR_WR_ENTRY(port);
+DLR_WR_ENTRY(test);
+DLR_WR_ENTRY(req);
+DLR_WR_ENTRY(resp);
+DLR_WR_ENTRY(link);
+DLR_WR_ENTRY(learn);
+DLR_WR_ENTRY(fault);
+DLR_WR_ENTRY(signon);
+DLR_WR_ENTRY(clear_rapid);
+DLR_WR_ENTRY(clear_partial);
+
+DLR_RD_ENTRY(all);
+DLR_RD_ENTRY(rev);
+DLR_RD_ENTRY(topology);
+DLR_RD_ENTRY(network);
+DLR_RD_ENTRY(super);
+DLR_WR_ENTRY(super_cfg);
+DLR_WR_ENTRY(fault_cnt);
+DLR_RD_ENTRY(last_active_1);
+DLR_RD_ENTRY(last_active_2);
+DLR_RD_ENTRY(part_cnt);
+DLR_RD_ENTRY(part_list);
+DLR_RD_ENTRY(active_super_addr);
+DLR_RD_ENTRY(active_super_prec);
+DLR_RD_ENTRY(cap);
+DLR_WR_ENTRY(port_1);
+DLR_WR_ENTRY(port_2);
+
+static struct attribute *dlr_attrs[] = {
+	&dlr_attr_info.attr,
+	&dlr_attr_node.attr,
+	&dlr_attr_prec.attr,
+	&dlr_attr_interval.attr,
+	&dlr_attr_timeout.attr,
+	&dlr_attr_vid.attr,
+	&dlr_attr_cfg.attr,
+	&dlr_attr_state.attr,
+	&dlr_attr_port.attr,
+	&dlr_attr_test.attr,
+	&dlr_attr_req.attr,
+	&dlr_attr_resp.attr,
+	&dlr_attr_link.attr,
+	&dlr_attr_learn.attr,
+	&dlr_attr_fault.attr,
+	&dlr_attr_signon.attr,
+	&dlr_attr_clear_rapid.attr,
+	&dlr_attr_clear_partial.attr,
+
+	&dlr_attr_all.attr,
+	&dlr_attr_rev.attr,
+	&dlr_attr_topology.attr,
+	&dlr_attr_network.attr,
+	&dlr_attr_super.attr,
+	&dlr_attr_super_cfg.attr,
+	&dlr_attr_fault_cnt.attr,
+	&dlr_attr_last_active_1.attr,
+	&dlr_attr_last_active_2.attr,
+	&dlr_attr_part_cnt.attr,
+	&dlr_attr_part_list.attr,
+	&dlr_attr_active_super_addr.attr,
+	&dlr_attr_active_super_prec.attr,
+	&dlr_attr_cap.attr,
+	&dlr_attr_port_1.attr,
+	&dlr_attr_port_2.attr,
+	NULL
+};
+
+static struct attribute_group dlr_group = {
+	.name  = "dlrfs",
+	.attrs  = dlr_attrs,
+};
+
+static void exit_dlr_sysfs(struct device *dev)
+{
+	sysfs_remove_group(&dev->kobj, &dlr_group);
+}
+
+static int init_dlr_sysfs(struct device *dev)
+{
+	int err;
+
+	err = sysfs_create_group(&dev->kobj, &dlr_group);
+	if (err)
+		return err;
+	return err;
+}
+
diff -Napur linux-linux4microchip-2022.10-orig/drivers/net/ethernet/micrel/ksz_hsr_api.h linux-linux4microchip-2022.10/drivers/net/ethernet/micrel/ksz_hsr_api.h
--- linux-linux4microchip-2022.10-orig/drivers/net/ethernet/micrel/ksz_hsr_api.h	1969-12-31 16:00:00.000000000 -0800
+++ linux-linux4microchip-2022.10/drivers/net/ethernet/micrel/ksz_hsr_api.h	2023-04-25 16:13:54.876163472 -0700
@@ -0,0 +1,53 @@
+/**
+ * Microchip HSR driver API header
+ *
+ * Copyright (c) 2017 Microchip Technology Inc.
+ *	Tristram Ha <Tristram.Ha@microchip.com>
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ */
+
+
+#ifndef KSZ_HSR_API_H
+#define KSZ_HSR_API_H
+
+#ifndef ETH_ALEN
+#define ETH_ALEN				6
+#endif
+
+
+enum {
+	DEV_INFO_HSR_LINK = DEV_INFO_LAST,
+};
+
+#define HSR_INFO_LINK_LOST			(1 << 0)
+
+
+#define HSR_GET_NETWORK_STATUS			2
+#define HSR_GET_RING_PARTICIPANTS_COUNT		8
+#define HSR_GET_RING_PARTICIPANTS_LIST		9
+#define HSR_GET_CAPABILITY_FLAGS		12
+
+#define HSR_CAP_DUPLICATE_DISCARD		(1 << 0)
+#define HSR_CAP_REDBOX_CAPABLE			(1 << 1)
+
+
+struct ksz_hsr_node {
+	u8 addr[ETH_ALEN];
+} __packed;
+
+union hsr_data {
+	struct ksz_hsr_node active;
+	u32 dword;
+	u16 word;
+	u8 byte;
+} __packed;
+
+#endif
diff -Napur linux-linux4microchip-2022.10-orig/drivers/net/ethernet/micrel/ksz_hsr.c linux-linux4microchip-2022.10/drivers/net/ethernet/micrel/ksz_hsr.c
--- linux-linux4microchip-2022.10-orig/drivers/net/ethernet/micrel/ksz_hsr.c	1969-12-31 16:00:00.000000000 -0800
+++ linux-linux4microchip-2022.10/drivers/net/ethernet/micrel/ksz_hsr.c	2023-11-09 10:30:35.238545018 -0800
@@ -0,0 +1,2481 @@
+/**
+ * Microchip HSR code
+ *
+ * Copyright (c) 2016-2023 Microchip Technology Inc.
+ *	Tristram Ha <Tristram.Ha@microchip.com>
+ *
+ * Copyright 2011-2014 Autronica Fire and Security AS
+
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ *
+ * Author(s):
+ *	2011-2014 Arvid Brodin, arvid.brodin@alten.se
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ */
+
+
+#if 1
+#define ANY_CAN_CREATE_ENTRY
+#endif
+#if 0
+#define REDBOX_NOT_SEND_SUPERVISION
+#endif
+#if 1
+#define REDBOX_NOT_REPLACE_MAC_ADDR
+#endif
+
+static int dbg_hsr;
+
+struct hsr_cfg_work {
+	struct work_struct work;
+	struct ksz_sw *sw;
+	u8 addr[ETH_ALEN];
+	u16 member;
+	u16 vlan;
+};
+
+static void proc_hsr_cfg_work(struct work_struct *work)
+{
+	struct hsr_cfg_work *cfg_work =
+		container_of(work, struct hsr_cfg_work, work);
+	struct ksz_sw *sw = cfg_work->sw;
+
+#ifdef CONFIG_KSZ_IBA
+	if (iba_stopped(sw)) {
+		kfree(cfg_work);
+		return;
+	}
+#endif
+
+	sw->ops->cfg_mac(sw, 0, cfg_work->addr, cfg_work->member,
+		false, false, 0);
+	if (cfg_work->vlan) {
+		if (cfg_work->member)
+			cfg_work->member = sw->HOST_MASK;
+		sw->ops->cfg_mac(sw, 0, cfg_work->addr, cfg_work->member,
+			false, true, cfg_work->vlan);
+	}
+
+	kfree(cfg_work);
+}  /* proc_hsr_work */
+
+static void proc_hsr_cfg(struct ksz_hsr_info *info, u8 *addr, u16 member)
+{
+	struct hsr_cfg_work *cfg_work;
+
+	cfg_work = kzalloc(sizeof(struct hsr_cfg_work), GFP_ATOMIC);
+	if (!cfg_work)
+		return;
+	INIT_WORK(&cfg_work->work, proc_hsr_cfg_work);
+	cfg_work->sw = info->sw_dev;
+	memcpy(cfg_work->addr, addr, ETH_ALEN);
+	cfg_work->member = member;
+	if (info->redbox)
+		cfg_work->vlan = info->redbox_vlan;
+	schedule_work(&cfg_work->work);
+}  /* proc_hsr_cfg */
+
+#if 0
+#ifndef CONFIG_KSZ_SWITCH
+static bool is_admin_up(struct net_device *dev)
+{
+	return dev && (dev->flags & IFF_UP);
+}
+#endif
+#endif
+
+#ifdef CONFIG_KSZ_SWITCH
+static
+#endif
+struct hsr_port *hsr_port_get_hsr(struct hsr_priv *hsr, enum hsr_port_type pt)
+{
+	struct ksz_hsr_info *info = container_of(hsr,
+		struct ksz_hsr_info, hsr);
+
+#ifdef CONFIG_KSZ_SWITCH
+	if (pt < HSR_PT_PORTS)
+		return &info->hsr_ports[pt];
+#endif
+	return NULL;
+}
+
+/* hsr_framereg */
+struct hsr_node {
+	struct list_head	mac_list;
+	unsigned char		MacAddressA[ETH_ALEN];
+	unsigned char		MacAddressB[ETH_ALEN];
+	/* Local slave through which AddrB frames are received from this node */
+	enum hsr_port_type	AddrB_port;
+	unsigned long		time_in[HSR_PT_PORTS];
+	bool			time_in_stale[HSR_PT_PORTS];
+	u16			seq_out[HSR_PT_PORTS];
+	unsigned long		time_out[HSR_PT_PORTS];
+	struct rcu_head		rcu_head;
+#ifdef CONFIG_KSZ_SWITCH
+	int			slave;
+#endif
+};
+
+
+/*	TODO: use hash lists for mac addresses (linux/jhash.h)?    */
+
+
+/* seq_nr_after(a, b) - return true if a is after (higher in sequence than) b,
+ * false otherwise.
+ */
+static bool seq_nr_after(u16 a, u16 b)
+{
+	/* Remove inconsistency where
+	 * seq_nr_after(a, b) == seq_nr_before(a, b)
+	 */
+	if ((int) b - a == 32768)
+		return false;
+
+	return (((s16) (b - a)) < 0);
+}
+#define seq_nr_before(a, b)		seq_nr_after((b), (a))
+#define seq_nr_after_or_eq(a, b)	(!seq_nr_before((a), (b)))
+#define seq_nr_before_or_eq(a, b)	(!seq_nr_after((a), (b)))
+
+
+#ifdef CONFIG_KSZ_SWITCH
+static
+#endif
+bool hsr_addr_is_self(struct hsr_priv *hsr, unsigned char *addr)
+{
+	struct hsr_node *node;
+
+	node = list_first_or_null_rcu(&hsr->self_node_db, struct hsr_node,
+				      mac_list);
+	if (!node) {
+		WARN_ONCE(1, "HSR: No self node\n");
+		return false;
+	}
+
+	if (ether_addr_equal(addr, node->MacAddressA))
+		return true;
+	if (ether_addr_equal(addr, node->MacAddressB))
+		return true;
+
+	return false;
+}
+
+/* Search for mac entry. Caller must hold rcu read lock.
+ */
+static struct hsr_node *find_node_by_AddrA(struct list_head *node_db,
+					   const unsigned char addr[ETH_ALEN])
+{
+	struct hsr_node *node;
+
+	list_for_each_entry_rcu(node, node_db, mac_list) {
+		if (ether_addr_equal(node->MacAddressA, addr))
+			return node;
+	}
+
+	return NULL;
+}
+
+
+/* Helper for device init; the self_node_db is used in hsr_rcv() to recognize
+ * frames from self that's been looped over the HSR ring.
+ */
+#ifdef CONFIG_KSZ_SWITCH
+static
+#endif
+int hsr_create_self_node(struct list_head *self_node_db,
+			 unsigned char addr_a[ETH_ALEN],
+			 unsigned char addr_b[ETH_ALEN])
+{
+	struct hsr_node *node, *oldnode;
+
+	node = kmalloc(sizeof(*node), GFP_KERNEL);
+	if (!node)
+		return -ENOMEM;
+
+	ether_addr_copy(node->MacAddressA, addr_a);
+	ether_addr_copy(node->MacAddressB, addr_b);
+
+	rcu_read_lock();
+	oldnode = list_first_or_null_rcu(self_node_db,
+						struct hsr_node, mac_list);
+	if (oldnode) {
+		list_replace_rcu(&oldnode->mac_list, &node->mac_list);
+		rcu_read_unlock();
+		synchronize_rcu();
+		kfree(oldnode);
+	} else {
+		rcu_read_unlock();
+		list_add_tail_rcu(&node->mac_list, self_node_db);
+	}
+
+	return 0;
+}
+
+
+/* Allocate an hsr_node and add it to node_db. 'addr' is the node's AddressA;
+ * seq_out is used to initialize filtering of outgoing duplicate frames
+ * originating from the newly added node.
+ */
+#ifdef CONFIG_KSZ_SWITCH
+static
+#endif
+struct hsr_node *hsr_add_node_(struct list_head *node_db, unsigned char addr[],
+			       u16 seq_out)
+{
+	struct hsr_node *node;
+	unsigned long now;
+	int i;
+
+	node = kzalloc(sizeof(*node), GFP_ATOMIC);
+	if (!node)
+		return NULL;
+
+#if 1
+dbg_msg("%s %x %02x:%02x:%02x:%02x:%02x:%02x %04x\n", __func__, (int)node,
+addr[0], addr[1], addr[2], addr[3], addr[4], addr[5], seq_out);
+#endif
+	ether_addr_copy(node->MacAddressA, addr);
+
+	/* We are only interested in time diffs here, so use current jiffies
+	 * as initialization. (0 could trigger an spurious ring error warning).
+	 */
+	now = jiffies;
+	for (i = 0; i < HSR_PT_PORTS; i++)
+		node->time_in[i] = now;
+	for (i = 0; i < HSR_PT_PORTS; i++)
+		node->seq_out[i] = seq_out;
+	for (i = 0; i < HSR_PT_PORTS; i++)
+		node->time_out[i] = now;
+
+	list_add_tail_rcu(&node->mac_list, node_db);
+
+	return node;
+}
+
+#ifdef CONFIG_KSZ_SWITCH
+static
+struct hsr_node *hsr_add_node(struct list_head *node_db, unsigned char addr[],
+			      u16 seq_out)
+{
+	struct hsr_node *node;
+
+	node = hsr_add_node_(node_db, addr, seq_out);
+	if (!node)
+		return node;
+
+	do {
+		struct hsr_priv *hsr = container_of(node_db,
+			struct hsr_priv, node_db);
+		struct ksz_hsr_info *info = container_of(hsr,
+			struct ksz_hsr_info, hsr);
+		int not_self;
+
+		not_self = memcmp(addr, info->src_addr, ETH_ALEN);
+		if (not_self)
+			proc_hsr_cfg(info, addr, info->member);
+		info->part_cnt++;
+	} while (0);
+
+	return node;
+}
+#endif
+
+#ifdef CONFIG_KSZ_SWITCH
+static
+struct hsr_node *hsr_add_slave(struct list_head *node_db,
+	unsigned char addr[], u16 seq_out)
+{
+	struct hsr_node *node;
+
+	node = hsr_add_node_(node_db, addr, seq_out);
+	if (!node)
+		return node;
+
+	node->slave = 1;
+
+	return node;
+}
+#endif
+
+/* Get the hsr_node from which 'skb' was sent.
+ */
+#ifdef CONFIG_KSZ_SWITCH
+static
+#endif
+struct hsr_node *hsr_get_node(struct list_head *node_db, struct sk_buff *skb,
+			      bool is_sup)
+{
+	struct hsr_node *node;
+	struct ethhdr *ethhdr;
+	struct ethhdr *exthdr;
+	u16 seq_out;
+
+	if (!skb_mac_header_was_set(skb))
+		return NULL;
+
+	ethhdr = (struct ethhdr *) skb_mac_header(skb);
+
+	list_for_each_entry_rcu(node, node_db, mac_list) {
+		if (ether_addr_equal(node->MacAddressA, ethhdr->h_source))
+			return node;
+		if (ether_addr_equal(node->MacAddressB, ethhdr->h_source))
+			return node;
+	}
+
+#ifndef ANY_CAN_CREATE_ENTRY
+	if (!is_sup)
+		return NULL; /* Only supervision frame may create node entry */
+#endif
+
+	exthdr = ethhdr;
+	if (exthdr->h_proto == htons(ETH_P_8021Q))
+		exthdr = (struct ethhdr *)((u8 *) exthdr + VLAN_HLEN);
+	if (exthdr->h_proto == htons(ETH_P_HSR))
+		exthdr = (struct ethhdr *)((u8 *) exthdr + HSR_HLEN);
+	else {
+		struct hsr_priv *hsr = container_of(node_db,
+			struct hsr_priv, node_db);
+
+		/* Supervision frame has its own sequence number. */
+		seq_out = hsr->sequence_nr - 1;
+		goto get_node_done;
+	}
+	if (exthdr->h_proto == htons(ETH_P_PRP)) {
+		/* Use the existing sequence_nr from the tag as starting point
+		 * for filtering duplicate frames.
+		 */
+		seq_out = hsr_get_skb_sequence_nr(skb) - 1;
+	} else {
+int i;
+for (i = 0; i < 20; i++)
+dbg_msg("%02x ", skb->data[i]);
+dbg_msg("  %04x\n", ethhdr->h_proto);
+#ifndef ANY_CAN_CREATE_ENTRY
+		WARN_ONCE(1, "%s: Non-HSR frame\n", __func__);
+#endif
+		seq_out = 0;
+	}
+
+get_node_done:
+	return hsr_add_node(node_db, ethhdr->h_source, seq_out);
+}
+
+/* Use the Supervision frame's info about an eventual MacAddressB for merging
+ * nodes that has previously had their MacAddressB registered as a separate
+ * node.
+ */
+#ifdef CONFIG_KSZ_SWITCH
+static
+#endif
+void hsr_handle_sup_frame(struct sk_buff *skb, struct hsr_node *node_curr,
+			  struct hsr_port *port_rcv)
+{
+	struct hsr_node *node_real;
+#ifdef CONFIG_KSZ_SWITCH
+	struct hsr_node *node_redbox = NULL;
+#endif
+	struct hsr_sup_payload *hsr_sp;
+	struct hsr_sup_type *hsr_stype;
+	struct list_head *node_db;
+	int i;
+
+	skb_pull(skb, sizeof(struct hsr_ethhdr_sp));
+	skb_pull(skb, sizeof(struct hsr_tag));
+	hsr_sp = (struct hsr_sup_payload *) skb->data;
+
+	if (ether_addr_equal(eth_hdr(skb)->h_source, hsr_sp->MacAddressA))
+		/* Not sent from MacAddressB of a PICS_SUBS capable node */
+		goto done;
+
+#ifdef CONFIG_KSZ_SWITCH
+	/* Check frame sent by RedBox. */
+	hsr_stype = (struct hsr_sup_type *)(hsr_sp + 1);
+	if (HSR_TLV_REDBOX == hsr_stype->HSR_TLV_Type) {
+		struct hsr_sup_payload *hsr_redbox;
+
+		hsr_redbox = (struct hsr_sup_payload *)(hsr_stype + 1);
+		if (ether_addr_equal(eth_hdr(skb)->h_source,
+		    hsr_redbox->MacAddressA))
+#if 1
+			node_redbox = node_curr;
+#else
+			goto done;
+#endif
+	}
+#endif
+
+	/* Merge node_curr (registered on MacAddressB) into node_real */
+	node_db = &port_rcv->hsr->node_db;
+	node_real = find_node_by_AddrA(node_db, hsr_sp->MacAddressA);
+	if (!node_real)
+{
+	u8 *data = (u8 *) eth_hdr(skb)->h_source;
+	u8 *addr = (u8 *) hsr_sp->MacAddressA;
+dbg_msg("add new: %02x:%02x:%02x; %02x:%02x:%02x\n",
+data[3], data[4], data[5],
+addr[3], addr[4], addr[5]);
+}
+	if (!node_real)
+		/* No frame received from AddrA of this node yet */
+		node_real = hsr_add_node(node_db, hsr_sp->MacAddressA,
+					 HSR_SEQNR_START - 1);
+	if (!node_real)
+		goto done; /* No mem */
+
+	/* From HSR side in case the entry was added in Redbox. */
+	if (node_real->slave) {
+		struct hsr_priv *hsr = container_of(node_db,
+			struct hsr_priv, node_db);
+		struct ksz_hsr_info *info = container_of(hsr,
+			struct ksz_hsr_info, hsr);
+
+		node_real->slave = 0;
+		proc_hsr_cfg(info, node_real->MacAddressA, info->member);
+	}
+	if (node_real == node_curr)
+		/* Node has already been merged */
+		goto done;
+
+#if 1
+	if (!node_redbox)
+#endif
+	ether_addr_copy(node_real->MacAddressB, eth_hdr(skb)->h_source);
+#if 0
+do {
+	u8 *data = (u8 *) eth_hdr(skb)->h_source;
+dbg_msg("%s %02x:%02x:%02x:%02x:%02x:%02x\n", __func__,
+data[0], data[1], data[2], data[3], data[4], data[5]);
+} while (0);
+#endif
+	for (i = 0; i < HSR_PT_PORTS; i++) {
+		if (!node_curr->time_in_stale[i] &&
+		    time_after(node_curr->time_in[i], node_real->time_in[i])) {
+			node_real->time_in[i] = node_curr->time_in[i];
+			node_real->time_in_stale[i] = node_curr->time_in_stale[i];
+		}
+		if (seq_nr_after(node_curr->seq_out[i], node_real->seq_out[i]))
+			node_real->seq_out[i] = node_curr->seq_out[i];
+	}
+	node_real->AddrB_port = port_rcv->type;
+#if 1
+	if (node_redbox)
+		goto done;
+#endif
+
+	list_del_rcu(&node_curr->mac_list);
+	kfree_rcu(node_curr, rcu_head);
+
+done:
+	skb_push(skb, sizeof(struct hsr_tag));
+	skb_push(skb, sizeof(struct hsr_ethhdr_sp));
+}
+
+
+/* 'skb' is a frame meant for this host, that is to be passed to upper layers.
+ *
+ * If the frame was sent by a node's B interface, replace the source
+ * address with that node's "official" address (MacAddressA) so that upper
+ * layers recognize where it came from.
+ */
+#ifdef CONFIG_KSZ_SWITCH
+static
+#endif
+void hsr_addr_subst_source(struct hsr_node *node, struct sk_buff *skb)
+{
+#ifdef DEBUG
+	if (!skb_mac_header_was_set(skb)) {
+		WARN_ONCE(1, "%s: Mac header not set\n", __func__);
+		return;
+	}
+#endif
+
+	memcpy(&eth_hdr(skb)->h_source, node->MacAddressA, ETH_ALEN);
+}
+
+#if 0
+/* 'skb' is a frame meant for another host.
+ * 'port' is the outgoing interface
+ *
+ * Substitute the target (dest) MAC address if necessary, so the it matches the
+ * recipient interface MAC address, regardless of whether that is the
+ * recipient's A or B interface.
+ * This is needed to keep the packets flowing through switches that learn on
+ * which "side" the different interfaces are.
+ */
+#ifdef CONFIG_KSZ_SWITCH
+static
+#endif
+void hsr_addr_subst_dest(struct hsr_node *node_src, struct sk_buff *skb,
+			 struct hsr_port *port)
+{
+	struct hsr_node *node_dst;
+
+#ifdef DEBUG
+	if (!skb_mac_header_was_set(skb)) {
+		WARN_ONCE(1, "%s: Mac header not set\n", __func__);
+		return;
+	}
+#endif
+
+	if (!is_unicast_ether_addr(eth_hdr(skb)->h_dest))
+		return;
+
+	node_dst = find_node_by_AddrA(&port->hsr->node_db, eth_hdr(skb)->h_dest);
+	if (!node_dst) {
+#if 1
+u8 *addr = eth_hdr(skb)->h_dest;
+dbg_msg("%s %02x:%02x:%02x:%02x:%02x:%02x\n", __func__,
+addr[0], addr[1], addr[2], addr[3], addr[4], addr[5]);
+#endif
+		WARN_ONCE(1, "%s: Unknown node\n", __func__);
+		return;
+	}
+	if (port->type != node_dst->AddrB_port)
+		return;
+
+	ether_addr_copy(eth_hdr(skb)->h_dest, node_dst->MacAddressB);
+}
+#endif
+
+
+#ifdef CONFIG_KSZ_SWITCH
+static
+#endif
+void hsr_register_frame_in(struct hsr_node *node, struct hsr_port *port,
+			   u16 sequence_nr)
+{
+	/* Don't register incoming frames without a valid sequence number. This
+	 * ensures entries of restarted nodes gets pruned so that they can
+	 * re-register and resume communications.
+	 */
+	if (seq_nr_before(sequence_nr, node->seq_out[port->type]))
+	{
+		unsigned long diff = jiffies - node->time_in[port->type];
+#if 0
+dbg_msg("%s %d %04x %04x %lu\n", __func__, port->type,
+sequence_nr, node->seq_out[port->type], diff);
+#endif
+		if (diff <= msecs_to_jiffies(HSR_ENTRY_FORGET_TIME))
+			return;
+	}
+
+	node->time_in[port->type] = jiffies;
+	node->time_in_stale[port->type] = false;
+}
+
+#ifdef CONFIG_KSZ_SWITCH
+static
+#endif
+void hsr_update_frame_out(struct hsr_port *port, struct hsr_node *node,
+	u16 sequence_nr)
+{
+	node->seq_out[port->type] = sequence_nr;
+}
+
+static int dbg_frame_out;
+
+/* 'skb' is a HSR Ethernet frame (with a HSR tag inserted), with a valid
+ * ethhdr->h_source address and skb->mac_header set.
+ *
+ * Return:
+ *	 1 if frame can be shown to have been sent recently on this interface,
+ *	 0 otherwise, or
+ *	 negative error code on error
+ */
+#ifdef CONFIG_KSZ_SWITCH
+static
+#endif
+int hsr_register_frame_out(struct hsr_port *port, struct hsr_node *node,
+			   u16 sequence_nr)
+{
+	if (seq_nr_before_or_eq(sequence_nr, node->seq_out[port->type]))
+	{
+		unsigned long diff = jiffies - node->time_out[port->type];
+#if 1
+#if 0
+if (port->type != HSR_PT_MASTER)
+#endif
+if (dbg_hsr < 10)
+dbg_msg("%s %x %d; %d %04x %04x %lu\n", __func__, (int)node, dbg_frame_out,
+port->type, sequence_nr,
+node->seq_out[port->type], diff);
+#endif
+		if (diff <= msecs_to_jiffies(HSR_ENTRY_FORGET_TIME))
+			return 1;
+	}
+	node->time_out[port->type] = jiffies;
+
+	node->seq_out[port->type] = sequence_nr;
+	return 0;
+}
+
+
+#if 0
+static struct hsr_port *get_late_port(struct hsr_priv *hsr,
+				      struct hsr_node *node)
+{
+	if (node->time_in_stale[HSR_PT_SLAVE_A])
+		return hsr_port_get_hsr(hsr, HSR_PT_SLAVE_A);
+	if (node->time_in_stale[HSR_PT_SLAVE_B])
+		return hsr_port_get_hsr(hsr, HSR_PT_SLAVE_B);
+
+	if (time_after(node->time_in[HSR_PT_SLAVE_B],
+		       node->time_in[HSR_PT_SLAVE_A] +
+					msecs_to_jiffies(MAX_SLAVE_DIFF)))
+		return hsr_port_get_hsr(hsr, HSR_PT_SLAVE_A);
+	if (time_after(node->time_in[HSR_PT_SLAVE_A],
+		       node->time_in[HSR_PT_SLAVE_B] +
+					msecs_to_jiffies(MAX_SLAVE_DIFF)))
+		return hsr_port_get_hsr(hsr, HSR_PT_SLAVE_B);
+
+	return NULL;
+}
+#endif
+
+
+#ifdef CONFIG_KSZ_SWITCH
+static void hsr_notify_link_lost(struct ksz_hsr_info *info)
+{
+	static u8 lost_buf[sizeof(struct ksz_resp_msg) +
+		sizeof(struct ksz_hsr_node)];
+
+dbg_msg(" hsr: %u %u:%u %u:%u\n", info->ring,
+	info->p1_down, info->p2_down, info->p1_lost, info->p2_lost);
+	if ((info->notifications & HSR_INFO_LINK_LOST)) {
+		struct ksz_resp_msg *msg = (struct ksz_resp_msg *) lost_buf;
+		struct ksz_hsr_node active;
+		struct ksz_sw *sw = info->sw_dev;
+		struct file_dev_info *dev_info;
+		size_t n = sizeof(struct ksz_resp_msg) +
+			sizeof(struct ksz_hsr_node);
+
+		msg->module = DEV_MOD_HSR;
+		msg->cmd = DEV_INFO_HSR_LINK;
+		msg->resp.data[0] = 0;
+		if (info->p1_down)
+			msg->resp.data[0] |= 0x01;
+		if (info->p2_down)
+			msg->resp.data[0] |= 0x02;
+		if (info->p1_lost)
+			msg->resp.data[0] |= 0x04;
+		if (info->p2_lost)
+			msg->resp.data[0] |= 0x08;
+		memcpy(active.addr, info->src_addr, ETH_ALEN);
+		memcpy(&msg->resp.data[1], &active,
+			sizeof(struct ksz_hsr_node));
+		dev_info = sw->dev_list[0];
+		while (dev_info) {
+			if ((dev_info->notifications[DEV_MOD_HSR] &
+			    HSR_INFO_LINK_LOST))
+				file_dev_setup_msg(dev_info, msg, n, NULL,
+						   NULL);
+			dev_info = dev_info->next;
+		}
+	}
+}  /* hsr_notify_link_lost */
+
+#ifdef CONFIG_HAVE_HSR_HW
+static void hsr_chk_ring(struct work_struct *work)
+{
+	struct hsr_node *node;
+	struct delayed_work *dwork = to_delayed_work(work);
+	struct ksz_hsr_info *info =
+		container_of(dwork, struct ksz_hsr_info, chk_ring);
+	struct hsr_priv *hsr = &info->hsr;
+	int change = false;
+	int no_drop_win = false;
+	u16 start_seq[2];
+	u16 exp_seq[2];
+
+#ifdef CONFIG_KSZ_IBA
+	if (iba_stopped(info->sw_dev))
+		return;
+#endif
+
+	memset(start_seq, 0, sizeof(start_seq));
+	memset(exp_seq, 0, sizeof(exp_seq));
+
+	list_for_each_entry_rcu(node, &hsr->node_db, mac_list) {
+		if (!memcmp(node->MacAddressA, info->src_addr, ETH_ALEN))
+			continue;
+		if (!node->slave) {
+			struct ksz_hsr_table entry;
+			struct ksz_sw *sw = info->sw_dev;
+
+			memcpy(entry.src_mac, node->MacAddressA, ETH_ALEN);
+			entry.path_id = 0;
+			sw_r_hsr_table(sw, 0, &entry);
+			if (entry.start_seq[0]) {
+				u16 win[2];
+
+				win[0] = entry.exp_seq[0] - entry.start_seq[0];
+				win[1] = entry.exp_seq[1] - entry.start_seq[1];
+				if (!win[0] && !win[1] &&
+				    info->seq_num != entry.exp_seq[0]) {
+					no_drop_win = true;
+					info->center = node;
+				}
+				if (info->center == node) {
+					exp_seq[0] = entry.exp_seq[0];
+					exp_seq[1] = entry.exp_seq[1];
+					start_seq[0] = entry.start_seq[0];
+					start_seq[1] = entry.start_seq[1];
+				}
+			}
+			if (no_drop_win)
+				break;
+		}
+	}
+
+	if (info->ring && !no_drop_win) {
+		if (info->center) {
+			int p;
+
+dbg_msg("%04x:%04x %04x:%04x\n", start_seq[0], start_seq[1],
+exp_seq[0], exp_seq[1]);
+			if (start_seq[0] == exp_seq[0]) {
+				info->p1_lost = 1;
+				p = 1;
+			} else {
+				info->p2_lost = 1;
+				p = 0;
+			}
+			info->seq_num = exp_seq[p];
+		}
+		info->ring = 0;
+		change = 1;
+	} else if (!info->ring && no_drop_win) {
+		info->ring = 1;
+		info->p1_down = info->p2_down =
+		info->p1_lost = info->p2_lost = 0;
+		change = 1;
+	}
+	if (change)
+		hsr_notify_link_lost(info);
+	info->check = 0;
+}  /* hsr_chk_ring */
+#endif
+#endif
+
+
+/* hsr_framereg */
+/* Remove stale sequence_nr records. Called by timer every
+ * HSR_LIFE_CHECK_INTERVAL (two seconds or so).
+ */
+#ifdef CONFIG_KSZ_SWITCH
+static
+#endif
+void hsr_prune_nodes(struct timer_list *t)
+{
+	struct hsr_priv *hsr = from_timer(hsr, t, prune_timer);
+	struct hsr_node *node;
+#if 0
+	struct hsr_port *port;
+#endif
+	unsigned long timestamp;
+	unsigned long time_a, time_b;
+
+	rcu_read_lock();
+	list_for_each_entry_rcu(node, &hsr->node_db, mac_list) {
+		/* Shorthand */
+		time_a = node->time_in[HSR_PT_SLAVE_A];
+		time_b = node->time_in[HSR_PT_SLAVE_B];
+
+		/* Check for timestamps old enough to risk wrap-around */
+		if (time_after(jiffies, time_a + MAX_JIFFY_OFFSET/2))
+			node->time_in_stale[HSR_PT_SLAVE_A] = true;
+		if (time_after(jiffies, time_b + MAX_JIFFY_OFFSET/2))
+			node->time_in_stale[HSR_PT_SLAVE_B] = true;
+
+		/* Get age of newest frame from node.
+		 * At least one time_in is OK here; nodes get pruned long
+		 * before both time_ins can get stale
+		 */
+		timestamp = time_a;
+		if (node->time_in_stale[HSR_PT_SLAVE_A] ||
+		    (!node->time_in_stale[HSR_PT_SLAVE_B] &&
+		    time_after(time_b, time_a)))
+			timestamp = time_b;
+
+#if 0
+		/* Warn of ring error only as long as we get frames at all */
+		if (time_is_after_jiffies(timestamp +
+					msecs_to_jiffies(1.5*MAX_SLAVE_DIFF))) {
+			rcu_read_lock();
+			port = get_late_port(hsr, node);
+			if (port != NULL)
+				hsr_nl_ringerror(hsr, node->MacAddressA, port);
+			rcu_read_unlock();
+		}
+#endif
+
+#ifdef CONFIG_KSZ_SWITCH
+		if (1 == node->slave && time_is_before_jiffies(timestamp +
+			msecs_to_jiffies(HSR_NODE_FORGET_TIME / 4))) {
+#if 1
+dbg_msg("drop: %02x:%02x:%02x:%02x:%02x:%02x %ld\n",
+node->MacAddressA[0],
+node->MacAddressA[1],
+node->MacAddressA[2],
+node->MacAddressA[3],
+node->MacAddressA[4],
+node->MacAddressA[5],
+jiffies - timestamp);
+#endif
+			node->slave = 2;
+		}
+#endif
+
+		/* Prune old entries */
+		if (time_is_before_jiffies(timestamp +
+					msecs_to_jiffies(HSR_NODE_FORGET_TIME))) {
+#ifdef CONFIG_KSZ_SWITCH
+			struct ksz_hsr_info *info = container_of(hsr,
+				struct ksz_hsr_info, hsr);
+			if (!node->slave) {
+				proc_hsr_cfg(info, node->MacAddressA, 0);
+				info->part_cnt--;
+			}
+
+#if 1
+dbg_msg("forget: %02x:%02x:%02x:%02x:%02x:%02x %lx %lx %lx; %ld\n",
+node->MacAddressA[0],
+node->MacAddressA[1],
+node->MacAddressA[2],
+node->MacAddressA[3],
+node->MacAddressA[4],
+node->MacAddressA[5],
+time_a, time_b, timestamp, jiffies - timestamp);
+#endif
+			if (node == info->center) {
+				info->center = NULL;
+				info->seq_num = 0;
+				info->ring = 0;
+			}
+#endif
+#if 0
+			hsr_nl_nodedown(hsr, node->MacAddressA);
+#endif
+			list_del_rcu(&node->mac_list);
+			/* Note that we need to free this entry later: */
+			kfree_rcu(node, rcu_head);
+		}
+	}
+	rcu_read_unlock();
+
+#ifdef CONFIG_KSZ_SWITCH
+	hsr->prune_timer.expires = jiffies + msecs_to_jiffies(PRUNE_PERIOD);
+	add_timer(&hsr->prune_timer);
+#endif
+}
+
+#ifdef CONFIG_KSZ_SWITCH
+static
+void hsr_rmv_slaves(struct ksz_hsr_info *info)
+{
+	struct hsr_priv *hsr = &info->hsr;
+	struct hsr_node *node;
+
+	rcu_read_lock();
+	list_for_each_entry_rcu(node, &hsr->node_db, mac_list) {
+
+		/* Prune slave entries */
+		if (node->slave) {
+			list_del_rcu(&node->mac_list);
+			/* Note that we need to free this entry later: */
+			kfree_rcu(node, rcu_head);
+		}
+	}
+	rcu_read_unlock();
+}
+#endif
+
+
+#if 0
+#ifdef CONFIG_KSZ_SWITCH
+static
+#endif
+void *hsr_get_next_node(struct hsr_priv *hsr, void *_pos,
+			unsigned char addr[ETH_ALEN])
+{
+	struct hsr_node *node;
+
+	if (!_pos) {
+		node = list_first_or_null_rcu(&hsr->node_db,
+					      struct hsr_node, mac_list);
+		if (node)
+			ether_addr_copy(addr, node->MacAddressA);
+		return node;
+	}
+
+	node = _pos;
+	list_for_each_entry_continue_rcu(node, &hsr->node_db, mac_list) {
+		ether_addr_copy(addr, node->MacAddressA);
+		return node;
+	}
+
+	return NULL;
+}
+
+
+#ifdef CONFIG_KSZ_SWITCH
+static
+#endif
+int hsr_get_node_data(struct hsr_priv *hsr,
+		      const unsigned char *addr,
+		      unsigned char addr_b[ETH_ALEN],
+		      unsigned int *addr_b_ifindex,
+		      int *if1_age,
+		      u16 *if1_seq,
+		      int *if2_age,
+		      u16 *if2_seq)
+{
+	struct hsr_node *node;
+	struct hsr_port *port;
+	unsigned long tdiff;
+
+
+	rcu_read_lock();
+	node = find_node_by_AddrA(&hsr->node_db, addr);
+	if (!node) {
+		rcu_read_unlock();
+		return -ENOENT;	/* No such entry */
+	}
+
+	ether_addr_copy(addr_b, node->MacAddressB);
+
+	tdiff = jiffies - node->time_in[HSR_PT_SLAVE_A];
+	if (node->time_in_stale[HSR_PT_SLAVE_A])
+		*if1_age = INT_MAX;
+#if HZ <= MSEC_PER_SEC
+	else if (tdiff > msecs_to_jiffies(INT_MAX))
+		*if1_age = INT_MAX;
+#endif
+	else
+		*if1_age = jiffies_to_msecs(tdiff);
+
+	tdiff = jiffies - node->time_in[HSR_PT_SLAVE_B];
+	if (node->time_in_stale[HSR_PT_SLAVE_B])
+		*if2_age = INT_MAX;
+#if HZ <= MSEC_PER_SEC
+	else if (tdiff > msecs_to_jiffies(INT_MAX))
+		*if2_age = INT_MAX;
+#endif
+	else
+		*if2_age = jiffies_to_msecs(tdiff);
+
+	/* Present sequence numbers as if they were incoming on interface */
+	*if1_seq = node->seq_out[HSR_PT_SLAVE_B];
+	*if2_seq = node->seq_out[HSR_PT_SLAVE_A];
+
+	if (node->AddrB_port != HSR_PT_NONE) {
+		port = hsr_port_get_hsr(hsr, node->AddrB_port);
+		*addr_b_ifindex = port->dev->ifindex;
+	} else {
+		*addr_b_ifindex = -1;
+	}
+
+	rcu_read_unlock();
+
+	return 0;
+}
+#endif
+/* hsr_framereg */
+
+static int hsr_dev_xmit(struct ksz_hsr_info *info, struct net_device *dev,
+	struct sk_buff *skb)
+{
+	int rc;
+	const struct net_device_ops *ops = dev->netdev_ops;
+	struct net_device **netdev = (struct net_device **)skb->cb;
+
+	*netdev = dev;
+	skb->dev = dev;
+	rc = ops->ndo_start_xmit(skb, skb->dev);
+	if (NETDEV_TX_BUSY == rc) {
+		dev_kfree_skb_irq(skb);
+	}
+	return rc;
+}  /* hsr_dev_xmit */
+
+static int hsr_xmit(struct ksz_hsr_info *info)
+{
+	struct sk_buff *skb;
+	u8 *frame = info->tx_frame;
+	int len = info->len;
+
+	/* Do not send if network device is not ready. */
+	if (!netif_running(info->dev) || !info->hsr_up)
+		return 0;
+
+	if (len < 60) {
+		memset(&frame[len], 0, 60 - len);
+		len = 60;
+	}
+
+	/* Add extra for tail tagging. */
+	skb = dev_alloc_skb(len + 4 + 8);
+	if (!skb)
+		return -ENOMEM;
+
+	memcpy(skb->data, info->tx_frame, len);
+
+	skb_put(skb, len);
+	skb->protocol = htons(ETH_P_PRP);
+	return hsr_dev_xmit(info, info->dev, skb);
+}  /* hsr_xmit */
+
+/* hsr_device */
+static void prep_hsr_supervision_frame(struct ksz_hsr_info *info)
+{
+	int len;
+	int tlen;
+	struct hsr_port *master;
+	struct hsr_sup_tag *hsr_stag;
+	struct hsr_sup_type *hsr_stype;
+	struct hsr_sup_payload *hsr_sp;
+	struct ethhdr *eth = (struct ethhdr *) info->master_sup_frame;
+
+	master = hsr_port_get_hsr(&info->hsr, HSR_PT_MASTER);
+
+	memcpy(eth->h_dest, master->hsr->sup_multicast_addr, ETH_ALEN);
+	memcpy(eth->h_source, info->src_addr, ETH_ALEN);
+	eth->h_proto = htons(ETH_P_PRP);
+	len = sizeof(struct ethhdr);
+	tlen = 60;
+
+	/* Use VLAN. */
+	if (info->vid) {
+		struct vlan_ethhdr *vlan = (struct vlan_ethhdr *)
+			info->master_sup_frame;
+
+		vlan->h_vlan_encapsulated_proto = vlan->h_vlan_proto;
+		vlan->h_vlan_TCI = htons(info->vid);
+		vlan->h_vlan_proto = htons(ETH_P_8021Q);
+		len += VLAN_HLEN;
+		tlen += VLAN_HLEN;
+	}
+
+	hsr_stag = (struct hsr_sup_tag *) &info->master_sup_frame[len];
+
+	/* Remember the supervision tag location. */
+	info->master_hsr_stag = hsr_stag;
+
+	/* HSR tag will be inserted by the standard hsr_fill_tag routine. */
+	set_hsr_stag_path(hsr_stag, 0);
+	set_hsr_stag_HSR_Ver(hsr_stag, 1);
+	len += sizeof(struct hsr_sup_tag);
+
+	hsr_stype = (struct hsr_sup_type *)(hsr_stag + 1);
+	hsr_stype->HSR_TLV_Type = HSR_TLV_LIFE_CHECK;
+	hsr_stype->HSR_TLV_Length = 6;
+	len += sizeof(struct hsr_sup_type);
+
+	/* Payload: MacAddressA */
+	hsr_sp = (struct hsr_sup_payload *)(hsr_stype + 1);
+	ether_addr_copy(hsr_sp->MacAddressA, info->src_addr);
+	len += sizeof(struct hsr_sup_payload);
+
+	/* Last TLV */
+	hsr_stype = (struct hsr_sup_type *)(hsr_sp + 1);
+	hsr_stype->HSR_TLV_Type = 0;
+	hsr_stype->HSR_TLV_Length = 0;
+	len += sizeof(struct hsr_sup_type);
+	if (len < tlen) {
+		memset(&info->master_sup_frame[len], 0, tlen - len);
+		len = tlen;
+	}
+	info->master_len = len;
+}  /* prep_hsr_supervision_frame */
+
+static void prep_hsr_supervision_slave_frame(struct ksz_hsr_info *info)
+{
+	int len;
+	int tlen;
+	struct hsr_port *master;
+	struct hsr_sup_tag *hsr_stag;
+	struct hsr_sup_type *hsr_stype;
+	struct hsr_sup_payload *hsr_sp;
+	struct ethhdr *eth = (struct ethhdr *) info->slave_sup_frame;
+
+	master = hsr_port_get_hsr(&info->hsr, HSR_PT_MASTER);
+
+	memcpy(eth->h_dest, master->hsr->sup_multicast_addr, ETH_ALEN);
+	memcpy(eth->h_source, info->src_addr, ETH_ALEN);
+	eth->h_proto = htons(ETH_P_PRP);
+	len = sizeof(struct ethhdr);
+	tlen = 60;
+
+	/* Use VLAN. */
+	if (info->vid) {
+		struct vlan_ethhdr *vlan = (struct vlan_ethhdr *)
+			info->slave_sup_frame;
+
+		vlan->h_vlan_encapsulated_proto = vlan->h_vlan_proto;
+		vlan->h_vlan_TCI = htons(info->vid);
+		vlan->h_vlan_proto = htons(ETH_P_8021Q);
+		len += VLAN_HLEN;
+		tlen += VLAN_HLEN;
+	}
+
+	hsr_stag = (struct hsr_sup_tag *) &info->slave_sup_frame[len];
+
+	/* Remember the supervision tag location. */
+	info->slave_hsr_stag = hsr_stag;
+
+	/* HSR tag will be inserted by the standard hsr_fill_tag routine. */
+	set_hsr_stag_path(hsr_stag, 0);
+	set_hsr_stag_HSR_Ver(hsr_stag, 1);
+	len += sizeof(struct hsr_sup_tag);
+
+	hsr_stype = (struct hsr_sup_type *)(hsr_stag + 1);
+	hsr_stype->HSR_TLV_Type = HSR_TLV_LIFE_CHECK;
+	hsr_stype->HSR_TLV_Length = 6;
+	len += sizeof(struct hsr_sup_type);
+
+	/* Payload: MacAddressA */
+	hsr_sp = (struct hsr_sup_payload *)(hsr_stype + 1);
+	len += sizeof(struct hsr_sup_payload);
+
+	/* Insert RedBox MAC address if available. */
+	hsr_stype = (struct hsr_sup_type *)(hsr_sp + 1);
+	hsr_stype->HSR_TLV_Type = HSR_TLV_REDBOX;
+	hsr_stype->HSR_TLV_Length = 6;
+	len += sizeof(struct hsr_sup_type);
+
+	hsr_sp = (struct hsr_sup_payload *)(hsr_stype + 1);
+	ether_addr_copy(hsr_sp->MacAddressA, info->redbox->dev_addr);
+
+	/* Cannot have different source MAC address. */
+	ether_addr_copy(hsr_sp->MacAddressA, info->src_addr);
+	len += sizeof(struct hsr_sup_payload);
+
+	/* Last TLV */
+	hsr_stype = (struct hsr_sup_type *)(hsr_sp + 1);
+	hsr_stype->HSR_TLV_Type = 0;
+	hsr_stype->HSR_TLV_Length = 0;
+	len += sizeof(struct hsr_sup_type);
+	if (len < tlen) {
+		memset(&info->slave_sup_frame[len], 0, tlen - len);
+		len = tlen;
+	}
+	info->slave_len = len;
+}  /* prep_hsr_supervision_slave_frame */
+
+static void send_hsr_supervision_frame(struct hsr_port *master)
+{
+	struct ksz_hsr_info *info = container_of(master->hsr,
+		struct ksz_hsr_info, hsr);
+	struct hsr_sup_tag *hsr_stag = info->hsr_stag;
+
+	hsr_stag->sequence_nr = htons(master->hsr->sup_sequence_nr);
+	master->hsr->sup_sequence_nr++;
+
+	hsr_xmit(info);
+}
+
+/* Announce (supervision frame) timer function
+ */
+static void hsr_announce(struct timer_list *t)
+{
+	struct hsr_priv *hsr = from_timer(hsr, t, announce_timer);
+	struct hsr_port *master;
+	struct ksz_hsr_info *info;
+	struct hsr_node *node;
+
+	info = container_of(hsr, struct ksz_hsr_info, hsr);
+
+	master = hsr_port_get_hsr(hsr, HSR_PT_MASTER);
+
+#ifdef CONFIG_KSZ_SWITCH
+	info->tx_frame = info->master_sup_frame;
+	info->len = info->master_len;
+	info->hsr_stag = info->master_hsr_stag;
+	send_hsr_supervision_frame(master);
+
+	info->tx_frame = info->slave_sup_frame;
+	info->len = info->slave_len;
+	info->hsr_stag = info->slave_hsr_stag;
+	list_for_each_entry_rcu(node, &hsr->node_db, mac_list) {
+		struct hsr_sup_type *hsr_stype;
+		struct hsr_sup_payload *hsr_sp;
+		struct hsr_sup_tag *hsr_stag = info->slave_hsr_stag;
+		struct vlan_ethhdr *vlan = (struct vlan_ethhdr *)
+			info->slave_sup_frame;
+		struct hsr_port *port;
+
+		if (1 != node->slave)
+			continue;
+
+		memcpy(vlan->h_source, node->MacAddressA, ETH_ALEN);
+
+		hsr_stype = (struct hsr_sup_type *)(hsr_stag + 1);
+		hsr_sp = (struct hsr_sup_payload *)(hsr_stype + 1);
+		ether_addr_copy(hsr_sp->MacAddressA, node->MacAddressA);
+#ifndef REDBOX_NOT_SEND_SUPERVISION
+		/* Update so that sequence number may not be too far apart. */
+		port = hsr_port_get_hsr(hsr, HSR_PT_SLAVE_A);
+		hsr_update_frame_out(port, node, hsr->sequence_nr - 1);
+		send_hsr_supervision_frame(master);
+#endif
+	}
+#endif
+
+	hsr->announce_timer.expires = jiffies +
+		msecs_to_jiffies(HSR_LIFE_CHECK_INTERVAL);
+
+#ifdef CONFIG_KSZ_SWITCH
+	if (netif_running(master->dev) && info->hsr_up)
+#endif
+		add_timer(&hsr->announce_timer);
+}
+
+
+/* hsr_forward */
+/* The uses I can see for these HSR supervision frames are:
+ * 1) Use the frames that are sent after node initialization ("HSR_TLV.Type =
+ *    22") to reset any sequence_nr counters belonging to that node. Useful if
+ *    the other node's counter has been reset for some reason.
+ *    --
+ *    Or not - resetting the counter and bridging the frame would create a
+ *    loop, unfortunately.
+ *
+ * 2) Use the LifeCheck frames to detect ring breaks. I.e. if no LifeCheck
+ *    frame is received from a particular node, we know something is wrong.
+ *    We just register these (as with normal frames) and throw them away.
+ *
+ * 3) Allow different MAC addresses for the two slave interfaces, using the
+ *    MacAddressA field.
+ */
+static bool is_supervision_frame(struct hsr_priv *hsr, struct sk_buff *skb)
+{
+	struct hsr_ethhdr_sp *hdr;
+
+	hdr = (struct hsr_ethhdr_sp *) skb_mac_header(skb);
+
+	if (!ether_addr_equal(hdr->ethhdr.h_dest,
+			      hsr->sup_multicast_addr))
+		return false;
+
+	/* Sent internally with VLAN tag. */
+	if (hdr->ethhdr.h_proto == htons(ETH_P_8021Q))
+		hdr = (struct hsr_ethhdr_sp *)((u8 *) hdr + VLAN_HLEN);
+
+	/* Received from outside with HSR tag. */
+	if (hdr->ethhdr.h_proto == htons(ETH_P_HSR))
+		hdr = (struct hsr_ethhdr_sp *)((u8 *) hdr + HSR_HLEN);
+	if (get_hsr_stag_HSR_ver(&hdr->hsr_sup) < 1)
+		return false;
+	if (hdr->sup_type.HSR_TLV_Type != HSR_TLV_LIFE_CHECK)
+		return false;
+	if (hdr->sup_type.HSR_TLV_Length != 6)
+		return false;
+
+	return true;
+}
+
+static struct sk_buff *create_stripped_skb(struct sk_buff *skb,
+					   struct hsr_frame_info *frame)
+{
+	int copylen;
+	unsigned char *dst, *src;
+
+#ifdef CONFIG_KSZ_SWITCH
+	copylen = 2 * ETH_ALEN;
+	if (frame->is_vlan)
+		copylen += VLAN_HLEN;
+
+	src = skb->data;
+	dst = skb_pull(skb, HSR_HLEN);
+	memmove(dst, src, copylen);
+#endif
+	skb_reset_mac_header(skb);
+
+#ifdef CONFIG_KSZ_SWITCH
+	/* Move HSR frame to non-HSR frame. */
+	frame->skb_std = skb;
+	frame->skb_hsr = NULL;
+#endif
+
+	return skb;
+}
+
+static struct sk_buff *frame_get_stripped_skb(struct hsr_frame_info *frame,
+					      struct hsr_port *port)
+{
+	if (!frame->skb_std)
+		frame->skb_std = create_stripped_skb(frame->skb_hsr, frame);
+	return frame->skb_std;
+}
+
+static void hsr_fill_tag(struct sk_buff *skb, struct hsr_frame_info *frame,
+			 struct hsr_port *port)
+{
+	int lane_id;
+	int lsdu_size;
+
+#ifdef CONFIG_KSZ_SWITCH
+	if (hsr_addr_is_self(port->hsr, &skb->data[6]))
+		lane_id = port->hsr->net_id;
+	else
+		lane_id = port->hsr->redbox_id;
+	lane_id <<= 1;
+#endif
+
+	lsdu_size = skb->len - 14;
+
+	if (lsdu_size < 60 - 14)
+		lsdu_size = 60 - 14;
+	if (frame->is_vlan)
+		lsdu_size -= 4;
+
+	set_hsr_tag_path(frame->hsr_tag, lane_id);
+	set_hsr_tag_LSDU_size(frame->hsr_tag, lsdu_size);
+	frame->hsr_tag->sequence_nr = htons(frame->sequence_nr);
+	*frame->proto = htons(ETH_P_HSR);
+}
+
+static struct sk_buff *create_tagged_skb(struct sk_buff *skb,
+					 struct hsr_frame_info *frame,
+					 struct hsr_port *port)
+{
+	int movelen;
+	unsigned char *dst, *src;
+
+#ifdef CONFIG_KSZ_SWITCH
+	movelen = 2 * ETH_ALEN;
+#endif
+	if (frame->is_vlan)
+		movelen += VLAN_HLEN;
+
+	src = skb_mac_header(skb);
+	dst = skb_push(skb, HSR_HLEN);
+	memmove(dst, src, movelen);
+	skb_reset_mac_header(skb);
+
+	hsr_fill_tag(skb, frame, port);
+
+#ifdef CONFIG_KSZ_SWITCH
+	/* Move non-HSR frame to HSR frame. */
+	frame->skb_hsr = skb;
+	frame->skb_std = NULL;
+#endif
+
+	return skb;
+}
+
+/* If the original frame was an HSR tagged frame, just clone it to be sent
+ * unchanged. Otherwise, create a private frame especially tagged for 'port'.
+ */
+static struct sk_buff *frame_get_tagged_skb(struct hsr_frame_info *frame,
+					    struct hsr_port *port)
+{
+	if (frame->skb_hsr)
+		return frame->skb_hsr;
+
+	return create_tagged_skb(frame->skb_std, frame, port);
+}
+
+/* Forward the frame through all devices except:
+ * - Back through the receiving device
+ * - If it's a HSR frame: through a device where it has passed before
+ * - To the local HSR master only if the frame is directly addressed to it, or
+ *   a non-supervision multicast or broadcast frame.
+ *
+ * HSR slave devices should insert a HSR tag into the frame, or forward the
+ * frame unchanged if it's already tagged. Interlink devices should strip HSR
+ * tags if they're of the non-HSR type (but only after duplicate discard). The
+ * master device always strips HSR tags.
+ */
+static void hsr_forward_do(struct hsr_frame_info *frame)
+{
+	struct hsr_port *from;
+	struct hsr_port *other;
+	struct hsr_port *port;
+	struct sk_buff *skb;
+	struct ksz_hsr_info *info;
+
+	from = frame->port_rcv;
+
+	/* Coming from host. */
+	if (from->type == HSR_PT_MASTER) {
+		port = hsr_port_get_hsr(from->hsr, HSR_PT_SLAVE_A);
+		other = hsr_port_get_hsr(from->hsr, HSR_PT_SLAVE_B);
+
+#if 0
+		/* Deliver frames directly addressed to us to master only */
+		if (frame->is_local_exclusive)
+{
+#if 1
+dbg_msg("local only: %d\n", port->type);
+#endif
+			goto tx_drop;
+}
+#endif
+		dbg_frame_out = frame->is_supervision;
+
+		/* Don't send frame over port where it has been sent before */
+		if (hsr_register_frame_out(port, frame->node_src,
+					   frame->sequence_nr))
+		{
+#if 0
+dbg_msg("saw before: %d %04x %d\n", port->type, frame->sequence_nr,
+frame->is_supervision);
+#endif
+
+			goto tx_drop;
+		}
+		hsr_update_frame_out(other, frame->node_src,
+			frame->sequence_nr);
+
+		/* Simulate incoming frame so that node is not removed. */
+		if (frame->is_supervision && !frame->node_src->slave) {
+			hsr_register_frame_in(frame->node_src, port,
+				frame->sequence_nr);
+		}
+
+		skb = frame_get_tagged_skb(frame, port);
+		skb->protocol = htons(ETH_P_HSR);
+
+		/*
+		 * Cannot use different source MAC address because
+		 * self-address filtering is used to drop frames sent by self.
+		 */
+		if (!hsr_addr_is_self(port->hsr,
+		    eth_hdr(frame->skb_hsr)->h_source)) {
+			info = container_of(from->hsr, struct ksz_hsr_info,
+					    hsr);
+
+			memcpy(eth_hdr(frame->skb_hsr)->h_source,
+			       info->src_addr, ETH_ALEN);
+		}
+		return;
+	} else {
+		int forward;
+		u16 path_id;
+
+		port = hsr_port_get_hsr(from->hsr, HSR_PT_MASTER);
+		if (from->type == HSR_PT_SLAVE_A)
+			other = hsr_port_get_hsr(from->hsr, HSR_PT_SLAVE_B);
+		else
+			other = hsr_port_get_hsr(from->hsr, HSR_PT_SLAVE_A);
+
+		if (hsr_addr_is_self(port->hsr,
+		    eth_hdr(frame->skb_hsr)->h_source)) {
+if (dbg_hsr < 10)
+dbg_msg(" S ");
+++dbg_hsr;
+			goto rx_drop;
+		}
+
+		/* Don't deliver locally unless we should */
+		if (!frame->is_local_dest)
+{
+dbg_msg("not local: %d\n", port->type);
+			goto rx_drop;
+}
+		dbg_frame_out = 4;
+
+		/* Don't send frame over port where it has been sent before */
+		if (hsr_register_frame_out(port, frame->node_src,
+					   frame->sequence_nr))
+		{
+#if 1
+if (dbg_hsr < 10) {
+	if (hsr_addr_is_self(port->hsr, eth_hdr(frame->skb_hsr)->h_source))
+dbg_msg(" self ");
+dbg_msg("saw before: %d %d %04x %d\n", from->type, port->type, frame->sequence_nr,
+frame->is_supervision);
+}
+++dbg_hsr;
+#endif
+
+			goto rx_drop;
+		}
+if (hsr_addr_is_self(port->hsr, eth_hdr(frame->skb_hsr)->h_source)) {
+if (dbg_hsr < 10)
+dbg_msg(" S ");
+}
+		info = container_of(from->hsr, struct ksz_hsr_info, hsr);
+
+		hsr_update_frame_out(other, frame->node_src,
+			frame->sequence_nr);
+		if (frame->is_supervision) {
+			hsr_handle_sup_frame(frame->skb_hsr,
+					     frame->node_src,
+					     frame->port_rcv);
+
+#ifdef CONFIG_HAVE_HSR_HW
+			if (!info->check) {
+				info->check = 1;
+				schedule_delayed_work(&info->chk_ring,
+						      msecs_to_jiffies(50));
+			}
+#endif
+			goto rx_drop;
+		}
+
+		/* Get the path id before the HSR tag is removed. */
+		path_id = get_hsr_tag_path(frame->hsr_tag);
+
+		skb = frame_get_stripped_skb(frame, port);
+		hsr_addr_subst_source(frame->node_src, skb);
+
+		/* No Redbox or Redbox is not up or no internal forwarding. */
+		if (!info->redbox || !info->redbox_up || !info->redbox_fwd)
+			return;
+		do {
+			struct ksz_sw *sw = info->sw_dev;
+
+			if (sw->info->forward & FWD_UCAST)
+				forward = 1;
+			else if (sw->info->forward & FWD_MCAST)
+				forward = 2;
+			else
+				forward = 0;
+		} while (0);
+#if 0
+		if (forward && (path_id >> 1) == info->hsr.redbox_id)
+			forward = 0;
+#endif
+#ifdef CONFIG_1588_PTP
+		/* Do not forward PTP messages. */
+		if (forward) {
+			struct ksz_sw *sw = info->sw_dev;
+
+			if (get_rx_tag_ptp(&sw->tag))
+				forward = 0;
+		}
+#endif
+		if (forward) {
+			struct sk_buff *new_skb;
+
+			if (1 == forward) {
+				new_skb = skb;
+				frame->skb_std = NULL;
+				frame->skb_hsr = NULL;
+			} else {
+				new_skb = skb_copy(skb, GFP_ATOMIC);
+			}
+			if (new_skb)
+				hsr_dev_xmit(info, info->redbox, new_skb);
+		}
+		return;
+
+	}
+tx_drop:
+	if (frame->skb_std) {
+		kfree_skb(frame->skb_std);
+		frame->skb_std = NULL;
+	}
+
+rx_drop:
+	if (frame->skb_hsr) {
+		kfree_skb(frame->skb_hsr);
+		frame->skb_hsr = NULL;
+	}
+}
+
+static void check_local_dest(struct hsr_priv *hsr, struct sk_buff *skb,
+			     struct hsr_frame_info *frame)
+{
+	if (hsr_addr_is_self(hsr, eth_hdr(skb)->h_dest)) {
+		frame->is_local_exclusive = true;
+		skb->pkt_type = PACKET_HOST;
+	} else {
+		frame->is_local_exclusive = false;
+	}
+
+	if ((skb->pkt_type == PACKET_HOST) ||
+	    (skb->data[0] & 0x01)) {
+		frame->is_local_dest = true;
+	} else {
+dbg_msg("not local_dest\n");
+		frame->is_local_dest = false;
+	}
+}
+
+static int hsr_fill_frame_info(struct hsr_frame_info *frame,
+			       struct sk_buff *skb, struct hsr_port *port)
+{
+	unsigned long irqflags;
+
+	frame->is_supervision = is_supervision_frame(port->hsr, skb);
+	frame->node_src = hsr_get_node(&port->hsr->node_db, skb,
+				       frame->is_supervision);
+	if (frame->node_src == NULL)
+		return -1; /* Unknown node and !is_supervision, or no mem */
+
+	/* Most likely this is a received frame. */
+	if (*frame->proto == htons(ETH_P_HSR)) {
+		frame->skb_std = NULL;
+		frame->skb_hsr = skb;
+		frame->sequence_nr = hsr_get_skb_sequence_nr(skb);
+
+	/* Received frame without HSR tag will not come here. */
+	} else {
+		frame->skb_std = skb;
+		frame->skb_hsr = NULL;
+		/* Sequence nr for the master node */
+		spin_lock_irqsave(&port->hsr->seqnr_lock, irqflags);
+		frame->sequence_nr = port->hsr->sequence_nr;
+		port->hsr->sequence_nr++;
+		spin_unlock_irqrestore(&port->hsr->seqnr_lock, irqflags);
+	}
+
+	frame->port_rcv = port;
+#if 1
+#ifdef CONFIG_KSZ_SWITCH
+	if (port->type != HSR_PT_MASTER)
+#endif
+		check_local_dest(port->hsr, skb, frame);
+#endif
+
+	return 0;
+}
+
+#ifdef CONFIG_KSZ_SWITCH
+static
+#endif
+int hsr_forward_skb(struct sk_buff *skb, struct hsr_port *port)
+{
+	struct hsr_frame_info _frame;
+	struct hsr_frame_info *frame = &_frame;
+
+	skb_reset_mac_header(skb);
+
+	/* Coming from host. */
+	if (port->type == HSR_PT_MASTER) {
+		struct vlan_ethhdr *vlan = (struct vlan_ethhdr *) skb->data;
+		u8 *proto;
+		u8 *hsr_tag;
+
+		memset(frame, 0, sizeof(struct hsr_frame_info));
+		proto = (u8 *) &vlan->h_vlan_proto;
+		hsr_tag = (u8 *) &vlan->h_vlan_TCI;
+		if (vlan->h_vlan_proto == htons(ETH_P_8021Q)) {
+			frame->is_vlan = true;
+			proto = (u8 *) &vlan->h_vlan_encapsulated_proto;
+			hsr_tag = (u8 *)(vlan + 1);
+		}
+		proto -= HSR_HLEN;
+		hsr_tag -= HSR_HLEN;
+		frame->proto = (u16 *) proto;
+		frame->hsr_tag = (struct hsr_tag *) hsr_tag;
+	} else {
+		struct ksz_hsr_info *info = container_of(port->hsr,
+			struct ksz_hsr_info, hsr);
+
+		frame = &info->frame;
+	}
+	if (hsr_fill_frame_info(frame, skb, port) < 0)
+		goto out_drop;
+	hsr_register_frame_in(frame->node_src, port, frame->sequence_nr);
+	hsr_forward_do(frame);
+
+	/* Frame is supervision or was dropped. */
+	if (!frame->skb_hsr && !frame->skb_std)
+{
+#if 0
+if (!frame->is_supervision)
+dbg_msg(" d ");
+#endif
+		return 0;
+}
+
+	/* Pass to upper layer or down. */
+	return 1;
+
+out_drop:
+dbg_msg("fwd drop: %02x:%02x:%02x:%02x:%02x:%02x  %02x:%02x:%02x:%02x:%02x:%02x\n",
+skb->data[0],
+skb->data[1],
+skb->data[2],
+skb->data[3],
+skb->data[4],
+skb->data[5],
+skb->data[6],
+skb->data[7],
+skb->data[8],
+skb->data[9],
+skb->data[10],
+skb->data[11]);
+#if 0
+	port->dev->stats.tx_dropped++;
+#endif
+	kfree_skb(skb);
+	return 0;
+}
+/* hsr_forward */
+
+#ifdef CONFIG_KSZ_SWITCH
+static void *check_hsr_frame(u8 *data, struct hsr_frame_info *frame)
+{
+	struct vlan_ethhdr *vlan = (struct vlan_ethhdr *) data;
+
+	frame->is_vlan = false;
+	if (vlan->h_vlan_proto == htons(ETH_P_8021Q)) {
+		frame->is_vlan = true;
+		frame->proto = &vlan->h_vlan_encapsulated_proto;
+		frame->hsr_tag = (struct hsr_tag *)(vlan + 1);
+		if (vlan->h_vlan_encapsulated_proto == htons(ETH_P_HSR))
+			return vlan + 1;
+	}
+
+	/* VLAN tag can be removed by the switch. */
+	if (vlan->h_vlan_proto == htons(ETH_P_HSR)) {
+		struct ethhdr *eth = (struct ethhdr *) data;
+
+		frame->proto = &vlan->h_vlan_proto;
+		frame->hsr_tag = (struct hsr_tag *)(eth + 1);
+		return eth + 1;
+	}
+	return NULL;
+}  /* check_hsr_frame */
+
+static int hsr_chk(struct ksz_hsr_info *info, struct sk_buff *skb, int port)
+{
+	struct vlan_ethhdr *vlan = (struct vlan_ethhdr *) skb->data;
+	struct ksz_sw *sw = info->sw_dev;
+	struct list_head *node_db;
+	struct sk_buff *new_skb;
+	struct hsr_node *node;
+	int forward;
+	int ret = 2;
+
+	if (!info->redbox)
+		return ret;
+
+	/* Stop processing if coming from HSR ports. */
+	if (port == info->ports[0] || port == info->ports[1])
+		return ret;
+
+	if (sw->info->forward & FWD_UCAST)
+		forward = 1;
+	else if (sw->info->forward & FWD_MCAST)
+		forward = 2;
+	else
+		forward = 0;
+
+	/* HSR device is not up. */
+	if (!info->hsr_up)
+		forward = 0;
+
+	node_db = &info->hsr.node_db;
+	node = find_node_by_AddrA(node_db, vlan->h_source);
+	if (!node) {
+		struct hsr_sup_type *hsr_stype;
+		struct hsr_sup_payload *hsr_sp;
+		struct hsr_sup_tag *hsr_stag = info->slave_hsr_stag;
+		struct hsr_port *master = hsr_port_get_hsr(&info->hsr,
+			HSR_PT_MASTER);
+
+		node = hsr_add_slave(node_db, vlan->h_source,
+			info->hsr.sequence_nr - 1);
+		if (!node)
+			return ret;
+
+		info->tx_frame = info->slave_sup_frame;
+		info->len = info->slave_len;
+		info->hsr_stag = info->slave_hsr_stag;
+
+		vlan = (struct vlan_ethhdr *) info->slave_sup_frame;
+
+		memcpy(vlan->h_source, node->MacAddressA, ETH_ALEN);
+
+		hsr_stype = (struct hsr_sup_type *)(hsr_stag + 1);
+		hsr_sp = (struct hsr_sup_payload *)(hsr_stype + 1);
+		ether_addr_copy(hsr_sp->MacAddressA, node->MacAddressA);
+#ifndef REDBOX_NOT_SEND_SUPERVISION
+		send_hsr_supervision_frame(master);
+#endif
+	} else {
+		struct hsr_port *port;
+
+		/* Remove the fixed entries in MAC table. */
+		if (!node->slave)
+			proc_hsr_cfg(info, node->MacAddressA, 0);
+		node->slave = 1;
+		node->time_in[HSR_PT_SLAVE_A] = jiffies;
+		node->time_in_stale[HSR_PT_SLAVE_A] = false;
+
+		/* Update so that sequence number may not be too far apart. */
+		port = hsr_port_get_hsr(&info->hsr, HSR_PT_SLAVE_A);
+		hsr_update_frame_out(port, node, info->hsr.sequence_nr - 1);
+	}
+
+	/* Do not forward internally. Need software bridge. */
+	if (!info->redbox_fwd)
+		return ret;
+	if (!forward)
+		return ret;
+
+	if (1 == forward) {
+		new_skb = skb;
+		ret = 0;
+	} else {
+		new_skb = skb_copy(skb, GFP_ATOMIC);
+		if (!new_skb)
+			return ret;
+	}
+
+#ifndef REDBOX_NOT_REPLACE_MAC_ADDR
+	/* Replace source MAC address. */
+	memcpy(&new_skb->data[6], info->src_addr, ETH_ALEN);
+#endif
+
+	/* Send frame to HSR device. */
+	hsr_dev_xmit(info, info->dev, new_skb);
+	return ret;
+}  /* hsr_chk */
+
+static void hsr_tx_proc(struct work_struct *work)
+{
+	bool last;
+	struct sk_buff *skb;
+	struct ksz_hsr_info *info = container_of(work, struct ksz_hsr_info,
+						 tx_proc);
+
+#ifdef CONFIG_KSZ_IBA
+	if (iba_stopped(info->sw_dev))
+		return;
+#endif
+
+	last = skb_queue_empty(&info->txq);
+	while (!last) {
+		skb = skb_dequeue(&info->txq);
+		last = skb_queue_empty(&info->txq);
+		if (!skb)
+			continue;
+		hsr_dev_xmit(info, info->redbox, skb);
+	}
+}  /* hsr_tx_proc */
+
+static void proc_hsr_tx(struct ksz_hsr_info *info, struct sk_buff *skb)
+{
+	skb_queue_tail(&info->txq, skb);
+	schedule_work(&info->tx_proc);
+}  /* proc_hsr_tx */
+
+static int hsr_fwd(struct ksz_hsr_info *info, struct sk_buff *skb)
+{
+	struct sk_buff *new_skb;
+	struct hsr_node *node;
+	struct list_head *node_db;
+	int forward = 0;
+	struct vlan_ethhdr *vlan = (struct vlan_ethhdr *) skb->data;
+
+	/* Multicast address is sent to both devices. */
+	if (vlan->h_dest[0] & 0x01) {
+		forward = 3;
+		goto fwd_next;
+	}
+
+	node_db = &info->hsr.node_db;
+	node = find_node_by_AddrA(node_db, vlan->h_dest);
+
+	/* Unicast address is not learned. */
+	if (!node) {
+#if 0
+dbg_msg(" %02x:%02x:%02x:%02x:%02x:%02x\n",
+vlan->h_dest[0],
+vlan->h_dest[1],
+vlan->h_dest[2],
+vlan->h_dest[3],
+vlan->h_dest[4],
+vlan->h_dest[5]);
+#endif
+		forward = 3;
+		goto fwd_next;
+	}
+	if (node->slave)
+		forward = 2;
+	else
+		forward = 1;
+
+fwd_next:
+	/* Multicast frame so need extra to send to eth1. */
+	if ((forward & 2) && forward != 2) {
+		new_skb = skb_copy(skb, GFP_ATOMIC);
+		if (new_skb)
+			proc_hsr_tx(info, new_skb);
+	}
+
+	/* Unicast frame to send to eth1. */
+	if (forward == 2) {
+		struct net_device **netdev = (struct net_device **)skb->cb;
+
+		*netdev = info->dev;
+	}
+	return forward;
+}  /* hsr_fwd */
+
+static int hsr_rcv(struct ksz_hsr_info *info, struct sk_buff *skb, int port)
+{
+	/* Accept only from port 1 or 2. */
+	if (port == info->ports[0])
+		port = 0;
+	else if (port == info->ports[1])
+		port = 1;
+	else
+		return 2;
+	if (check_hsr_frame(skb->data, &info->frame)) {
+		enum hsr_port_type pt;
+		struct hsr_port *from;
+
+		if (1 == port)
+			pt = HSR_PT_SLAVE_B;
+		else
+			pt = HSR_PT_SLAVE_A;
+		from = hsr_port_get_hsr(&info->hsr, pt);
+		return hsr_forward_skb(skb, from);
+	}
+	return 2;
+}  /* hsr_rcv */
+#endif
+
+/* hsr_slave */
+#ifdef CONFIG_KSZ_SWITCH
+static
+#endif
+void hsr_add_port(struct hsr_priv *hsr, struct net_device *dev,
+	enum hsr_port_type type)
+{
+	struct hsr_port *port;
+
+	port = hsr_port_get_hsr(hsr, type);
+
+	port->hsr = hsr;
+	port->dev = dev;
+	port->type = type;
+}
+
+/* Default multicast address for HSR Supervision frames */
+static const unsigned char def_multicast_addr[ETH_ALEN] __aligned(2) = {
+	0x01, 0x15, 0x4e, 0x00, 0x01, 0x00
+};
+
+static void prep_hsr_addr(struct ksz_hsr_info *info, u8 *src)
+{
+	struct hsr_sup_type *hsr_stype;
+	struct hsr_sup_payload *hsr_sp;
+	struct vlan_ethhdr *vlan = (struct vlan_ethhdr *)
+		info->master_sup_frame;
+
+	memcpy(info->src_addr, src, ETH_ALEN);
+
+	memcpy(vlan->h_source, src, ETH_ALEN);
+	hsr_stype = (struct hsr_sup_type *)(info->master_hsr_stag + 1);
+	hsr_sp = (struct hsr_sup_payload *)(hsr_stype + 1);
+	ether_addr_copy(hsr_sp->MacAddressA, info->src_addr);
+}  /* prep_hsr_addr */
+
+static void prep_hsr_redbox_addr(struct ksz_hsr_info *info)
+{
+	struct hsr_sup_type *hsr_stype;
+	struct hsr_sup_payload *hsr_sp;
+
+	if (!info->redbox)
+		return;
+	hsr_stype = (struct hsr_sup_type *)(info->slave_hsr_stag + 1);
+	hsr_sp = (struct hsr_sup_payload *)(hsr_stype + 1);
+	hsr_stype = (struct hsr_sup_type *)(hsr_sp + 1);
+	hsr_sp = (struct hsr_sup_payload *)(hsr_stype + 1);
+	ether_addr_copy(hsr_sp->MacAddressA, info->redbox->dev_addr);
+	ether_addr_copy(hsr_sp->MacAddressA, info->src_addr);
+}  /* prep_hsr_redbox_addr */
+
+static void hsr_change_addr(struct ksz_hsr_info *info, struct net_device *dev)
+{
+	struct ksz_sw *sw = info->sw_dev;
+
+	/* Do not do anything if device is not ready. */
+	if (!info->dev || !netif_running(dev))
+		return;
+	if (info->dev == dev) {
+		if (!memcmp(info->src_addr, dev->dev_addr, ETH_ALEN))
+			return;
+		sw->ops->cfg_mac(sw, BRIDGE_ADDR_ENTRY, info->src_addr, 0,
+			false, false, 0);
+		if (info->redbox_vlan)
+			sw->ops->cfg_mac(sw, DEV_1_ADDR_ENTRY, info->src_addr,
+					 0, false, false, 0);
+		prep_hsr_addr(info, dev->dev_addr);
+		sw->ops->cfg_mac(sw, BRIDGE_ADDR_ENTRY, info->src_addr,
+			sw->HOST_MASK, false, false, 0);
+		if (info->redbox_vlan)
+			sw->ops->cfg_mac(sw, DEV_1_ADDR_ENTRY, info->src_addr,
+					 sw->HOST_MASK, false, true,
+					 info->redbox_vlan);
+	} else {
+		prep_hsr_redbox_addr(info);
+	}
+}  /* hsr_change_addr */
+
+static void hsr_link_change(struct ksz_hsr_info *info, int link1, int link2)
+{
+	info->p1_down = !link1;
+	info->p2_down = !link2;
+	if (info->ring && (info->p1_down || info->p2_down)) {
+		info->ring = 0;
+		hsr_notify_link_lost(info);
+	}
+}  /* hsr_link_change */
+
+static u8 hsr_get_redbox_id(struct ksz_hsr_info *info)
+{
+	return info->hsr.redbox_id;
+}  /* hsr_get_redbox_id */
+
+static void hsr_set_redbox_id(struct ksz_hsr_info *info, u8 id)
+{
+	info->hsr.redbox_id = id;
+}  /* hsr_set_redbox_id */
+
+static u8 hsr_get_net_id(struct ksz_hsr_info *info)
+{
+	return info->hsr.net_id;
+}  /* hsr_get_net_id */
+
+static void hsr_set_net_id(struct ksz_hsr_info *info, u8 id)
+{
+	info->hsr.net_id = id;
+}  /* hsr_set_net_id */
+
+static void prep_hsr_mcast(struct net_device *dev)
+{
+	char addr[MAX_ADDR_LEN];
+
+	memcpy(addr, def_multicast_addr, ETH_ALEN);
+	dev_mc_add(dev, addr);
+}  /* prep_hsr_mcast */
+
+/* hsr_device */
+static void hsr_check_announce(struct ksz_hsr_info *info)
+{
+	int state;
+	struct hsr_priv *hsr = &info->hsr;
+
+	if (info->state < 0)
+		return;
+	state = netif_running(info->dev) && info->hsr_up;
+	if (state != info->state) {
+dbg_msg("%s %d %d\n", __func__, info->state, state);
+		if (state) {
+			hsr->announce_timer.expires = jiffies +
+				msecs_to_jiffies(1);
+			add_timer(&hsr->announce_timer);
+		} else
+			del_timer(&hsr->announce_timer);
+		info->state = state;
+	}
+}
+
+static void hsr_dev_init(struct ksz_hsr_info *info)
+{
+	struct hsr_priv *hsr = &info->hsr;
+
+	INIT_LIST_HEAD(&hsr->node_db);
+	INIT_LIST_HEAD(&hsr->self_node_db);
+
+	spin_lock_init(&hsr->seqnr_lock);
+
+	timer_setup(&hsr->announce_timer, hsr_announce, 0);
+
+	timer_setup(&hsr->prune_timer, hsr_prune_nodes, 0);
+
+	ether_addr_copy(hsr->sup_multicast_addr, def_multicast_addr);
+#ifdef CONFIG_KSZ_SWITCH
+	hsr->sup_multicast_addr[ETH_ALEN - 1] = 0;
+#endif
+
+	hsr_add_port(hsr, info->dev, HSR_PT_MASTER);
+
+	hsr_add_port(hsr, info->dev, HSR_PT_SLAVE_A);
+	hsr_add_port(hsr, info->dev, HSR_PT_SLAVE_B);
+
+#ifdef CONFIG_KSZ_SWITCH
+	info->state = -1;
+#endif
+}
+
+static int hsr_dev_finalize(struct ksz_hsr_info *info)
+{
+	struct hsr_priv *hsr = &info->hsr;
+	int res;
+
+	res = hsr_create_self_node(&hsr->self_node_db, info->dev->dev_addr,
+				   info->dev->dev_addr);
+	if (res < 0)
+		return res;
+
+	/* Overflow soon to find bugs easier: */
+	hsr->sequence_nr = HSR_SEQNR_START;
+	hsr->sup_sequence_nr = HSR_SEQNR_START;
+	hsr->redbox_id = 7;
+	hsr->net_id = 0;
+
+#if 1
+	msleep(HSR_NODE_REBOOT_INTERVAL);
+#endif
+#ifdef CONFIG_KSZ_SWITCH
+	/* Ready for announcement. */
+	info->state = 0;
+	hsr_check_announce(info);
+#endif
+
+	hsr->prune_timer.expires = jiffies + msecs_to_jiffies(PRUNE_PERIOD);
+	add_timer(&hsr->prune_timer);
+
+	return 0;
+}
+
+static void hsr_dev_destroy(struct ksz_hsr_info *info)
+{
+	struct hsr_priv *hsr = &info->hsr;
+	struct hsr_node *node;
+
+	node = list_first_or_null_rcu(&hsr->self_node_db, struct hsr_node,
+				      mac_list);
+	if (node) {
+		list_del_rcu(&node->mac_list);
+		kfree_rcu(node, rcu_head);
+	}
+	list_for_each_entry_rcu(node, &hsr->node_db, mac_list) {
+		list_del_rcu(&node->mac_list);
+		kfree_rcu(node, rcu_head);
+	}
+	del_timer_sync(&hsr->prune_timer);
+	del_timer_sync(&hsr->announce_timer);
+	info->state = -1;
+}
+
+static void setup_hsr(struct ksz_hsr_info *info, struct net_device *dev, int i)
+{
+	info->dev = dev;
+	info->hsr_index = i;
+	info->vid = 0;
+
+	hsr_dev_init(info);
+	prep_hsr_supervision_frame(info);
+	info->tx_frame = info->master_sup_frame;
+	info->len = info->master_len;
+	info->hsr_stag = info->master_hsr_stag;
+	prep_hsr_mcast(dev);
+
+	/* Redbox is setup first. */
+	if (info->redbox)
+		prep_hsr_supervision_slave_frame(info);
+}  /* setup_hsr */
+
+static void setup_hsr_redbox(struct ksz_hsr_info *info, struct net_device *dev,
+			     int i, bool fwd)
+{
+	info->redbox = dev;
+	info->redbox_index = i;
+	info->redbox_fwd = fwd;
+
+	/* Main HSR device is setup. */
+	if (info->dev)
+		prep_hsr_supervision_slave_frame(info);
+}  /* setup_hsr_redbox */
+
+static int hsr_get_attrib(struct ksz_hsr_info *info, int subcmd, int size,
+	int *req_size, size_t *len, u8 *data, int *output)
+{
+	struct hsr_node *node;
+	struct ksz_hsr_node *active;
+	struct hsr_priv *hsr = &info->hsr;
+	union hsr_data *attrib = (union hsr_data *) data;
+
+	*len = 0;
+	*output = 0;
+	switch (subcmd) {
+	case HSR_GET_NETWORK_STATUS:
+		*len = 1;
+		attrib->byte = info->ring;
+		break;
+	case HSR_GET_CAPABILITY_FLAGS:
+		*len = 4;
+		attrib->dword = info->cap;
+		break;
+	case HSR_GET_RING_PARTICIPANTS_COUNT:
+		*len = 2;
+		attrib->word = info->part_cnt;
+		break;
+	case HSR_GET_RING_PARTICIPANTS_LIST:
+		*len = sizeof(struct ksz_hsr_node);
+		*len *= info->part_cnt;
+		if (size < *len) {
+			break;
+		}
+		active = (struct ksz_hsr_node *) data;
+		rcu_read_lock();
+		list_for_each_entry_rcu(node, &hsr->node_db, mac_list) {
+			if (!node->slave) {
+				memcpy(&active->addr, node->MacAddressA,
+					ETH_ALEN);
+				active++;
+			}
+		}
+		rcu_read_unlock();
+		break;
+	}
+	if (!*len)
+		return DEV_IOC_INVALID_CMD;
+	if (size < *len) {
+		*req_size = *len + SIZEOF_ksz_request;
+		return DEV_IOC_INVALID_LEN;
+	}
+	return DEV_IOC_OK;
+}  /* hsr_get_attrib */
+
+static int hsr_dev_req(struct ksz_hsr_info *hsr, char *arg, void *info)
+{
+	struct ksz_request *req = (struct ksz_request *) arg;
+	int len;
+	int maincmd;
+	int req_size;
+	int subcmd;
+	int output;
+	size_t param_size;
+	u8 data[PARAM_DATA_SIZE];
+	struct ksz_resp_msg *msg = (struct ksz_resp_msg *) data;
+	int err = 0;
+	int result = 0;
+	struct ksz_sw *sw = hsr->sw_dev;
+
+	get_user_data(&req_size, &req->size, info);
+	get_user_data(&maincmd, &req->cmd, info);
+	get_user_data(&subcmd, &req->subcmd, info);
+	get_user_data(&output, &req->output, info);
+	len = req_size - SIZEOF_ksz_request;
+
+	maincmd &= 0xffff;
+	switch (maincmd) {
+	case DEV_CMD_INFO:
+		switch (subcmd) {
+		case DEV_INFO_INIT:
+			req_size = SIZEOF_ksz_request + 4;
+			if (len >= 4) {
+				data[0] = 'M';
+				data[1] = 'i';
+				data[2] = 'c';
+				data[3] = 'r';
+				data[4] = 0;
+				data[5] = hsr->member;
+				err = write_user_data(data, req->param.data,
+					6, info);
+				if (err)
+					goto dev_ioctl_done;
+			} else
+				result = DEV_IOC_INVALID_LEN;
+			break;
+		case DEV_INFO_EXIT:
+
+		/* fall through */
+		case DEV_INFO_QUIT:
+
+			/* Not called through char device. */
+			if (!info)
+				break;
+			file_dev_clear_notify(sw->dev_list[0], info,
+					      DEV_MOD_HSR,
+					      &hsr->notifications);
+			msg->module = DEV_MOD_HSR;
+			msg->cmd = DEV_INFO_QUIT;
+			msg->resp.data[0] = 0;
+			file_dev_setup_msg(info, msg, 8, NULL, NULL);
+			break;
+		case DEV_INFO_NOTIFY:
+			if (len >= 4) {
+				struct file_dev_info *dev_info = info;
+				uint *notify = (uint *) data;
+
+				_chk_ioctl_size(len, 4, 0, &req_size, &result,
+					&req->param, data, info);
+				dev_info->notifications[DEV_MOD_HSR] =
+					*notify;
+				hsr->notifications |= *notify;
+			}
+			break;
+		default:
+			result = DEV_IOC_INVALID_CMD;
+			break;
+		}
+		break;
+	case DEV_CMD_PUT:
+		if (_chk_ioctl_size(len, len, 0, &req_size, &result,
+		    &req->param, data, info))
+			goto dev_ioctl_resp;
+#if 0
+		result = hsr_set_attrib(hsr, subcmd, len, &req_size, data,
+			&output);
+		if (result)
+			goto dev_ioctl_resp;
+#endif
+		put_user_data(&output, &req->output, info);
+		break;
+	case DEV_CMD_GET:
+		result = hsr_get_attrib(hsr, subcmd, len, &req_size,
+			&param_size, data, &output);
+		if (result)
+			goto dev_ioctl_resp;
+		err = write_user_data(data, req->param.data, param_size, info);
+		if (err)
+			goto dev_ioctl_done;
+		req_size = param_size + SIZEOF_ksz_request;
+		put_user_data(&output, &req->output, info);
+		break;
+	default:
+		result = DEV_IOC_INVALID_CMD;
+		break;
+	}
+
+dev_ioctl_resp:
+	put_user_data(&req_size, &req->size, info);
+	put_user_data(&result, &req->result, info);
+
+	/* Return ERESTARTSYS so that the system call is called again. */
+	if (result < 0)
+		err = result;
+
+dev_ioctl_done:
+	return err;
+}  /* hsr_dev_req */
+
+static struct hsr_ops hsr_ops = {
+	.change_addr		= hsr_change_addr,
+	.link_change		= hsr_link_change,
+	.check_announce		= hsr_check_announce,
+	.get_redbox_id		= hsr_get_redbox_id,
+	.set_redbox_id		= hsr_set_redbox_id,
+	.get_net_id		= hsr_get_net_id,
+	.set_net_id		= hsr_set_net_id,
+
+	.dev_req		= hsr_dev_req,
+};
+
+static void prep_hsr(struct ksz_hsr_info *info, struct net_device *dev,
+	u8 *src)
+{
+	struct ksz_sw *sw = info->sw_dev;
+
+	info->dev = dev;
+	info->center = NULL;
+	info->seq_num = 0;
+	info->ring = 0;
+	prep_hsr_addr(info, src);
+	prep_hsr_redbox_addr(info);
+	if (info->vid) {
+		struct vlan_ethhdr *vlan = (struct vlan_ethhdr *)
+			info->master_sup_frame;
+
+		vlan->h_vlan_TCI = htons(info->vid);
+		vlan = (struct vlan_ethhdr *) info->slave_sup_frame;
+		vlan->h_vlan_TCI = htons(info->vid);
+	}
+	if (info->redbox) {
+		int i;
+
+		for (i = 0; i < sw->eth_cnt; i++) {
+			if (sw->eth_maps[i].proto & HSR_REDBOX) {
+				info->redbox_vlan = sw->eth_maps[i].vlan;
+				break;
+			}
+		}
+	}
+	sw->ops->cfg_mac(sw, BRIDGE_ADDR_ENTRY, info->src_addr, sw->HOST_MASK,
+			 false, false, 0);
+	if (info->redbox_vlan)
+		sw->ops->cfg_mac(sw, DEV_1_ADDR_ENTRY, info->src_addr,
+				 sw->HOST_MASK, false, true,
+				 info->redbox_vlan);
+	skb_queue_head_init(&info->txq);
+	INIT_WORK(&info->tx_proc, hsr_tx_proc);
+	hsr_dev_finalize(info);
+}  /* prep_hsr */
+
+static void sw_setup_hsr(struct ksz_sw *sw)
+{
+/*
+ * THa  2015/12/06
+ * The HSR register 0x640 needs to be set, even though the value read is 3.
+ * The NODE_UNICAST bit in register 0x644 needs to be turned off for multicast
+ * address to work.
+ * If HSR_LEARN_UCAST_DISABLE bit in register 0x645 is turned on, multicast
+ * address also does not work, even though HSR_LEARN_MCAST_DISABLE bit in
+ * register 0x644 can be used to control that.
+ */
+	SW_D data;
+	int n;
+	u16 mask = 0;
+
+	for (n = 0; n < sw->eth_cnt; n++) {
+		if (sw->eth_maps[n].proto & HSR_HW) {
+			mask = sw->eth_maps[n].mask;
+			break;
+		}
+	}
+	sw->reg->w32(sw, REG_HSR_PORT_MAP__4, mask);
+	data = SW_R(sw, REG_HSR_ALU_CTRL_0__1);
+	data &= ~HSR_NODE_UNICAST;
+	SW_W(sw, REG_HSR_ALU_CTRL_0__1, data);
+	if ((sw->overrides & HAVE_MORE_THAN_2_PORTS) && 1 == sw->eth_cnt)
+		sw_cfg_port_base_vlan(sw, sw->HOST_PORT, mask | sw->HOST_MASK);
+}  /* sw_setup_hsr */
+
+static void stop_hsr(struct ksz_hsr_info *info)
+{
+	hsr_dev_destroy(info);
+}
+
+static void start_hsr_redbox(struct ksz_hsr_info *info, struct net_device *dev)
+{
+	info->redbox_up = netif_carrier_ok(dev);
+}
+
+static void stop_hsr_redbox(struct ksz_hsr_info *info, struct net_device *dev)
+{
+	info->redbox_up = 0;
+}
+
+static void ksz_hsr_exit(struct ksz_hsr_info *info)
+{
+	struct sk_buff *skb;
+	bool last;
+
+	flush_work(&info->tx_proc);
+#ifdef CONFIG_HAVE_HSR_HW
+	cancel_delayed_work_sync(&info->chk_ring);
+#endif
+	last = skb_queue_empty(&info->txq);
+	while (!last) {
+		skb = skb_dequeue(&info->txq);
+		if (skb)
+			kfree_skb(skb);
+		last = skb_queue_empty(&info->txq);
+	}
+}  /* ksz_hsr_exit */
+
+static void ksz_hsr_init(struct ksz_hsr_info *info, struct ksz_sw *sw)
+{
+	info->state = -1;
+#ifdef CONFIG_HAVE_HSR_HW
+	info->cap = HSR_CAP_DUPLICATE_DISCARD;
+	info->cap |= HSR_CAP_REDBOX_CAPABLE;
+#endif
+	info->sw_dev = sw;
+	info->ops = &hsr_ops;
+
+#ifdef CONFIG_HAVE_HSR_HW
+	INIT_DELAYED_WORK(&info->chk_ring, hsr_chk_ring);
+#endif
+}  /* ksz_hsr_init */
+
diff -Napur linux-linux4microchip-2022.10-orig/drivers/net/ethernet/micrel/ksz_hsr.h linux-linux4microchip-2022.10/drivers/net/ethernet/micrel/ksz_hsr.h
--- linux-linux4microchip-2022.10-orig/drivers/net/ethernet/micrel/ksz_hsr.h	1969-12-31 16:00:00.000000000 -0800
+++ linux-linux4microchip-2022.10/drivers/net/ethernet/micrel/ksz_hsr.h	2023-07-24 16:54:01.968055719 -0700
@@ -0,0 +1,106 @@
+/**
+ * Microchip HSR driver header
+ *
+ * Copyright (c) 2016-2023 Microchip Technology Inc.
+ *	Tristram Ha <Tristram.Ha@microchip.com>
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ */
+
+
+#ifndef KSZ_HSR_H
+#define KSZ_HSR_H
+
+#include "hsr_main.h"
+#include "ksz_hsr_api.h"
+
+
+struct hsr_frame_info {
+	struct sk_buff *skb_std;
+	struct sk_buff *skb_hsr;
+	struct hsr_port *port_rcv;
+	struct hsr_node *node_src;
+	u16 sequence_nr;
+	u16 *proto;
+	struct hsr_tag *hsr_tag;
+	bool is_supervision;
+	bool is_vlan;
+	bool is_local_dest;
+	bool is_local_exclusive;
+};
+
+
+struct ksz_hsr_info;
+
+struct hsr_ops {
+	void (*change_addr)(struct ksz_hsr_info *info, struct net_device *dev);
+	void (*link_change)(struct ksz_hsr_info *info, int link1, int link2);
+	void (*check_announce)(struct ksz_hsr_info *info);
+	u8 (*get_redbox_id)(struct ksz_hsr_info *info);
+	void (*set_redbox_id)(struct ksz_hsr_info *info, u8 id);
+	u8 (*get_net_id)(struct ksz_hsr_info *info);
+	void (*set_net_id)(struct ksz_hsr_info *info, u8 id);
+
+	int (*dev_req)(struct ksz_hsr_info *hsr, char *arg, void *info);
+
+};
+
+
+struct ksz_hsr_info {
+	u16 vid;
+	u8 src_addr[ETH_ALEN];
+
+	void *sw_dev;
+	struct net_device *dev;
+	struct net_device *redbox;
+	struct hsr_priv hsr;
+	struct hsr_port hsr_ports[HSR_PT_PORTS];
+	struct hsr_node *center;
+
+	u8 master_sup_frame[80];
+	u8 slave_sup_frame[80];
+	u8 *tx_frame;
+	struct hsr_sup_tag *hsr_stag;
+	struct hsr_sup_tag *master_hsr_stag;
+	struct hsr_sup_tag *slave_hsr_stag;
+	struct hsr_frame_info frame;
+	int master_len;
+	int slave_len;
+	int len;
+	int state;
+	u8 ports[2];
+	u8 hsr_index;
+	u8 redbox_index;
+	u32 cap;
+	u16 member;
+	u16 part_cnt;
+	struct sk_buff_head txq;
+	struct work_struct tx_proc;
+	struct delayed_work chk_ring;
+	u16 redbox_vlan;
+	u16 seq_num;
+	u16 check:1;
+	u16 ring:1;
+	u16 p1_down:1;
+	u16 p2_down:1;
+	u16 p1_lost:1;
+	u16 p2_lost:1;
+	u16 redbox_fwd:1;
+	u16 redbox_up:1;
+	u16 hsr_up:1;
+
+	uint notifications;
+
+	uint overrides;
+
+	const struct hsr_ops *ops;
+};
+
+#endif
diff -Napur linux-linux4microchip-2022.10-orig/drivers/net/ethernet/micrel/ksz_iba.c linux-linux4microchip-2022.10/drivers/net/ethernet/micrel/ksz_iba.c
--- linux-linux4microchip-2022.10-orig/drivers/net/ethernet/micrel/ksz_iba.c	1969-12-31 16:00:00.000000000 -0800
+++ linux-linux4microchip-2022.10/drivers/net/ethernet/micrel/ksz_iba.c	2023-11-09 10:06:38.044987949 -0800
@@ -0,0 +1,3221 @@
+/**
+ * Microchip IBA code
+ *
+ * Copyright (c) 2015-2023 Microchip Technology Inc.
+ *	Tristram Ha <Tristram.Ha@microchip.com>
+ *
+ * Copyright (c) 2013-2015 Micrel, Inc.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ */
+
+
+#ifdef DEBUG
+#if 0
+#define SHOW_IBA
+#endif
+#endif
+
+#if 1
+#define RETRY_IBA
+#endif
+
+#define ETH_P_IBA			IBA_TAG_TYPE
+
+#define VALID_IBA_VAL			\
+	(SW_IBA_ENABLE | SW_IBA_DA_MATCH | SW_IBA_INIT |		\
+	 (SW_IBA_PORT_M << SW_IBA_PORT_S) | SW_IBA_FRAME_TPID_M)
+
+static void prepare_iba(struct ksz_iba_info *iba, u8 *dst, u8 *src)
+{
+#ifndef CAPTURE_IBA
+	if (iba->dst != dst)
+		memcpy(iba->dst, dst, ETH_ALEN);
+	if (iba->src != src)
+		memcpy(iba->src, src, ETH_ALEN);
+#endif
+	memcpy(iba->packet, iba->dst, ETH_ALEN);
+	memcpy(&iba->packet[ETH_ALEN], iba->src, ETH_ALEN);
+
+	iba->frame->tag.type = htons(iba->tag_type);
+	iba->frame->tag.prio = 0;
+	iba->frame->tag.cfi = 0;
+	iba->frame->tag.mode = 1;
+	iba->frame->format.format = htons(IBA_FORMAT_KSZ98XX);
+	iba->frame->format.reserved = 0;
+
+	iba->cmds[0].cmd = 0;
+}  /* prepare_iba */
+
+static void *iba_command(void *frame, int *size, u32 cmd, int cnt, u32 *data)
+{
+	struct iba_cmd *iba = frame;
+	int i;
+	int len = 4;
+	int final_len = *size + sizeof(u32) * cnt + 8;
+
+	if (final_len > IBA_LEN_MAX && cmd) {
+		cnt = (IBA_LEN_MAX - *size - 8) / (int) sizeof(u32);
+		if (cnt > 0)
+			data[0] = cnt - 1;
+		else
+			cmd = 0;
+dbg_msg(" command: %d"NL, cnt);
+	}
+
+	iba->cmd = htonl(cmd);
+	for (i = 0; i < cnt; i++) {
+		iba->data[i] = htonl(data[i]);
+		len += 4;
+	}
+	frame = &iba->data[i];
+	*size += len;
+	return frame;
+}  /* iba_command */
+
+static unsigned long last_iba_jiffies;
+static int dbg_iba;
+static int last_ok_iba;
+static int last_ok_reg;
+
+static void *iba_pre_cmd(struct ksz_iba_info *info, u16 code)
+{
+	struct iba_frame *iba = info->frame;
+
+if (info->respid != info->seqid) {
+dbg_msg(" pre %x %x; %x; %x %x"NL, info->respid, info->seqid, code,
+last_iba_jiffies, jiffies);
+dbg_iba = 1;
+}
+last_iba_jiffies = jiffies;
+	info->index = 0;
+	info->len = sizeof(struct iba_frame) - sizeof(struct iba_cmd) +
+		ETH_ALEN * 2;
+	iba->code = htons(code);
+	return &iba->cmd;
+}  /* iba_pre_cmd */
+
+static u32 iba_get_val(u32 size, u32 val)
+{
+	int shift;
+
+	switch (size) {
+	case IBA_CMD_32:
+		break;
+	case IBA_CMD_16:
+		val >>= 16;
+		break;
+	case IBA_CMD_16_M:
+		val >>= 8;
+		val &= 0xffff;
+		break;
+	case IBA_CMD_16_H:
+		val &= 0xffff;
+		break;
+	case IBA_CMD_24:
+		val >>= 8;
+		break;
+	case IBA_CMD_24_H:
+		val &= 0xffffff;
+		break;
+	default:
+		switch (size) {
+		case IBA_CMD_BYTE_0:
+			shift = 3;
+			break;
+		case IBA_CMD_BYTE_1:
+			shift = 2;
+			break;
+		case IBA_CMD_BYTE_2:
+			shift = 1;
+			break;
+		default:
+			shift = 0;
+			break;
+		}
+		val >>= shift * 8;
+		val &= 0xff;
+	}
+	return val;
+}  /* iba_get_val */
+
+static u32 iba_set_size(u32 addr, u32 size)
+{
+	switch (size) {
+	case IBA_CMD_8:
+		size = IBA_CMD_8 >> (addr & 3);
+		break;
+	case IBA_CMD_16:
+		if (addr & 2)
+			size = IBA_CMD_16 >> 2;
+		else if (addr & 1)
+			size = IBA_CMD_16 >> 1;
+		break;
+	case IBA_CMD_24:
+		size = IBA_CMD_24 >> (addr & 1);
+		break;
+	}
+	return size;
+}  /* iba_set_size */
+
+static u32 iba_set_val(u32 size, u32 addr, u32 val)
+{
+	switch (size) {
+	case IBA_CMD_8:
+		val &= 0xff;
+		val <<= (3 - (addr & 3)) * 8;
+		break;
+	case IBA_CMD_16:
+		val &= 0xffff;
+		if (!(addr & 2)) {
+			if (addr & 1)
+				val <<= 1 * 8;
+			else
+				val <<= 2 * 8;
+		}
+		break;
+	case IBA_CMD_24:
+		val &= 0xffffff;
+		val <<= (1 - (addr & 1)) * 8;
+		break;
+	}
+	return val;
+}  /* iba_set_val */
+
+#if 0
+/**
+ * iba_chk_regs - IBA register check
+ * @sw:		The switch instance.
+ * @cmds:	The IBA command.
+ * @data:	The IBA data.
+ *
+ * This routine checks the value written to specific registers to determine
+ * the correct tail tag length to use.
+ */
+static void iba_chk_regs(struct ksz_sw *sw, u32 cmds, u32 data)
+{
+	u32 port_reg;
+	u32 reg = cmds & IBA_CMD_ADDR_M;
+	u32 size = cmds & IBA_CMD_32;
+	u32 val = iba_get_val(size, data);
+	int need_ptp_tag = 0;
+	int need_tail_tag = 0;
+
+	if (IBA_CMD_BYTE_1 == size && (REG_PTP_MSG_CONF1 + 1) == reg) {
+		need_ptp_tag++;
+		if (val & PTP_ENABLE)
+			need_ptp_tag++;
+	} else if (IBA_CMD_16 == size && REG_PTP_MSG_CONF1 == reg) {
+		need_ptp_tag++;
+		if (val & PTP_ENABLE)
+			need_ptp_tag++;
+	} else if (IBA_CMD_32 == size && REG_PTP_MSG_CONF1 == reg) {
+		need_ptp_tag++;
+		if (val & (PTP_ENABLE << 16))
+			need_ptp_tag++;
+	}
+
+/* KSZ9567 S1 needs to have PTP tag all the time. */
+#if 1
+	if (!(sw->features & NEW_CAP) &&
+	    sw->TAIL_TAG_LOOKUP >= 0x100 && 1 == need_ptp_tag)
+		need_ptp_tag = 2;
+#endif
+	if (2 == need_ptp_tag)
+		sw->overrides |= PTP_TAG;
+	else if (1 == need_ptp_tag)
+		sw->overrides &= ~PTP_TAG;
+
+	port_reg = PORT_CTRL_ADDR(sw->HOST_PORT, REG_PORT_CTRL_0);
+	if (IBA_CMD_BYTE_0 == size && port_reg == reg) {
+		need_tail_tag++;
+		if (val & PORT_TAIL_TAG_ENABLE)
+			need_tail_tag++;
+	} else if (IBA_CMD_16 == size && port_reg == reg) {
+		need_tail_tag++;
+		if (val & (PORT_TAIL_TAG_ENABLE << 8))
+			need_tail_tag++;
+	} else if (IBA_CMD_32 == size && port_reg == reg) {
+		need_tail_tag++;
+		if (val & (PORT_TAIL_TAG_ENABLE << 24))
+			need_tail_tag++;
+	}
+	if (2 == need_tail_tag)
+		sw->overrides |= TAIL_TAGGING;
+	else if (1 == need_tail_tag)
+		sw->overrides &= ~TAIL_TAGGING;
+}  /* iba_chk_regs */
+#endif
+
+static u32 last_iba_addr;
+
+static void *iba_cmd_data(struct ksz_iba_info *info, u32 cmd, u32 size,
+	u32 addr)
+{
+	int cnt = 1;
+	int shift = IBA_CMD_S;
+	struct iba_frame *iba = info->frame;
+
+	if (iba->code == htons(IBA_CODE_BURST)) {
+		cnt = info->data[0] + 1;
+		shift = IBA_BURST_S;
+	} else {
+		if (IBA_CMD_16 == size && 3 == (addr & 3))
+			pr_info("16-bit used with register ended with 3"NL);
+
+		/* write can be 8-bit, 16-bit, 24-bit, or 32-bit. */
+		if (IBA_CMD_READ != cmd) {
+			if (IBA_CMD_WRITE == cmd &&
+			    REG_SW_IBA__4 <= addr && addr < REG_SW_IBA__4 + 4) {
+				int shift;
+				u32 mask;
+
+				shift = addr - REG_SW_IBA__4;
+				mask = VALID_IBA_VAL;
+				if (size == IBA_CMD_8)
+					shift = 3 - shift;
+				else if (size == IBA_CMD_16)
+					shift = 2 - shift;
+				shift *= 8;
+				mask >>= shift;
+				if (size == IBA_CMD_8)
+					mask &= 0xff;
+				else if (size == IBA_CMD_16)
+					mask &= 0xffff;
+				if (((info->cfg >> shift) & mask) !=
+				    (info->data[0] & mask)) {
+#ifndef CONFIG_KSZ_IBA_ONLY
+					info->use_iba |=
+						IBA_USE_CODE_NO_WAIT |
+						IBA_USE_CODE_TURN_OFF;
+#else
+					/* Do not allow changing IBA port. */
+					info->data[0] = info->cfg;
+					addr = REG_SW_IBA__4;
+					size = IBA_CMD_32;
+#endif
+				}
+			}
+			info->data[0] = iba_set_val(size, addr, info->data[0]);
+		}
+		size = iba_set_size(addr, size);
+	}
+	cmd <<= shift;
+	cmd |= size;
+	cmd |= addr & IBA_CMD_ADDR_M;
+	last_iba_addr = addr;
+	info->cmds[info->index].data[0] = info->data[0];
+	info->cmds[info->index++].cmd = cmd;
+	info->fptr = iba_command(info->fptr, &info->len, cmd, cnt, info->data);
+	if (info->len + 4 >= IBA_LEN_MAX && iba->code == htons(IBA_CODE_BURST))
+		info->cmds[info->index - 1].data[0] = info->data[0];
+	return info->fptr;
+}  /* iba_cmd_data */
+
+static void *iba_post_cmd(struct ksz_iba_info *info)
+{
+	struct iba_frame *iba = info->frame;
+
+if (info->respid != info->seqid) {
+dbg_msg(" post %x %x"NL, info->respid, info->seqid);
+dbg_iba = 1;
+}
+	info->cmds[info->index++].cmd = 0;
+	info->fptr = iba_command(info->fptr, &info->len, 0, 0, NULL);
+	iba->tag.seqid = ++info->seqid;
+	iba->length = htons(info->len);
+	return info->fptr;
+}  /* iba_post_cmd */
+
+/**
+ * iba_xmit - Transmit IBA request.
+ * @info:	The IBA instance.
+ *
+ * This function prepares IBA request for transmit.
+ */
+static int iba_xmit(struct ksz_iba_info *info)
+{
+	int len = ntohs(info->frame->length);
+	int rc;
+	struct net_device *netdev = info->dev;
+	const struct net_device_ops *ops = netdev->netdev_ops;
+	struct sk_buff *skb;
+	int timeout = 5;
+
+	if (len < 60) {
+		memset(&info->packet[len], 0, 60 - len);
+		len = 60;
+	}
+
+	/* Add extra for tail tagging. */
+	skb = dev_alloc_skb(len + 4 + 8);
+	if (!skb)
+		return -ENOMEM;
+
+	memcpy(skb->data, info->packet, len);
+	skb_put(skb, len);
+	skb->protocol = htons(ETH_P_IBA);
+	skb->dev = netdev;
+	do {
+		struct ksz_sw *sw = info->sw_dev;
+
+		rc = ops->ndo_start_xmit(skb, skb->dev);
+		if (NETDEV_TX_BUSY == rc) {
+			rc = wait_event_interruptible_timeout(sw->queue,
+				!netif_queue_stopped(netdev),
+				50 * HZ / 1000);
+
+			rc = NETDEV_TX_BUSY;
+		}
+	} while (NETDEV_TX_BUSY == rc && timeout--);
+	return rc;
+}  /* iba_xmit */
+
+#ifdef VERIFY_IBA
+static void prepare_cmd(struct ksz_iba_info *info, int message)
+{
+	memset(info->data, 0, sizeof(u32) * IBA_BURST_CNT_MAX);
+	switch (message) {
+	case 0:
+		info->fptr = iba_pre_cmd(info, IBA_CODE_NORMAL);
+		info->data[0] = 0;
+		info->fptr = iba_cmd_data(info, IBA_CMD_READ,
+			IBA_CMD_32, 0);
+		info->fptr = iba_cmd_data(info, IBA_CMD_READ,
+			IBA_CMD_32, 4);
+		break;
+	case 1:
+		info->fptr = iba_pre_cmd(info, IBA_CODE_NORMAL);
+		info->data[0] = 0;
+		info->fptr = iba_cmd_data(info, IBA_CMD_WRITE,
+			IBA_CMD_32, REG_PTP_UNIT_INDEX__4);
+		info->data[0] = TS_RESET;
+		info->fptr = iba_cmd_data(info, IBA_CMD_WRITE,
+			IBA_CMD_32, REG_PTP_CTRL_STAT__4);
+		info->data[0] = TS_RESET;
+		info->fptr = iba_cmd_data(info, IBA_CMD_WAIT_ON_0,
+			IBA_CMD_32, REG_PTP_CTRL_STAT__4);
+		info->data[0] = TS_ENABLE;
+		info->fptr = iba_cmd_data(info, IBA_CMD_WRITE,
+			IBA_CMD_32, REG_PTP_CTRL_STAT__4);
+		info->data[0] = TS_ENABLE;
+		info->fptr = iba_cmd_data(info, IBA_CMD_WAIT_ON_1,
+			IBA_CMD_32, REG_PTP_CTRL_STAT__4);
+		info->data[0] = 0;
+		info->fptr = iba_cmd_data(info, IBA_CMD_READ,
+			IBA_CMD_32, REG_PTP_CTRL_STAT__4);
+		break;
+	case 2:
+		info->fptr = iba_pre_cmd(info, IBA_CODE_NORMAL);
+		info->data[0] = 0xB;
+		info->data[0] <<= MIB_COUNTER_INDEX_S;
+		info->data[0] |= MIB_COUNTER_READ;
+		info->fptr = iba_cmd_data(info, IBA_CMD_WRITE,
+			IBA_CMD_32,
+			PORT_CTRL_ADDR(0, REG_PORT_MIB_CTRL_STAT__4));
+		info->data[0] = 0;
+		info->fptr = iba_cmd_data(info, IBA_CMD_READ,
+			IBA_CMD_32,
+			PORT_CTRL_ADDR(0, REG_PORT_MIB_CTRL_STAT__4));
+		info->fptr = iba_cmd_data(info, IBA_CMD_READ,
+			IBA_CMD_32,
+			PORT_CTRL_ADDR(0, REG_PORT_MIB_DATA));
+		break;
+	case 3:
+		info->fptr = iba_pre_cmd(info, IBA_CODE_BURST);
+		info->data[0] = 2;
+		info->data[1] = 0x12340000;
+		info->data[2] = 0x56780000;
+		info->fptr = iba_cmd_data(info, IBA_BURST_WRITE,
+			IBA_CMD_32, REG_PTP_CTRL_STAT__4);
+		info->data[0] = 4;
+		info->data[1] = 0x12340000;
+		info->data[2] = 0x56780000;
+		info->data[3] = 0x00001234;
+		info->data[4] = 0x00005678;
+		info->fptr = iba_cmd_data(info, IBA_BURST_WRITE,
+			IBA_CMD_32, REG_PTP_CTRL_STAT__4);
+		info->data[0] = 0;
+		info->fptr = iba_cmd_data(info, IBA_BURST_READ,
+			IBA_CMD_32, REG_PTP_CTRL_STAT__4);
+		info->data[0] = 60;
+		info->fptr = iba_cmd_data(info, IBA_BURST_READ,
+			IBA_CMD_32, REG_PTP_CTRL_STAT__4);
+		if (info->len + 4 >= IBA_LEN_MAX)
+			break;
+		info->data[0] = 0;
+		info->fptr = iba_cmd_data(info, IBA_BURST_READ,
+			IBA_CMD_32, REG_PTP_CTRL_STAT__4);
+		break;
+	case 4:
+#if 1
+		info->fptr = iba_pre_cmd(info, IBA_CODE_NORMAL);
+#else
+		info->fptr = iba_pre_cmd(info, 4);
+#endif
+		info->data[0] = 0;
+		info->fptr = iba_cmd_data(info, IBA_CMD_READ,
+			IBA_CMD_32, REG_PTP_INT_STATUS__4);
+		info->data[0] = 2;
+		info->fptr = iba_cmd_data(info, IBA_CMD_WRITE,
+			IBA_CMD_32, REG_PTP_INT_STATUS__4);
+		info->data[0] = 2;
+		info->fptr = iba_cmd_data(info, IBA_CMD_WAIT_ON_0,
+			IBA_CMD_32, REG_PTP_INT_STATUS__4);
+#if 0
+		info->data[0] = 2;
+		info->fptr = iba_cmd_data(info, IBA_CMD_WAIT_ON_1,
+			IBA_CMD_32, REG_PTP_INT_STATUS__4);
+#endif
+		info->data[0] = 0;
+		info->fptr = iba_cmd_data(info, IBA_CMD_READ,
+			IBA_CMD_32, REG_PTP_INT_STATUS__4);
+#if 0
+		info->fptr = iba_cmd_data(info, 3,
+			IBA_CMD_32, REG_PTP_INT_STATUS__4);
+#endif
+		break;
+	case 5:
+		info->fptr = iba_pre_cmd(info, IBA_CODE_BURST);
+		info->data[0] = 63;
+		info->fptr = iba_cmd_data(info, IBA_BURST_READ,
+			IBA_CMD_32, REG_PTP_CTRL_STAT__4);
+		break;
+	case 6:
+		info->fptr = iba_pre_cmd(info, IBA_CODE_NORMAL);
+#if 1
+		info->data[0] = 0x1;
+		info->fptr = iba_cmd_data(info, IBA_CMD_WAIT_ON_0, IBA_CMD_32,
+			0x300);
+#endif
+		info->data[0] = 0x00800000;
+		info->fptr = iba_cmd_data(info, IBA_CMD_WAIT_ON_1, IBA_CMD_32,
+			0x00);
+		info->data[0] = 0x10000000;
+		info->fptr = iba_cmd_data(info, IBA_CMD_WAIT_ON_0, IBA_CMD_32,
+			0x300);
+		info->data[0] = 0x10000000;
+		info->fptr = iba_cmd_data(info, IBA_CMD_WAIT_ON_0, IBA_CMD_32,
+			0x304);
+		info->data[0] = 0x8;
+		info->fptr = iba_cmd_data(info, IBA_CMD_WAIT_ON_1, IBA_CMD_32,
+			0x304);
+		break;
+	case 7:
+		info->fptr = iba_pre_cmd(info, IBA_CODE_NORMAL);
+#if 0
+		info->data[0] = 0;
+		info->fptr = iba_cmd_data(info, IBA_CMD_READ, IBA_CMD_32,
+			0x300);
+		info->data[0] = 0;
+		info->fptr = iba_cmd_data(info, IBA_CMD_READ, IBA_CMD_32,
+			0x300);
+#endif
+		info->data[0] = 0;
+		info->fptr = iba_cmd_data(info, IBA_CMD_READ, IBA_CMD_32,
+			0x304);
+		info->data[0] = 0x10000001;
+		info->fptr = iba_cmd_data(info, IBA_CMD_WRITE_1, IBA_CMD_32,
+			0x304);
+		info->data[0] = 0;
+		info->fptr = iba_cmd_data(info, IBA_CMD_READ, IBA_CMD_32,
+			0x304);
+		info->data[0] = 0x10000001;
+		info->fptr = iba_cmd_data(info, IBA_CMD_WRITE_0, IBA_CMD_32,
+			0x304);
+		info->data[0] = 0;
+		info->fptr = iba_cmd_data(info, IBA_CMD_READ, IBA_CMD_32,
+			0x304);
+		info->data[0] = 0x10000001;
+		info->fptr = iba_cmd_data(info, IBA_CMD_WRITE_1, IBA_CMD_32,
+			0x304);
+		info->data[0] = 0;
+		info->fptr = iba_cmd_data(info, IBA_CMD_READ, IBA_CMD_32,
+			0x304);
+		info->data[0] = 0x10000001;
+		info->fptr = iba_cmd_data(info, IBA_CMD_WRITE_0, IBA_CMD_32,
+			0x304);
+		break;
+	case 8:
+		info->fptr = iba_pre_cmd(info, IBA_CODE_NORMAL);
+#if 0
+		info->data[0] = 0;
+		info->fptr = iba_cmd_data(info, IBA_CMD_READ, IBA_CMD_32,
+			0x300);
+#endif
+		info->data[0] = 0;
+		info->fptr = iba_cmd_data(info, IBA_CMD_READ, IBA_CMD_32,
+			0x300);
+		info->data[0] = 0;
+		info->fptr = iba_cmd_data(info, IBA_CMD_READ, IBA_CMD_32,
+			0x304);
+		info->data[0] = 0x10000001;
+		info->fptr = iba_cmd_data(info, IBA_CMD_WRITE_1, IBA_CMD_32,
+			0x304);
+		info->data[0] = 0;
+		info->fptr = iba_cmd_data(info, IBA_CMD_READ, IBA_CMD_32,
+			0x304);
+		info->data[0] = 0x10000001;
+		info->fptr = iba_cmd_data(info, IBA_CMD_WRITE_0, IBA_CMD_32,
+			0x304);
+		info->data[0] = 0;
+		info->fptr = iba_cmd_data(info, IBA_CMD_READ, IBA_CMD_32,
+			0x304);
+		info->data[0] = 0x10000001;
+		info->fptr = iba_cmd_data(info, IBA_CMD_WRITE_1, IBA_CMD_32,
+			0x304);
+		info->data[0] = 0;
+		info->fptr = iba_cmd_data(info, IBA_CMD_READ, IBA_CMD_32,
+			0x304);
+		info->data[0] = 0x10000001;
+		info->fptr = iba_cmd_data(info, IBA_CMD_WRITE_0, IBA_CMD_32,
+			0x304);
+		break;
+	default:
+		info->fptr = iba_pre_cmd(info, IBA_CODE_NORMAL);
+		info->fptr = iba_cmd_data(info, IBA_BURST_READ,
+			IBA_CMD_32, REG_PTP_CTRL_STAT__4);
+	}
+	info->fptr = iba_post_cmd(info);
+}
+
+static int dbg_iba_test;
+static int iba_test(struct ksz_iba_info *info, int n)
+{
+	int k;
+	int rc;
+	unsigned long wait;
+
+	prepare_cmd(info, n);
+	info->regs[0].cmd = (u32) -1;
+	init_completion(&info->done);
+	rc = iba_xmit(info);
+	if (rc) {
+		dbg_msg("send err: %d"NL, rc);
+		return rc;
+	}
+	wait = wait_for_completion_timeout(&info->done, info->delay_ticks);
+	dbg_msg("wait: %lx"NL, wait);
+
+	k = 0;
+	while (info->regs[k].cmd != (u32) -1) {
+		dbg_msg("%08x=%08x"NL, info->regs[k].cmd,
+			info->regs[k].data[0]);
+		k++;
+	}
+#ifndef TEST_IBA
+	do {
+		struct ksz_sw *sw = info->sw_dev;
+		u32 status;
+
+if (mutex_is_locked(sw->reglock))
+printk(" reg locked"NL);
+		mutex_lock(sw->reglock);
+		info->use_iba = 0;
+		status = sw->old->r32(sw, REG_SW_IBA_STATUS__4);
+		dbg_msg("status %08x q:%d p:%d d:%d f:%d o:%d m:%d"NL, status,
+			!!(status & SW_IBA_REQ),
+			!!(status & SW_IBA_RESP),
+			!!(status & SW_IBA_DA_MISMATCH),
+			!!(status & SW_IBA_FMT_MISMATCH),
+			!!(status & SW_IBA_CODE_ERROR),
+			!!(status & SW_IBA_CMD_ERROR));
+		status = sw->old->r32(sw, REG_SW_IBA_STATES__4);
+		dbg_msg("states %08x %u"NL, status,
+			((status >> SW_IBA_PACKET_SIZE_S) &
+			SW_IBA_PACKET_SIZE_M) * 4);
+		status = sw->old->r32(sw, REG_SW_IBA_RESULT__4);
+		dbg_msg("result %08x %u"NL, status, status >> SW_IBA_SIZE_S);
+		info->use_iba = 1;
+		mutex_unlock(sw->reglock);
+	} while (0);
+#endif
+
+	return 0;
+}
+#endif
+
+static struct ksz_iba_info *iba_info;
+
+static void iba_lock(struct ksz_sw *sw)
+{
+	const struct ksz_sw_reg_ops *reg_ops = sw->reg;
+
+	mutex_lock(sw->hwlock);
+	++sw->info->iba.cnt;
+	if (sw->reg != reg_ops) {
+printk(KERN_ALERT "%s changed"NL, __func__);
+dbg_msg("  %s changed"NL, __func__);
+		mutex_unlock(sw->hwlock);
+		sw->reg->lock(sw);
+	}
+}  /* iba_lock */
+
+static void iba_unlock(struct ksz_sw *sw)
+{
+	if (sw->info->iba.cnt)
+		--sw->info->iba.cnt;
+	else
+printk("wrong release"NL);
+	mutex_unlock(sw->hwlock);
+}  /* iba_unlock */
+
+/**
+ * This helper function is used to prepare the read registers for use with
+ * the iba_r_post function.
+ */
+static u32 *iba_prepare_data(u32 reg, u32 *data)
+{
+	data[0] = reg;
+	if (-1 == reg)
+		return data + 1;
+	data[1] = 0xdeadfeed;
+	return data + 2;
+}  /* iba_prepare_data */
+
+/**
+ * This helper routine is used to check the allocated buffers for use with
+ * the iba_reqs function.
+ */
+static void assert_buf(const char *name, int i, size_t func_size, u32 *buf,
+	u32 *data, size_t buf_size)
+{
+	int _assert = false;
+
+	if ((i + 1) > func_size / sizeof(void *)) {
+		printk(KERN_INFO "  [%s func] %u %u"NL,
+			name, i, (int)(func_size / sizeof(void *)));
+		_assert = true;
+	}
+	if (data > buf + buf_size / sizeof(u32)) {
+		printk(KERN_INFO "  [%s data] %u"NL,
+			name, (int)(data - buf));
+		_assert = true;
+	}
+	if (_assert)
+		BUG();
+}  /* assert_buf */
+
+/**
+ * iba_r_pre - IBA register read pre-processing
+ * @info:	The IBA instance.
+ * @in:		The input pointer.
+ * @obj:	The object pointer.
+ *
+ * This function prepares IBA for register read operation.
+ *
+ * Return the IBA frame pointer.
+ */
+static void *iba_r_pre(struct ksz_iba_info *info, void *in, void *obj)
+{
+	u32 *data = in;
+
+	iba_cmd_set(info, IBA_CMD_READ, data[0], data[1], 0);
+	return info->fptr;
+}  /* iba_r_pre */
+
+/**
+ * iba_r_post - IBA register read post-processing
+ * @info:	The IBA instance.
+ * @out:	The output pointer.
+ * @obj:	The object pointer.
+ *
+ * This function retrieves the result of IBA register read operation.
+ *
+ * Return number of registers read.
+ */
+static int iba_r_post(struct ksz_iba_info *info, void *out, void *obj)
+{
+	u32 *data = out;
+	int i = 0;
+
+if (info->seqid != info->respid)
+dbg_msg(" id %x %x"NL, info->seqid, info->respid);
+	while (info->regs[i].cmd != (u32) -1) {
+		if (IBA_CMD_READ == (info->regs[i].cmd >> IBA_CMD_S)) {
+			u32 reg = (info->regs[i].cmd & IBA_CMD_ADDR_M);
+			u32 size = (info->regs[i].cmd & IBA_CMD_32);
+			int j = 0;
+
+			while (data[j] != -1) {
+				if (reg == data[j] &&
+				    (data[j + 1] & 0xffff0000) == 0xdead0000) {
+					data[j + 1] = iba_get_val(size,
+						info->regs[i].data[0]);
+					break;
+				}
+				j += 2;
+			}
+		}
+		i++;
+	}
+	return i;
+}  /* iba_r_post */
+
+/**
+ * iba_w_pre - IBA register write pre-processing
+ * @info:	The IBA instance.
+ * @in:		The input pointer.
+ * @obj:	The object pointer.
+ *
+ * This function prepares IBA for register write operation.
+ *
+ * Return the IBA frame pointer.
+ */
+static void *iba_w_pre(struct ksz_iba_info *info, void *in, void *obj)
+{
+	u32 *data = in;
+
+	iba_cmd_set(info, IBA_CMD_WRITE, data[0], data[1], data[2]);
+	return info->fptr;
+}  /* iba_w_pre */
+
+/**
+ * sw_setup_iba - Switch IBA setup
+ * @sw:		The switch instance.
+ *
+ * This routines setups IBA function of the switch.
+ */
+static void sw_setup_iba(struct ksz_sw *sw)
+{
+	u32 data;
+	u32 mask;
+
+	data = sw->old->r32(sw, REG_SW_IBA__4);
+
+	/* Register value may become random after switch power down. */
+	mask = (SW_IBA_ENABLE | (SW_IBA_PORT_M << SW_IBA_PORT_S));
+	if (sw->info->iba.cfg &&
+	    (sw->info->iba.cfg & mask) != (data & mask))
+		data = sw->info->iba.cfg;
+	sw->info->iba.tag_type = (data & SW_IBA_FRAME_TPID_M);
+	data &= ~(SW_IBA_PORT_M << SW_IBA_PORT_S);
+	data |= sw->HOST_PORT << SW_IBA_PORT_S;
+	data |= SW_IBA_ENABLE;
+	data |= SW_IBA_INIT;
+#if 0
+	data |= SW_IBA_DA_MATCH;
+#endif
+	sw->old->w32(sw, REG_SW_IBA__4, data);
+	data &= ~SW_IBA_INIT;
+	sw->info->iba.cfg = data;
+	dbg_msg("status %08x"NL, sw->old->r32(sw, REG_SW_IBA_STATUS__4));
+	dbg_msg("states %08x"NL, sw->old->r32(sw, REG_SW_IBA_STATES__4));
+	dbg_msg("result %08x"NL, sw->old->r32(sw, REG_SW_IBA_RESULT__4));
+}  /* sw_setup_iba */
+
+/**
+ * iba_to_spi - Switch IBA to SPI
+ * @sw:		The switch instance.
+ * @info:	The IBA instance.
+ *
+ * This routine switches from using IBA to using SPI.
+ */
+static void iba_to_spi(struct ksz_sw *sw, struct ksz_iba_info *info)
+{
+	bool restart = true;
+
+	if (info->use_iba & IBA_USE_CODE_TURN_OFF) {
+		info->use_iba &=
+			~(IBA_USE_CODE_NO_WAIT | IBA_USE_CODE_TURN_OFF);
+		restart = false;
+		printk(KERN_ALERT "stop using IBA"NL);
+	}
+	if (IBA_USE_CODE_PREPARE <= info->use_iba) {
+if (info->use_iba != IBA_USE_CODE_LOST)
+dbg_msg(" iba stops: %x"NL, info->use_iba);
+		info->use_iba = IBA_USE_CODE_LOST;
+		return;
+	}
+
+	/* Not calling from interrupt handling. */
+	if (sw->intr_using != 2)
+		mutex_lock(sw->reglock);
+	sw_set_spi(sw, info);
+	if (sw->intr_using != 2)
+		mutex_unlock(sw->hwlock);
+	if (restart) {
+		printk(KERN_ALERT "revert to SPI"NL);
+
+#ifdef RETRY_IBA
+		sw_setup_iba(sw);
+		schedule_delayed_work(&sw->set_ops, msecs_to_jiffies(1000));
+#endif
+	}
+}  /* iba_to_spi */
+
+static void iba_dbg_states(struct ksz_iba_info *info)
+{
+	int i;
+	u32 status;
+	struct ksz_sw *sw = info->sw_dev;
+	int iba_test_override = (sw->overrides & IBA_TEST);
+	int use_iba = info->use_iba;
+
+#ifdef CONFIG_KSZ_IBA_ONLY
+	/* Network communication no longer works. */
+	if ((info->use_iba & IBA_USE_CODE_MASK) >= IBA_USE_CODE_HARD_RESET)
+		return;
+#endif
+
+	if (IBA_USE_CODE_PREPARE <= info->use_iba)
+		return;
+#if 0
+iba_test_override = 1;
+#endif
+	if (!iba_test_override)
+		return;
+
+dbg_msg(" w seq: %x"NL, info->seqid);
+if (sw->intr_using < 2 && mutex_is_locked(sw->reglock))
+printk(" reg locked: %d"NL, sw->intr_using);
+	if (sw->intr_using < 2)
+		mutex_lock(sw->reglock);
+	info->use_iba = 0;
+	status = sw->old->r32(sw, REG_SW_IBA_STATUS__4);
+	dbg_msg("status %08x q:%d p:%d d:%d f:%d o:%d m:%d"NL, status,
+		!!(status & SW_IBA_REQ),
+		!!(status & SW_IBA_RESP),
+		!!(status & SW_IBA_DA_MISMATCH),
+		!!(status & SW_IBA_FMT_MISMATCH),
+		!!(status & SW_IBA_CODE_ERROR),
+		!!(status & SW_IBA_CMD_ERROR));
+	status = sw->old->r32(sw, REG_SW_IBA_STATES__4);
+	dbg_msg("states %08x %u"NL, status, ((status >> SW_IBA_PACKET_SIZE_S) &
+		SW_IBA_PACKET_SIZE_M) * 4);
+	status = sw->old->r32(sw, REG_SW_IBA_RESULT__4);
+	dbg_msg("result %08x %u"NL, status, status >> SW_IBA_SIZE_S);
+	info->use_iba = use_iba;
+	if (sw->intr_using < 2)
+		mutex_unlock(sw->reglock);
+
+	for (i = 0; i < ntohs(info->frame->length); i++) {
+		dbg_msg("%02x ", info->packet[i]);
+		if (15 == (i % 16))
+			dbg_msg(NL);
+	}
+	if (i % 16)
+		dbg_msg(NL);
+}  /* iba_dbg_states */
+
+/**
+ * iba_reqs - IBA register request
+ * @info:	The IBA instance.
+ * @in:		The input pointer.
+ * @out:	The output pointer.
+ * @obj:	The object pointer.
+ * @func:	The pre-processing routines.
+ * @post:	The post-processing function.
+ *
+ * This function sends a request with many pre-processing routines to IBA and
+ * waits for a response.
+ *
+ */
+static int iba_reqs(struct ksz_iba_info *info, void **in, void *out, void *obj,
+	void **func,
+	int (*post)(struct ksz_iba_info *info, void *out, void *obj))
+{
+	int rc;
+	unsigned long wait;
+	u16 code = IBA_CODE_NORMAL;
+	void *(*prepare)(struct ksz_iba_info *info, void *in, void *obj);
+
+	do {
+		struct ksz_sw *sw = info->sw_dev;
+
+		if (!mutex_is_locked(sw->hwlock))
+			pr_alert("IBA not locked"NL);
+	} while (0);
+	memset(info->data, 0, sizeof(u32) * IBA_BURST_CNT_MAX);
+	info->fptr = iba_pre_cmd(info, code);
+
+	do {
+		prepare = *func;
+		info->fptr = prepare(info, *in, obj);
+		++func;
+		++in;
+	} while (*func);
+
+	info->fptr = iba_post_cmd(info);
+	info->regs[0].cmd = (u32) -1;
+	init_completion(&info->done);
+	rc = iba_xmit(info);
+	if (rc) {
+		iba_dbg_states(info);
+
+		/* Not testing if IBA is okay. */
+		if (!(info->use_iba & IBA_USE_CODE_TESTING))
+			iba_to_spi(info->sw_dev, info);
+		return 0;
+	}
+	if (info->use_iba & IBA_USE_CODE_NO_WAIT)
+		wait = 1;
+	else
+	wait = wait_for_completion_timeout(&info->done, info->delay_ticks);
+	if (!wait) {
+if (dbg_iba)
+dbg_msg("  w timeout"NL);
+else
+dbg_msg(" first fail: %02x %04x %04x"NL, info->respid, last_ok_reg,
+	last_iba_addr);
+		iba_dbg_states(info);
+
+		/* Not testing if IBA is okay. */
+		if (!(info->use_iba & IBA_USE_CODE_TESTING))
+			iba_to_spi(info->sw_dev, info);
+		return 0;
+	}
+	if (info->use_iba & IBA_USE_CODE_TURN_OFF)
+		iba_to_spi(info->sw_dev, info);
+
+	rc = 1;
+	if (post)
+		rc = post(info, out, obj);
+	return rc * 4;
+}  /* iba_reqs */
+
+/**
+ * iba_req - IBA basic register request
+ * @info:	The IBA instance.
+ * @in:		The input pointer.
+ * @out:	The output pointer.
+ * @obj:	The object pointer.
+ * @prepare:	The pre-processing routine.
+ * @post:	The post-processing function.
+ *
+ * This function sends a request to IBA and waits for a response.
+ *
+ * Return number of bytes read.
+ */
+static int iba_req(struct ksz_iba_info *info, void *in, void *out, void *obj,
+	void *(*prepare)(struct ksz_iba_info *info, void *in, void *obj),
+	int (*post)(struct ksz_iba_info *info, void *out, void *obj))
+{
+	int rc;
+	void *func[2];
+	void *data_in[1];
+	int i = 0;
+
+#ifdef CONFIG_KSZ_IBA_ONLY
+	/* Network communication no longer works. */
+	if ((info->use_iba & IBA_USE_CODE_MASK) >= IBA_USE_CODE_HARD_RESET)
+		return 0;
+#endif
+
+	data_in[i] = in;
+	func[i++] = prepare;
+
+	func[i] = NULL;
+	rc = iba_reqs(info, data_in, out, obj, func, post);
+	return rc;
+}  /* iba_req */
+
+/**
+ * iba_r - IBA basic register read
+ * @info:	The IBA instance.
+ * @reg:	The register to read.
+ * @size:	The data size.
+ *
+ * This function reads a register through IBA.
+ */
+static u32 iba_r(struct ksz_iba_info *info, unsigned reg, u32 size)
+{
+	u32 data[4];
+	int rc;
+	static int iba_r_enter;
+
+	if (IBA_USE_CODE_LOST == info->use_iba) {
+		return 0;
+	}
+	if (IBA_USE_CODE_HARD_RESET == info->use_iba)
+		printk(KERN_WARNING " %s %x"NL, __func__, reg);
+#if 1
+if (info->respid != info->seqid || iba_r_enter) {
+dbg_msg(" iba_r %x %x %d; %x %x; %d"NL, info->respid, info->seqid, info->cnt, reg,
+last_ok_reg, iba_r_enter);
+}
+#endif
+	++iba_r_enter;
+	data[0] = size;
+	data[1] = reg;
+	data[2] = 0xdeadbeaf;
+	data[3] = -1;
+	rc = iba_req(info, data, data + 1, NULL, iba_r_pre, iba_r_post);
+	if (!rc)
+dbg_msg("r %x %x %08x"NL, reg, size, data[2]);
+else if (dbg_iba)
+dbg_msg(" ?? %d %08x"NL, rc, data[2]);
+	last_ok_reg = reg;
+	--iba_r_enter;
+	return data[2];
+}  /* iba_r */
+
+/**
+ * iba_r8 - IBA 8-bit register read
+ * @info:	The IBA instance.
+ * @reg:	The register to read.
+ *
+ * This function reads a 8-bit register through IBA.
+ */
+static u8 iba_r8(struct ksz_sw *sw, unsigned reg)
+{
+	return (u8) iba_r(&sw->info->iba, reg, IBA_CMD_8);
+}  /* iba_r8 */
+
+/**
+ * iba_r16 - IBA 16-bit register read
+ * @info:	The IBA instance.
+ * @reg:	The register to read.
+ *
+ * This function reads a 16-bit register through IBA.
+ */
+static u16 iba_r16(struct ksz_sw *sw, unsigned reg)
+{
+	return (u16) iba_r(&sw->info->iba, reg, IBA_CMD_16);
+}  /* iba_r16 */
+
+/**
+ * iba_r24 - IBA 24-bit register read
+ * @info:	The IBA instance.
+ * @reg:	The register to read.
+ *
+ * This function reads a 24-bit register through IBA.
+ */
+static u32 iba_r24(struct ksz_sw *sw, unsigned reg)
+{
+	return iba_r(&sw->info->iba, reg, IBA_CMD_24);
+}  /* iba_r24 */
+
+/**
+ * iba_r32 - IBA 32-bit register read
+ * @info:	The IBA instance.
+ * @reg:	The register to read.
+ *
+ * This function reads a 32-bit register through IBA.
+ */
+static u32 iba_r32(struct ksz_sw *sw, unsigned reg)
+{
+	return iba_r(&sw->info->iba, reg, IBA_CMD_32);
+}  /* iba_r32 */
+
+/**
+ * iba_w - IBA basic register write
+ * @info:	The IBA instance.
+ * @reg:	The register to write.
+ * @val:	The value to write.
+ * @size:	The data size.
+ *
+ * This function writes a register through IBA.
+ */
+static void iba_w(struct ksz_iba_info *info, unsigned reg, unsigned val,
+	u32 size)
+{
+	u32 data[3];
+	int rc;
+
+	if (IBA_USE_CODE_LOST == info->use_iba) {
+		return;
+	}
+	if (IBA_USE_CODE_HARD_RESET == info->use_iba)
+		printk(KERN_WARNING " %s %x"NL, __func__, reg);
+	data[0] = size;
+	data[1] = reg;
+	data[2] = val;
+	rc = iba_req(info, data, NULL, NULL, iba_w_pre, NULL);
+	if (!rc)
+dbg_msg("w %x %x %x"NL, reg, size, info->use_iba);
+}  /* iba_w */
+
+/**
+ * iba_w8 - IBA 8-bit register write
+ * @info:	The IBA instance.
+ * @reg:	The register to write.
+ * @val:	The value to write.
+ *
+ * This function writes a 8-bit register through IBA.
+ */
+static void iba_w8(struct ksz_sw *sw, unsigned reg, unsigned val)
+{
+	iba_w(&sw->info->iba, reg, val, IBA_CMD_8);
+}  /* iba_w8 */
+
+/**
+ * iba_w16 - IBA 16-bit register write
+ * @info:	The IBA instance.
+ * @reg:	The register to write.
+ * @val:	The value to write.
+ *
+ * This function writes a 16-bit register through IBA.
+ */
+static void iba_w16(struct ksz_sw *sw, unsigned reg, unsigned val)
+{
+	iba_w(&sw->info->iba, reg, val, IBA_CMD_16);
+}  /* iba_w16 */
+
+/**
+ * iba_w24 - IBA 24-bit register write
+ * @info:	The IBA instance.
+ * @reg:	The register to write.
+ * @val:	The value to write.
+ *
+ * This function writes a 24-bit register through IBA.
+ */
+static void iba_w24(struct ksz_sw *sw, unsigned reg, unsigned val)
+{
+	iba_w(&sw->info->iba, reg, val, IBA_CMD_24);
+}  /* iba_w24 */
+
+/**
+ * iba_w32 - IBA 32-bit register write
+ * @info:	The IBA instance.
+ * @reg:	The register to write.
+ * @val:	The value to write.
+ *
+ * This function writes a 32-bit register through IBA.
+ */
+static void iba_w32(struct ksz_sw *sw, unsigned reg, unsigned val)
+{
+	iba_w(&sw->info->iba, reg, val, IBA_CMD_32);
+}  /* iba_w32 */
+
+/**
+ * iba_get_pre - IBA burst read pre-processing
+ * @info:	The IBA instance.
+ * @cnt:	The buffer count.
+ * @buf:	The buffer.
+ *
+ * This routine prepares IBA for burst read operation.
+ */
+static void iba_get_pre(u32 *data, int cnt, char *buf)
+{}
+
+/**
+ * iba_get_post - IBA burst read post-processing
+ * @info:	The IBA instance.
+ * @out:	The output pointer.
+ * @obj:	The object pointer.
+ * @b:		Endian indication.
+ *
+ * This helper function retrieves the result of IBA burst read operation.
+ *
+ * Return number of registers read.
+ */
+static int iba_get_post(struct ksz_iba_info *info, void *out, void *obj, int b)
+{
+	u32 *ptr = (u32 *) out;
+	int i = 0;
+
+	while (info->regs[i].cmd != (u32) -1) {
+		*ptr = iba_get_val((info->regs[i].cmd & IBA_CMD_32),
+			info->regs[i].data[0]);
+		if (b)
+			*ptr = cpu_to_be32(*ptr);
+		ptr++;
+		i++;
+	}
+	return i;
+}  /* iba_get_post */
+
+/**
+ * iba_get_post_be - IBA big-endian burst read post-processing
+ * @info:	The IBA instance.
+ * @out:	The output pointer.
+ * @obj:	The object pointer.
+ *
+ * This function retrieves the result of IBA big-endian burst read operation.
+ *
+ * Return number of registers read.
+ */
+static int iba_get_post_be(struct ksz_iba_info *info, void *out, void *obj)
+{
+	return iba_get_post(info, out, obj, 1);
+}  /* iba_get_post_be */
+
+/**
+ * iba_get_post_le - IBA little-endian burst read post-processing
+ * @info:	The IBA instance.
+ * @out:	The output pointer.
+ * @obj:	The object pointer.
+ *
+ * This function retrieves the result of IBA little-endian burst read
+ * operation.
+ *
+ * Return number of registers read.
+ */
+static int iba_get_post_le(struct ksz_iba_info *info, void *out, void *obj)
+{
+	return iba_get_post(info, out, obj, 0);
+}  /* iba_get_post_le */
+
+/**
+ * iba_set_pre - IBA burst write pre-processing
+ * @info:	The IBA instance.
+ * @in:		The input pointer.
+ * @obj:	The object pointer.
+ *
+ * This routine prepares IBA for burst write operation.
+ */
+static void iba_set_pre(u32 *data, int cnt, char *buf)
+{
+	u32 *ptr = (u32 *) buf;
+	int i;
+
+	i = 0;
+	if (cnt > 1)
+		i = 1;
+	while (cnt > 0) {
+		data[i++] = *ptr++;
+		cnt--;
+	}
+}  /* iba_set_pre */
+
+/**
+ * iba_set_post - IBA burst write post-processing
+ * @info:	The IBA instance.
+ * @out:	The output pointer.
+ * @obj:	The object pointer.
+ *
+ * This function retrieves the result of IBA burst write operation.
+ *
+ * Return number of registers written.
+ */
+static int iba_set_post(struct ksz_iba_info *info, void *out, void *obj)
+{
+	int i = 0;
+
+	while (info->regs[i].cmd != (u32) -1) {
+		dbg_msg("%08x=%08x"NL, info->regs[i].cmd,
+			info->regs[i].data[0]);
+		i++;
+	}
+	return i;
+}  /* iba_set_post */
+
+/**
+ * iba_burst - IBA burst request
+ * @info:	The IBA instance.
+ * @addr:	The starting address.
+ * @cnt:	The number of addresses.
+ * @buf:	Buffer holding the data.
+ * @write:	Write indication.
+ * @prepare:	The pre-processing routine.
+ * @post:	The post-processing function.
+ *
+ * This function sends a burst request to IBA and waits for a response.
+ *
+ * Return number of bytes read.
+ */
+static int iba_burst(struct ksz_iba_info *info, u32 addr, size_t cnt,
+	char *buf, int write, void (*prepare)(u32 *data, int cnt, char *buf),
+	int (*post)(struct ksz_iba_info *info, void *out, void *obj))
+{
+	int mult;
+	int rc;
+	unsigned long wait;
+	u32 val;
+	u16 code = IBA_CODE_NORMAL;
+	u32 cmd = IBA_CMD_READ;
+	u32 size = IBA_CMD_32;
+	void *data = buf;
+
+#ifdef CONFIG_KSZ_IBA_ONLY
+	/* Network communication no longer works. */
+	if ((info->use_iba & IBA_USE_CODE_MASK) >= IBA_USE_CODE_HARD_RESET)
+		return 0;
+#endif
+
+	memset(info->data, 0, sizeof(u32) * IBA_BURST_CNT_MAX);
+	if (cnt > 4) {
+		mult = cnt / 4;
+		info->data[0] = mult;
+		code = IBA_CODE_BURST;
+		cmd = IBA_BURST_READ;
+		if contain_reg(addr, cnt, REG_SW_IBA__4) {
+			u32 *ptr = (u32 *) buf;
+			u32 loc = (REG_SW_IBA__4 - addr) / 4;
+
+			if (write &&
+			    ((ptr[loc] & VALID_IBA_VAL) !=
+			    (info->cfg & VALID_IBA_VAL))) {
+#ifndef CONFIG_KSZ_IBA_ONLY
+				info->use_iba |=
+					IBA_USE_CODE_NO_WAIT |
+					IBA_USE_CODE_TURN_OFF;
+#else
+				/* Do not allow changing IBA port. */
+				ptr[loc] = info->cfg;
+#endif
+			}
+		}
+	} else {
+		mult = 1;
+		if (1 == cnt) {
+#ifdef VERIFY_IBA
+if (!dbg_iba_test) {
+iba_test(info, 7);
+dbg_iba_test = 1;
+}
+#endif
+			if (write) {
+				u8 *ptr = data;
+
+				val = *ptr;
+				data = &val;
+			}
+			size = IBA_CMD_8;
+		} else if (2 == cnt) {
+			if (write) {
+				u16 *ptr = data;
+
+				val = *ptr;
+				data = &val;
+			}
+			size = IBA_CMD_16;
+		} else if (addr & 1)
+			size = IBA_CMD_8;
+		else if (addr & 2)
+			size = IBA_CMD_16;
+	}
+	cmd += write;
+	info->fptr = iba_pre_cmd(info, code);
+
+	prepare(info->data, mult, data);
+	info->fptr = iba_cmd_data(info, cmd, size, addr);
+
+	info->fptr = iba_post_cmd(info);
+	info->regs[0].cmd = (u32) -1;
+	init_completion(&info->done);
+	rc = iba_xmit(info);
+	if (rc) {
+		iba_dbg_states(info);
+		iba_to_spi(info->sw_dev, info);
+		return 0;
+	}
+	if (info->use_iba & IBA_USE_CODE_NO_WAIT)
+		wait = 1;
+	else
+	wait = wait_for_completion_timeout(&info->done, info->delay_ticks);
+	if (!wait) {
+dbg_msg("burst to"NL);
+		iba_dbg_states(info);
+		iba_to_spi(info->sw_dev, info);
+		return 0;
+	}
+	if (info->use_iba & IBA_USE_CODE_TURN_OFF)
+		iba_to_spi(info->sw_dev, info);
+
+	rc = post(info, data, NULL);
+	rc *= 4;
+	return rc;
+}  /* iba_burst */
+
+static void iba_r_buf(struct ksz_sw *sw, unsigned reg, void *buf, size_t count)
+{
+	u8 *orig_buf = buf;
+	size_t orig_cnt = count;
+	int start = 0;
+
+	/* Not in multiple of 4. */
+	if ((count & 3) || (reg & 3)) {
+		orig_buf = buf;
+		orig_cnt = count;
+		start = reg & 3;
+		reg &= ~3;
+		buf = sw->info->iba.buf;
+		count += start;
+		count += 3;
+		count &= ~3;
+	}
+	iba_burst(&sw->info->iba, reg, count, buf, 0,
+		iba_get_pre, iba_get_post_be);
+	if (orig_buf != buf)
+		memcpy(orig_buf, &sw->info->iba.buf[start], orig_cnt);
+}  /* iba_r_buf */
+
+static u32 buf_to_val(u8 *buf, int i, int cnt)
+{
+	int j;
+	u32 val = buf[i];
+
+	for (j = 1; j < cnt; j++) {
+		val <<= 8;
+		val |= buf[i + j];
+	}
+	return val;
+}  /* buf_to_val */
+
+/**
+ * w_buf_pre - IBA buffer write pre-processing
+ * @info:	The IBA instance.
+ * @in:		The input pointer.
+ * @obj:	The object pointer.
+ *
+ * This function prepares IBA for buffer write operation.
+ *
+ * Return the IBA frame pointer.
+ */
+static void *w_buf_pre(struct ksz_iba_info *info, void *in, void *obj)
+{
+	u32 *data = in;
+	u8 *buf = obj;
+	u16 reg = data[0];
+	size_t cnt = data[1];
+	int i;
+	u32 size;
+	u32 val;
+
+	/* Register may not be in 4-byte boundary. */
+	switch (reg & 3) {
+	case 1:
+		size = IBA_CMD_24;
+		i = 3;
+		break;
+	case 2:
+		size = IBA_CMD_16;
+		i = 2;
+		break;
+	case 3:
+		size = IBA_CMD_8;
+		i = 1;
+		break;
+	default:
+		size = IBA_CMD_32;
+		i = 4;
+		break;
+	}
+
+	/* Count may be too small. */
+	if (i > cnt) {
+		i = cnt;
+		switch (i) {
+		case 1:
+			size = IBA_CMD_8;
+			break;
+		case 2:
+			size = IBA_CMD_16;
+			break;
+		default:
+			size = IBA_CMD_24;
+			break;
+		}
+	}
+
+	/* Prepare the initial value. */
+	val = buf_to_val(buf, 0, i);
+
+	cnt -= i;
+	iba_cmd_set(info, IBA_CMD_WRITE, size, reg, val);
+	reg &= ~3;
+	size = IBA_CMD_32;
+	while (cnt >= 4) {
+		val = buf_to_val(buf, i, 4);
+		reg += 4;
+		iba_cmd_set(info, IBA_CMD_WRITE, size, reg, val);
+		i += 4;
+		cnt -= 4;
+	}
+	if (cnt) {
+		switch (cnt) {
+		case 1:
+			size = IBA_CMD_8;
+			break;
+		case 2:
+			size = IBA_CMD_16;
+			break;
+		default:
+			size = IBA_CMD_24;
+			break;
+		}
+		val = buf_to_val(buf, i, cnt);
+		reg += 4;
+		iba_cmd_set(info, IBA_CMD_WRITE, size, reg, val);
+	}
+	return info->fptr;
+}  /* w_buf_pre */
+
+static void iba_w_buf(struct ksz_sw *sw, unsigned reg, void *buf, size_t count)
+{
+	/* Not in multiple of 4. */
+	if ((count & 3) || (reg & 3)) {
+		u32 data[3];
+
+		data[0] = reg;
+		data[1] = count;
+		iba_req(&sw->info->iba, data, NULL, buf, w_buf_pre,
+			iba_set_post);
+	} else {
+		int i;
+		u32 *src = buf;
+		u32 *dst = (u32 *) sw->info->iba.buf;
+
+		for (i = 0; i < count; i += 4)
+			*dst++ = be32_to_cpu(*src++);
+		buf = sw->info->iba.buf;
+		iba_burst(&sw->info->iba, reg, count, buf, 1,
+			iba_set_pre, iba_set_post);
+	}
+}  /* iba_w_buf */
+
+static int iba_get(struct ksz_sw *sw, u32 reg, size_t count, void *buf)
+{
+	int rc;
+
+	rc = iba_burst(&sw->info->iba, reg, count, buf, 0,
+		iba_get_pre, iba_get_post_le);
+
+	/*
+	 * Return zero to let the calling program know the boundary must be
+	 * 32-bit.
+	 */
+	if (4 == count && (reg & 3))
+		rc = 0;
+	return rc;
+}  /* iba_get */
+
+static int iba_set(struct ksz_sw *sw, u32 reg, size_t count, void *buf)
+{
+	int rc;
+
+	rc = iba_burst(&sw->info->iba, reg, count, buf, 1,
+		iba_set_pre, iba_set_post);
+	if (!rc) {
+		u32 *ptr = buf;
+		u32 val;
+
+		switch (count) {
+		case 4:
+			val = htonl(*ptr);
+			break;
+		case 2:
+			val = htons(*ptr);
+			break;
+		case 1:
+			val = *ptr;
+			break;
+		default:
+			ptr = NULL;
+			break;
+		}
+		if (ptr)
+			sw->ops->chk_regs(sw, reg, (u8 *)&val, count);
+	}
+	return rc;
+}  /* iba_set */
+
+/**
+ * r_mac_table_pre - IBA MAC table read pre-processing
+ * @info:	The IBA instance.
+ *
+ * This routine prepares IBA for MAC table read operation.
+ */
+static void r_mac_table_pre(struct ksz_iba_info *info)
+{
+	iba_cmd_set(info, IBA_CMD_READ, IBA_CMD_32, REG_SW_ALU_VAL_A, 0);
+	iba_cmd(info, IBA_CMD_READ, IBA_CMD_32, REG_SW_ALU_VAL_B);
+	iba_cmd(info, IBA_CMD_READ, IBA_CMD_32, REG_SW_ALU_VAL_C);
+	iba_cmd(info, IBA_CMD_READ, IBA_CMD_32, REG_SW_ALU_VAL_D);
+}  /* r_mac_table_pre */
+
+/**
+ * r_mac_table_post - IBA MAC table read post-processing
+ * @info:	The IBA instance.
+ * @out:	The output pointer.
+ * @obj:	The object pointer.
+ *
+ * This function retrieves the result of IBA MAC table read operation.
+ *
+ * Return number of registers read.
+ */
+static int r_mac_table_post(struct ksz_iba_info *info, void *out, void *obj)
+{
+	u16 *entry = out;
+	struct ksz_mac_table *mac = obj;
+	int i = 0;
+	u32 data[5];
+
+	memset(data, 0, sizeof(data));
+	while (info->regs[i].cmd != (u32) -1) {
+		if (IBA_CMD_READ == (info->regs[i].cmd >> IBA_CMD_S)) {
+			u32 reg = (info->regs[i].cmd & IBA_CMD_ADDR_M);
+			u32 size = (info->regs[i].cmd & IBA_CMD_32);
+
+			switch (reg) {
+			case REG_SW_ALU_VAL_A:
+				data[0] = iba_get_val(size,
+					info->regs[i].data[0]);
+				break;
+			case REG_SW_ALU_VAL_B:
+				data[1] = iba_get_val(size,
+					info->regs[i].data[0]);
+				break;
+			case REG_SW_ALU_VAL_C:
+				data[2] = iba_get_val(size,
+					info->regs[i].data[0]);
+				break;
+			case REG_SW_ALU_VAL_D:
+				data[3] = iba_get_val(size,
+					info->regs[i].data[0]);
+				get_mac_table_info(mac, data);
+				++mac;
+				memset(data, 0, sizeof(data));
+				break;
+			case REG_SW_LUE_INDEX_0__2:
+				data[4] = iba_get_val(size,
+					info->regs[i].data[0]);
+				if (entry) {
+					*entry = (data[4] & ENTRY_INDEX_M) +
+						1;
+					++entry;
+				}
+				break;
+			}
+		}
+		i++;
+	}
+	return i;
+}  /* r_mac_table_post */
+
+/**
+ * w_mac_table_pre - IBA MAC table write pre-processing
+ * @info:	The IBA instance.
+ * @mac:	The MAC table entries.
+ *
+ * This routine prepares IBA for MAC table write operation.
+ */
+static void w_mac_table_pre(struct ksz_iba_info *info,
+	struct ksz_mac_table *mac)
+{
+	u32 data[4];
+
+	set_mac_table_info(mac, data);
+	iba_cmd_set(info, IBA_CMD_WRITE, IBA_CMD_32, REG_SW_ALU_VAL_A,
+		data[0]);
+	iba_cmd_set(info, IBA_CMD_WRITE, IBA_CMD_32, REG_SW_ALU_VAL_B,
+		data[1]);
+	iba_cmd_set(info, IBA_CMD_WRITE, IBA_CMD_32, REG_SW_ALU_VAL_C,
+		data[2]);
+	iba_cmd_set(info, IBA_CMD_WRITE, IBA_CMD_32, REG_SW_ALU_VAL_D,
+		data[3]);
+}  /* w_mac_table_pre */
+
+struct dyn_mac_param {
+	u16 addr;
+	u8 *src_addr;
+	u16 src_fid;
+};
+
+/**
+ * s_dyn_mac_pre - IBA dynamic MAC table set pre-processing
+ * @info:	The IBA instance.
+ * @addr:	The address of the table entry.
+ * @src_addr:	The source address.
+ * @src_fid:	The source FID.
+ *
+ * This helper routine prepares IBA for dynamic MAC table set operation.
+ */
+static u32 s_dyn_mac_pre(struct ksz_iba_info *info, u16 addr, u8 *src_addr,
+	u16 src_fid)
+{
+	u32 ctrl;
+	u32 data;
+
+	ctrl = 0;
+	if (addr) {
+		data = (addr - 1) & ALU_DIRECT_INDEX_M;
+		iba_cmd_set(info, IBA_CMD_WRITE, IBA_CMD_32,
+			REG_SW_ALU_INDEX_1, data);
+		ctrl |= ALU_DIRECT;
+	} else {
+		data = (u32) src_fid << ALU_FID_INDEX_S;
+		data |= ((u32) src_addr[0] << 8) | src_addr[1];
+		iba_cmd_set(info, IBA_CMD_WRITE, IBA_CMD_32,
+			REG_SW_ALU_INDEX_0, data);
+		data = ((u32) src_addr[2] << 24) |
+			((u32) src_addr[3] << 16) |
+			((u32) src_addr[4] << 8) | src_addr[5];
+		iba_cmd_set(info, IBA_CMD_WRITE, IBA_CMD_32,
+			REG_SW_ALU_INDEX_1, data);
+	}
+	ctrl |= ALU_START;
+	return ctrl;
+}  /* s_dyn_mac_pre */
+
+/**
+ * r_dyn_mac_pre - IBA dynamic MAC table read pre-processing
+ * @info:	The IBA instance.
+ * @in:		The input pointer.
+ * @obj:	The object pointer.
+ *
+ * This function prepares IBA for dynamic MAC table read operation.
+ *
+ * Return the IBA frame pointer.
+ */
+static void *r_dyn_mac_pre(struct ksz_iba_info *info, void *in, void *obj)
+{
+	struct dyn_mac_param *param = in;
+	u8 *src_addr = param->src_addr;
+	u16 src_fid = param->src_fid;
+	u16 addr = param->addr;
+	u32 ctrl;
+
+	ctrl = s_dyn_mac_pre(info, addr, src_addr, src_fid);
+	ctrl |= ALU_READ;
+
+	/* Dynamic MAC table does not use USE_FID bit and does not clear it. */
+	iba_cmd_set(info, IBA_CMD_WRITE, IBA_CMD_32, REG_SW_ALU_VAL_B, 0);
+	iba_cmd_set(info, IBA_CMD_WRITE, IBA_CMD_32, REG_SW_ALU_CTRL__4, ctrl);
+	iba_cmd_set(info, IBA_CMD_WAIT_ON_0, IBA_CMD_32, REG_SW_ALU_CTRL__4,
+		ALU_START);
+	r_mac_table_pre(info);
+
+	/* Hash read. */
+	if (!addr) {
+		iba_cmd_set(info, IBA_CMD_READ, IBA_CMD_16,
+			REG_SW_LUE_INDEX_0__2, 0);
+	}
+	return info->fptr;
+}  /* r_dyn_mac_pre */
+
+/**
+ * iba_r_dyn_mac_hw - read from dynamic MAC table using IBA
+ * @sw:		The switch instance.
+ * @addr:	The address of the table entry.
+ * @src_addr:	The source address.
+ * @src_fid:	The source FID.
+ * @mac:	Buffer to store the MAC table entry.
+ * @entry:	Buffer to store the actual entry if available.
+ *
+ * This function reads an entry of the dynamic MAC table using IBA.
+ */
+static int iba_r_dyn_mac_hw(struct ksz_sw *sw, u16 addr, u8 *src_addr,
+	u16 src_fid, struct ksz_mac_table *mac, u16 *entry)
+{
+	struct dyn_mac_param data;
+
+	data.addr = addr;
+	data.src_addr = src_addr;
+	data.src_fid = src_fid;
+	return iba_req(&sw->info->iba, &data, entry, mac, r_dyn_mac_pre,
+		r_mac_table_post);
+}  /* iba_r_dyn_mac_hw */
+
+/**
+ * w_dyn_mac_pre - IBA dynamic MAC table write pre-processing
+ * @info:	The IBA instance.
+ * @in:		The input pointer.
+ * @obj:	The object pointer.
+ *
+ * This function prepares IBA for dynamic MAC table write operation.
+ *
+ * Return the IBA frame pointer.
+ */
+static void *w_dyn_mac_pre(struct ksz_iba_info *info, void *in, void *obj)
+{
+	struct ksz_mac_table *mac = obj;
+	struct dyn_mac_param *param = in;
+	u8 *src_addr = param->src_addr;
+	u16 src_fid = param->src_fid;
+	u16 addr = param->addr;
+	u32 ctrl;
+
+	ctrl = s_dyn_mac_pre(info, addr, src_addr, src_fid);
+	ctrl |= ALU_WRITE;
+	w_mac_table_pre(info, mac);
+	iba_cmd_set(info, IBA_CMD_WRITE, IBA_CMD_32, REG_SW_ALU_CTRL__4, ctrl);
+	return info->fptr;
+}  /* w_dyn_mac_pre */
+
+/**
+ * iba_w_dyn_mac_hw - write to dynamic MAC table using IBA
+ * @sw:		The switch instance.
+ * @addr:	The address of the table entry.
+ * @src_addr:	The source address.
+ * @src_fid:	The source FID.
+ * @mac:	The MAC table entry.
+ *
+ * This function writes an entry of the dynamic MAC table using IBA.
+ */
+static int iba_w_dyn_mac_hw(struct ksz_sw *sw, u16 addr, u8 *src_addr,
+	u16 src_fid, struct ksz_mac_table *mac)
+{
+	struct dyn_mac_param data;
+
+	data.addr = addr;
+	data.src_addr = src_addr;
+	data.src_fid = src_fid;
+	return iba_req(&sw->info->iba, &data, NULL, mac, w_dyn_mac_pre, NULL);
+}  /* iba_w_dyn_mac_hw */
+
+/**
+ * start_dyn_mac_pre - IBA dynamic MAC table start pre-processing
+ * @info:	The IBA instance.
+ * @in:		The input pointer.
+ * @obj:	The object pointer.
+ *
+ * This function prepares IBA for dynamic MAC table start operation.
+ *
+ * Return the IBA frame pointer.
+ */
+static void *start_dyn_mac_pre(struct ksz_iba_info *info, void *in, void *obj)
+{
+	u32 ctrl;
+
+	ctrl = ALU_SEARCH;
+	ctrl |= ALU_START;
+
+	/* Dynamic MAC table does not use USE_FID bit and does not clear it. */
+	iba_cmd_set(info, IBA_CMD_WRITE, IBA_CMD_32, REG_SW_ALU_VAL_B, 0);
+	iba_cmd_set(info, IBA_CMD_WRITE, IBA_CMD_32, REG_SW_ALU_CTRL__4, ctrl);
+	return info->fptr;
+}  /* start_dyn_mac_pre */
+
+/**
+ * iba_start_dyn_mac_hw - start dynamic MAC table search using IBA
+ * @sw:		The switch instance.
+ *
+ * This function starts dynamic MAC table search using IBA.
+ */
+static int iba_start_dyn_mac_hw(struct ksz_sw *sw)
+{
+	return iba_req(&sw->info->iba, NULL, NULL, NULL, start_dyn_mac_pre,
+		NULL);
+}  /* iba_start_dyn_mac_hw */
+
+/**
+ * g_dyn_mac_pre - IBA dynamic MAC table retrieve pre-processing
+ * @info:	The IBA instance.
+ * @in:		The input pointer.
+ * @obj:	The object pointer.
+ *
+ * This function prepares IBA for dynamic MAC table retrieve operation.
+ *
+ * Return the IBA frame pointer.
+ */
+static void *g_dyn_mac_pre(struct ksz_iba_info *info, void *in, void *obj)
+{
+	r_mac_table_pre(info);
+	return info->fptr;
+}  /* g_dyn_mac_pre */
+
+/**
+ * iba_g_dyn_mac_hw - retrieve dynamic MAC table result using IBA
+ * @sw:		The switch instance.
+ * @mac:	Buffer to store the MAC table entry.
+ *
+ * This function retrieves dynamic MAC table result using IBA.
+ */
+static int iba_g_dyn_mac_hw(struct ksz_sw *sw, struct ksz_mac_table *mac)
+{
+	return iba_req(&sw->info->iba, NULL, NULL, mac, g_dyn_mac_pre,
+		r_mac_table_post);
+}  /* iba_g_dyn_mac_hw */
+
+/**
+ * stop_dyn_mac_pre - IBA dynamic MAC table stop pre-processing
+ * @info:	The IBA instance.
+ * @in:		The input pointer.
+ * @obj:	The object pointer.
+ *
+ * This function prepares IBA for dynamic MAC table stop operation.
+ *
+ * Return the IBA frame pointer.
+ */
+static void *stop_dyn_mac_pre(struct ksz_iba_info *info, void *in, void *obj)
+{
+	iba_cmd_set(info, IBA_CMD_WRITE, IBA_CMD_32, REG_SW_ALU_CTRL__4, 0);
+	iba_cmd_set(info, IBA_CMD_READ, IBA_CMD_32, REG_SW_ALU_CTRL__4, 0);
+	return info->fptr;
+}  /* stop_dyn_mac_pre */
+
+/**
+ * stop_dyn_mac_post - IBA dynamic MAC table stop post-processing
+ * @info:	The IBA instance.
+ * @out:	The output pointer.
+ * @obj:	The object pointer.
+ *
+ * This function retrieves the result of IBA dynamic MAC table stop operation.
+ *
+ * Return number of registers read.
+ */
+static int stop_dyn_mac_post(struct ksz_iba_info *info, void *out, void *obj)
+{
+	u32 *data = out;
+	int i = 0;
+
+	while (info->regs[i].cmd != (u32) -1) {
+		if (IBA_CMD_READ == (info->regs[i].cmd >> IBA_CMD_S)) {
+			u32 reg = (info->regs[i].cmd & IBA_CMD_ADDR_M);
+			u32 size = (info->regs[i].cmd & IBA_CMD_32);
+
+			if (reg == REG_SW_ALU_CTRL__4)
+				*data = iba_get_val(size,
+					info->regs[i].data[0]);
+		}
+		i++;
+	}
+	return i;
+}  /* stop_dyn_mac_post */
+
+/**
+ * iba_stop_dyn_mac_hw - stop dynamic MAC table search using IBA
+ * @sw:		The switch instance.
+ *
+ * This function stops dynamic MAC table search using IBA.
+ *
+ * Return the last MAC table control.
+ */
+static u32 iba_stop_dyn_mac_hw(struct ksz_sw *sw)
+{
+	u32 ctrl;
+
+	iba_req(&sw->info->iba, NULL, &ctrl, NULL, stop_dyn_mac_pre,
+		stop_dyn_mac_post);
+	return ctrl;
+}  /* iba_stop_dyn_mac_hw */
+
+/**
+ * r_sta_mac_pre - IBA static MAC table read pre-processing
+ * @info:	The IBA instance.
+ * @in:		The input pointer.
+ * @obj:	The object pointer.
+ *
+ * This function prepares IBA for static MAC table read operation.
+ *
+ * Return the IBA frame pointer.
+ */
+static void *r_sta_mac_pre(struct ksz_iba_info *info, void *in, void *obj)
+{
+	u32 *data = in;
+	int cnt = 0;
+	int num = data[0];
+	u32 *ctrl = &data[1];
+
+	do {
+		iba_cmd_set(info, IBA_CMD_WRITE, IBA_CMD_32,
+			REG_SW_ALU_STAT_CTRL__4, *ctrl);
+		iba_cmd_set(info, IBA_CMD_WAIT_ON_0, IBA_CMD_32,
+			REG_SW_ALU_STAT_CTRL__4, ALU_STAT_START);
+		r_mac_table_pre(info);
+		++cnt;
+		++ctrl;
+	} while (cnt < num);
+	return info->fptr;
+}  /* r_sta_mac_pre */
+
+/**
+ * iba_r_sta_mac_hw - read from static MAC table using IBA
+ * @sw:		The switch instance.
+ * @ctrl:	The control values for read operation.
+ * @num:	Number of entries to read.
+ * @mac:	Buffer to hold the MAC table entries.
+ *
+ * This function reads from static MAC table using IBA.
+ */
+static int iba_r_sta_mac_hw(struct ksz_sw *sw, u32 ctrl[], int num,
+	struct ksz_mac_table *mac)
+{
+	u32 data[MAX_IBA_MAC_ENTRIES + 1];
+
+	if (num > MAX_IBA_MAC_ENTRIES)
+		num = MAX_IBA_MAC_ENTRIES;
+	data[0] = num;
+	memcpy(&data[1], ctrl, sizeof(u32) * num);
+	return iba_req(&sw->info->iba, data, NULL, mac, r_sta_mac_pre,
+		r_mac_table_post);
+}  /* iba_r_sta_mac_hw */
+
+/**
+ * w_sta_mac_pre - IBA static MAC table write pre-processing
+ * @info:	The IBA instance.
+ * @in:		The input pointer.
+ * @obj:	The object pointer.
+ *
+ * This function prepares IBA for static MAC table write operation.
+ *
+ * Return the IBA frame pointer.
+ */
+static void *w_sta_mac_pre(struct ksz_iba_info *info, void *in, void *obj)
+{
+	u32 *data = in;
+	int cnt;
+	int num = data[0];
+	struct ksz_mac_table *mac = obj;
+
+	for (cnt = 0; cnt < num; cnt++, data++) {
+		w_mac_table_pre(info, mac);
+		iba_cmd_set(info, IBA_CMD_WRITE, IBA_CMD_32,
+			REG_SW_ALU_STAT_CTRL__4, data[1]);
+		iba_cmd_set(info, IBA_CMD_WAIT_ON_0, IBA_CMD_32,
+			REG_SW_ALU_STAT_CTRL__4, ALU_STAT_START);
+		++mac;
+	}
+	return info->fptr;
+}  /* w_sta_mac_pre */
+
+/**
+ * iba_w_sta_mac_hw - write to static MAC table using IBA
+ * @sw:		The switch instance.
+ * @ctrl:	The control values for write operation.
+ * @num:	Number of entries to write.
+ * @mac:	MAC table entries.
+ *
+ * This function writes to static MAC table using IBA.
+ */
+static int iba_w_sta_mac_hw(struct ksz_sw *sw, u32 ctrl[], int num,
+	struct ksz_mac_table *mac)
+{
+	u32 data[MAX_IBA_MAC_ENTRIES + 1];
+
+	if (num > MAX_IBA_MAC_ENTRIES)
+		num = MAX_IBA_MAC_ENTRIES;
+	data[0] = num;
+	memcpy(&data[1], ctrl, sizeof(u32) * num);
+	return iba_req(&sw->info->iba, data, NULL, mac, w_sta_mac_pre, NULL);
+}  /* iba_w_sta_mac_hw */
+
+/**
+ * r_vlan_table_pre - IBA VLAN table read pre-processing
+ * @info:	The IBA instance.
+ * @in:		The input pointer.
+ * @obj:	The object pointer.
+ *
+ * This function prepares IBA for VLAN table read operation.
+ *
+ * Return the IBA frame pointer.
+ */
+static void *r_vlan_table_pre(struct ksz_iba_info *info, void *in, void *obj)
+{
+	u32 *data = in;
+	u16 addr = data[3];
+	u32 ctrl;
+	int cnt = 0;
+	int num = *((int *) obj);
+
+	if (num > MAX_IBA_VLAN_ENTRIES)
+		num = MAX_IBA_VLAN_ENTRIES;
+	do {
+		iba_cmd_set(info, IBA_CMD_WRITE, IBA_CMD_16,
+			REG_SW_VLAN_ENTRY_INDEX__2, addr);
+		ctrl = VLAN_READ;
+		ctrl |= VLAN_START;
+		iba_cmd_set(info, IBA_CMD_WRITE, IBA_CMD_8, REG_SW_VLAN_CTRL,
+			ctrl);
+		iba_cmd_set(info, IBA_CMD_WAIT_ON_0, IBA_CMD_8,
+			REG_SW_VLAN_CTRL, VLAN_START);
+		iba_cmd_set(info, IBA_CMD_READ, IBA_CMD_32,
+			REG_SW_VLAN_ENTRY__4, 0);
+		iba_cmd_set(info, IBA_CMD_READ, IBA_CMD_32,
+			REG_SW_VLAN_ENTRY_UNTAG__4, 0);
+		iba_cmd_set(info, IBA_CMD_READ, IBA_CMD_32,
+			REG_SW_VLAN_ENTRY_PORTS__4, 0);
+		++cnt;
+		++addr;
+	} while (cnt < num);
+	return info->fptr;
+}  /* r_vlan_table_pre */
+
+/**
+ * r_vlan_table_post - IBA VLAN table read post-processing
+ * @info:	The IBA instance.
+ * @out:	The output pointer.
+ * @obj:	The object pointer.
+ *
+ * This function retrieves the result of IBA VLAN table read peration.
+ *
+ * Return number of registers read.
+ */
+static int r_vlan_table_post(struct ksz_iba_info *info, void *out, void *obj)
+{
+	int i = 0;
+	u32 *data = out;
+	int num = *((int *) obj);
+
+	memset(data, 0, sizeof(u32) * num * READ_VLAN_ENTRY_SIZE);
+	while (info->regs[i].cmd != (u32) -1) {
+		if (IBA_CMD_READ == (info->regs[i].cmd >> IBA_CMD_S)) {
+			u32 reg = (info->regs[i].cmd & IBA_CMD_ADDR_M);
+			u32 size = (info->regs[i].cmd & IBA_CMD_32);
+
+			switch (reg) {
+			case REG_SW_VLAN_ENTRY__4:
+				data[0] = iba_get_val(size,
+					info->regs[i].data[0]);
+				break;
+			case REG_SW_VLAN_ENTRY_UNTAG__4:
+				data[1] = iba_get_val(size,
+					info->regs[i].data[0]);
+				break;
+			case REG_SW_VLAN_ENTRY_PORTS__4:
+				data[2] = iba_get_val(size,
+					info->regs[i].data[0]);
+				data += READ_VLAN_ENTRY_SIZE;
+				break;
+			}
+		}
+		i++;
+	}
+	return i;
+}  /* r_vlan_table_post */
+
+/**
+ * iba_r_vlan_hw - read from VLAN table using IBA
+ * @sw:		The switch instance.
+ * @data:	Buffer to hold the VLAN data.
+ * @num:	Number of entries to read.
+ *
+ * This function reads from VLAN table using IBA.
+ */
+static int iba_r_vlan_hw(struct ksz_sw *sw, u32 data[], int num)
+{
+	return iba_req(&sw->info->iba, data, data, &num, r_vlan_table_pre,
+		r_vlan_table_post);
+}  /* iba_r_vlan_hw */
+
+/**
+ * w_vlan_table_pre - IBA VLAN table write pre-processing
+ * @info:	The IBA instance.
+ * @in:		The input pointer.
+ * @obj:	The object pointer.
+ *
+ * This function prepares IBA for VLAN table write operation.
+ *
+ * Return the IBA frame pointer.
+ */
+static void *w_vlan_table_pre(struct ksz_iba_info *info, void *in,
+	void *obj)
+{
+	u32 *data = in;
+	u16 addr = data[3];
+	u32 ctrl;
+	int cnt;
+	int num = *((int *) obj);
+
+	if (num > MAX_IBA_VLAN_ENTRIES)
+		num = MAX_IBA_VLAN_ENTRIES;
+	for (cnt = 0; cnt < num; cnt++) {
+		iba_cmd_set(info, IBA_CMD_WRITE, IBA_CMD_32,
+			REG_SW_VLAN_ENTRY__4, data[0]);
+		iba_cmd_set(info, IBA_CMD_WRITE, IBA_CMD_32,
+			REG_SW_VLAN_ENTRY_UNTAG__4, data[1]);
+		iba_cmd_set(info, IBA_CMD_WRITE, IBA_CMD_32,
+			REG_SW_VLAN_ENTRY_PORTS__4, data[2]);
+		addr = data[3];
+		iba_cmd_set(info, IBA_CMD_WRITE, IBA_CMD_16,
+			REG_SW_VLAN_ENTRY_INDEX__2, addr);
+		ctrl = VLAN_WRITE;
+		ctrl |= VLAN_START;
+		iba_cmd_set(info, IBA_CMD_WRITE, IBA_CMD_8, REG_SW_VLAN_CTRL,
+			ctrl);
+		iba_cmd_set(info, IBA_CMD_WAIT_ON_0, IBA_CMD_8,
+			REG_SW_VLAN_CTRL, VLAN_START);
+		data += WRITE_VLAN_ENTRY_SIZE;
+	}
+	return info->fptr;
+}  /* w_vlan_table_pre */
+
+/**
+ * iba_w_vlan_hw - write to VLAN table using IBA
+ * @sw:		The switch instance.
+ * @data:	VLAN data to write.
+ * @num:	Number of entries to write.
+ *
+ * This function writes to VLAN table using IBA.
+ */
+static int iba_w_vlan_hw(struct ksz_sw *sw, u32 data[], int num)
+{
+	return iba_req(&sw->info->iba, data, NULL, &num, w_vlan_table_pre,
+		NULL);
+}  /* iba_w_vlan_hw */
+
+#ifdef CONFIG_KSZ_HSR
+/**
+ * r_hsr_table_pre - IBA HSR table read pre-processing
+ * @info:	The IBA instance.
+ *
+ * This routine prepares IBA for HSR table read operation.
+ */
+static void r_hsr_table_pre(struct ksz_iba_info *info)
+{
+	int i;
+
+	for (i = 0; i < 7; i++) {
+		iba_cmd_set(info, IBA_CMD_READ, IBA_CMD_32,
+			REG_HSR_ALU_VAL_A + i * 4, 0);
+	}
+}  /* r_hsr_table_pre */
+
+/**
+ * r_hsr_table_post - IBA HSR table read post-processing
+ * @info:	The IBA instance.
+ * @out:	The output pointer.
+ * @obj:	The object pointer.
+ *
+ * This function retrieves the result of IBA HSR table read operation.
+ *
+ * Return number of registers read.
+ */
+static int r_hsr_table_post(struct ksz_iba_info *info, void *out, void *obj)
+{
+	struct ksz_hsr_table *hsr = obj;
+	int i = 0;
+	u32 data[7];
+
+	memset(data, 0, sizeof(data));
+	while (info->regs[i].cmd != (u32) -1) {
+		if (IBA_CMD_READ == (info->regs[i].cmd >> IBA_CMD_S)) {
+			u32 reg = (info->regs[i].cmd & IBA_CMD_ADDR_M);
+			u32 size = (info->regs[i].cmd & IBA_CMD_32);
+
+			switch (reg) {
+			case REG_HSR_ALU_VAL_A:
+				data[0] = iba_get_val(size,
+					info->regs[i].data[0]);
+				break;
+			case REG_HSR_ALU_VAL_B:
+				data[1] = iba_get_val(size,
+					info->regs[i].data[0]);
+				break;
+			case REG_HSR_ALU_VAL_C:
+				data[2] = iba_get_val(size,
+					info->regs[i].data[0]);
+				break;
+			case REG_HSR_ALU_VAL_D:
+				data[3] = iba_get_val(size,
+					info->regs[i].data[0]);
+				break;
+			case REG_HSR_ALU_VAL_E:
+				data[4] = iba_get_val(size,
+					info->regs[i].data[0]);
+				break;
+			case REG_HSR_ALU_VAL_F:
+				data[5] = iba_get_val(size,
+					info->regs[i].data[0]);
+				break;
+			case REG_HSR_ALU_VAL_G:
+				data[6] = iba_get_val(size,
+					info->regs[i].data[0]);
+				get_hsr_table_info(hsr, data);
+				++hsr;
+				memset(data, 0, sizeof(data));
+				break;
+			}
+		}
+		i++;
+	}
+	return i;
+}  /* r_hsr_table_post */
+
+/**
+ * w_hsr_table_pre - IBA HSR table write pre-processing
+ * @info:	The IBA instance.
+ * @mac:	The HSR table entries.
+ *
+ * This routine prepares IBA for HSR table write operation.
+ */
+static void w_hsr_table_pre(struct ksz_iba_info *info,
+	struct ksz_hsr_table *hsr)
+{
+	u32 data[7];
+	int i;
+
+	set_hsr_table_info(hsr, data);
+	for (i = 0; i < 7; i++) {
+		iba_cmd_set(info, IBA_CMD_WRITE, IBA_CMD_32,
+			REG_HSR_ALU_VAL_A + i * 4, data[i]);
+	}
+}  /* w_hsr_table_pre */
+
+/**
+ * s_hsr_pre - IBA HSR table set pre-processing
+ * @info:	The IBA instance.
+ * @addr:	The address of the table entry.
+ * @src_addr:	The source address.
+ * @path_id:	The path ID.
+ *
+ * This helper routine prepares IBA for HSR table set operation.
+ */
+static u32 s_hsr_pre(struct ksz_iba_info *info, u16 addr, u8 *src_addr,
+	u8 path_id)
+{
+	u32 ctrl;
+	u32 data;
+
+	ctrl = 0;
+	if (addr) {
+		data = (addr - 1) & HSR_DIRECT_INDEX_M;
+		iba_cmd_set(info, IBA_CMD_WRITE, IBA_CMD_32,
+			REG_HSR_ALU_INDEX_2, data);
+		ctrl |= HSR_DIRECT;
+	} else {
+		data = 0;
+		iba_cmd_set(info, IBA_CMD_WRITE, IBA_CMD_32,
+			REG_HSR_ALU_INDEX_0, data);
+		data = ((u32) src_addr[0] << 8) | src_addr[1];
+		iba_cmd_set(info, IBA_CMD_WRITE, IBA_CMD_32,
+			REG_HSR_ALU_INDEX_1, data);
+		data = ((u32) src_addr[2] << 24) |
+			((u32) src_addr[3] << 16) |
+			((u32) src_addr[4] << 8) | src_addr[5];
+		iba_cmd_set(info, IBA_CMD_WRITE, IBA_CMD_32,
+			REG_HSR_ALU_INDEX_2, data);
+		data = path_id & HSR_PATH_INDEX_M;
+		iba_cmd_set(info, IBA_CMD_WRITE, IBA_CMD_32,
+			REG_HSR_ALU_INDEX_3, data);
+	}
+	ctrl |= HSR_START;
+	return ctrl;
+}  /* s_hsr_pre */
+
+/**
+ * r_hsr_pre - IBA HSR table read pre-processing
+ * @info:	The IBA instance.
+ * @in:		The input pointer.
+ * @obj:	The object pointer.
+ *
+ * This function prepares IBA for HSR table read operation.
+ *
+ * Return the IBA frame pointer.
+ */
+static void *r_hsr_pre(struct ksz_iba_info *info, void *in, void *obj)
+{
+	u32 *data = in;
+	u16 addr = data[0];
+	struct ksz_hsr_table *hsr = obj;
+	u32 ctrl;
+
+	ctrl = s_hsr_pre(info, addr, hsr->src_mac, hsr->path_id);
+	ctrl |= HSR_READ;
+	iba_cmd_set(info, IBA_CMD_WRITE, IBA_CMD_32, REG_HSR_ALU_CTRL__4,
+		ctrl);
+	iba_cmd_set(info, IBA_CMD_WAIT_ON_0, IBA_CMD_32, REG_HSR_ALU_CTRL__4,
+		HSR_START);
+	r_hsr_table_pre(info);
+	return info->fptr;
+}  /* r_hsr_pre */
+
+/**
+ * iba_r_hsr_hw - read from HSR table using IBA
+ * @sw:		The switch instance.
+ * @addr:	The address of the table entry.
+ * @hsr:	Buffer to store the HSR table entry.
+ *
+ * This function reads an entry of the HSR table using IBA.
+ */
+static int iba_r_hsr_hw(struct ksz_sw *sw, u16 addr,
+	struct ksz_hsr_table *hsr)
+{
+	u32 data[1];
+
+	data[0] = addr;
+	return iba_req(&sw->info->iba, data, NULL, hsr, r_hsr_pre,
+		r_hsr_table_post);
+}  /* iba_r_hsr_hw */
+
+/**
+ * w_hsr_pre - IBA HSR table write pre-processing
+ * @info:	The IBA instance.
+ * @in:		The input pointer.
+ * @obj:	The object pointer.
+ *
+ * This function prepares IBA for HSR table write operation.
+ *
+ * Return the IBA frame pointer.
+ */
+static void *w_hsr_pre(struct ksz_iba_info *info, void *in, void *obj)
+{
+	u32 *data = in;
+	struct ksz_hsr_table *hsr = obj;
+	u16 addr = data[0];
+	u32 ctrl;
+
+	ctrl = s_hsr_pre(info, addr, hsr->src_mac, hsr->path_id);
+	ctrl |= HSR_WRITE;
+	w_hsr_table_pre(info, hsr);
+	iba_cmd_set(info, IBA_CMD_WRITE, IBA_CMD_32, REG_HSR_ALU_CTRL__4,
+		ctrl);
+	return info->fptr;
+}  /* w_hsr_pre */
+
+/**
+ * iba_w_hsr_hw - write to HSR table using IBA
+ * @sw:		The switch instance.
+ * @addr:	The address of the table entry.
+ * @hsr:	The HSR table entry.
+ *
+ * This function writes an entry of the HSR table using IBA.
+ */
+static int iba_w_hsr_hw(struct ksz_sw *sw, u16 addr,
+	struct ksz_hsr_table *hsr)
+{
+	u32 data[1];
+
+	data[0] = addr;
+	return iba_req(&sw->info->iba, data, NULL, hsr, w_hsr_pre, NULL);
+}  /* iba_w_hsr_hw */
+
+/**
+ * start_hsr_pre - IBA HSR table start pre-processing
+ * @info:	The IBA instance.
+ * @in:		The input pointer.
+ * @obj:	The object pointer.
+ *
+ * This function prepares IBA for HSR table start operation.
+ *
+ * Return the IBA frame pointer.
+ */
+static void *start_hsr_pre(struct ksz_iba_info *info, void *in, void *obj)
+{
+	u32 ctrl;
+
+	ctrl = HSR_SEARCH;
+	ctrl |= HSR_START;
+	iba_cmd_set(info, IBA_CMD_WRITE, IBA_CMD_32, REG_HSR_ALU_CTRL__4,
+		ctrl);
+	return info->fptr;
+}  /* start_hsr_pre */
+
+/**
+ * iba_start_hsr_hw - start HSR table search using IBA
+ * @sw:		The switch instance.
+ *
+ * This function starts HSR table search using IBA.
+ */
+static int iba_start_hsr_hw(struct ksz_sw *sw)
+{
+	return iba_req(&sw->info->iba, NULL, NULL, NULL, start_hsr_pre, NULL);
+}  /* iba_start_hsr_hw */
+
+/**
+ * g_hsr_pre - IBA HSR table retrieve pre-processing
+ * @info:	The IBA instance.
+ * @in:		The input pointer.
+ * @obj:	The object pointer.
+ *
+ * This function prepares IBA for HSR table retrieve operation.
+ *
+ * Return the IBA frame pointer.
+ */
+static void *g_hsr_pre(struct ksz_iba_info *info, void *in, void *obj)
+{
+	r_hsr_table_pre(info);
+	return info->fptr;
+}  /* g_hsr_pre */
+
+/**
+ * iba_g_hsr_hw - retrieve HSR table result using IBA
+ * @sw:		The switch instance.
+ * @hsr:	Buffer to store the HSR table entry.
+ *
+ * This function retrieves HSR table result using IBA.
+ */
+static int iba_g_hsr_hw(struct ksz_sw *sw, struct ksz_hsr_table *hsr)
+{
+	return iba_req(&sw->info->iba, NULL, NULL, hsr, g_hsr_pre,
+		r_hsr_table_post);
+}  /* iba_g_hsr_hw */
+
+/**
+ * stop_hsr_pre - IBA HSR table stop pre-processing
+ * @info:	The IBA instance.
+ * @in:		The input pointer.
+ * @obj:	The object pointer.
+ *
+ * This function prepares IBA for HSR table stop operation.
+ *
+ * Return the IBA frame pointer.
+ */
+static void *stop_hsr_pre(struct ksz_iba_info *info, void *in, void *obj)
+{
+	u32 ctrl;
+
+	ctrl = 0;
+	iba_cmd_set(info, IBA_CMD_WRITE, IBA_CMD_32, REG_HSR_ALU_CTRL__4,
+		ctrl);
+	iba_cmd_set(info, IBA_CMD_READ, IBA_CMD_32, REG_HSR_ALU_CTRL__4, 0);
+	return info->fptr;
+}  /* stop_hsr_pre */
+
+/**
+ * stop_hsr_post - IBA HSR table stop post-processing
+ * @info:	The IBA instance.
+ * @out:	The output pointer.
+ * @obj:	The object pointer.
+ *
+ * This function retrieves the result of IBA HSR table stop operation.
+ *
+ * Return number of registers read.
+ */
+static int stop_hsr_post(struct ksz_iba_info *info, void *out, void *obj)
+{
+	u32 *data = out;
+	int i = 0;
+
+	while (info->regs[i].cmd != (u32) -1) {
+		if (IBA_CMD_READ == (info->regs[i].cmd >> IBA_CMD_S)) {
+			u32 reg = (info->regs[i].cmd & IBA_CMD_ADDR_M);
+			u32 size = (info->regs[i].cmd & IBA_CMD_32);
+
+			if (reg == REG_HSR_ALU_CTRL__4)
+				*data = iba_get_val(size,
+					info->regs[i].data[0]);
+		}
+		i++;
+	}
+	return i;
+}  /* stop_hsr_post */
+
+/**
+ * iba_stop_hsr_hw - stop HSR table search using IBA
+ * @sw:		The switch instance.
+ *
+ * This function stops HSR table search using IBA.
+ *
+ * Return the last HSR table control.
+ */
+static u32 iba_stop_hsr_hw(struct ksz_sw *sw)
+{
+	u32 ctrl;
+
+	iba_req(&sw->info->iba, NULL, &ctrl, NULL, stop_hsr_pre,
+		stop_hsr_post);
+	return ctrl;
+}  /* iba_stop_hsr_hw */
+#endif
+
+/**
+ * r_mib_cnt_pre - IBA MIB counter read pre-processing
+ * @info:	The IBA instance.
+ * @in:		The input pointer.
+ * @obj:	The object pointer.
+ *
+ * This function prepares IBA for MIB counter read operation.
+ *
+ * Return the IBA frame pointer.
+ */
+static void *r_mib_cnt_pre(struct ksz_iba_info *info, void *in, void *obj)
+{
+	u32 ctrl;
+	u32 *data = in;
+	u32 *port_in = obj;
+	int cnt;
+	int num = data[0];
+	uint p = port_in[0];
+	u32 freeze = port_in[1];
+
+	for (cnt = 0; cnt < num; cnt++, data++) {
+		ctrl = data[1] & MIB_COUNTER_INDEX_M;
+		ctrl <<= MIB_COUNTER_INDEX_S;
+		ctrl |= MIB_COUNTER_READ;
+		ctrl |= freeze;
+		iba_cmd_set(info, IBA_CMD_WRITE, IBA_CMD_32,
+			PORT_CTRL_ADDR(p, REG_PORT_MIB_CTRL_STAT__4),
+			ctrl);
+		iba_cmd_set(info, IBA_CMD_WAIT_ON_1, IBA_CMD_32,
+			PORT_CTRL_ADDR(p, REG_PORT_MIB_CTRL_STAT__4),
+			MIB_COUNTER_VALID);
+		iba_cmd_set(info, IBA_CMD_READ, IBA_CMD_32,
+			PORT_CTRL_ADDR(p, REG_PORT_MIB_DATA), 0);
+	}
+	return info->fptr;
+}  /* r_mib_cnt_pre */
+
+/**
+ * r_mib_cnt_post - IBA MIB counter read post-processing
+ * @info:	The IBA instance.
+ * @out:	The output pointer.
+ * @obj:	The object pointer.
+ *
+ * This function retrieves the result of IBA MIB counter read peration.
+ *
+ * Return number of registers read.
+ */
+static int r_mib_cnt_post(struct ksz_iba_info *info, void *out, void *obj)
+{
+	u32 cmd;
+	int i = 0;
+	u32 *data = out;
+	u32 *port_in = obj;
+	uint p = port_in[0];
+
+	while (info->regs[i].cmd != (u32) -1) {
+		cmd = (info->regs[i].cmd >> IBA_CMD_S);
+		if (IBA_CMD_READ == cmd || IBA_CMD_WAIT_ON_1 == cmd) {
+			u32 reg = (info->regs[i].cmd & IBA_CMD_ADDR_M);
+			u32 size = (info->regs[i].cmd & IBA_CMD_32);
+
+#if 1
+if (((reg >> 12) & 0xf) != p + 1)
+dbg_msg(" ?? %s %x %x"NL, __func__, reg, p);
+#endif
+			reg &= ((1 << 12) - 1);
+			switch (reg) {
+			case REG_PORT_MIB_CTRL_STAT__4:
+				data[0] = iba_get_val(size,
+					info->regs[i].data[0]);
+				break;
+			case REG_PORT_MIB_DATA:
+				data[1] = iba_get_val(size,
+					info->regs[i].data[0]);
+				data += READ_MIB_ENTRY_SIZE;
+				break;
+			}
+		}
+		i++;
+	}
+	return i;
+}  /* r_mib_cnt_post */
+
+/**
+ * iba_r_mib_cnt_hw - read MIB counters using IBA
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ * @addr:	The addresses of the counters.
+ * @num:	Number of entries to read.
+ * @data:	Buffer to store the counters.
+ *
+ * This function reads MIB counters of the port using IBA.
+ */
+static int iba_r_mib_cnt_hw(struct ksz_sw *sw, uint port, u32 addr[], int num,
+			    u32 data[])
+{
+	u32 data_in[MAX_IBA_MIB_ENTRIES + 1];
+	u32 obj[2];
+	struct ksz_port_cfg *cfg = get_port_cfg(sw, port);
+	u32 freeze = cfg->freeze ? MIB_COUNTER_FLUSH_FREEZE : 0;
+	uint p = port;
+
+	obj[0] = p;
+	obj[1] = freeze;
+	if (num > MAX_IBA_MIB_ENTRIES)
+		num = MAX_IBA_MIB_ENTRIES;
+	data_in[0] = num;
+	memcpy(&data_in[1], addr, sizeof(u32) * num);
+	memset(data, 0, sizeof(u32) * num * READ_MIB_ENTRY_SIZE);
+	return iba_req(&sw->info->iba, data_in, data, obj, r_mib_cnt_pre,
+		r_mib_cnt_post);
+}  /* iba_r_mib_cnt_hw */
+
+#if defined(CONFIG_HAVE_KSZ9897)
+/**
+ * r_acl_table_pre - IBA ACL table read pre-processing
+ * @info:	The IBA instance.
+ * @in:		The input pointer.
+ * @obj:	The object pointer.
+ *
+ * This function prepares IBA for ACL table read operation.
+ *
+ * Return the IBA frame pointer.
+ */
+static void *r_acl_table_pre(struct ksz_iba_info *info, void *in, void *obj)
+{
+	u32 *data = in;
+	uint *port = obj;
+	u16 addr = data[4];
+	u32 ctrl = (addr & PORT_ACL_INDEX_M);
+	int i;
+
+	iba_cmd_set(info, IBA_CMD_WRITE, IBA_CMD_8,
+		PORT_CTRL_ADDR(*port, REG_PORT_ACL_CTRL_0), ctrl);
+	iba_cmd_set(info, IBA_CMD_WAIT_ON_1, IBA_CMD_32,
+		PORT_CTRL_ADDR(*port, REG_PORT_ACL_CTRL_0 & ~3),
+		PORT_ACL_READ_DONE << 8);
+	for (i = 0; i < 4; i++) {
+		iba_cmd_set(info, IBA_CMD_READ, IBA_CMD_32,
+			PORT_CTRL_ADDR(*port, REG_PORT_ACL_0 + 4 * i), 0);
+	}
+	return info->fptr;
+}  /* r_acl_table_pre */
+
+/**
+ * r_acl_table_post - IBA ACL table read post-processing
+ * @info:	The IBA instance.
+ * @out:	The output pointer.
+ * @obj:	The object pointer.
+ *
+ * This function retrieves the result of IBA ACL table read peration.
+ *
+ * Return number of registers read.
+ */
+static int r_acl_table_post(struct ksz_iba_info *info, void *out, void *obj)
+{
+	int i = 0;
+	u32 *data = out;
+
+	while (info->regs[i].cmd != (u32) -1) {
+		if (IBA_CMD_READ == (info->regs[i].cmd >> IBA_CMD_S)) {
+			u32 reg = (info->regs[i].cmd & IBA_CMD_ADDR_M);
+			u32 size = (info->regs[i].cmd & IBA_CMD_32);
+
+			reg &= ((1 << 12) - 1);
+			switch (reg) {
+			case REG_PORT_ACL_0:
+				data[0] = iba_get_val(size,
+					info->regs[i].data[0]);
+				data[0] = cpu_to_be32(data[0]);
+				break;
+			case REG_PORT_ACL_4:
+				data[1] = iba_get_val(size,
+					info->regs[i].data[0]);
+				data[1] = cpu_to_be32(data[1]);
+				break;
+			case REG_PORT_ACL_8:
+				data[2] = iba_get_val(size,
+					info->regs[i].data[0]);
+				data[2] = cpu_to_be32(data[2]);
+				break;
+			case REG_PORT_ACL_C:
+				data[3] = iba_get_val(size,
+					info->regs[i].data[0]);
+				data[3] = cpu_to_be32(data[3]);
+				break;
+			}
+		}
+		i++;
+	}
+	return i;
+}  /* r_acl_table_post */
+
+/**
+ * iba_r_acl_hw - read from ACL table using IBA
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ * @addr:	The ACL index.
+ * @data:	Buffer to hold the ACL data.
+ *
+ * This function reads from ACL table of the port using IBA.
+ */
+static int iba_r_acl_hw(struct ksz_sw *sw, uint port, u16 addr, u8 data[])
+{
+	u32 *ptr_32 = (u32 *) data;
+
+	memset(data, 0, sizeof(u32) * 4);
+	ptr_32[4] = addr;
+	return iba_req(&sw->info->iba, data, data, &port, r_acl_table_pre,
+		r_acl_table_post);
+}  /* iba_r_acl_hw */
+
+/**
+ * w_acl_table_pre - IBA ACL table write pre-processing
+ * @info:	The IBA instance.
+ * @in:		The input pointer.
+ * @obj:	The object pointer.
+ *
+ * This function prepares IBA for ACL table write operation.
+ *
+ * Return the IBA frame pointer.
+ */
+static void *w_acl_table_pre(struct ksz_iba_info *info, void *in, void *obj)
+{
+	u32 *data = in;
+	uint *port = obj;
+	u16 addr = data[4];
+	u32 ctrl = (addr & PORT_ACL_INDEX_M) | PORT_ACL_WRITE;
+	int i;
+
+	for (i = 0; i < 4; i++) {
+		iba_cmd_set(info, IBA_CMD_WRITE, IBA_CMD_32,
+			PORT_CTRL_ADDR(*port, REG_PORT_ACL_0 + 4 * i),
+			be32_to_cpu(data[i]));
+	}
+	iba_cmd_set(info, IBA_CMD_WRITE, IBA_CMD_8,
+		PORT_CTRL_ADDR(*port, REG_PORT_ACL_CTRL_0), ctrl);
+	iba_cmd_set(info, IBA_CMD_WAIT_ON_1, IBA_CMD_32,
+		PORT_CTRL_ADDR(*port, REG_PORT_ACL_CTRL_0 & ~3),
+		PORT_ACL_WRITE_DONE << 8);
+	return info->fptr;
+}  /* w_acl_table_pre */
+
+/**
+ * iba_w_acl_hw - write to ACL table using IBA
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ * @addr:	The ACL index.
+ * @data:	The ACL data to write.
+ *
+ * This function writes to ACL table of the port using IBA.
+ */
+static int iba_w_acl_hw(struct ksz_sw *sw, uint port, u16 addr, u8 data[])
+{
+	u32 *ptr_32 = (u32 *) data;
+
+	ptr_32[4] = addr;
+	return iba_req(&sw->info->iba, data, NULL, &port, w_acl_table_pre,
+		NULL);
+}  /* iba_w_acl_hw */
+#endif
+
+static struct ksz_sw_reg_ops sw_iba_ops = {
+	.lock			= iba_lock,
+	.unlock			= iba_unlock,
+
+	.r8			= iba_r8,
+	.r16			= iba_r16,
+	.r24			= iba_r24,
+	.r32			= iba_r32,
+	.w8			= iba_w8,
+	.w16			= iba_w16,
+	.w24			= iba_w24,
+	.w32			= iba_w32,
+
+	.r			= iba_r_buf,
+	.w			= iba_w_buf,
+
+	.get			= iba_get,
+	.set			= iba_set,
+
+	.r_dyn_mac_hw		= iba_r_dyn_mac_hw,
+	.w_dyn_mac_hw		= iba_w_dyn_mac_hw,
+	.start_dyn_mac_hw	= iba_start_dyn_mac_hw,
+	.g_dyn_mac_hw		= iba_g_dyn_mac_hw,
+	.stop_dyn_mac_hw	= iba_stop_dyn_mac_hw,
+	.r_sta_mac_hw		= iba_r_sta_mac_hw,
+	.w_sta_mac_hw		= iba_w_sta_mac_hw,
+	.r_vlan_hw		= iba_r_vlan_hw,
+	.w_vlan_hw		= iba_w_vlan_hw,
+
+#ifdef CONFIG_KSZ_HSR
+	.r_hsr_hw		= iba_r_hsr_hw,
+	.w_hsr_hw		= iba_w_hsr_hw,
+	.start_hsr_hw		= iba_start_hsr_hw,
+	.g_hsr_hw		= iba_g_hsr_hw,
+	.stop_hsr_hw		= iba_stop_hsr_hw,
+#endif
+
+	.r_mib_cnt_hw		= iba_r_mib_cnt_hw,
+
+#if defined(CONFIG_HAVE_KSZ9897)
+	.r_acl_hw		= iba_r_acl_hw,
+	.w_acl_hw		= iba_w_acl_hw,
+#endif
+};
+
+/**
+ * iba_rcv - Receive IBA response.
+ * @info:	The IBA instance.
+ * @skb:	The received socket buffer.
+ *
+ * This function processes IBA response.
+ */
+static int iba_rcv(struct ksz_iba_info *info, struct sk_buff *skb)
+{
+	int i;
+	int j;
+	int k;
+	int cnt;
+	int len;
+	int cmd_shift;
+	u32 cmd;
+	u32 cmds;
+	u32 addr;
+	u32 data;
+	struct iba_cmd *frame;
+	struct iba_frame *iba;
+	u8 *ptr;
+	int ret = 1;
+	u8 *rxdata = skb->data;
+	int rxlen = skb->len;
+
+	ptr = rxdata;
+	ptr += ETH_ALEN * 2;
+	iba = (struct iba_frame *) ptr;
+
+	if (iba->tag.type != htons(info->tag_type) ||
+	    iba->format.format != htons(IBA_FORMAT_KSZ98XX))
+		goto out_drop;
+
+if (dbg_iba)
+dbg_msg(" iba rx: %x %x"NL, info->seqid, iba->tag.seqid);
+
+	if (!info->cmds[0].cmd)
+		goto out_drop;
+
+#if 0
+	dbg_msg("seq: %x"NL, iba->tag.seqid);
+#endif
+	if (iba->tag.seqid != info->seqid)
+		goto out_debug;
+
+	len = ntohs(iba->length);
+	cnt = rxlen;
+	if (len != cnt) {
+		if (rxlen > 61 && len + 4 != cnt)
+			dbg_msg("len: %d != %d"NL, len, cnt);
+		if (len > cnt)
+			len = cnt;
+	}
+	len -= ETH_ALEN * 2 + sizeof(struct iba_frame) -
+		sizeof(struct iba_cmd);
+	if (ntohs(iba->code) == IBA_CODE_NORMAL) {
+#ifdef SHOW_IBA
+		dbg_msg("normal"NL);
+#endif
+		cmd_shift = IBA_CMD_S;
+	} else {
+#ifdef SHOW_IBA
+		dbg_msg("burst"NL);
+#endif
+		cmd_shift = IBA_BURST_S;
+	}
+	frame = &iba->cmd;
+	j = 0;
+	k = 0;
+	while (len >= 4 && frame->cmd) {
+		cmd = ntohl(frame->cmd);
+		if (0xdeadbeef == cmd) {
+dbg_msg("apb: %08x"NL, last_iba_addr);
+			break;
+		}
+		cmds = cmd;
+		addr = cmd & IBA_CMD_ADDR_M;
+		i = 0;
+		data = ntohl(frame->data[i++]);
+		if (cmd != info->cmds[j].cmd || (IBA_BURST_S == cmd_shift
+				&& data != info->cmds[j].data[0]))
+			dbg_msg("?cmd %x=%x %x=%x"NL, info->cmds[j].cmd, cmd,
+				info->cmds[j].data[0], data);
+		cmd >>= cmd_shift;
+		if (IBA_BURST_S == cmd_shift) {
+			cnt = data;
+			if (IBA_BURST_WRITE == cmd) {
+#ifdef SHOW_IBA
+				dbg_msg("w: %08x=%d"NL, addr, cnt);
+#endif
+#if 0
+				i += cnt;
+#else
+				for (; i <= cnt; i++) {
+					data = ntohl(frame->data[i]);
+#if 0
+					iba_chk_regs(info->sw_dev, cmds, data);
+#endif
+					cmds += 4;
+#ifdef SHOW_IBA
+					dbg_msg("%08x ", data);
+#endif
+				}
+#ifdef SHOW_IBA
+				if (cnt)
+					dbg_msg(NL);
+#endif
+#endif
+			} else if (IBA_BURST_READ == cmd) {
+#ifdef SHOW_IBA
+				dbg_msg("r: %08x=%d"NL, addr, cnt);
+#endif
+				info->regs[k].cmd = cmds;
+				for (; i <= cnt; i++) {
+					data = ntohl(frame->data[i]);
+					info->regs[k++].data[0] = data;
+					info->regs[k].cmd =
+						info->regs[k - 1].cmd + 4;
+#ifdef SHOW_IBA
+					dbg_msg("%08x ", data);
+#endif
+				}
+#ifdef SHOW_IBA
+				if (cnt)
+					dbg_msg(NL);
+#endif
+			} else
+				break;
+			len -= sizeof(u32) * cnt;
+		} else {
+			switch (cmd) {
+			case IBA_CMD_READ:
+				info->regs[k].cmd = cmds;
+				info->regs[k++].data[0] = data;
+#ifdef SHOW_IBA
+				dbg_msg("r: ");
+#endif
+				break;
+			case IBA_CMD_WRITE:
+#if 0
+				iba_chk_regs(info->sw_dev, cmds, data);
+#endif
+#ifdef SHOW_IBA
+				dbg_msg("w: ");
+#endif
+				break;
+			case IBA_CMD_WAIT_ON_0:
+				info->regs[k].cmd = cmds;
+				info->regs[k++].data[0] = data;
+#ifdef SHOW_IBA
+				dbg_msg("z: ");
+#endif
+				break;
+			case IBA_CMD_WAIT_ON_1:
+				info->regs[k].cmd = cmds;
+				info->regs[k++].data[0] = data;
+#ifdef SHOW_IBA
+				dbg_msg("s: ");
+#endif
+				break;
+			case IBA_CMD_WRITE_0:
+				info->regs[k].cmd = cmds;
+				info->regs[k++].data[0] = data;
+#ifdef SHOW_IBA
+				dbg_msg("0: ");
+#endif
+				break;
+			case IBA_CMD_WRITE_1:
+				info->regs[k].cmd = cmds;
+				info->regs[k++].data[0] = data;
+#ifdef SHOW_IBA
+				dbg_msg("1: ");
+#endif
+				break;
+			}
+#ifdef SHOW_IBA
+			dbg_msg("%08x=%08x"NL, addr, data);
+#endif
+		}
+		j++;
+		len -= sizeof(struct iba_cmd);
+		frame = (struct iba_cmd *) &frame->data[i];
+	}
+#ifdef SHOW_IBA
+	dbg_msg(NL);
+#endif
+	ptr = rxdata;
+	if (len != 4)
+		dbg_msg("?len: %d"NL, len);
+	if (info->cmds[j].cmd != 0)
+		dbg_msg("? %x"NL, info->cmds[j].cmd);
+	if (len != 4 && info->cmds[j].cmd != 0) {
+		for (i = 0; i < rxlen + ETH_ALEN * 2 + 2; i++) {
+			dbg_msg("%02x ", ptr[i]);
+			if (15 == (i % 16))
+				dbg_msg(NL);
+		}
+		if (15 != (i % 16))
+			dbg_msg(NL);
+	}
+	info->cmds[0].cmd = 0;
+	info->regs[k].cmd = (u32) -1;
+	info->respid = iba->tag.seqid;
+if (dbg_iba) {
+dbg_iba = 0;
+dbg_msg("ok %x %x"NL, info->seqid, last_ok_iba);
+}
+last_ok_iba = info->respid;
+
+	dev_kfree_skb_irq(skb);
+	complete(&info->done);
+	return 0;
+
+out_debug:
+dbg_iba = 1;
+dbg_msg("last ok: %x"NL, last_ok_iba);
+	dbg_msg("seq: %x"NL, info->seqid);
+#if 0
+	for (i = 0; i < sizeof(struct iba_frame) + 14; i++) {
+		dbg_msg("%02x ", ptr[i]);
+		if (15 == (i % 16))
+			dbg_msg(NL);
+	}
+	if (15 != (i % 16))
+		dbg_msg(NL);
+#endif
+	for (i = 0; i < ntohs(info->frame->length); i++) {
+		dbg_msg("%02x ", info->packet[i]);
+		if (15 == (i % 16))
+			dbg_msg(NL);
+	}
+	if (15 != (i % 16))
+		dbg_msg(NL);
+	for (i = 0; i < rxlen; i++) {
+		dbg_msg("%02x ", rxdata[i]);
+		if (15 == (i % 16))
+			dbg_msg(NL);
+	}
+	if (15 != (i % 16))
+		dbg_msg(NL);
+
+out_drop:
+	return ret;
+}  /* iba_rcv */
+
+static struct iba_ops common_iba_ops = {
+	.get_val		= iba_get_val,
+	.cmd_data		= iba_cmd_data,
+	.prepare_data		= iba_prepare_data,
+	.assert			= assert_buf,
+	.r_pre			= iba_r_pre,
+	.r_post			= iba_r_post,
+	.w_pre			= iba_w_pre,
+	.get_post_be		= iba_get_post_be,
+	.get_post_le		= iba_get_post_le,
+	.get_pre		= iba_get_pre,
+	.req			= iba_req,
+	.reqs			= iba_reqs,
+	.burst			= iba_burst,
+};
+
+static void ksz_iba_init(struct ksz_iba_info *iba, struct ksz_sw *sw)
+{
+	u32 data;
+	u16 tag_type;
+
+	/* Running nuttcp UDP TX can affect IBA communication if too short. */
+	data = 800;
+	iba->delay_ticks = msecs_to_jiffies(data);
+
+	/* Cannot really read hardware for current tag type. */
+	tag_type = ETH_P_IBA;
+
+	iba->sw_dev = sw;
+	iba->packet = kzalloc(IBA_LEN_MAX, GFP_KERNEL);
+	iba->buf = kzalloc(IBA_LEN_MAX, GFP_KERNEL);
+	iba->data = kzalloc(IBA_BURST_CNT_MAX * sizeof(u32), GFP_KERNEL);
+	iba->regs = kmalloc(IBA_BURST_CNT_MAX * sizeof(struct iba_cmd),
+		GFP_KERNEL);
+	iba->cmds = kmalloc(IBA_BURST_CNT_MAX * sizeof(struct iba_cmd) / 4,
+		GFP_KERNEL);
+	iba->frame = (struct iba_frame *) &iba->packet[ETH_ALEN * 2];
+	iba->tag_type = tag_type;
+	iba->dst[0] = 0x01;
+	iba->dst[1] = 0x00;
+	iba->dst[2] = 0x5E;
+	iba->dst[3] = 0x00;
+	iba->dst[4] = 0x01;
+	iba->dst[5] = 0x81;
+#ifndef CONFIG_KSZ_IBA_ONLY
+#ifndef CAPTURE_IBA
+	memcpy(iba->dst, sw->info->mac_addr, ETH_ALEN);
+#endif
+#endif
+	iba->src[0] = 0x00;
+#ifdef CAPTURE_IBA
+	iba->src[0] = 0x01;
+#endif
+	iba->src[1] = 0x10;
+	iba->src[2] = 0xA1;
+	iba->src[3] = 0x98;
+	iba->src[4] = 0x97;
+	iba->src[5] = 0x81;
+
+	init_completion(&iba->done);
+	init_waitqueue_head(&iba->queue);
+	mutex_init(&iba->lock);
+
+	iba->ops = &common_iba_ops;
+
+	iba_info = iba;
+}  /* ksz_iba_init */
+
+static void ksz_iba_exit(struct ksz_iba_info *iba)
+{
+	kfree(iba->cmds);
+	kfree(iba->regs);
+	kfree(iba->data);
+	kfree(iba->buf);
+	kfree(iba->packet);
+}  /* ksz_iba_exit */
+
diff -Napur linux-linux4microchip-2022.10-orig/drivers/net/ethernet/micrel/ksz_iba.h linux-linux4microchip-2022.10/drivers/net/ethernet/micrel/ksz_iba.h
--- linux-linux4microchip-2022.10-orig/drivers/net/ethernet/micrel/ksz_iba.h	1969-12-31 16:00:00.000000000 -0800
+++ linux-linux4microchip-2022.10/drivers/net/ethernet/micrel/ksz_iba.h	2023-11-17 16:14:09.553025529 -0800
@@ -0,0 +1,189 @@
+/**
+ * Microchip IBA header
+ *
+ * Copyright (c) 2015-2023 Microchip Technology Inc.
+ *	Tristram Ha <Tristram.Ha@microchip.com>
+ *
+ * Copyright (c) 2013-2015 Micrel, Inc.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ */
+
+
+#ifndef KSZ_IBA_H
+#define KSZ_IBA_H
+
+
+#define IBA_TAG_TYPE		0x40FE
+
+struct iba_tag {
+	u16 type;
+#ifdef __BIG_ENDIAN_BITFIELD
+	u8 prio:3;
+	u8 cfi:1;
+	u8 mode:4;
+#else
+	u8 mode:4;
+	u8 cfi:1;
+	u8 prio:3;
+#endif
+	u8 seqid;
+} __packed;
+
+#define IBA_FORMAT_KSZ98XX	0x9800
+
+struct iba_format {
+	u16 format;
+	u16 reserved;
+} __packed;
+
+#define IBA_CODE_NORMAL		0x0001
+#define IBA_CODE_BURST		0x0002
+
+#define IBA_CMD_READ		1
+#define IBA_CMD_WRITE		2
+#define IBA_CMD_WAIT_ON_0	4
+#define IBA_CMD_WAIT_ON_1	5
+#define IBA_CMD_WRITE_0		6
+#define IBA_CMD_WRITE_1		7
+#define IBA_CMD_S		29
+
+#define IBA_CMD_BYTE_0		(1 << 27)
+#define IBA_CMD_BYTE_1		(1 << 26)
+#define IBA_CMD_BYTE_2		(1 << 25)
+#define IBA_CMD_BYTE_3		(1 << 24)
+
+#define IBA_CMD_32		\
+	(IBA_CMD_BYTE_0 | IBA_CMD_BYTE_1 | IBA_CMD_BYTE_2 | IBA_CMD_BYTE_3)
+#define IBA_CMD_24		\
+	(IBA_CMD_BYTE_0 | IBA_CMD_BYTE_1 | IBA_CMD_BYTE_2)
+#define IBA_CMD_24_H		\
+	(IBA_CMD_BYTE_1 | IBA_CMD_BYTE_2 | IBA_CMD_BYTE_3)
+#define IBA_CMD_16		(IBA_CMD_BYTE_0 | IBA_CMD_BYTE_1)
+#define IBA_CMD_16_M		(IBA_CMD_BYTE_1 | IBA_CMD_BYTE_2)
+#define IBA_CMD_16_H		(IBA_CMD_BYTE_2 | IBA_CMD_BYTE_3)
+#define IBA_CMD_8		(IBA_CMD_BYTE_0)
+
+#define IBA_CMD_ADDR_M		((1 << 24) - 1)
+
+#define IBA_BURST_READ		1
+#define IBA_BURST_WRITE		2
+#define IBA_BURST_S		30
+#define IBA_BURST_CNT_MAX	(1 << 7)
+#define IBA_BURST_CNT_M		((1 << 7) - 1)
+
+struct iba_cmd {
+	u32 cmd;
+	u32 data[1];
+} __packed;
+
+struct iba_frame {
+	struct iba_tag tag;
+	u16 length;
+	struct iba_format format;
+	u16 code;
+	struct iba_cmd cmd;
+};
+
+struct ksz_iba_info;
+
+struct iba_ops {
+	u32 (*get_val)(u32 size, u32 val);
+	void *(*cmd_data)(struct ksz_iba_info *info, u32 cmd, u32 size,
+		u32 addr);
+	u32 *(*prepare_data)(u32 reg, u32 *data);
+	void (*assert)(const char *name, int i, size_t func_size, u32 *buf,
+		u32 *data, size_t buf_size);
+
+	void *(*r_pre)(struct ksz_iba_info *info, void *in, void *obj);
+	int (*r_post)(struct ksz_iba_info *info, void *out, void *obj);
+	void *(*w_pre)(struct ksz_iba_info *info, void *in, void *obj);
+	void (*get_pre)(u32 *data, int cnt, char *buf);
+	int (*get_post_be)(struct ksz_iba_info *info, void *out, void *obj);
+	int (*get_post_le)(struct ksz_iba_info *info, void *out, void *obj);
+
+	int (*req)(struct ksz_iba_info *info, void *in, void *out, void *obj,
+		void *(*prepare)(struct ksz_iba_info *info, void *in, void *obj),
+		int (*post)(struct ksz_iba_info *info, void *out, void *obj));
+	int (*reqs)(struct ksz_iba_info *info, void **in, void *out, void *obj,
+		void **func,
+		int (*post)(struct ksz_iba_info *info, void *out, void *obj));
+
+	int (*burst)(struct ksz_iba_info *info, u32 addr, size_t cnt,
+		char *buf, int write,
+		void (*prepare)(u32 *data, int cnt, char *buf),
+		int (*post)(struct ksz_iba_info *info, void *out, void *obj));
+};
+
+#define IBA_LEN_MAX		288
+
+struct ksz_iba_info {
+	int use_iba;
+	void *sw_dev;
+	u16 tag_type;
+	u8 dst[ETH_ALEN];
+	u8 src[ETH_ALEN];
+	u8 *buf;
+	u8 *packet;
+	struct iba_frame *frame;
+	struct iba_cmd *cmds;
+	struct iba_cmd *regs;
+	u8 seqid;
+	u8 respid;
+	unsigned long delay_ticks;
+	struct mutex lock;
+	int cnt;
+	u32 cfg;
+
+	/* OS dependent variables. */
+	void *dev;
+	struct completion done;
+	wait_queue_head_t queue;
+	bool ready;
+
+	/* Used for putting in commands. */
+	u32 *data;
+	void *fptr;
+	int index;
+	int len;
+
+	const struct iba_ops *ops;
+};
+
+/* Macros to use on common calls. */
+#define iba_assert(iba, name, i, size, buf, data, buf_size)		\
+	iba->ops->assert(name, i, size, buf, data, buf_size)
+
+#define iba_prepare(iba, reg, data)					\
+	iba->ops->prepare_data(reg, data)
+
+#define iba_cmd(iba, cmd, size, addr)					\
+	iba->fptr = iba->ops->cmd_data(iba, cmd, size, addr)
+
+#define iba_cmd_set(iba, cmd, size, addr, d)				\
+{									\
+	iba->data[0] = (d);						\
+	iba->fptr = iba->ops->cmd_data(iba, cmd, size, addr);		\
+}
+
+#define IBA_USE_CODE_MASK		0x0f
+#define IBA_USE_CODE_OFF		0
+#define IBA_USE_CODE_ON			1
+#define IBA_USE_CODE_PREPARE		2
+#define IBA_USE_CODE_ONLY		3
+#define IBA_USE_CODE_HARD_RESET		4
+#define IBA_USE_CODE_LOST		5
+#define IBA_USE_CODE_SOFT_RESET		6
+
+#define IBA_USE_CODE_TESTING		0x80
+#define IBA_USE_CODE_NO_WAIT		0x40
+#define IBA_USE_CODE_TURN_OFF		0x20
+
+#endif
diff -Napur linux-linux4microchip-2022.10-orig/drivers/net/ethernet/micrel/ksz_mac.c linux-linux4microchip-2022.10/drivers/net/ethernet/micrel/ksz_mac.c
--- linux-linux4microchip-2022.10-orig/drivers/net/ethernet/micrel/ksz_mac.c	1969-12-31 16:00:00.000000000 -0800
+++ linux-linux4microchip-2022.10/drivers/net/ethernet/micrel/ksz_mac.c	2024-02-19 18:34:40.889284260 -0800
@@ -0,0 +1,1125 @@
+
+
+#ifndef KSZ_MAC_C
+#define KSZ_MAC_C
+
+
+static int sw_extra_mtu;
+
+#if !defined(get_sysfs_data) || defined(CONFIG_KSZ_SWITCH_EMBEDDED)
+static void get_sysfs_data_(struct net_device *dev,
+	struct semaphore **proc_sem, struct ksz_port **port)
+{
+	struct ksz_mac *priv = get_netdev_priv(dev);
+	struct sw_priv *hw_priv;
+
+	hw_priv = priv->parent;
+	*port = &priv->port;
+	*proc_sem = &hw_priv->proc_sem;
+}  /* get_sysfs_data */
+#endif
+
+#ifndef get_sysfs_data
+#define get_sysfs_data		get_sysfs_data_
+#endif
+
+#ifndef CONFIG_KSZ_SWITCH_EMBEDDED
+
+#if defined(CONFIG_HAVE_KSZ9897)
+#include "ksz_sw_sysfs_9897.c"
+#elif defined(CONFIG_HAVE_KSZ8795)
+#include "ksz_sw_sysfs_8795.c"
+#elif defined(CONFIG_HAVE_KSZ8895)
+#include "ksz_sw_sysfs_8895.c"
+#elif defined(CONFIG_HAVE_KSZ8863)
+#include "ksz_sw_sysfs.c"
+#elif defined(CONFIG_HAVE_KSZ8463)
+#include "ksz_sw_sysfs.c"
+#elif defined(CONFIG_HAVE_LAN937X)
+#include "../microchip/lan937x_sw_sysfs.c"
+#endif
+
+#ifdef CONFIG_1588_PTP
+#ifdef CONFIG_HAVE_LAN937X
+#include "../microchip/lan937x_ptp_sysfs.c"
+#else
+#include "ksz_ptp_sysfs.c"
+#endif
+#endif
+
+#ifdef CONFIG_KSZ_DLR
+#include "ksz_dlr_sysfs.c"
+#endif
+#endif
+
+static inline int sw_is_switch(struct ksz_sw *sw)
+{
+	return sw != NULL;
+}
+
+static void copy_old_skb(struct sk_buff *old, struct sk_buff *skb)
+{
+	if (old->ip_summed) {
+		int offset = old->head - old->data;
+
+		skb->head = skb->data + offset;
+	}
+	skb->dev = old->dev;
+	skb->sk = old->sk;
+	skb->protocol = old->protocol;
+	skb->ip_summed = old->ip_summed;
+	skb->csum = old->csum;
+	skb_shinfo(skb)->tx_flags = skb_shinfo(old)->tx_flags;
+	skb_set_network_header(skb, ETH_HLEN);
+
+	dev_kfree_skb_any(old);
+}  /* copy_old_skb */
+
+#ifndef CONFIG_KSZ_IBA_ONLY
+static int sw_device_seen;
+
+static struct ksz_sw *check_avail_switch(struct net_device *netdev, int id)
+{
+	struct phy_device *phydev = NULL;
+	char phy_id[MII_BUS_ID_SIZE];
+	char bus_id[MII_BUS_ID_SIZE];
+	phy_interface_t phy_mode;
+	struct ksz_sw *sw = NULL;
+	struct device *d;
+
+	/* Check whether MII switch exists. */
+	phy_mode = PHY_INTERFACE_MODE_MII;
+	snprintf(bus_id, MII_BUS_ID_SIZE, "sw.%d", id);
+	snprintf(phy_id, MII_BUS_ID_SIZE, PHY_ID_FMT, bus_id, 0);
+	d = bus_find_device_by_name(&mdio_bus_type, NULL, phy_id);
+	if (!d)
+		return NULL;
+	phydev = to_phy_device(d);
+	if (!phydev->attached_dev) {
+		struct phy_priv *phydata = phydev->priv;
+
+		sw = phydata->port->sw;
+
+		/*
+		 * In case multiple devices mode is used and this phydev is not
+		 * attached again.
+		 */
+		if (sw)
+			phydev->interface = sw->interface;
+	}
+	put_device(d);
+	return sw;
+}  /* check_avail_switch */
+
+static int sw_mac_chk(struct ksz_mac *priv)
+{
+	struct ksz_sw *sw;
+
+	sw = priv->port.sw;
+	if (!sw) {
+		sw = check_avail_switch(priv->net, sw_device_seen);
+		if (!sw_is_switch(sw))
+			return -ENXIO;
+	}
+	priv->port.sw = sw;
+	return 0;
+}
+#endif
+
+
+static void dev_set_multicast(struct ksz_mac *priv, int multicast)
+{
+	if ((!multicast && priv->multi) || (multicast && !priv->multi)) {
+		struct ksz_mac *hw_priv = priv->hw_priv;
+		u8 hw_multi = hw_priv->hw_multi;
+
+		if (multicast)
+			++hw_priv->hw_multi;
+		else
+			--hw_priv->hw_multi;
+		priv->multi = multicast;
+
+		/* Turn on/off all multicast mode. */
+		if (hw_priv->hw_multi <= 1 && hw_multi <= 1)
+			hw_set_multicast(hw_priv->dev, hw_priv->hw_multi);
+	}
+}  /* dev_set_multicast */
+
+static void dev_set_promisc(struct ksz_mac *priv, int promisc)
+{
+	if (promisc != priv->promisc) {
+		struct ksz_mac *hw_priv = priv->hw_priv;
+		u8 hw_promisc = hw_priv->hw_promisc;
+
+		if (promisc)
+			++hw_priv->hw_promisc;
+		else
+			--hw_priv->hw_promisc;
+		priv->promisc = promisc;
+
+		/* Turn on/off promiscuous mode. */
+		if (hw_priv->hw_promisc <= 1 && hw_promisc <= 1)
+			hw_set_promisc(hw_priv->dev, hw_priv->hw_promisc);
+	}
+}  /* dev_set_promisc */
+
+static void promisc_reset_work(struct work_struct *work)
+{
+	struct delayed_work *dwork = to_delayed_work(work);
+	struct ksz_mac *hw_priv = container_of(dwork, struct ksz_mac,
+					       promisc_reset);
+
+	hw_priv->hw_promisc = 0;
+	hw_set_promisc(hw_priv->dev, hw_priv->promisc);
+}  /* promisc_reset_work */
+
+#ifndef KSZ_USE_SUBQUEUE
+static void stop_dev_queues(struct ksz_sw *sw, struct net_device *hw_dev)
+{
+	if (sw_is_switch(sw)) {
+		int dev_count = sw->dev_count + sw->dev_offset;
+		struct net_device *net;
+		int p;
+
+		for (p = 0; p < dev_count; p++) {
+			net = sw->netdev[p];
+			if (!net || net == hw_dev)
+				continue;
+			if (netif_running(net) &&
+			    !netif_queue_stopped(net)) {
+				netif_stop_queue(net);
+			}
+		}
+	}
+}  /* stop_dev_queues */
+
+static void wake_dev_queues(struct ksz_sw *sw, struct net_device *hw_dev)
+{
+	if (sw_is_switch(sw)) {
+		int dev_count = sw->dev_count + sw->dev_offset;
+		struct net_device *net;
+		int p;
+
+		for (p = 0; p < dev_count; p++) {
+			net = sw->netdev[p];
+			if (!net || net == hw_dev)
+				continue;
+			if (netif_running(net) &&
+			    netif_queue_stopped(net)) {
+				netif_wake_queue(net);
+
+				/* Need to update trans_start. */
+				netif_trans_update(net);
+			}
+		}
+		wake_up_interruptible(&sw->queue);
+	}
+}  /* wake_dev_queues */
+#else
+static void stop_dev_subqueues(struct ksz_sw *sw, struct net_device *hw_dev,
+			       int q)
+{
+	if (sw_is_switch(sw)) {
+		int dev_count = sw->dev_count + sw->dev_offset;
+		struct net_device *net;
+		int p;
+
+		for (p = 0; p < dev_count; p++) {
+			net = sw->netdev[p];
+			if (!net || net == hw_dev)
+				continue;
+			if (netif_running(net)) {
+				if (q >= 0) {
+					if (!__netif_subqueue_stopped(net, q))
+						netif_stop_subqueue(net, q);
+				} else {
+					netif_tx_stop_all_queues(net);
+				}
+			}
+		}
+	}
+}  /* stop_dev_subqueues */
+
+static void wake_dev_subqueues(struct ksz_sw *sw, struct net_device *hw_dev,
+			       int q)
+{
+	if (sw_is_switch(sw)) {
+		int dev_count = sw->dev_count + sw->dev_offset;
+		struct net_device *net;
+		int p;
+
+		for (p = 0; p < dev_count; p++) {
+			net = sw->netdev[p];
+			if (!net || net == hw_dev)
+				continue;
+			if (netif_running(net)) {
+				if (q >= 0) {
+					if (__netif_subqueue_stopped(net, q))
+						netif_wake_subqueue(net, q);
+				} else {
+					netif_tx_wake_all_queues(net);
+				}
+			}
+		}
+		wake_up_interruptible(&sw->queue);
+	}
+}  /* wake_dev_subqueues */
+#endif
+
+static bool sw_set_rx_mode(struct net_device *dev)
+{
+	int multicast = ((dev->flags & IFF_ALLMULTI) == IFF_ALLMULTI);
+	struct ksz_mac *priv = get_netdev_priv(dev);
+	struct ksz_mac *hw_priv = priv->hw_priv;
+	struct ksz_sw *sw = priv->port.sw;
+
+	if (!sw_is_switch(sw))
+		return false;
+
+	/* set_rx_mode is always called at least wice.  First is to set
+	 * multicast hash entries.  Second is to change promiscuous or all
+	 * multicast mode.  For all multicast mode the second call does not
+	 * indicate setting multicast hash.
+	 */
+	dev_set_promisc(priv, ((dev->flags & IFF_PROMISC) == IFF_PROMISC));
+
+	/* Multicast is enabled but not all multicast. */
+	if (!(dev->flags & IFF_ALLMULTI) &&
+	    (dev->flags & IFF_MULTICAST)) {
+		/* Used to handle unknown multicast forwarding if not using
+		 * multiple network devices.
+		 */
+		sw->net_ops->set_multi(sw, dev, &priv->port);
+
+		/* Need to turn on multicast for multiple network devices. */
+		if (sw->dev_count > 1) {
+			multicast |= 2;
+		}
+	}
+	dev_set_multicast(priv, multicast);
+
+	/* Promiscuous mode or all multicast mode is set. */
+	if (hw_priv->hw_promisc || hw_priv->hw_multi)
+		return true;
+	return false;
+}  /* sw_set_rx_mode */
+
+static bool sw_mac_set_addr(struct net_device *dev, struct ksz_mac *priv)
+{
+	struct ksz_sw *sw = priv->port.sw;
+	struct ksz_mac *hw_priv;
+	u8 hw_promisc;
+	u8 promisc;
+
+	if (!sw_is_switch(sw))
+		return false;
+	hw_priv = priv->hw_priv;
+	hw_promisc = hw_priv->hw_promisc;
+
+	promisc = sw->net_ops->set_mac_addr(sw, dev, hw_promisc,
+					    priv->port.first_port);
+	if (promisc != hw_priv->hw_promisc) {
+
+		/* A hack to accept changed KSZ9897 IBA response. */
+		if (!hw_priv->hw_promisc && 2 == promisc) {
+			promisc = 1;
+			schedule_delayed_work(&hw_priv->promisc_reset, 10);
+		}
+		hw_priv->hw_promisc = promisc;
+
+		/* Turn on/off promiscuous mode. */
+		if (hw_priv->hw_promisc <= 1 && hw_promisc <= 1)
+			hw_set_promisc(hw_priv->dev, hw_priv->hw_promisc);
+	}
+	if (priv->hw_priv && priv != priv->hw_priv)
+		return true;
+	return false;
+}  /* sw_mac_set_addr */
+
+#ifdef KSZ_USE_PRIVATE_IOCTL
+static int sw_private_ioctl(struct net_device *dev, struct ifreq *ifr,
+			    void __user *data, int cmd)
+{
+	struct ksz_mac *priv = get_netdev_priv(dev);
+	struct ksz_sw *sw = priv->port.sw;
+	int result = -EOPNOTSUPP;
+
+	if (!sw_is_switch(sw))
+		return result;
+	switch (cmd) {
+#ifdef CONFIG_1588_PTP
+	case SIOCDEVPRIVATE + 15:
+		if ((sw->features & PTP_HW)) {
+			struct ptp_info *ptp = &sw->ptp_hw;
+
+			result = ptp->ops->dev_req(ptp, ifr->ifr_data, NULL);
+		}
+		break;
+#endif
+#ifdef CONFIG_KSZ_MRP
+	case SIOCDEVPRIVATE + 14:
+		if ((sw->features & MRP_SUPPORT)) {
+			struct mrp_info *mrp = &sw->mrp;
+
+			result = mrp->ops->dev_req(mrp, ifr->ifr_data);
+		}
+		break;
+#endif
+	case SIOCDEVPRIVATE + 13:
+		result = sw->ops->dev_req(sw, ifr->ifr_data, NULL);
+		break;
+	default:
+		break;
+	}
+	return result;
+}  /* sw_private_ioctl */
+#endif
+
+#ifdef KSZ_USE_IOCTL
+static int sw_ioctl(struct net_device *dev, struct ifreq *rq, int cmd)
+{
+	int result = -EOPNOTSUPP;
+
+#ifdef CONFIG_1588_PTP
+	if (cmd == SIOCSHWTSTAMP) {
+		struct ksz_mac *priv = get_netdev_priv(dev);
+		struct ksz_sw *sw = priv->port.sw;
+
+		if (sw_is_switch(sw) && (sw->features & PTP_HW)) {
+			struct ptp_info *ptp;
+			u16 ports;
+			int i;
+			int p;
+
+			ports = 0;
+			if (priv->port.port_cnt > 1) {
+				p = priv->port.first_port +
+				    priv->port.port_cnt - 1;
+				ports = (1 << p);
+			} else {
+				for (i = 0, p = priv->port.first_port - 1;
+				     i < priv->port.port_cnt; i++, p++)
+					ports |= (1 << p);
+			}
+			ptp = &sw->ptp_hw;
+			result = ptp->ops->hwtstamp_ioctl(ptp, rq, ports);
+		}
+	}
+#endif
+	return result;
+}  /* sw_ioctl */
+#endif
+
+#ifdef KSZ_USE_MSGLEVEL
+static void sw_set_msglevel(struct net_device *net, struct ksz_mac *priv,
+			    u32 level)
+{
+	struct ksz_sw *sw = priv->port.sw;
+
+	if (sw_is_switch(sw)) {
+		struct net_device *sw_net;
+		int i;
+
+		/* All network devices share NETIF_MSG_LINK. */
+		priv->msg_enable = level;
+		for (i = 0; i < sw->dev_count + sw->dev_offset; i++) {
+			sw_net = sw->netdev[i];
+			if (!sw_net || sw_net == net)
+				continue;
+			priv = get_netdev_priv(sw_net);
+			if (level & NETIF_MSG_LINK)
+				priv->msg_enable |= NETIF_MSG_LINK;
+			else
+				priv->msg_enable &= ~NETIF_MSG_LINK;
+		}
+		if (level & NETIF_MSG_LINK)
+			sw->msg_enable |= NETIF_MSG_LINK;
+		else
+			sw->msg_enable &= ~NETIF_MSG_LINK;
+	}
+}  /* sw_set_msglevel */
+#endif
+
+#if defined(CONFIG_HAVE_KSZ9897) || defined(CONFIG_HAVE_LAN937X)
+static int priv_multi(void *ptr)
+{
+	struct ksz_mac *priv = get_ksz_mac(ptr);
+
+	return (priv->multi & 1);
+}  /* priv_multi */
+#endif
+
+static int priv_promisc(void *ptr)
+{
+	struct ksz_mac *priv = get_ksz_mac(ptr);
+
+	return priv->promisc;
+}  /* priv_promisc */
+
+#if !defined(CONFIG_HAVE_KSZ9897) && !defined(CONFIG_HAVE_LAN937X)
+static int priv_match_multi(void *ptr, u8 *data)
+{
+	struct ksz_mac *priv = get_ksz_mac(ptr);
+	struct netdev_hw_addr *ha;
+	int drop = true;
+
+	netdev_for_each_mc_addr(ha, priv->net) {
+		if (!memcmp(data, ha->addr, ETH_ALEN)) {
+			drop = false;
+			break;
+		}
+	}
+	return drop;
+}  /* priv_match_multi */
+#endif
+
+static void *sw_rx_proc(struct ksz_sw *sw, struct sk_buff *skb, int *rxlen)
+{
+	struct net_device *dev;
+	struct ksz_mac *priv;
+	int len = skb->len;
+	int rx_port = 0;
+	int forward = 0;
+	void *mac_priv;
+	int tag = 0;
+#ifdef CONFIG_1588_PTP
+	struct ptp_info *ptp = &sw->ptp_hw;
+	int ptp_tag = 0;
+#endif
+
+	dev = sw->net_ops->rx_dev(sw, skb->data, &len, &tag, &rx_port);
+	if (!dev) {
+		dev_kfree_skb_any(skb);
+		return NULL;
+	}
+
+	/* vlan_get_tag requires network device in socket buffer. */
+	skb->dev = dev;
+
+	/* __skb_set_length */
+	if (len != skb->len) {
+		skb->len = len;
+		skb_set_tail_pointer(skb, skb->len);
+	}
+
+	mac_priv = netdev_priv(dev);
+	priv = get_ksz_mac(mac_priv);
+
+	/* Internal packets handled by the switch. */
+	if (!sw->net_ops->drv_rx(sw, skb, rx_port)) {
+		/* Tell MAC driver real packet was received. */
+		*rxlen = skb->len;
+		return NULL;
+	}
+
+	/* dev can change to the one specifying multicast hash. */
+	if (!sw->net_ops->match_pkt(sw, &dev, (void **) &mac_priv, priv_promisc,
+#if defined(CONFIG_HAVE_KSZ9897) || defined(CONFIG_HAVE_LAN937X)
+	    priv_multi,
+#else
+	    priv_match_multi,
+#endif
+	    skb, priv->hw_priv->hw_promisc)) {
+		dev_kfree_skb_irq(skb);
+		return NULL;
+	}
+
+#ifdef CONFIG_1588_PTP
+	if (sw->features & PTP_HW) {
+		if (ptp->ops->drop_pkt(ptp, skb, sw->vlan_id, &tag, &ptp_tag,
+				       &forward)) {
+			dev_kfree_skb_any(skb);
+			return NULL;
+		}
+	}
+#endif
+
+	/* Need to forward to VLAN devices for PAE messages. */
+	if (!forward) {
+		struct ethhdr *eth = (struct ethhdr *) skb->data;
+
+		if (eth->h_proto == htons(0x888E) ||
+		    eth->h_proto == htons(0x88F7))
+			forward = FWD_VLAN_DEV | FWD_STP_DEV;
+	}
+
+	/* No VLAN port forwarding; need to send to parent. */
+	if ((forward & FWD_VLAN_DEV) && !tag)
+		forward &= ~FWD_VLAN_DEV;
+	dev = sw->net_ops->parent_rx(sw, dev, &forward);
+
+	/* dev may change. */
+	if (dev != skb->dev) {
+		skb->dev = dev;
+		mac_priv = netdev_priv(dev);
+	}
+
+	sw->net_ops->port_vlan_rx(skb, forward, tag);
+
+	return mac_priv;
+}  /* sw_rx_proc */
+
+static struct sk_buff *sw_mac_tx_pre(struct sk_buff *skb, struct ksz_mac *priv,
+				     int chk_csum)
+{
+	struct ksz_port *port = &priv->port;
+	struct ksz_sw *sw = port->sw;
+	bool csum_not_ok = false;
+
+	if (!sw_is_switch(sw))
+		return skb;
+
+	if (skb->ip_summed) {
+		if (chk_csum & 1)
+			csum_not_ok = using_tail_tag(sw);
+
+#ifdef CONFIG_KSZ_HSR
+		else if (chk_csum & 2)
+			csum_not_ok = using_hsr(sw);
+#endif
+	}
+	if (csum_not_ok) {
+		struct sk_buff *nskb;
+		int len = skb->len;
+
+		len += sw_extra_mtu;
+		nskb = dev_alloc_skb(len);
+		if (nskb) {
+			skb_copy_and_csum_dev(skb, nskb->data);
+			skb->ip_summed = 0;
+			nskb->len = skb->len;
+			copy_old_skb(skb, nskb);
+			skb = nskb;
+		}
+	}
+	return skb;
+}  /* sw_mac_tx_pre */
+
+static struct sk_buff *sw_mac_tx(struct net_device *dev, struct sk_buff *skb,
+				 struct ksz_mac *priv)
+{
+	struct ksz_port *port = &priv->port;
+	struct ksz_sw *sw = port->sw;
+
+	if (!sw_is_switch(sw))
+		return skb;
+
+	skb = sw->net_ops->final_skb(sw, skb, dev, port);
+	if (!skb) {
+		return NULL;
+	}
+	return skb;
+}  /* sw_mac_tx */
+
+static bool sw_mac_open_first(struct net_device *dev, struct ksz_mac *priv,
+			      int *rx_mode)
+{
+	struct ksz_sw *sw = priv->port.sw;
+	struct ksz_mac *hw_priv;
+
+	if (!sw_is_switch(sw))
+		return false;
+	hw_priv = priv->hw_priv;
+	priv->multi = false;
+	priv->promisc = false;
+	if (hw_priv->opened > 0) {
+		netif_carrier_off(dev);
+		return true;
+	}
+	if (0 == hw_priv->opened) {
+		struct net_device *main_dev = hw_priv->net;
+
+		/* Need to wait for adjust_link to start operation. */
+		hw_priv->port.ready = false;
+		hw_priv->hw_multi = 0;
+		hw_priv->hw_promisc = 0;
+		sw_reset_mac_mib(hw_priv);
+		*rx_mode = sw->net_ops->open_dev(sw, main_dev, &hw_priv->port,
+						 main_dev->dev_addr);
+	}
+	return false;
+}  /* sw_mac_open_first */
+
+static void sw_mac_open_next(struct ksz_sw *sw, struct ksz_mac *hw_priv,
+			     int rx_mode)
+{
+	if (0 == hw_priv->opened) {
+		if (rx_mode & 1) {
+			hw_priv->hw_multi = 1;
+			hw_set_multicast(hw_priv->dev, hw_priv->hw_multi);
+		}
+		if (rx_mode & 2) {
+			hw_priv->hw_promisc = 1;
+			hw_set_promisc(hw_priv->dev, hw_priv->hw_promisc);
+		}
+		sw->net_ops->open(sw);
+	}
+}  /* sw_mac_open_second */
+
+static bool sw_mac_open_final(struct ksz_sw *sw, struct net_device *dev,
+			      struct ksz_mac *hw_priv, struct ksz_mac *priv)
+{
+	sw->net_ops->open_port(sw, dev, &priv->port);
+	hw_priv->opened++;
+	return (hw_priv->opened > 1);
+}  /* sw_mac_open_final */
+
+
+#ifdef CONFIG_KSZ_IBA_ONLY
+static bool sw_chk_iba(struct ksz_mac *priv, bool *do_exit)
+{
+	struct ksz_sw *sw = priv->port.sw;
+	bool init = false;
+
+	if (sw_is_switch(sw)) {
+
+		/* Still under initialization in IBA-only mode. */
+		if (IBA_USE_CODE_PREPARE == sw->info->iba.use_iba) {
+			cancel_delayed_work_sync(&sw->set_ops);
+
+			/* May not started yet. */
+			if (IBA_USE_CODE_PREPARE == sw->info->iba.use_iba) {
+				kfree(sw->dev);
+				priv->port.sw = NULL;
+				init = true;
+			}
+			if (do_exit)
+				*do_exit = false;
+		}
+	}
+	return init;
+}
+#endif
+
+static bool sw_mac_close(struct net_device *dev, struct ksz_mac *priv, int iba)
+{
+	struct ksz_mac *hw_priv;
+	struct ksz_sw *sw;
+
+#ifdef CONFIG_KSZ_IBA_ONLY
+	sw_chk_iba(priv, NULL);
+#endif
+	sw = priv->port.sw;
+	if (!sw_is_switch(sw))
+		return false;
+	hw_priv = priv->hw_priv;
+	dev_set_multicast(priv, false);
+	dev_set_promisc(priv, false);
+	sw->net_ops->close_port(sw, dev, &priv->port);
+	hw_priv->opened--;
+	if (!hw_priv->opened) {
+		bool ok_to_close = true;
+
+		/* Not invoked during closing down the MAC. */
+#ifdef CONFIG_KSZ_IBA
+		ok_to_close = (sw->info->iba.use_iba != IBA_USE_CODE_LOST);
+#endif
+		if (ok_to_close) {
+			sw->net_ops->close(sw);
+
+#ifdef CONFIG_KSZ_IBA
+			/* Do not reset switch at the end. */
+			if (sw->info->iba.use_iba >= IBA_USE_CODE_ONLY)
+				sw->info->iba.use_iba = iba;
+#endif
+			sw->net_ops->stop(sw, true);
+
+#ifdef CONFIG_KSZ_IBA_ONLY
+			schedule_work(&hw_priv->rmv_dev);
+#endif
+		}
+	} else {
+		netif_carrier_off(dev);
+		return true;
+	}
+
+	/* Reset ready indication. */
+	hw_priv->port.ready = false;
+
+	/* Last network device to turn off is not the first. */
+	if (dev != sw->netdev[0]) {
+		if (hw_priv->saved_phy)
+			phy_stop(hw_priv->saved_phy);
+
+		/* Do not want to call phy_stop for this phydev. */
+		if (dev->phydev && dev->phydev->mdio.bus)
+			phy_detach(dev->phydev);
+		dev->phydev = NULL;
+	}
+	return false;
+}
+
+#ifndef KSZ_USE_PHYLINK
+static void sw_adjust_link(struct net_device *dev)
+{}
+#endif
+
+#if defined(CONFIG_HAVE_KSZ9897) || defined(CONFIG_HAVE_LAN937X)
+static int get_net_ready(struct net_device *dev)
+{
+	struct ksz_mac *priv = get_netdev_priv(dev);
+
+	return priv->hw_priv->port.ready;
+}  /* get_net_ready */
+#endif
+
+static void prep_sw_first(struct ksz_sw *sw, int *port_count,
+			  int *mib_port_count, int *dev_count, char *dev_name,
+			  const void *phylink_ops)
+{
+	*port_count = 1;
+	*mib_port_count = 1;
+	*dev_count = 1;
+	dev_name[0] = '\0';
+#if defined(CONFIG_HAVE_KSZ9897) || defined(CONFIG_HAVE_LAN937X)
+	sw->net_ops->get_ready = get_net_ready;
+#endif
+	sw->net_ops->setup_special(sw, port_count, mib_port_count, dev_count,
+				   phylink_ops);
+}  /* prep_sw_first */
+
+static void prep_sw_dev(struct ksz_sw *sw, struct ksz_mac *priv, int i,
+			int port_count, int mib_port_count, char *dev_name,
+			phy_interface_t phy_mode)
+{
+	priv->phy_addr = sw->net_ops->setup_dev(sw, priv->net, dev_name,
+						&priv->port, i, port_count,
+						mib_port_count);
+
+#if defined(CONFIG_PHYLINK) || defined(CONFIG_PHYLINK_MODULE)
+	/* MAC driver uses phylink. */
+	if (sw->phylink_ops) {
+		/* Use the phylink created by the switch driver. */
+		if (!priv->phylink)
+			priv->phylink = priv->port.pl;
+		return;
+	}
+#endif
+	if (!priv->net->phydev) {
+		char phy_id[MII_BUS_ID_SIZE];
+		char bus_id[MII_BUS_ID_SIZE];
+
+		snprintf(bus_id, MII_BUS_ID_SIZE, "sw.%d", sw->id);
+		snprintf(phy_id, MII_BUS_ID_SIZE, PHY_ID_FMT, bus_id,
+			 priv->phy_addr);
+		phy_attach(priv->net, phy_id, phy_mode);
+	}
+}  /* prep_sw_dev */
+
+#ifdef CONFIG_OF
+#if defined(CONFIG_KSZ_IBA_ONLY) || defined(CONFIG_KSZ_SMI)
+#ifdef KSZ_USE_SPI_DEV
+static int get_sw_irq(struct device **ext_dev)
+{
+	int spi_bus, spi_select;
+	struct device *dev;
+	char name[20];
+
+	for (spi_bus = 0; spi_bus < 2; spi_bus++) {
+		for (spi_select = 0; spi_select < 4; spi_select++) {
+			sprintf(name, "spi%d.%d\n", spi_bus, spi_select);
+			dev = bus_find_device_by_name(&spi_bus_type, NULL,
+						      name);
+			if (dev && dev->of_node) {
+				int irq = of_irq_get(dev->of_node, 0);
+
+				if (ext_dev)
+					*ext_dev = dev;
+				return irq;
+			}
+		}
+	}
+	return -1;
+}  /* get_sw_irq */
+#endif
+#endif
+#endif
+
+#ifdef CONFIG_KSZ_IBA_ONLY
+/**
+ * netdev_start_iba - Start using IBA for register access
+ *
+ * This routine starts using IBA for register access.
+ */
+static void netdev_start_iba(struct work_struct *work)
+{
+	struct delayed_work *dwork = to_delayed_work(work);
+	struct ksz_sw *sw = container_of(dwork, struct ksz_sw, set_ops);
+	struct ksz_iba_info *iba = &sw->info->iba;
+	struct net_device *dev = sw->main_dev;
+	struct ksz_port *port = sw->main_port;
+	struct ksz_mac *priv;
+	int rx_mode;
+
+	if (IBA_USE_CODE_PREPARE != iba->use_iba)
+		return;
+
+	/* Communication is not ready if a cable connection is used. */
+	if (!port->ready) {
+		port->iba_ready = false;
+		schedule_delayed_work(&sw->set_ops, 1);
+		return;
+	}
+
+	/* Need some time after link is established. */
+	if (!port->iba_ready) {
+		port->iba_ready = true;
+		schedule_delayed_work(&sw->set_ops, 10);
+		return;
+	}
+
+	priv = get_netdev_priv(dev);
+
+	sw->reg = &sw_iba_ops;
+	iba->cnt = 0;
+	if (ksz_probe_next(sw->dev)) {
+		priv->parent = NULL;
+		priv->port.sw = NULL;
+
+		/* Turn on link for regular network operation. */
+		netif_carrier_on(dev);
+		return;
+	}
+
+#ifdef CONFIG_1588_PTP
+	sw->ptp_hw.reg = &ptp_iba_ops;
+#endif
+
+	sw_mac_init(dev, priv);
+
+	priv->multi = false;
+	priv->promisc = false;
+
+	priv->hw_multi = 0;
+	priv->hw_promisc = 0;
+	sw_reset_mac_mib(priv);
+	rx_mode = sw->net_ops->open_dev(sw, dev, &priv->port, dev->dev_addr);
+	if (rx_mode & 1) {
+		priv->hw_multi = 1;
+		hw_set_multicast(priv->dev, priv->hw_multi);
+	}
+	if (rx_mode & 2) {
+		priv->hw_promisc = 1;
+		hw_set_promisc(priv->dev, priv->hw_promisc);
+	}
+	sw->net_ops->open(sw);
+
+	sw->net_ops->open_port(sw, dev, &priv->port);
+	priv->opened++;
+
+	/* set_rx_mode is called before open. */
+	sw_set_rx_mode(priv->net);
+
+	/* Signal IBA initialization is complete. */
+	if (IBA_USE_CODE_PREPARE == sw->info->iba.use_iba)
+		sw->info->iba.use_iba = IBA_USE_CODE_ONLY;
+}  /* netdev_start_iba */
+
+static int create_sw_dev(struct net_device *dev, struct ksz_mac *priv)
+{
+	struct sw_priv *ks;
+	struct ksz_sw *sw;
+	int ret;
+
+	/*
+	 * Stop normal traffic from going out until the switch is
+	 * configured to block looping frames.
+	 */
+	netif_carrier_off(dev);
+
+	ks = kzalloc(sizeof(struct sw_priv), GFP_KERNEL);
+	ks->hw_dev = dev;
+	ks->dev = &dev->dev;
+
+#ifdef CONFIG_OF
+#ifdef KSZ_USE_SPI_DEV
+	ks->irq = get_sw_irq(&ks->of_dev);
+#endif
+#endif
+
+	sw = &ks->sw;
+	ret = ksz_probe_prep(ks, dev);
+	if (ret)
+		return ret;
+
+	sw->net_ops->get_ready = get_net_ready;
+	sw->netdev[0] = dev;
+	sw->netport[0] = &priv->port;
+	sw->main_dev = dev;
+	sw->main_port = &priv->port;
+	sw->dev_count = 1;
+
+	INIT_DELAYED_WORK(&sw->set_ops, netdev_start_iba);
+
+	sw_set_dev(sw, dev, dev->dev_addr);
+
+	priv->parent = sw->dev;
+	priv->port.sw = sw;
+
+	sw->main_port->iba_ready = true;
+
+#ifdef DEBUG_MSG
+	init_dbg();
+#endif
+	return 0;
+}  /* create_sw_dev */
+#endif
+
+static void sw_mac_init_pre(void)
+{
+	/* Accommodate tail tag. */
+	sw_extra_mtu = 1;
+#ifdef CONFIG_1588_PTP
+	/* Accommodate PTP. */
+	sw_extra_mtu += 4;
+#endif
+#ifdef CONFIG_KSZ_HSR
+	/* Accommodate HSR. */
+	sw_extra_mtu += 6;
+#endif
+
+	/* Make it even. */
+	sw_extra_mtu += 1;
+	sw_extra_mtu &= ~1;
+
+#ifdef CONFIG_KSZ8795_EMBEDDED
+	ksz8795_init();
+#endif
+#ifdef CONFIG_KSZ8895_EMBEDDED
+	ksz8895_init();
+#endif
+#ifdef CONFIG_KSZ9897_EMBEDDED
+	ksz9897_init();
+#endif
+#ifdef CONFIG_LAN937X_EMBEDDED
+	lan937x_init();
+#endif
+}
+
+static void sw_mac_exit(struct ksz_mac *priv)
+{
+	struct net_device *dev = priv->net;
+	struct ksz_sw *sw = priv->port.sw;
+	int i;
+
+#ifndef CONFIG_KSZ_IBA_ONLY
+#ifdef CONFIG_KSZ_DLR
+	if (sw->features & DLR_HW)
+		exit_dlr_sysfs(&dev->dev);
+#endif
+#ifdef CONFIG_1588_PTP
+	if (sw->features & PTP_HW)
+		exit_ptp_sysfs(&priv->ptp_sysfs, &dev->dev);
+#endif
+	exit_sw_sysfs(sw, &priv->sysfs, &dev->dev);
+#endif
+#if defined(CONFIG_PHYLINK) || defined(CONFIG_PHYLINK_MODULE)
+	if (priv->port.pl == priv->phylink)
+		priv->port.pl = NULL;
+#endif
+	for (i = 1; i < sw->dev_count + sw->dev_offset; i++) {
+		dev = sw->netdev[i];
+		if (!dev)
+			continue;
+		priv = get_netdev_priv(dev);
+		cancel_delayed_work_sync(&priv->port.link_update);
+#if defined(CONFIG_PHYLINK) || defined(CONFIG_PHYLINK_MODULE)
+		if (priv->phylink) {
+			if (priv->port.pl == priv->phylink)
+				priv->port.pl = NULL;
+			phylink_destroy(priv->phylink);
+			priv->phylink = NULL;
+		}
+#endif
+		unregister_netdev(dev);
+		if (dev->phydev && dev->phydev->mdio.bus)
+			phy_detach(dev->phydev);
+		free_netdev(dev);
+	}
+}  /* sw_mac_exit */
+
+#ifdef CONFIG_KSZ_IBA_ONLY
+static void rmv_dev_work(struct work_struct *work)
+{
+	struct ksz_mac *priv = container_of(work, struct ksz_mac, rmv_dev);
+	struct ksz_sw *sw = priv->port.sw;
+
+	sw_mac_exit(priv);
+	sw->net_ops->leave_dev(sw);
+	ksz_remove(sw->dev);
+	priv->port.sw = NULL;
+}  /* rmv_dev_work */
+#endif
+
+static void sw_mac_remove(struct net_device *dev, struct ksz_mac *priv)
+{
+	struct ksz_sw *sw = priv->port.sw;
+	bool do_exit = true;
+
+	if (!sw_is_switch(sw))
+		return;
+
+#ifdef CONFIG_KSZ_IBA_ONLY
+	if (sw_chk_iba(priv, &do_exit))
+		return;
+#endif
+	if (do_exit) {
+
+#ifdef CONFIG_KSZ_IBA
+		/* Indicate no more network communication. */
+		if (sw->info->iba.use_iba >= IBA_USE_CODE_ONLY)
+			sw->info->iba.use_iba = IBA_USE_CODE_LOST;
+		else if (sw->info->iba.use_iba == IBA_USE_CODE_ON)
+			sw->net_ops->close_port(sw, sw->main_dev,
+						sw->main_port);
+#endif
+
+		/* Remove all other network devices. */
+		sw_mac_exit(priv);
+
+		/* Indicate MAC is being turned off. */
+		if (netif_running(sw->netdev[0]))
+			priv->opened = 0;
+
+		cancel_delayed_work_sync(&priv->port.link_update);
+		sw->net_ops->stop(sw, true);
+
+		/* Restore phy device if it was replaced. */
+		if (dev->phydev && dev->phydev != priv->saved_phy) {
+			if (dev->phydev->mdio.bus) {
+				/* Need to clear the pointer. */
+				dev->phydev->adjust_link = NULL;
+				phy_detach(dev->phydev);
+			}
+			dev->phydev = priv->saved_phy;
+		}
+	}
+
+#ifdef CONFIG_KSZ_SMI
+	if (priv->sw_pdev)
+		smi_remove(priv->sw_pdev);
+#endif
+	sw->net_ops->leave_dev(sw);
+#ifdef CONFIG_KSZ_IBA_ONLY
+	ksz_remove(sw->dev);
+#endif
+	priv->port.sw = NULL;
+#if defined(CONFIG_KSZ_IBA_ONLY) && defined(DEBUG_MSG)
+	exit_dbg();
+#endif
+#ifdef CONFIG_KSZ8795_EMBEDDED
+	ksz8795_exit();
+#endif
+#ifdef CONFIG_KSZ8895_EMBEDDED
+	ksz8895_exit();
+#endif
+#ifdef CONFIG_KSZ9897_EMBEDDED
+	ksz9897_exit();
+#endif
+#ifdef CONFIG_LAN937X_EMBEDDED
+	lan937x_exit();
+#endif
+}  /* sw_mac_remove */
+
+#endif
+
diff -Napur linux-linux4microchip-2022.10-orig/drivers/net/ethernet/micrel/ksz_mac.h linux-linux4microchip-2022.10/drivers/net/ethernet/micrel/ksz_mac.h
--- linux-linux4microchip-2022.10-orig/drivers/net/ethernet/micrel/ksz_mac.h	1969-12-31 16:00:00.000000000 -0800
+++ linux-linux4microchip-2022.10/drivers/net/ethernet/micrel/ksz_mac.h	2023-12-06 22:21:38.000000000 -0800
@@ -0,0 +1,57 @@
+
+
+#ifndef KSZ_MAC_H
+#define KSZ_MAC_H
+
+#if defined(CONFIG_HAVE_KSZ9897)
+#include "ksz_cfg_9897.h"
+#elif defined(CONFIG_HAVE_KSZ8795)
+#include "ksz_cfg_8795.h"
+#elif defined(CONFIG_HAVE_KSZ8895)
+#include "ksz_cfg_8895.h"
+#elif defined(CONFIG_HAVE_KSZ8863)
+#include "ksz_cfg_8863.h"
+#elif defined(CONFIG_HAVE_KSZ8463)
+#include "ksz_cfg_8463.h"
+#elif defined(CONFIG_HAVE_LAN937X)
+#include "../microchip/lan937x_cfg.h"
+#endif
+
+#ifndef CONFIG_KSZ_SWITCH_EMBEDDED
+#ifdef CONFIG_HAVE_LAN937X
+#include "../microchip/lan937x_dev.h"
+#else
+#include "ksz_spi_net.h"
+#endif
+#endif
+
+struct ksz_mac {
+	void			*dev;
+	struct net_device	*net;
+	struct ksz_mac		*hw_priv;
+	struct platform_device	*sw_pdev;
+	struct phy_device	dummy_phy;
+	struct phy_device	*saved_phy;
+	struct ksz_port		port;
+	spinlock_t		tx_lock;
+	int			msg_enable;
+	int			phy_addr;
+	u32			multi:2;
+	u32			promisc:1;
+	u8			opened;
+	u8			hw_multi;
+	u8			hw_promisc;
+	void			*parent;
+	struct delayed_work	promisc_reset;
+	struct work_struct	rmv_dev;
+	struct ksz_sw_sysfs	sysfs;
+#ifdef CONFIG_1588_PTP
+	struct ksz_ptp_sysfs	ptp_sysfs;
+#endif
+#if defined(CONFIG_PHYLINK) || defined(CONFIG_PHYLINK_MODULE)
+	struct phylink		*phylink;
+#endif
+};
+
+#endif
+
diff -Napur linux-linux4microchip-2022.10-orig/drivers/net/ethernet/micrel/ksz_mac_pre.c linux-linux4microchip-2022.10/drivers/net/ethernet/micrel/ksz_mac_pre.c
--- linux-linux4microchip-2022.10-orig/drivers/net/ethernet/micrel/ksz_mac_pre.c	1969-12-31 16:00:00.000000000 -0800
+++ linux-linux4microchip-2022.10/drivers/net/ethernet/micrel/ksz_mac_pre.c	2023-11-27 13:24:48.000000000 -0800
@@ -0,0 +1,66 @@
+
+
+#ifndef KSZ_MAC_PRE
+#define KSZ_MAC_PRE
+
+static struct ksz_mac *get_ksz_mac(void *ptr);
+static struct ksz_mac *get_netdev_priv(struct net_device *dev);
+static void sw_reset_mac_mib(struct ksz_mac *priv);
+static void hw_set_multicast(void *priv, int multicast);
+static void hw_set_promisc(void *priv, int promisc);
+static int sw_mac_init(struct net_device *net, struct ksz_mac *priv);
+
+
+#if defined(CONFIG_IBA_KSZ9897) || defined(CONFIG_IBA_LAN937X)
+#define CONFIG_KSZ_IBA_ONLY
+#endif
+
+#ifdef CONFIG_KSZ_SWITCH_EMBEDDED
+#include <linux/of_irq.h>
+#include <linux/spi/spi.h>
+#include <linux/crc32.h>
+#include <linux/ip.h>
+#include <net/ip.h>
+#include <net/ipv6.h>
+
+/* Need to predefine get_sysfs_data. */
+
+#ifndef get_sysfs_data
+struct ksz_port;
+
+static void get_sysfs_data_(struct net_device *dev,
+	struct semaphore **proc_sem, struct ksz_port **port);
+
+#define get_sysfs_data		get_sysfs_data_
+#endif
+
+static void copy_old_skb(struct sk_buff *old, struct sk_buff *skb);
+#define DO_NOT_USE_COPY_SKB
+
+#if defined(CONFIG_IBA_KSZ9897)
+#include "iba-ksz9897.c"
+#elif defined(CONFIG_HAVE_KSZ9897)
+#include "i2c-ksz9897.c"
+#elif defined(CONFIG_HAVE_KSZ8795)
+#include "spi-ksz8795.c"
+#elif defined(CONFIG_SMI_KSZ8895)
+#include "smi-ksz8895.c"
+#elif defined(CONFIG_HAVE_KSZ8895)
+#include "spi-ksz8895.c"
+#elif defined(CONFIG_SMI_KSZ8863)
+#include "smi-ksz8863.c"
+#elif defined(CONFIG_HAVE_KSZ8863)
+#include "i2c-ksz8863.c"
+#elif defined(CONFIG_IBA_LAN937X)
+#include "../microchip/iba-lan937x.c"
+#elif defined(CONFIG_SMI_LAN937X)
+#include "../microchip/smi-lan937x.c"
+#elif defined(CONFIG_HAVE_LAN937X)
+#include "../microchip/spi-lan937x.c"
+#endif
+#endif
+
+#include "ksz_mac.h"
+
+#endif
+
diff -Napur linux-linux4microchip-2022.10-orig/drivers/net/ethernet/micrel/ksz_mrp_api.h linux-linux4microchip-2022.10/drivers/net/ethernet/micrel/ksz_mrp_api.h
--- linux-linux4microchip-2022.10-orig/drivers/net/ethernet/micrel/ksz_mrp_api.h	1969-12-31 16:00:00.000000000 -0800
+++ linux-linux4microchip-2022.10/drivers/net/ethernet/micrel/ksz_mrp_api.h	2023-04-25 16:13:54.876163472 -0700
@@ -0,0 +1,174 @@
+/**
+ * Microchip MRP driver API header
+ *
+ * Copyright (c) 2015-2017 Microchip Technology Inc.
+ *	Tristram Ha <Tristram.Ha@microchip.com>
+ *
+ * Copyright (c) 2014-2015 Micrel, Inc.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ */
+
+
+#ifndef KSZ_MRP_API_H
+#define KSZ_MRP_API_H
+
+#ifndef ETH_ALEN
+#define ETH_ALEN			6
+#endif
+
+
+enum {
+	DEV_IOC_MRP_REPORT = DEV_IOC_LAST,
+};
+
+enum {
+	DEV_MRP_ATTRIBUTE,
+};
+
+struct MRP_mac {
+	u8 addr[ETH_ALEN];
+} __packed;
+
+struct MRP_vlan {
+	u16 id;
+} __packed;
+
+enum {
+	SRP_IGNORED,
+	SRP_ASKING_FAILED,
+	SRP_READY,
+	SRP_READY_FAILED,
+	SRP_ADVERTISE,
+	SRP_FAILED,
+};
+
+struct SRP_listener {
+	u8 id[8];
+	u8 substate;
+} __packed;
+
+struct SRP_talker {
+	u8 id[8];
+	u8 dest[ETH_ALEN];
+	u16 vlan_id;
+	u16 MaxFrameSize;
+	u16 MaxIntervalFrames;
+	u8 reserved:4;
+	u8 rank:1;
+	u8 priority:3;
+	u32 AccumulatedLatency;
+	u8 bridge_id[8];
+	u8 FailureCode;
+} __packed;
+
+enum {
+	RFC_NO_ERROR,
+	RFC_NO_BANDWIDTH,
+	RFC_NO_RESOURCES,
+	RFC_NO_BANDWIDTH_FOR_TRAFFIC_CLASS,
+	RFC_STREAM_ID_USED,
+	RFC_DEST_ADDR_USED,
+	RFC_PREEMPTED_BY_RANK,
+	RFC_LATENCY_CHANGED,
+	RFC_PORT_IS_NOT_AVB,
+	RFC_USE_DIFF_DEST_ADDR,
+	RFC_OUT_OF_MSRP_RESOURCE,
+	RFC_OUT_OF_MMRP_RESOURCE,
+	RFC_CANNOT_STORE_DEST_ADDR,
+	RFC_PRIORITY_IS_NOT_SR_CLASS,
+	RFC_MAXFRAMESIZE_TOO_LARGE,
+	RFC_MAXFANINPORTS_LIMIT_REACHED,
+	RFC_FIRSTVALUE_CHANGED,
+	RFC_VLAN_BLOCKED,
+	RFC_VLAN_TAGGING_DISABLED,
+	RFC_SR_CLASS_PRIORITY_MISMATCHED,
+};
+
+enum {
+	SR_CLASS_G,
+	SR_CLASS_F,
+	SR_CLASS_E,
+	SR_CLASS_D,
+	SR_CLASS_C,
+	SR_CLASS_B,
+	SR_CLASS_A,
+	SR_CLASS_NUM
+};
+
+#define SR_CLASS_A_ID	6
+#define SR_CLASS_B_ID	5
+
+struct SRP_domain_class {
+	u8 id;
+	u8 priority;
+	u16 vlan_id;
+} __packed;
+
+union mrp_data {
+	struct MRP_mac mac;
+	struct MRP_vlan vlan;
+	struct SRP_talker talker;
+	struct SRP_listener listener;
+	struct SRP_domain_class domain;
+	u32 data[4];
+} __packed;
+
+enum {
+	MRP_ACTION_RX,
+	MRP_ACTION_TX,
+	MRP_ACTION_TX_NEW,
+	MRP_ACTION_LV,
+	MRP_ACTION_ON,
+	MRP_ACTION_OFF,
+	MRP_ACTION_DECL,
+	MRP_ACTION_DROP,
+
+	MRP_ACTION_DBG,
+	MRP_ACTION_SPEED,
+	MRP_ACTION_DELTA,
+	MRP_ACTION_CHK_TALKER,
+	MRP_ACTION_CHK_REG,
+};
+
+enum {
+	MRP_TYPE_UNKNOWN,
+	MRP_TYPE_MAC,
+	MRP_TYPE_VLAN,
+	MRP_TYPE_TALKER,
+	MRP_TYPE_LISTENER,
+	MRP_TYPE_DOMAIN,
+	MRP_TYPE_PORT,
+};
+
+struct mrp_cfg_options {
+	u8 action;
+	u8 type;
+	u8 port;
+	u8 new_decl;
+	union mrp_data data;
+} __packed;
+
+#define SIZEOF_MRP_mac	\
+	(sizeof(struct MRP_mac) + sizeof(u8) * 4)
+
+#define SIZEOF_MRP_vlan	\
+	(sizeof(struct MRP_vlan) + sizeof(u8) * 4)
+
+#define SIZEOF_SRP_talker	\
+	(sizeof(struct SRP_talker) + sizeof(u8) * 4)
+
+#define SIZEOF_SRP_listener	\
+	(sizeof(struct SRP_listener) + sizeof(u8) * 4)
+
+#define SIZEOF_SRP_domain_class	\
+	(sizeof(struct SRP_domain_class) + sizeof(u8) * 4)
+
+#endif
diff -Napur linux-linux4microchip-2022.10-orig/drivers/net/ethernet/micrel/ksz_mrp.c linux-linux4microchip-2022.10/drivers/net/ethernet/micrel/ksz_mrp.c
--- linux-linux4microchip-2022.10-orig/drivers/net/ethernet/micrel/ksz_mrp.c	1969-12-31 16:00:00.000000000 -0800
+++ linux-linux4microchip-2022.10/drivers/net/ethernet/micrel/ksz_mrp.c	2024-01-02 14:15:29.352817312 -0800
@@ -0,0 +1,8722 @@
+/**
+ * Microchip MRP driver code
+ *
+ * Copyright (c) 2015-2023 Microchip Technology Inc.
+ *	Tristram Ha <Tristram.Ha@microchip.com>
+ *
+ * Copyright (c) 2014-2015 Micrel, Inc.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ */
+
+
+/* CONFIG_KSZ_AVB enables hardware bandwidth programming.
+   CONFIG_KSZ_MRP enables MMRP and MVRP.
+   CONFIG_KSZ_MSRP enables MSRP.  Needs CONFIG_KSZ_AVB and CONFIG_KSZ_MRP.
+*/
+
+#if !defined(CONFIG_KSZ_AVB) || !defined(CONFIG_KSZ_MRP)
+#undef CONFIG_KSZ_MSRP
+#endif
+
+#ifdef CONFIG_KSZ_MRP
+#if 0
+#define DEBUG_MRP_MEM
+#endif
+#if 1
+#define DEBUG_MRP_OPER
+#endif
+#if 1
+#define DEBUG_MVRP
+static int dbg_mrp_vlan = 1;
+#endif
+#if 1
+#define DEBUG_MSRP
+#endif
+#endif
+
+#if 1
+#ifdef CONFIG_KSZ_MRP
+static int mrp_10_1_2f_hack;
+static int fqtss_hack;
+static int fqtss_34_2_3_hack;
+#endif
+#ifdef CONFIG_KSZ_MSRP
+static int mrp_10_1_8a_hack;
+static int mrp_10_5_1_hack;
+static int mrp_10_5_1c_hack;
+static int mrp_10_5_1d_hack;
+static int msrp_35_1_14g_hack;
+static int fqtss_34_2_1b_hack;
+static int fqtss_34_2_5b_hack;
+static int fqtss_34_2_9b_hack;
+static int regeneration_hack;
+#endif
+#endif
+
+
+static const u8 bcast_addr[] = {
+	0xFF, 0xFF, 0xFF, 0xFF, 0xFF, 0xFF,
+};
+
+static const u8 maap_addr[][6] = {
+	/* Used by MAAP. */
+	{ 0x91, 0xE0, 0xF0, 0x00, 0xFF, 0x00 },
+	/* Used by AVB Discovery Protocol. */
+	{ 0x91, 0xE0, 0xF0, 0x01, 0x00, 0x00 },
+	/* Used by AVB AECP Test Status Message. */
+	{ 0x01, 0x1B, 0xC5, 0x0A, 0xC0, 0x00 },
+
+	/* This is sent by Mac and contains only a repeat of these bytes!? */
+	{ 0xAB, 0xBA, 0xCA, 0xFE, 0xBE, 0xEF },
+};
+
+#define BCAST_DA_ACL_ENTRY		10
+#define AVB_BOUNDARY_ACL_ENTRY		11
+
+struct avtpdu {
+	u8 subtype;
+#ifdef __BIG_ENDIAN_BITFIELD
+	u8 sv:1;
+	u8 version:3;
+	u8 mr:1;
+	u8 fsd:2;
+	u8 tv:2;
+#else
+	u8 tv:2;
+	u8 fsd:2;
+	u8 mr:1;
+	u8 version:3;
+	u8 sv:1;
+#endif
+	u8 seq_num;
+#ifdef __BIG_ENDIAN_BITFIELD
+	u8 format_spec_data_1:7;
+	u8 tu:1;
+#else
+	u8 tu:1;
+	u8 format_spec_data_1:7;
+#endif
+	u8 stream_id[8];
+	u32 avtp_timestamp;
+	u32 format_spec_data_2;
+	u16 stream_data_len;
+	u16 format_spec_data_3;
+} __packed;
+
+struct avtpdu_ctrl {
+#ifdef __BIG_ENDIAN_BITFIELD
+	u32 subtype:8;
+	u32 sv:1;
+	u32 version:3;
+	u32 format_spec_data:9;
+	u32 ctrl_data_len:11;
+#else
+	u32 ctrl_data_len:11;
+	u32 format_spec_data:9;
+	u8 version:3;
+	u8 sv:1;
+	u32 subtype:8;
+#endif
+	u8 stream_id[8];
+} __packed;
+
+struct maap_pdu {
+	u8 subtype;
+#ifdef __BIG_ENDIAN_BITFIELD
+	u8 sv:1;
+	u8 version:3;
+	u8 message_type:4;
+	u16 maap_version:5;
+	u16 ctrl_data_len:11;
+#else
+	u8 message_type:4;
+	u8 version:3;
+	u8 sv:1;
+	union {
+		struct {
+			u16 ctrl_data_len:11;
+			u16 maap_version:5;
+		} __packed data_len;
+		u16 data;
+	} __packed len;
+#endif
+	u8 stream_id[8];
+	u8 req_start_addr[6];
+	u16 req_cnt;
+	u8 conflict_start_addr[6];
+	u16 conflict_cnt;
+} __packed;
+
+#define AVTP_SUBTYPE_AAF		0x02
+#define AVTP_SUBTYPE_CVF		0x03
+#define AVTP_SUBTYPE_CRF		0x04
+#define AVTP_SUBTYPE_TSCF		0x05
+#define AVTP_SUBTYPE_SVF		0x06
+#define AVTP_SUBTYPE_RVF		0x07
+#define AVTP_SUBTYPE_VSF_STREAM		0x6F
+#define AVTP_SUBTYPE_EF_STREAM		0x7F
+#define AVTP_SUBTYPE_NTSCF		0x82
+#define AVTP_SUBTYPE_ADP		0xFA
+#define AVTP_SUBTYPE_AECP		0xFB
+#define AVTP_SUBTYPE_ACMP		0xFC
+#define AVTP_SUBTYPE_MAAP		0xFE
+#define AVTP_SUBTYPE_EF_CONTROL		0xFF
+
+#ifdef CONFIG_KSZ_AVB
+static struct mrp_port_info *mrp_get_port_info(struct mrp_info *mrp, u8 port)
+{
+	return &mrp->port_info[port];
+}  /* mrp_get_port_info */
+
+static void setup_acl_drop(struct mrp_info *mrp, uint port)
+{
+	struct ksz_sw *sw = mrp->parent;
+	struct ksz_port_cfg *cfg = get_port_cfg(sw, port);
+	struct ksz_acl_table *acl;
+	int i = BCAST_DA_ACL_ENTRY;
+
+	mutex_lock(&sw->acllock);
+
+	acl = &cfg->acl_info[i];
+	acl->mode = ACL_MODE_LAYER_2;
+	acl->enable = ACL_ENABLE_2_BOTH;
+	acl->equal = 1;
+	acl->src = 0;
+	memcpy(acl->mac, bcast_addr, ETH_ALEN);
+	acl->eth_type = 0x22F0;
+
+	acl->first_rule = i;
+	acl->ruleset = (1 << i);
+	acl->ruleset = 0;
+
+	acl->map_mode = ACL_MAP_MODE_REPLACE;
+	acl->ports = 0;
+	sw_w_acl_table(sw, port, i, acl);
+
+	mutex_unlock(&sw->acllock);
+}  /* setup_acl_drop */
+
+static void setup_acl_remap(struct mrp_info *mrp, uint port)
+{
+	struct ksz_sw *sw = mrp->parent;
+	struct ksz_port_cfg *cfg = get_port_cfg(sw, port);
+	struct mrp_port_info *info = mrp_get_port_info(mrp, port);
+	struct ksz_acl_table *acl;
+	int i = AVB_BOUNDARY_ACL_ENTRY;
+
+	mutex_lock(&sw->acllock);
+
+	acl = &cfg->acl_info[i];
+	acl->mode = ACL_MODE_LAYER_2;
+	acl->enable = ACL_ENABLE_2_TYPE;
+	acl->equal = 1;
+	acl->eth_type = 0x22F0;
+
+#ifdef CONFIG_KSZ_MSRP
+	if (regeneration_hack) {
+		acl->eth_type = 0;
+		acl->equal = 0;
+	}
+#endif
+
+	acl->first_rule = i;
+	acl->ruleset = (1 << i);
+
+	acl->map_mode = ACL_MAP_MODE_DISABLE;
+	acl->ports = 0;
+	acl->prio_mode = ACL_PRIO_MODE_REPLACE;
+	acl->prio = info->priority[SR_CLASS_B].regenerated_priority;
+	acl->vlan_prio_replace = 1;
+	acl->vlan_prio = info->priority[SR_CLASS_B].regenerated_priority;
+	sw_w_acl_table(sw, port, i, acl);
+
+	mutex_unlock(&sw->acllock);
+}  /* setup_acl_remap */
+
+static void enable_acl_remap(struct mrp_info *mrp, uint port, bool remap,
+			     bool drop)
+{
+	struct ksz_sw *sw = mrp->parent;
+	struct ksz_port_cfg *cfg = get_port_cfg(sw, port);
+	struct ksz_acl_table *acl;
+	int i = AVB_BOUNDARY_ACL_ENTRY;
+
+	mutex_lock(&sw->acllock);
+
+	acl = &cfg->acl_info[i];
+dbg_msg(" remap: %d=%d-%d %d:%d"NL, port, remap, drop, cfg->avb_a, cfg->avb_b);
+	if (remap)
+		acl->ruleset = (1 << i);
+	else
+		acl->ruleset = 0;
+	sw_w_acl_ruleset(sw, port, i, acl);
+
+	i = BCAST_DA_ACL_ENTRY;
+	acl = &cfg->acl_info[i];
+	if (drop)
+		acl->ruleset = (1 << i);
+	else
+		acl->ruleset = 0;
+	sw_w_acl_ruleset(sw, port, i, acl);
+
+	mutex_unlock(&sw->acllock);
+	sw->ops->acquire(sw);
+	if (drop) {
+		if (!(mrp->mcast_ports & (1 << port))) {
+			mrp->mcast_ports |= (1 << port);
+			if (!mrp->mcast_port_cnt)
+				sw->ops->fwd_unk_mcast(sw, true);
+			mrp->mcast_port_cnt++;
+		}
+	} else {
+		if (mrp->mcast_ports & (1 << port)) {
+			mrp->mcast_ports &= ~(1 << port);
+			mrp->mcast_port_cnt--;
+			if (!mrp->mcast_port_cnt)
+				sw->ops->fwd_unk_mcast(sw, false);
+		}
+	}
+	sw->ops->release(sw);
+}  /* enable_acl_remap */
+
+#define SW_CREDIT_SHAPING_SCALE	0x10000
+#define SW_CREDIT_SHAPING_S	16
+
+#define CREDIT_PERCENTAGE_S	(16 + 9)
+
+#define NETWORK_SPEED_IN_MBIT	1000000
+
+static char *format_num(char *str, u32 num)
+{
+	u32 num0;
+	u32 num1;
+	u32 num2;
+	u32 num3;
+
+	num0 = num % 1000;
+	num1 = (num / 1000) % 1000;
+	num2 = (num / 1000000) % 1000;
+	num3 = (num / 1000000000);
+	if (num3)
+		sprintf(str, "%u,%03u,%03u,%03u", num3, num2, num1, num0);
+	else if (num2)
+		sprintf(str, "%u,%03u,%03u", num2, num1, num0);
+	else if (num1)
+		sprintf(str, "%u,%3u", num1, num0);
+	else
+		sprintf(str, "%u", num0);
+	return str;
+}  /* format_num */
+
+static char *format_per(char *str, u32 per, bool verbose)
+{
+	u64 val;
+	u32 num;
+	u32 num0;
+	u32 num1;
+
+	val = per;
+	val *= 1000;
+	val += 1 << (CREDIT_PERCENTAGE_S - 1);
+	val >>= CREDIT_PERCENTAGE_S;
+	num = (u32) val;
+	num0 = num % 1000;
+	num1 = num / 1000;
+	if (num1)
+		sprintf(str, "%u.%03u", num1, num0);
+	else
+		sprintf(str, "0.%03u", num0);
+	if (verbose)
+		strcpy(str, PER_CHAR);
+	return str;
+}  /* format_per */
+
+static u16 get_credit_increment(u32 speed, u32 bandwidth)
+{
+	u64 val;
+
+	speed *= NETWORK_SPEED_IN_MBIT;
+
+	/* Cannot get higher than the running speed. */
+	if (bandwidth > speed)
+		return 0;
+	val = bandwidth;
+	val <<= SW_CREDIT_SHAPING_S;
+	val += speed / 2;
+	val = div_u64_u32(val, speed);
+
+	/* Cannot become zero. */
+	if (!val)
+		val = 1;
+	return (u16) val;
+}  /* get_credit_increment */
+
+static u16 get_credit_watermark(u16 size)
+{
+	/* Preamable + vlan_ethhdr + CRC + IFG = 42 */
+	size += 8 + 14 + 4 + 4 + 12;
+	return size;
+}  /* get_credit_watermark */
+
+static u32 get_idle_slope(u32 speed, u32 bandwidth)
+{
+	u64 val;
+
+	speed *= NETWORK_SPEED_IN_MBIT;
+
+	/* Cannot get higher than the running speed. */
+	if (bandwidth > speed)
+		return 0;
+	val = bandwidth;
+	val *= 100;
+	val <<= CREDIT_PERCENTAGE_S;
+	val = div_u64_u32(val, speed);
+	return (u32) val;
+}  /* get_idle_slope */
+
+static u32 get_send_slope(u32 idle_slope)
+{
+	u32 slope;
+
+	slope = 100;
+	slope <<= CREDIT_PERCENTAGE_S;
+	slope -= idle_slope;
+	return slope;
+}  /* get_send_slope */
+
+static u8 get_queue_priority(struct mrp_info *mrp, int tc)
+{
+	return mrp->queue[tc];
+}  /* get_queue_priority */
+
+static void srp_cfg_credit_shaper(struct mrp_info *mrp, uint port,
+	struct mrp_port_info *info, struct mrp_traffic_info *traffic)
+{
+	u16 credit;
+	u32 idle;
+	u32 send;
+	u8 queue;
+#ifdef DEBUG_MRP_OPER
+	char bw_str[20];
+	char idle_str[20];
+	char send_str[20];
+#endif
+	struct ksz_sw *sw = mrp->parent;
+
+	if (!(sw->features & AVB_SUPPORT))
+		return;
+	credit = get_credit_increment(info->speed, traffic->bandwidth_used);
+	idle = get_idle_slope(info->speed, traffic->bandwidth_used);
+	send = get_send_slope(idle);
+	queue = traffic->queue;
+	info->bandwidth[traffic->queue].operIdleSlope = idle;
+
+#ifdef DEBUG_MRP_OPER
+	format_num(bw_str, traffic->bandwidth_used);
+	format_per(idle_str, idle, true);
+	format_per(send_str, send, true);
+dbg_msg("  %s %d:%d=%u; %s %s %s"NL, __func__, port,
+	queue, credit,
+	bw_str, idle_str, send_str);
+#endif
+
+#if defined(CONFIG_HAVE_KSZ9897) || defined(CONFIG_HAVE_LAN937X)
+	do {
+		if (credit < 2)
+			credit = info->credit[queue];
+		sw->reg->lock(sw);
+		if (credit > 1) {
+			port_set_increment(sw, port, queue, credit);
+#if 0
+			port_set_schedule_mode(sw, port, queue,
+				MTI_SCHEDULE_STRICT_PRIO);
+			port_set_shaping(sw, port, queue, MTI_SHAPING_SRP);
+#endif
+		} else {
+#if 0
+			port_set_schedule_mode(sw, port, queue,
+				MTI_SCHEDULE_WRR);
+			port_set_shaping(sw, port, queue, MTI_SHAPING_OFF);
+#endif
+		}
+		sw->reg->unlock(sw);
+	} while (0);
+#endif
+}  /* srp_cfg_credit_shaper */
+
+static void srp_cfg_idle_slope(struct mrp_info *mrp, uint port, uint queue,
+	struct mrp_port_info *info, u32 idle)
+{
+	u16 credit;
+	u16 credit_lo;
+	u16 credit_hi;
+	u32 send;
+	u64 idle_slope;
+#if 0
+	char bw_str[20];
+	char idle_str[20];
+	char send_str[20];
+#endif
+	struct ksz_sw *sw = mrp->parent;
+
+	idle_slope = info->speed;
+	idle_slope *= NETWORK_SPEED_IN_MBIT;
+	idle_slope *= idle;
+	send = 100;
+	send <<= CREDIT_PERCENTAGE_S;
+	idle_slope = div_u64_u32(idle_slope, send);
+
+	credit = get_credit_increment(info->speed, idle_slope);
+	info->credit[queue] = credit;
+	send = get_send_slope(idle);
+	credit_hi = get_credit_watermark(mrp->max_interference_size);
+	credit_lo = get_credit_watermark(1500);
+#if 0
+	format_num(bw_str, idle_slope);
+	format_per(idle_str, idle, true);
+	format_per(send_str, send, true);
+dbg_msg("  %s %d:%d=%u %u %u; %s %s %s"NL, __func__, port,
+	queue, credit, credit_hi, credit_lo,
+	bw_str, idle_str, send_str);
+#endif
+
+#if defined(CONFIG_HAVE_KSZ9897) || defined(CONFIG_HAVE_LAN937X)
+	sw->reg->lock(sw);
+	if (credit > 1) {
+		port_set_hi_water_mark(sw, port, queue, credit_hi);
+		port_set_lo_water_mark(sw, port, queue, credit_lo);
+		port_set_increment(sw, port, queue, credit);
+		port_set_schedule_mode(sw, port, queue,
+			MTI_SCHEDULE_STRICT_PRIO);
+		port_set_shaping(sw, port, queue, MTI_SHAPING_SRP);
+	} else {
+		port_set_schedule_mode(sw, port, queue,
+			MTI_SCHEDULE_WRR);
+		port_set_shaping(sw, port, queue, MTI_SHAPING_OFF);
+	}
+	sw->reg->unlock(sw);
+#endif
+}  /* srp_cfg_idle_slope */
+
+static u32 calculate_max_bandwidth(u32 speed, u32 percent)
+{
+	u64 bandwidth;
+
+	bandwidth = speed;
+	bandwidth *= percent;
+	bandwidth *= NETWORK_SPEED_IN_MBIT;
+	bandwidth = div_u64_u32(bandwidth, 100);
+	return (u32) bandwidth;
+}  /* calculate_max_bandwidth */
+
+static int get_traffic_index(int tc)
+{
+	if (SR_CLASS_A == tc)
+		return 1;
+	return 0;
+}
+
+static struct mrp_traffic_info *get_traffic_info(struct mrp_port_info *port,
+	int tc)
+{
+	if (SR_CLASS_A == tc)
+		return &port->traffic[1];
+	else
+		return &port->traffic[0];
+}  /* get_traffic_info */
+#endif
+
+#ifdef CONFIG_KSZ_MRP
+static void mrp_cfg_dest_addr(struct mrp_info *mrp, u8 index, u8 *dest,
+	u32 ports, u16 fid)
+{
+	struct ksz_sw *sw = mrp->parent;
+
+#ifdef DEBUG_MRP_OPER
+dbg_msg("  %s %d=%02x:%02x:%02x:%02x:%02x:%02x %04x %x"NL, __func__, index,
+dest[0], dest[1], dest[2], dest[3], dest[4],dest[5],
+ports, fid);
+#endif
+	ports &= SRP_PORT_AVAIL | SRP_PORT_READY;
+	sw->ops->cfg_mac(sw, index, dest, ports, false, fid != 0, fid);
+}  /* mrp_cfg_dest_addr */
+
+static void mrp_cfg_vlan(struct mrp_info *mrp, u8 index, u16 vid, u16 fid,
+	u32 ports)
+{
+	struct ksz_sw *sw = mrp->parent;
+
+#ifdef DEBUG_MVRP
+if (dbg_mrp_vlan || vid == 2 || vid > 4090)
+dbg_msg("  %s %d=%x %x %04x"NL, __func__, index, vid, fid, ports);
+#endif
+	ports &= SRP_PORT_AVAIL | SRP_PORT_READY;
+
+#if 0
+	/* Needed to catch unwanted multicast traffic. */
+	if (ports)
+		ports |= sw->HOST_MASK;
+#endif
+	sw->ops->cfg_vlan(sw, index, vid, fid, ports);
+}  /* mrp_cfg_vlan */
+
+#if 0
+static int mrp_req_cfg_vlan(struct mrp_info *mrp, u8 index, u16 vid, u16 fid,
+	u32 ports)
+{
+	struct sk_buff *skb;
+	u16 *data;
+	u32 *dword;
+
+	skb = alloc_skb(64, GFP_ATOMIC);
+	if (!skb)
+		return -ENOMEM;
+
+	data = (u16 *)skb->data;
+	*data++ = index;
+	*data++ = vid;
+	*data++ = fid;
+	dword = (u32 *)data;
+	*dword = ports;
+	skb_queue_tail(&mrp->vlanq, skb);
+	if (!mrp->vlanq_sched) {
+		mrp->vlanq_sched = 1;
+		schedule_work(&mrp->cfg_vlan);
+	}
+	return 0;
+}  /* mrp_req_cfg_vlan */
+#endif
+
+static void mrp_cfg_vlan_work(struct work_struct *work)
+{
+#ifdef DEBUG_MVRP
+	int save_dbg_vlan;
+#endif
+	u16 *data;
+	u32 *dword;
+	u8 index;
+	u16 vid;
+	u16 fid;
+	u32 ports;
+	bool last;
+	struct sk_buff *skb;
+	struct mrp_info *mrp = container_of(work, struct mrp_info, cfg_vlan);
+
+#ifdef CONFIG_KSZ_IBA
+	if (iba_stopped(mrp->parent))
+		return;
+#endif
+
+	last = skb_queue_empty(&mrp->vlanq);
+	while (!last) {
+		skb = skb_dequeue(&mrp->vlanq);
+		last = skb_queue_empty(&mrp->vlanq);
+		if (!skb)
+			continue;
+		data = (u16 *)skb->data;
+		index = (u8) *data++;
+		vid = *data++;
+		fid = *data++;
+		dword = (u32 *)data;
+		ports = *dword;
+#ifdef DEBUG_MVRP
+		save_dbg_vlan = dbg_mrp_vlan;
+		dbg_mrp_vlan = 0;
+#endif
+		mrp_cfg_vlan(mrp, index, vid, fid, ports);
+#ifdef DEBUG_MVRP
+		dbg_mrp_vlan = save_dbg_vlan;
+#endif
+		kfree_skb(skb);
+	}
+	mrp->vlanq_sched = 0;
+}  /* mrp_cfg_vlan_work */
+
+static int cmp_mac(void *first, void *second)
+{
+	int cmp;
+	struct mrp_mac_info *a = first;
+	struct mrp_mac_info *b = second;
+
+#if 0
+dbg_msg("%s %02x:%02x:%02x:%02x:%02x:%02x=%x %02x:%02x:%02x:%02x:%02x:%02x=%x"NL, __func__,
+a->addr[0], a->addr[1], a->addr[2], a->addr[3], a->addr[4], a->addr[5], a->fid,
+b->addr[0], b->addr[1], b->addr[2], b->addr[3], b->addr[4], b->addr[5], b->fid);
+#endif
+	cmp = a->fid - b->fid;
+	if (!cmp)
+		cmp = memcmp(a->addr, b->addr, ETH_ALEN);
+	return cmp;
+}  /* cmp_mac */
+
+static void show_mac_info(void *this)
+{
+	struct mrp_mac_info *info = this;
+
+	dbg_msg(
+		"%02x:%02x:%02x:%02x:%02x:%02x %d=%03x r:%04x s:%04x t:%04x"NL,
+		info->addr[0], info->addr[1], info->addr[2],
+		info->addr[3], info->addr[4], info->addr[5],
+		info->index, info->fid,
+		info->mrp_ports, info->srp_ports, info->tx_ports);
+}  /* show_mac_info */
+
+static int cmp_vlan(void *first, void *second)
+{
+	int cmp;
+	struct mrp_vlan_info *a = first;
+	struct mrp_vlan_info *b = second;
+
+	cmp = a->vid - b->vid;
+	if (!cmp)
+		cmp = memcmp(a->addr, b->addr, ETH_ALEN);
+	return cmp;
+}  /* cmp_vlan */
+
+static void show_vlan_info(void *this)
+{
+	struct mrp_vlan_info *info = this;
+
+	if (info->addr[0] != 0xff)
+		dbg_msg(
+			"[%02x:%02x:%02x:%02x:%02x:%02x] ",
+			info->addr[0], info->addr[1], info->addr[2],
+			info->addr[3], info->addr[4], info->addr[5]);
+	dbg_msg(
+		"%d=%03x.%03x r:%04x t:%04x"NL, info->index,
+		info->vid, info->fid, info->ports, info->tx_ports);
+}  /* show_vlan_info */
+
+#ifdef CONFIG_KSZ_MSRP
+static int get_traffic_class(struct mrp_info *mrp, u8 prio)
+{
+	return mrp->tc[prio];
+}  /* get_traffic_class */
+
+static int frames_per_sec(int traffic_class)
+{
+	int frames;
+
+	switch (traffic_class) {
+	case SR_CLASS_A:
+		frames = 8000;
+		break;
+	case SR_CLASS_B:
+	default:
+		frames = 4000;
+	}
+	return frames;
+}  /* frames_per_sec */
+
+static u64 calculate_bandwidth(u32 size, u32 interval, u32 frames)
+{
+	u64 bandwidth;
+
+	/* Mininum frame size is 68. */
+#if 1
+	if (size < 46 && fqtss_34_2_5b_hack)
+		size = 46 - 4;
+	else
+#endif
+	if (size < 46)
+		size = 46;
+#if 1
+	if (size < 100 && fqtss_34_2_9b_hack)
+		size = 100;
+#endif
+	bandwidth = size;
+
+	/* Preamable + vlan_ethhdr + CRC + IFG */
+	bandwidth += 8 + 14 + 4 + 4 + 12;
+
+	/* AVnu MSRP test says to add one more byte for bridge. */
+	bandwidth += 1;
+	bandwidth *= interval;
+	bandwidth *= frames;
+	bandwidth *= 8;
+	return bandwidth;
+}  /* calculate_bandwidth */
+
+static u64 get_stream_age(struct mrp_port_info *info)
+{
+	struct timespec64 ts;
+	u64 age;
+
+	ts = ktime_to_timespec64(ktime_get_real());
+	age = ts.tv_sec;
+	age *= 1000000000;
+	age += ts.tv_nsec;
+	if (age == info->age)
+		age++;
+	info->age = age;
+	return age;
+}
+
+static void prepare_stream_info(struct SRP_reserv *reserv,
+				struct srp_stream_info *x)
+{
+	x->reserv = reserv;
+	x->id = reserv->stream->id;
+	x->age = reserv->streamAge;
+	x->rank = reserv->stream->rank;
+}  /* prepare_stream_info */
+
+static int cmp_stream(struct srp_stream_info *a, struct srp_stream_info *b)
+{
+	int cmp;
+
+	/* Only rank and stream age are compared. */
+	cmp = a->rank - b->rank;
+	if (cmp)
+		return cmp;
+
+	if (a->age > b->age)
+		return 1;
+	else if (a->age < b->age)
+		return -1;
+	cmp = memcmp(a->id, b->id, 8);
+	return cmp;
+}  /* cmp_stream */
+
+static int cmp_lower_stream(void *first, void *second)
+{
+	return cmp_stream(second, first);
+}  /* cmp_lower_stream */
+
+static int cmp_higher_stream(void *first, void *second)
+{
+	return cmp_stream(first, second);
+}  /* cmp_higher_stream */
+
+static void show_stream_info(void *this)
+{
+	struct srp_stream_info *info = this;
+
+	dbg_msg(
+		"r:%u t:%08llx %02x:%02x:%02x:%02x:%02x:%02x:%02x:%02x"NL,
+		info->rank, info->age,
+		info->id[0], info->id[1], info->id[2], info->id[3],
+		info->id[4], info->id[5], info->id[6], info->id[7]);
+}  /* show_stream_info */
+#endif
+
+#ifdef DEBUG_MRP_BASIC
+static void *get_show(int (*cmp)(void *a, void *b))
+{
+	if (cmp == cmp_mac)
+		return show_mac_info;
+	else if (cmp == cmp_vlan)
+		return show_vlan_info;
+
+#ifdef CONFIG_KSZ_MSRP
+	else
+		return show_stream_info;
+#else
+	else
+		return NULL;
+#endif
+}  /* get_show */
+#endif
+
+static void mrp_init_list(struct mrp_node_anchor *list)
+{
+	list->last = &list->anchor;
+	list->anchor.next = NULL;
+}  /* mrp_init_list */
+
+static void *mrp_find_node(struct mrp_node_anchor *list,
+	int (*cmp)(void *a, void *b), void *data)
+{
+	int c;
+	struct mrp_node *prev;
+	struct mrp_node *next;
+
+	prev = &list->anchor;
+	next = prev->next;
+	while (next) {
+		c = cmp(next->data, data);
+
+		/* Exact match. */
+		if (!c)
+			goto mrp_find_node_done;
+
+		/* Will not be found as list is sorted. */
+		if (c > 0) {
+#ifdef DEBUG_MRP_BASIC
+			void (*show)(void *this) = get_show(cmp);
+
+dbg_msg(" %s ", __func__);
+			show(next->data);
+#endif
+			next = NULL;
+			break;
+		}
+		prev = next;
+		next = prev->next;
+	}
+
+mrp_find_node_done:
+	list->last = prev;
+	return next;
+}  /* mrp_find_node */
+
+static void mrp_insert_node(struct mrp_node_anchor *list,
+	int (*cmp)(void *a, void *b), struct mrp_node *this)
+{
+	int c;
+	struct mrp_node *prev;
+	struct mrp_node *next;
+
+	if (list->last != &list->anchor) {
+		prev = list->last;
+		next = prev->next;
+#ifdef DEBUG_MRP_BASIC
+dbg_msg(" %s ", __func__);
+		if (next) {
+			void (*show)(void *this) = get_show(cmp);
+
+			show(next->data);
+		} else
+dbg_msg("last one"NL);
+#endif
+		c = 1;
+		if (next) {
+			c = cmp(next->data, this->data);
+		}
+		if (c > 0) {
+			this->next = prev->next;
+			prev->next = this;
+			list->last = &list->anchor;
+			list->cnt++;
+			return;
+		}
+	}
+	prev = &list->anchor;
+	next = prev->next;
+	while (next) {
+		c = cmp(next->data, this->data);
+
+		/* Stop if next one is higher in the list. */
+		if (c > 0)
+			break;
+		prev = next;
+		next = prev->next;
+	}
+	this->next = prev->next;
+	prev->next = this;
+	list->last = &list->anchor;
+	list->cnt++;
+}  /* mrp_insert_node */
+
+static struct mrp_node *mrp_delete_this_node(struct mrp_node_anchor *list,
+	int (*cmp)(void *a, void *b), struct mrp_node *this, int delete)
+{
+	int c;
+	struct mrp_node *prev;
+	struct mrp_node *next;
+
+	if (list->last != &list->anchor) {
+		prev = list->last;
+		next = prev->next;
+#ifdef DEBUG_MRP_BASIC
+dbg_msg(" %s ", __func__);
+		if (next) {
+			void (*show)(void *this) = get_show(cmp);
+
+			show(next->data);
+		} else
+dbg_msg("last one"NL);
+#endif
+		c = 1;
+		if (next == this)
+			c = 0;
+		else if (next)
+			c = cmp(next->data, this->data);
+
+		/* Exact match. */
+		if (!c)
+			goto mrp_delete_this_node_done;
+	}
+	prev = &list->anchor;
+	next = prev->next;
+	while (next) {
+		c = cmp(next->data, this->data);
+
+		/* Exact match. */
+		if (!c)
+			goto mrp_delete_this_node_done;
+
+		/* Stop if next one is higher in the list. */
+		if (c > 0)
+			break;
+		prev = next;
+		next = prev->next;
+	}
+
+	/* Nothing is removed. */
+	return NULL;
+
+mrp_delete_this_node_done:
+	prev->next = this->next;
+	list->last = &list->anchor;
+	list->cnt--;
+
+	/* Just remove the node. */
+	if (!delete)
+		return next;
+
+	/* Free the node data. */
+#ifdef DEBUG_MRP_MEM
+if (delete > 1)
+dbg_msg(" %s %p"NL, __func__, next->data);
+#endif
+	if (delete > 1)
+		kfree(next->data);
+
+#ifdef DEBUG_MRP_MEM
+dbg_msg(" %s %p"NL, __func__, next);
+#endif
+	/* Free the node. */
+	kfree(next);
+	return NULL;
+}  /* mrp_delete_this_node */
+
+static void mrp_delete_node(struct mrp_node_anchor *list,
+	int (*cmp)(void *a, void *b), struct mrp_node *this)
+{
+	mrp_delete_this_node(list, cmp, this, 2);
+}  /* mrp_delete_node */
+
+static struct mrp_node *mrp_remove_node(struct mrp_node_anchor *list,
+	int (*cmp)(void *a, void *b), struct mrp_node *this)
+{
+	return mrp_delete_this_node(list, cmp, this, 0);
+}  /* mrp_remove_node */
+
+static void mrp_show_node(struct mrp_node_anchor *list,
+	void (*show)(void *a))
+{
+	struct mrp_node *next;
+
+	next = list->anchor.next;
+	while (next) {
+		show(next->data);
+		next = next->next;
+	}
+}  /* mrp_show_node */
+#endif
+
+#ifdef CONFIG_KSZ_MSRP
+static struct SRP_stream *srp_create_stream(u8 *id, u8 *dest, u16 vlan_id,
+	u16 size, u16 interval, u8 prio, u8 rank, u8 reserved, u32 latency)
+{
+	struct SRP_stream *stream;
+
+	stream = kzalloc(sizeof(struct SRP_stream), GFP_KERNEL);
+#ifdef DEBUG_MRP_MEM
+dbg_msg(" %s %p"NL, __func__, stream);
+#endif
+	if (stream) {
+		memcpy(stream->id, id, 8);
+		memcpy(stream->dest, dest, ETH_ALEN);
+		stream->vlan_id = vlan_id;
+		stream->MaxFrameSize = size;
+		stream->MaxIntervalFrames = interval;
+		stream->priority = prio;
+		stream->rank = rank;
+		stream->reserved = reserved;
+		stream->latency = latency;
+	}
+	return stream;
+}  /* srp_create_stream */
+
+static struct SRP_stream *srp_find_stream_id(struct mrp_info *mrp, u8 *id)
+{
+	int cmp;
+	struct SRP_stream *stream;
+
+	stream = mrp->stream_by_id.id_next;
+	while (stream) {
+		cmp = memcmp(stream->id, id, 8);
+		if (!cmp)
+			break;
+		stream = stream->id_next;
+	}
+	return stream;
+}  /* srp_find_stream_id */
+
+static struct SRP_stream *srp_find_dest_addr(struct mrp_info *mrp, u8 *dest)
+{
+	int cmp;
+	struct SRP_stream *stream;
+
+	stream = mrp->stream_by_dest.dest_next;
+	while (stream) {
+		cmp = memcmp(stream->dest, dest, ETH_ALEN);
+		if (!cmp)
+			break;
+		stream = stream->dest_next;
+	}
+	return stream;
+}  /* srp_find_dest_addr */
+
+static void srp_insert_stream_by_id(struct mrp_info *mrp,
+	struct SRP_stream *stream)
+{
+	struct SRP_stream *prev;
+	struct SRP_stream *next;
+
+	prev = &mrp->stream_by_id;
+	next = prev->id_next;
+	while (next) {
+		if (memcmp(next->id, stream->id, 8) > 0)
+			break;
+		prev = next;
+		next = prev->id_next;
+	}
+	if (next) {
+		stream->id_next = next;
+		next->id_prev = stream;
+	}
+	stream->id_prev = prev;
+	prev->id_next = stream;
+}  /* srp_insert_stream_by_id */
+
+static void srp_insert_stream_by_dest(struct mrp_info *mrp,
+	struct SRP_stream *stream)
+{
+	struct SRP_stream *prev;
+	struct SRP_stream *next;
+
+	prev = &mrp->stream_by_dest;
+	next = prev->dest_next;
+	while (next) {
+		if (memcmp(next->dest, stream->dest, ETH_ALEN) > 0)
+			break;
+		prev = next;
+		next = prev->dest_next;
+	}
+	if (next) {
+		stream->dest_next = next;
+		next->dest_prev = stream;
+	}
+	stream->dest_prev = prev;
+	prev->dest_next = stream;
+}  /* srp_insert_stream_by_dest */
+
+static void srp_remove_stream(struct SRP_stream *stream, int free)
+{
+	if (stream->id_next)
+		stream->id_next->id_prev = stream->id_prev;
+	stream->id_prev->id_next = stream->id_next;
+	if (stream->dest_next)
+		stream->dest_next->dest_prev = stream->dest_prev;
+	stream->dest_prev->dest_next = stream->dest_next;
+#ifdef DEBUG_MRP_MEM
+dbg_msg(" %s %p %d"NL, __func__, stream, free);
+#endif
+	if (free)
+		kfree(stream);
+}  /* srp_remove_stream */
+
+static struct SRP_reserv *srp_create_reserv(u8 *id, u8 dir, u8 dec,
+					    u32 latency, const u8 *bridge_id,
+					    u8 code)
+{
+	struct SRP_reserv *reserv;
+
+	reserv = kzalloc(sizeof(struct SRP_reserv), GFP_KERNEL);
+#ifdef DEBUG_MRP_MEM
+dbg_msg(" %s %p"NL, __func__, reserv);
+#endif
+	if (reserv) {
+		memcpy(reserv->id, id, 8);
+		reserv->direction = dir;
+		reserv->declaration = dec;
+		reserv->latency = latency;
+		memcpy(reserv->bridge_id, bridge_id, 8);
+		reserv->code = code;
+		reserv->rx_code = code;
+		reserv->ticks = jiffies;
+	}
+	return reserv;
+}  /* srp_create_reserv */
+
+static struct SRP_reserv *srp_find_reserv(struct SRP_reserv *head, u8 *id,
+					  u8 dir)
+{
+	int cmp;
+	struct SRP_reserv *reserv;
+
+	reserv = head->next;
+	while (reserv) {
+		cmp = memcmp(reserv->id, id, 8);
+		if (!cmp && dir == reserv->direction)
+			break;
+		if (cmp > 0)
+			return NULL;
+		reserv = reserv->next;
+	}
+	return reserv;
+}  /* srp_find_reserv */
+
+static void srp_insert_reserv(struct SRP_reserv *head,
+			      struct SRP_reserv *reserv)
+{
+	int cmp;
+	struct SRP_reserv *prev;
+	struct SRP_reserv *next;
+
+	prev = head;
+	next = prev->next;
+	while (next) {
+		cmp = memcmp(next->id, reserv->id, 8);
+		if (cmp > 0)
+			break;
+		if (0 == cmp && next->declaration > reserv->declaration)
+			break;
+		prev = next;
+		next = prev->next;
+	}
+	if (next) {
+		reserv->next = next;
+		next->prev = reserv;
+	}
+	reserv->prev = prev;
+	prev->next = reserv;
+}  /* srp_insert_reserv */
+
+static void srp_remove_reserv(struct SRP_reserv *reserv, int free)
+{
+	if (reserv->next)
+		reserv->next->prev = reserv->prev;
+	reserv->prev->next = reserv->next;
+#ifdef DEBUG_MRP_MEM
+dbg_msg(" %s %p %d"NL, __func__, reserv, free);
+#endif
+	if (free)
+		kfree(reserv);
+}  /* srp_remove_reserv */
+
+static void chk_reserv(struct mrp_port_info *info, uint port)
+{
+	struct SRP_reserv *reserv;
+	int tc;
+	struct mrp_traffic_info *traffic;
+
+#if 0
+	if (!info->link)
+#else
+	if (!info->link && port > 2)
+#endif
+		return;
+dbg_msg("%d %d:"NL, info->index, port);
+dbg_msg("  registered: %p"NL, &info->registered);
+	reserv = info->registered.next;
+	while (reserv) {
+dbg_msg("%p %02x:%02x:%02x %d %d %02d=%04x"NL, reserv,
+reserv->id[5], reserv->id[6], reserv->id[7], reserv->direction,
+	reserv->declaration, reserv->code, reserv->code_bits);
+		reserv = reserv->next;
+	}
+dbg_msg("  declared: %p"NL, &info->declared);
+	reserv = info->declared.next;
+	while (reserv) {
+dbg_msg("%p %02x:%02x:%02x %d %d %02d=%04x"NL, reserv,
+reserv->id[5], reserv->id[6], reserv->id[7], reserv->direction,
+	reserv->declaration, reserv->code, reserv->code_bits);
+		reserv = reserv->next;
+	}
+	for (tc = SR_CLASS_A; tc >= SR_CLASS_B; tc--) {
+		traffic = get_traffic_info(info, tc);
+dbg_msg("  active:"NL);
+		mrp_show_node(&traffic->active, show_stream_info);
+#if 1
+dbg_msg("  passive:"NL);
+		mrp_show_node(&traffic->passive, show_stream_info);
+dbg_msg("m=%u u=%u l=%u"NL,
+		traffic->bandwidth_max,
+		traffic->bandwidth_used,
+		traffic->bandwidth_left);
+#endif
+	}
+dbg_msg("T:%u"NL, info->bandwidth_used);
+dbg_msg(NL);
+}  /* chk_reserv */
+#endif
+
+#ifdef CONFIG_KSZ_MRP
+static struct mrp_node *mrp_alloc_node(size_t data_size)
+{
+	struct mrp_node *node;
+
+	node = kzalloc(sizeof(struct mrp_node), GFP_KERNEL);
+#ifdef DEBUG_MRP_MEM
+dbg_msg(" %s %p %u"NL, __func__, node, data_size);
+#endif
+	if (!node)
+		return NULL;
+	node->data = kzalloc(data_size, GFP_KERNEL);
+#ifdef DEBUG_MRP_MEM
+dbg_msg(" %s %p"NL, __func__, node->data);
+#endif
+	if (!node->data) {
+		kfree(node);
+		return NULL;
+	}
+	return node;
+}  /* mrp_alloc_node */
+
+static void mrp_free_node(struct mrp_node *node)
+{
+#ifdef DEBUG_MRP_MEM
+dbg_msg(" %s %p %p"NL, __func__, node->data, node);
+#endif
+	kfree(node->data);
+	kfree(node);
+}  /* mrp_free_node */
+
+static struct mrp_node *mrp_get_mac_info(struct mrp_node_anchor *list,
+	u8 *addr, u16 fid)
+{
+	struct mrp_node *node;
+	struct mrp_mac_info data;
+
+	data.fid = fid;
+	memcpy(data.addr, addr, ETH_ALEN);
+	node = mrp_find_node(list, cmp_mac, &data);
+	if (!node) {
+		struct mrp_mac_info *info;
+
+		node = mrp_alloc_node(sizeof(struct mrp_mac_info));
+		if (!node)
+			return NULL;
+		info = node->data;
+		info->fid = fid;
+		memcpy(info->addr, data.addr, ETH_ALEN);
+		mrp_insert_node(list, cmp_mac, node);
+	}
+	return node;
+}  /* mrp_get_mac_info */
+
+static struct mrp_node *mrp_get_vlan_info(struct mrp_node_anchor *list,
+	u16 vid, u8 *addr)
+{
+	struct mrp_node *node;
+	struct mrp_vlan_info data;
+
+	data.vid = vid;
+	if (addr)
+		memcpy(data.addr, addr, ETH_ALEN);
+	else
+		memset(data.addr, 0xff, ETH_ALEN);
+	node = mrp_find_node(list, cmp_vlan, &data);
+	if (!node) {
+		struct mrp_vlan_info *info;
+
+		node = mrp_alloc_node(sizeof(struct mrp_vlan_info));
+		if (!node)
+			return NULL;
+		info = node->data;
+		info->vid = vid;
+		memcpy(info->addr, data.addr, ETH_ALEN);
+		mrp_insert_node(list, cmp_vlan, node);
+	}
+	return node;
+}  /* mrp_get_vlan_info */
+
+#if 0
+static u16 mrp_find_vlan_ports(struct mrp_node_anchor *list, u16 vid,
+	u8 *index, u16 *fid)
+{
+	struct mrp_node *prev;
+	struct mrp_node *next;
+	struct mrp_vlan_info *info;
+	u16 ports = 0;
+
+	prev = &list->anchor;
+	next = prev->next;
+	while (next) {
+		info = next->data;
+		if (vid == info->vid && !(info->ports & SRP_PORT_IGNORE)) {
+			ports |= info->ports;
+			if (info->ports) {
+				if (index && *index != info->index)
+					*index = info->index;
+				if (fid && *fid != info->fid)
+					*fid = info->fid;
+			}
+		} else if (vid < info->vid)
+			break;
+		prev = next;
+		next = prev->next;
+	}
+	return ports;
+}  /* mrp_find_vlan_ports */
+#endif
+
+static struct mrp_report *mrp_create_report(struct SRP_reserv *reserv,
+	u8 port)
+{
+	struct mrp_report *attrib;
+
+	attrib = kzalloc(sizeof(struct mrp_report), GFP_KERNEL);
+#ifdef DEBUG_MRP_MEM
+dbg_msg(" %s %p"NL, __func__, attrib);
+#endif
+	if (attrib) {
+		attrib->attrib = reserv;
+		attrib->port = port;
+	}
+	return attrib;
+}  /* mrp_create_report */
+
+static void add_attrib_report(struct mrp_info *mrp, void *ptr, u8 action,
+	u8 type, u8 port)
+{
+	struct mrp_report *attrib;
+
+	attrib = mrp_create_report(ptr, port);
+	if (!attrib)
+		return;
+
+	attrib->action = action;
+	attrib->type = type;
+
+	if (mrp->report_tail)
+		mrp->report_tail->next = attrib;
+	mrp->report_tail = attrib;
+	if (!mrp->report_head)
+		mrp->report_head = attrib;
+}  /* add_attrib_report */
+
+static u8 mrp_alloc_mac(struct mrp_info *mrp)
+{
+	struct ksz_sw *sw = mrp->parent;
+
+	return sw->ops->alloc_mac(sw);
+}  /* mrp_alloc_mac */
+
+static void mrp_free_mac(struct mrp_info *mrp, u8 index)
+{
+	struct ksz_sw *sw = mrp->parent;
+
+	sw->ops->free_mac(sw, index);
+}  /* mrp_free_mac */
+
+static u8 mrp_alloc_vlan(struct mrp_info *mrp)
+{
+	struct ksz_sw *sw = mrp->parent;
+
+	return sw->ops->alloc_vlan(sw);
+}  /* mrp_alloc_vlan */
+
+static void mrp_free_vlan(struct mrp_info *mrp, u8 index)
+{
+	struct ksz_sw *sw = mrp->parent;
+
+	sw->ops->free_vlan(sw, index);
+}  /* mrp_free_vlan */
+
+static u16 mrp_alloc_fid(struct mrp_info *mrp, u16 vid)
+{
+	struct ksz_sw *sw = mrp->parent;
+
+	return sw->ops->alloc_fid(sw, vid);
+}  /* mrp_alloc_fid */
+
+static void mrp_free_fid(struct mrp_info *mrp, u16 fid)
+{
+	struct ksz_sw *sw = mrp->parent;
+
+	sw->ops->free_fid(sw, fid);
+}  /* mrp_free_fid */
+
+#ifdef CONFIG_KSZ_MSRP
+static u16 mrp_get_fid(struct mrp_info *mrp, u16 vid, u8 *addr)
+{
+	struct mrp_node *node;
+	struct mrp_vlan_info data;
+	u16 fid = 0;
+
+	data.vid = vid;
+#if 0
+	memcpy(data.addr, addr, ETH_ALEN);
+#else
+	memset(data.addr, 0xFF, ETH_ALEN);
+#endif
+	node = mrp_find_node(&mrp->vlan_list, cmp_vlan, &data);
+	if (node) {
+		struct mrp_vlan_info *vlan = node->data;
+
+		fid = vlan->fid;
+	}
+	return fid;
+}  /* mrp_get_fid */
+#endif
+
+static int proc_mrp_lv(struct mrp_info *mrp, struct mrp_node *node, u16 ports,
+	u16 *tx_ports, u8 type)
+{
+	uint n;
+	uint p;
+	struct ksz_sw *sw = mrp->parent;
+	int result = DEV_IOC_OK;
+
+#ifdef DEBUG_MVRP
+if (dbg_mrp_vlan)
+#endif
+dbg_msg(" %s %x %x"NL, __func__, ports, *tx_ports);
+	if (!ports) {
+
+		/* Ask all ports to withdraw the declaration. */
+		for (n = 0; n <= mrp->ports; n++) {
+			p = get_phy_port(sw, n);
+			if (*tx_ports & (1 << p)) {
+				add_attrib_report(mrp, node,
+					MRP_ACTION_LV, type, p);
+			}
+		}
+		*tx_ports = 0;
+	} else {
+		uint q;
+		int cnt = 0;
+
+		for (n = 0; n <= mrp->ports; n++) {
+			p = get_phy_port(sw, n);
+			if (ports & (1 << p)) {
+				q = p;
+				cnt++;
+			}
+		}
+		if (1 == cnt) {
+			*tx_ports &= ~(1 << q);
+			add_attrib_report(mrp, node, MRP_ACTION_LV, type, q);
+		}
+	}
+	if (mrp->report_head)
+		result = DEV_IOC_MRP_REPORT;
+	return result;
+}  /* proc_mrp_lv */
+
+static int proc_mrp_lv_mac(struct mrp_info *mrp, uint port,
+	struct MRP_mac *mac)
+{
+	struct mrp_node *node;
+	struct mrp_mac_info data;
+	struct mrp_mac_info *info;
+	u16 mrp_ports;
+	u16 ports;
+	uint m = BIT(port);
+	int result = DEV_IOC_OK;
+
+dbg_msg(" %s %d=%02x:%02x:%02x:%02x:%02x:%02x"NL, __func__, port,
+	mac->addr[0],
+	mac->addr[1],
+	mac->addr[2],
+	mac->addr[3],
+	mac->addr[4],
+	mac->addr[5]);
+
+	data.fid = 0;
+	memcpy(data.addr, mac->addr, ETH_ALEN);
+	node = mrp_find_node(&mrp->mac_list, cmp_mac, &data);
+	if (!node)
+		return DEV_IOC_INVALID_CMD;
+
+	info = node->data;
+	mrp_ports = info->mrp_ports;
+	ports = info->ports;
+	info->rx_ports &= ~m;
+	info->mrp_ports &= ~m;
+	info->ports = info->mrp_ports | info->srp_ports;
+
+	/* There is no change in port membership. */
+	if (mrp_ports == info->mrp_ports)
+		return result;
+	if (ports != info->ports) {
+		mrp_cfg_dest_addr(mrp, info->index, info->addr, info->ports,
+			info->fid);
+		if (!info->ports) {
+			mrp_free_mac(mrp, info->index);
+			if (mrp->no_report)
+				mrp_delete_node(&mrp->mac_list, cmp_mac, node);
+			else
+				mrp_remove_node(&mrp->mac_list, cmp_mac, node);
+		}
+	}
+	if (!mrp->no_report)
+		result = proc_mrp_lv(mrp, node, info->mrp_ports,
+			&info->tx_ports, MRP_TYPE_MAC);
+#ifdef DEBUG
+	mrp_show_node(&mrp->mac_list, show_mac_info);
+#endif
+	return result;
+}  /* proc_mrp_lv_mac */
+
+static int proc_mrp_rx_mac(struct mrp_info *mrp, uint port,
+	struct MRP_mac *mac, u8 new_decl)
+{
+	struct mrp_node *node;
+	struct mrp_mac_info *info;
+	u16 mrp_ports;
+	u16 ports;
+	uint n;
+	uint q;
+	uint m = BIT(port);
+	struct ksz_sw *sw = mrp->parent;
+	int result = DEV_IOC_OK;
+
+dbg_msg(" %s %d=%d %02x:%02x:%02x:%02x:%02x:%02x"NL, __func__, port,
+	new_decl,
+	mac->addr[0],
+	mac->addr[1],
+	mac->addr[2],
+	mac->addr[3],
+	mac->addr[4],
+	mac->addr[5]);
+
+	node = mrp_get_mac_info(&mrp->mac_list, mac->addr, 0);
+	if (!node)
+		return -ENOMEM;
+	info = node->data;
+	info->rx_ports |= m;
+	info->srp_ports &= ~SRP_PORT_BLACKLIST;
+	info->ports &= ~SRP_PORT_BLACKLIST;
+	mrp_ports = info->mrp_ports;
+	ports = info->ports;
+	if (new_decl & 0x80)
+		new_decl &= ~0x80;
+	else
+		info->mrp_ports |= m;
+	info->ports = info->mrp_ports | info->srp_ports;
+
+	/* There is change in port membership. */
+	if (mrp_ports != info->mrp_ports) {
+
+		/* First time setting up MAC table. */
+		if (!ports) {
+			info->index = mrp_alloc_mac(mrp);
+			if (!info->index) {
+				mrp_delete_node(&mrp->mac_list, cmp_mac, node);
+				return -ENOMEM;
+			}
+		}
+		mrp_cfg_dest_addr(mrp, info->index, info->addr, info->ports,
+			info->fid);
+	}
+#ifdef DEBUG
+	mrp_show_node(&mrp->mac_list, show_mac_info);
+#endif
+	if (mrp->no_report)
+		return result;
+
+	if (!new_decl) {
+		struct ksz_sw *sw = mrp->parent;
+
+		if (sw->ops->get_tcDetected(sw, port))
+			new_decl = true;
+	}
+
+	/* Ask all other ports to declare the attribute. */
+	for (n = 0; n <= mrp->ports; n++) {
+		q = get_phy_port(sw, n);
+		if (q != port && (mrp->tx_ports & (1 << q)) &&
+		    (mrp->mmrp_tx_ports & (1 << q))) {
+			u8 action = MRP_ACTION_TX;
+			int existed = info->tx_ports & (1 << q);
+
+			/* Need to update declaration. */
+			if (new_decl) {
+				action = MRP_ACTION_TX_NEW;
+				existed = false;
+			}
+
+			/* Attribute was declared. */
+			if (existed)
+				continue;
+			info->tx_ports |= (1 << q);
+
+			add_attrib_report(mrp, node, action,
+				MRP_TYPE_MAC, q);
+		}
+	}
+	if (mrp->report_head)
+		result = DEV_IOC_MRP_REPORT;
+	return result;
+}  /* proc_mrp_rx_mac */
+
+static int proc_mrp_lv_vlan(struct mrp_info *mrp, uint port,
+	struct MRP_vlan *vlan)
+{
+	struct mrp_node *node;
+	struct mrp_vlan_info *info;
+	struct mrp_vlan_info data;
+	u16 ports;
+	uint m = BIT(port);
+	int result = DEV_IOC_OK;
+
+#ifdef DEBUG_MVRP
+if (dbg_mrp_vlan || vlan->id == 2 || vlan->id > 4090)
+dbg_msg(" %s %d=%d"NL, __func__, port, vlan->id);
+#endif
+	data.vid = vlan->id;
+	memset(data.addr, 0xff, ETH_ALEN);
+	node = mrp_find_node(&mrp->vlan_list, cmp_vlan, &data);
+	if (!node)
+		return DEV_IOC_INVALID_CMD;
+
+	info = node->data;
+	ports = info->ports;
+	info->rx_ports &= ~m;
+	info->ports &= ~m;
+
+	/* There is no change in port membership. */
+	if (ports == info->ports)
+		return result;
+#if 0
+	ports = mrp_find_vlan_ports(&mrp->vlan_list, data.vid, NULL, NULL);
+#endif
+	ports = info->ports;
+	mrp_cfg_vlan(mrp, info->index, info->vid, info->fid, ports);
+	if (!ports) {
+		mrp_free_fid(mrp, info->fid);
+		mrp_free_vlan(mrp, info->index);
+	}
+	if (!info->ports) {
+		if (mrp->no_report)
+			mrp_delete_node(&mrp->vlan_list, cmp_vlan, node);
+		else
+			mrp_remove_node(&mrp->vlan_list, cmp_vlan, node);
+#ifdef DEBUG_MVRP
+		if (mrp->vlan_list.cnt < 3)
+			dbg_mrp_vlan = 1;
+#endif
+	}
+
+	if (!mrp->no_report)
+		result = proc_mrp_lv(mrp, node, info->ports,
+			&info->tx_ports, MRP_TYPE_VLAN);
+#ifdef DEBUG_MVRP
+#ifdef DEBUG
+if (dbg_mrp_vlan)
+	mrp_show_node(&mrp->vlan_list, show_vlan_info);
+#endif
+#endif
+	return result;
+}  /* proc_mrp_lv_vlan */
+
+static int proc_mrp_rx_vlan(struct mrp_info *mrp, uint port,
+	struct MRP_vlan *vlan, u8 new_decl)
+{
+	struct mrp_node *node;
+	struct mrp_vlan_info *info;
+	struct ksz_port_cfg *cfg;
+	u16 ports;
+	int bit;
+	int index;
+	uint n;
+	uint q;
+	uint m = BIT(port);
+	struct ksz_sw *sw = mrp->parent;
+	int result = DEV_IOC_OK;
+
+#ifdef DEBUG_MVRP
+if (dbg_mrp_vlan || vlan->id == 2 || vlan->id > 4090)
+dbg_msg(" %s %d=%x:%d"NL, __func__, port, new_decl, vlan->id);
+#endif
+	node = mrp_get_vlan_info(&mrp->vlan_list, vlan->id, NULL);
+	if (!node)
+		return -ENOMEM;
+	info = node->data;
+	ports = info->ports;
+	info->rx_ports |= m;
+
+	/* MRP.c.10.1.5b
+	 * Non-participant does not send out declarations, but received
+	 * declaration is propagated, and traffic is not forwarded.
+	 */
+	if (new_decl & 0x80)
+		new_decl &= ~0x80;
+	else
+		info->ports |= m;
+
+	/* MVRP.c
+	 *
+	 */
+	cfg = get_port_cfg(sw, port);
+	index = vlan->id / VID_IN_DATA;
+	bit = vlan->id % VID_IN_DATA;
+	if (cfg->restricted) {
+		if (!(sw->info->vid[index] & (1 << bit)))
+			return result;
+	}
+
+	/* There is change in port membership. */
+	if (ports != info->ports) {
+#if 0
+		info->ports |= SRP_PORT_IGNORE;
+		ports = mrp_find_vlan_ports(&mrp->vlan_list, vlan->id,
+			&info->index, &info->fid);
+		info->ports &= ~SRP_PORT_IGNORE;
+#endif
+
+		/* First time setting up VLAN table. */
+		if (!ports) {
+			info->index = mrp_alloc_vlan(mrp);
+			if (!info->index) {
+				mrp_delete_node(&mrp->vlan_list, cmp_vlan,
+					node);
+				return -ENOMEM;
+			}
+			info->fid = mrp_alloc_fid(mrp, info->vid);
+		}
+		ports |= info->ports;
+		mrp_cfg_vlan(mrp, info->index, info->vid, info->fid, ports);
+	}
+#ifdef DEBUG_MVRP
+#ifdef DEBUG
+	if (dbg_mrp_vlan)
+		mrp_show_node(&mrp->vlan_list, show_vlan_info);
+	if (mrp->vlan_list.cnt > 4) {
+		if (dbg_mrp_vlan)
+dbg_msg(" stop dbg vlan"NL);
+		dbg_mrp_vlan = 0;
+	}
+#endif
+#endif
+	if (mrp->no_report)
+		return result;
+
+#if 1
+	if (mrp->rx_ports) {
+		if (mrp->rx_ports != sw->rx_ports[0])
+dbg_msg(" rx: %x %x"NL, mrp->rx_ports, sw->rx_ports[0]);
+	}
+#endif
+	if (!(mrp->rx_ports & m))
+		return result;
+
+	if (!new_decl) {
+		if (sw->ops->get_tcDetected(sw, port))
+			new_decl = true;
+	} else if (new_decl != 1)
+		return result;
+
+	/* Ask all other ports to declare the attribute. */
+	for (n = 0; n <= mrp->ports; n++) {
+		q = get_phy_port(sw, n);
+		if (q != port && (mrp->tx_ports & (1 << q)) &&
+		    (mrp->mvrp_tx_ports & (1 << q))) {
+			u8 action = MRP_ACTION_TX;
+			int existed = info->tx_ports & (1 << q);
+
+			/* Need to update declaration. */
+			if (new_decl) {
+				action = MRP_ACTION_TX_NEW;
+				existed = false;
+			}
+
+			/* Attribute was declared. */
+			if (existed)
+				continue;
+			info->tx_ports |= (1 << q);
+
+			add_attrib_report(mrp, node, action,
+				MRP_TYPE_VLAN, q);
+		}
+	}
+	if (mrp->report_head)
+		result = DEV_IOC_MRP_REPORT;
+	return result;
+}  /* proc_mrp_rx_vlan */
+#endif
+
+#ifdef CONFIG_KSZ_MSRP
+#define RFC_NO_RESOURCES_BIT		BIT(0)
+#define RFC_LATENCY_CHANGED_BIT		BIT(1)
+#define RFC_FIRSTVALUE_CHANGED_BIT	BIT(2)
+#define RFC_MAXFRAMESIZE_BIT		BIT(3)
+#define RFC_ASCAPABLE_BIT		BIT(4)
+#define RFC_SRP_BOUNDARY_BIT		BIT(5)
+#define RFC_PRIORITY_BIT		BIT(6)
+#define RFC_NO_BANDWIDTH_BIT		BIT(7)
+#define RFC_NO_BANDWIDTH_TC_BIT		BIT(8)
+#define RFC_STREAM_ID_BIT		BIT(9)
+#define RFC_DEST_ADDR_BIT		BIT(10)
+#define RFC_PREEMPTED_BIT		BIT(11)
+#define RFC_SR_MISMATCHED_BIT		BIT(12)
+#define RFC_VLAN_TAGGING_BIT		BIT(13)
+#define RFC_CHECK_BIT			BIT(31)
+
+static u8 msrp_failure_code(u32 code_bits)
+{
+	u8 code = RFC_NO_ERROR;
+
+	if (code_bits & RFC_NO_RESOURCES_BIT)
+		code = RFC_NO_RESOURCES;
+	else if (code_bits & RFC_LATENCY_CHANGED_BIT)
+		code = RFC_LATENCY_CHANGED;
+	else if (code_bits & RFC_FIRSTVALUE_CHANGED_BIT)
+		code = RFC_FIRSTVALUE_CHANGED;
+	else if (code_bits & RFC_MAXFRAMESIZE_BIT)
+		code = RFC_MAXFRAMESIZE_TOO_LARGE;
+	else if (code_bits & (RFC_ASCAPABLE_BIT | RFC_SRP_BOUNDARY_BIT))
+		code = RFC_PORT_IS_NOT_AVB;
+	else if (code_bits & RFC_PRIORITY_BIT)
+		code = RFC_PRIORITY_IS_NOT_SR_CLASS;
+	else if (code_bits & RFC_NO_BANDWIDTH_BIT)
+		code = RFC_NO_BANDWIDTH;
+	else if (code_bits & RFC_NO_BANDWIDTH_TC_BIT)
+		code = RFC_NO_BANDWIDTH_FOR_TRAFFIC_CLASS;
+	else if (code_bits & RFC_STREAM_ID_BIT)
+		code = RFC_STREAM_ID_USED;
+	else if (code_bits & RFC_DEST_ADDR_BIT)
+		code = RFC_DEST_ADDR_USED;
+	else if (code_bits & RFC_PREEMPTED_BIT)
+		code = RFC_PREEMPTED_BY_RANK;
+	else if (code_bits & RFC_SR_MISMATCHED_BIT)
+		code = RFC_SR_CLASS_PRIORITY_MISMATCHED;
+	else if (code_bits & RFC_VLAN_TAGGING_BIT)
+		code = RFC_VLAN_TAGGING_DISABLED;
+	return code;
+}  /* msrp_failure_code */
+#endif
+
+static void proc_mrp_attribute(struct mrp_info *mrp, u8 *data);
+
+#ifdef CONFIG_KSZ_AVB
+static void mrp_set_delta(struct mrp_info *mrp, u8 port, u32 A, u32 B)
+{
+	u8 data[40];
+	struct mrp_cfg_options *cmd = (struct mrp_cfg_options *) data;
+	struct ksz_sw *sw = mrp->parent;
+
+if (!(mrp->mask & (1 << port)))
+dbg_msg(" !!! %s %d"NL, __func__, port);
+	cmd->action = MRP_ACTION_DELTA;
+	cmd->type = MRP_TYPE_PORT;
+	cmd->port = get_log_port(sw, port);
+	cmd->new_decl = 0;
+	cmd->data.data[0] = A;
+	cmd->data.data[1] = B;
+	proc_mrp_attribute(mrp, data);
+}  /* mrp_set_delta */
+
+static void mrp_set_speed(struct mrp_info *mrp, u8 port, u32 speed, bool duplex)
+{
+	u8 data[40];
+	struct mrp_cfg_options *cmd = (struct mrp_cfg_options *) data;
+	struct ksz_sw *sw = mrp->parent;
+
+if (!(mrp->mask & (1 << port)))
+dbg_msg(" !!! %s %d"NL, __func__, port);
+	cmd->action = MRP_ACTION_SPEED;
+	cmd->type = MRP_TYPE_PORT;
+	cmd->port = get_log_port(sw, port);
+	cmd->new_decl = 0;
+	cmd->data.data[0] = speed;
+	cmd->data.data[1] = duplex;
+	proc_mrp_attribute(mrp, data);
+}  /* mrp_set_speed */
+#endif
+
+#ifdef CONFIG_KSZ_MSRP
+static void mrp_chk_talker(struct mrp_info *mrp, u8 port)
+{
+	u8 data[40];
+	struct mrp_cfg_options *cmd = (struct mrp_cfg_options *) data;
+	struct ksz_sw *sw = mrp->parent;
+
+if (!(mrp->mask & (1 << port)))
+dbg_msg(" !!! %s %d"NL, __func__, port);
+	cmd->action = MRP_ACTION_CHK_TALKER;
+	cmd->type = MRP_TYPE_PORT;
+	cmd->port = get_log_port(sw, port);
+	cmd->new_decl = 0;
+	proc_mrp_attribute(mrp, data);
+}  /* mrp_chk_talker */
+
+static void mrp_chk_registered(struct mrp_info *mrp, u8 port)
+{
+	u8 data[40];
+	struct mrp_cfg_options *cmd = (struct mrp_cfg_options *) data;
+	struct ksz_sw *sw = mrp->parent;
+
+if (!(mrp->mask & (1 << port)))
+dbg_msg(" !!! %s %d"NL, __func__, port);
+	cmd->action = MRP_ACTION_CHK_REG;
+	cmd->type = MRP_TYPE_PORT;
+	cmd->port = get_log_port(sw, port);
+	cmd->new_decl = 0;
+	proc_mrp_attribute(mrp, data);
+}  /* mrp_chk_registered */
+
+static int srp_update_mac(struct mrp_info *mrp, u8 *addr, u16 fid, u16 ports,
+	int up)
+{
+	struct mrp_node_anchor *list;
+	struct mrp_node *node;
+	struct mrp_mac_info *mac;
+	struct mrp_mac_info *update;
+
+	node = mrp_get_mac_info(&mrp->mac_list, addr, fid);
+	if (!node)
+		return -ENOMEM;
+	mac = node->data;
+	mac->srp_ports &= ~SRP_PORT_BLACKLIST;
+	if (up) {
+		/* Forward in port. */
+		mac->srp_ports |= ports;
+		ports = mac->srp_ports | mac->mrp_ports;
+		list = &mrp->mac_up;
+	} else {
+		/* Filter in port. */
+		mac->srp_ports &= ~ports;
+		ports = mac->ports & ~ports;
+		list = &mrp->mac_down;
+	}
+	mac->ports = mac->srp_ports | mac->mrp_ports;
+#ifdef DEBUG_MRP_BASIC
+	mrp_show_node(&mrp->mac_list, show_mac_info);
+#endif
+	node = mrp_get_mac_info(list, addr, fid);
+	if (!node)
+		return -ENOMEM;
+	update = node->data;
+	update->ports = ports;
+	update->index = mac->index;
+	return 0;
+}  /* srp_update_mac */
+
+#if 0
+static int srp_update_vlan(struct mrp_info *mrp, u16 vid, u8 *addr, u16 ports,
+	int up)
+{
+	struct mrp_node_anchor *list;
+	struct mrp_node *node;
+	struct mrp_vlan_info *vlan;
+	struct mrp_vlan_info *update;
+
+	node = mrp_get_vlan_info(&mrp->vlan_list, vid, addr);
+	if (!node)
+		return -ENOMEM;
+	vlan = node->data;
+	if (up) {
+		/* Forward in port. */
+		vlan->ports |= ports;
+		list = &mrp->vlan_up;
+	} else {
+		/* Filter in port. */
+		vlan->ports &= ~ports;
+		list = &mrp->vlan_down;
+	}
+#ifdef DEBUG_MRP_BASIC
+	mrp_show_node(&mrp->vlan_list, show_vlan_info);
+#endif
+	node = mrp_get_vlan_info(list, vid, addr);
+	if (!node)
+		return -ENOMEM;
+	update = node->data;
+	update->fid = vlan->fid;
+	update->ports = vlan->ports;
+	return 0;
+}  /* srp_update_vlan */
+#endif
+
+static int stream_iter(struct mrp_node_anchor *list, void *param[],
+		       int (*oper)(struct mrp_node *prev,
+				   struct srp_stream_info *data,
+				   void *param[]))
+{
+	int rc;
+	struct mrp_node *prev;
+	struct mrp_node *next;
+
+	prev = &list->anchor;
+	next = prev->next;
+	while (next) {
+		rc = oper(prev, next->data, param);
+
+		/* return ok or some other codes */
+		if (rc != -EAGAIN)
+			return rc;
+
+		prev = next;
+		next = prev->next;
+	}
+	return -EAGAIN;
+}
+
+static int stream_oper(struct mrp_node_anchor *list, void *param[],
+		       int (*oper)(struct mrp_node *prev,
+				   struct srp_stream_info *data,
+				   void *param[]))
+{
+	int rc;
+	struct mrp_node *prev;
+	struct mrp_node *next;
+
+	prev = &list->anchor;
+	next = prev->next;
+	while (next) {
+		rc = oper(prev, next->data, param);
+
+		/* no more operation */
+		if (!rc)
+			return 0;
+
+		/* stream not removed */
+		if (rc == -EAGAIN)
+			prev = next;
+		next = prev->next;
+	}
+	return -EAGAIN;
+}
+
+static void mrp_set_traffic(struct mrp_info *mrp, struct SRP_stream *stream,
+			    uint port, bool open)
+{
+	u16 fid;
+	u16 ports;
+	int result;
+
+	ports = 1 << port;
+	fid = mrp_get_fid(mrp, stream->vlan_id, stream->dest);
+	result = srp_update_mac(mrp, stream->dest, fid, ports, open);
+#if 0
+	if (!result)
+		result = srp_update_vlan(mrp, stream->vlan_id, stream->dest,
+					 ports, open);
+#endif
+}
+
+static int stream_drop(struct mrp_node *prev,
+		       struct srp_stream_info *data, void *param[])
+{
+	struct mrp_node *next;
+	struct SRP_reserv *reserv;
+	struct SRP_stream *stream;
+	struct mrp_info *mrp = param[0];
+	struct mrp_port_info *info = param[1];
+	struct mrp_traffic_info *traffic = param[2];
+	int *port = param[3];
+	struct SRP_reserv *t_reserv = param[4];
+	int *active = param[5];
+
+	reserv = data->reserv;
+	if (reserv == t_reserv) {
+		if (*active) {
+			u32 bandwidth;
+
+			stream = reserv->stream;
+			bandwidth = stream->bandwidth;
+
+			traffic->bandwidth_used -= bandwidth;
+			traffic->bandwidth_left = traffic->bandwidth_max -
+				traffic->bandwidth_used;
+			if (traffic->bandwidth_other)
+				*traffic->bandwidth_other =
+					*traffic->bandwidth_avail +
+					traffic->bandwidth_left;
+			info->bandwidth_used -= bandwidth;
+			info->bandwidth_left = info->bandwidth_max -
+				info->bandwidth_used;
+
+			mrp_set_traffic(mrp, stream, *port, false);
+		}
+
+		/* Remove node from list. */
+		next = prev->next;
+		prev->next = next->next;
+		mrp_free_node(next);
+
+		/* stop doing next in the list */
+		return 0;
+	}
+	return -EAGAIN;
+}
+
+static bool drop_reserv(struct mrp_info *mrp, u8 port,
+			struct mrp_port_info *info,
+			struct SRP_reserv *reserv, int active)
+{
+	struct mrp_node_anchor *list;
+	struct mrp_traffic_info *traffic;
+	int rc;
+	int tc;
+	void *param[6];
+	bool result = false;
+
+	tc = get_traffic_class(mrp, reserv->stream->priority);
+	traffic = get_traffic_info(info, tc);
+	if (active)
+		list = &traffic->active;
+	else
+		list = &traffic->passive;
+	param[0] = mrp;
+	param[1] = info;
+	param[2] = traffic;
+	param[3] = &port;
+	param[4] = reserv;
+	param[5] = &active;
+	rc = stream_oper(list, param, stream_drop);
+	if (!rc)
+		result = true;
+
+	return result;
+}  /* drop_reserv */
+
+static void srp_cfg_mac(struct mrp_info *mrp, struct mrp_node_anchor *list)
+{
+	struct mrp_node *prev;
+	struct mrp_node *next;
+	struct mrp_mac_info *mac;
+
+	prev = &list->anchor;
+	next = prev->next;
+	while (next) {
+		prev->next = next->next;
+
+		mac = next->data;
+		mrp_cfg_dest_addr(mrp, mac->index, mac->addr, mac->ports,
+			mac->fid);
+
+		mrp_free_node(next);
+
+		next = prev->next;
+	}
+	list->last = &list->anchor;
+}  /* srp_cfg_mac */
+
+#if 0
+static void srp_cfg_vlan(struct mrp_info *mrp, struct mrp_node_anchor *list)
+{
+	struct mrp_node *prev;
+	struct mrp_node *next;
+	struct mrp_vlan_info *vlan;
+	u16 ports;
+
+	prev = &list->anchor;
+	next = prev->next;
+	while (next) {
+		prev->next = next->next;
+
+		vlan = next->data;
+		ports = mrp_find_vlan_ports(&mrp->vlan_list, vlan->vid,
+			&vlan->index, &vlan->fid);
+		mrp_cfg_vlan(mrp, vlan->index, vlan->vid, vlan->fid, ports);
+
+		mrp_free_node(next);
+
+		next = prev->next;
+	}
+	list->last = &list->anchor;
+}  /* srp_cfg_vlan */
+#endif
+
+#if 0
+static int stream_chk_size(struct mrp_node *prev,
+			   struct srp_stream_info *data, void *param[])
+{
+	struct SRP_reserv *reserv;
+	struct SRP_stream *stream;
+	struct mrp_traffic_info *traffic = param[0];
+
+	reserv = data->reserv;
+	stream = reserv->stream;
+	if (stream->MaxFrameSize > traffic->max_frame_size)
+		traffic->max_frame_size = stream->MaxFrameSize;
+	return -EAGAIN;
+}
+#endif
+
+static void srp_cfg_reserv(struct mrp_info *mrp, struct mrp_port_info *port)
+{
+	uint n;
+	uint p;
+#if 0
+	u32 max_size;
+#endif
+	int tc;
+	struct mrp_port_info *info;
+	struct mrp_traffic_info *traffic;
+	struct ksz_sw *sw = mrp->parent;
+#if 0
+	void *param[6];
+#endif
+
+	srp_cfg_mac(mrp, &mrp->mac_down);
+#if 0
+	srp_cfg_vlan(mrp, &mrp->vlan_down);
+#endif
+	for (n = 0; n <= mrp->ports; n++) {
+		p = get_phy_port(sw, n);
+		info = mrp_get_port_info(mrp, p);
+		if (port && info != port)
+			continue;
+		if (!info->status.msrpPortEnabledStatus)
+			continue;
+		if (!info->link)
+			continue;
+		for (tc = SR_CLASS_A; tc >= SR_CLASS_B; tc--) {
+			traffic = get_traffic_info(info, tc);
+
+#if 0
+			/* Find out the maximum frame size. */
+			max_size = traffic->max_frame_size;
+			traffic->max_frame_size = 0;
+			param[0] = traffic;
+			stream_iter(&traffic->active, param, stream_chk_size);
+			if (max_size && !traffic->max_frame_size)
+				traffic->max_frame_size = max_size;
+			if (max_size != traffic->max_frame_size) {
+dbg_msg("  max size: %u %u"NL, max_size, traffic->max_frame_size);
+				/* Make sure credit settings will be updated. */
+				if (traffic->bandwidth_set)
+					traffic->bandwidth_set = 1;
+			}
+#endif
+			if (traffic->bandwidth_used !=
+			    traffic->bandwidth_set) {
+				srp_cfg_credit_shaper(mrp, p, info, traffic);
+				traffic->bandwidth_set =
+					traffic->bandwidth_used;
+			}
+		}
+	}
+	srp_cfg_mac(mrp, &mrp->mac_up);
+#if 0
+	srp_cfg_vlan(mrp, &mrp->vlan_up);
+#endif
+}  /* srp_cfg_reserv */
+
+static struct mrp_node *create_stream_info(struct mrp_info *mrp,
+					   struct SRP_reserv *t_reserv)
+{
+	struct mrp_node *node;
+	struct srp_stream_info *info;
+
+	node = mrp_alloc_node(sizeof(struct srp_stream_info));
+	if (!node)
+		return NULL;
+	info = node->data;
+	prepare_stream_info(t_reserv, info);
+	return node;
+}  /* create_stream_info */
+
+static void add_reserv(struct mrp_traffic_info *traffic, int active,
+		       struct mrp_node *node)
+{
+	struct mrp_node_anchor *list;
+	int (*cmp)(void *a, void *b);
+
+	if (active) {
+		list = &traffic->active;
+		cmp = cmp_lower_stream;
+	} else {
+		list = &traffic->passive;
+		cmp = cmp_higher_stream;
+	}
+	mrp_insert_node(list, cmp, node);
+}  /* add_reserv */
+
+static struct SRP_reserv *mrp_link_listener(struct mrp_info *mrp,
+					    struct mrp_port_info *info,
+					    struct SRP_stream *stream)
+{
+	struct SRP_reserv *reserv;
+
+	/* Create linked listener declaration. */
+	reserv = srp_find_reserv(&info->declared, stream->id,
+				 SRP_LISTENER);
+	if (!reserv) {
+
+		/* Listener declaration is not known yet. */
+		reserv = srp_create_reserv(stream->id, SRP_LISTENER,
+					   0, 0, mrp->id, 0);
+		if (!reserv)
+			return NULL;
+
+		reserv->stream = stream;
+		srp_insert_reserv(&info->declared, reserv);
+	}
+	reserv->stream = stream;
+	return reserv;
+}  /* mrp_link_listener */
+
+static int mrp_update_listener(struct mrp_info *mrp,
+			       struct SRP_reserv *l_reserv,
+			       struct SRP_stream *stream, u8 new_decl)
+{
+	int declaration;
+	int n;
+	struct SRP_reserv *reserv;
+	struct SRP_reserv *t_reserv;
+	struct mrp_port_info *info;
+	int result = DEV_IOC_OK;
+	int active = mrp->status.msrpEnabledStatus &&
+		     info->status.msrpPortEnabledStatus;
+
+	info = mrp_get_port_info(mrp, stream->in_port);
+	if (!l_reserv)
+		l_reserv = srp_find_reserv(&info->declared, stream->id,
+					   SRP_LISTENER);
+	if (!l_reserv) {
+dbg_msg("  ! L decl not found: %d"NL, stream->in_port);
+	}
+	if (!l_reserv)
+		return result;
+
+	declaration = 0;
+	for (n = 0; n <= mrp->ports; n++) {
+		if (n == stream->in_port)
+			continue;
+		info = mrp_get_port_info(mrp, n);
+		active = mrp->status.msrpEnabledStatus &&
+			info->status.msrpPortEnabledStatus;
+		if (!active)
+			continue;
+		reserv = srp_find_reserv(&info->registered, stream->id,
+					 SRP_LISTENER);
+		t_reserv = srp_find_reserv(&info->declared, stream->id,
+					 SRP_TALKER);
+		if (reserv && t_reserv) {
+			if (SRP_FAILED == t_reserv->declaration)
+				declaration |= SRP_ASKING_FAILED_SCALE;
+			else
+				declaration |= (1 << reserv->declaration);
+		} else if (reserv) {
+dbg_msg("  no t_reserv?"NL);
+				declaration |= SRP_ASKING_FAILED_SCALE;
+		}
+	}
+
+	/* info will need to be initialized. */
+
+	/* No more listeners. */
+	if (!declaration) {
+		int free = true;
+
+#ifdef DEBUG_MSRP
+dbg_msg("listener leaving"NL);
+#endif
+		info = mrp_get_port_info(mrp, stream->in_port);
+		active = mrp->status.msrpEnabledStatus &&
+			info->status.msrpPortEnabledStatus;
+		if (!mrp->no_report && active) {
+			free = false;
+			add_attrib_report(mrp, l_reserv, MRP_ACTION_LV,
+				MRP_TYPE_LISTENER, stream->in_port);
+			result = DEV_IOC_MRP_REPORT;
+		}
+		srp_remove_reserv(l_reserv, free);
+		return result;
+	}
+
+	if (declaration > SRP_READY_SCALE)
+		declaration = SRP_READY_FAILED;
+	else if (declaration == SRP_READY_SCALE)
+		declaration = SRP_READY;
+	else
+		declaration = SRP_ASKING_FAILED;
+
+#if 1
+	if (fqtss_34_2_1b_hack &&
+	    declaration == l_reserv->declaration && declaration == SRP_READY) {
+		l_reserv->declaration = SRP_READY_FAILED;
+		new_decl = true;
+	}
+#endif
+
+	/* Declaration is different. */
+	if (declaration != l_reserv->declaration) {
+		l_reserv->declaration = declaration;
+#ifdef DEBUG_MSRP
+dbg_msg("l dec: %d=%d"NL, stream->in_port, declaration);
+#endif
+		info = mrp_get_port_info(mrp, stream->in_port);
+		active = mrp->status.msrpEnabledStatus &&
+			info->status.msrpPortEnabledStatus;
+		if (!mrp->no_report && active) {
+			u8 action = MRP_ACTION_TX;
+
+			if (new_decl)
+				action = MRP_ACTION_TX_NEW;
+			add_attrib_report(mrp, l_reserv, action,
+				MRP_TYPE_LISTENER, stream->in_port);
+
+			/* Ask application to retrieve attributes. */
+			result = DEV_IOC_MRP_REPORT;
+		}
+	}
+	return result;
+}  /* mrp_update_listener */
+
+static int stream_clr_mark(struct mrp_node *prev,
+			struct srp_stream_info *data, void *param[])
+{
+	int *chk = param[0];
+
+	if (*chk && data->mark)
+dbg_msg("  mark !!"NL);
+	data->mark = false;
+	return -EAGAIN;
+}
+
+static int stream_set_mark(struct mrp_node *prev,
+			struct srp_stream_info *data, void *param[])
+{
+	data->mark = true;
+	return -EAGAIN;
+}
+
+static int stream_chk_bandwidth(struct mrp_node *prev,
+			struct srp_stream_info *data, void *param[])
+{
+	int cmp;
+	struct srp_stream_info *b = param[0];
+	u32 *avail = param[1];
+	u32 *required = param[2];
+	int *one_stream = param[3];
+	char bw_str1[20];
+	char bw_str2[20];
+	struct SRP_reserv *reserv;
+
+	cmp = cmp_lower_stream(b, data);
+#if 0
+dbg_msg(" cmp %d  %d:%d %llx:%llx %02x:%02x"NL, cmp,
+b->rank, data->rank, b->age, data->age,
+b->id[7], data->id[7]);
+#endif
+
+	/* stream has higher priority. */
+	if (cmp < 0)
+		return -ENXIO;
+	if (cmp == 0)
+		return 0;
+	if (!data->mark) {
+		reserv = data->reserv;
+		if (*one_stream) {
+			*avail = reserv->stream->bandwidth;
+		} else {
+			*avail += reserv->stream->bandwidth;
+			data->mark = true;
+		}
+		format_num(bw_str1, *avail);
+		format_num(bw_str2, *required);
+dbg_msg("  avail: %s %s"NL, bw_str1, bw_str2);
+		if (*avail >= *required) {
+
+			/* in case it is not marked above */
+			data->mark = true;
+			return 0;
+		}
+	}
+	return -EAGAIN;
+}
+
+static int stream_decr_bandwidth(struct mrp_node *prev,
+				 struct srp_stream_info *data, void *param[])
+{
+	u32 bandwidth;
+	struct SRP_reserv *reserv;
+	struct mrp_port_info *info = param[0];
+	struct mrp_traffic_info *traffic = param[1];
+	int *mark = param[2];
+
+	if (traffic->bandwidth_used > traffic->bandwidth_max) {
+		reserv = data->reserv;
+		bandwidth = reserv->stream->bandwidth;
+		reserv->code_bits = RFC_NO_BANDWIDTH_TC_BIT;
+		if (info->bandwidth_used > info->bandwidth_max)
+dbg_msg(" no: %u %u"NL, info->bandwidth_used, info->bandwidth_max);
+		if (info->bandwidth_used > info->bandwidth_max)
+			reserv->code_bits |= RFC_NO_BANDWIDTH_BIT;
+		traffic->bandwidth_used -= bandwidth;
+		info->bandwidth_used -= bandwidth;
+		data->mark = true;
+		*mark = true;
+	}
+	return -EAGAIN;
+}
+
+static int stream_cmp(struct mrp_node *prev, struct srp_stream_info *data,
+		      void *param[])
+{
+	struct SRP_reserv *reserv = param[0];
+
+	if (data->reserv == reserv) {
+		data->mark = true;
+		return 0;
+	}
+	return -EAGAIN;
+}
+
+static int stream_stop(struct mrp_node *prev, struct srp_stream_info *data,
+		       void *param[])
+{
+	struct mrp_node *next;
+	struct SRP_reserv *reserv;
+	struct SRP_stream *stream;
+	struct mrp_info *mrp = param[0];
+	struct mrp_traffic_info *traffic = param[1];
+	int *port = param[2];
+	int result;
+
+	if (data->mark) {
+		u8 decl;
+
+		data->mark = false;
+
+		reserv = data->reserv;
+		decl = reserv->declaration;
+		reserv->code = msrp_failure_code(reserv->code_bits);
+		if (reserv->code)
+			reserv->declaration = SRP_FAILED;
+
+		stream = reserv->stream;
+		mrp_set_traffic(mrp, stream, *port, false);
+
+		if (!mrp->no_report && decl != reserv->declaration) {
+dbg_msg("  no band stop: %x"NL, reserv->code_bits);
+			add_attrib_report(mrp, reserv, MRP_ACTION_TX_NEW,
+				MRP_TYPE_TALKER, *port);
+		}
+
+		result = mrp_update_listener(mrp, NULL, stream, false);
+
+		/* Remove node from list. */
+		next = prev->next;
+		prev->next = next->next;
+		next->next = NULL;
+
+		add_reserv(traffic, false, next);
+
+		return -ENODEV;
+	}
+	return -EAGAIN;
+}
+
+static int stream_reset(struct mrp_node *prev,
+			struct srp_stream_info *data, void *param[])
+{
+	struct SRP_reserv *reserv;
+
+	reserv = data->reserv;
+	if (reserv->code_bits &
+	    ~(RFC_NO_BANDWIDTH_BIT | RFC_NO_BANDWIDTH_TC_BIT |
+	     RFC_PREEMPTED_BIT))
+		return -EAGAIN;
+	if (!reserv->pair)
+printk(" no pair!"NL);
+	if (!reserv->pair)
+		return -EAGAIN;
+	reserv->pair->code = RFC_NO_ERROR;
+	return -EAGAIN;
+}  /* stream_reset */
+
+static int stream_start(struct mrp_node *prev,
+			struct srp_stream_info *data, void *param[])
+{
+	struct mrp_node *next;
+	struct SRP_reserv *reserv;
+	struct SRP_stream *stream;
+	struct mrp_info *mrp = param[0];
+	struct mrp_port_info *info = param[1];
+	struct mrp_traffic_info *traffic = param[2];
+	int *port = param[3];
+	int result;
+	u32 bandwidth;
+	bool adv = false;
+
+	reserv = data->reserv;
+	if (reserv->code_bits &
+	    ~(RFC_NO_BANDWIDTH_BIT | RFC_NO_BANDWIDTH_TC_BIT |
+	     RFC_PREEMPTED_BIT) || reserv->pair->code != RFC_NO_ERROR)
+{
+#if 0
+dbg_msg(" other error: %x %d"NL, reserv->code_bits, reserv->pair->code);
+#endif
+		return -EAGAIN;
+}
+	bandwidth = reserv->stream->bandwidth;
+	traffic->bandwidth_left = traffic->bandwidth_max -
+		traffic->bandwidth_used;
+	if (bandwidth <= traffic->bandwidth_left) {
+		/* Remove node from list. */
+		next = prev->next;
+		prev->next = next->next;
+		next->next = NULL;
+
+		add_reserv(traffic, true, next);
+
+		traffic->bandwidth_used += bandwidth;
+		info->bandwidth_used += bandwidth;
+
+		if (data->mark) {
+			data->mark = false;
+			return -ENODEV;
+		}
+
+		if (reserv->declaration != SRP_ADVERTISE) {
+			reserv->declaration = SRP_ADVERTISE;
+			reserv->code_bits = 0;
+			reserv->code = RFC_NO_ERROR;
+			adv = true;
+		}
+
+		stream = reserv->stream;
+		mrp_set_traffic(mrp, stream, *port, true);
+
+		if (!mrp->no_report && adv) {
+			add_attrib_report(mrp, reserv, MRP_ACTION_TX_NEW,
+				MRP_TYPE_TALKER, *port);
+		}
+
+		result = mrp_update_listener(mrp, NULL, stream, false);
+
+		return -ENODEV;
+	} else if (data->mark) {
+dbg_msg(" no bandwidth for this"NL);
+		data->mark = false;
+
+		reserv->declaration = SRP_FAILED;
+		reserv->code_bits = RFC_NO_BANDWIDTH_TC_BIT;
+		info->bandwidth_left = info->bandwidth_max -
+			info->bandwidth_used;
+		if (bandwidth > info->bandwidth_left)
+			reserv->code_bits |= RFC_NO_BANDWIDTH_BIT;
+		reserv->code = msrp_failure_code(reserv->code_bits);
+
+		stream = reserv->stream;
+		mrp_set_traffic(mrp, stream, *port, false);
+
+		if (!mrp->no_report) {
+			add_attrib_report(mrp, reserv, MRP_ACTION_TX_NEW,
+				MRP_TYPE_TALKER, *port);
+		}
+
+		result = mrp_update_listener(mrp, NULL, stream, false);
+
+	} else if (bandwidth <= info->bandwidth_left &&
+		   (reserv->code_bits & RFC_NO_BANDWIDTH_BIT)) {
+		reserv->code_bits &= ~RFC_NO_BANDWIDTH_BIT;
+		reserv->code = msrp_failure_code(reserv->code_bits);
+		if (!mrp->no_report) {
+dbg_msg("  no tc"NL);
+			add_attrib_report(mrp, reserv,
+					  MRP_ACTION_TX,
+					  MRP_TYPE_TALKER, *port);
+		}
+	} else if (bandwidth > info->bandwidth_left &&
+		   !(reserv->code_bits & RFC_NO_BANDWIDTH_BIT)) {
+		reserv->code_bits |= RFC_NO_BANDWIDTH_BIT;
+		reserv->code = msrp_failure_code(reserv->code_bits);
+		if (!mrp->no_report) {
+dbg_msg("  no bw"NL);
+			add_attrib_report(mrp, reserv,
+					  MRP_ACTION_TX,
+					  MRP_TYPE_TALKER, *port);
+		}
+	} else if (reserv->pair && reserv->pair->code == RFC_NO_ERROR) {
+		reserv->pair->code = RFC_NO_BANDWIDTH;
+	}
+	return -EAGAIN;
+}
+
+static int stream_drop_other(struct mrp_node *prev,
+			     struct srp_stream_info *data, void *param[])
+{
+	struct mrp_node *next;
+	struct SRP_reserv *reserv;
+	struct SRP_stream *stream;
+	struct mrp_info *mrp = param[0];
+	struct mrp_traffic_info *traffic = param[1];
+	int *port = param[2];
+	int *avail = param[3];
+	int result;
+
+dbg_msg("%s %p %p"NL, __func__, param, mrp);
+	if (data->mark) {
+		u32 bandwidth;
+
+		data->mark = false;
+
+		reserv = data->reserv;
+		stream = reserv->stream;
+
+		bandwidth = stream->bandwidth;
+		*avail += bandwidth;
+
+		traffic->bandwidth_left += bandwidth;
+		traffic->bandwidth_used -= bandwidth;
+		if (traffic->bandwidth_other)
+			*traffic->bandwidth_other =
+				*traffic->bandwidth_avail +
+				traffic->bandwidth_left;
+
+		reserv->declaration = SRP_FAILED;
+		reserv->code_bits = RFC_PREEMPTED_BIT;
+		reserv->code = RFC_PREEMPTED_BY_RANK;
+
+		mrp_set_traffic(mrp, stream, *port, false);
+
+		if (!mrp->no_report)
+			add_attrib_report(mrp, reserv, MRP_ACTION_TX_NEW,
+				MRP_TYPE_TALKER, *port);
+
+		result = mrp_update_listener(mrp, NULL, stream, false);
+
+		/* Remove node from list. */
+		next = prev->next;
+		prev->next = next->next;
+		next->next = NULL;
+
+		add_reserv(traffic, false, next);
+
+		return -ENODEV;
+	}
+	return -EAGAIN;
+}
+
+static bool have_bandwidth(struct mrp_port_info *info,
+			   struct mrp_traffic_info *traffic,
+			   struct SRP_reserv *t_reserv,
+			   u32 required_bandwidth)
+{
+	u32 avail_bandwidth = 0;
+	int cmp;
+	int one;
+	void *param[6];
+	struct srp_stream_info b;
+	int rc;
+
+	prepare_stream_info(t_reserv, &b);
+
+	/* Make sure stream age is latest for correct comparison. */
+	if (!b.age)
+		b.age = get_stream_age(info);
+
+	cmp = 1;
+	param[0] = &cmp;
+	rc = stream_iter(&traffic->active, param, stream_clr_mark);
+
+	param[0] = &b;
+	param[1] = &avail_bandwidth;
+	param[2] = &required_bandwidth;
+	one = 1;
+	param[3] = &one;
+	rc = stream_iter(&traffic->active, param, stream_chk_bandwidth);
+	if (!rc)
+		return true;
+
+	param[0] = &b;
+	param[1] = &avail_bandwidth;
+	param[2] = &required_bandwidth;
+	one = 0;
+	param[3] = &one;
+	rc = stream_iter(&traffic->active, param, stream_chk_bandwidth);
+	if (!rc)
+		return true;
+
+dbg_msg(" no bandwidth %s %u %u"NL, __func__, required_bandwidth, info->bandwidth_left);
+
+	/* No bandwidth at all for the new stream. */
+	return false;
+}  /* have_bandwidth */
+
+static bool drop_other_reserv(struct mrp_info *mrp, u8 port,
+			      struct mrp_port_info *info,
+			      struct mrp_traffic_info *traffic,
+			      struct SRP_reserv *t_reserv,
+			      u32 required_bandwidth, bool drop)
+{
+	u32 avail_bandwidth = 0;
+	void *param[6];
+	int rc;
+	bool ok;
+
+	ok = have_bandwidth(info, traffic, t_reserv, required_bandwidth);
+	if (!ok) {
+		return false;
+	} else if (!drop) {
+		int cmp = 0;
+
+		/* reset marked streams */
+		param[0] = &cmp;
+		rc = stream_iter(&traffic->active, param, stream_clr_mark);
+		return true;
+	}
+
+	param[0] = mrp;
+	param[1] = traffic;
+	param[2] = &port;
+	param[3] = &avail_bandwidth;
+dbg_msg(" stream_drop_other %p %p"NL, param, mrp);
+	rc = stream_oper(&traffic->active, param, stream_drop_other);
+
+	/* Increase bandwidth */
+	info->bandwidth_left += avail_bandwidth;
+	info->bandwidth_used -= avail_bandwidth;
+	return true;
+}  /* drop_other_reserv */
+
+static bool drop_active_reserv(struct mrp_info *mrp, u8 port,
+			struct mrp_port_info *info,
+			struct mrp_traffic_info *traffic,
+			struct SRP_reserv *reserv)
+{
+	int rc;
+	void *param[6];
+	bool drop = false;
+
+dbg_msg("%s"NL, __func__);
+	param[0] = reserv;
+	rc = stream_iter(&traffic->active, param, stream_cmp);
+
+	/* A stream was marked for stopping.*/
+	if (!rc) {
+		u32 bandwidth = reserv->stream->bandwidth;
+
+		traffic->bandwidth_used -= bandwidth;
+		traffic->bandwidth_left += bandwidth;
+		if (traffic->bandwidth_other)
+			*traffic->bandwidth_other = *traffic->bandwidth_avail +
+				traffic->bandwidth_left;
+		info->bandwidth_used -= bandwidth;
+		info->bandwidth_left += bandwidth;
+		param[0] = mrp;
+		param[1] = traffic;
+		param[2] = &port;
+		rc = stream_oper(&traffic->active, param, stream_stop);
+		drop = true;
+	}
+
+	return drop;
+}
+
+static void stop_active_reserv(struct mrp_info *mrp, u8 port,
+			struct mrp_port_info *info,
+			struct mrp_traffic_info *traffic)
+{
+	int rc;
+	void *param[6];
+	bool mark = false;
+
+	param[0] = info;
+	if (traffic->bandwidth_used > traffic->bandwidth_max) {
+		param[1] = traffic;
+		param[2] = &mark;
+		rc = stream_iter(&traffic->active, param,
+			stream_decr_bandwidth);
+		traffic->bandwidth_left = traffic->bandwidth_max -
+			traffic->bandwidth_used;
+
+		/* Increase Class B available bandwidth. */
+		if (traffic->bandwidth_other)
+			*traffic->bandwidth_other =
+				*traffic->bandwidth_avail +
+				traffic->bandwidth_left;
+		info->bandwidth_left = info->bandwidth_max -
+			info->bandwidth_used;
+	}
+
+	/* A stream was marked for stopping.*/
+	if (mark) {
+		param[0] = mrp;
+		param[1] = traffic;
+		param[2] = &port;
+		rc = stream_oper(&traffic->active, param, stream_stop);
+	}
+}  /* stop_active_reserv */
+
+static void start_passive_reserv(struct mrp_info *mrp, u8 port,
+				 struct mrp_port_info *info)
+{
+	int rc;
+	int tc;
+	struct mrp_traffic_info *traffic;
+	void *param[6];
+
+	param[0] = mrp;
+	param[1] = info;
+	param[3] = &port;
+	for (tc = SR_CLASS_A; tc >= SR_CLASS_B; tc--) {
+		traffic = get_traffic_info(info, tc);
+		param[2] = traffic;
+		rc = stream_oper(&traffic->passive, param, stream_start);
+		traffic->bandwidth_left = traffic->bandwidth_max -
+			traffic->bandwidth_used;
+
+		/* Decrease Class B available bandwidth. */
+		if (traffic->bandwidth_other)
+			*traffic->bandwidth_other =
+				*traffic->bandwidth_avail +
+				traffic->bandwidth_left;
+	}
+	info->bandwidth_left = info->bandwidth_max - info->bandwidth_used;
+}  /* start_passive_reserv */
+
+static void mrp_combine_list(struct mrp_node_anchor *old_list,
+	struct mrp_node_anchor *new_list)
+{
+	struct mrp_node *prev;
+	struct mrp_node *next;
+
+	prev = &old_list->anchor;
+	next = prev->next;
+	while (next) {
+
+		/* Remove node from list. */
+		prev->next = next->next;
+		next->next = NULL;
+		mrp_insert_node(new_list, cmp_higher_stream, next);
+		next = prev->next;
+	}
+}  /* mrp_combine_list */
+
+static void chk_passive_reserv(struct mrp_info *mrp, u8 port,
+			struct mrp_port_info *info)
+{
+	int rc;
+	int tc;
+	struct mrp_traffic_info *traffic;
+	void *param[6];
+
+	for (tc = SR_CLASS_A; tc >= SR_CLASS_B; tc--) {
+		traffic = get_traffic_info(info, tc);
+		rc = stream_iter(&traffic->active, param, stream_set_mark);
+		rc = stream_iter(&traffic->passive, param, stream_reset);
+		traffic->bandwidth_used = 0;
+		mrp_combine_list(&traffic->active, &traffic->passive);
+#if 0
+		mrp_show_node(&traffic->passive, show_stream_info);
+#endif
+	}
+	info->bandwidth_used = 0;
+	start_passive_reserv(mrp, port, info);
+}
+
+static u32 chk_avail_tx_bandwidth(struct mrp_info *mrp, u8 port,
+				  struct mrp_port_info *info, int tc,
+				  struct SRP_reserv *t_reserv, bool drop)
+{
+	u32 bandwidth;
+	struct mrp_traffic_info *traffic;
+	char bw[20];
+	char bw_max[20];
+	char bw_left[20];
+	char tbw_max[20];
+	char tbw_left[20];
+	u32 code_bits = 0;
+	struct SRP_stream *stream = t_reserv->stream;
+
+	traffic = get_traffic_info(info, tc);
+	traffic->bandwidth_left = traffic->bandwidth_max -
+		traffic->bandwidth_used;
+	bandwidth = stream->bandwidth;
+	format_num(bw, (u32)bandwidth);
+	format_num(bw_max, info->bandwidth_max);
+	format_num(bw_left, info->bandwidth_left);
+	format_num(tbw_max, traffic->bandwidth_max);
+	format_num(tbw_left, traffic->bandwidth_left);
+#if 0
+dbg_msg("bw:%u  %s  %s  %s  %s"NL, port, bw, bw_left, tbw_left, tbw_max);
+#endif
+
+	if (bandwidth > traffic->bandwidth_left)
+		code_bits |= RFC_NO_BANDWIDTH_TC_BIT;
+	if (bandwidth > info->bandwidth_left)
+		code_bits |= RFC_NO_BANDWIDTH_BIT;
+
+	/* See if other streams can be dropped to accommodate this one. */
+	if (code_bits && bandwidth <= traffic->bandwidth_max) {
+		bandwidth -= traffic->bandwidth_left;
+		if (drop_other_reserv(mrp, port, info, traffic, t_reserv,
+				      bandwidth, drop))
+			code_bits = 0;
+		else if (tc == SR_CLASS_A &&
+			 !(code_bits & RFC_NO_BANDWIDTH_TC_BIT)) {
+
+			/* Class A will have bandwidth from Class B. */
+			code_bits = 0;
+			if (drop) {
+				u32 left;
+				u32 used;
+				struct mrp_traffic_info *traffic_b;
+
+				traffic_b = get_traffic_info(info, SR_CLASS_B);
+				used = traffic->bandwidth_used + bandwidth;
+				left = traffic->bandwidth_max - used;
+				traffic_b->bandwidth_max =
+					traffic_b->bandwidth_delta + left;
+				stop_active_reserv(mrp, port, info, traffic_b);
+			}
+		}
+	}
+
+	return code_bits;
+}  /* chk_avail_tx_bandwidth */
+
+static int chk_avail_bandwidth(struct mrp_info *mrp, u8 port,
+			struct mrp_port_info *info,
+			       struct SRP_reserv *t_reserv)
+{
+	u32 bandwidth;
+	int tc;
+	struct mrp_traffic_info *traffic;
+	int result;
+	struct SRP_stream *stream = t_reserv->stream;
+
+	tc = get_traffic_class(mrp, stream->priority);
+	traffic = get_traffic_info(info, tc);
+	bandwidth = stream->bandwidth;
+	t_reserv->code_bits = chk_avail_tx_bandwidth(mrp, port, info, tc,
+						     t_reserv, true);
+	t_reserv->code = msrp_failure_code(t_reserv->code_bits);
+
+	if (RFC_NO_ERROR == t_reserv->code) {
+		struct mrp_node *active;
+		u16 fid;
+		u16 ports = 1 << port;
+
+		/* Reduce bandwidth */
+		info->bandwidth_left -= bandwidth;
+		info->bandwidth_used += bandwidth;
+		traffic->bandwidth_left -= bandwidth;
+		traffic->bandwidth_used += bandwidth;
+		if (traffic->bandwidth_other) {
+			*traffic->bandwidth_other -= bandwidth;
+		}
+#if 0
+		if (stream->MaxFrameSize > traffic->max_frame_size)
+			traffic->max_frame_size = stream->MaxFrameSize;
+#endif
+
+		active = create_stream_info(mrp, t_reserv);
+		if (!active)
+			return SRP_ASKING_FAILED;
+
+		add_reserv(traffic, true, active);
+
+		fid = mrp_get_fid(mrp, stream->vlan_id, stream->dest);
+		result = srp_update_mac(mrp, stream->dest, fid, ports, true);
+		if (result)
+			return SRP_ASKING_FAILED;
+#if 0
+		result = srp_update_vlan(mrp,
+			stream->vlan_id, stream->dest, ports, true);
+#endif
+		return SRP_READY;
+	} else {
+		struct mrp_node *passive;
+
+		t_reserv->declaration = SRP_FAILED;
+dbg_msg("no bw: %d"NL, t_reserv->code);
+		if (!mrp->no_report)
+			add_attrib_report(mrp, t_reserv, MRP_ACTION_TX_NEW,
+				MRP_TYPE_TALKER, port);
+		passive = create_stream_info(mrp, t_reserv);
+		if (passive)
+			add_reserv(traffic, false, passive);
+	}
+	return SRP_ASKING_FAILED;
+}  /* chk_avail_bandwidth */
+
+static void chk_talker_decl(struct mrp_info *mrp, u8 port,
+			struct mrp_port_info *info)
+{
+	struct SRP_reserv *reserv;
+
+	reserv = info->declared.next;
+	while (reserv) {
+		/* talker not paired with a listener */
+		if (reserv->direction == SRP_TALKER && !reserv->pair &&
+		    ((reserv->code && (reserv->code_bits &
+		    (RFC_NO_BANDWIDTH_BIT | RFC_NO_BANDWIDTH_TC_BIT))) ||
+		    !reserv->code)) {
+			int tc;
+			u8 code = reserv->code;
+
+			tc = get_traffic_class(mrp, reserv->stream->priority);
+			reserv->code_bits =
+				chk_avail_tx_bandwidth(mrp, port, info, tc,
+						       reserv, false);
+			reserv->code = msrp_failure_code(reserv->code_bits);
+			if (code != reserv->code) {
+				if (reserv->code)
+					reserv->declaration = SRP_FAILED;
+				else
+					reserv->declaration = SRP_ADVERTISE;
+				add_attrib_report(mrp, reserv,
+					MRP_ACTION_TX_NEW, MRP_TYPE_TALKER,
+					port);
+			}
+		}
+		reserv = reserv->next;
+	}
+}  /* chk_talker_decl */
+
+static int update_reserv(struct mrp_info *mrp, u8 port,
+	struct mrp_port_info *info)
+{
+	struct mrp_traffic_info *traffic;
+	int result = DEV_IOC_OK;
+	int tc;
+#ifdef DEBUG
+	char bw_used[20];
+	char bw_remain[20];
+#endif
+
+	if (!mrp->status.msrpEnabledStatus)
+		return result;
+	for (tc = SR_CLASS_A; tc >= SR_CLASS_B; tc--) {
+		traffic = get_traffic_info(info, tc);
+		stop_active_reserv(mrp, port, info, traffic);
+#ifdef DEBUG
+		format_num(bw_used, traffic->bandwidth_used);
+		format_num(bw_remain, traffic->bandwidth_max);
+dbg_msg("used %d: %s %s"NL, tc, bw_used, bw_remain);
+#endif
+	}
+	chk_passive_reserv(mrp, port, info);
+
+#ifdef DEBUG
+	for (tc = SR_CLASS_A; tc >= SR_CLASS_B; tc--) {
+		traffic = get_traffic_info(info, tc);
+dbg_msg("  active:"NL);
+		mrp_show_node(&traffic->active, show_stream_info);
+dbg_msg("  passive:"NL);
+		mrp_show_node(&traffic->passive, show_stream_info);
+		format_num(bw_used, traffic->bandwidth_used);
+dbg_msg("used %d: %s"NL, tc, bw_used);
+	}
+#endif
+
+	srp_cfg_reserv(mrp, info);
+
+	chk_talker_decl(mrp, port, info);
+
+	if (mrp->report_head)
+		result = DEV_IOC_MRP_REPORT;
+	return result;
+}  /* update_reserv */
+
+static int proc_mrp_tx_listener(struct mrp_info *mrp, u8 port,
+				struct mrp_port_info *info,
+				struct SRP_reserv *l_reserv,
+				struct SRP_reserv *t_reserv,
+				struct SRP_stream *stream)
+{
+	struct mrp_traffic_info *traffic;
+	int tc;
+	int result = DEV_IOC_OK;
+	u8 declaration = l_reserv->declaration;
+
+	/* Talker is not ready. */
+	if (SRP_FAILED == stream->t_reserv->declaration) {
+dbg_msg("talker not ready"NL);
+		declaration = SRP_ASKING_FAILED;
+	} else {
+
+		/* Find talker reservation if already created. */
+		if (!t_reserv)
+			t_reserv = srp_find_reserv(&info->declared,
+						   l_reserv->id, SRP_TALKER);
+		if (!t_reserv) {
+/* talker reservation may not be created yet because of link down. */
+dbg_msg("  no t_reserv"NL);
+			declaration = SRP_ASKING_FAILED;
+		} else {
+			if (SRP_FAILED == t_reserv->declaration)
+				declaration = SRP_ASKING_FAILED;
+#ifdef DEBUG_MSRP_
+dbg_msg(" asked: %d"NL, declaration);
+#endif
+			if (!t_reserv->streamAge) {
+				t_reserv->streamAge = get_stream_age(info);
+				t_reserv->pair = l_reserv;
+				l_reserv->pair = t_reserv;
+			}
+		}
+	}
+#ifdef DEBUG_MSRP
+	if (t_reserv)
+dbg_msg("  T:%d L:%d D:%d"NL, t_reserv->code, l_reserv->code, declaration);
+	else
+dbg_msg("  L:%d D:%d"NL, l_reserv->code, declaration);
+#endif
+
+	tc = get_traffic_class(mrp, stream->priority);
+	traffic = get_traffic_info(info, tc);
+
+	/* Check available bandwidth. */
+	if (declaration != SRP_ASKING_FAILED) {
+#ifdef DEBUG_MSRP_
+dbg_msg("not ask failed: %d"NL, l_reserv->code);
+#endif
+
+		/* Actual reservation is not made yet. */
+		if (RFC_NO_RESOURCES == l_reserv->code) {
+#ifdef DEBUG_MSRP_
+dbg_msg("not yet"NL);
+#endif
+			declaration = chk_avail_bandwidth(mrp, port, info,
+				t_reserv);
+			l_reserv->code = RFC_NO_BANDWIDTH;
+			if (declaration != SRP_ASKING_FAILED) {
+				l_reserv->code = RFC_NO_ERROR;
+				srp_cfg_reserv(mrp, info);
+				chk_talker_decl(mrp, port, info);
+			}
+		} else {
+			l_reserv->code = RFC_NO_ERROR;
+			start_passive_reserv(mrp, port, info);
+			srp_cfg_reserv(mrp, info);
+			chk_talker_decl(mrp, port, info);
+		}
+	} else if (RFC_NO_RESOURCES != l_reserv->code) {
+
+dbg_msg(" no lis: %d"NL, l_reserv->code);
+		/* Listener is no longer ready to receive stream. */
+		if (l_reserv->code == RFC_NO_ERROR &&
+		    drop_active_reserv(mrp, port, info, traffic, t_reserv)) {
+
+			/* Used to indicate reservation is not done yet. */
+			l_reserv->code = RFC_NO_BANDWIDTH;
+			start_passive_reserv(mrp, port, info);
+			srp_cfg_reserv(mrp, info);
+			chk_talker_decl(mrp, port, info);
+		}
+	} else if (t_reserv) {
+		struct mrp_node *passive;
+
+		l_reserv->code = RFC_NO_BANDWIDTH;
+#ifdef DEBUG_MSRP
+dbg_msg("add pas"NL);
+#endif
+		passive = create_stream_info(mrp, t_reserv);
+		if (passive)
+			add_reserv(traffic, false, passive);
+	}
+
+	/* Used to find out whether to send declaration back to talker. */
+	mrp->listeners++;
+
+	if (mrp->report_head)
+		result = DEV_IOC_MRP_REPORT;
+	return result;
+}  /* proc_mrp_tx_listener */
+
+static int mrp_delete_listener(struct mrp_info *mrp, u8 port,
+	struct mrp_port_info *info, struct SRP_reserv *l_reserv)
+{
+	struct SRP_reserv *t_reserv;
+	bool drop = false;
+	int result = DEV_IOC_OK;
+	int active = mrp->status.msrpEnabledStatus &&
+		     info->status.msrpPortEnabledStatus;
+
+	/* Check if a talker reservation is using bandwidth. */
+	t_reserv = srp_find_reserv(&info->declared, l_reserv->id, SRP_TALKER);
+	if (!t_reserv) {
+		struct SRP_stream *stream = l_reserv->stream;
+
+		/* Delete listener reservation when leaving. */
+		srp_remove_reserv(l_reserv, true);
+		if (stream)
+			result = mrp_update_listener(mrp, NULL,
+				stream, false);
+		return result;
+	}
+
+	if (t_reserv->declaration == SRP_ADVERTISE &&
+	    l_reserv->code == RFC_NO_ERROR) {
+		drop = drop_reserv(mrp, port, info, t_reserv, true);
+	}
+	if (drop) {
+#ifdef DEBUG_MSRP
+dbg_msg(" drop active"NL);
+#endif
+#if 1
+		if (active)
+		start_passive_reserv(mrp, port, info);
+#endif
+		srp_cfg_reserv(mrp, info);
+#if 1
+		if (active)
+		chk_talker_decl(mrp, port, info);
+#endif
+	} else {
+#ifdef DEBUG_MSRP
+dbg_msg(" drop passive"NL);
+#endif
+		drop_reserv(mrp, port, info, t_reserv, false);
+	}
+
+#if 0
+	/* Remember the listener state. */
+	declaration = l_reserv->declaration;
+#endif
+
+	t_reserv->pair = NULL;
+
+	/* Delete listener reservation when leaving. */
+	srp_remove_reserv(l_reserv, true);
+
+	/* Update the listener attribute reported back to talker. */
+	result = mrp_update_listener(mrp, NULL, t_reserv->stream, false);
+
+	/* Listener reservation is not using bandwidth if not ready. */
+
+	/* no reservation */
+	t_reserv->streamAge = 0;
+
+	if (mrp->report_head)
+		result = DEV_IOC_MRP_REPORT;
+	return result;
+}  /* mrp_delete_listener */
+
+static int proc_mrp_lv_listener(struct mrp_info *mrp, u8 port,
+	struct SRP_listener *listener)
+{
+	struct SRP_reserv *l_reserv;
+	struct SRP_reserv *t_reserv;
+	struct mrp_port_info *info;
+	int declaration;
+	int result = DEV_IOC_OK;
+	bool drop = false;
+
+	info = mrp_get_port_info(mrp, port);
+#ifdef DEBUG_MSRP
+dbg_msg(" %s p:%d %02x:%02x:%02x:%02x:%02x:%02x:%02x:%02x s:%u"NL, __func__,
+port,
+listener->id[0],
+listener->id[1],
+listener->id[2],
+listener->id[3],
+listener->id[4],
+listener->id[5],
+listener->id[6],
+listener->id[7],
+listener->substate);
+#endif
+	l_reserv = srp_find_reserv(&info->registered, listener->id,
+				   SRP_LISTENER);
+	if (!l_reserv)
+{
+dbg_msg("  ! not found: %p %d"NL, info, port);
+chk_reserv(info, port);
+}
+	if (!l_reserv)
+		return result;
+	l_reserv->ticks = jiffies - l_reserv->ticks;
+	if (l_reserv->ticks < 100)
+dbg_msg(" ?! ");
+dbg_msg(" t:%u"NL, l_reserv->ticks * 10 / 1000);
+
+	/* Check if a talker reservation is using bandwidth. */
+	t_reserv = srp_find_reserv(&info->declared, listener->id, SRP_TALKER);
+	if (!t_reserv) {
+		struct SRP_stream *stream = l_reserv->stream;
+
+		/* Delete listener reservation when leaving. */
+		srp_remove_reserv(l_reserv, true);
+		if (stream)
+			result = mrp_update_listener(mrp, NULL,
+				stream, false);
+		return result;
+	}
+
+	if (t_reserv->declaration == SRP_ADVERTISE &&
+	    l_reserv->code == RFC_NO_ERROR) {
+		drop = drop_reserv(mrp, port, info, t_reserv, true);
+	}
+	if (drop) {
+#ifdef DEBUG_MSRP
+dbg_msg(" drop active"NL);
+#endif
+		start_passive_reserv(mrp, port, info);
+		srp_cfg_reserv(mrp, info);
+		chk_talker_decl(mrp, port, info);
+	} else {
+#ifdef DEBUG_MSRP
+dbg_msg(" drop passive"NL);
+#endif
+		drop_reserv(mrp, port, info, t_reserv, false);
+	}
+
+	/* Remember the listener state. */
+	declaration = l_reserv->declaration;
+
+	t_reserv->pair = NULL;
+
+	/* Delete listener reservation when leaving. */
+	srp_remove_reserv(l_reserv, true);
+
+	/* Update the listener attribute reported back to talker. */
+	result = mrp_update_listener(mrp, NULL, t_reserv->stream, false);
+
+	/* Listener reservation is not using bandwidth if not ready. */
+
+	/* no reservation */
+	t_reserv->streamAge = 0;
+
+	if (mrp->report_head)
+		result = DEV_IOC_MRP_REPORT;
+	return result;
+}  /* proc_mrp_lv_listener */
+
+static int proc_mrp_rx_listener(struct mrp_info *mrp, u8 port,
+	struct SRP_listener *listener, u8 new_decl)
+{
+	int result = DEV_IOC_OK;
+	u8 declaration = SRP_READY;
+	struct SRP_reserv *l_reserv;
+	struct SRP_stream *stream;
+	struct mrp_port_info *info;
+
+	info = mrp_get_port_info(mrp, port);
+#ifdef DEBUG_MSRP
+dbg_msg(" %s p:%d %d %02x:%02x:%02x:%02x:%02x:%02x:%02x:%02x s:%u"NL, __func__,
+port, new_decl,
+listener->id[0],
+listener->id[1],
+listener->id[2],
+listener->id[3],
+listener->id[4],
+listener->id[5],
+listener->id[6],
+listener->id[7],
+listener->substate);
+#endif
+	if (SRP_IGNORED == listener->substate)
+		return DEV_IOC_INVALID_CMD;
+
+	declaration = listener->substate;
+	stream = srp_find_stream_id(mrp, listener->id);
+
+	/* Create reservation if new. */
+	l_reserv = srp_find_reserv(&info->registered, listener->id,
+				   SRP_LISTENER);
+	if (!l_reserv) {
+		l_reserv = srp_create_reserv(listener->id, SRP_LISTENER,
+					     declaration, 0, mrp->id,
+					     RFC_NO_RESOURCES);
+		if (!l_reserv)
+			return -ENOMEM;
+
+		srp_insert_reserv(&info->registered, l_reserv);
+	} else {
+		l_reserv->declaration = declaration;
+	}
+
+	/* Stream may not exist yet when listener is received. */
+	l_reserv->stream = stream;
+
+	/* There is a talker. */
+	if (stream && stream->t_reserv && stream->in_port != port) {
+		int rc;
+		struct SRP_reserv *reserv;
+
+		mrp->listeners = 0;
+		result = proc_mrp_tx_listener(mrp, port, info, l_reserv, NULL,
+			stream);
+
+		/* Likely due to errors. */
+		if (!mrp->listeners)
+			return result;
+
+		info = mrp_get_port_info(mrp, stream->in_port);
+		reserv = mrp_link_listener(mrp, info, stream);
+		if (!reserv)
+			return -ENOMEM;
+
+		if (!new_decl) {
+			struct ksz_sw *sw = mrp->parent;
+
+			if (sw->ops->get_tcDetected(sw, port))
+				new_decl = true;
+		}
+
+		/* Listener declaration will be updated in following call. */
+		rc = mrp_update_listener(mrp, reserv, stream, new_decl);
+		if (DEV_IOC_OK != rc)
+			result = rc;
+	}
+else {
+if (!stream)
+dbg_msg("no talker stream?"NL);
+else if (!stream->t_reserv)
+dbg_msg("no talker reserv?"NL);
+else
+dbg_msg("wrong port: %d %d"NL, port, stream->in_port);
+}
+	if (mrp->report_head)
+		result = DEV_IOC_MRP_REPORT;
+	return result;
+}  /* proc_mrp_rx_listener */
+
+static u32 mrp_chk_as_port(struct mrp_info *mrp, u8 port,
+			struct mrp_port_info *info, int tc)
+{
+	int avb_a;
+	int avb_b;
+	int asCapable;
+	int i;
+	u32 code_bits = 0;
+	struct ksz_port_cfg *cfg;
+	struct ksz_sw *sw = mrp->parent;
+
+	cfg = get_port_cfg(sw, port);
+	avb_a = cfg->avb_a;
+	avb_b = cfg->avb_b;
+	asCapable = cfg->asCapable | cfg->asCapable_set;
+	if (!asCapable)
+		code_bits |= RFC_ASCAPABLE_BIT;
+	if (tc != SR_CLASS_A && tc != SR_CLASS_B)
+		code_bits |= RFC_PRIORITY_BIT;
+	for (i = SR_CLASS_A; i >= SR_CLASS_B; i--) {
+		if (i != tc)
+			continue;
+		if (mrp->domain[tc - SR_CLASS_B].id &&
+		    info->priority[tc].SRPdomainBoundaryPort)
+			code_bits |= RFC_SRP_BOUNDARY_BIT;
+	}
+
+#if 1
+	if (fqtss_34_2_1b_hack && port < 3)
+		code_bits = 0;
+#endif
+
+	/* need at least 100 Mbps and full-duplex */
+	if (!info->duplex)
+		code_bits |= RFC_ASCAPABLE_BIT;
+	if (tc == SR_CLASS_A && mrp->domain[1].id)
+		cfg->avb_a = code_bits ? false : true;
+	else if (tc == SR_CLASS_B)
+		cfg->avb_b = code_bits ? false : true;
+if (port < 3 && (avb_a != cfg->avb_a || avb_b != cfg->avb_b))
+dbg_msg("  avb: %d=%d:%d %x"NL, port, cfg->avb_a, cfg->avb_b, code_bits);
+	return code_bits;
+}  /* mrp_chk_as_port */
+
+static void mrp_chk_rx_talker(struct mrp_info *mrp, u8 port,
+				struct mrp_port_info *info,
+				  struct SRP_talker *talker,
+				  struct SRP_reserv *reserv)
+{
+	u8 code = talker->FailureCode;
+	u8 new_code = reserv->code;
+	u32 code_bits = 0;
+	const u8 *bridge_id = talker->bridge_id;
+	struct SRP_stream *stream_id = NULL;
+
+	stream_id = srp_find_stream_id(mrp, talker->id);
+
+	/* Stream should alway be found unless out of resource. */
+	if (!stream_id) {
+		code_bits |= RFC_NO_RESOURCES_BIT;
+		goto get_talker_done;
+	}
+	if (memcmp(stream_id->dest, talker->dest, ETH_ALEN) &&
+	    (stream_id->MaxFrameSize != talker->MaxFrameSize ||
+	    stream_id->MaxIntervalFrames != talker->MaxIntervalFrames)) {
+dbg_msg(" used by other: %x"NL, code_bits);
+		code_bits |= RFC_STREAM_ID_BIT;
+		code_bits |= RFC_FIRSTVALUE_CHANGED_BIT;
+	}
+	if (code_bits)
+		goto get_talker_done;
+
+	/* Check FirstValue. */
+	if (stream_id->latency != talker->AccumulatedLatency)
+{
+dbg_msg(" L: %x %x"NL, stream_id->latency, talker->AccumulatedLatency);
+		code_bits |= RFC_LATENCY_CHANGED_BIT;
+}
+#if 1
+	if (msrp_35_1_14g_hack)
+		talker->reserved = 0;
+#endif
+	if (memcmp(stream_id->dest, talker->dest, ETH_ALEN) ||
+	    stream_id->vlan_id != talker->vlan_id ||
+	    stream_id->MaxFrameSize != talker->MaxFrameSize ||
+	    stream_id->MaxIntervalFrames != talker->MaxIntervalFrames ||
+	    stream_id->priority != talker->priority ||
+	    stream_id->rank != talker->rank ||
+	    stream_id->reserved != talker->reserved) {
+		code_bits |= RFC_FIRSTVALUE_CHANGED_BIT;
+		goto get_talker_done;
+	}
+
+	/* Use original code from talker. */
+	if (code)
+		goto get_talker_done;
+
+	code_bits |= mrp_chk_as_port(mrp, port, info,
+		get_traffic_class(mrp, talker->priority));
+	if (talker->MaxFrameSize > 1500)
+		code_bits |= RFC_MAXFRAMESIZE_BIT;
+	if (!talker->MaxIntervalFrames)
+		code_bits |= RFC_NO_BANDWIDTH_BIT;
+
+get_talker_done:
+	new_code = msrp_failure_code(code_bits);
+#ifdef DEBUG_MSRP
+if (port < 3 && new_code)
+dbg_msg(" new_code rx: %04x %d %d"NL, code_bits, code, new_code);
+#endif
+
+	/* This bridge finds a problem. */
+	if (!code && new_code != code) {
+		bridge_id = mrp->id;
+		code = new_code;
+		memcpy(reserv->bridge_id, bridge_id, 8);
+	}
+	reserv->code = code;
+	reserv->code_bits = code_bits;
+}  /* mrp_chk_rx_talker */
+
+static void mrp_chk_tx_talker(struct mrp_info *mrp, u8 port,
+				struct mrp_port_info *info,
+				  struct SRP_reserv *reserv)
+{
+	int tc;
+	u8 code = reserv->code;
+	u8 new_code = reserv->code;
+	u32 code_bits = 0;
+	const u8 *bridge_id = reserv->bridge_id;
+
+	tc = get_traffic_class(mrp, reserv->stream->priority);
+	code_bits |= mrp_chk_as_port(mrp, port, info, tc);
+	if (code || code_bits)
+		goto chk_tx_talker_done;
+
+	do {
+		int bit;
+		int index;
+		struct ksz_port_cfg *cfg;
+		struct ksz_sw *sw = mrp->parent;
+
+		cfg = get_port_cfg(sw, port);
+		index = reserv->stream->vlan_id / VID_IN_DATA;
+		bit = reserv->stream->vlan_id % VID_IN_DATA;
+		if (cfg->untagged[index] & (1 << bit)) {
+			code_bits = RFC_VLAN_TAGGING_BIT;
+			goto chk_tx_talker_done;
+		}
+	} while (0);
+
+	code_bits = chk_avail_tx_bandwidth(mrp, port, info, tc, reserv, false);
+
+chk_tx_talker_done:
+	new_code = msrp_failure_code(code_bits);
+#ifdef DEBUG_MSRP
+if (new_code && port < 3)
+dbg_msg(" new_code tx: %d=%04x %d %d"NL, port, code_bits, code, new_code);
+#endif
+
+	/* This bridge finds a problem. */
+	if (new_code != code) {
+		bridge_id = mrp->id;
+		code = new_code;
+		memcpy(reserv->bridge_id, bridge_id, 8);
+	}
+	reserv->code = code;
+	reserv->code_bits = code_bits;
+
+	reserv->declaration = code ? SRP_FAILED : SRP_ADVERTISE;
+
+#if 0
+	/* A listener is paired. */
+	if (reserv->pair) {
+		if (code || reserv->pair->declaration == SRP_ASKING_FAILED)
+			reserv->pair->code = RFC_NO_BANDWIDTH;
+		else
+			reserv->pair->code = RFC_NO_ERROR;
+	}
+#endif
+}  /* mrp_chk_tx_talker */
+
+static u32 mrp_max_latency(struct mrp_info *mrp, struct mrp_port_info *info,
+			int index,
+			struct SRP_talker *talker)
+{
+	u32 MaxPacketSize = 1542;
+	u32 MaxAllocBand;
+	u32 portTransmitRate = info->speed;
+	u32 MaxFrameSize = talker->MaxFrameSize;
+	u32 t_Interval = 125000;
+	u64 t_AllStreams;
+	u32 t_MaxPacketSize;
+	u32 t_StreamPacket;
+	u32 t_Mbps = 1;
+	u32 t_IPG = 12;
+	int q1;
+	int q2;
+	u64 val;
+	u32 rem;
+
+if (!portTransmitRate)
+dbg_msg(" !! %s"NL, __func__);
+	if (!portTransmitRate)
+		portTransmitRate = 100;
+	q1 = get_queue_priority(mrp, SR_CLASS_A);
+	q2 = get_queue_priority(mrp, SR_CLASS_B);
+	MaxAllocBand = info->bandwidth[q1].deltaBandwidth +
+		       info->bandwidth[q2].deltaBandwidth;
+if (!MaxAllocBand)
+dbg_msg(" !! %s"NL, __func__);
+	if (!MaxAllocBand)
+		MaxAllocBand = 75;
+	if (!index)
+		t_Interval *= 2;
+	if (portTransmitRate != 1000) {
+		t_Mbps *= 10;
+		t_IPG *= 10;
+	}
+	MaxFrameSize += 22 + 8;
+	t_AllStreams = MaxAllocBand * t_Interval;
+	t_StreamPacket = MaxFrameSize * 8 * t_Mbps;
+	t_MaxPacketSize = MaxPacketSize * 8 * t_Mbps;
+	val = t_AllStreams;
+	val = div_u64_u32(val, portTransmitRate);
+	val -= t_StreamPacket;
+	val -= t_IPG;
+	val *= portTransmitRate;
+	val = div_u64_u32(val, MaxAllocBand);
+	val += t_StreamPacket;
+	val += t_MaxPacketSize;
+	rem = (u32) val;
+	rem += info->latency[index].portTcMaxLatency;
+	return rem;
+}
+
+static int proc_mrp_tx_talker(struct mrp_info *mrp, u8 port,
+			      struct SRP_talker *talker,
+			      struct SRP_reserv *reserv, u8 new_decl)
+{
+	int index;
+	u8 declaration;
+	u8 code = talker->FailureCode;
+	const u8 *bridge_id = talker->bridge_id;
+	u32 latency = talker->AccumulatedLatency;
+	struct SRP_stream *stream_id = NULL;
+	struct SRP_reserv *t_reserv;
+	struct SRP_reserv *l_reserv;
+	struct mrp_port_info *info;
+	int result = DEV_IOC_OK;
+
+	info = mrp_get_port_info(mrp, port);
+	stream_id = srp_find_stream_id(mrp, talker->id);
+
+	/* Create reservation if new. */
+	t_reserv = srp_find_reserv(&info->declared, talker->id, SRP_TALKER);
+	code = reserv->code;
+	bridge_id = reserv->bridge_id;
+	declaration = code ? SRP_FAILED : SRP_ADVERTISE;
+	index = get_traffic_index(get_traffic_class(mrp, talker->priority));
+#if 1
+	if (!mrp_10_5_1c_hack)
+#endif
+	latency += mrp_max_latency(mrp, info, index, talker);
+	if (!t_reserv) {
+		if (!code)
+			bridge_id = mrp->id;
+		t_reserv = srp_create_reserv(talker->id, SRP_TALKER,
+					     declaration, latency, bridge_id,
+					     code);
+		if (!t_reserv)
+			return -ENOMEM;
+
+		t_reserv->stream = stream_id;
+		srp_insert_reserv(&info->declared, t_reserv);
+	} else {
+		t_reserv->declaration = declaration;
+		memcpy(t_reserv->bridge_id, bridge_id, 8);
+		t_reserv->code = code;
+		t_reserv->rx_code = code;
+	}
+	t_reserv->code_bits = reserv->code_bits;
+
+	/* Check outgoing failures. */
+	if (!t_reserv->code)
+		mrp_chk_tx_talker(mrp, port, info, t_reserv);
+
+	if (!mrp->no_report) {
+		u8 action = MRP_ACTION_TX;
+
+		if (!new_decl && !code && t_reserv->code)
+			new_decl = true;
+		if (new_decl)
+			action = MRP_ACTION_TX_NEW;
+		add_attrib_report(mrp, t_reserv, action,
+			MRP_TYPE_TALKER, port);
+	}
+
+	l_reserv = srp_find_reserv(&info->registered, talker->id,
+				   SRP_LISTENER);
+	if (l_reserv) {
+		int rc;
+
+dbg_msg("have listen: %d=%p"NL, port, l_reserv);
+		rc = proc_mrp_tx_listener(mrp, port, info, l_reserv, t_reserv,
+			t_reserv->stream);
+	}
+
+	/* Ask application to retrieve attributes. */
+	if (mrp->report_head)
+		result = DEV_IOC_MRP_REPORT;
+	return result;
+}  /* proc_mrp_tx_talker */
+
+static int mrp_delete_talker(struct mrp_info *mrp, u8 port,
+	struct mrp_port_info *info, struct SRP_reserv *reserv)
+{
+	struct SRP_reserv *l_reserv;
+	struct SRP_reserv *t_reserv;
+	uint n;
+	uint q;
+	struct ksz_sw *sw = mrp->parent;
+	int result = DEV_IOC_OK;
+	struct mrp_node *mac_node;
+#if 1
+	struct mrp_node *vlan_node;
+	struct mrp_vlan_info data;
+#endif
+	struct mrp_mac_info mac_data;
+	u16 fid = 0;
+	int free = true;
+	int active = mrp->status.msrpEnabledStatus &&
+		     info->status.msrpPortEnabledStatus;
+	int active_other;
+
+	/* No listener propagation if no talker. */
+	l_reserv = srp_find_reserv(&info->declared, reserv->id,
+		SRP_LISTENER);
+	if (l_reserv) {
+		srp_remove_reserv(l_reserv, true);
+	}
+
+	for (n = 0; n <= mrp->ports; n++) {
+		q = get_phy_port(sw, n);
+		if (q == port)
+			continue;
+
+		info = mrp_get_port_info(mrp, q);
+		active_other = mrp->status.msrpEnabledStatus &&
+			info->status.msrpPortEnabledStatus;
+		if (!info->status.msrpPortEnabledStatus)
+			continue;
+		t_reserv = srp_find_reserv(&info->declared, reserv->id,
+					   SRP_TALKER);
+		if (!t_reserv)
+			continue;
+
+		/* Unlink listener from talker. */
+		l_reserv = srp_find_reserv(&info->registered, reserv->id,
+					   SRP_LISTENER);
+		if (l_reserv) {
+			bool dropped = false;
+
+			l_reserv->pair = NULL;
+
+			/* stream will be deleted */
+			l_reserv->stream = NULL;
+
+			/* Drop reservation if listener is ready. */
+			if (l_reserv->declaration != SRP_ASKING_FAILED) {
+				dropped = drop_reserv(mrp, q, info, t_reserv,
+						      true);
+				if (dropped) {
+if (l_reserv->code != RFC_NO_ERROR)
+dbg_msg(" not ok? %d"NL, l_reserv->code);
+#if 1
+					if (active_other)
+					start_passive_reserv(mrp, q, info);
+#endif
+				}
+#ifdef DEBUG_MSRP
+else
+dbg_msg(" not dropped"NL);
+#endif
+			}
+			if (!dropped)
+				drop_reserv(mrp, q, info, t_reserv, false);
+			l_reserv->code = RFC_NO_RESOURCES;
+		}
+
+		free = true;
+#if 1
+		/* Reservation will be freed after reporting leaving. */
+		if (!mrp->no_report && active_other) {
+			free = false;
+			add_attrib_report(mrp, t_reserv, MRP_ACTION_LV,
+				MRP_TYPE_TALKER, q);
+		}
+#endif
+		srp_remove_reserv(t_reserv, free);
+	}
+
+	srp_cfg_reserv(mrp, NULL);
+
+	free = true;
+#if 1
+	/* This is purely used to free the stream after reporting. */
+	if (!mrp->no_report && active) {
+		free = false;
+		add_attrib_report(mrp, reserv, MRP_ACTION_LV, MRP_TYPE_TALKER,
+			sw->port_cnt);
+	}
+#endif
+
+#if 1
+	data.vid = reserv->stream->vlan_id;
+	memcpy(data.addr, reserv->stream->dest, ETH_ALEN);
+	vlan_node = mrp_find_node(&mrp->vlan_list, cmp_vlan, &data);
+	if (vlan_node) {
+#if 0
+		u16 ports;
+#endif
+		struct mrp_vlan_info *vlan = NULL;
+
+#ifdef DEBUG_MRP_BASIC
+		mrp_show_node(&mrp->vlan_list, show_vlan_info);
+#endif
+		vlan = vlan_node->data;
+		fid = vlan->fid;
+		vlan->ports &= SRP_PORT_AVAIL;
+#if 0
+		ports = mrp_find_vlan_ports(&mrp->vlan_list, data.vid, NULL,
+			NULL);
+		mrp_cfg_vlan(mrp, vlan->index, vlan->vid, vlan->fid, ports);
+		if (!ports) {
+			mrp_free_fid(mrp, vlan->fid);
+			mrp_free_vlan(mrp, vlan->index);
+		}
+
+		/* Nobody is using the VLAN. */
+		if (!vlan->ports)
+			mrp_delete_node(&mrp->vlan_list, cmp_vlan, vlan_node);
+#endif
+	} else
+		fid = mrp_alloc_fid(mrp, data.vid);
+#endif
+	mac_data.fid = fid;
+	memcpy(mac_data.addr, reserv->stream->dest, ETH_ALEN);
+	mac_node = mrp_find_node(&mrp->mac_list, cmp_mac, &mac_data);
+#if 1
+	if (!mac_node) {
+dbg_msg(" not found! %d %02x:%02x"NL, fid, mac_data.addr[4], mac_data.addr[5]);
+		mrp_show_node(&mrp->mac_list, show_mac_info);
+	}
+#endif
+	if (mac_node) {
+		struct mrp_mac_info *mac = NULL;
+
+#ifdef DEBUG_MRP_BASIC
+		mrp_show_node(&mrp->mac_list, show_mac_info);
+#endif
+		mac = mac_node->data;
+		mac->srp_ports &= SRP_PORT_AVAIL;
+		mac->ports = mac->srp_ports | mac->mrp_ports;
+		mrp_cfg_dest_addr(mrp, mac->index, mac->addr, mac->ports,
+			mac->fid);
+		if (!mac->ports) {
+			mrp_free_mac(mrp, mac->index);
+			mrp_delete_node(&mrp->mac_list, cmp_mac, mac_node);
+		}
+	}
+	srp_remove_stream(reserv->stream, free);
+	srp_remove_reserv(reserv, free);
+
+	for (n = 0; n <= mrp->ports; n++) {
+		q = get_phy_port(sw, n);
+		if (q == port)
+			continue;
+
+		info = mrp_get_port_info(mrp, q);
+		active_other = mrp->status.msrpEnabledStatus &&
+			info->status.msrpPortEnabledStatus;
+		if (!active_other)
+			continue;
+		chk_talker_decl(mrp, q, info);
+	}
+
+	if (mrp->report_head)
+		result = DEV_IOC_MRP_REPORT;
+	return result;
+}  /* mrp_delete_talker */
+
+static int proc_mrp_lv_talker(struct mrp_info *mrp, u8 port,
+	struct SRP_talker *talker)
+{
+	struct SRP_reserv *l_reserv;
+	struct SRP_reserv *reserv;
+	struct SRP_reserv *t_reserv;
+	struct mrp_port_info *info;
+	int result = DEV_IOC_OK;
+	uint n;
+	uint q;
+	struct ksz_sw *sw = mrp->parent;
+	struct mrp_node *mac_node;
+#if 1
+	struct mrp_node *vlan_node;
+	struct mrp_vlan_info data;
+#endif
+	struct mrp_mac_info mac_data;
+	u16 fid = 0;
+	int free = true;
+
+	info = mrp_get_port_info(mrp, port);
+#ifdef DEBUG_MSRP
+dbg_msg(" %s p:%d %02x:%02x:%02x:%02x:%02x:%02x:%02x:%02x"NL, __func__,
+port,
+talker->id[0],
+talker->id[1],
+talker->id[2],
+talker->id[3],
+talker->id[4],
+talker->id[5],
+talker->id[6],
+talker->id[7]);
+dbg_msg("  %02x:%02x:%02x:%02x:%02x:%02x %03x %u:%u f:%u i:%u l:%u c:%u"NL,
+talker->dest[0],
+talker->dest[1],
+talker->dest[2],
+talker->dest[3],
+talker->dest[4],
+talker->dest[5],
+talker->vlan_id, talker->priority, talker->rank,
+talker->MaxFrameSize, talker->MaxIntervalFrames, talker->AccumulatedLatency,
+talker->FailureCode);
+#endif
+	reserv = srp_find_reserv(&info->registered, talker->id, SRP_TALKER);
+	if (!reserv)
+		return result;
+	reserv->ticks = jiffies - reserv->ticks;
+	if (reserv->ticks < 100)
+dbg_msg(" ?! ");
+dbg_msg(" t:%u"NL, reserv->ticks * 10 / 1000);
+
+	/* No listener propagation if no talker. */
+	l_reserv = srp_find_reserv(&info->declared, talker->id, SRP_LISTENER);
+	if (l_reserv) {
+		free = false;
+		add_attrib_report(mrp, l_reserv, MRP_ACTION_LV,
+			MRP_TYPE_LISTENER, port);
+		srp_remove_reserv(l_reserv, free);
+		free = true;
+	}
+
+	for (n = 0; n <= mrp->ports; n++) {
+		q = get_phy_port(sw, n);
+		if (q == port)
+			continue;
+
+		info = mrp_get_port_info(mrp, q);
+#if 1
+		if (!info->status.msrpPortEnabledStatus)
+			continue;
+#endif
+		t_reserv = srp_find_reserv(&info->declared, talker->id,
+					   SRP_TALKER);
+		if (!t_reserv)
+			continue;
+
+		/* Unlink listener from talker. */
+		l_reserv = srp_find_reserv(&info->registered, talker->id,
+					   SRP_LISTENER);
+		if (l_reserv) {
+			bool dropped = false;
+
+			l_reserv->pair = NULL;
+
+			/* stream will be deleted */
+			l_reserv->stream = NULL;
+
+			/* Drop reservation if listener is ready. */
+			if (l_reserv->declaration != SRP_ASKING_FAILED) {
+				dropped = drop_reserv(mrp, q, info, t_reserv,
+						      true);
+				if (dropped) {
+if (l_reserv->code != RFC_NO_ERROR)
+dbg_msg(" not ok? %d"NL, l_reserv->code);
+					start_passive_reserv(mrp, q, info);
+				}
+#ifdef DEBUG_MSRP
+else
+dbg_msg(" not dropped"NL);
+#endif
+			}
+			if (!dropped)
+				drop_reserv(mrp, q, info, t_reserv, false);
+			l_reserv->code = RFC_NO_RESOURCES;
+		}
+
+		/* Reservation will be freed after reporting leaving. */
+		if (!mrp->no_report) {
+			free = false;
+			add_attrib_report(mrp, t_reserv, MRP_ACTION_LV,
+				MRP_TYPE_TALKER, q);
+		}
+		srp_remove_reserv(t_reserv, free);
+	}
+
+	srp_cfg_reserv(mrp, NULL);
+
+	/* This is purely used to free the stream after reporting. */
+	if (!mrp->no_report) {
+		free = false;
+		add_attrib_report(mrp, reserv, MRP_ACTION_LV, MRP_TYPE_TALKER,
+			sw->port_cnt);
+	}
+if (memcmp(reserv->stream->dest, talker->dest, ETH_ALEN))
+dbg_msg("  not same dest! %02x:%02x %02x:%02x"NL,
+reserv->stream->dest[4], reserv->stream->dest[5],
+talker->dest[4], talker->dest[5]);
+#if 1
+	data.vid = reserv->stream->vlan_id;
+	memcpy(data.addr, reserv->stream->dest, ETH_ALEN);
+	vlan_node = mrp_find_node(&mrp->vlan_list, cmp_vlan, &data);
+	if (vlan_node) {
+#if 0
+		u16 ports;
+#endif
+		struct mrp_vlan_info *vlan = NULL;
+
+#ifdef DEBUG_MRP_BASIC
+		mrp_show_node(&mrp->vlan_list, show_vlan_info);
+#endif
+		vlan = vlan_node->data;
+		fid = vlan->fid;
+		vlan->ports &= SRP_PORT_AVAIL;
+#if 0
+		ports = mrp_find_vlan_ports(&mrp->vlan_list, data.vid, NULL,
+			NULL);
+		mrp_cfg_vlan(mrp, vlan->index, vlan->vid, vlan->fid, ports);
+		if (!ports) {
+			mrp_free_fid(mrp, vlan->fid);
+			mrp_free_vlan(mrp, vlan->index);
+		}
+
+		/* Nobody is using the VLAN. */
+		if (!vlan->ports)
+			mrp_delete_node(&mrp->vlan_list, cmp_vlan, vlan_node);
+#endif
+	} else
+		fid = mrp_alloc_fid(mrp, data.vid);
+#endif
+	mac_data.fid = fid;
+	memcpy(mac_data.addr, reserv->stream->dest, ETH_ALEN);
+	mac_node = mrp_find_node(&mrp->mac_list, cmp_mac, &mac_data);
+#if 1
+	if (!mac_node) {
+dbg_msg(" not found! %d %02x:%02x"NL, fid, mac_data.addr[4], mac_data.addr[5]);
+		mrp_show_node(&mrp->mac_list, show_mac_info);
+	}
+#endif
+	if (mac_node) {
+		struct mrp_mac_info *mac = NULL;
+
+#ifdef DEBUG_MRP_BASIC
+		mrp_show_node(&mrp->mac_list, show_mac_info);
+#endif
+		mac = mac_node->data;
+		mac->srp_ports &= SRP_PORT_AVAIL;
+		mac->ports = mac->srp_ports | mac->mrp_ports;
+		mrp_cfg_dest_addr(mrp, mac->index, mac->addr, mac->ports,
+			mac->fid);
+		if (!mac->ports) {
+			mrp_free_mac(mrp, mac->index);
+			mrp_delete_node(&mrp->mac_list, cmp_mac, mac_node);
+		}
+	}
+	srp_remove_stream(reserv->stream, free);
+	srp_remove_reserv(reserv, free);
+
+	for (n = 0; n <= mrp->ports; n++) {
+		q = get_phy_port(sw, n);
+		if (q == port)
+			continue;
+
+		info = mrp_get_port_info(mrp, q);
+		if (!info->status.msrpPortEnabledStatus)
+			continue;
+		chk_talker_decl(mrp, q, info);
+	}
+
+	if (mrp->report_head)
+		result = DEV_IOC_MRP_REPORT;
+	return result;
+}  /* proc_mrp_lv_talker */
+
+static int proc_mrp_rx_talker(struct mrp_info *mrp, u8 port,
+	struct SRP_talker *talker, u8 new_decl)
+{
+	uint n;
+	uint q;
+	struct ksz_sw *sw = mrp->parent;
+	int result = DEV_IOC_OK;
+	u8 declaration;
+	struct SRP_reserv *reserv;
+	struct SRP_stream *stream;
+	struct mrp_port_info *info;
+	u8 code = talker->FailureCode;
+	u32 code_bits = 0;
+
+	info = mrp_get_port_info(mrp, port);
+#ifdef DEBUG_MSRP
+dbg_msg(" %s p:%d %d %02x:%02x:%02x:%02x:%02x:%02x:%02x:%02x"NL, __func__,
+port,
+new_decl,
+talker->id[0],
+talker->id[1],
+talker->id[2],
+talker->id[3],
+talker->id[4],
+talker->id[5],
+talker->id[6],
+talker->id[7]);
+dbg_msg("  %02x:%02x:%02x:%02x:%02x:%02x %03x %u:%u f:%u i:%u l:%u c:%u"NL,
+talker->dest[0],
+talker->dest[1],
+talker->dest[2],
+talker->dest[3],
+talker->dest[4],
+talker->dest[5],
+talker->vlan_id, talker->priority, talker->rank,
+talker->MaxFrameSize, talker->MaxIntervalFrames, talker->AccumulatedLatency,
+talker->FailureCode);
+#endif
+
+	/* Create stream if new. */
+	stream = srp_find_stream_id(mrp, talker->id);
+	if (!stream) {
+		stream = srp_find_dest_addr(mrp, talker->dest);
+		if (stream)
+			code_bits = RFC_DEST_ADDR_BIT;
+		stream = srp_create_stream(talker->id, talker->dest,
+			talker->vlan_id, talker->MaxFrameSize,
+			talker->MaxIntervalFrames, talker->priority,
+			talker->rank, talker->reserved,
+			talker->AccumulatedLatency);
+		if (stream) {
+#ifdef DEBUG_MSRP
+			char bw_str1[20];
+#endif
+			int tc = get_traffic_class(mrp, stream->priority);
+
+			stream->bandwidth =
+				calculate_bandwidth(stream->MaxFrameSize,
+						    stream->MaxIntervalFrames,
+						    frames_per_sec(tc));
+#ifdef DEBUG_MSRP
+		format_num(bw_str1, stream->bandwidth);
+dbg_msg("bw: %s"NL, bw_str1);
+#endif
+			srp_insert_stream_by_id(mrp, stream);
+			srp_insert_stream_by_dest(mrp, stream);
+		}
+	}
+	if (!stream)
+		return -ENOMEM;
+
+	/* Create reservation if new. */
+	reserv = srp_find_reserv(&info->registered, talker->id, SRP_TALKER);
+	declaration = code ? SRP_FAILED : SRP_ADVERTISE;
+	if (!reserv) {
+		struct mrp_node *node;
+		struct mrp_mac_info *mac;
+#if 0
+		struct mrp_vlan_info *vlan;
+		u8 index;
+#endif
+		u16 fid;
+		u16 ports = 1 << port;
+
+		reserv = srp_create_reserv(talker->id, SRP_TALKER,
+					   declaration,
+					   talker->AccumulatedLatency,
+					   talker->bridge_id, code);
+		if (!reserv)
+			return -ENOMEM;
+
+		/* Indicate registration source. */
+		reserv->streamAge = 0;
+		srp_insert_reserv(&info->registered, reserv);
+		stream->in_port = port;
+		stream->t_reserv = reserv;
+
+#if 0
+		/* Setup initial VLAN configuration. */
+		ports = mrp_find_vlan_ports(&mrp->vlan_list, talker->vlan_id,
+			&index, &fid);
+		node = mrp_get_vlan_info(&mrp->vlan_list, talker->vlan_id,
+#if 0
+			talker->dest);
+#else
+			NULL);
+#endif
+		if (!node)
+			return -ENOMEM;
+		vlan = node->data;
+#if 0
+		vlan->ports = SRP_PORT_READY;
+#endif
+		ports = vlan->ports;
+
+		/* First time setting up VLAN table. */
+		if (!ports) {
+			vlan->index = mrp_alloc_vlan(mrp);
+			vlan->fid = mrp_alloc_fid(mrp, vlan->vid);
+#if 1
+			vlan->ports = SRP_PORT_READY;
+#endif
+			mrp_cfg_vlan(mrp, vlan->index, vlan->vid, vlan->fid,
+				vlan->ports);
+#if 0
+		} else {
+			vlan->index = index;
+			vlan->fid = fid;
+		}
+#endif
+		}
+#endif
+		fid = mrp_alloc_fid(mrp, talker->vlan_id);
+
+		/* Setup initial MAC configuration. */
+		node = mrp_get_mac_info(&mrp->mac_list, talker->dest, fid);
+		if (!node)
+			return -ENOMEM;
+		mac = node->data;
+		ports = mac->ports;
+		mac->rx_ports |= ports;
+		mac->srp_ports = SRP_PORT_READY;
+		mac->ports = mac->mrp_ports | mac->srp_ports;
+
+		/* First time setting up MAC table. */
+		if (!ports) {
+			int tc;
+			struct ksz_port_cfg *cfg;
+			struct ksz_sw *sw = mrp->parent;
+
+			tc = get_traffic_class(mrp, stream->priority);
+			cfg = get_port_cfg(sw, port);
+			if ((tc == SR_CLASS_A && cfg->avb_a) ||
+			    (tc == SR_CLASS_B && cfg->avb_b))
+				ports = mac->ports;
+			else
+				ports = SRP_PORT_OTHER;
+			mac->index = mrp_alloc_mac(mrp);
+			mrp_cfg_dest_addr(mrp, mac->index, mac->addr,
+				ports, mac->fid);
+		}
+#ifdef DEBUG_MRP_BASIC
+		mrp_show_node(&mrp->mac_list, show_mac_info);
+		mrp_show_node(&mrp->vlan_list, show_vlan_info);
+#endif
+	} else {
+		reserv->declaration = declaration;
+		reserv->latency = talker->AccumulatedLatency;
+		memcpy(reserv->bridge_id, talker->bridge_id, 8);
+		reserv->code = code;
+		reserv->rx_code = code;
+	}
+	reserv->stream = stream;
+	if (code_bits) {
+		reserv->code_bits = code_bits;
+		reserv->code = msrp_failure_code(reserv->code_bits);
+		memcpy(reserv->bridge_id, mrp->id, 8);
+	} else
+
+	/* Check incoming failures. */
+	mrp_chk_rx_talker(mrp, port, info, talker, reserv);
+
+	if (!new_decl) {
+		struct ksz_sw *sw = mrp->parent;
+
+		if (sw->ops->get_tcDetected(sw, port))
+			new_decl = true;
+		if (!new_decl && !talker->FailureCode && reserv->code)
+			new_decl = true;
+	}
+
+	mrp->listeners = 0;
+	for (n = 0; n <= mrp->ports; n++) {
+		q = get_phy_port(sw, n);
+		if (q == port)
+			continue;
+		info = mrp_get_port_info(mrp, q);
+		if (!info->status.msrpPortEnabledStatus)
+			continue;
+		if (!info->link)
+			continue;
+		result = proc_mrp_tx_talker(mrp, q, talker, reserv, new_decl);
+		if (result < 0)
+			return result;
+		reserv->tx_ports |= (1 << q);
+	}
+
+	/* There are no listeners. */
+	if (!mrp->listeners)
+		return result;
+
+	info = mrp_get_port_info(mrp, port);
+	reserv = mrp_link_listener(mrp, info, stream);
+	if (!reserv)
+		return -ENOMEM;
+
+	/* Listener declaration will be updated in following call. */
+	result = mrp_update_listener(mrp, reserv, stream, false);
+	return result;
+}  /* proc_mrp_rx_talker */
+
+static void mrp_change_tx_reserv(struct mrp_info *mrp, u8 port,
+	struct mrp_port_info *info,
+	struct SRP_reserv *reserv, bool *drop)
+{
+	u8 action;
+	u8 code;
+	u8 decl;
+
+	if (SRP_TALKER != reserv->direction || reserv->rx_code)
+		return;
+
+	code = reserv->code;
+	decl = reserv->declaration;
+
+	mrp_chk_tx_talker(mrp, port, info, reserv);
+	if (code == reserv->code)
+		return;
+
+	action = MRP_ACTION_TX;
+
+	if (reserv->streamAge) {
+		if (reserv->code == RFC_PORT_IS_NOT_AVB) {
+			int tc = get_traffic_class(mrp,
+				reserv->stream->priority);
+
+			drop[tc] = true;
+		}
+	}
+	if (decl != reserv->declaration)
+		action = MRP_ACTION_TX_NEW;
+	add_attrib_report(mrp, reserv, action, MRP_TYPE_TALKER, port);
+}  /* mrp_change_tx_reserv */
+
+static void mrp_drop_reserv(struct mrp_info *mrp, u8 port,
+			struct mrp_port_info *info, bool *drop)
+{
+	struct mrp_traffic_info *traffic;
+	int rc;
+	int tc;
+	void *param[6];
+
+	for (tc = SR_CLASS_A; tc >= SR_CLASS_B; tc--) {
+		traffic = get_traffic_info(info, tc);
+		if (!drop[tc])
+			continue;
+
+		/* Mark all streams to stop. */
+		rc = stream_iter(&traffic->active, param, stream_set_mark);
+		info->bandwidth_used -= traffic->bandwidth_used;
+		traffic->bandwidth_used = 0;
+		traffic->bandwidth_left = traffic->bandwidth_max;
+		if (traffic->bandwidth_other)
+			*traffic->bandwidth_other = *traffic->bandwidth_avail +
+				traffic->bandwidth_left;
+		param[0] = mrp;
+		param[1] = traffic;
+		param[2] = &port;
+		rc = stream_oper(&traffic->active, param, stream_stop);
+	}
+	info->bandwidth_left = info->bandwidth_max - info->bandwidth_used;
+	chk_passive_reserv(mrp, port, info);
+
+	srp_cfg_reserv(mrp, info);
+}  /* mrp_drop_reserv */
+
+static bool mrp_match_tx_reserv(struct mrp_info *mrp, u8 port,
+	struct mrp_port_info *info,
+	struct SRP_reserv *reserv, struct SRP_reserv *t_reserv)
+{
+	bool drop[8];
+	u8 code;
+	u8 decl;
+
+	if (SRP_TALKER != t_reserv->direction ||
+	    t_reserv->stream != reserv->stream)
+		return false;
+
+	code = t_reserv->code;
+	decl = t_reserv->declaration;
+
+	/* use error codes from receive side */
+	t_reserv->code_bits = reserv->code_bits;
+	t_reserv->code = reserv->code;
+	t_reserv->rx_code = reserv->code;
+	t_reserv->declaration = t_reserv->code ? SRP_FAILED : SRP_ADVERTISE;
+	if (!reserv->code)
+		mrp_chk_tx_talker(mrp, port, info, t_reserv);
+	drop[SR_CLASS_A] = 0;
+	drop[SR_CLASS_B] = 0;
+	if (code != t_reserv->code) {
+		u8 action = MRP_ACTION_TX;
+
+dbg_msg(" co: %d %d"NL, code, t_reserv->code);
+		if (t_reserv->streamAge) {
+			if (t_reserv->code == RFC_PORT_IS_NOT_AVB) {
+				int tc = get_traffic_class(mrp,
+					t_reserv->stream->priority);
+
+dbg_msg("drop"NL);
+				drop[tc] = true;
+			}
+		}
+		if (decl != t_reserv->declaration)
+			action = MRP_ACTION_TX_NEW;
+		add_attrib_report(mrp, t_reserv, action, MRP_TYPE_TALKER, port);
+	}
+	mrp_drop_reserv(mrp, port, info, drop);
+	return true;
+}
+
+static void mrp_change_rx_reserv(struct mrp_info *mrp, u8 port,
+	struct mrp_port_info *info,
+	struct SRP_reserv *reserv)
+{
+	struct SRP_reserv *t_reserv;
+	int n;
+	u8 code;
+
+	if (SRP_TALKER != reserv->direction || reserv->rx_code)
+		return;
+
+	code = reserv->code;
+
+	/* check if these bits are still there */
+	reserv->code_bits &= ~(
+		RFC_ASCAPABLE_BIT |
+		RFC_SRP_BOUNDARY_BIT |
+		RFC_PRIORITY_BIT);
+	reserv->code_bits |= mrp_chk_as_port(mrp, port, info,
+		get_traffic_class(mrp, reserv->stream->priority));
+	reserv->code = msrp_failure_code(reserv->code_bits);
+	if (code == reserv->code)
+		return;
+dbg_msg(" c: %d=%d %d"NL, port, code, reserv->code);
+	for (n = 0; n <= mrp->ports; n++) {
+		if (n == port)
+			continue;
+		if (!(reserv->tx_ports & (1 << n)))
+			continue;
+		info = mrp_get_port_info(mrp, n);
+		if (!info->status.msrpPortEnabledStatus)
+			continue;
+		t_reserv = info->declared.next;
+		while (t_reserv) {
+			if (mrp_match_tx_reserv(mrp, n, info, reserv,
+					     t_reserv))
+				break;
+			t_reserv = t_reserv->next;
+		}
+	}
+}  /* mrp_change_rx_reserv */
+
+static void mrp_fwd_addr(struct mrp_info *mrp, uint port, bool avb)
+{
+	struct mrp_node *prev;
+	struct mrp_node *next;
+	struct mrp_mac_info *mac;
+
+	prev = &mrp->mac_list.anchor;
+	next = prev->next;
+	while (next) {
+		mac = next->data;
+		if ((mac->rx_ports & (1 << port))) {
+			u16 ports;
+
+			if (avb)
+				ports = mac->ports;
+			else
+				ports = SRP_PORT_OTHER;
+			mrp_cfg_dest_addr(mrp, mac->index, mac->addr, ports,
+					  mac->fid);
+		}
+		prev = next;
+		next = prev->next;
+	}
+}  /* mrp_fwd_addr */
+
+static int proc_mrp_chk_talker(struct mrp_info *mrp, u8 port, int tc)
+{
+	struct mrp_port_info *info;
+	struct SRP_reserv *reserv;
+	int avb_a;
+	int avb_b;
+	int result = DEV_IOC_OK;
+	bool drop[8];
+	struct ksz_port_cfg *cfg;
+	struct ksz_sw *sw = mrp->parent;
+
+	cfg = get_port_cfg(sw, port);
+	avb_a = cfg->avb_a;
+	avb_b = cfg->avb_b;
+	info = mrp_get_port_info(mrp, port);
+	if (!tc) {
+		mrp_chk_as_port(mrp, port, info, SR_CLASS_A);
+		tc = SR_CLASS_B;
+	}
+	mrp_chk_as_port(mrp, port, info, tc);
+	if (avb_b != cfg->avb_b) {
+#if 0
+		if (port < mrp->ports)
+#endif
+			enable_acl_remap(mrp, port, !cfg->avb_b, cfg->avb_b);
+		mrp_fwd_addr(mrp, port, cfg->avb_b);
+	} else if (!avb_b && avb_a != cfg->avb_a) {
+#if 0
+		if (port < mrp->ports)
+#endif
+			enable_acl_remap(mrp, port, !cfg->avb_a, cfg->avb_a);
+		mrp_fwd_addr(mrp, port, cfg->avb_a);
+	}
+	reserv = info->registered.next;
+	while (reserv) {
+		mrp_change_rx_reserv(mrp, port, info, reserv);
+		reserv = reserv->next;
+	}
+	drop[SR_CLASS_A] = 0;
+	drop[SR_CLASS_B] = 0;
+	reserv = info->declared.next;
+	while (reserv) {
+		mrp_change_tx_reserv(mrp, port, info, reserv, drop);
+		reserv = reserv->next;
+	}
+	mrp_drop_reserv(mrp, port, info, drop);
+
+	if (mrp->report_head)
+		result = DEV_IOC_MRP_REPORT;
+	return result;
+}  /* proc_mrp_chk_talker */
+
+static int is_host_port(struct mrp_info *mrp, u8 port)
+{
+	struct ksz_sw *sw = mrp->parent;
+
+	return (port == sw->HOST_PORT);
+}  /* is_host_port */
+
+static int proc_mrp_lv_domain(struct mrp_info *mrp, u8 port,
+	struct SRP_domain_class *domain)
+{
+	int result = DEV_IOC_OK;
+	struct mrp_port_info *info;
+	struct SRP_domain_class *self = NULL;
+
+dbg_msg(" %s %d=%d %d %d"NL, __func__, port,
+	domain->id, domain->priority, domain->vlan_id);
+	if (is_host_port(mrp, port))
+		return result;
+	info = mrp_get_port_info(mrp, port);
+	if (SR_CLASS_B <= domain->id && domain->id <= SR_CLASS_A)
+		self = &mrp->domain[domain->id - SR_CLASS_B];
+	if (self && (self->id == domain->id &&
+	    self->priority == domain->priority)) {
+		if (!info->priority[domain->id].SRPdomainBoundaryPort) {
+			info->priority[domain->id].SRPdomainBoundaryPort = 1;
+			result = proc_mrp_chk_talker(mrp, port, domain->id);
+		}
+	}
+
+	return result;
+}  /* proc_mrp_lv_domain */
+
+static int proc_mrp_rx_domain(struct mrp_info *mrp, u8 port,
+	struct SRP_domain_class *domain)
+{
+	int result = DEV_IOC_OK;
+	struct mrp_port_info *info;
+	struct SRP_domain_class *self = NULL;
+
+dbg_msg(" %s %d=%d %d %d"NL, __func__, port,
+	domain->id, domain->priority, domain->vlan_id);
+	if (is_host_port(mrp, port))
+		return result;
+	if (domain->id >= SR_CLASS_NUM)
+		return result;
+	info = mrp_get_port_info(mrp, port);
+	if (SR_CLASS_B <= domain->id && domain->id <= SR_CLASS_A)
+		self = &mrp->domain[domain->id - SR_CLASS_B];
+	if (self && (self->id == domain->id &&
+	    self->priority == domain->priority)) {
+		if (info->priority[domain->id].SRPdomainBoundaryPort) {
+			info->priority[domain->id].SRPdomainBoundaryPort = 0;
+			result = proc_mrp_chk_talker(mrp, port, domain->id);
+		}
+	} else if (!info->priority[domain->id].SRPdomainBoundaryPort) {
+		info->priority[domain->id].SRPdomainBoundaryPort = 1;
+		if (self)
+			result = proc_mrp_chk_talker(mrp, port, domain->id);
+	}
+
+	return result;
+}  /* proc_mrp_rx_domain */
+
+static int proc_mrp_chk_registered(struct mrp_info *mrp, u8 port)
+{
+	struct SRP_reserv *reserv;
+	struct mrp_port_info *info;
+	int i;
+	int result = DEV_IOC_OK;
+
+dbg_msg("%s %d"NL, __func__, port);
+	for (i = 0; i <= mrp->ports; i++) {
+		if (i == port)
+			continue;
+		info = mrp_get_port_info(mrp, i);
+		reserv = info->registered.next;
+		while (reserv) {
+if (SRP_TALKER == reserv->direction)
+dbg_msg(" t:%d %x"NL, info->index, reserv->tx_ports);
+			if (SRP_TALKER == reserv->direction &&
+			    !(reserv->tx_ports & (1 << port))) {
+				struct SRP_talker talker;
+				struct SRP_reserv *l_reserv;
+				int result;
+
+dbg_msg(" need talker decl: %d"NL, i);
+				memcpy(&talker, reserv->stream, 25);
+dbg_msg(" latency: %u %u"NL, talker.AccumulatedLatency, reserv->stream->latency);
+				memcpy(talker.bridge_id, reserv->bridge_id, 8);
+				talker.FailureCode = reserv->code;
+				result = proc_mrp_tx_talker(mrp, port, &talker,
+							    reserv, true);
+				if (result < 0)
+					continue;
+				l_reserv = srp_find_reserv(&info->declared,
+							   reserv->stream->id,
+							   SRP_LISTENER);
+				if (l_reserv)
+					mrp_update_listener(mrp,
+							    l_reserv,
+							    reserv->stream,
+							    false);
+				chk_reserv(info, i);
+			}
+			reserv = reserv->next;
+		}
+	}
+	if (mrp->report_head)
+		result = DEV_IOC_MRP_REPORT;
+	return result;
+}  /* proc_mrp_chk_registered */
+#endif
+
+#ifdef CONFIG_KSZ_MRP
+static int proc_mrp_get_tx(struct mrp_info *mrp, struct mrp_cfg_options *cmd,
+			   int *output)
+{
+	struct mrp_report *attrib;
+
+#ifdef CONFIG_KSZ_MSRP
+	struct SRP_reserv *reserv;
+	struct SRP_stream *stream;
+#endif
+	struct ksz_sw *sw = mrp->parent;
+	int result = DEV_IOC_MRP_REPORT;
+
+	attrib = mrp->report_head;
+	if (!attrib)
+		return DEV_IOC_INVALID_CMD;
+
+	*output = 0;
+	cmd->action = attrib->action;
+	cmd->type = attrib->type;
+	cmd->port = get_log_port(sw, attrib->port);
+dbg_msg(" %s %d %d"NL, __func__, attrib->port, cmd->port);
+	if (MRP_TYPE_MAC == attrib->type) {
+		struct MRP_mac *mac = &cmd->data.mac;
+		struct mrp_node *node = attrib->attrib;
+		struct mrp_mac_info *info = node->data;
+
+		memcpy(mac->addr, info->addr, ETH_ALEN);
+		*output = SIZEOF_MRP_mac;
+		if (MRP_ACTION_LV == attrib->action &&
+		    attrib->port == mrp->ports &&
+		    !info->ports) {
+			mrp_free_node(node);
+		}
+	} else if (MRP_TYPE_VLAN == attrib->type) {
+		struct MRP_vlan *vlan = &cmd->data.vlan;
+		struct mrp_node *node = attrib->attrib;
+		struct mrp_vlan_info *info = node->data;
+
+		vlan->id = info->vid;
+		*output = SIZEOF_MRP_vlan;
+		if (MRP_ACTION_LV == attrib->action &&
+		    attrib->port == mrp->ports &&
+		    !info->ports) {
+			mrp_free_node(node);
+		}
+
+#ifdef CONFIG_KSZ_MSRP
+	} else if (MRP_TYPE_DOMAIN == attrib->type) {
+		struct SRP_domain_class *domain = &cmd->data.domain;
+
+		*domain = mrp->domain[0];
+		*output = SIZEOF_SRP_domain_class;
+	} else if (MRP_TYPE_LISTENER == attrib->type ||
+		   MRP_TYPE_TALKER == attrib->type) {
+		reserv = attrib->attrib;
+		if (SRP_TALKER == reserv->direction) {
+			struct SRP_talker *talker = &cmd->data.talker;
+
+			stream = reserv->stream;
+			memcpy(talker->id, stream->id, 8);
+			memcpy(talker->dest, stream->dest, ETH_ALEN);
+			talker->vlan_id = stream->vlan_id;
+			talker->MaxFrameSize = stream->MaxFrameSize;
+			talker->MaxIntervalFrames = stream->MaxIntervalFrames;
+			talker->priority = stream->priority;
+			talker->rank = stream->rank;
+			talker->reserved = 0;
+			talker->AccumulatedLatency = reserv->latency;
+			memcpy(talker->bridge_id, reserv->bridge_id, 8);
+			talker->FailureCode = reserv->code;
+			*output = SIZEOF_SRP_talker;
+		} else {
+			memcpy(cmd->data.listener.id, reserv->id, 8);
+			cmd->data.listener.substate = reserv->declaration;
+			*output = SIZEOF_SRP_listener;
+		}
+
+		/* Did receive Leave indication. */
+		if (MRP_ACTION_LV == attrib->action) {
+			struct mrp_report *next = attrib->next;
+
+#ifdef DEBUG_MRP_MEM
+dbg_msg(" r %s %p"NL, __func__, reserv);
+#endif
+			kfree(reserv);
+
+			/* Special one to free the stream. */
+			if (next && next->port == sw->port_cnt) {
+				reserv = next->attrib;
+#ifdef DEBUG_MRP_MEM
+dbg_msg(" s %s %p"NL, __func__, reserv->stream);
+dbg_msg(" r2 %s %p"NL, __func__, reserv);
+#endif
+				kfree(reserv->stream);
+				kfree(reserv);
+
+#ifdef DEBUG_MRP_MEM
+dbg_msg(" 1 %s %p"NL, __func__, attrib);
+#endif
+				kfree(attrib);
+				attrib = next;
+			}
+		}
+#endif
+	}
+
+	/* Assume there are more attributes to report. */
+	mrp->report_head = attrib->next;
+	if (mrp->report_tail == attrib) {
+		mrp->report_tail = NULL;
+		result = DEV_IOC_OK;
+	}
+#ifdef DEBUG_MRP_MEM
+dbg_msg(" %s %p"NL, __func__, attrib);
+#endif
+	kfree(attrib);
+	return result;
+}  /* proc_mrp_get_tx */
+
+static int proc_mrp_get_attribute(struct mrp_info *mrp, u8 *data, int *output)
+{
+	struct mrp_cfg_options *cmd = (struct mrp_cfg_options *) data;
+	int result = DEV_IOC_OK;
+
+	switch (cmd->type) {
+	case MRP_TYPE_UNKNOWN:
+		result = proc_mrp_get_tx(mrp, cmd, output);
+		break;
+	case MRP_TYPE_MAC:
+		break;
+	case MRP_TYPE_VLAN:
+		break;
+
+#ifdef CONFIG_KSZ_MSRP
+	case MRP_TYPE_DOMAIN:
+		break;
+	case MRP_TYPE_LISTENER:
+		break;
+	case MRP_TYPE_TALKER:
+		break;
+#endif
+	}
+	return result;
+}  /* proc_mrp_get_attribute */
+#endif
+
+#ifdef CONFIG_KSZ_AVB
+static int mrp_set_bandwidth(struct mrp_port_info *info)
+{
+	int q0;
+	int q1;
+	int tc;
+	u32 bandwidth;
+	struct mrp_traffic_info *traffic;
+	int check = false;
+
+	q0 = info->traffic[0].queue;
+	q1 = info->traffic[1].queue;
+	bandwidth = calculate_max_bandwidth(info->speed,
+		info->bandwidth[q1].deltaBandwidth +
+		info->bandwidth[q0].deltaBandwidth);
+
+	/* The link speed has changed. */
+	if (bandwidth != info->bandwidth_max) {
+		info->bandwidth_max = bandwidth;
+
+		/* Total bandwidth left will be set again if being used. */
+		if (info->bandwidth_used < info->bandwidth_max)
+			info->bandwidth_left = info->bandwidth_max -
+				info->bandwidth_used;
+		else
+			info->bandwidth_left = 0;
+	}
+
+	for (tc = SR_CLASS_A; tc >= SR_CLASS_B; tc--) {
+		traffic = get_traffic_info(info, tc);
+		bandwidth = calculate_max_bandwidth(info->speed,
+			info->bandwidth[traffic->queue].deltaBandwidth);
+		traffic->bandwidth_delta = bandwidth;
+		traffic->bandwidth_max = bandwidth;
+	}
+	for (tc = SR_CLASS_A; tc >= SR_CLASS_B; tc--) {
+		traffic = get_traffic_info(info, tc);
+		if (traffic->bandwidth_used < traffic->bandwidth_max)
+			traffic->bandwidth_left = traffic->bandwidth_max -
+				traffic->bandwidth_used;
+		else
+			/* Bandwidth left will be set after checking. */
+			traffic->bandwidth_left = 0;
+		if (traffic->bandwidth_other)
+			*traffic->bandwidth_other += traffic->bandwidth_left;
+		if (traffic->bandwidth_used)
+			check = true;
+#ifdef CONFIG_KSZ_MSRP
+		if (info->declared.next)
+			check = true;
+#endif
+	}
+if (check)
+dbg_msg("%s %d"NL, __func__, check);
+	return check;
+}  /* mrp_set_bandwidth */
+
+#ifdef CONFIG_KSZ_MRP
+static void mrp_event(struct mrp_info *mrp, uint p, enum mrp_event event);
+#endif
+
+static int proc_mrp_set_speed(struct mrp_info *mrp, u8 port, u32 speed,
+			      bool duplex)
+{
+	struct mrp_port_info *info;
+	int link;
+	int result = DEV_IOC_OK;
+
+	if (speed != 1000 && speed != 100 && speed != 10 && speed != 0)
+		return DEV_IOC_INVALID_CMD;
+	info = mrp_get_port_info(mrp, port);
+	if (speed) {
+		if (speed >= 100 && duplex)
+			info->duplex = 1;
+		else
+			info->duplex = 0;
+	}
+	if (speed && speed != info->speed) {
+		info->speed = speed;
+		if (mrp_set_bandwidth(info)) {
+#ifdef CONFIG_KSZ_MSRP
+			result = update_reserv(mrp, port, info);
+#endif
+		}
+	}
+	link = info->link;
+	info->link = speed != 0;
+if (port < 4)
+dbg_msg("%s %d=%d:%d %d %d"NL, __func__, port, speed, duplex, info->link, link);
+#ifdef CONFIG_KSZ_MSRP
+	if (link != info->link && info->link && mrp->started) {
+		int rc;
+
+		rc = proc_mrp_chk_registered(mrp, port);
+		if (result == DEV_IOC_OK)
+			result = rc;
+#ifdef CONFIG_KSZ_MRP
+		mrp_event(mrp, port, MRP_EVENT_REDECLARE);
+#endif
+	}
+#if 0
+	if (link != info->link && !info->link && mrp->started) {
+		mrp_event(mrp, port, MRP_EVENT_FLUSH);
+	}
+#endif
+#endif
+	return result;
+}  /* proc_mrp_set_speed */
+
+static int proc_mrp_set_delta(struct mrp_info *mrp, u8 port, u32 A, u32 B)
+{
+	struct mrp_port_info *info;
+	int q1;
+	int q2;
+	int result = DEV_IOC_OK;
+
+dbg_msg("%s %d=%d %d"NL, __func__, port, A, B);
+	if (A + B > 95)
+		return DEV_IOC_INVALID_CMD;
+	info = mrp_get_port_info(mrp, port);
+	q1 = get_queue_priority(mrp, SR_CLASS_A);
+	q2 = get_queue_priority(mrp, SR_CLASS_B);
+	info->bandwidth[q1].deltaBandwidth = A;
+	info->bandwidth[q2].deltaBandwidth = B;
+	if (mrp_set_bandwidth(info)) {
+	char bw_str1[20];
+	char bw_str2[20];
+	char bw_str3[20];
+	char bw_str4[20];
+	char bw_str5[20];
+		format_num(bw_str1, info->bandwidth_used);
+		format_num(bw_str2, info->traffic[1].bandwidth_max);
+		format_num(bw_str3, info->traffic[1].bandwidth_used);
+		format_num(bw_str4, info->traffic[0].bandwidth_max);
+		format_num(bw_str5, info->traffic[0].bandwidth_used);
+dbg_msg("bw used: %d %s; %s %s; %s %s"NL, port,
+bw_str1, bw_str2, bw_str3, bw_str4, bw_str5);
+#ifdef CONFIG_KSZ_MSRP
+		result = update_reserv(mrp, port, info);
+#endif
+	}
+	return result;
+}  /* proc_mrp_set_delta */
+#endif
+
+#ifdef CONFIG_KSZ_MRP
+static int proc_mrp_xmit(struct mrp_info *mrp, uint p, struct sk_buff *skb);
+
+#include "mrp.c"
+#include "mmrp.c"
+#include "mvrp.c"
+
+#ifdef CONFIG_KSZ_MSRP
+#include "msrp.c"
+
+static void mrp_get_talker(struct SRP_talker *talker,
+	struct srp_talker_failed *attr)
+{
+	memcpy(talker, attr, sizeof(struct srp_talker_failed));
+	talker->vlan_id = ntohs(talker->vlan_id);
+	talker->MaxFrameSize = ntohs(talker->MaxFrameSize);
+	talker->MaxIntervalFrames = ntohs(talker->MaxIntervalFrames);
+	talker->AccumulatedLatency = ntohl(talker->AccumulatedLatency);
+}  /* mrp_get_talker */
+
+static void mrp_set_talker(struct SRP_talker *talker,
+	struct srp_talker_failed *attr)
+{
+	memcpy(attr, talker, sizeof(struct srp_talker_failed));
+	attr->vlan_id = htons(attr->vlan_id);
+	attr->max_frame_size = htons(attr->max_frame_size);
+	attr->max_interval_frames = htons(attr->max_interval_frames);
+	attr->accumulated_latency = htonl(attr->accumulated_latency);
+}  /* mrp_set_talker */
+
+static void mrp_get_listener(struct SRP_listener *listener,
+	struct srp_listener *attr)
+{
+	memcpy(listener->id, attr->id, 8);
+	listener->substate = attr->substate;
+}  /* mrp_get_listener */
+
+static void mrp_set_listener(struct SRP_listener *listener,
+	struct srp_listener *attr)
+{
+	memcpy(attr->id, listener->id, 8);
+	attr->substate = listener->substate;
+}  /* mrp_set_listener */
+
+static void mrp_get_domain(struct SRP_domain_class *domain,
+	struct srp_domain *attr)
+{
+	domain->id = attr->class_id;
+	domain->priority = attr->class_priority;
+	domain->vlan_id = ntohs(attr->class_vid);
+}  /* mrp_get_domain */
+
+static void mrp_set_domain(struct SRP_domain_class *domain,
+	struct srp_domain *attr)
+{
+	attr->class_id = domain->id;
+	attr->class_priority = domain->priority;
+	attr->class_vid = htons(domain->vlan_id);
+}  /* mrp_set_domain */
+#endif
+
+static void proc_mrp_req_lv(struct mrp_info *mrp, uint port,
+			    struct mrp_application *appl)
+{
+	struct mrp_port *p;
+	struct mrp_applicant *app;
+
+	p = &mrp->mrp_ports[port];
+	app = rcu_dereference(p->applicants[appl->type]);
+
+	mrp_join_timer_exec(app);
+}  /* proc_mrp_req_lv */
+
+static int proc_mrp_join_mac(struct mrp_info *mrp, uint port,
+	struct MRP_mac *mac, u8 new_decl)
+{
+	struct mrp_port *p;
+	struct mrp_applicant *app;
+
+	p = &mrp->mrp_ports[port];
+	app = rcu_dereference(p->applicants[mac_mrp_app.type]);
+
+	mmrp_req_join_mac(app, mac->addr, new_decl);
+	return 0;
+}  /* proc_mrp_join_mac */
+
+static int proc_leave_mac(struct mrp_info *mrp, uint port,
+	struct MRP_mac *mac)
+{
+	struct mrp_port *p;
+	struct mrp_applicant *app;
+
+	p = &mrp->mrp_ports[port];
+	app = rcu_dereference(p->applicants[mac_mrp_app.type]);
+
+	return mmrp_req_leave_mac(app, mac->addr);
+}  /* proc_leave_mac */
+
+static int proc_mrp_join_vlan(struct mrp_info *mrp, uint port,
+	struct MRP_vlan *vlan, u8 new_decl)
+{
+	struct mrp_port *p;
+	struct mrp_applicant *app;
+
+	p = &mrp->mrp_ports[port];
+	app = rcu_dereference(p->applicants[vlan_mrp_app.type]);
+
+	mvrp_req_join(app, vlan->id, new_decl);
+	return 0;
+}  /* proc_mrp_join_vlan */
+
+static int proc_leave_vlan(struct mrp_info *mrp, uint port,
+	struct MRP_vlan *vlan)
+{
+	struct mrp_port *p;
+	struct mrp_applicant *app;
+
+	p = &mrp->mrp_ports[port];
+	app = rcu_dereference(p->applicants[vlan_mrp_app.type]);
+
+	return mvrp_req_leave(app, vlan->id);
+}  /* proc_leave_vlan */
+
+#ifdef CONFIG_KSZ_MSRP
+static int proc_mrp_new_talker(struct mrp_info *mrp, uint port,
+	struct SRP_talker *talker, u8 new_decl)
+{
+	struct srp_talker_failed attr;
+	struct mrp_port *p;
+	struct mrp_applicant *app;
+
+	p = &mrp->mrp_ports[port];
+	app = rcu_dereference(p->applicants[srp_mrp_app.type]);
+
+	/* MSRP may be disabled in this port. */
+	if (!app)
+		return 0;
+
+	mrp_set_talker(talker, &attr);
+	msrp_req_new_talker(app, &attr, new_decl);
+	return 0;
+}  /* proc_mrp_new_talker */
+
+static int proc_leave_talker(struct mrp_info *mrp, uint port,
+	struct SRP_talker *talker)
+{
+	struct srp_talker_failed attr;
+	struct mrp_port *p;
+	struct mrp_applicant *app;
+
+	p = &mrp->mrp_ports[port];
+	app = rcu_dereference(p->applicants[srp_mrp_app.type]);
+
+	mrp_set_talker(talker, &attr);
+	return msrp_req_leave_talker(app, &attr);
+}  /* proc_leave_talker */
+
+static int proc_mrp_new_listener(struct mrp_info *mrp, uint port,
+	struct SRP_listener *listener, u8 new_decl)
+{
+	struct srp_listener attr;
+	struct mrp_port *p;
+	struct mrp_applicant *app;
+
+	p = &mrp->mrp_ports[port];
+	app = rcu_dereference(p->applicants[srp_mrp_app.type]);
+
+	/* MSRP may be disabled in this port. */
+	if (!app)
+		return 0;
+
+	mrp_set_listener(listener, &attr);
+	msrp_req_new_listener(app, &attr, new_decl);
+	return 0;
+}  /* proc_mrp_new_listener */
+
+static int proc_leave_listener(struct mrp_info *mrp, uint port,
+	struct SRP_listener *listener)
+{
+	struct srp_listener attr;
+	struct mrp_port *p;
+	struct mrp_applicant *app;
+
+	p = &mrp->mrp_ports[port];
+	app = rcu_dereference(p->applicants[srp_mrp_app.type]);
+
+	mrp_set_listener(listener, &attr);
+	return msrp_req_leave_listener(app, &attr);
+}  /* proc_leave_listener */
+
+static int proc_mrp_join_domain(struct mrp_info *mrp, uint port,
+	struct SRP_domain_class *domain, u8 new_decl)
+{
+	struct srp_domain attr;
+	struct mrp_port *p;
+	struct mrp_applicant *app;
+
+	p = &mrp->mrp_ports[port];
+	app = rcu_dereference(p->applicants[srp_mrp_app.type]);
+
+	/* MSRP may be disabled in this port. */
+	if (!app)
+		return 0;
+
+	mrp_set_domain(domain, &attr);
+	msrp_req_join_domain(app, &attr, new_decl);
+	return 0;
+}  /* proc_mrp_join_domain */
+
+static int proc_leave_domain(struct mrp_info *mrp, uint port,
+	struct SRP_domain_class *domain)
+{
+	struct srp_domain attr;
+	struct mrp_port *p;
+	struct mrp_applicant *app;
+
+	p = &mrp->mrp_ports[port];
+	app = rcu_dereference(p->applicants[srp_mrp_app.type]);
+
+	mrp_set_domain(domain, &attr);
+	return msrp_req_leave_domain(app, &attr);
+}  /* proc_leave_domain */
+#endif
+#endif
+
+static int proc_mrp_set_attribute(struct mrp_info *mrp, u8 *data)
+{
+	struct mrp_cfg_options *cmd = (struct mrp_cfg_options *) data;
+	struct ksz_sw *sw = mrp->parent;
+	int result = DEV_IOC_OK;
+#ifdef CONFIG_KSZ_MRP
+	int tx_leave = false;
+#endif
+	uint n = cmd->port;
+	uint p;
+
+	p = get_phy_port(sw, n);
+	switch (cmd->type) {
+#ifdef CONFIG_KSZ_MRP
+	case MRP_TYPE_MAC:
+		if (MRP_ACTION_RX == cmd->action)
+			result = proc_mrp_rx_mac(mrp, p, &cmd->data.mac,
+				cmd->new_decl);
+		else if (MRP_ACTION_LV == cmd->action)
+			result = proc_mrp_lv_mac(mrp, p, &cmd->data.mac);
+		else if (MRP_ACTION_DECL == cmd->action)
+			result = proc_mrp_join_mac(mrp, p, &cmd->data.mac,
+				cmd->new_decl);
+		else if (MRP_ACTION_DROP == cmd->action)
+			tx_leave = proc_leave_mac(mrp, p, &cmd->data.mac);
+		else if (MRP_ACTION_ON == cmd->action) {
+			mrp->no_report = 1;
+			result = proc_mrp_rx_mac(mrp, p, &cmd->data.mac,
+				cmd->new_decl);
+			mrp->no_report = 0;
+		} else if (MRP_ACTION_OFF == cmd->action) {
+			mrp->no_report = 1;
+			result = proc_mrp_lv_mac(mrp, p, &cmd->data.mac);
+			mrp->no_report = 0;
+		} else
+			mrp_show_node(&mrp->mac_list, show_mac_info);
+		if (tx_leave)
+			proc_mrp_req_lv(mrp, p, &mac_mrp_app);
+		break;
+	case MRP_TYPE_VLAN:
+		if (MRP_ACTION_RX == cmd->action)
+			result = proc_mrp_rx_vlan(mrp, p, &cmd->data.vlan,
+				cmd->new_decl);
+		else if (MRP_ACTION_LV == cmd->action)
+			result = proc_mrp_lv_vlan(mrp, p, &cmd->data.vlan);
+		else if (MRP_ACTION_DECL == cmd->action)
+			result = proc_mrp_join_vlan(mrp, p,
+				&cmd->data.vlan, cmd->new_decl);
+		else if (MRP_ACTION_DROP == cmd->action)
+			tx_leave = proc_leave_vlan(mrp, p, &cmd->data.vlan);
+		else if (MRP_ACTION_ON == cmd->action) {
+			mrp->no_report = 1;
+			result = proc_mrp_rx_vlan(mrp, p, &cmd->data.vlan,
+				cmd->new_decl);
+			mrp->no_report = 0;
+		} else if (MRP_ACTION_OFF == cmd->action) {
+			mrp->no_report = 1;
+			result = proc_mrp_lv_vlan(mrp, p, &cmd->data.vlan);
+			mrp->no_report = 0;
+		} else
+			mrp_show_node(&mrp->vlan_list, show_vlan_info);
+		if (tx_leave)
+			proc_mrp_req_lv(mrp, p, &vlan_mrp_app);
+		break;
+
+#ifdef CONFIG_KSZ_MSRP
+	case MRP_TYPE_DOMAIN:
+		if (MRP_ACTION_RX == cmd->action)
+			result = proc_mrp_rx_domain(mrp, p,
+				&cmd->data.domain);
+		else if (MRP_ACTION_LV == cmd->action)
+			result = proc_mrp_lv_domain(mrp, p,
+				&cmd->data.domain);
+		else if (MRP_ACTION_DECL == cmd->action)
+			result = proc_mrp_join_domain(mrp, p,
+				&cmd->data.domain, cmd->new_decl);
+		else if (MRP_ACTION_DROP == cmd->action)
+			tx_leave = proc_leave_domain(mrp, p, &cmd->data.domain);
+		else if (MRP_ACTION_ON == cmd->action) {
+			mrp->no_report = 1;
+			result = proc_mrp_rx_domain(mrp, p,
+				&cmd->data.domain);
+			mrp->no_report = 0;
+		} else if (MRP_ACTION_OFF == cmd->action) {
+			mrp->no_report = 1;
+			result = proc_mrp_lv_domain(mrp, p,
+				&cmd->data.domain);
+			mrp->no_report = 0;
+		}
+		if (tx_leave)
+			proc_mrp_req_lv(mrp, p, &srp_mrp_app);
+		break;
+	case MRP_TYPE_LISTENER:
+		if (MRP_ACTION_RX == cmd->action)
+			result = proc_mrp_rx_listener(mrp, p,
+				&cmd->data.listener, cmd->new_decl);
+		else if (MRP_ACTION_LV == cmd->action)
+			result = proc_mrp_lv_listener(mrp, p,
+				&cmd->data.listener);
+		else if (MRP_ACTION_DECL == cmd->action)
+			result = proc_mrp_new_listener(mrp, p,
+				&cmd->data.listener, cmd->new_decl);
+		else if (MRP_ACTION_DROP == cmd->action)
+			tx_leave = proc_leave_listener(mrp, p,
+				&cmd->data.listener);
+		else if (MRP_ACTION_ON == cmd->action) {
+			mrp->no_report = 1;
+			result = proc_mrp_rx_listener(mrp, p,
+				&cmd->data.listener, cmd->new_decl);
+			mrp->no_report = 0;
+		} else if (MRP_ACTION_OFF == cmd->action) {
+			mrp->no_report = 1;
+			result = proc_mrp_lv_listener(mrp, p,
+				&cmd->data.listener);
+			mrp->no_report = 0;
+		}
+		if (tx_leave)
+			proc_mrp_req_lv(mrp, p, &srp_mrp_app);
+		break;
+	case MRP_TYPE_TALKER:
+		if (MRP_ACTION_RX == cmd->action)
+			result = proc_mrp_rx_talker(mrp, p,
+				&cmd->data.talker, cmd->new_decl);
+		else if (MRP_ACTION_LV == cmd->action)
+			result = proc_mrp_lv_talker(mrp, p,
+				&cmd->data.talker);
+		else if (MRP_ACTION_DECL == cmd->action)
+			result = proc_mrp_new_talker(mrp, p,
+				&cmd->data.talker, cmd->new_decl);
+		else if (MRP_ACTION_DROP == cmd->action)
+			tx_leave = proc_leave_talker(mrp, p, &cmd->data.talker);
+		else if (MRP_ACTION_ON == cmd->action) {
+			mrp->no_report = 1;
+			result = proc_mrp_rx_talker(mrp, p,
+				&cmd->data.talker, cmd->new_decl);
+			mrp->no_report = 0;
+		} else if (MRP_ACTION_OFF == cmd->action) {
+			mrp->no_report = 1;
+			result = proc_mrp_lv_talker(mrp, p,
+				&cmd->data.talker);
+			mrp->no_report = 0;
+		} else {
+			int n;
+			struct mrp_port_info *info;
+
+			for (n = 0; n <= mrp->ports; n++) {
+				info = mrp_get_port_info(mrp, n);
+				if (!info->status.msrpPortEnabledStatus)
+					continue;
+				chk_reserv(info, n);
+			}
+		}
+		if (tx_leave)
+			proc_mrp_req_lv(mrp, p, &srp_mrp_app);
+		break;
+#endif
+#endif
+	case MRP_TYPE_PORT:
+
+#ifdef CONFIG_KSZ_AVB
+		if (MRP_ACTION_SPEED == cmd->action) {
+			int speed = cmd->data.data[0];
+			bool duplex = cmd->data.data[1];
+
+			result = proc_mrp_set_speed(mrp, p, speed, duplex);
+		} else if (MRP_ACTION_DELTA == cmd->action) {
+			u32 a = cmd->data.data[0];
+			u32 b = cmd->data.data[1];
+
+			result = proc_mrp_set_delta(mrp, p, a, b);
+		}
+#endif
+#ifdef CONFIG_KSZ_MSRP
+		if (MRP_ACTION_CHK_TALKER == cmd->action) {
+			result = proc_mrp_chk_talker(mrp, p, 0);
+		} else if (MRP_ACTION_CHK_REG == cmd->action) {
+			result = proc_mrp_chk_registered(mrp, p);
+		}
+#endif
+		break;
+	}
+	return result;
+}  /* proc_mrp_set_attribute */
+
+#ifdef CONFIG_KSZ_MSRP
+static void mrp_cfg_mcast_addr(struct mrp_info *mrp, u16 fid, u8 *dest,
+			       u16 ports)
+{
+	struct mrp_node *mac_node;
+	struct ksz_sw *sw = mrp->parent;
+
+	sw->ops->cfg_mac(sw, 1, dest, ports, true, fid != 0, fid);
+	mac_node = mrp_get_mac_info(&mrp->mac_list, dest, fid);
+	if (mac_node) {
+		struct mrp_mac_info *mac;
+
+		mac = mac_node->data;
+
+		/* Indicate address is programmed in the lookup table. */
+		mac->srp_ports |= SRP_PORT_SET;
+	}
+}  /* mrp_cfg_mcast_addr */
+
+static void mrp_cfg_mac_work(struct work_struct *work)
+{
+	u16 *fid;
+	u16 *ports;
+	u8 addr[ETH_ALEN];
+	bool last;
+	struct sk_buff *skb;
+	struct mrp_info *mrp = container_of(work, struct mrp_info, cfg_mac);
+
+#ifdef CONFIG_KSZ_IBA
+	if (iba_stopped(mrp->parent))
+		return;
+#endif
+
+	last = skb_queue_empty(&mrp->macq);
+	while (!last) {
+		skb = skb_dequeue(&mrp->macq);
+		last = skb_queue_empty(&mrp->macq);
+		if (!skb)
+			continue;
+		memcpy(addr, skb->data, ETH_ALEN);
+		fid = (u16 *)&skb->data[ETH_ALEN];
+		ports = (u16 *)&skb->data[ETH_ALEN + 2];
+		mrp_cfg_mcast_addr(mrp, *fid, addr, *ports);
+		kfree_skb(skb);
+	}
+	mrp->macq_sched = 0;
+}  /* mrp_cfg_mac_work */
+
+static int mrp_req_cfg_mac(struct mrp_info *mrp, u8 *addr, u16 fid, u16 ports)
+{
+	struct sk_buff *skb;
+	u16 *data;
+
+	skb = alloc_skb(64, GFP_ATOMIC);
+	if (!skb)
+		return -ENOMEM;
+
+	memcpy(skb->data, addr, ETH_ALEN);
+	data = (u16 *)&skb->data[ETH_ALEN];
+	*data = fid;
+	data++;
+	*data = ports;
+	skb_queue_tail(&mrp->macq, skb);
+	if (!mrp->macq_sched) {
+		mrp->macq_sched = 1;
+		schedule_work(&mrp->cfg_mac);
+	}
+	return 0;
+}  /* mrp_req_cfg_mac */
+#endif
+
+#ifdef CONFIG_KSZ_MRP
+static void mrp_rx_proc(struct work_struct *work)
+{
+	bool last;
+	struct sk_buff *skb;
+	struct mrp_applicant **data;
+	struct mrp_applicant *app;
+	struct mrp_info *mrp = container_of(work, struct mrp_info, rx_proc);
+
+#ifdef CONFIG_KSZ_IBA
+	if (iba_stopped(mrp->parent))
+		return;
+#endif
+
+	last = skb_queue_empty(&mrp->rxq);
+	while (!last) {
+		skb = skb_dequeue(&mrp->rxq);
+		last = skb_queue_empty(&mrp->rxq);
+		if (!skb)
+			continue;
+		data = (struct mrp_applicant **)skb->cb;
+		app = *data;
+		app->rxpdu(app, &skb->data[sizeof(struct ethhdr)],
+			skb->len - sizeof(struct ethhdr));
+		app->cleanup(app);
+		kfree_skb(skb);
+	}
+}  /* mrp_rx_proc */
+
+static int proc_mrp_xmit(struct mrp_info *mrp, uint p, struct sk_buff *skb)
+{
+	int rc;
+	u16 tx_ports;
+	u32 ports;
+	struct ksz_sw *sw = mrp->parent;
+	const struct net_device_ops *ops = sw->main_dev->netdev_ops;
+	int result = DEV_IOC_OK;
+
+	/* Send to host port by simulating receiving. */
+	if (p == sw->HOST_PORT) {
+		skb->data[11] ^= 0x01;
+		skb->protocol = eth_type_trans(skb, skb->dev);
+		netif_rx_ni(skb);
+		return 0;
+	}
+
+	/* Do not send if network device is not ready. */
+	if (!netif_running(sw->main_dev)) {
+		kfree_skb(skb);
+		return 0;
+	}
+
+	tx_ports = sw->tx_ports[0] & mrp->tx_ports;
+#if 1
+if (!(tx_ports & (1 << p)) && sw->tx_ports[0] != mrp->tx_ports) {
+dbg_msg("  tx close: %d %x %x"NL, p, sw->tx_ports[0], mrp->tx_ports);
+}
+#endif
+
+	/* Do not send to port if its link is lost. */
+	if (media_disconnected == sw->port_state[p].state ||
+	    !(tx_ports & (1 << p))) {
+		kfree_skb(skb);
+		return 0;
+	}
+
+	if (skb->len < 60) {
+		int len = 60 - skb->len;
+		memset(&skb->data[skb->len], 0, len);
+		skb_put(skb, len);
+	}
+	ports = 1 << p;
+	ports |= TAIL_TAG_SET_QUEUE;
+	sw->net_ops->add_tail_tag(sw, skb, ports);
+	do {
+		rc = ops->ndo_start_xmit(skb, skb->dev);
+		if (NETDEV_TX_BUSY == rc) {
+			rc = wait_event_interruptible_timeout(sw->queue,
+				!netif_queue_stopped(sw->main_dev),
+				50 * HZ / 1000);
+
+			rc = NETDEV_TX_BUSY;
+		}
+	} while (NETDEV_TX_BUSY == rc);
+	return result;
+}  /* proc_mrp_mrp_xmit */
+#endif
+
+static void proc_mrp_cmd(struct mrp_info *mrp, struct mrp_work *parent)
+{
+#ifdef CONFIG_KSZ_MRP
+	u8 *data = parent->param.data;
+#endif
+	int result = DEV_IOC_OK;
+
+	parent->output = parent->option;
+	switch (parent->cmd) {
+	case DEV_CMD_INFO:
+		switch (parent->subcmd) {
+		case DEV_INFO_INIT:
+			break;
+		case DEV_INFO_EXIT:
+			break;
+		default:
+			result = DEV_IOC_INVALID_CMD;
+			break;
+		}
+		break;
+	case DEV_CMD_PUT:
+		switch (parent->subcmd) {
+		case DEV_MRP_ATTRIBUTE:
+#ifdef CONFIG_KSZ_MRP
+			result = proc_mrp_set_attribute(mrp, data);
+#endif
+			break;
+		default:
+			result = DEV_IOC_INVALID_CMD;
+			break;
+		}
+		break;
+	case DEV_CMD_GET:
+		switch (parent->subcmd) {
+		case DEV_MRP_ATTRIBUTE:
+#ifdef CONFIG_KSZ_MRP
+			result = proc_mrp_get_attribute(mrp, data,
+							&parent->output);
+#endif
+			break;
+		}
+		break;
+	default:
+		result = DEV_IOC_INVALID_CMD;
+		break;
+	}
+	parent->result = result;
+	parent->used = false;
+	complete(&parent->done);
+}  /* proc_mrp_cmd */
+
+static void proc_mrp_work(struct work_struct *work)
+{
+	struct mrp_access *info =
+		container_of(work, struct mrp_access, work);
+	struct mrp_info *mrp =
+		container_of(info, struct mrp_info, hw_access);
+	struct mrp_work *cmd;
+
+#ifdef CONFIG_KSZ_IBA
+	if (iba_stopped(mrp->parent))
+		return;
+#endif
+
+	cmd = &info->works[info->head];
+	while (cmd->used) {
+		proc_mrp_cmd(mrp, cmd);
+		info->head++;
+		info->head &= MRP_WORK_LAST;
+		cmd = &info->works[info->head];
+	}
+}  /* proc_mrp_work */
+
+static int proc_mrp_hw_access(struct mrp_info *mrp, int cmd, int subcmd,
+	int option, void *data, size_t len, int *output, struct sk_buff *skb,
+	int wait)
+{
+	struct mrp_access *access;
+	struct mrp_work *work;
+	int ret = 0;
+
+	access = &mrp->hw_access;
+	work = &access->works[access->tail];
+	if (work->used) {
+		pr_alert("work full"NL);
+		return -EFAULT;
+	}
+	work->skb = skb;
+	work->cmd = cmd;
+	work->subcmd = subcmd;
+	work->option = option;
+	memcpy(work->param.data, data, len);
+	work->used = true;
+	access->tail++;
+	access->tail &= MRP_WORK_LAST;
+	init_completion(&work->done);
+	schedule_work(&access->work);
+	if (!wait)
+		goto hw_access_end;
+	wait_for_completion(&work->done);
+
+	ret = work->result;
+	*output = work->output;
+	if (DEV_CMD_GET == work->cmd) {
+		int rc = ret;
+
+		if (DEV_IOC_MRP_REPORT == rc) {
+			rc = DEV_IOC_OK;
+			len = *output;
+		}
+		if (DEV_IOC_OK == rc)
+			memcpy(data, work->param.data, len);
+	}
+
+hw_access_end:
+	return ret;
+}  /* proc_mrp_hw_access */
+
+static void exit_mrp_work(struct mrp_info *mrp)
+{
+	struct mrp_access *access;
+	struct mrp_work *work;
+	int i;
+
+	access = &mrp->hw_access;
+	for (i = 0; i < MRP_WORK_NUM; i++) {
+		work = &access->works[i];
+		flush_work(&work->work);
+	}
+	flush_work(&access->work);
+}  /* exit_mrp_work */
+
+static void init_mrp_work(struct mrp_info *mrp)
+{
+	struct mrp_access *access;
+	struct mrp_work *work;
+	int i;
+
+	access = &mrp->hw_access;
+	for (i = 0; i < MRP_WORK_NUM; i++) {
+		work = &access->works[i];
+		work->mrp = mrp;
+		work->index = i;
+		work->prev = &access->works[(i - 1) & MRP_WORK_LAST];
+		INIT_WORK(&work->work, proc_mrp_work);
+		init_completion(&work->done);
+	}
+	access->head = access->tail = 0;
+	INIT_WORK(&access->work, proc_mrp_work);
+}  /* init_mrp_work */
+
+#ifdef CONFIG_KSZ_MSRP
+static void mrp_proc_proto(struct mrp_info *mrp, u8 *addr, u16 port)
+{
+#if 0
+	struct maap_pdu *maap;
+	u16 data;
+
+	maap = (struct maap_pdu *)&addr[14];
+	if (maap->subtype == AVTP_SUBTYPE_MAAP) {
+		data = ntohs(maap->len.data);
+dbg_msg(" maap: %u=m:%x v:%d %u %u l:%u"NL
+"%02x:%02x:%02x:%02x:%02x:%02x:%02x:%02x %02x:%02x:%02x:%02x:%02x:%02x %u"NL,
+		port,
+		maap->message_type, maap->version, maap->sv,
+		data >> 11, data & 0x7f,
+		maap->stream_id[0], maap->stream_id[1],
+		maap->stream_id[2], maap->stream_id[3],
+		maap->stream_id[4], maap->stream_id[5],
+		maap->stream_id[6], maap->stream_id[7],
+		maap->req_start_addr[0],
+		maap->req_start_addr[1],
+		maap->req_start_addr[2],
+		maap->req_start_addr[3],
+		maap->req_start_addr[4],
+		maap->req_start_addr[5],
+		ntohs(maap->req_cnt));
+	} else if (maap->subtype == AVTP_SUBTYPE_ADP) {
+#if 0
+dbg_msg(" adp: %u="NL, port);
+#endif
+	}
+#endif
+}  /* mrp_proc_proto */
+
+static int mrp_chk_mcast(struct mrp_info *mrp, u8 *addr, u16 vid, u16 prio,
+			 u16 proto, uint port)
+{
+	struct mrp_node *mac_node;
+	struct mrp_mac_info *mac;
+	struct ksz_port_cfg *cfg;
+	struct maap_pdu *maap;
+#if 0
+	struct sk_buff *skb;
+	u16 *data;
+#endif
+	u16 ports;
+	u16 fid = 0;
+	struct vlan_ethhdr *vlan = (struct vlan_ethhdr *) addr;
+	struct ksz_sw *sw = mrp->parent;
+	int avb_a;
+	int avb_b;
+	int avb_0 = 0;
+	int *avb = &avb_0;
+	int ignore;
+	int result;
+
+	cfg = &sw->info->port_cfg[port];
+	avb_a = cfg->avb_a;
+	avb_b = cfg->avb_b;
+
+	maap = (struct maap_pdu *)&addr[14];
+	if (proto == 0x22F0) {
+		if (maap->subtype == AVTP_SUBTYPE_MAAP) {
+			if (memcmp(vlan->h_dest, maap_addr[0], ETH_ALEN))
+dbg_msg("  %02x:%02x:%02x:%02x:%02x:%02x  ",
+	vlan->h_dest[0], vlan->h_dest[1], vlan->h_dest[2],
+	vlan->h_dest[3], vlan->h_dest[4], vlan->h_dest[5]);
+dbg_msg(" maap: %d"NL, maap->message_type);
+		}
+		if (maap->subtype == AVTP_SUBTYPE_MAAP ||
+		    !memcmp(vlan->h_dest, maap_addr[0], ETH_ALEN))
+			return 1;
+	}
+
+	if (vlan->h_vlan_proto == htons(ETH_P_8021Q))
+		maap = (struct maap_pdu *)(vlan + 1);
+
+	/* Priority is already remapped to 0 if port is not AVB. */
+
+	/* SR Class A supported */
+	if (prio == mrp->prio[SR_CLASS_A] && mrp->domain[1].id)
+		avb = &avb_a;
+	if (prio == mrp->prio[SR_CLASS_B])
+		avb = &avb_b;
+
+#if 0
+	/* XMOS sends this MAAP periodically. */
+	if (vlan->h_vlan_proto != htons(ETH_P_8021Q)) {
+		if (!(0x01 <= addr[15] && addr[15] <= 0x03))
+dbg_msg(" MAAP? %02X:%02X:%02X:%02X:%02X:%02X %02x"NL,
+addr[0], addr[1], addr[2], addr[3], addr[4], addr[5], addr[15]);
+		if (0x01 <= addr[15] && addr[15] <= 0x03)
+			return 1;
+	}
+#endif
+	if (vid > 1) {
+#if 0
+		ports = mrp_find_vlan_ports(&mrp->vlan_list, vid, NULL,
+			&fid);
+		if (!ports)
+#endif
+			fid = sw->info->vid2fid[vid];
+	}
+
+	/* Check if the multicast address is registered. */
+	mac_node = mrp_get_mac_info(&mrp->mac_list, addr, fid);
+	if (!mac_node)
+		return -ENOMEM;
+
+	mac = mac_node->data;
+	ports = mac->srp_ports | mac->mrp_ports;
+
+	/* Address is used by MRP and already programmed into lookup table. */
+	if (ports & SRP_PORT_READY)
+		return 2;
+
+#if 1
+	if (!mac->ticks) {
+dbg_msg(" drop: %d=%d=%02X:%02X:%02X:%02X:%02X:%02X  %02X"NL, port, prio,
+addr[0], addr[1], addr[2], addr[3], addr[4], addr[5], maap->subtype);
+	}
+#endif
+	mac->ticks = jiffies + msecs_to_jiffies(10000);
+
+	ignore = (ports & SRP_PORT_BLACKLIST) &&
+		!(mac->rx_ports & SRP_PORT_IGNORE);
+
+	/* Address is programmed into lookup table. */
+	if (ports & SRP_PORT_SET)
+		result = 0;
+	else
+		result = !*avb;
+	mac->rx_ports &= ~SRP_PORT_IGNORE;
+
+	/* Address is already seen before. */
+	if (ignore)
+		return result;
+
+	mac->rx_ports |= (1 << port);
+	mac->srp_ports |= SRP_PORT_BLACKLIST;
+	if (!*avb)
+		ports = sw->PORT_MASK & ~sw->HOST_MASK;
+	else
+		ports = 0;
+
+	return mrp_req_cfg_mac(mrp, addr, fid, ports);
+}  /* mrp_chk_mcast */
+#endif
+
+static int mrp_dev_req(struct mrp_info *mrp, char *arg)
+{
+	struct ksz_request *req = (struct ksz_request *) arg;
+	int len;
+	int maincmd;
+	int req_size;
+	int subcmd;
+	int output;
+	u8 data[PARAM_DATA_SIZE];
+	u8 cmd_data[4];
+	int err = 0;
+	int result = 0;
+	struct mrp_cfg_options *cmd = (struct mrp_cfg_options *) cmd_data;
+	size_t param_size = 0;
+
+	/* Assume success. */
+	result = DEV_IOC_OK;
+
+	/* Check request size. */
+	__get_user(req_size, &req->size);
+	if (chk_ioctl_size(req_size, SIZEOF_ksz_request, 0, &req_size,
+			&result, NULL, NULL))
+		goto dev_ioctl_resp;
+
+	err = 0;
+	__get_user(maincmd, &req->cmd);
+	__get_user(subcmd, &req->subcmd);
+	__get_user(output, &req->output);
+	len = req_size - SIZEOF_ksz_request;
+
+	if (DEV_MRP_ATTRIBUTE == subcmd) {
+		if (chk_ioctl_size(len, 4, SIZEOF_ksz_request, &req_size,
+				&result, &req->param, cmd_data))
+			goto dev_ioctl_resp;
+
+		switch (cmd->type) {
+		case MRP_TYPE_PORT:
+			param_size = 4;
+			if (maincmd == DEV_CMD_PUT)
+				param_size += sizeof(u32) * 4;
+			break;
+		case MRP_TYPE_MAC:
+			param_size = SIZEOF_MRP_mac;
+			break;
+		case MRP_TYPE_VLAN:
+			param_size = SIZEOF_MRP_vlan;
+			break;
+
+#ifdef CONFIG_KSZ_MSRP
+		case MRP_TYPE_DOMAIN:
+			param_size = SIZEOF_SRP_domain_class;
+			break;
+		case MRP_TYPE_LISTENER:
+			param_size = SIZEOF_SRP_listener;
+			break;
+		case MRP_TYPE_TALKER:
+		default:
+			param_size = SIZEOF_SRP_talker;
+			break;
+#endif
+		}
+	}
+
+	switch (maincmd) {
+	case DEV_CMD_INFO:
+		switch (subcmd) {
+		case DEV_INFO_INIT:
+			req_size = SIZEOF_ksz_request + 4;
+			if (len >= 4) {
+				data[0] = 'M';
+				data[1] = 'i';
+				data[2] = 'c';
+				data[3] = 'r';
+				data[4] = mrp->version;
+				data[5] = mrp->ports;
+				if (!access_ok(req->param.data,
+						6) ||
+						copy_to_user(req->param.data,
+						data, 6)) {
+					err = -EFAULT;
+					goto dev_ioctl_done;
+				}
+				result = proc_mrp_hw_access(mrp,
+					maincmd, subcmd, 0,
+					data, 6, &output, NULL, true);
+			} else
+				result = DEV_IOC_INVALID_LEN;
+			break;
+		case DEV_INFO_EXIT:
+			result = proc_mrp_hw_access(mrp,
+				maincmd, subcmd, 0,
+				data, 0, &output, NULL, true);
+			fallthrough;
+
+		case DEV_INFO_QUIT:
+			break;
+		default:
+			result = DEV_IOC_INVALID_CMD;
+			break;
+		}
+		break;
+	case DEV_CMD_PUT:
+		switch (subcmd) {
+		case DEV_MRP_ATTRIBUTE:
+			if (chk_ioctl_size(len, param_size,
+					SIZEOF_ksz_request,
+					&req_size, &result, &req->param, data))
+				goto dev_ioctl_resp;
+			result = proc_mrp_hw_access(mrp,
+				maincmd, subcmd, 0,
+				data, len, &output, NULL, true);
+			break;
+		default:
+			result = DEV_IOC_INVALID_CMD;
+			break;
+		}
+		break;
+	case DEV_CMD_GET:
+		switch (subcmd) {
+		case DEV_MRP_ATTRIBUTE:
+			if (chk_ioctl_size(len, param_size,
+					SIZEOF_ksz_request,
+					&req_size, &result, &req->param, data))
+				goto dev_ioctl_resp;
+
+#ifdef CONFIG_KSZ_MSRP
+			if (MRP_ACTION_SPEED == cmd->action) {
+				uint port;
+				struct ksz_sw *sw = mrp->parent;
+
+				port = get_phy_port(sw, cmd->port);
+				__put_user(mrp->port_info[port].speed,
+					&req->output);
+				break;
+			}
+#endif
+			result = proc_mrp_hw_access(mrp,
+				maincmd, subcmd, 0,
+				data, len, &output, NULL, true);
+			if (!access_ok(req->param.data,
+					param_size) ||
+					copy_to_user(req->param.data, data,
+					param_size)) {
+				err = -EFAULT;
+				goto dev_ioctl_done;
+			}
+			break;
+		}
+		break;
+	default:
+		result = DEV_IOC_INVALID_CMD;
+		break;
+	}
+
+dev_ioctl_resp:
+	__put_user(req_size, &req->size);
+	__put_user(result, &req->result);
+
+	/* Return ERESTARTSYS so that the system call is called again. */
+	if (result < 0)
+		err = result;
+
+dev_ioctl_done:
+	return err;
+}  /* mrp_dev_req */
+
+#ifdef CONFIG_KSZ_MRP
+static void mrp_change_attr(struct mrp_applicant *app, void *value, u8 len,
+			    u8 type, enum mrp_event event)
+{
+	struct mrp_attr *attr;
+
+	attr = mrp_attr_lookup(app, value, len, type);
+	if (attr) {
+		u8 action;
+
+dbg_msg(" attr ");
+		/* Check the attribute is being declared. */
+		action = mrp_tx_action_table[attr->state];
+		if (action != MRP_TX_ACTION_S_JOIN_IN &&
+		    action != MRP_TX_ACTION_S_JOIN_IN_OPTIONAL)
+			action = mrp_tx_la_action_table[attr->state];
+
+		/* Change Join to New. */
+		if (action == MRP_TX_ACTION_S_JOIN_IN ||
+		    action == MRP_TX_ACTION_S_JOIN_IN_OPTIONAL)
+{
+dbg_msg(" event"NL);
+			if (mrp_attr_event(app, attr, event))
+				mrp_join_timer_arm(app);
+}
+	}
+}
+
+static void mrp_update_event(struct mrp_applicant *app,
+			     struct mrp_applicant *to, enum mrp_event event)
+{
+	struct rb_node *node, *next;
+	struct mrp_attr *attr;
+
+	for (node = rb_first(&app->mad);
+	     next = node ? rb_next(node) : NULL, node != NULL;
+	     node = next) {
+		attr = rb_entry(node, struct mrp_attr, node);
+
+		if (attr->reg_state != MRP_REGISTRAR_IN)
+			continue;
+dbg_msg(" in %d:%02x%02x %d %d -> %d"NL, app->port,
+attr->value[0], attr->value[1], attr->len, attr->type, to->port);
+		mrp_change_attr(to, attr->value, attr->len, attr->type, event);
+	}
+}
+
+static void mrp_event_port_app(struct mrp_port *port,
+	struct mrp_application *appl, enum mrp_event event)
+{
+	struct mrp_applicant *app;
+
+	app = rcu_dereference(port->applicants[appl->type]);
+	mrp_mad_event(app, event);
+	if (event == MRP_EVENT_LV && app->timer_arm.join)
+		mrp_join_timer_exec(app);
+}  /* mrp_event_port_app */
+
+static void mrp_new_event_port_app(struct mrp_info *mrp, uint p,
+				struct mrp_application *appl)
+{
+	struct mrp_applicant *app;
+	struct mrp_applicant *to;
+	int n;
+
+	app = rcu_dereference(mrp->mrp_ports[p].applicants[appl->type]);
+	for (n = 0; n <= mrp->ports; n++) {
+		if (n == p)
+			continue;
+		to = rcu_dereference(mrp->mrp_ports[n].applicants[appl->type]);
+		mrp_update_event(app, to, MRP_EVENT_NEW);
+	}
+}
+
+static void mrp_event(struct mrp_info *mrp, uint p, enum mrp_event event)
+{
+	uint m = BIT(p);
+
+	if (!(mrp->mask & m))
+		return;
+	if (mrp->mmrp_rx_ports & m) {
+		mrp_event_port_app(&mrp->mrp_ports[p], &mac_mrp_app, event);
+	}
+	if (mrp->mvrp_rx_ports & m) {
+		mrp_event_port_app(&mrp->mrp_ports[p], &vlan_mrp_app, event);
+	}
+
+#ifdef CONFIG_KSZ_MSRP
+	if (mrp->status.msrpEnabledStatus &&
+	    mrp->port_info[p].status.msrpPortEnabledStatus) {
+		mrp_event_port_app(&mrp->mrp_ports[p], &srp_mrp_app, event);
+	}
+#endif
+}  /* mrp_event */
+
+static void mrp_setup_vlan(struct mrp_info *mrp, u16 vid,
+			   struct ksz_vlan_table *vlan)
+{
+	struct mrp_node *node;
+	struct mrp_vlan_info *info;
+	struct mrp_vlan_info data;
+	struct MRP_vlan mrp_vlan;
+	int cnt;
+	uint n;
+	uint p;
+	uint q;
+	struct ksz_sw *sw = mrp->parent;
+
+	if (!mrp->started)
+		return;
+	if (vid == 0 || vid == 1 || vid == 4095)
+		return;
+	cnt = 0;
+	q = 0;
+	if (vlan->valid) {
+		for (n = 0; n <= sw->mib_port_cnt; n++) {
+			p = get_phy_port(sw, n);
+			if (vlan->ports & (1 << p)) {
+				q = p;
+				cnt++;
+			}
+		}
+	}
+	if (cnt) {
+		int bit;
+		int index;
+		u16 ports;
+		int new_decl = false;
+
+#if 1
+		node = mrp_get_vlan_info(&mrp->vlan_list, vid, NULL);
+		if (!node)
+			return;
+
+		info = node->data;
+		ports = info->ports;
+		info->set_ports = vlan->ports;
+		info->ports = info->set_ports;
+#if 0
+			ports = mrp_find_vlan_ports(&mrp->vlan_list, vid,
+				&info->index, &info->fid);
+#endif
+		if (!ports) {
+			info->index = mrp_alloc_vlan(mrp);
+			info->fid = vlan->fid;
+dbg_msg(" index: %d %d"NL, info->index, info->fid);
+		}
+#endif
+		index = vid / VID_IN_DATA;
+		bit = vid % VID_IN_DATA;
+		if (!(sw->info->vid[index] & (1 << bit)))
+			new_decl = true;
+		if (cnt > 1)
+			q = sw->port_cnt;
+		mrp_vlan.id = vid;
+dbg_msg(" q %d %x"NL, q, info->tx_ports);
+		for (n = 0; n <= sw->mib_port_cnt; n++) {
+			p = get_phy_port(sw, n);
+			if (p == q) {
+				if (info->tx_ports & BIT(p)) {
+					info->tx_ports &= ~BIT(p);
+					if (proc_leave_vlan(mrp, p, &mrp_vlan))
+						proc_mrp_req_lv(mrp, p, &vlan_mrp_app);
+				}
+				continue;
+			}
+			if (!(info->tx_ports & BIT(p))) {
+				proc_mrp_join_vlan(mrp, p, &mrp_vlan, new_decl);
+				info->tx_ports |= BIT(p);
+			}
+		}
+	} else {
+		data.vid = vid;
+		memset(data.addr, 0xff, ETH_ALEN);
+		node = mrp_find_node(&mrp->vlan_list, cmp_vlan, &data);
+		if (node) {
+			u16 ports;
+
+			info = node->data;
+			info->set_ports = 0;
+			info->ports = info->rx_ports;
+#if 0
+			ports = mrp_find_vlan_ports(&mrp->vlan_list, data.vid,
+						    NULL, NULL);
+#endif
+			ports = info->ports;
+			if (!ports) {
+				mrp_free_fid(mrp, info->fid);
+				mrp_free_vlan(mrp, info->index);
+			}
+dbg_msg(" delete: %x"NL, info->ports);
+
+			/* Nobody is using the VLAN. */
+			if (!info->ports)
+				mrp_delete_node(&mrp->vlan_list, cmp_vlan,
+						node);
+		}
+		mrp_vlan.id = vid;
+		for (n = 0; n <= sw->mib_port_cnt; n++) {
+			p = get_phy_port(sw, n);
+			if (proc_leave_vlan(mrp, p, &mrp_vlan))
+				proc_mrp_req_lv(mrp, p, &vlan_mrp_app);
+		}
+	}
+}  /* mrp_setup_vlan */
+
+static void proc_mrp_rcv(struct mrp_info *mrp, struct mrp_applicant *app,
+			 struct sk_buff *skb)
+{
+	struct mrp_applicant **data = (struct mrp_applicant **)skb->cb;
+
+	*data = app;
+	skb_queue_tail(&mrp->rxq, skb);
+	schedule_work(&mrp->rx_proc);
+}  /* proc_mrp_rcv */
+
+static int mrp_rcv(struct mrp_info *mrp, struct sk_buff *skb, uint p)
+{
+	int mac_oper;
+	struct mrp_application *appl = NULL;
+	struct ethhdr *eth = (struct ethhdr *) skb->data;
+	struct ksz_sw *sw = mrp->parent;
+
+	/* MAC_Operational. */
+	mac_oper = (sw->dev_ports & (1 << p));
+
+	if (eth->h_proto == htons(ETH_P_MVRP)) {
+/* MVRP.c.11.2.3a allows registration with non-forwarding ports. */
+#if 0
+		mac_oper &= sw->rx_ports[0];
+#endif
+		appl = &vlan_mrp_app;
+		if (!(mrp->mvrp_rx_ports & (1 << p)))
+			mac_oper = false;
+	} else if (eth->h_proto == htons(ETH_P_MMRP)) {
+		mac_oper &= sw->rx_ports[0];
+		appl = &mac_mrp_app;
+		if (!(mrp->mmrp_rx_ports & (1 << p)))
+			mac_oper = false;
+
+#ifdef CONFIG_KSZ_MSRP
+	} else if (eth->h_proto == htons(ETH_P_MSRP)) {
+		mac_oper &= sw->rx_ports[0];
+		appl = &srp_mrp_app;
+		if (!mrp->status.msrpEnabledStatus ||
+		    !mrp->port_info[p].status.msrpPortEnabledStatus)
+			mac_oper = false;
+#endif
+	}
+	if (appl) {
+		struct mrp_port *port;
+		struct mrp_applicant *app;
+		struct ksz_port_info *info;
+		uint n;
+		uint q;
+
+#ifdef DBG_MRP_
+
+dbg_msg("  R:"NL);
+		for (i = 0; i < skb->len; i++) {
+			dbg_msg("%02x ", skb->data[i]);
+			if ((i % 16) == 15)
+				dbg_msg(NL);
+		}
+		if ((i % 16))
+			dbg_msg(NL);
+#endif
+
+		/* Check for MRP message sent by self. */
+		for (n = 1; n <= sw->mib_port_cnt; n++) {
+			q = get_phy_port(sw, n);
+			info = get_port_info(sw, q);
+			if (!memcmp(eth->h_source, info->mac_addr, ETH_ALEN)) {
+				mac_oper = false;
+				break;
+			}
+		}
+		if (!mac_oper) {
+			kfree_skb(skb);
+			return 0;
+		}
+
+		port = &mrp->mrp_ports[p];
+		app = rcu_dereference(port->applicants[appl->type]);
+		if (!app) {
+dbg_msg(" no app! %d=%d"NL, p, appl->type);
+			kfree_skb(skb);
+			return 0;
+		}
+
+		/* Destination MAC address needs to match MRP address. */
+		if (memcmp(app->group_address, eth->h_dest, ETH_ALEN))
+			goto done;
+		proc_mrp_rcv(mrp, app, skb);
+		return 0;
+	}
+
+done:
+	if (!memcmp(mrp->svlan_addr, eth->h_dest, ETH_ALEN)) {
+		/* Forward frame if port is C-VLAN. */
+	}
+
+	/* Known MRP type. */
+	if (appl) {
+		kfree_skb(skb);
+		return 0;
+	}
+	return 1;
+}  /* mrp_rcv */
+#endif
+
+#ifdef CONFIG_KSZ_MSRP
+static void mrp_delete_reserv(struct mrp_info *mrp, u8 port, int direction)
+{
+	struct mrp_port_info *info;
+	struct SRP_reserv *reserv;
+	struct SRP_reserv *next;
+
+	info = mrp_get_port_info(mrp, port);
+	reserv = info->registered.next;
+	while (reserv) {
+		next = reserv->next;
+		if (reserv->direction == direction) {
+			if (SRP_TALKER == direction)
+				mrp_delete_talker(mrp, port, info, reserv);
+			else
+				mrp_delete_listener(mrp, port, info, reserv);
+		}
+		reserv = next;
+	}
+}
+
+static void mrp_reset_reserv(struct mrp_info *mrp, uint first, uint last)
+{
+	struct mrp_port_info *info;
+	struct SRP_reserv *reserv;
+	struct SRP_reserv *next;
+	uint n;
+	uint p;
+	struct ksz_sw *sw = mrp->parent;
+
+	for (n = first; n <= last; n++) {
+		p = get_phy_port(sw, n);
+		mrp_delete_reserv(mrp, p, SRP_LISTENER);
+	}
+	for (n = first; n <= last; n++) {
+		p = get_phy_port(sw, n);
+		mrp_delete_reserv(mrp, p, SRP_TALKER);
+	}
+	for (n = first; n <= last; n++) {
+		p = get_phy_port(sw, n);
+		info = mrp_get_port_info(mrp, p);
+		reserv = info->declared.next;
+		while (reserv) {
+			next = reserv->next;
+			if (SRP_TALKER == reserv->direction &&
+			    reserv->stream) {
+				struct mrp_port_info *rx;
+				struct SRP_reserv *t_reserv;
+				struct SRP_stream *stream = reserv->stream;
+
+				rx = mrp_get_port_info(mrp, stream->in_port);
+				t_reserv = srp_find_reserv(&rx->registered,
+					stream->id, SRP_TALKER);
+				if (t_reserv) {
+					t_reserv->tx_ports &= ~BIT(p);
+				}
+			}
+			srp_remove_reserv(reserv, true);
+			reserv = next;
+		}
+	}
+}  /* mrp_reset_resev */
+#endif
+
+static void proc_mrp_attribute(struct mrp_info *mrp, u8 *data)
+{
+	uint p;
+	u8 in_port;
+#ifdef CONFIG_KSZ_MRP
+	int output;
+#endif
+	int result;
+	struct ksz_sw *sw = mrp->parent;
+	struct mrp_cfg_options *cmd = (struct mrp_cfg_options *) data;
+
+	mutex_lock(&mrp->lock);
+	in_port = cmd->port;
+	result = proc_mrp_set_attribute(mrp, data);
+	while (DEV_IOC_MRP_REPORT == result) {
+		cmd->action = MRP_ACTION_TX;
+		cmd->type = MRP_TYPE_UNKNOWN;
+#ifdef CONFIG_KSZ_MRP
+		result = proc_mrp_get_attribute(mrp, data, &output);
+#endif
+		if (cmd->action != MRP_ACTION_TX &&
+		    cmd->action != MRP_ACTION_TX_NEW &&
+		    cmd->action != MRP_ACTION_LV)
+			continue;
+
+		cmd->new_decl = false;
+		if (MRP_ACTION_TX_NEW == cmd->action) {
+			cmd->action = MRP_ACTION_TX;
+			cmd->new_decl = true;
+		}
+		p = get_phy_port(sw, cmd->port);
+#ifdef CONFIG_KSZ_MRP
+		if (MRP_TYPE_MAC == cmd->type) {
+			if (MRP_ACTION_TX == cmd->action)
+				proc_mrp_join_mac(mrp, p, &cmd->data.mac,
+					cmd->new_decl);
+			else
+				mrp->mac_tx[p] |=
+					proc_leave_mac(mrp, p,
+						       &cmd->data.mac);
+		} else if (MRP_TYPE_VLAN == cmd->type) {
+			if (MRP_ACTION_TX == cmd->action)
+				proc_mrp_join_vlan(mrp, p, &cmd->data.vlan,
+					cmd->new_decl);
+			else
+				mrp->vlan_tx[p] |=
+					proc_leave_vlan(mrp, p,
+							&cmd->data.vlan);
+
+#ifdef CONFIG_KSZ_MSRP
+		} else if (MRP_TYPE_TALKER == cmd->type) {
+			if (MRP_ACTION_TX == cmd->action)
+				proc_mrp_new_talker(mrp, p,
+					&cmd->data.talker, cmd->new_decl);
+			else
+				mrp->srp_tx[p] |=
+					proc_leave_talker(mrp, p,
+							   &cmd->data.talker);
+		} else if (MRP_TYPE_LISTENER == cmd->type) {
+			if (MRP_ACTION_TX == cmd->action)
+				proc_mrp_new_listener(mrp, p,
+					&cmd->data.listener, cmd->new_decl);
+			else
+				mrp->srp_tx[p] |=
+					proc_leave_listener(mrp, p,
+							     &cmd->data.listener);
+		} else if (MRP_TYPE_DOMAIN == cmd->type) {
+			if (MRP_ACTION_TX == cmd->action)
+				proc_mrp_join_domain(mrp, p,
+					&cmd->data.domain, cmd->new_decl);
+			else
+				mrp->srp_tx[p] |=
+					proc_leave_domain(mrp, p,
+							   &cmd->data.domain);
+#endif
+		}
+#endif
+	}
+	mutex_unlock(&mrp->lock);
+}  /* proc_mrp_attribute */
+
+#ifdef CONFIG_KSZ_MRP
+static void mmrp_acton(struct mrp_applicant *app, struct mrp_attr *attr)
+{
+	u8 data[40];
+	struct mrp_cfg_options *cmd = (struct mrp_cfg_options *) data;
+	struct mrp_info *mrp = app->parent;
+	struct ksz_sw *sw = mrp->parent;
+
+	cmd->port = get_log_port(sw, app->port);
+	if (MMRP_ATTR_MAC == attr->type) {
+		memcpy(cmd->data.mac.addr, attr->value, ETH_ALEN);
+		cmd->type = MRP_TYPE_MAC;
+		cmd->new_decl = MRP_NOTIFY_NEW == attr->notify;
+		if (!(app->normal & (1 << attr->type)))
+			cmd->new_decl |= 0x80;
+		if (MRP_NOTIFY_LV == attr->notify)
+			cmd->action = MRP_ACTION_LV;
+		else
+			cmd->action = MRP_ACTION_RX;
+	}
+	attr->notify = MRP_NOTIFY_NONE;
+	if (cmd->type != MRP_TYPE_UNKNOWN)
+		proc_mrp_attribute(mrp, data);
+}  /* mmrp_acton */
+
+static void mmrp_cleanup(struct mrp_applicant *app)
+{
+	uint n;
+	uint p;
+	struct mrp_info *mrp = app->parent;
+	struct ksz_sw *sw = mrp->parent;
+
+	for (n = 0; n <= sw->mib_port_cnt; n++) {
+		p = get_phy_port(sw, n);
+		if (mrp->mac_tx[p]) {
+			mrp->mac_tx[p] = 0;
+			proc_mrp_req_lv(mrp, p, &mac_mrp_app);
+		}
+	}
+}  /* mmrp_cleanup */
+
+static void mvrp_acton(struct mrp_applicant *app, struct mrp_attr *attr)
+{
+	u8 data[40];
+	struct mrp_cfg_options *cmd = (struct mrp_cfg_options *) data;
+	struct mrp_info *mrp = app->parent;
+	struct ksz_sw *sw = mrp->parent;
+
+	cmd->port = get_log_port(sw, app->port);
+	if (MVRP_ATTR_VID == attr->type) {
+		u16 *vid = (u16 *) attr->value;
+
+		cmd->data.vlan.id = ntohs(*vid);
+		cmd->type = MRP_TYPE_VLAN;
+		cmd->new_decl = MRP_NOTIFY_NEW == attr->notify;
+		if (!(app->normal & (1 << attr->type)))
+			cmd->new_decl |= 0x80;
+		if (MRP_NOTIFY_LV == attr->notify)
+			cmd->action = MRP_ACTION_LV;
+		else
+			cmd->action = MRP_ACTION_RX;
+	}
+	attr->notify = MRP_NOTIFY_NONE;
+	if (cmd->type != MRP_TYPE_UNKNOWN)
+		proc_mrp_attribute(mrp, data);
+}  /* mvrp_acton */
+
+static void mvrp_cleanup(struct mrp_applicant *app)
+{
+	uint n;
+	uint p;
+	struct mrp_info *mrp = app->parent;
+	struct ksz_sw *sw = mrp->parent;
+
+	for (n = 0; n <= sw->mib_port_cnt; n++) {
+		p = get_phy_port(sw, n);
+		if (mrp->vlan_tx[p]) {
+			mrp->vlan_tx[p] = 0;
+			proc_mrp_req_lv(mrp, p, &vlan_mrp_app);
+		}
+	}
+}  /* mvrp_cleanup */
+
+#ifdef CONFIG_KSZ_MSRP
+static void msrp_acton(struct mrp_applicant *app, struct mrp_attr *attr)
+{
+	u8 data[40];
+	struct mrp_cfg_options *cmd = (struct mrp_cfg_options *) data;
+	struct mrp_info *mrp = app->parent;
+	struct ksz_sw *sw = mrp->parent;
+
+	cmd->port = get_log_port(sw, app->port);
+	switch (attr->type) {
+	case MSRP_ATTR_TALKER_FAILED:
+	{
+		struct SRP_talker *talker = &cmd->data.talker;
+		struct srp_talker_failed *attr_talker =
+			(struct srp_talker_failed *) attr->value;
+
+		mrp_get_talker(talker, attr_talker);
+		cmd->type = MRP_TYPE_TALKER;
+		cmd->new_decl = MRP_NOTIFY_NEW == attr->notify;
+		if (MRP_NOTIFY_LV == attr->notify)
+			cmd->action = MRP_ACTION_LV;
+		else
+			cmd->action = MRP_ACTION_RX;
+		break;
+	}
+	case MSRP_ATTR_LISTENER:
+	{
+		struct SRP_listener *listener = &cmd->data.listener;
+		struct srp_listener *attr_listener =
+			(struct srp_listener *) attr->value;
+
+		mrp_get_listener(listener, attr_listener);
+		cmd->type = MRP_TYPE_LISTENER;
+		cmd->new_decl = MRP_NOTIFY_NEW == attr->notify;
+		if (MRP_NOTIFY_LV == attr->notify)
+			cmd->action = MRP_ACTION_LV;
+		else
+			cmd->action = MRP_ACTION_RX;
+		break;
+	}
+	case MSRP_ATTR_DOMAIN:
+	{
+		struct SRP_domain_class *domain = &cmd->data.domain;
+		struct srp_domain *attr_domain =
+			(struct srp_domain *) attr->value;
+
+		mrp_get_domain(domain, attr_domain);
+		cmd->type = MRP_TYPE_DOMAIN;
+		cmd->new_decl = MRP_NOTIFY_NEW == attr->notify;
+		if (MRP_NOTIFY_LV == attr->notify)
+			cmd->action = MRP_ACTION_LV;
+		else
+			cmd->action = MRP_ACTION_RX;
+		break;
+	}
+	default:
+		break;
+	}
+	attr->notify = MRP_NOTIFY_NONE;
+	if (cmd->type != MRP_TYPE_UNKNOWN)
+		proc_mrp_attribute(mrp, data);
+}  /* msrp_acton */
+
+static void msrp_cleanup(struct mrp_applicant *app)
+{
+	uint n;
+	uint p;
+	struct mrp_info *mrp = app->parent;
+	struct ksz_sw *sw = mrp->parent;
+
+	for (n = 0; n <= sw->mib_port_cnt; n++) {
+		p = get_phy_port(sw, n);
+		if (mrp->srp_tx[p]) {
+			mrp->srp_tx[p] = 0;
+			proc_mrp_req_lv(mrp, p, &srp_mrp_app);
+		}
+	}
+}  /* msrp_cleanup */
+#endif
+#endif
+
+static void sw_setup_mrp(struct ksz_sw *sw)
+{
+	struct ksz_mac_table *entry;
+	struct ksz_alu_table *alu;
+	int i;
+	int j;
+
+	i = sw->info->multi_sys;
+	for (j = 0; j < 3; j++) {
+		entry = &sw->info->mac_table[--i];
+		memcpy(entry->addr, maap_addr[j], ETH_ALEN);
+		entry->fid = 0;
+		if ((entry->addr[0] & 0x0F) == 0x01)
+			entry->ports = sw->PORT_MASK;
+		else
+			entry->ports = sw->PORT_MASK & ~sw->HOST_MASK;
+		alu = &sw->info->alu_table[i];
+		alu->forward = FWD_MAIN_DEV | FWD_MCAST | FWD_KNOWN;
+		alu->owner = 1;
+		alu->valid = 1;
+		alu->index = 0;
+		alu->type = 2;
+		sw->ops->cfg_mac(sw, 0, maap_addr[j], entry->ports, false,
+				 false, 0);
+	}
+	sw->info->multi_sys = i;
+}  /* sw_setup_mrp */
+
+static void setup_mrp(struct mrp_info *mrp, struct net_device *dev)
+{
+#ifdef CONFIG_KSZ_MRP
+
+#ifndef NETIF_F_HW_VLAN_CTAG_FILTER
+	prandom32_seed(&rnd, get_random_int());
+#endif
+	memcpy(mrp->cvlan_addr, vlan_mrp_app.group_address, ETH_ALEN);
+	memcpy(mrp->svlan_addr, vlan_mrp_app.group_address, ETH_ALEN);
+	mrp->svlan_addr[5] = 0x0D;
+#endif
+}  /* setup_mrp */
+
+#ifdef CONFIG_KSZ_MRP
+static struct mrp_applicant *mrp_start_port_app(struct mrp_info *mrp,
+	uint p, struct net_device *dev, struct mrp_application *appl)
+{
+	int err;
+	struct mrp_applicant *app;
+	struct mrp_port *port;
+	struct ksz_sw *sw = mrp->parent;
+	struct ksz_port_info *info = &sw->port_info[p];
+
+	port = &mrp->mrp_ports[p];
+	err = mrp_init_applicant(port, mrp, p, dev, appl);
+	app = rcu_dereference(port->applicants[appl->type]);
+	memcpy(app->src_addr, info->mac_addr, ETH_ALEN);
+	return app;
+}  /* mrp_start_port_app */
+
+static void mrp_stop_port_app(struct mrp_port *port,
+	struct mrp_application *appl, int attrtype, bool rx)
+{
+	struct rb_node *node, *next;
+	struct mrp_attr *attr;
+	struct mrp_applicant *app;
+
+	app = rcu_dereference(port->applicants[appl->type]);
+	if (!app)
+printk(" %s!!\n", __func__);
+	if (!app)
+		return;
+	for (node = rb_first(&app->mad);
+	     next = node ? rb_next(node) : NULL, node != NULL;
+	     node = next) {
+		attr = rb_entry(node, struct mrp_attr, node);
+
+		if (rx || (attr->type == attrtype &&
+		    attr->reg_state == MRP_REGISTRAR_MT))
+			mrp_attr_destroy(app, attr);
+	}
+	if (rx)
+		mrp_uninit_applicant(port, appl);
+}  /* mrp_stop_port_app */
+
+static void mrp_start_mmrp_port_app(struct mrp_info *mrp, uint p,
+				struct net_device *dev)
+{
+	struct mrp_applicant *app;
+
+	app = mrp_start_port_app(mrp, p, dev, &mac_mrp_app);
+	mmrp_init_application(app, mmrp_acton, mmrp_cleanup);
+	mrp_periodic_event(app, MRP_EVENT_PERIODIC_ENABLE);
+}  /* mrp_start_mmrp_port_app */
+
+static void mrp_start_mmrp_app(struct mrp_info *mrp)
+{
+	uint n;
+	uint p;
+	int err;
+	struct ksz_sw *sw = mrp->parent;
+	struct net_device *dev = sw->main_dev;
+
+	err = dev_mc_add(dev, mac_mrp_app.group_address);
+	for (n = 0; n <= mrp->ports; n++) {
+		p = get_phy_port(sw, n);
+		if (!(mrp->mmrp_rx_ports & (1 << p)))
+			continue;
+
+		mrp_start_mmrp_port_app(mrp, p, dev);
+	}
+}  /* mrp_start_mmrp_app */
+
+static void mrp_stop_mmrp_app(struct mrp_info *mrp)
+{
+	uint n;
+	uint p;
+	struct ksz_sw *sw = mrp->parent;
+	struct net_device *dev = sw->main_dev;
+
+	for (n = 0; n <= mrp->ports; n++) {
+		p = get_phy_port(sw, n);
+		if (!(mrp->mmrp_rx_ports & (1 << p)))
+			continue;
+
+		mrp_stop_port_app(&mrp->mrp_ports[p], &mac_mrp_app, 0, true);
+	}
+	dev_mc_del(dev, mac_mrp_app.group_address);
+}  /* mrp_stop_mmrp_app */
+
+static void mrp_start_mvrp_port_app(struct mrp_info *mrp, uint p,
+				struct net_device *dev)
+{
+	struct mrp_applicant *app;
+
+	app = mrp_start_port_app(mrp, p, dev, &vlan_mrp_app);
+	mvrp_init_application(app, mvrp_acton, mvrp_cleanup);
+	app->group_address = mrp->cvlan_addr;
+	mrp_periodic_event(app, MRP_EVENT_PERIODIC_ENABLE);
+}  /* mrp_start_mvrp_port_app */
+
+static void mrp_start_mvrp_app(struct mrp_info *mrp)
+{
+	uint n;
+	uint p;
+	int err;
+	struct ksz_sw *sw = mrp->parent;
+	struct net_device *dev = sw->main_dev;
+
+	err = dev_mc_add(dev, vlan_mrp_app.group_address);
+	err = dev_mc_add(dev, mrp->svlan_addr);
+	for (n = 0; n <= mrp->ports; n++) {
+		p = get_phy_port(sw, n);
+		if (!(mrp->mvrp_rx_ports & (1 << p)))
+			continue;
+
+		mrp_start_mvrp_port_app(mrp, p, dev);
+	}
+}  /* mrp_start_mvrp_app */
+
+static void mrp_stop_mvrp_app(struct mrp_info *mrp)
+{
+	uint n;
+	uint p;
+	struct ksz_sw *sw = mrp->parent;
+	struct net_device *dev = sw->main_dev;
+
+	for (n = 0; n <= mrp->ports; n++) {
+		p = get_phy_port(sw, n);
+		if (!(mrp->mvrp_rx_ports & (1 << p)))
+			continue;
+
+		mrp_stop_port_app(&mrp->mrp_ports[p], &vlan_mrp_app, 0, true);
+	}
+	dev_mc_del(dev, vlan_mrp_app.group_address);
+	dev_mc_del(dev, mrp->svlan_addr);
+}  /* mrp_stop_mvrp_app */
+
+#ifdef CONFIG_KSZ_MSRP
+static void mrp_start_msrp_port_app(struct mrp_info *mrp, uint p,
+				struct net_device *dev)
+{
+	struct mrp_applicant *app;
+
+	app = mrp_start_port_app(mrp, p, dev, &srp_mrp_app);
+	msrp_init_application(app, msrp_acton, msrp_cleanup);
+	mrp_periodic_event(app, MRP_EVENT_PERIODIC_DISABLE);
+}  /* mrp_start_msrp_port_app */
+
+static void mrp_start_msrp_app(struct mrp_info *mrp)
+{
+	uint n;
+	uint p;
+	int err;
+	struct ksz_sw *sw = mrp->parent;
+	struct net_device *dev = sw->main_dev;
+
+	err = dev_mc_add(dev, srp_mrp_app.group_address);
+	for (n = 0; n <= mrp->ports; n++) {
+		p = get_phy_port(sw, n);
+		if (!mrp->port_info[p].status.msrpPortEnabledStatus)
+			continue;
+
+		mrp_start_msrp_port_app(mrp, p, dev);
+	}
+}  /* mrp_start_msrp_app */
+
+static void mrp_stop_msrp_app(struct mrp_info *mrp)
+{
+	uint n;
+	uint p;
+	struct ksz_sw *sw = mrp->parent;
+	struct net_device *dev = sw->main_dev;
+
+	for (n = 0; n <= mrp->ports; n++) {
+		p = get_phy_port(sw, n);
+		if (!mrp->port_info[p].status.msrpPortEnabledStatus)
+			continue;
+
+		mrp_stop_port_app(&mrp->mrp_ports[p], &srp_mrp_app, 0, true);
+	}
+	dev_mc_del(dev, srp_mrp_app.group_address);
+}  /* mrp_stop_msrp_app */
+#endif
+
+#if 0
+static void proc_mrp_chk_declared(struct mrp_info *mrp, uint port, int on)
+{
+	int result;
+	struct mrp_node *prev;
+	struct mrp_node *next;
+	struct mrp_mac_info *mac;
+	struct mrp_vlan_info *vlan;
+	struct ksz_sw *sw = mrp->parent;
+	u8 lp = sw_get_net_port(sw, 0, mrp->ports, port);
+
+#if 1
+	struct SRP_reserv *reserv;
+	struct mrp_port_info *info;
+
+	info = &mrp->port_info[port];
+		if (!mrp->port_info[p].status.msrpPortEnabledStatus)
+			continue;
+
+		mrp_start_msrp_port_decl(mrp, &mrp->mrp_ports[p]);
+	reserv = info->declared.next;
+	while (reserv) {
+		if (on) {
+			reserv->tx_ports |= BIT(lp);
+			if (reserv->direction == SRP_TALKER) {
+				struct SRP_talker talker;
+
+				memcpy(&talker, reserv->stream, 25);
+				memcpy(talker.bridge_id, reserv->bridge_id, 8);
+				talker.FailureCode = reserv->code;
+				result = proc_mrp_new_talker(mrp, port,
+					&talker, true);
+			} else if (reserv->direction == SRP_LISTENER) {
+				struct SRP_listener listener;
+
+				memcpy(&listener, reserv->stream, 8);
+				listener.substate = reserv->declaration;
+				result = proc_mrp_new_listener(mrp, port,
+					&listener, true);
+			}
+		} else {
+			reserv->tx_ports &= ~BIT(lp);
+		}
+		reserv = reserv->next;
+	}
+#endif
+	prev = &mrp->mac_list.anchor;
+	next = prev->next;
+	while (next) {
+		mac = next->data;
+
+		if (mac->tx_ports & BIT(lp) && !on) {
+			mac->tx_ports &= ~BIT(lp);
+		} else if (!(mac->tx_ports & BIT(port)) && on) {
+			struct MRP_mac mrp_mac;
+
+			memcpy(mrp_mac.addr, mac->addr, ETH_ALEN);
+			mac->tx_ports |= BIT(lp);
+			result = proc_mrp_join_mac(mrp, port,
+				&mrp_mac, true);
+		}
+		prev = next;
+		next = prev->next;
+	}
+	prev = &mrp->vlan_list.anchor;
+	next = prev->next;
+	while (next) {
+		vlan = next->data;
+
+		if (vlan->tx_ports & BIT(port) && !on) {
+			vlan->tx_ports &= ~BIT(lp);
+		} else if (!(vlan->tx_ports & BIT(port)) && on) {
+			struct MRP_vlan mrp_vlan;
+
+			vlan->tx_ports |= BIT(lp);
+			mrp_vlan.id = vlan->vid;
+			result = proc_mrp_join_vlan(mrp, port,
+				&mrp_vlan, true);
+		}
+		prev = next;
+		next = prev->next;
+	}
+}  /* proc_mrp_chk_declared */
+#endif
+
+static void mrp_start_app(struct mrp_info *mrp)
+{
+	mrp_start_mmrp_app(mrp);
+	mrp_start_mvrp_app(mrp);
+
+#ifdef CONFIG_KSZ_MSRP
+	if (mrp->status.msrpEnabledStatus)
+		mrp_start_msrp_app(mrp);
+#endif
+}  /* mrp_start_app */
+
+static void mrp_stop_app(struct mrp_info *mrp)
+{
+	mrp_stop_mmrp_app(mrp);
+	mrp_stop_mvrp_app(mrp);
+
+#ifdef CONFIG_KSZ_MSRP
+	if (mrp->status.msrpEnabledStatus)
+		mrp_stop_msrp_app(mrp);
+#endif
+}  /* mrp_stop_app */
+
+static void mrp_mmrp_decl(struct mrp_info *mrp, uint p, bool on)
+{
+	int result;
+	struct mrp_node *prev;
+	struct mrp_node *next;
+	struct mrp_mac_info *mac;
+	uint m = BIT(p);
+
+	prev = &mrp->mac_list.anchor;
+	next = prev->next;
+	while (next) {
+		mac = next->data;
+
+		if ((mac->tx_ports & m) && !on) {
+			/* Leave request will be sent. */
+			mac->tx_ports &= ~m;
+		} else if ((mac->mrp_ports & ~m) &&
+			   !(mac->tx_ports & m) && on) {
+			struct MRP_mac mrp_mac;
+
+			memcpy(mrp_mac.addr, mac->addr, ETH_ALEN);
+			mac->tx_ports |= m;
+			result = proc_mrp_join_mac(mrp, p, &mrp_mac, true);
+		}
+		prev = next;
+		next = prev->next;
+	}
+}  /* mrp_mmrp_decl */
+
+static void mrp_mvrp_decl(struct mrp_info *mrp, uint p, bool on)
+{
+	int result;
+	struct mrp_node *prev;
+	struct mrp_node *next;
+	struct mrp_vlan_info *vlan;
+	uint m = BIT(p);
+
+	prev = &mrp->vlan_list.anchor;
+	next = prev->next;
+	while (next) {
+		vlan = next->data;
+
+		if (vlan->ports & SRP_PORT_AVAIL) {
+			if ((vlan->tx_ports & m) && !on) {
+				/* Leave request will be sent. */
+				vlan->tx_ports &= ~m;
+			} else if (!(vlan->tx_ports & m) && on) {
+				struct MRP_vlan mrp_vlan;
+
+				vlan->tx_ports |= m;
+				mrp_vlan.id = vlan->vid;
+				result = proc_mrp_join_vlan(mrp, p, &mrp_vlan,
+							    true);
+			}
+		}
+		prev = next;
+		next = prev->next;
+	}
+}  /* mrp_mvrp_decl */
+
+#ifdef CONFIG_KSZ_MSRP
+static void mrp_msrp_decl(struct mrp_info *mrp, uint p, bool on)
+{
+	int result;
+	struct SRP_reserv *reserv;
+	struct mrp_port_info *info;
+	uint m = BIT(p);
+
+	info = &mrp->port_info[p];
+	reserv = info->declared.next;
+	while (reserv) {
+		if (on) {
+			reserv->tx_ports |= m;
+			if (reserv->direction == SRP_TALKER) {
+				struct SRP_talker talker;
+
+				memcpy(&talker, reserv->stream, 25);
+				memcpy(talker.bridge_id, reserv->bridge_id, 8);
+				talker.FailureCode = reserv->code;
+				result = proc_mrp_new_talker(mrp, p,
+					&talker, true);
+			} else if (reserv->direction == SRP_LISTENER) {
+				struct SRP_listener listener;
+
+				memcpy(&listener, reserv->stream, 8);
+				listener.substate = reserv->declaration;
+				result = proc_mrp_new_listener(mrp, p,
+					&listener, true);
+			}
+		} else {
+			/* Leave request will be sent. */
+			reserv->tx_ports &= ~m;
+		}
+		reserv = reserv->next;
+	}
+	if (on) {
+#if 0
+struct SRP_domain_class domain;
+memcpy(&domain, &mrp->domain[0], sizeof(domain));
+domain.id = 5;
+domain.priority = 6;
+proc_mrp_join_domain(mrp, p, &domain, true);
+domain.id = 6;
+domain.priority = 7;
+proc_mrp_join_domain(mrp, p, &domain, true);
+#endif
+		proc_mrp_join_domain(mrp, p, &mrp->domain[0], true);
+		if (mrp->domain[1].id)
+			proc_mrp_join_domain(mrp, p, &mrp->domain[1], true);
+	}
+}  /* mrp_msrp_decl */
+#endif
+
+static void mmrp_close_port(struct mrp_info *mrp, uint p)
+{
+	/* MMRP is not enabled. */
+	if (!(mrp->mmrp_tx_ports & BIT(p)))
+		return;
+	mrp_mmrp_decl(mrp, p, false);
+}  /* mmrp_close_port */
+
+static void mmrp_open_port(struct mrp_info *mrp, uint p)
+{
+	struct mrp_applicant *app;
+	struct mrp_port *port = &mrp->mrp_ports[p];
+
+	/* MMRP is not enabled. */
+	if (!(mrp->mmrp_tx_ports & BIT(p)))
+		return;
+	app = rcu_dereference(port->applicants[mac_mrp_app.type]);
+if (!app)
+dbg_msg(" %s"NL, __func__);
+	if (!app) {
+		struct ksz_sw *sw = mrp->parent;
+
+		mrp_start_mmrp_port_app(mrp, p, sw->main_dev);
+	}
+	mrp_mmrp_decl(mrp, p, true);
+}  /* mmrp_open_port */
+
+static void mvrp_close_port(struct mrp_info *mrp, uint p)
+{
+	/* MVRP is not enabled. */
+	if (!(mrp->mvrp_tx_ports & BIT(p)))
+		return;
+	mrp_mvrp_decl(mrp, p, false);
+}  /* mvrp_close_port */
+
+static void mvrp_open_port(struct mrp_info *mrp, uint p)
+{
+	struct mrp_applicant *app;
+	struct mrp_port *port = &mrp->mrp_ports[p];
+
+	/* MVRP is not enabled. */
+	if (!(mrp->mvrp_tx_ports & BIT(p)))
+		return;
+	app = rcu_dereference(port->applicants[vlan_mrp_app.type]);
+if (!app)
+dbg_msg(" %s"NL, __func__);
+	if (!app) {
+		struct ksz_sw *sw = mrp->parent;
+
+		mrp_start_mvrp_port_app(mrp, p, sw->main_dev);
+	}
+	mrp_mvrp_decl(mrp, p, true);
+}  /* mvrp_open_port */
+
+#ifdef CONFIG_KSZ_MSRP
+static void msrp_close_port(struct mrp_info *mrp, uint p)
+{
+	/* MSRP is not enabled. */
+	if (!mrp->status.msrpEnabledStatus ||
+	    !mrp->port_info[p].status.msrpPortEnabledStatus)
+		return;
+	mrp_msrp_decl(mrp, p, false);
+}  /* msrp_close_port */
+
+static void msrp_open_port(struct mrp_info *mrp, uint p)
+{
+	struct mrp_applicant *app;
+	struct mrp_port *port = &mrp->mrp_ports[p];
+
+	/* MSRP is not enabled. */
+	if (!mrp->status.msrpEnabledStatus ||
+	    !mrp->port_info[p].status.msrpPortEnabledStatus)
+		return;
+	app = rcu_dereference(port->applicants[srp_mrp_app.type]);
+if (!app)
+dbg_msg(" %s"NL, __func__);
+	if (!app) {
+		struct ksz_sw *sw = mrp->parent;
+
+		mrp_start_msrp_port_app(mrp, p, sw->main_dev);
+	}
+	mrp_msrp_decl(mrp, p, true);
+}  /* msrp_open_port */
+#endif
+
+static void mrp_close_port(struct mrp_info *mrp, uint p)
+{
+	uint m = BIT(p);
+
+dbg_msg("%s %d=%d"NL, __func__, p, mrp->started);
+	if (!mrp->started || !(mrp->tx_ports & m))
+		return;
+	if (!(mrp->mask & m))
+		return;
+#ifdef CONFIG_KSZ_MSRP
+	msrp_close_port(mrp, p);
+#endif
+	mvrp_close_port(mrp, p);
+	mmrp_close_port(mrp, p);
+
+	/* Every declaration leaves. */
+	mrp_event(mrp, p, MRP_EVENT_LV);
+#if 1
+	mrp_event(mrp, p, MRP_EVENT_R_LV);
+#endif
+	mrp->tx_ports &= ~m;
+	mrp->rx_ports = mrp->tx_ports;
+}  /* mrp_close_port */
+
+static void mrp_open_port(struct mrp_info *mrp, uint p)
+{
+	uint m = BIT(p);
+
+dbg_msg("%s %d=%d"NL, __func__, p, mrp->started);
+	if (!mrp->started || (mrp->tx_ports & m))
+		return;
+	if (!(mrp->mask & m))
+		return;
+#ifdef CONFIG_KSZ_MSRP
+	msrp_open_port(mrp, p);
+#endif
+	mvrp_open_port(mrp, p);
+	mmrp_open_port(mrp, p);
+	mrp->tx_ports |= m;
+	mrp->rx_ports = mrp->tx_ports;
+}  /* mrp_open_port */
+
+#ifdef CONFIG_KSZ_MSRP
+static void msrp_open_ports(struct mrp_info *mrp)
+{
+	uint n;
+	uint p;
+	struct ksz_sw *sw = mrp->parent;
+
+	for (n = 0; n <= mrp->ports; n++) {
+		p = get_phy_port(sw, n);
+		if (p == sw->HOST_PORT ||
+		    sw->info->port_cfg[p].stp_state[0] == STP_STATE_FORWARDING)
+			msrp_open_port(mrp, p);
+	}
+}  /* msrp_open_ports */
+#endif
+
+static void mrp_open_ports(struct mrp_info *mrp)
+{
+	uint n;
+	uint p;
+	struct ksz_sw *sw = mrp->parent;
+
+	for (n = 0; n <= mrp->ports; n++) {
+		p = get_phy_port(sw, n);
+		if (p == sw->HOST_PORT ||
+		    sw->info->port_cfg[p].stp_state[0] == STP_STATE_FORWARDING)
+			mrp_open_port(mrp, p);
+	}
+}  /* mrp_open_ports */
+
+static void mrp_from_backup(struct mrp_info *mrp, uint p)
+{
+dbg_msg("%s %d"NL, __func__, p);
+	mrp->rx_ports |= (1 << p);
+
+	/* mrp_open_port will be called when the port is forwarding. */
+}  /* mrp_from_backup */
+
+static void mrp_to_backup(struct mrp_info *mrp, uint p)
+{
+dbg_msg("%s %d"NL, __func__, p);
+	mrp_close_port(mrp, p);
+}  /* mrp_to_backup */
+
+static void mrp_from_designated(struct mrp_info *mrp, uint p, bool alt)
+{
+dbg_msg("%s %d=%d"NL, __func__, p, alt);
+	if (!mrp->started)
+		return;
+	mrp_event(mrp, p, MRP_EVENT_REDECLARE);
+	if (alt) {
+		mrp_close_port(mrp, p);
+	}
+}  /* mrp_from_designated */
+
+static void mrp_to_designated(struct mrp_info *mrp, uint p)
+{
+	uint m = BIT(p);
+
+dbg_msg("%s %d"NL, __func__, p);
+	if (!mrp->started)
+		return;
+	mrp_event(mrp, p, MRP_EVENT_FLUSH);
+	if (!(mrp->rx_ports & m)) {
+dbg_msg("  fwd again"NL);
+		mrp->rx_ports |= m;
+
+		/* mrp_open_port will be called when the port is forwarding. */
+	}
+}  /* mrp_to_designated */
+
+static void mrp_tc_detected(struct mrp_info *mrp, uint p)
+{
+	uint m = BIT(p);
+
+dbg_msg("%s %d"NL, __func__, p);
+	if (!mrp->started)
+		return;
+	if (!(mrp->mask & m))
+		return;
+	if (mrp->mmrp_rx_ports & m) {
+		mrp_new_event_port_app(mrp, p, &mac_mrp_app);
+	}
+	if (mrp->mvrp_rx_ports & m) {
+		mrp_new_event_port_app(mrp, p, &vlan_mrp_app);
+	}
+
+#ifdef CONFIG_KSZ_MSRP
+	if (mrp->status.msrpEnabledStatus &&
+	    mrp->port_info[p].status.msrpPortEnabledStatus) {
+		mrp_new_event_port_app(mrp, p, &srp_mrp_app);
+	}
+#endif
+}  /* mrp_tc_detected */
+
+static void mrp_start(struct mrp_info *mrp)
+{
+	mrp_start_app(mrp);
+	mrp->started = true;
+	mrp_open_ports(mrp);
+}  /* mrp_start */
+
+static void mrp_stop(struct mrp_info *mrp)
+{
+	if (mrp->started)
+		mrp_stop_app(mrp);
+	mrp->started = false;
+}  /* mrp_stop */
+#endif
+
+#ifdef CONFIG_KSZ_AVB
+static void mrp_open_avb(struct mrp_info *mrp)
+{
+	struct ksz_sw *sw = mrp->parent;
+	struct mrp_traffic_info *traffic;
+	struct mrp_port_info *info;
+	uint n;
+	uint p;
+	u8 tc;
+
+	if (!(sw->features & AVB_SUPPORT))
+		return;
+	for (n = 0; n <= mrp->ports; n++) {
+		p = get_phy_port(sw, n);
+		info = mrp_get_port_info(mrp, p);
+		for (tc = SR_CLASS_A; tc >= SR_CLASS_B; tc--) {
+			traffic = get_traffic_info(info, tc);
+			if (!traffic->bandwidth_used)
+				continue;
+			srp_cfg_credit_shaper(mrp, p, info, traffic);
+			traffic->bandwidth_set = traffic->bandwidth_used;
+		}
+		if (n) {
+			struct ksz_port_cfg *cfg = get_port_cfg(sw, p);
+			bool remap = true;
+			bool drop = false;
+
+			if (!(sw->features & MRP_SUPPORT)) {
+				cfg->avb_a = true;
+				cfg->avb_b = true;
+				remap = false;
+				drop = false;
+			}
+			setup_acl_drop(mrp, p);
+			setup_acl_remap(mrp, p);
+			enable_acl_remap(mrp, p, remap, drop);
+		}
+		for (tc = 0; tc < PRIO_QUEUES; tc++)
+			srp_cfg_idle_slope(mrp, p, tc, info,
+				info->bandwidth[tc].adminIdleSlope);
+	}
+}  /* mrp_open_avb */
+#endif
+
+#ifdef CONFIG_KSZ_MRP
+static void mrp_open_mrp(struct mrp_info *mrp)
+{
+	struct mrp_node *prev;
+	struct mrp_node *next;
+	struct mrp_mac_info *mac;
+	struct mrp_vlan_info *vlan;
+	u16 ports = 0;
+	struct mrp_vlan_info *prev_vlan = NULL;
+
+	prev = &mrp->mac_list.anchor;
+	next = prev->next;
+	while (next) {
+		mac = next->data;
+		mrp_cfg_dest_addr(mrp, mac->index, mac->addr, mac->ports,
+			mac->fid);
+		prev = next;
+		next = prev->next;
+	}
+
+	prev = &mrp->vlan_list.anchor;
+	next = prev->next;
+	while (next) {
+		vlan = next->data;
+
+		/* The very first one. */
+		if (!prev_vlan)
+			prev_vlan = vlan;
+		if (prev_vlan->vid == vlan->vid)
+			ports |= vlan->ports;
+		else {
+			mrp_cfg_vlan(mrp, prev_vlan->index, prev_vlan->vid,
+				prev_vlan->fid, ports);
+			prev_vlan = vlan;
+			ports = vlan->ports;
+		}
+		prev = next;
+		next = prev->next;
+	}
+	if (ports)
+		mrp_cfg_vlan(mrp, vlan->index, vlan->vid, vlan->fid, ports);
+}  /* mrp_open_mrp */
+#endif
+
+#if defined(CONFIG_KSZ_AVB) || defined(CONFIG_KSZ_MRP)
+static void mrp_open(struct mrp_info *mrp)
+{
+#ifdef CONFIG_KSZ_AVB
+	mrp_open_avb(mrp);
+#endif
+#ifdef CONFIG_KSZ_MRP
+	mrp_open_mrp(mrp);
+#endif
+}  /* mrp_open */
+#endif
+
+#ifdef CONFIG_KSZ_MRP
+static void mrp_clr_blocked_addr(struct mrp_info *mrp, int hw_access)
+{
+	struct mrp_node *prev;
+	struct mrp_node *next;
+	struct mrp_mac_info *mac;
+
+	prev = &mrp->mac_list.anchor;
+	next = prev->next;
+	while (next) {
+		mac = next->data;
+		if ((mac->rx_ports & SRP_PORT_DROP) &&
+		    (mac->srp_ports & SRP_PORT_BLACKLIST)) {
+dbg_msg("  rmv: %02x:%02x:%02x:%02x:%02x:%02x %d"NL,
+mac->addr[0], mac->addr[1], mac->addr[2], mac->addr[3], mac->addr[4], mac->addr[5], mac->fid);
+			if (hw_access)
+				mrp_cfg_dest_addr(mrp, mac->index, mac->addr,
+					0, mac->fid);
+
+			/* Remove node from list. */
+			prev->next = next->next;
+			mrp_free_node(next);
+		} else
+			prev = next;
+		next = prev->next;
+	}
+	mrp->mac_list.last = &mrp->mac_list.anchor;
+}  /* mrp_clr_blocked_addr */
+
+static void mrp_chk_blocked_addr(struct mrp_info *mrp)
+{
+	struct mrp_node *prev;
+	struct mrp_node *next;
+	struct mrp_mac_info *mac;
+	struct ksz_sw *sw = mrp->parent;
+
+	prev = &mrp->mac_list.anchor;
+	next = prev->next;
+	while (next) {
+		mac = next->data;
+		if ((mac->srp_ports & SRP_PORT_BLACKLIST) &&
+		    time_after_eq(jiffies, mac->ticks)) {
+			uint n;
+			uint p;
+			bool avb;
+			u16 ports;
+			struct ksz_port_cfg *cfg;
+
+			if (mac->rx_ports & SRP_PORT_IGNORE)
+				mac->rx_ports |= SRP_PORT_DROP;
+			mac->rx_ports |= SRP_PORT_IGNORE;
+			avb = true;
+			for (n = 1; n <= sw->mib_port_cnt; n++) {
+				p = get_phy_port(sw, n);
+				if (mac->rx_ports & (1 << p)) {
+					cfg = get_port_cfg(sw, p);
+					if (!cfg->avb_b) {
+						avb = false;
+						break;
+					}
+				}
+			}
+			if (avb)
+				ports = sw->HOST_MASK;
+			else
+				ports = sw->PORT_MASK;
+			sw->ops->cfg_mac(sw, mac->index, mac->addr, ports,
+					 false, mac->fid != 0, mac->fid);
+			mac->ticks = jiffies + msecs_to_jiffies(10000);
+		}
+		prev = next;
+		next = prev->next;
+	}
+	mrp_clr_blocked_addr(mrp, true);
+}  /* mrp_chk_blocked_addr */
+#endif
+
+#ifdef CONFIG_KSZ_MRP
+static void mrp_close(struct mrp_info *mrp, int hw_access)
+{
+	if (mrp->started) {
+		struct ksz_sw *sw = mrp->parent;
+
+		mrp_close_port(mrp, sw->HOST_PORT);
+		mrp_clr_blocked_addr(mrp, hw_access);
+#if 0
+		mrp->started = false;
+#endif
+	}
+}  /* mrp_close */
+#else
+static void mrp_close(struct mrp_info *mrp, int hw_access)
+{
+}
+#endif
+
+#ifdef CONFIG_KSZ_AVB
+static void mrp_set_slope(struct mrp_info *mrp, uint port, int index,
+			  struct mrp_port_info *info, uint per)
+{
+	u32 slope;
+	u64 val = per;
+
+	val <<= CREDIT_PERCENTAGE_S;
+	val += 500;
+	val = div_u64_u32(val, 1000);
+	slope = (u32)val;
+	info->bandwidth[index].adminIdleSlope = slope;
+	info->bandwidth[index].operIdleSlope = slope;
+	srp_cfg_idle_slope(mrp, port, index, info, slope);
+}  /* mrp_set_slope */
+#endif
+
+static ssize_t sysfs_mrp_read(struct ksz_sw *sw, int proc_num, ssize_t len,
+	char *buf)
+{
+#ifdef CONFIG_KSZ_AVB
+	struct mrp_info *mrp = &sw->mrp;
+#endif
+	int chk = 0;
+	int type = SHOW_HELP_NONE;
+	char note[40];
+
+	note[0] = '\0';
+	switch (proc_num) {
+#ifdef CONFIG_KSZ_MSRP
+	case PROC_GET_MSRP_INFO:
+		if (!(sw->features & AVB_SUPPORT))
+			return 0;
+		if (!(sw->features & MRP_SUPPORT))
+			return 0;
+		len += sprintf(buf + len,
+			"max interference size %u"NL,
+			mrp->max_interference_size);
+		if (regeneration_hack)
+			len += sprintf(buf + len,
+				"regeneration"NL);
+#ifdef CONFIG_1588_PTP
+		if (ba_hack)
+			len += sprintf(buf + len,
+				"BA"NL);
+#endif
+		if (mrp_10_1_2f_hack)
+			len += sprintf(buf + len,
+				"MRP.10.1.2F"NL);
+		if (mrp_10_1_8a_hack)
+			len += sprintf(buf + len,
+				"MRP.10.1.8A"NL);
+		if (mrp_10_5_1_hack)
+			len += sprintf(buf + len,
+				"MRP.10.5.1"NL);
+		if (mrp_10_5_1c_hack)
+			len += sprintf(buf + len,
+				"MRP.10.5.1C"NL);
+		if (mrp_10_5_1d_hack)
+			len += sprintf(buf + len,
+				"MRP.10.5.1D"NL);
+		if (msrp_35_1_14g_hack)
+			len += sprintf(buf + len,
+				"MSRP.35.1.14G"NL);
+		if (fqtss_hack)
+			len += sprintf(buf + len,
+				"FQTSS"NL);
+		if (fqtss_34_2_3_hack)
+			len += sprintf(buf + len,
+				"FQTSS.34.2.3"NL);
+		if (fqtss_34_2_1b_hack)
+			len += sprintf(buf + len,
+				"FQTSS.34.2.1B"NL);
+		if (fqtss_34_2_5b_hack)
+			len += sprintf(buf + len,
+				"FQTSS.34.2.5B"NL);
+		if (fqtss_34_2_9b_hack)
+			len += sprintf(buf + len,
+				"FQTSS.34.2.9B"NL);
+		break;
+	case PROC_SET_MSRP_ENABLED:
+		chk = mrp->status.msrpEnabledStatus;
+		type = SHOW_HELP_ON_OFF;
+		break;
+#endif
+#ifdef CONFIG_KSZ_AVB
+	case PROC_SET_MSRP_SR_A:
+		chk = mrp->domain[1].id != 0;
+		type = SHOW_HELP_ON_OFF;
+		break;
+#endif
+	default:
+		type = SHOW_HELP_NONE;
+		break;
+	}
+	return sysfs_show(len, buf, type, chk, note, sw->verbose);
+}  /* sysfs_mrp_read */
+
+static int sysfs_mrp_write(struct ksz_sw *sw, int proc_num, int num,
+	const char *buf)
+{
+#ifdef CONFIG_KSZ_AVB
+	struct mrp_info *mrp = &sw->mrp;
+#endif
+	int processed = true;
+
+	if (!(sw->features & AVB_SUPPORT))
+		return false;
+	switch (proc_num) {
+#ifdef CONFIG_KSZ_MSRP
+	case PROC_GET_MSRP_INFO:
+		if (!(sw->features & MRP_SUPPORT))
+			break;
+		if (!strncmp(buf, "mrp_10.1.2f", 11))
+			mrp_10_1_2f_hack = true;
+		else if (!strncmp(buf, "mrp_10.1.8a", 11))
+			mrp_10_1_8a_hack = true;
+		else if (!strncmp(buf, "mrp_10.5.1c", 11))
+			mrp_10_5_1c_hack = true;
+		else if (!strncmp(buf, "mrp_10.5.1d", 11))
+			mrp_10_5_1d_hack = true;
+		else if (!strncmp(buf, "mrp_10.5.1", 10))
+			mrp_10_5_1_hack = true;
+		else if (!strncmp(buf, "msrp_35.1.14g", 13))
+			msrp_35_1_14g_hack = true;
+		else if (!strncmp(buf, "fqtss_34.2.3", 12))
+			fqtss_34_2_3_hack = true;
+		else if (!strncmp(buf, "fqtss_34.2.1b", 13))
+			fqtss_34_2_1b_hack = true;
+		else if (!strncmp(buf, "fqtss_34.2.5b", 13))
+			fqtss_34_2_5b_hack = true;
+		else if (!strncmp(buf, "fqtss_34.2.9b", 13))
+			fqtss_34_2_9b_hack = true;
+		else if (!strncmp(buf, "fqtss", 5))
+			fqtss_hack = true;
+#ifdef CONFIG_1588_PTP
+		else if (!strncmp(buf, "ba", 2))
+			ba_hack = true;
+#endif
+		else if (!strncmp(buf, "regeneration", 12)) {
+			if (!regeneration_hack) {
+				uint n;
+				uint p;
+
+				regeneration_hack = true;
+				for (n = 0; n <= mrp->ports; n++) {
+					p = get_phy_port(sw, n);
+					setup_acl_remap(mrp, p);
+				}
+			}
+		} else if (!strncmp(buf, "reset", 5)) {
+#ifdef CONFIG_1588_PTP
+			ba_hack = false;
+#endif
+			mrp_10_1_2f_hack = false;
+			mrp_10_1_8a_hack = false;
+			mrp_10_5_1_hack = false;
+			mrp_10_5_1c_hack = false;
+			mrp_10_5_1d_hack = false;
+			msrp_35_1_14g_hack = false;
+			fqtss_hack = false;
+			fqtss_34_2_3_hack = false;
+			fqtss_34_2_1b_hack = false;
+			fqtss_34_2_5b_hack = false;
+			fqtss_34_2_9b_hack = false;
+			if (regeneration_hack) {
+				uint n;
+				uint p;
+
+				regeneration_hack = false;
+				for (n = 0; n <= mrp->ports; n++) {
+					p = get_phy_port(sw, n);
+					setup_acl_remap(mrp, p);
+				}
+			}
+		}
+else
+dbg_msg(" ?? %s]"NL, buf);
+		break;
+	case PROC_SET_MSRP_ENABLED:
+		if (!(sw->features & MRP_SUPPORT))
+			break;
+		if (mrp->status.msrpEnabledStatus != !!num) {
+			mutex_lock(&mrp->lock);
+			if (!mrp->status.msrpEnabledStatus) {
+				mrp_reset_reserv(mrp, 0, mrp->ports);
+				mrp->status.msrpEnabledStatus = true;
+				mrp_start_msrp_app(mrp);
+				msrp_open_ports(mrp);
+			} else {
+				mrp_stop_msrp_app(mrp);
+				mrp->status.msrpEnabledStatus = false;
+			}
+			mutex_unlock(&mrp->lock);
+		}
+		break;
+#endif
+#ifdef CONFIG_KSZ_AVB
+	case PROC_SET_MSRP_SR_A:
+	{
+#ifdef CONFIG_KSZ_MSRP
+		struct srp_domain domain;
+		struct mrp_applicant *app;
+#endif
+		struct mrp_port_info *info;
+		u32 delta_a;
+		u32 delta_b;
+		int q1;
+		int q2;
+		uint n;
+		uint p;
+		int join = 0;
+
+		if (!(sw->features & MRP_SUPPORT))
+			break;
+		if (num && !mrp->domain[1].id) {
+			mrp->domain[1].id = SR_CLASS_A;
+			join = 2;
+		} else if (!num && mrp->domain[1].id) {
+			join = 1;
+		}
+		if (!join)
+			break;
+		--join;
+#ifdef CONFIG_KSZ_MSRP
+		domain.class_id = mrp->domain[1].id;
+		domain.class_priority = mrp->domain[1].priority;
+		domain.class_vid = htons(mrp->domain[1].vlan_id);
+		for (n = 0; n <= mrp->ports; n++) {
+			p = get_phy_port(sw, n);
+
+			app = rcu_dereference(mrp->mrp_ports[p].applicants[
+					      srp_mrp_app.type]);
+			if (!app)
+				continue;
+			if (join)
+				msrp_req_join_domain(app, &domain, true);
+			else
+				msrp_req_leave_domain(app, &domain);
+		}
+#endif
+		if (!join) {
+			mrp->domain[1].id = 0;
+		}
+		for (n = 0; n <= mrp->ports; n++) {
+			p = get_phy_port(sw, n);
+			info = &mrp->port_info[p];
+			q1 = get_queue_priority(mrp, SR_CLASS_A);
+			q2 = get_queue_priority(mrp, SR_CLASS_B);
+			delta_a = info->bandwidth[q1].deltaBandwidth;
+			delta_b = info->bandwidth[q2].deltaBandwidth;
+			if (join) {
+				if (delta_a > 95)
+					delta_a = 95;
+				if (delta_a + delta_b > 95)
+					delta_b = 95 - delta_a;
+			} else {
+				delta_b += delta_a;
+				delta_a = 0;
+				if (delta_b > 95)
+					delta_b = 95;
+			}
+			mrp_set_delta(mrp, p, delta_a, delta_b);
+		}
+		break;
+	}
+#endif
+	default:
+		processed = false;
+		break;
+	}
+	return processed;
+}  /* sysfs_mrp_write */
+
+static ssize_t sysfs_mrp_port_read(struct ksz_sw *sw, int proc_num, uint n,
+	ssize_t len, char *buf)
+{
+	struct mrp_info *mrp = &sw->mrp;
+#ifdef CONFIG_KSZ_AVB
+	struct mrp_port_info *info;
+	int index;
+	char per_str[20];
+#endif
+	uint m;
+	uint port;
+	struct ksz_port_cfg *cfg;
+#ifdef CONFIG_KSZ_MRP
+	struct mrp_port *mrp_p;
+#endif
+	int chk = 0;
+	int type = SHOW_HELP_NUM;
+	char note[40];
+
+	note[0] = '\0';
+#ifdef CONFIG_KSZ_MRP
+	switch (proc_num) {
+	case PROC_SET_PORT_MMRP_ENABLED:
+	case PROC_SET_PORT_MMRP_MAC:
+	case PROC_SET_PORT_MMRP_SVC:
+	case PROC_SET_PORT_MMRP_REG:
+	case PROC_SET_PORT_MVRP_ENABLED:
+	case PROC_SET_PORT_MVRP_VID:
+	case PROC_SET_PORT_MVRP_REG:
+		if (!(sw->features & MRP_SUPPORT))
+			return 0;
+		break;
+	}
+#endif
+	port = get_sysfs_port(sw, n);
+	m = BIT(port);
+	if (!(mrp->mask & m))
+		return 0;
+	cfg = get_port_cfg(sw, port);
+#ifdef CONFIG_KSZ_MRP
+	mrp_p = &mrp->mrp_ports[port];
+#endif
+#ifdef CONFIG_KSZ_AVB
+	index = cfg->q_index;
+	info = &mrp->port_info[port];
+#endif
+	switch (proc_num) {
+#ifdef CONFIG_KSZ_MRP
+	case PROC_SET_PORT_MMRP_ENABLED:
+		chk = !!(mrp->mmrp_rx_ports & m);
+		type = SHOW_HELP_ON_OFF;
+		break;
+	case PROC_SET_PORT_MMRP_MAC:
+		chk = 0;
+		if (mrp->mmrp_rx_ports & m) {
+			struct mrp_applicant *app;
+
+			app = rcu_dereference(mrp_p->
+					      applicants[mac_mrp_app.type]);
+			chk = !!(app->normal & (1 << MMRP_ATTR_MAC));
+		}
+		type = SHOW_HELP_ON_OFF;
+		break;
+	case PROC_SET_PORT_MMRP_SVC:
+		chk = 0;
+		if (mrp->mmrp_rx_ports & m) {
+			struct mrp_applicant *app;
+
+			app = rcu_dereference(mrp_p->
+					      applicants[mac_mrp_app.type]);
+			chk = !!(app->normal & (1 << MMRP_ATTR_SVC));
+		}
+		type = SHOW_HELP_ON_OFF;
+		break;
+	case PROC_SET_PORT_MMRP_REG:
+		if (mrp->mmrp_rx_ports & m) {
+			struct mrp_applicant *app;
+			struct mrp_attr *attr;
+			char *str;
+			struct ksz_mac_table *entry;
+
+			entry = &sw->info->mac_entry;
+			app = rcu_dereference(mrp_p->
+					      applicants[mac_mrp_app.type]);
+			attr = mrp_attr_lookup(app, entry->addr, ETH_ALEN,
+					MMRP_ATTR_MAC);
+			str = "unregistered";
+			if (attr) {
+				if (attr->fix_state != MRP_REGISTRAR_LV) {
+					if (attr->fix_state == MRP_REGISTRAR_IN)
+						str = "fixed";
+					else
+						str = "forbidden";
+				} else if (attr->reg_state == MRP_REGISTRAR_IN)
+					str = "registered";
+			}
+			len += sprintf(buf + len,
+				"%s"NL, str);
+		}
+		type = SHOW_HELP_NONE;
+		break;
+	case PROC_SET_PORT_MVRP_ENABLED:
+		chk = !!(mrp->mvrp_rx_ports & m);
+		type = SHOW_HELP_ON_OFF;
+		break;
+	case PROC_SET_PORT_MVRP_VID:
+		chk = 0;
+		if (mrp->mvrp_rx_ports & m) {
+			struct mrp_applicant *app;
+
+			app = rcu_dereference(mrp_p->
+					      applicants[vlan_mrp_app.type]);
+			chk = !!(app->normal & (1 << MVRP_ATTR_VID));
+		}
+		type = SHOW_HELP_ON_OFF;
+		break;
+	case PROC_SET_PORT_MVRP_REG:
+		if (mrp->mvrp_rx_ports & m) {
+			struct mrp_applicant *app;
+			struct mrp_attr *attr;
+			char *str;
+			__be16 vlan_id = htons(sw->vlan_index);
+
+			app = rcu_dereference(mrp_p->
+					      applicants[vlan_mrp_app.type]);
+			attr = mrp_attr_lookup(app, &vlan_id, sizeof(vlan_id),
+					MVRP_ATTR_VID);
+			str = "unregistered";
+			if (attr) {
+				if (attr->fix_state != MRP_REGISTRAR_LV) {
+					if (attr->fix_state == MRP_REGISTRAR_IN)
+						str = "fixed";
+					else
+						str = "forbidden";
+				} else if (attr->reg_state == MRP_REGISTRAR_IN)
+					str = "registered";
+			}
+			len += sprintf(buf + len,
+				"%s"NL, str);
+		}
+		type = SHOW_HELP_NONE;
+		break;
+#endif
+#ifdef CONFIG_KSZ_MSRP
+	case PROC_SET_PORT_MSRP_ENABLED:
+		chk = info->status.msrpPortEnabledStatus;
+		type = SHOW_HELP_ON_OFF;
+		break;
+#endif
+#ifdef CONFIG_KSZ_AVB
+	case PROC_SET_PORT_ASCAPABLE:
+		chk = cfg->asCapable;
+		if (!chk && cfg->asCapable_set)
+			chk = 2;
+		break;
+	case PROC_SET_TC_DELTA_BANDWIDTH:
+		chk = info->bandwidth[index].deltaBandwidth;
+		break;
+	case PROC_SET_TC_ADMIN_IDLE_MBPS:
+	{
+		u64 val = info->bandwidth[index].adminIdleSlope;
+
+		val *= 10;
+		val *= info->speed;
+		val += 1 << (CREDIT_PERCENTAGE_S - 1);
+		val >>= CREDIT_PERCENTAGE_S;
+		chk = (u32)val;
+		if (chk >= 1000)
+			len += sprintf(buf + len,
+				"%u,%03u Kbps"NL, chk / 1000, chk % 1000);
+		else
+			len += sprintf(buf + len,
+				"%u Kbps"NL, chk);
+		type = SHOW_HELP_NONE;
+		break;
+	}
+	case PROC_SET_TC_ADMIN_IDLE_SLOPE:
+		format_per(per_str, info->bandwidth[index].adminIdleSlope,
+			   sw->verbose);
+		len += sprintf(buf + len,
+			"%s"NL, per_str);
+		type = SHOW_HELP_NONE;
+		break;
+	case PROC_GET_TC_OPER_IDLE_SLOPE:
+		format_per(per_str, info->bandwidth[index].operIdleSlope,
+			   sw->verbose);
+		len += sprintf(buf + len,
+			"%s"NL, per_str);
+		type = SHOW_HELP_NONE;
+		break;
+	case PROC_SET_TC_ALGORITHM:
+		chk = info->algorithm[index].algorithm;
+		if (sw->verbose) {
+			switch (chk) {
+			case 0:
+				strcpy(note, " (strict priority)");
+				break;
+			case 1:
+				strcpy(note, " (credit-based shaping)");
+				break;
+			case 3:
+				strcpy(note, " (weighted round robin)");
+				break;
+			default:
+				strcpy(note, " (reserved)");
+				break;
+			}
+		}
+		type = SHOW_HELP_SPECIAL;
+		break;
+	case PROC_GET_SR_B_RX_PRIO:
+		chk = info->priority[SR_CLASS_B].received_priority;
+		break;
+	case PROC_SET_SR_B_TX_PRIO:
+		chk = info->priority[SR_CLASS_B].regenerated_priority;
+		break;
+	case PROC_GET_SR_B_SRP_DOMAIN_BOUNDARY:
+		chk = info->priority[SR_CLASS_B].SRPdomainBoundaryPort;
+		break;
+	case PROC_SET_SR_B_LATENCY:
+		index = get_traffic_index(SR_CLASS_B);
+		chk = info->latency[index].portTcMaxLatency;
+		break;
+	case PROC_GET_SR_A_RX_PRIO:
+		chk = info->priority[SR_CLASS_A].received_priority;
+		break;
+	case PROC_SET_SR_A_TX_PRIO:
+		chk = info->priority[SR_CLASS_A].regenerated_priority;
+		break;
+	case PROC_GET_SR_A_SRP_DOMAIN_BOUNDARY:
+		chk = info->priority[SR_CLASS_A].SRPdomainBoundaryPort;
+		break;
+	case PROC_SET_SR_A_LATENCY:
+		index = get_traffic_index(SR_CLASS_A);
+		chk = info->latency[index].portTcMaxLatency;
+		break;
+	case PROC_SET_MAX_FRAME_SIZE:
+		chk = info->max_frame_size;
+		break;
+	case PROC_SET_MAX_INTERVAL_FRAMES:
+		chk = info->max_interval_frames;
+		break;
+	case PROC_SET_CLASS_PRIO:
+		type = SHOW_HELP_NONE;
+		break;
+#endif
+	default:
+		type = SHOW_HELP_NONE;
+		break;
+	}
+	return sysfs_show(len, buf, type, chk, note, sw->verbose);
+}  /* sysfs_mrp_port_read */
+
+static int sysfs_mrp_port_write(struct ksz_sw *sw, int proc_num, uint n,
+	int num, const char *buf)
+{
+	struct mrp_info *mrp = &sw->mrp;
+#ifdef CONFIG_KSZ_AVB
+	struct mrp_port_info *info;
+	int index;
+#endif
+	uint m;
+	uint port;
+	struct ksz_port_cfg *cfg;
+#ifdef CONFIG_KSZ_MRP
+	struct mrp_port *mrp_p;
+#endif
+	int processed = true;
+
+#ifdef CONFIG_LAN937X_MRP
+	switch (proc_num) {
+	case PROC_SET_PORT_MMRP_ENABLED:
+	case PROC_SET_PORT_MMRP_MAC:
+	case PROC_SET_PORT_MMRP_SVC:
+	case PROC_SET_PORT_MMRP_REG:
+	case PROC_SET_PORT_MVRP_ENABLED:
+	case PROC_SET_PORT_MVRP_VID:
+	case PROC_SET_PORT_MVRP_REG:
+		if (!(sw->features & MRP_SUPPORT))
+			return true;
+		break;
+	}
+#endif
+	port = get_sysfs_port(sw, n);
+	m = BIT(port);
+	if (!(mrp->mask & m))
+		return false;
+	cfg = get_port_cfg(sw, port);
+#ifdef CONFIG_KSZ_MRP
+	mrp_p = &mrp->mrp_ports[port];
+#endif
+#ifdef CONFIG_KSZ_AVB
+	index = cfg->q_index;
+	info = &mrp->port_info[port];
+#endif
+	switch (proc_num) {
+#ifdef CONFIG_KSZ_MRP
+	case PROC_SET_PORT_MMRP_ENABLED:
+		if (!!(mrp->mmrp_rx_ports & m) != !!num) {
+			if (num) {
+				mrp->mmrp_rx_ports |= m;
+				mrp->mmrp_tx_ports |= m;
+				mrp_start_mmrp_port_app(mrp, port,
+							sw->main_dev);
+			} else {
+				mrp->mmrp_rx_ports &= ~m;
+				mrp_stop_port_app(mrp_p,
+						  &mac_mrp_app, 0, true);
+			}
+		}
+		break;
+	case PROC_SET_PORT_MMRP_MAC:
+		if (mrp->mvrp_rx_ports & m) {
+			int on;
+			struct mrp_applicant *app;
+
+			app = rcu_dereference(mrp_p->
+					      applicants[mac_mrp_app.type]);
+			on = !!(app->normal & (1 << MMRP_ATTR_MAC));
+			if (on != !!num) {
+				if (num) {
+					app->normal |= (1 << MMRP_ATTR_MAC);
+				} else {
+					app->normal &= ~(1 << MMRP_ATTR_MAC);
+					mrp_stop_port_app(mrp_p,
+							  &mac_mrp_app,
+							  MMRP_ATTR_MAC, false);
+				}
+			}
+		}
+		break;
+	case PROC_SET_PORT_MMRP_SVC:
+		if (mrp->mvrp_rx_ports & m) {
+			int on;
+			struct mrp_applicant *app;
+
+			app = rcu_dereference(mrp_p->
+					      applicants[mac_mrp_app.type]);
+			on = !!(app->normal & (1 << MMRP_ATTR_SVC));
+			if (on != !!num) {
+				if (num) {
+					app->normal |= (1 << MMRP_ATTR_SVC);
+				} else {
+					app->normal &= ~(1 << MMRP_ATTR_SVC);
+					mrp_stop_port_app(mrp_p,
+							  &mac_mrp_app,
+							  MMRP_ATTR_SVC, false);
+				}
+			}
+		}
+		break;
+	case PROC_SET_PORT_MMRP_REG:
+		if (mrp->mmrp_rx_ports & m) {
+			struct mrp_applicant *app;
+			enum mrp_registrar_state state;
+			struct ksz_mac_table *entry;
+
+			entry = &sw->info->mac_entry;
+			if (num == 2)
+				state = MRP_REGISTRAR_MT;
+			else if (num == 1)
+				state = MRP_REGISTRAR_IN;
+			else
+				state = MRP_REGISTRAR_LV;
+			app = rcu_dereference(mrp_p->
+					      applicants[mac_mrp_app.type]);
+			mmrp_req_set_mac(app, entry->addr, state);
+		}
+		break;
+	case PROC_SET_PORT_MVRP_ENABLED:
+		if (!!(mrp->mvrp_rx_ports & m) != !!num) {
+			if (num) {
+				mrp->mvrp_rx_ports |= m;
+				mrp->mvrp_tx_ports |= m;
+				mrp_start_mvrp_port_app(mrp, port,
+							sw->main_dev);
+			} else {
+				mrp->mvrp_rx_ports &= ~m;
+				mrp_stop_port_app(mrp_p,
+						  &vlan_mrp_app, 0, true);
+			}
+		}
+		break;
+	case PROC_SET_PORT_MVRP_VID:
+		if (mrp->mvrp_rx_ports & m) {
+			int on;
+			struct mrp_applicant *app;
+
+			app = rcu_dereference(mrp_p->
+					      applicants[vlan_mrp_app.type]);
+			on = !!(app->normal & (1 << MVRP_ATTR_VID));
+			if (on != !!num) {
+				if (num) {
+					app->normal |= (1 << MVRP_ATTR_VID);
+				} else {
+					app->normal &= ~(1 << MVRP_ATTR_VID);
+					mrp_stop_port_app(mrp_p,
+							  &vlan_mrp_app,
+							  MVRP_ATTR_VID, false);
+				}
+			}
+		}
+		break;
+	case PROC_SET_PORT_MVRP_REG:
+		if (mrp->mvrp_rx_ports & m) {
+			struct mrp_applicant *app;
+			enum mrp_registrar_state state;
+
+			if (num == 2)
+				state = MRP_REGISTRAR_MT;
+			else if (num == 1)
+				state = MRP_REGISTRAR_IN;
+			else
+				state = MRP_REGISTRAR_LV;
+			app = rcu_dereference(mrp_p->
+					      applicants[vlan_mrp_app.type]);
+			mvrp_req_set(app, sw->vlan_index, state);
+		}
+		break;
+#endif
+#ifdef CONFIG_KSZ_MSRP
+	case PROC_SET_PORT_MSRP_ENABLED:
+	{
+		if (!(sw->features & AVB_SUPPORT))
+			break;
+		if (info->status.msrpPortEnabledStatus == !!num)
+			break;
+
+		if (!info->status.msrpPortEnabledStatus) {
+			uint l = get_log_port(sw, port);
+
+			mutex_lock(&mrp->lock);
+			mrp_reset_reserv(mrp, l, l);
+			info->status.msrpPortEnabledStatus = true;
+			mrp_start_msrp_port_app(mrp, port, sw->main_dev);
+			msrp_open_port(mrp, port);
+			mutex_unlock(&mrp->lock);
+			mrp_chk_registered(mrp, port);
+		} else {
+			mutex_lock(&mrp->lock);
+			mrp_stop_port_app(mrp_p,
+					  &srp_mrp_app, 0, true);
+			info->status.msrpPortEnabledStatus = false;
+			mutex_unlock(&mrp->lock);
+		}
+		break;
+	}
+#endif
+#ifdef CONFIG_KSZ_AVB
+	case PROC_SET_PORT_ASCAPABLE:
+		if (!(sw->features & AVB_SUPPORT))
+			break;
+		cfg->asCapable_set = !!num;
+		if (sw->features & MRP_SUPPORT)
+			break;
+		break;
+	case PROC_SET_TC_DELTA_BANDWIDTH:
+		if (index == mrp->queue[SR_CLASS_B] && num >= 0 &&
+		    num + info->deltaBandwidth <= 95 &&
+		    num + info->deltaBandwidth > 0)
+			mrp_set_delta(mrp, port, info->deltaBandwidth, num);
+		if (index == mrp->queue[SR_CLASS_A] && num >= 0 && num <= 95)
+			info->deltaBandwidth = num;
+		break;
+	case PROC_SET_TC_ADMIN_IDLE_MBPS:
+		if (!info->speed)
+			break;
+		if (num >= info->speed * 1000)
+			num = (info->speed * 1000) - 1;
+		num *= 100;
+		num /= info->speed;
+		fallthrough;
+
+	case PROC_SET_TC_ADMIN_IDLE_SLOPE:
+		if (num >= 0 && num < 100000) {
+			mrp_set_slope(mrp, port, index, info, num);
+		}
+		break;
+	case PROC_SET_TC_ALGORITHM:
+		if (0 <= num && num < 2)
+			info->algorithm[index].algorithm = num;
+		else if (3 == num)
+			info->algorithm[index].algorithm = num;
+
+#if defined(CONFIG_HAVE_KSZ9897) || defined(CONFIG_HAVE_LAN937X)
+		sw->reg->lock(sw);
+		if (0 == num || 1 == num) {
+			port_set_schedule_mode(sw, port, index,
+				MTI_SCHEDULE_STRICT_PRIO);
+			port_set_shaping(sw, port, index,
+				1 == num ? MTI_SHAPING_SRP : MTI_SHAPING_OFF);
+		} else if (3 == num) {
+			port_set_schedule_mode(sw, port, index,
+				MTI_SCHEDULE_WRR);
+			port_set_shaping(sw, port, index,
+				MTI_SHAPING_OFF);
+		}
+		sw->reg->unlock(sw);
+#endif
+		break;
+	case PROC_SET_SR_B_TX_PRIO:
+		if (0 <= num && num < 8)
+			info->priority[SR_CLASS_B].regenerated_priority = num;
+		break;
+	case PROC_SET_SR_B_LATENCY:
+		index = get_traffic_index(SR_CLASS_B);
+		if (0 <= num && num < 100000)
+			info->latency[index].portTcMaxLatency = num;
+		break;
+	case PROC_SET_SR_A_TX_PRIO:
+		if (0 <= num && num < 8)
+			info->priority[SR_CLASS_A].regenerated_priority = num;
+		break;
+	case PROC_SET_SR_A_LATENCY:
+		index = get_traffic_index(SR_CLASS_A);
+		if (0 <= num && num < 100000)
+			info->latency[index].portTcMaxLatency = num;
+		break;
+	case PROC_SET_MAX_FRAME_SIZE:
+		if (num < 46)
+			num = 46;
+		info->max_frame_size = num;
+		break;
+	case PROC_SET_MAX_INTERVAL_FRAMES:
+		if (num < 1)
+			num = 1;
+		info->max_interval_frames = num;
+		break;
+	case PROC_SET_CLASS_PRIO:
+		if (0 <= num && num <= 7) {
+			u32 cnt;
+
+			cnt = 0;
+			if (num == 2)
+				cnt = 4000;
+			else if (num == 3)
+				cnt = 8000;
+			if (cnt && info->speed) {
+				u32 queue = cfg->tc_map[0];
+				u32 overhead;
+				u32 rate;
+				u16 data;
+
+				queue >>= num * PORT_TC_MAP_S;
+				queue &= PORT_TC_MAP_M;
+				overhead = 22;
+				sw->ops->acquire(sw);
+				data = sw->reg->r16(sw, REG_AVB_STRATEGY__2);
+				sw->ops->release(sw);
+				if (data & SW_SHAPING_CREDIT_ACCT)
+					overhead += 20;
+				rate = info->max_frame_size;
+				rate += overhead;
+				rate *= info->max_interval_frames;
+				rate *= cnt;
+				rate *= 8;
+				rate /= 1000;
+				if (rate >= info->speed * 1000)
+					rate = (info->speed * 1000) - 1;
+				rate *= 100;
+				rate /= info->speed;
+				mrp_set_slope(mrp, port, queue, info, rate);
+			}
+		}
+		break;
+#endif
+	default:
+		processed = false;
+		break;
+	}
+	return processed;
+}  /* sysfs_mrp_port_write */
+
+#ifdef CONFIG_KSZ_MRP
+static void leave_mrp(struct mrp_info *mrp)
+{
+}  /* leave_mrp */
+#endif
+
+static void mrp_init(struct mrp_info *mrp)
+{
+	struct ksz_sw *sw = mrp->parent;
+
+#ifdef CONFIG_KSZ_AVB
+	struct mrp_traffic_info *traffic;
+	struct mrp_port_info *info;
+	char bw_str1[20];
+	char bw_str2[20];
+	char bw_str3[20];
+	char bw_str4[20];
+	char bw_str5[20];
+	uint port;
+	int index;
+	int tc;
+#endif
+
+	mrp->access = create_singlethread_workqueue("mrp_access");
+	init_mrp_work(mrp);
+	mutex_init(&mrp->lock);
+	skb_queue_head_init(&mrp->rxq);
+	skb_queue_head_init(&mrp->macq);
+	skb_queue_head_init(&mrp->vlanq);
+#ifdef CONFIG_KSZ_MSRP
+	INIT_WORK(&mrp->cfg_mac, mrp_cfg_mac_work);
+#endif
+#ifdef CONFIG_KSZ_MRP
+	INIT_WORK(&mrp->cfg_vlan, mrp_cfg_vlan_work);
+	INIT_WORK(&mrp->rx_proc, mrp_rx_proc);
+#endif
+
+	/* Number of ports can be capped for testing purpose. */
+	mrp->ports = sw->mib_port_cnt;
+	mrp->mask = sw->PORT_MASK;
+	mrp->other = sw->PORT_MASK & ~sw->HOST_MASK;
+	mrp->mmrp_rx_ports = mrp->mmrp_tx_ports = mrp->mask;
+	mrp->mvrp_rx_ports = mrp->mvrp_tx_ports = mrp->mask;
+
+#ifdef CONFIG_KSZ_MRP
+	mrp_init_list(&mrp->mac_list);
+	mrp_init_list(&mrp->vlan_list);
+#endif
+
+#ifdef CONFIG_KSZ_AVB
+	mrp->id = sw->ops->get_br_id(sw);
+	mrp->tc[2] = SR_CLASS_B;
+	mrp->tc[3] = SR_CLASS_A;
+	mrp->prio[SR_CLASS_B] = 2;
+	mrp->prio[SR_CLASS_A] = 3;
+	mrp->queue[SR_CLASS_B] = 2;
+	mrp->queue[SR_CLASS_A] = 3;
+	mrp->domain[0].id = SR_CLASS_B;
+	mrp->domain[0].priority = mrp->prio[SR_CLASS_B];
+	mrp->domain[0].vlan_id = 2;
+	mrp->domain[1].id = SR_CLASS_A;
+#if 0
+	mrp->domain[1].id = 0;
+#endif
+	mrp->domain[1].priority = mrp->prio[SR_CLASS_A];
+	mrp->domain[1].vlan_id = 2;
+	mrp->max_interference_size = 1500;
+
+#ifdef CONFIG_KSZ_MSRP
+	mrp_init_list(&mrp->mac_down);
+	mrp_init_list(&mrp->mac_up);
+	mrp_init_list(&mrp->vlan_down);
+	mrp_init_list(&mrp->vlan_up);
+#endif
+
+	for (port = 0; port < sw->port_cnt; port++) {
+		info = &mrp->port_info[port];
+		info->index = port;
+		info->max_frame_size = 46;
+		info->max_interval_frames = 1;
+#ifdef CONFIG_KSZ_MSRP
+		if (mrp->status.msrpEnabledStatus)
+			info->status.msrpPortEnabledStatus = true;
+#endif
+		info->speed = 100;
+		if (sw->port_info[port].tx_rate)
+			info->speed = sw->port_info[port].tx_rate /
+				TX_RATE_UNIT;
+
+		tc = mrp->queue[SR_CLASS_A];
+		info->bandwidth[tc].deltaBandwidth = 50;
+#if 0
+		info->bandwidth[tc].deltaBandwidth = 75;
+#endif
+		info->bandwidth[tc].adminIdleSlope = 75 << CREDIT_PERCENTAGE_S;
+		info->bandwidth[tc].operIdleSlope =
+			info->bandwidth[tc].adminIdleSlope;
+		info->deltaBandwidth = info->bandwidth[tc].deltaBandwidth;
+
+		tc = mrp->queue[SR_CLASS_B];
+		info->bandwidth[tc].deltaBandwidth = 25;
+#if 0
+		info->bandwidth[tc].deltaBandwidth = 0;
+#endif
+		info->bandwidth[tc].adminIdleSlope = 75 << CREDIT_PERCENTAGE_S;
+		info->bandwidth[tc].operIdleSlope =
+			info->bandwidth[tc].adminIdleSlope;
+
+		tc = 0;
+		info->bandwidth[tc].deltaBandwidth = 0;
+		info->bandwidth[tc].adminIdleSlope = 75 << CREDIT_PERCENTAGE_S;
+		info->bandwidth[tc].operIdleSlope =
+			info->bandwidth[tc].adminIdleSlope;
+
+		for (tc = 0; tc < PRIO_QUEUES; tc++) {
+			info->bandwidth[tc].traffic_class = tc;
+			info->algorithm[tc].traffic_class = tc;
+		}
+		for (tc = 0; tc < PRIO_QUEUES - 2; tc++)
+			info->algorithm[tc].algorithm = 3;
+		for (; tc < PRIO_QUEUES; tc++)
+			info->algorithm[tc].algorithm = 1;
+
+		for (tc = SR_CLASS_A; tc >= SR_CLASS_B; tc--) {
+			index = get_traffic_index(tc);
+			info->latency[index].traffic_class = tc;
+			info->latency[index].portTcMaxLatency = 1000;
+			if (tc == SR_CLASS_A)
+				info->latency[index].portTcMaxLatency = 50000;
+			if (tc == SR_CLASS_B)
+				info->latency[index].portTcMaxLatency = 50000;
+
+			info->priority[tc].received_priority =
+				mrp->prio[tc];
+			info->priority[tc].regenerated_priority = 0;
+			if (port != sw->HOST_PORT)
+				info->priority[tc].SRPdomainBoundaryPort = 1;
+
+			traffic = get_traffic_info(info, tc);
+			traffic->queue = get_queue_priority(mrp, tc);
+
+#ifdef CONFIG_KSZ_MSRP
+			mrp_init_list(&traffic->active);
+			mrp_init_list(&traffic->passive);
+#endif
+		}
+		info->traffic[1].bandwidth_avail =
+			&info->traffic[0].bandwidth_delta;
+		info->traffic[1].bandwidth_other =
+			&info->traffic[0].bandwidth_max;
+		info->traffic[0].bandwidth_avail = NULL;
+		info->traffic[0].bandwidth_other = NULL;
+		mrp_set_bandwidth(info);
+
+		format_num(bw_str1, info->bandwidth_left);
+		format_num(bw_str2, info->traffic[1].bandwidth_max);
+		format_num(bw_str3, info->traffic[1].bandwidth_left);
+		format_num(bw_str4, info->traffic[0].bandwidth_max);
+		format_num(bw_str5, info->traffic[0].bandwidth_left);
+if (port < 2)
+dbg_msg("bw: %d %s; %s %s; %s %s"NL, port,
+bw_str1, bw_str2, bw_str3, bw_str4, bw_str5);
+	}
+#endif
+dbg_msg("%s %x %x"NL, __func__, SRP_PORT_AVAIL, SRP_PORT_OTHER);
+}  /* mrp_init */
+
+static void mrp_exit(struct mrp_info *mrp)
+{
+	bool last;
+	struct sk_buff *skb;
+
+#ifdef CONFIG_KSZ_MSRP
+	flush_work(&mrp->cfg_mac);
+#endif
+#ifdef CONFIG_KSZ_MRP
+	flush_work(&mrp->cfg_vlan);
+	flush_work(&mrp->rx_proc);
+#endif
+	last = skb_queue_empty(&mrp->rxq);
+	while (!last) {
+		skb = skb_dequeue(&mrp->rxq);
+		if (skb)
+			kfree_skb(skb);
+		last = skb_queue_empty(&mrp->rxq);
+	}
+	last = skb_queue_empty(&mrp->macq);
+	while (!last) {
+		skb = skb_dequeue(&mrp->macq);
+		if (skb)
+			kfree_skb(skb);
+		last = skb_queue_empty(&mrp->macq);
+	}
+	last = skb_queue_empty(&mrp->vlanq);
+	while (!last) {
+		skb = skb_dequeue(&mrp->vlanq);
+		if (skb)
+			kfree_skb(skb);
+		last = skb_queue_empty(&mrp->vlanq);
+	}
+	exit_mrp_work(mrp);
+	if (mrp->access) {
+		destroy_workqueue(mrp->access);
+		mrp->access = NULL;
+	}
+}  /* mrp_exit */
+
+static struct mrp_ops mrp_ops = {
+	.init			= mrp_init,
+	.exit			= mrp_exit,
+
+	.dev_req		= mrp_dev_req,
+
+#ifdef CONFIG_KSZ_MRP
+	.from_backup		= mrp_from_backup,
+	.to_backup		= mrp_to_backup,
+	.from_designated	= mrp_from_designated,
+	.to_designated		= mrp_to_designated,
+	.tc_detected		= mrp_tc_detected,
+
+#ifdef CONFIG_KSZ_MSRP
+	.chk_talker		= mrp_chk_talker,
+#endif
+	.setup_vlan		= mrp_setup_vlan,
+#endif
+};
+
diff -Napur linux-linux4microchip-2022.10-orig/drivers/net/ethernet/micrel/ksz_mrp.h linux-linux4microchip-2022.10/drivers/net/ethernet/micrel/ksz_mrp.h
--- linux-linux4microchip-2022.10-orig/drivers/net/ethernet/micrel/ksz_mrp.h	1969-12-31 16:00:00.000000000 -0800
+++ linux-linux4microchip-2022.10/drivers/net/ethernet/micrel/ksz_mrp.h	2023-04-25 16:13:54.880163472 -0700
@@ -0,0 +1,370 @@
+/**
+ * Microchip MRP driver header
+ *
+ * Copyright (c) 2015-2019 Microchp Technology Inc.
+ *	Tristram Ha <Tristram.Ha@microchip.com>
+ *
+ * Copyright (c) 2014-2015 Micrel, Inc.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ */
+
+
+#ifndef KSZ_MRP_H
+#define KSZ_MRP_H
+
+#include "ksz_mrp_api.h"
+
+#if 0
+#define DBG_MRP
+#endif
+#if 0
+#define DBG_MRP_APP
+#endif
+#if 0
+#define DBG_MRP_REG
+#endif
+#if 0
+#define DBG_MRP_RX
+#endif
+#if 0
+#define DBG_MRP_TX
+#endif
+#if 0
+#define MRP_BASIC
+#endif
+#ifdef CONFIG_KSZ_MRP
+#include "mrp.h"
+#endif
+
+
+struct mrp_node {
+	void *data;
+	struct mrp_node *next;
+};
+
+struct mrp_node_anchor {
+	struct mrp_node anchor;
+	struct mrp_node *last;
+	int cnt;
+};
+
+#ifdef CONFIG_KSZ_MSRP
+struct SRP_bridge_base {
+	uint msrpEnabledStatus:1;
+	uint msrpTalkerPruning:1;
+	uint msrpMaxFanInPorts;
+	uint msrpLatencyMaxFrameSize;
+};
+
+struct SRP_bridge_port {
+	uint msrpPortEnabledStatus:1;
+	uint Failed_Registrations;
+	u8 Last_PDU_Origin[ETH_ALEN];
+	u16 SR_PVID;
+};
+
+struct SRP_reserv;
+
+struct SRP_stream {
+	u8 id[8];
+	u8 dest[ETH_ALEN];
+	u16 vlan_id;
+	u16 MaxFrameSize;
+	u16 MaxIntervalFrames;
+	u8 reserved:4;
+	u8 rank:1;
+	u8 priority:3;
+	u32 latency;
+	u32 bandwidth;
+
+	u8 in_port;
+	struct SRP_reserv *t_reserv;
+
+	struct SRP_stream *id_prev;
+	struct SRP_stream *id_next;
+	struct SRP_stream *dest_prev;
+	struct SRP_stream *dest_next;
+} __packed;
+
+enum {
+	SRP_TALKER,
+	SRP_LISTENER,
+};
+
+#define SRP_ASKING_FAILED_SCALE		(1 << SRP_ASKING_FAILED)
+#define SRP_READY_SCALE			(1 << SRP_READY)
+#define SRP_READY_FAILED_SCALE		(1 << SRP_READY_FAILED)
+
+struct SRP_reserv {
+	u8 id[8];
+	u8 direction;
+	u8 declaration;
+	u32 latency;
+	u8 bridge_id[8];
+	u8 rx_code;
+	u8 code;
+	u32 code_bits;
+	u16 tx_ports;
+
+	uint dropped_frames;
+	u64 streamAge;
+	unsigned long ticks;
+
+	struct SRP_stream *stream;
+	struct SRP_reserv *pair;
+
+	struct SRP_reserv *next;
+	struct SRP_reserv *prev;
+};
+#endif
+
+#ifdef CONFIG_KSZ_AVB
+struct SRP_bandwidth_availability_parameter {
+	u8 traffic_class;
+	int deltaBandwidth;
+	u32 adminIdleSlope;
+	u32 operIdleSlope;
+};
+
+struct SRP_latency_parameter {
+	u8 traffic_class;
+	u32 portTcMaxLatency;
+};
+
+struct SRP_priority_regeneration_override {
+	u8 received_priority;
+	u8 regenerated_priority;
+	int SRPdomainBoundaryPort;
+};
+
+struct SRP_transmission_selection_algorithm {
+	u8 traffic_class;
+	int algorithm;
+};
+
+struct mrp_traffic_info {
+	u32 bandwidth_delta;
+	u32 bandwidth_max;
+	u32 bandwidth_left;
+	u32 bandwidth_used;
+	u32 bandwidth_set;
+	u32 *bandwidth_avail;
+	u32 *bandwidth_other;
+	u32 max_frame_size;
+	u8 queue;
+
+#ifdef CONFIG_KSZ_MSRP
+	struct mrp_node_anchor active;
+	struct mrp_node_anchor passive;
+#endif
+};
+#endif
+
+struct mrp_report {
+	void *attrib;
+	u8 action;
+	u8 type;
+	u8 port;
+	struct mrp_report *next;
+};
+
+#define SRP_PORT_AVAIL			(mrp->mask)
+#define SRP_PORT_OTHER			(mrp->other)
+#define SRP_PORT_SET			(1 << 11)
+#define SRP_PORT_DROP			(1 << 12)
+#define SRP_PORT_IGNORE			(1 << 13)
+#define SRP_PORT_BLACKLIST		(1 << 14)
+#define SRP_PORT_READY			(1 << 15)
+
+struct mrp_mac_info {
+	u16 fid;
+	u8 addr[ETH_ALEN];
+	u16 ports;
+	u16 set_ports;
+	u16 mrp_ports;
+	u16 srp_ports;
+	u16 rx_ports;
+	u16 tx_ports;
+	u8 index;
+	unsigned long ticks;
+};
+
+struct mrp_vlan_info {
+	u16 vid;
+	u16 fid;
+	u8 addr[ETH_ALEN];
+	u16 ports;
+	u16 set_ports;
+	u16 rx_ports;
+	u16 tx_ports;
+	u8 index;
+};
+
+struct srp_stream_info {
+	struct SRP_reserv *reserv;
+	u8 *id;
+	u64 age;
+	u8 rank;
+	u8 mark:1;
+};
+
+#ifdef CONFIG_KSZ_AVB
+struct mrp_port_info {
+	u32 bandwidth_max;
+	u32 bandwidth_left;
+	u32 bandwidth_used;
+	u32 speed;
+	u32 max_frame_size;
+	u32 max_interval_frames;
+	u64 age;
+	u32 link:1;
+	u32 duplex:1;
+	u8 index;
+	u16 credit[PRIO_QUEUES];
+	int deltaBandwidth;
+	struct mrp_traffic_info traffic[2];
+
+	struct SRP_bandwidth_availability_parameter bandwidth[PRIO_QUEUES];
+	struct SRP_latency_parameter latency[2];
+	struct SRP_priority_regeneration_override priority[SR_CLASS_NUM];
+	struct SRP_transmission_selection_algorithm algorithm[PRIO_QUEUES];
+
+#ifdef CONFIG_KSZ_MSRP
+	struct SRP_bridge_port status;
+
+	struct SRP_reserv declared;
+	struct SRP_reserv registered;
+#endif
+};
+#endif
+
+struct mrp_info;
+
+struct mrp_work {
+	struct work_struct work;
+	struct completion done;
+	struct mrp_info *mrp;
+	struct sk_buff *skb;
+	int cmd;
+	int subcmd;
+	int option;
+	int output;
+	int result;
+	int used;
+	int index;
+	union {
+		struct mrp_cfg_options cfg;
+		u8 data[8];
+	} param;
+	struct mrp_work *prev;
+};
+
+#define MRP_WORK_NUM			(1 << 4)
+#define MRP_WORK_LAST			(MRP_WORK_NUM - 1)
+
+struct mrp_access {
+	struct work_struct work;
+	int index;
+	int head;
+	int tail;
+	struct mrp_work works[MRP_WORK_NUM];
+};
+
+struct mrp_ops {
+	void (*init)(struct mrp_info *mrp);
+	void (*exit)(struct mrp_info *mrp);
+	int (*dev_req)(struct mrp_info *mrp, char *arg);
+
+	void (*from_backup)(struct mrp_info *mrp, uint p);
+	void (*to_backup)(struct mrp_info *mrp, uint p);
+	void (*from_designated)(struct mrp_info *mrp, uint p, bool alt);
+	void (*to_designated)(struct mrp_info *mrp, uint p);
+	void (*tc_detected)(struct mrp_info *mrp, uint p);
+
+	void (*chk_talker)(struct mrp_info *mrp, u8 port);
+
+	void (*setup_vlan)(struct mrp_info *mrp, u16 vid,
+			   struct ksz_vlan_table *vlan);
+};
+
+struct mrp_info {
+	void *parent;
+	struct mutex lock;
+	struct mrp_access hw_access;
+	struct workqueue_struct *access;
+	struct sk_buff_head rxq;
+	struct sk_buff_head txq;
+	struct sk_buff_head macq;
+	struct sk_buff_head vlanq;
+	int macq_sched;
+	int vlanq_sched;
+	struct work_struct cfg_mac;
+	struct work_struct cfg_vlan;
+	struct work_struct rx_proc;
+	u8 version;
+	u8 ports;
+	u8 started;
+	u32 mask;
+	u32 other;
+	u32 rx_ports;
+	u32 tx_ports;
+	u32 mmrp_rx_ports;
+	u32 mmrp_tx_ports;
+	u32 mvrp_rx_ports;
+	u32 mvrp_tx_ports;
+	uint no_report;
+	int listeners;
+
+	const struct mrp_ops *ops;
+
+#ifdef CONFIG_KSZ_AVB
+	const u8 *id;
+	u8 tc[8];
+	u8 prio[SR_CLASS_NUM];
+	u8 queue[SR_CLASS_NUM];
+	u32 max_interference_size;
+	u16 mcast_ports;
+	int mcast_port_cnt;
+
+	struct mrp_port_info port_info[TOTAL_PORT_NUM];
+
+	struct SRP_domain_class domain[2];
+
+#ifdef CONFIG_KSZ_MSRP
+	struct SRP_bridge_base status;
+
+	struct SRP_stream stream_by_id;
+	struct SRP_stream stream_by_dest;
+
+	struct mrp_node_anchor mac_down;
+	struct mrp_node_anchor mac_up;
+	struct mrp_node_anchor vlan_down;
+	struct mrp_node_anchor vlan_up;
+#endif
+#endif
+
+#ifdef CONFIG_KSZ_MRP
+	struct mrp_node_anchor mac_list;
+	struct mrp_node_anchor vlan_list;
+
+	struct mrp_report *report_head;
+	struct mrp_report *report_tail;
+
+	struct mrp_port mrp_ports[TOTAL_PORT_NUM];
+	int mac_tx[TOTAL_PORT_NUM];
+	int vlan_tx[TOTAL_PORT_NUM];
+	int srp_tx[TOTAL_PORT_NUM];
+	u8 cvlan_addr[ETH_ALEN];
+	u8 svlan_addr[ETH_ALEN];
+#endif
+};
+
+#endif
diff -Napur linux-linux4microchip-2022.10-orig/drivers/net/ethernet/micrel/ksz_mstp.c linux-linux4microchip-2022.10/drivers/net/ethernet/micrel/ksz_mstp.c
--- linux-linux4microchip-2022.10-orig/drivers/net/ethernet/micrel/ksz_mstp.c	1969-12-31 16:00:00.000000000 -0800
+++ linux-linux4microchip-2022.10/drivers/net/ethernet/micrel/ksz_mstp.c	2023-10-11 12:24:05.000000000 -0700
@@ -0,0 +1,6520 @@
+/**
+ * Microchip MSTP code
+ *
+ * Copyright (c) 2016-2021 Microchip Technology Inc.
+ *	Tristram Ha <Tristram.Ha@microchip.com>
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ */
+
+
+#include <crypto/hash.h>
+
+
+#if 0
+#define DBG_STP_STATE
+#ifdef DBG_STP_STATE
+#if 1
+#endif
+#if 0
+#define DBG_STP_STATE_RX
+#define DBG_STP_STATE_PROTO
+#define DBG_STP_STATE_INFO
+#define DBG_STP_STATE_ROLE_TR
+#define DBG_STP_STATE_TX
+#define DBG_STP_STATE_TR
+#define DBG_STP_STATE_TOPOLOGY
+#endif
+#endif
+#endif
+
+#if 0
+#define DBG_STP_PORT_BLOCK
+#endif
+#if 0
+#define DBG_STP_PORT_FLUSH
+#endif
+#if 0
+#define DBG_STP_ROLE
+#endif
+#if 0
+#define DBG_STP_RX
+#endif
+#if 0
+#define DBG_STP_TX
+#endif
+
+
+#ifndef FALSE
+#define FALSE	0
+#define TRUE	1
+#endif
+
+
+#define BPDU_TYPE_CONFIG	0
+#define BPDU_TYPE_CONFIG_RSTP	2
+#define BPDU_TYPE_TCN		0x80
+
+
+#define TOPOLOGY_CHANGE		(1 << 0)
+#define PROPOSAL		(1 << 1)
+#define PORT_ROLE_S		2
+#define PORT_ROLE_MASTER	0
+#define PORT_ROLE_ALTERNATE	1
+#define PORT_ROLE_ROOT		2
+#define PORT_ROLE_DESIGNATED	3
+#define LEARNING		(1 << 4)
+#define FORWARDING		(1 << 5)
+#define AGREEMENT		(1 << 6)
+#define TOPOLOGY_CHANGE_ACK	(1 << 7)
+#define MASTER			(1 << 7)
+
+
+static u8 CFG_DIGEST_SIGN_KEY[] = {
+	0x13, 0xAC, 0x06, 0xA6, 0x2E, 0x47, 0xFD, 0x51,
+	0xF9, 0x5D, 0x2B, 0xA2, 0x43, 0xCD, 0x03, 0x46
+};
+
+#if 0
+static u8 ALL_VID_CIST[] = {
+	0,
+	'A', 'L', 'L', 0, 0, 0, 0, 0,
+	0, 0, 0, 0, 0, 0, 0, 0,
+	0, 0, 0, 0, 0, 0, 0, 0,
+	0, 0, 0, 0, 0, 0, 0, 0,
+	0, 0,
+	0xAC, 0x36, 0x17, 0x7F, 0x50, 0x28, 0x3C, 0xD4,
+	0xB8, 0x38, 0x21, 0xD8, 0xAB, 0x26, 0xDE, 0x62
+};
+
+static u8 ALL_VID_MSTID_1[] = {
+	0,
+	'M', 'S', 'T', 'I', 'D', '1', 0, 0,
+	0, 0, 0, 0, 0, 0, 0, 0,
+	0, 0, 0, 0, 0, 0, 0, 0,
+	0, 0, 0, 0, 0, 0, 0, 0,
+	0, 0,
+	0xE1, 0x3A, 0x80, 0xF1, 0x1E, 0xD0, 0x85, 0x6A,
+	0xCD, 0x4E, 0xE3, 0x47, 0x69, 0x41, 0xC7, 0x3B
+};
+
+static u8 VID_MOD_32_MSTID[] = {
+	0,
+	'M', 'S', 'T', 'I', 'D', 0, 0, 0,
+	0, 0, 0, 0, 0, 0, 0, 0,
+	0, 0, 0, 0, 0, 0, 0, 0,
+	0, 0, 0, 0, 0, 0, 0, 0,
+	0, 0,
+	0x9D, 0x14, 0x5C, 0x26, 0x7D, 0xBE, 0x9F, 0xB5,
+	0xD8, 0x93, 0x44, 0x1B, 0xE3, 0xBA, 0x08, 0xCE
+};
+#endif
+
+static u16 get_bpdu_time(u16 time)
+{
+	int val;
+
+	val = ntohs(time);
+
+	/* Round up to whole second. */
+	val += 255;
+	return (u16)(val / 256);
+}  /* get_bpdu_time */
+
+static void set_bpdu_time(u16 *dst, u16 src)
+{
+	if (src < 256) {
+		src *= 256;
+		*dst = htons(src);
+	} else
+		*dst = 0xffff;
+}  /* set_bpdu_time */
+
+static void set_bpdu_times(struct bpdu *bpdu, struct mstp_times *t,
+	struct mstp_times *h)
+{
+	set_bpdu_time(&bpdu->message_age, t->message_age);
+	set_bpdu_time(&bpdu->max_age, t->max_age);
+	set_bpdu_time(&bpdu->hello_time, h->hello_time);
+	set_bpdu_time(&bpdu->forward_delay, t->forward_delay);
+}  /* set_bpdu_times */
+
+static void prep_bpdu(struct bpdu *bpdu, struct stp_prio *p,
+	struct mstp_times *t, struct mstp_times *h)
+{
+	bpdu->protocol = 0;
+	bpdu->flags = 0;
+	memcpy(&bpdu->root, p, sizeof(struct stp_prio));
+	set_bpdu_times(bpdu, t, h);
+	bpdu->version_1_length = 0;
+}  /* prep_bpdu */
+
+static void prep_stp(struct bpdu *bpdu, struct stp_prio *p,
+	struct mstp_times *t, struct mstp_times *h)
+{
+	prep_bpdu(bpdu, p, t, h);
+	bpdu->version = 0;
+	bpdu->type = BPDU_TYPE_CONFIG;
+}  /* prep_stp */
+
+static void prep_rstp(struct bpdu *bpdu, struct stp_prio *p,
+	struct mstp_times *t, struct mstp_times *h)
+{
+	prep_bpdu(bpdu, p, t, h);
+	bpdu->version = 2;
+	bpdu->type = BPDU_TYPE_CONFIG_RSTP;
+}  /* prep_rstp */
+
+#if defined(DBG_STP_RX) || defined(DBG_STP_TX)
+static void disp_bpdu(struct bpdu *bpdu)
+{
+	u8 role;
+
+	if (BPDU_TYPE_TCN != bpdu->type) {
+		dbg_msg("%04x=%02x%02x%02x%02x%02x%02x "
+			"%04x=%02x%02x%02x%02x%02x%02x:"
+			"%02x%02x %u\n",
+			ntohs(bpdu->root.prio),
+			bpdu->root.addr[0],
+			bpdu->root.addr[1],
+			bpdu->root.addr[2],
+			bpdu->root.addr[3],
+			bpdu->root.addr[4],
+			bpdu->root.addr[5],
+			ntohs(bpdu->bridge_id.prio),
+			bpdu->bridge_id.addr[0],
+			bpdu->bridge_id.addr[1],
+			bpdu->bridge_id.addr[2],
+			bpdu->bridge_id.addr[3],
+			bpdu->bridge_id.addr[4],
+			bpdu->bridge_id.addr[5],
+			bpdu->port_id.prio, bpdu->port_id.num,
+			ntohl(bpdu->root_path_cost));
+		dbg_msg("%u %u %u %u  ",
+			htons(bpdu->message_age) / 256,
+			htons(bpdu->max_age) / 256,
+			htons(bpdu->hello_time) / 256,
+			htons(bpdu->forward_delay) / 256);
+		dbg_msg("%02X:", bpdu->flags);
+		if (bpdu->flags & TOPOLOGY_CHANGE_ACK)
+			dbg_msg("K");
+		else
+			dbg_msg("-");
+		if (bpdu->flags & AGREEMENT)
+			dbg_msg("A");
+		else
+			dbg_msg("-");
+		if (bpdu->flags & FORWARDING)
+			dbg_msg("F");
+		else
+			dbg_msg("-");
+		if (bpdu->flags & LEARNING)
+			dbg_msg("L");
+		else
+			dbg_msg("-");
+		role = bpdu->flags >> PORT_ROLE_S;
+		role &= PORT_ROLE_DESIGNATED;
+		switch (role) {
+		case PORT_ROLE_ALTERNATE:
+			dbg_msg("N");
+			break;
+		case PORT_ROLE_ROOT:
+			dbg_msg("R");
+			break;
+		case PORT_ROLE_DESIGNATED:
+			dbg_msg("D");
+			break;
+		default:
+			if (BPDU_TYPE_CONFIG_RSTP == bpdu->type)
+				dbg_msg("?");
+			else
+				dbg_msg("-");
+		}
+		if (bpdu->flags & PROPOSAL)
+			dbg_msg("P");
+		else
+			dbg_msg("-");
+		if (bpdu->flags & TOPOLOGY_CHANGE)
+			dbg_msg("T");
+		else
+			dbg_msg("-");
+	}
+	dbg_msg("  %04x %u %02x",
+		htons(bpdu->protocol),
+		bpdu->version, bpdu->type);
+	if (BPDU_TYPE_CONFIG_RSTP == bpdu->type)
+		dbg_msg(" %u", bpdu->version_1_length);
+	dbg_msg("\n");
+	if (BPDU_TYPE_CONFIG_RSTP == bpdu->type && bpdu->version >= 3) {
+		int i;
+		int len;
+		struct msti_cfg *cfg;
+		struct mstp_bpdu *mstp = &bpdu->mstp;
+
+		len = ntohs(mstp->version_3_length);
+		len += 2;
+		for (i = 0; i < 16; i++)
+			dbg_msg("%02X", mstp->MCID.digest[i]);
+		dbg_msg("  %u %u  ", mstp->MCID.selector,
+			htons(mstp->MCID.revision));
+		for (i = 0; i < 32; i++) {
+			if (mstp->MCID.name[i] != 0)
+				dbg_msg("%c", mstp->MCID.name[i]);
+			else
+				break;
+		}
+		dbg_msg("\n");
+		dbg_msg("%04x=%02x%02x%02x%02x%02x%02x  %u %u  %u\n",
+			ntohs(mstp->id.prio),
+			mstp->id.addr[0],
+			mstp->id.addr[1],
+			mstp->id.addr[2],
+			mstp->id.addr[3],
+			mstp->id.addr[4],
+			mstp->id.addr[5],
+			ntohl(mstp->internal_root_path_cost),
+			mstp->max_hops, len);
+		len -= sizeof(struct mstp_bpdu);
+		cfg = (struct msti_cfg *)(mstp + 1);
+		while (len >= sizeof(struct msti_cfg)) {
+			dbg_msg("%04x=%02x%02x%02x%02x%02x%02x  ",
+				ntohs(cfg->root.prio),
+				cfg->root.addr[0],
+				cfg->root.addr[1],
+				cfg->root.addr[2],
+				cfg->root.addr[3],
+				cfg->root.addr[4],
+				cfg->root.addr[5]);
+			if (cfg->flags & MASTER)
+				dbg_msg("M");
+			else
+				dbg_msg("-");
+			if (cfg->flags & AGREEMENT)
+				dbg_msg("A");
+			else
+				dbg_msg("-");
+			if (cfg->flags & FORWARDING)
+				dbg_msg("F");
+			else
+				dbg_msg("-");
+			if (cfg->flags & LEARNING)
+				dbg_msg("L");
+			else
+				dbg_msg("-");
+			role = cfg->flags >> PORT_ROLE_S;
+			role &= PORT_ROLE_DESIGNATED;
+			switch (role) {
+			case PORT_ROLE_ALTERNATE:
+				dbg_msg("N");
+				break;
+			case PORT_ROLE_ROOT:
+				dbg_msg("R");
+				break;
+			case PORT_ROLE_DESIGNATED:
+				dbg_msg("D");
+				break;
+			default:
+				dbg_msg("M");
+			}
+			if (cfg->flags & PROPOSAL)
+				dbg_msg("P");
+			else
+				dbg_msg("-");
+			if (cfg->flags & TOPOLOGY_CHANGE)
+				dbg_msg("T");
+			else
+				dbg_msg("-");
+			dbg_msg("  %02x %02x %u %u\n",
+				cfg->br_prio, cfg->port_prio, cfg->max_hops,
+				ntohl(cfg->root_path_cost));
+			cfg++;
+			len -= sizeof(struct msti_cfg);
+		}
+	}
+}
+#endif
+
+static struct bpdu *chk_bpdu(u8 *data, u16 *size)
+{
+	struct llc *llc = (struct llc *) &data[12];
+	u16 len = ntohs(llc->len);
+
+	if (len < 1500) {
+		if (0x42 == llc->dsap &&
+		    0x42 == llc->ssap &&
+		    0x03 == llc->ctrl)
+		if (size)
+			*size = len - 3;
+		return (struct bpdu *)(llc + 1);
+	}
+	return NULL;
+}  /* chk_bpdu */
+
+
+#define STP_TIMER_TICK		200
+#define STP_TIMER_SCALE		1000
+
+#define to_stp_timer(x)		((x) * STP_TIMER_SCALE)
+#define from_stp_timer(x)	((x) / STP_TIMER_SCALE)
+#define NEQ(x, y)		(abs((y) - (x)) >= STP_TIMER_SCALE)
+
+
+#define BridgeIdentifier	(p->br->mvars[p->br->MSTI].br_id_)
+#define BridgePriority		(p->br->mvars[p->br->MSTI].bridgePrio_)
+#define BridgeTimes		(p->br->mvars[p->br->MSTI].bridgeTimes_)
+#define rootPortId		(p->br->mvars[p->br->MSTI].rootPortId_)
+#define rootPriority		(p->br->mvars[p->br->MSTI].rootPrio_)
+#define rootTimes		(p->br->mvars[p->br->MSTI].rootTimes_)
+
+#define timeSinceTC		(p->br->mvars[p->br->MSTI].TC_sec_)
+#define cntTC			(p->br->mvars[p->br->MSTI].TC_cnt_)
+#define isTC			(p->br->mvars[p->br->MSTI].TC_set_)
+
+/* MigrateTime is only used internally for timer. */
+#define MigrateTime		(p->br->vars.MigrateTime_)
+#define TxHoldCount		(p->br->vars.TxHoldCount_)
+#define ForceProtocolVersion	(p->br->vars.ForceProtocolVersion_)
+#define MstConfigId		(p->br->vars.MstConfigId_)
+
+
+#define edgeDelayWhile		(p->vars.timers[0])
+#define fdWhile			(p->vars.timers[1])
+#define helloWhen		(p->vars.timers[2])
+
+#define mdelayWhile		(p->mvars[p->MSTI].timers[0])
+#define rbWhile			(p->mvars[p->MSTI].timers[1])
+#define rcvdInfoWhile		(p->mvars[p->MSTI].timers[2])
+#define rrWhile			(p->mvars[p->MSTI].timers[3])
+#define tcWhile			(p->mvars[p->MSTI].timers[4])
+#define tcDetected		(p->mvars[p->MSTI].timers[5])
+#define tcPropWhile		(p->mvars[p->MSTI].timers[6])
+
+#define AdminEdgePort		(p->vars.admin_var.AdminEdgePort_)
+#define AdminPortPathCost	(p->vars.admin_var.adminPortPathCost_)
+#define AutoEdgePort		(p->vars.admin_var.AutoEdgePort_)
+#define AutoIsolate		(p->vars.admin_var.AutoIsolate_)
+#define enableBPDUrx		(p->vars.admin_var.enableBPDUrx_)
+#define enableBPDUtx		(p->vars.admin_var.enableBPDUtx_)
+#define adminPointToPointMAC	(p->vars.admin_var.adminPointToPointMAC_)
+#define operPointToPointMAC	(p->vars.admin_var.operPointToPointMAC_)
+
+#define ageingTime		(p->vars.stp_var.ageingTime_)
+#define infoInternal		(p->vars.stp_var.infoInternal_)
+#define isolate			(p->vars.stp_var.isolate_)
+#define master			(p->vars.stp_var.master_)
+#define mastered		(p->vars.stp_var.mastered_)
+#define mcheck			(p->vars.stp_var.mcheck_)
+#define newInfo			(p->vars.stp_var.newInfo_)
+#define newInfoMsti		(p->vars.stp_var.newInfoMsti_)
+#define operEdge		(p->vars.stp_var.operEdge_)
+#define portEnabled		(p->vars.stp_var.portEnabled_)
+#define rcvdBPDU		(p->vars.stp_var.rcvdBPDU_)
+#define rcvdInternal		(p->vars.stp_var.rcvdInternal_)
+#define rcvdRSTP		(p->vars.stp_var.rcvdRSTP_)
+#define rcvdSTP			(p->vars.stp_var.rcvdSTP_)
+#define rcvdTcAck		(p->vars.stp_var.rcvdTcAck_)
+#define rcvdTcn			(p->vars.stp_var.rcvdTcn_)
+#define restrictedRole		(p->vars.stp_var.restrictedRole_)
+#define restrictedTcn		(p->vars.stp_var.restrictedTcn_)
+#define sendRSTP		(p->vars.stp_var.sendRSTP_)
+#define tcAck			(p->vars.stp_var.tcAck_)
+#define tick			(p->vars.stp_var.tick_)
+#define txCount			(p->vars.stp_var.txCount_)
+#define ExternalPortPathCost	(p->vars.stp_var.PortPathCost_)
+
+#define agree			(p->mvars[p->MSTI].stp_var.agree_)
+#define agreed			(p->mvars[p->MSTI].stp_var.agreed_)
+#define designatedPriority	(p->mvars[p->MSTI].stp_var.desgPrio_)
+#define designatedTimes		(p->mvars[p->MSTI].stp_var.desgTimes_)
+#define CISTdesignatedTimes	(p->mvars[0].stp_var.desgTimes_)
+#define disputed		(p->mvars[p->MSTI].stp_var.disputed_)
+#define fdbFlush		(p->mvars[p->MSTI].stp_var.fdbFlush_)
+#define forward			(p->mvars[p->MSTI].stp_var.forward_)
+#define forwarding		(p->mvars[p->MSTI].stp_var.forwarding_)
+#define infoIs			(p->mvars[p->MSTI].stp_var.infoIs_)
+#define CISTinfoIs		(p->mvars[0].stp_var.infoIs_)
+#define learn			(p->mvars[p->MSTI].stp_var.learn_)
+#define learning		(p->mvars[p->MSTI].stp_var.learning_)
+#define msgPriority		(p->mvars[p->MSTI].stp_var.msgPrio_)
+#define CISTmsgPriority		(p->mvars[0].stp_var.msgPrio_)
+#define msgTimes		(p->mvars[p->MSTI].stp_var.msgTimes_)
+#define portId			(p->mvars[p->MSTI].stp_var.portId_)
+#define portPriority		(p->mvars[p->MSTI].stp_var.portPrio_)
+#define CISTportPriority	(p->mvars[0].stp_var.portPrio_)
+#define portTimes		(p->mvars[p->MSTI].stp_var.portTimes_)
+#define CISTportTimes		(p->mvars[0].stp_var.portTimes_)
+#define proposed		(p->mvars[p->MSTI].stp_var.proposed_)
+#define proposing		(p->mvars[p->MSTI].stp_var.proposing_)
+#define rcvdInfo		(p->mvars[p->MSTI].stp_var.rcvdInfo_)
+#define rcvdMsg			(p->mvars[p->MSTI].stp_var.rcvdMsg_)
+#define rcvdTc			(p->mvars[p->MSTI].stp_var.rcvdTc_)
+#define reRoot			(p->mvars[p->MSTI].stp_var.reRoot_)
+#define reselect		(p->mvars[p->MSTI].stp_var.reselect_)
+#define role			(p->mvars[p->MSTI].stp_var.role_)
+#define selected		(p->mvars[p->MSTI].stp_var.selected_)
+#define selectedRole		(p->mvars[p->MSTI].stp_var.selectedRole_)
+#define CISTselectedRole	(p->mvars[0].stp_var.selectedRole_)
+#define sync			(p->mvars[p->MSTI].stp_var.sync_)
+#define synced			(p->mvars[p->MSTI].stp_var.synced_)
+#define tcProp			(p->mvars[p->MSTI].stp_var.tcProp_)
+#define updtInfo		(p->mvars[p->MSTI].stp_var.updtInfo_)
+#define InternalPortPathCost	(p->mvars[p->MSTI].stp_var.PortPathCost_)
+
+#define bpduVersion		(p->vars.bpduVersion_)
+#define bpduType		(p->vars.bpduType_)
+#define bpduMCID		(p->vars.bpduMCID_)
+
+#define bpduFlags		(p->mvars[p->MSTI].bpduFlags_)
+#define bpduRole		(p->mvars[p->MSTI].bpduRole_)
+#define bpduPriority		(p->mvars[p->MSTI].bpduPrio_)
+#define bpduTimes		(p->mvars[p->MSTI].bpduTimes_)
+
+#define DesignatedPort		(ROLE_DESIGNATED == role)
+#define RootPort		(ROLE_ROOT == role)
+#define AlternatePort		(ROLE_ALTERNATE == role)
+#define BackupPort		(ROLE_BACKUP == role)
+#define MasterPort		(ROLE_MASTER == role)
+#define DisabledPort		(ROLE_DISABLED == role)
+
+#define ForwardPort		\
+	(DesignatedPort || RootPort || MasterPort)
+
+#define BridgeFwdDelay		(BridgeTimes.forward_delay)
+#define BridgeHelloTime		(BridgeTimes.hello_time)
+#define BridgeMaxAge		(BridgeTimes.max_age)
+#define BridgeMaxHops		(BridgeTimes.max_hops)
+
+#define AdminEdge		AdminEdgePort
+#define AutoEdge		AutoEdgePort
+
+#define EdgeDelay()		(operPointToPointMAC ? MigrateTime : MaxAge)
+#define forwardDelay()		(sendRSTP ? HelloTime : FwdDelay)
+
+/* These times are in timer unit. */
+#define FwdDelay		to_stp_timer(CISTdesignatedTimes.forward_delay)
+#define HelloTime		to_stp_timer(CISTportTimes.hello_time)
+#define MaxAge			to_stp_timer(CISTdesignatedTimes.max_age)
+
+#define rstpVersion		(ForceProtocolVersion >= 2)
+#define stpVersion		(ForceProtocolVersion < 2)
+
+
+/* Shortcuts for some common qualifications. */
+#define canChange		(selected && !updtInfo)
+#define canSend			\
+	((txCount < TxHoldCount) && (helloWhen != 0))
+
+/*
+ * agree means the port accepts incoming Designated Port, and AGREEMNT flag is
+ * set.
+ * It means all ports are synchronized so that no loop occurs.
+ *
+ * agreed means the proposing Designated Port receives AGREEMENT and so can
+ * stop proposing and can enable learning and forwarding immediately.
+ * Not operPointToPointMAC causes that not to happen and the Designated Port
+ * keeps proposing, but half-duplex connection will cause operPointToPoint to
+ * be FALSE.  In old time half-duplex was associated with hub and what is
+ * called Shared Media.  The sent BPDU may be dropped because of collisions,
+ * but that is not what happens here.
+ * agreed is used to qualify synced after betterorsameInfo call for Designated
+ * Port.  It is set when the port becomes forwarding, but if the port is
+ * already forwarding it is never set and so synced cannot be set, causing
+ * the Root Port not to send anything back to the Designated Port.
+ *
+ * proposing means the Designated Port is asking approval before opening the
+ * port, and PROPOSAL flag is set.
+ * It is set when the Designated Port is not forwarding.  It is reset when
+ * AGREEMENT flag is received, or when infoIs is set to Mine.
+ * Does it mean in !operPointToPointMAC the PROPOSAL flag will be dropped when
+ * the Designated Port information is changed?!
+ * There is a case that after becoming Root Port proposing is not reset.  It
+ * should not be set in DESIGNATED_PROPOSE state.
+ *
+ * proposed means receiving PROPOSAL from Desginated Port.  It will be reset
+ * either agree is TRUE or not.  The only difference is setSyncTree will be
+ * called when agree is FALSE.
+ *
+ * sync is set in the setSyncTree call.  It is reset when synced is set.
+ * It is reset for Root Port but not Alternate Port.  It is required to be
+ * FALSE for Designated Port to move to learning state.  If it is TRUE then
+ * Designated Port moves to discarding state.
+ *
+ * synced is used in the allSynced call.  That call is only used to get into
+ * _AGREED states in which proposed is reset and agree is set and a BPDU will
+ * be sent.  It is also needed when Master Port goes to learning state.
+ * proposed and synced are mutually exclusive.
+ *
+ * master is set for Root Port or Designated Port in MSTI when one of the ports
+ * is Master Port.  It is also set when mastered is set in a Root Port or
+ * Designated Port.  It is used for later enhancement.
+ *
+ * mastered means receiving MASTER.  It is used for later enhancement.
+ *
+ * operEdge means the port is not connected to another bridge so some variables
+ * will be automatically set to put the port in forwarding state.
+ *
+ * isolate means the port will not go into learning state when it does not
+ * receive any BPDU.  This means operEdge is also not set due to AdminEdge or
+ * AutoEdge not being configured.
+ *
+ * AutoIsolate sets proposing when the port becomes Designated Port.  It has an
+ * effect of going to isolated mode without sending a Proposal.
+ *
+ * tcAck is used by Designated Port to set TOPOLOGY_CHANGE_ACK flag in response
+ * of TOPOLOGY_CHANGE flag.  It is no longer used in MSTP and the flag is
+ * replaced with MASTER flag.  The topology change notification is
+ * automatically shutdown after 3 seconds.  That may be why acknowledgement is
+ * not needed.
+ *
+ * rcvdTc is not accepted from inferior designated port.
+ *
+ * rcvdTcAck is used by Root Port to stop sending topology change immediately.
+ *
+ * newInfo is set to send out BPDU.  It is checked periodically so that
+ * Designated Port can always send but Root Port only sends when there is a
+ * topology change.
+ * It is set when agree is set the first time.  However, Alternate Port does
+ * not always send after becoming one.
+ * There is a case a BPDU is sent during initializion when the port role is not
+ * defined yet.  It should be handled in the _DISABLE_PORT state.
+ * HelloTime is zero when Port Transmit state machine starts.  Because of that
+ * PERIODIC is called instead.  Is that the original intention?
+ * Moving the Port Transmission state machine to last fixes this problem.
+ * However, newInfo is still not cleared because of the OR operation.
+ *
+ * reselect means port information is being changed so selected will not be set
+ * to TRUE.  Typically selected is set to FALSE while reselect is set so that
+ * role selection is done.
+ *
+ * selected means the role selection is completed so states can be changed.  It
+ * is set to FALSE when infoIs is changed.
+ *
+ * updtInfo is set when the port is becoming Designated Port or its parameters
+ * are changed.  It is reset when the Designated Port sets infoIs to Mine.  It
+ * is explicitly set to FALSE for other port roles.
+ * !updtInfo is required in many state changes.  Basically it means the
+ * Designated Port needs to set infoIs to Mine before moving to other states.
+ * There is a case that a Designated Port changing to Root Port will set
+ * proposing, even though it will not be a Designated Port.
+ *
+ * infoIs is initialized in Port Information state machine, so it should be run
+ * before Port Role Selection state machine.
+ *
+ * reRoot is set by new Root Port to force prior Root Ports to Discarding
+ * before it becomes forwarding.  It is mostly done by a prior Alternate Port
+ * or a Designated Port not in forwarding state.
+ *
+ * selectedRole is initialized by updtRoleDisabledTree in INIT_BRIDGE, so
+ * Port Role Selection state machine should be run before Port Role Transitions
+ * state machine.
+ *
+ * There is a problem that selectedRole is changed to DesignatedPort but
+ * DISABLE_PORT changes role like selectedRole is DisabledPort and is stuck in
+ * DISABLED_PORT state.  Fixed in MSTP to explicitly set role to DisabledPort.
+ *
+ * MaxAge is initialized in updtRolesTree, so INIT_PORT should be called after
+ * that.
+ */
+
+
+#define CMP(first, second)	memcmp(&first, &second, sizeof(first))
+#define COPY(first, second)	memcpy(&first, &second, sizeof(first))
+#define ZERO(first)		memset(&first, 0, sizeof(first))
+
+
+#define COPY_PRIO(vector, p, id)	\
+	memcpy(&(vector.prio), &p, sizeof(p));	\
+	memcpy(&(vector.port_id), &id, sizeof(id));
+
+
+static void copy_prio(struct mstp_prio *m, void *p)
+{
+	struct stp_prio *s = p;
+
+	memcpy(&m->root, &s->root, sizeof(struct _bridge_id) + sizeof(u32) +
+		sizeof(struct _bridge_id));
+	memcpy(&m->bridge_id, &s->bridge_id, sizeof(struct _bridge_id));
+	memcpy(&m->port_id, &s->port_id, sizeof(struct _port_id));
+	m->internal_root_path_cost = 0;
+}
+
+static int superiorPriority(struct mstp_prio *first, struct mstp_prio *second)
+{
+	int prio;
+
+	prio = memcmp(first, second, sizeof(struct mstp_prio));
+	if (prio > 0 &&
+	    !memcmp(first->bridge_id.addr, second->bridge_id.addr, ETH_ALEN) &&
+	    first->port_id.num == second->port_id.num)
+		prio = -256;
+#ifdef DBG_STP_ROLE
+if (prio < -127)
+dbg_msg("  %s same br/port\n", __func__);
+#endif
+	return (prio < 0);
+}
+
+static void dbgPriority(struct mstp_prio *prio, struct _port_id *id)
+{
+	int j;
+	u8 *data;
+
+	data = (u8 *) prio;
+	for (j = 0; j < sizeof(struct mstp_prio); j++) {
+		if (8 == j || 12 == j || 20 == j || 24 == j)
+			dbg_msg(" ");
+		dbg_msg("%02x", data[j]);
+	}
+	if (id) {
+		dbg_msg(" ");
+		data = (u8 *) id;
+		for (j = 0; j < sizeof(struct _port_id); j++)
+			dbg_msg("%02x", data[j]);
+	}
+	dbg_msg("\n");
+}
+
+static int betterSamePriority(struct mstp_prio *first, struct mstp_prio *second)
+{
+	int prio;
+
+	prio = memcmp(first, second, sizeof(struct mstp_prio));
+	return (prio <= 0);
+}
+
+static int betterVector(struct mstp_vector *first, struct mstp_vector *second)
+{
+	int prio;
+
+	prio = memcmp(&first->prio, &second->prio, sizeof(struct mstp_prio));
+	if (!prio) {
+#ifdef DBG_STP_ROLE
+dbg_msg(" same prio\n");
+		dbgPriority(&first->prio, &first->port_id);
+		dbgPriority(&second->prio, &second->port_id);
+#endif
+		prio = memcmp(&first->port_id, &second->port_id,
+			sizeof(struct _port_id));
+	}
+	return (prio < 0);
+}
+
+
+static int next_msti(struct ksz_stp_bridge *br, int msti)
+{
+	int cnt = NUM_OF_MSTI - msti - 1;
+
+	while (!(br->MSTI_enabled & (1 << msti)) && cnt > 0) {
+		++msti;
+		--cnt;
+	}
+	return msti;
+}
+
+#define FOREACH_P_IN_T(statements...)					\
+{									\
+	int c;								\
+	for (c = 0; c < p->br->port_cnt; c++) {				\
+		p = &p->br->ports[c];					\
+		p->MSTI = p->br->MSTI;					\
+		statements						\
+	}								\
+}
+
+#define FOREACH_T_IN_P(statements...)					\
+{									\
+	int c;								\
+	int m;								\
+if (p->MSTI) \
+dbg_msg(" !!! %s %d %d\n", __func__, p->port_index, p->MSTI); \
+	for (c = 0, m = 0; c < p->br->MSTI_cnt; c++, m++) {		\
+		m = next_msti(p->br, m);				\
+		p->MSTI = m;						\
+		statements						\
+	}								\
+	p->MSTI = 0;							\
+}
+
+#define FOREACH_M_IN_P(statements...)					\
+{									\
+	int c;								\
+	int m;								\
+if (p->MSTI) \
+dbg_msg(" !!! %s\n", __func__); \
+	for (c = 1, m = 1; c < p->br->MSTI_cnt; c++, m++) {		\
+		m = next_msti(p->br, m);				\
+		p->MSTI = m;						\
+		statements						\
+	}								\
+	p->MSTI = 0;							\
+}
+
+
+#define PATH_COST		20000000
+
+static uint computePathCost(int speed)
+{
+	if (speed > 0)
+		return speed < PATH_COST ? PATH_COST / speed : 1;
+	else
+		return PATH_COST * 10;
+}  /* computePathCost */
+
+static int checkP2P(struct ksz_stp_port *p)
+{
+	u8 p2p;
+
+	if (ADMIN_P2P_AUTO == adminPointToPointMAC)
+		p2p = !!p->duplex;
+	else
+		p2p = (ADMIN_P2P_FORCE_TRUE == adminPointToPointMAC);
+	if (p2p != operPointToPointMAC) {
+		operPointToPointMAC = p2p;
+		return TRUE;
+	}
+	return FALSE;
+}  /* checkP2P */
+
+static int checkPathCost(struct ksz_stp_port *p)
+{
+	uint pathCost;
+
+	if (!AdminPortPathCost)
+		pathCost = computePathCost(p->speed);
+	else
+		pathCost = AdminPortPathCost;
+	if (pathCost != ExternalPortPathCost) {
+		ExternalPortPathCost = pathCost;
+		InternalPortPathCost = pathCost;
+		reselect = TRUE;
+		selected = FALSE;
+		FOREACH_M_IN_P(
+			InternalPortPathCost = pathCost;
+			reselect = TRUE;
+			selected = FALSE;
+		)
+		return TRUE;
+	}
+	return FALSE;
+}  /* checkPathCost */
+
+static int checkParameters(int hello_time, int max_age, int fwd_delay)
+{
+	if (max_age < 6 || max_age > 40)
+		return FALSE;
+	if (fwd_delay < 4 || fwd_delay > 30)
+		return FALSE;
+	if (2 * (fwd_delay - 1) < max_age)
+		return FALSE;
+	if (max_age < 2 * (hello_time + 1))
+		return FALSE;
+	return TRUE;
+}  /* checkParameters */
+
+static int make_hmac_md5(void *in_data, int len, u8 *out_data)
+{
+	struct crypto_ahash *hmac_md5;
+	struct ahash_request req;
+	struct scatterlist sg[1];
+	int err;
+
+	hmac_md5 = crypto_alloc_ahash("hmac(md5)", 0, CRYPTO_ALG_ASYNC);
+	if (IS_ERR(hmac_md5)) {
+		return -ENOMEM;
+	}
+
+	err = crypto_ahash_init(&req);
+	if (err)
+		goto out;
+	err = crypto_ahash_setkey(hmac_md5, CFG_DIGEST_SIGN_KEY, 16);
+	if (err)
+		goto out;
+
+	sg_init_one(sg, in_data, len);
+	ahash_request_set_tfm(&req, hmac_md5);
+	ahash_request_set_callback(&req, 0, NULL, NULL);
+	ahash_request_set_crypt(&req, sg, out_data, len);
+	err = crypto_ahash_digest(&req);
+	ahash_request_zero(&req);
+
+out:
+	crypto_free_ahash(hmac_md5);
+	return err;
+}
+
+static void recalc_cfg_digest(struct ksz_stp_port *p)
+{
+	int vid;
+	struct ksz_stp_bridge *br = p->br;
+	struct ksz_stp_info *stp = br->parent;
+	struct ksz_sw *sw = stp->sw_dev;
+
+	br->vid2mstid[0] = br->vid2mstid[NUM_OF_VID + 1] = 0;
+	for (vid = 1; vid <= NUM_OF_VID; vid++)
+		br->vid2mstid[vid] =
+			htons(sw->info->fid2mstid[sw->info->vid2fid[vid]]);
+
+	make_hmac_md5(br->vid2mstid, sizeof(br->vid2mstid),
+		MstConfigId.digest);
+}
+
+static void sw_cfg_forwarding(struct ksz_sw *sw, uint port, bool open)
+{
+	struct ksz_sw_info *info = sw->info;
+	int i = info->port_cfg[port].mstp;
+	u8 member = info->member[i];
+	uint m = BIT(port);
+
+	if (open)
+		member |= m;
+	else
+		member &= ~m;
+	if (member != info->member[i]) {
+		info->member[i] = member;
+		bridge_change(sw);
+#ifdef CONFIG_KSZ_MRP
+		if (open && (sw->features & MRP_SUPPORT)) {
+			struct mrp_info *mrp = &sw->mrp;
+
+			mrp_open_port(mrp, port);
+		}
+#endif
+	}
+}  /* sw_cfg_forwarding */
+
+static void doFlush_(struct ksz_stp_port *p)
+{
+	struct ksz_stp_info *stp = p->br->parent;
+	struct ksz_sw *sw = stp->sw_dev;
+	int i = p->port_index;
+#ifdef DBG_STP_PORT_FLUSH
+	struct ksz_stp_port *q;
+	struct ksz_stp_dbg_times *x;
+	struct ksz_stp_dbg_times *y;
+#endif
+
+	if (operEdge) {
+dbg_msg(" no flush\n");
+		fdbFlush = FALSE;
+		return;
+	}
+	sw->ops->acquire(sw);
+	port_cfg_mstp(sw, i, p->MSTI);
+	sw_flush_dyn_mac_table(sw, i);
+	sw->ops->release(sw);
+	fdbFlush = FALSE;
+#ifdef DBG_STP_PORT_FLUSH
+	q = p;
+	y = &q->dbg_times[p->MSTI];
+	for (i = 0; i < p->br->port_cnt; i++) {
+		p = &p->br->ports[i];
+		if (q == p)
+			continue;
+		x = &p->dbg_times[p->MSTI];
+		if (y->lastPriority.port_id.num) {
+			int cmp = memcmp(&y->lastPriority.bridge_id,
+				&x->downPriority.bridge_id,
+				sizeof(struct _bridge_id));
+			if (!cmp && x->learn_jiffies) {
+dbg_msg(" %ld [%ld] %02x%02x ", jiffies - y->alt_jiffies,
+jiffies - x->learn_jiffies, y->lastPriority.port_id.prio, y->lastPriority.port_id.num);
+				x->learn_jiffies = 0;
+				y->lastPriority.port_id.num = 0;
+				y->learn_jiffies = 0;
+				y->alt_jiffies = 0;
+			}
+		}
+	}
+	p = q;
+	switch (y->role_ & 0x3) {
+	case PORT_ROLE_ROOT:
+		dbg_msg("  R");
+		break;
+	case PORT_ROLE_ALTERNATE:
+		dbg_msg("  A");
+		break;
+	case PORT_ROLE_DESIGNATED:
+		if (RootPort)
+			dbg_msg("  r");
+		else if (AlternatePort)
+			dbg_msg("  a");
+		else
+			dbg_msg("  u");
+		break;
+	default:
+		if (DisabledPort)
+			dbg_msg("  Z");
+		else
+			dbg_msg("  M");
+		break;
+	}
+dbg_msg("  F:%d:%d\n", q->port_index, q->MSTI);
+#endif
+}  /* doFlush_ */
+
+#define doFlush()			doFlush_(p)
+
+static void stp_chk_flush(struct ksz_stp_port *p)
+{
+#ifdef DBG_STP_PORT_FLUSH
+	struct ksz_stp_port *q;
+	struct ksz_stp_bridge *br = p->br;
+	uint i;
+	struct ksz_stp_dbg_times *x = &p->dbg_times[p->MSTI];
+	struct ksz_stp_dbg_times *y;
+
+	if (bpduRole == x->role_)
+		return;
+#if 0
+dbg_msg("%d:%d %d %d %d %s\n", p->port_index, p->MSTI, bpduRole,
+rcvdTc, learning, __func__);
+#endif
+	x->role_ = bpduRole;
+	if (rcvdTc || !learning) {
+		int flush = 0;
+
+		for (i = 0; i < br->port_cnt; i++) {
+			q = &br->ports[i];
+			if (q == p)
+				continue;
+			y = &q->dbg_times[p->MSTI];
+			if (y->downPriority.port_id.num) {
+				int cmp = memcmp(&y->downPriority.bridge_id,
+					&msgPriority.bridge_id,
+					sizeof(struct _bridge_id));
+				if (!cmp) {
+					if (learning)
+						x->learn_jiffies = jiffies;
+					flush = q->port_index + 1;
+					y->role_ = ROLE_ALT_BACKUP | 0x80;
+					break;
+				}
+			}
+		}
+dbg_msg("  %s %d:%d=%d\n", __func__, p->port_index, p->MSTI, flush);
+	}
+	if (bpduRole == ROLE_ALT_BACKUP)
+		x->alt_jiffies = jiffies;
+	if (bpduRole == ROLE_ROOT) {
+		COPY(x->downPriority, msgPriority);
+		COPY(x->lastPriority, msgPriority);
+	}
+#endif
+}
+
+static int allSynced_(struct ksz_stp_port *p)
+{
+	struct ksz_stp_port *q = p;
+	int ret = TRUE;
+	int all_others = FALSE;
+	int skip_root = FALSE;
+
+	if (DesignatedPort || MasterPort)
+		all_others = TRUE;
+	else if (!DisabledPort)
+		skip_root = TRUE;
+
+	/* Check only when role is not in transition. */
+	FOREACH_P_IN_T(
+		if (!(selected && (role == selectedRole) && !updtInfo)) {
+#if 0
+dbg_msg(" allSync: %d= %d %d %d %d\n", i, selected, role, selectedRole, synced);
+#endif
+			ret = FALSE;
+			break;
+		} else {
+			if ((all_others && p == q) ||
+			    (skip_root && RootPort))
+				continue;
+			if (!synced) {
+				ret = FALSE;
+				break;
+			}
+		}
+	)
+	return ret;
+}
+
+#define allSynced()			allSynced_(p)
+
+/* Called in PortTransmit state machine only. */
+static int allTransmitReady_(struct ksz_stp_port *p)
+{
+	int ret = TRUE;
+
+	FOREACH_T_IN_P(
+		if (!selected || updtInfo) {
+			ret = FALSE;
+			break;
+		}
+	)
+	return ret;
+}
+
+#define allTransmitReady()		allTransmitReady_(p)
+
+static int cist_(struct ksz_stp_port *p)
+{
+	return 0 == p->MSTI;
+}
+
+#define cist()				cist_(p)
+
+static int cistRootPort_(struct ksz_stp_port *p)
+{
+	int ret;
+	int MSTI = p->MSTI;
+
+	p->MSTI = 0;
+	ret = (ROLE_ROOT == role);
+	p->MSTI = MSTI;
+	return ret;
+}
+
+#define cistRootPort()			cistRootPort_(p)
+
+static int cistDesignatedPort_(struct ksz_stp_port *p)
+{
+	int ret;
+	int MSTI = p->MSTI;
+
+	p->MSTI = 0;
+	ret = (ROLE_DESIGNATED == role);
+	p->MSTI = MSTI;
+	return ret;
+}
+
+#define cistDesignatedPort()		cistDesignatedPort_(p)
+
+static int msti_(struct ksz_stp_port *p)
+{
+	return p->MSTI > 0;
+}
+
+#define msti()				msti_(p)
+
+/* Called in PortTransmit state machine only. */
+static int mstiDesignatedOrTCpropagatingRootPort_(struct ksz_stp_port *p)
+{
+	int ret = FALSE;
+
+	FOREACH_M_IN_P(
+		if (DesignatedPort || (RootPort && (tcWhile != 0))) {
+			ret = TRUE;
+			break;
+		}
+	)
+	return ret;
+}
+
+#define mstiDesignatedOrTCpropagatingRootPort()	\
+	mstiDesignatedOrTCpropagatingRootPort_(p)
+
+/* Called in PortTransmit state machine only. */
+static int mstiMasterPort_(struct ksz_stp_port *p)
+{
+	int ret = FALSE;
+
+	FOREACH_M_IN_P(
+		if (MasterPort) {
+			ret = TRUE;
+			break;
+		}
+	)
+	return ret;
+}
+
+#define mstiMasterPort()		mstiMasterPort_(p)
+
+static int rcvdAnyMsg_(struct ksz_stp_port *p)
+{
+	int ret = FALSE;
+
+	FOREACH_T_IN_P(
+		if (rcvdMsg) {
+			ret = TRUE;
+			break;
+		}
+	)
+	return ret;
+}
+
+#define rcvdAnyMsg()			rcvdAnyMsg_(p)
+
+static int rcvdCistMsg_(struct ksz_stp_port *p)
+{
+	int ret = FALSE;
+
+	if (rcvdMsg)
+		ret = TRUE;
+	return ret;
+}
+
+#define rcvdCistMsg()			rcvdCistMsg_(p)
+
+static int rcvdMstiMsg_(struct ksz_stp_port *p)
+{
+	int MSTI = p->MSTI;
+	int ret = TRUE;
+
+	p->MSTI = 0;
+	if (rcvdMsg)
+		ret = FALSE;
+	p->MSTI = MSTI;
+	if (!rcvdMsg)
+		ret = FALSE;
+	return ret;
+}
+
+#define rcvdMstiMsg()			rcvdMstiMsg_(p)
+
+static int reRooted_(struct ksz_stp_port *p)
+{
+	struct ksz_stp_port *q = p;
+	int ret = TRUE;
+
+	FOREACH_P_IN_T(
+		if (p == q)
+			continue;
+		if (0 != rrWhile) {
+			ret = FALSE;
+			break;
+		}
+	)
+	return ret;
+}
+
+#define reRooted()			reRooted_(p)
+
+static int updtCistInfo_(struct ksz_stp_port *p)
+{
+	int ret = FALSE;
+
+	ret = updtInfo;
+	return ret;
+}
+
+#define updtCistInfo()			updtCistInfo_(p)
+
+static int updtMstiInfo_(struct ksz_stp_port *p)
+{
+	int MSTI = p->MSTI;
+	int ret = FALSE;
+
+	if (updtInfo)
+		return TRUE;
+	p->MSTI = 0;
+	ret = updtInfo;
+	p->MSTI = MSTI;
+	return ret;
+}
+
+#define updtMstiInfo()			updtMstiInfo_(p)
+
+static void newInfoXst_(struct ksz_stp_port *p, int set)
+{
+	if (msti())
+		newInfoMsti = set;
+	else
+		newInfo = set;
+#if 0
+dbg_msg("%s %d=%d %d\n", __func__, p->port_index, newInfo, newInfoMsti);
+#endif
+}
+
+#define newInfoXst(s)			newInfoXst_(p, s)
+
+static int rcvdXstMsg_(struct ksz_stp_port *p)
+{
+	if (msti())
+		return rcvdMstiMsg();
+	return rcvdCistMsg();
+}
+
+#define rcvdXstMsg()			rcvdXstMsg_(p)
+
+static int updtXstInfo_(struct ksz_stp_port *p)
+{
+	if (msti())
+		return updtMstiInfo();
+	return updtCistInfo();
+}
+
+#define updtXstInfo()			updtXstInfo_(p)
+
+static int betterorsameInfo_(struct ksz_stp_port *p, u8 newInfoIs)
+{
+	int ret = FALSE;
+
+	if (INFO_TYPE_RECEIVED == newInfoIs && INFO_TYPE_RECEIVED == infoIs) {
+		ret = betterSamePriority(&msgPriority, &portPriority);
+#ifdef DBG_STP_ROLE_
+dbg_msg("  recd: %d\n", ret);
+#endif
+	} else if (INFO_TYPE_MINE == newInfoIs && INFO_TYPE_MINE == infoIs) {
+		ret = betterSamePriority(&designatedPriority, &portPriority);
+#ifdef DBG_STP_ROLE_
+dbg_msg("  mine: %d\n", ret);
+#endif
+#if 0
+	} else if (INFO_TYPE_MINE == newInfoIs && INFO_TYPE_AGED == infoIs) {
+		ret = betterSamePriority(&designatedPriority, &portPriority);
+dbgPriority(&designatedPriority, NULL);
+dbgPriority(&portPriority, NULL);
+#endif
+	}
+#ifdef DBG_STP_ROLE_
+dbg_msg("%s %d\n", __func__, ret);
+#endif
+	return ret;
+}
+
+#define betterorsameInfo(i)		betterorsameInfo_(p, i)
+
+static void clearAllRcvdMsgs_(struct ksz_stp_port *p)
+{
+	FOREACH_T_IN_P(
+		rcvdMsg = FALSE;
+	)
+}
+
+#define clearAllRcvdMsgs()		clearAllRcvdMsgs_(p)
+
+static void clearReselectTree_(struct ksz_stp_bridge *br)
+{
+	struct ksz_stp_port *p = &br->ports[0];
+
+	FOREACH_P_IN_T(
+		reselect = FALSE;
+	)
+}
+
+#define clearReselectTree()		clearReselectTree_(br)
+
+static void disableForwarding_(struct ksz_stp_port *p)
+{
+	struct ksz_stp_info *stp = p->br->parent;
+	struct ksz_sw *sw = stp->sw_dev;
+	int i = p->port_index;
+
+	sw->ops->acquire(sw);
+	port_cfg_mstp(sw, i, p->MSTI);
+	if (!learning) {
+#ifdef DBG_STP_PORT_BLOCK
+		struct ksz_stp_dbg_times *x = &p->dbg_times[p->MSTI];
+
+		x->block_jiffies = jiffies;
+#endif
+		if (portEnabled)
+			port_set_stp_state(sw, i, STP_STATE_BLOCKED);
+		sw_cfg_forwarding(sw, i, false);
+	} else {
+		port_cfg_rx(sw, i, false);
+		port_cfg_tx(sw, i, false);
+	}
+	sw->ops->release(sw);
+}
+
+#define disableForwarding()		disableForwarding_(p)
+
+static void disableLearning_(struct ksz_stp_port *p)
+{
+	struct ksz_stp_info *stp = p->br->parent;
+	struct ksz_sw *sw = stp->sw_dev;
+	int i = p->port_index;
+
+	sw->ops->acquire(sw);
+	port_cfg_mstp(sw, i, p->MSTI);
+	port_cfg_dis_learn(sw, i, true);
+	sw->ops->release(sw);
+}
+
+#define disableLearning()		disableLearning_(p)
+
+static void enableForwarding_(struct ksz_stp_port *p)
+{
+	struct ksz_stp_info *stp = p->br->parent;
+	struct ksz_sw *sw = stp->sw_dev;
+	int i = p->port_index;
+
+	sw->ops->acquire(sw);
+	port_cfg_mstp(sw, i, p->MSTI);
+#ifdef DBG_STP_PORT_BLOCK
+	do {
+		struct ksz_stp_dbg_times *x = &p->dbg_times[p->MSTI];
+
+		if (x->block_jiffies)
+dbg_msg("  B:%d:%d=%ld\n", p->port_index, p->MSTI, jiffies - x->block_jiffies);
+		x->block_jiffies = 0;
+	} while (0);
+#endif
+	if (learning) {
+		port_set_stp_state(sw, i, STP_STATE_FORWARDING);
+		sw_cfg_forwarding(sw, i, true);
+	} else {
+		port_cfg_rx(sw, i, true);
+		port_cfg_tx(sw, i, true);
+	}
+	sw->ops->release(sw);
+}
+
+#define enableForwarding()		enableForwarding_(p)
+
+static void enableLearning_(struct ksz_stp_port *p)
+{
+	struct ksz_stp_info *stp = p->br->parent;
+	struct ksz_sw *sw = stp->sw_dev;
+	int i = p->port_index;
+
+	sw->ops->acquire(sw);
+	port_cfg_mstp(sw, i, p->MSTI);
+	port_cfg_dis_learn(sw, i, false);
+	sw->ops->release(sw);
+#ifdef DBG_STP_PORT_BLOCK
+	p->dbg_times[p->MSTI].learn_jiffies = jiffies;
+#endif
+}
+
+#define enableLearning()		enableLearning_(p)
+
+static int fromSameRegion_(struct ksz_stp_port *p)
+{
+	if (ForceProtocolVersion >= 3 && rcvdRSTP &&
+	    !memcmp(&bpduMCID, &MstConfigId, sizeof(struct mst_cfg_id)))
+		return TRUE;
+	return FALSE;
+}
+
+#define fromSameRegion()		fromSameRegion_(p)
+
+static void newTcDetected_(struct ksz_stp_port *p)
+{
+	if (tcDetected)
+		return;
+	if (rcvdRSTP) {
+		tcDetected = to_stp_timer(CISTportTimes.hello_time + 1);
+	} else {
+		tcDetected = rootTimes.max_age + rootTimes.forward_delay;
+		tcDetected = to_stp_timer(tcDetected);
+	}
+	do {
+		struct ksz_stp_info *stp = p->br->parent;
+		struct ksz_sw *sw = stp->sw_dev;
+
+		sw->ops->tc_detected(sw, p->port_index);
+	} while (0);
+}
+
+#define newTcDetected()			newTcDetected_(p)
+
+static void newTcWhile_(struct ksz_stp_port *p)
+{
+	if (tcWhile)
+		return;
+	if (sendRSTP) {
+		tcWhile = to_stp_timer(CISTportTimes.hello_time + 1);
+		if (p->MSTI)
+			newInfoMsti = TRUE;
+		else
+			newInfo = TRUE;
+	} else {
+		tcWhile = rootTimes.max_age + rootTimes.forward_delay;
+		tcWhile = to_stp_timer(tcWhile);
+	}
+	if (!isTC) {
+		timeSinceTC = 0;
+		cntTC++;
+	}
+	isTC |= (1 << p->port_index);
+}
+
+#define newTcWhile()			newTcWhile_(p)
+
+static u8 rcvInfo_(struct ksz_stp_port *p)
+{
+	int prio;
+	int time;
+
+	if (BPDU_TYPE_TCN == bpduType) {
+		rcvdTcn = TRUE;
+		return INFO_OTHER;
+	}
+
+	if (BPDU_TYPE_CONFIG == bpduType)
+		bpduRole = ROLE_DESIGNATED;
+
+	COPY(msgPriority, bpduPriority);
+	COPY(msgTimes, bpduTimes);
+
+	prio = CMP(msgPriority, portPriority);
+	if (cist())
+		time = CMP(msgTimes, portTimes);
+	else
+		time = msgTimes.max_hops - portTimes.max_hops;
+	if (bpduRole == ROLE_DESIGNATED) {
+		if (superiorPriority(&msgPriority, &portPriority))
+			return INFO_SUPERIOR_DESIGNATED;
+
+		if (0 == prio && time != 0)
+			return INFO_SUPERIOR_DESIGNATED;
+
+		if (0 == prio && 0 == time)
+			return INFO_REPEATED_DESIGNATED;
+
+		if (prio > 0)
+			return INFO_INFERIOR_DESIGNATED;
+	}
+
+	if ((bpduRole == ROLE_ROOT || bpduRole == ROLE_ALT_BACKUP) &&
+	    prio >= 0)
+		return INFO_INFERIOR_ROOT_ALT;
+
+	return INFO_OTHER;
+}
+
+#define rcvInfo()			rcvInfo_(p)
+
+static void rcvMsgs_(struct ksz_stp_port *p)
+{
+	p->MSTI = 0;
+	rcvdMsg = TRUE;
+	if (rcvdInternal) {
+		FOREACH_M_IN_P(
+			if (bpduFlags)
+				rcvdMsg = TRUE;
+		)
+	}
+	if (rcvdTcn) {
+		FOREACH_M_IN_P(
+			rcvdTcn = TRUE;
+		)
+	}
+}
+
+#define rcvMsgs()			rcvMsgs_(p)
+
+static void recordAgreement_(struct ksz_stp_port *p)
+{
+	/*
+	 * Not operPointToPointMAC will keep proposing and root port sending
+	 * agreement forever.
+	 */
+	if (cist()) {
+		if (rstpVersion && operPointToPointMAC &&
+		    (bpduFlags & AGREEMENT)) {
+			agreed = TRUE;
+			proposing = FALSE;
+			if (!rcvdInternal) {
+				FOREACH_M_IN_P(
+					agreed = TRUE;
+					proposing = FALSE;
+				)
+			}
+		} else {
+			agreed = FALSE;
+		}
+	} else {
+		if (operPointToPointMAC &&
+		    !memcmp(&CISTmsgPriority, &CISTportPriority,
+		    sizeof(struct _bridge_id) * 2 + sizeof(u32)) &&
+		    (bpduFlags & AGREEMENT)) {
+			agreed = TRUE;
+			proposing = FALSE;
+		} else {
+			agreed = FALSE;
+		}
+	}
+}
+
+#define recordAgreement()		recordAgreement_(p)
+
+static void recordDispute_(struct ksz_stp_port *p)
+{
+	if ((bpduFlags & LEARNING)) {
+		disputed = TRUE;
+		agreed = FALSE;
+		if (!p->MSTI && !rcvdInternal) {
+			FOREACH_M_IN_P(
+				disputed = TRUE;
+				agreed = FALSE;
+			)
+		}
+	}
+}
+
+#define recordDispute()			recordDispute_(p)
+
+static void recordMastered_(struct ksz_stp_port *p)
+{
+	if (!p->MSTI && !rcvdInternal) {
+		FOREACH_M_IN_P(
+			mastered = FALSE;
+		)
+	}
+	if (p->MSTI) {
+		if (operPointToPointMAC && (bpduFlags & MASTER))
+			mastered = TRUE;
+		else
+			mastered = FALSE;
+	}
+}
+
+#define recordMastered()		recordMastered_(p)
+
+static void recordPriority_(struct ksz_stp_port *p)
+{
+	COPY(portPriority, msgPriority);
+}
+
+#define recordPriority()		recordPriority_(p)
+
+static void recordProposal_(struct ksz_stp_port *p)
+{
+	if (bpduRole == ROLE_DESIGNATED && (bpduFlags & PROPOSAL))
+		proposed = TRUE;
+	if (!p->MSTI && !rcvdInternal) {
+		int flag = proposed;
+
+		FOREACH_M_IN_P(
+			proposed = flag;
+		)
+	}
+}
+
+#define recordProposal()		recordProposal_(p)
+
+#define MIN_COMPAT_HELLO_TIME		1
+#define FIXED_HELLO_TIME		2
+
+static void recordTimes_(struct ksz_stp_port *p)
+{
+	if (cist()) {
+		if (checkParameters(2, msgTimes.max_age,
+		    msgTimes.forward_delay)) {
+			COPY(portTimes, msgTimes);
+
+			portTimes.hello_time = FIXED_HELLO_TIME;
+			portTimes.max_hops = msgTimes.max_hops;
+		} else
+			portTimes.message_age = msgTimes.message_age;
+	} else
+		portTimes.max_hops = msgTimes.max_hops;
+}
+
+#define recordTimes()			recordTimes_(p)
+
+static void setReRootTree_(struct ksz_stp_port *p)
+{
+	FOREACH_P_IN_T(
+		reRoot = TRUE;
+	)
+}
+
+#define setReRootTree()			setReRootTree_(p)
+
+static void setSelectedTree_(struct ksz_stp_bridge *br)
+{
+	struct ksz_stp_port *p = &br->ports[0];
+
+	FOREACH_P_IN_T(
+		if (reselect)
+{
+dbg_msg("  !! %s %d\n", __func__, p->port_index);
+			return;
+}
+	)
+	FOREACH_P_IN_T(
+		selected = TRUE;
+	)
+}
+
+#define setSelectedTree()		setSelectedTree_(br)
+
+static void setSyncTree_(struct ksz_stp_port *p)
+{
+	FOREACH_P_IN_T(
+		sync = TRUE;
+	)
+}
+
+#define setSyncTree()			setSyncTree_(p)
+
+static void setTcFlags_(struct ksz_stp_port *p)
+{
+	if (bpduFlags & TOPOLOGY_CHANGE) {
+		rcvdTc = TRUE;
+		if (!p->MSTI && !rcvdInternal) {
+			FOREACH_M_IN_P(
+				rcvdTc = TRUE;
+			)
+		}
+	}
+	if (!p->MSTI && (bpduFlags & TOPOLOGY_CHANGE_ACK))
+		rcvdTcAck = TRUE;
+}
+
+#define setTcFlags()			setTcFlags_(p)
+
+static void setTcPropTree_(struct ksz_stp_port *p)
+{
+	struct ksz_stp_port *q = p;
+
+	if (restrictedTcn)
+		return;
+#if 1
+	/*
+	 * Topology Change Acknowledge is no longer used!  The port will
+	 * receive at least 2 Toplogy Change.  With self detection the
+	 * PROPAGATING state may be activated up to 4 times!
+	 * In newTcWhile the timer will only be updated if it is empty, but
+	 * doFlush will be called many times which is really not necessary.
+	 */
+	if (sendRSTP) {
+		if (tcPropWhile)
+			return;
+		tcPropWhile = to_stp_timer(CISTportTimes.hello_time + 1);
+	}
+#endif
+	FOREACH_P_IN_T(
+		if (p == q)
+			continue;
+		tcProp = TRUE;
+	)
+}
+
+#define setTcPropTree()			setTcPropTree_(p)
+
+static void syncMaster_(struct ksz_stp_bridge *br)
+{
+	struct ksz_stp_port *p = &br->ports[0];
+
+dbg_msg("%s\n", __func__);
+	FOREACH_P_IN_T(
+		if (!infoInternal)
+			continue;
+		FOREACH_M_IN_P(
+			agree = agreed = synced = FALSE;
+			sync = TRUE;
+		)
+	)
+}
+
+#define syncMaster()			syncMaster_(p->br)
+
+static int stp_xmit(struct ksz_stp_info *stp, u8 port)
+{
+	int rc;
+	struct sk_buff *skb;
+	u8 *frame = stp->tx_frame;
+	struct ksz_sw *sw = stp->sw_dev;
+	int len = stp->len;
+	uint ports;
+	const struct net_device_ops *ops = stp->dev->netdev_ops;
+	struct llc *llc = (struct llc *) &frame[12];
+	struct ksz_port_info *info = get_port_info(sw, port);
+
+	/* Do not send if network device is not ready. */
+	if (!netif_running(stp->dev))
+		return 0;
+
+	ports = (1 << port);
+	ports |= TAIL_TAG_SET_OVERRIDE;
+	ports |= TAIL_TAG_SET_QUEUE;
+
+	len += 3;
+	llc->len = htons(len);
+	len += 14;
+	if (len < 60) {
+		memset(&frame[len], 0, 60 - len);
+		len = 60;
+	}
+
+	/* Add extra for tail tagging. */
+	skb = dev_alloc_skb(len + 4 + 8);
+	if (!skb)
+		return -ENOMEM;
+
+	memcpy(skb->data, stp->tx_frame, len);
+	memcpy(&skb->data[6], info->mac_addr, ETH_ALEN);
+
+	skb_put(skb, len);
+	sw->net_ops->add_tail_tag(sw, skb, ports);
+	skb->protocol = htons(STP_TAG_TYPE);
+	skb->dev = stp->dev;
+	do {
+		struct ksz_sw *sw = stp->sw_dev;
+
+		rc = ops->ndo_start_xmit(skb, skb->dev);
+		if (NETDEV_TX_BUSY == rc) {
+			rc = wait_event_interruptible_timeout(sw->queue,
+				!netif_queue_stopped(stp->dev),
+				50 * HZ / 1000);
+
+			rc = NETDEV_TX_BUSY;
+		}
+	} while (NETDEV_TX_BUSY == rc);
+	return rc;
+}  /* stp_xmit */
+
+static void txConfig_(struct ksz_stp_port *p)
+{
+	int rc;
+	struct stp_prio prio;
+	struct ksz_stp_info *stp = p->br->parent;
+	struct bpdu *bpdu = stp->bpdu;
+
+	memcpy(&prio.root, &designatedPriority.root,
+		sizeof(struct _bridge_id) + sizeof(u32));
+	memcpy(&prio.bridge_id, &designatedPriority.bridge_id,
+		sizeof(struct _bridge_id) + sizeof(struct _port_id));
+	prep_stp(bpdu, &prio, &designatedTimes, &portTimes);
+	bpdu->flags = 0;
+	if (tcWhile != 0)
+		bpdu->flags |= TOPOLOGY_CHANGE;
+	if (tcAck)
+		bpdu->flags |= TOPOLOGY_CHANGE_ACK;
+
+	stp->len = sizeof(struct bpdu_v1) - 1;
+	rc = stp_xmit(stp, p->port_index);
+}
+
+#define txConfig()			txConfig_(p)
+
+static void txRstp_(struct ksz_stp_port *p)
+{
+	int rc;
+	struct stp_prio prio;
+	struct ksz_stp_info *stp = p->br->parent;
+	struct bpdu *bpdu = stp->bpdu;
+	u8 r = role;
+
+if (!portEnabled)
+dbg_msg(" ?! %s %d %d\n", __func__, p->port_index, r);
+	memcpy(&prio.root, &designatedPriority.root,
+		sizeof(struct _bridge_id) + sizeof(u32));
+	memcpy(&prio.bridge_id, &designatedPriority.regional_root,
+		sizeof(struct _bridge_id));
+	memcpy(&prio.port_id, &designatedPriority.port_id,
+		sizeof(struct _port_id));
+	prep_rstp(bpdu, &prio, &designatedTimes, &portTimes);
+	if (AlternatePort || BackupPort)
+		r = PORT_ROLE_ALTERNATE;
+if (r != PORT_ROLE_ALTERNATE && r != PORT_ROLE_ROOT && r != PORT_ROLE_DESIGNATED)
+dbg_msg("  invalid role %d\n", r);
+	r &= PORT_ROLE_DESIGNATED;
+	r <<= PORT_ROLE_S;
+	bpdu->flags = r;
+	if (tcWhile != 0)
+		bpdu->flags |= TOPOLOGY_CHANGE;
+	if (agree)
+		bpdu->flags |= AGREEMENT;
+	if (proposing)
+		bpdu->flags |= PROPOSAL;
+	if (learning)
+		bpdu->flags |= LEARNING;
+	if (forwarding)
+		bpdu->flags |= FORWARDING;
+	/*
+	 * Topology Change Acknowledge is no longer used in MSTP as the bit is
+	 * used by Master indication, similar to Agreement.
+	 */
+	if (ForceProtocolVersion < 3 && tcAck)
+		bpdu->flags |= TOPOLOGY_CHANGE_ACK;
+
+	stp->len = sizeof(struct bpdu_v1);
+	if (ForceProtocolVersion >= 3) {
+		struct mstp_bpdu *mstp = &bpdu->mstp;
+		struct msti_cfg *cfg = (struct msti_cfg *)(bpdu + 1);
+
+		bpdu->version = 3;
+		stp->len = sizeof(struct bpdu_v1);
+		memcpy(&mstp->MCID, &MstConfigId, sizeof(struct mst_cfg_id));
+		mstp->internal_root_path_cost =
+			designatedPriority.internal_root_path_cost;
+		memcpy(&mstp->id, &designatedPriority.bridge_id,
+			sizeof(struct _bridge_id));
+		mstp->max_hops = designatedTimes.max_hops;
+		FOREACH_M_IN_P(
+			p->br->MSTI = p->MSTI;
+			r = role;
+			if (AlternatePort || BackupPort)
+				r = PORT_ROLE_ALTERNATE;
+			else if (MasterPort)
+				r = PORT_ROLE_MASTER;
+			r &= PORT_ROLE_DESIGNATED;
+			r <<= PORT_ROLE_S;
+			cfg->flags = r;
+			if (tcWhile != 0)
+				cfg->flags |= TOPOLOGY_CHANGE;
+			if (agree)
+				cfg->flags |= AGREEMENT;
+			if (proposing)
+				cfg->flags |= PROPOSAL;
+			if (learning)
+				cfg->flags |= LEARNING;
+			if (forwarding)
+				cfg->flags |= FORWARDING;
+			if (master)
+				cfg->flags |= MASTER;
+			memcpy(&cfg->root, &designatedPriority.regional_root,
+				sizeof(struct _bridge_id));
+			cfg->root_path_cost =
+				designatedPriority.internal_root_path_cost;
+			cfg->br_prio = BridgeIdentifier.prio & 0xF0;
+			cfg->port_prio = portId.prio & 0xF0;
+			cfg->max_hops = designatedTimes.max_hops;
+			cfg++;
+		)
+		p->MSTI = p->br->MSTI = 0;
+		mstp->version_3_length = sizeof(struct mstp_bpdu);
+		mstp->version_3_length +=
+			(p->br->MSTI_cnt - 1) * sizeof(struct msti_cfg);
+		stp->len += mstp->version_3_length;
+		mstp->version_3_length = htons(mstp->version_3_length - 2);
+	}
+
+#ifdef DBG_STP_TX
+	do {
+		int cmp = memcmp(&p->tx_bpdu0, bpdu, stp->len);
+
+		if (cmp) {
+			p->dbg_tx++;
+			memcpy(&p->tx_bpdu0, bpdu, stp->len);
+		}
+		if (p->dbg_tx) {
+			dbg_msg("<T> %d: ", p->port_index);
+			disp_bpdu(bpdu);
+			p->dbg_tx--;
+		}
+	} while (0);
+#endif
+	rc = stp_xmit(stp, p->port_index);
+}
+
+#define txRstp()			txRstp_(p)
+
+static void txTcn_(struct ksz_stp_port *p)
+{
+	int rc;
+	struct ksz_stp_info *stp = p->br->parent;
+	struct bpdu *bpdu = stp->bpdu;
+
+	bpdu->protocol = 0;
+	bpdu->version = 0;
+	bpdu->type = BPDU_TYPE_TCN;
+
+	stp->len = 4;
+	rc = stp_xmit(stp, p->port_index);
+}
+
+#define txTcn()				txTcn_(p)
+
+static void updtBPDUVersion_(struct ksz_stp_port *p)
+{
+	if ((0 == bpduVersion || 1 == bpduVersion) &&
+	    (BPDU_TYPE_TCN == bpduType || BPDU_TYPE_CONFIG == bpduType))
+		rcvdSTP = TRUE;
+	if (BPDU_TYPE_CONFIG_RSTP == bpduType)
+		rcvdRSTP = TRUE;
+}
+
+#define updtBPDUVersion()		updtBPDUVersion_(p)
+
+static void updtRcvdInfoWhile_(struct ksz_stp_port *p)
+{
+	/*
+	 * Definition is not clear!
+	 * It is mentioned several times that Message Age should be less than
+	 * Max Age in BPDU to be accepted.
+	 */
+	if (!rcvdInternal && portTimes.message_age + 1 <= portTimes.max_age)
+		rcvdInfoWhile = to_stp_timer(3 * portTimes.hello_time);
+	else if (rcvdInternal && portTimes.max_hops > 1)
+		rcvdInfoWhile = to_stp_timer(3 * portTimes.hello_time);
+	else
+		rcvdInfoWhile = 0;
+}
+
+#define updtRcvdInfoWhile()		updtRcvdInfoWhile_(p)
+
+static void updtRoleDisabledTree_(struct ksz_stp_bridge *br)
+{
+	struct ksz_stp_port *p = &br->ports[0];
+
+	FOREACH_P_IN_T(
+		selectedRole = ROLE_DISABLED;
+	)
+}
+
+#define updtRoleDisabledTree()		updtRoleDisabledTree_(br)
+
+static u32 add_path_cost(u32 x, u32 y)
+{
+	u32 z;
+
+	z = ntohl(x) + y;
+	return htonl(z);
+}
+
+static void updtRole(struct ksz_stp_port *p, struct mstp_prio *priority)
+{
+	int id;
+	int prio;
+	int time;
+#ifdef DBG_STP_PORT_FLUSH
+	struct ksz_stp_dbg_times *x = &p->dbg_times[p->MSTI];
+#endif
+
+	COPY(designatedPriority, rootPriority);
+	COPY(designatedPriority.bridge_id, BridgeIdentifier);
+	COPY(designatedPriority.port_id, portId);
+
+	COPY(designatedTimes, rootTimes);
+
+	/*
+	 * HelloTime was changed to come froom portTimes rather than
+	 * designatedTimes.
+	 */
+	portTimes.hello_time = designatedTimes.hello_time;
+
+	if (INFO_TYPE_UNKNOWN == infoIs)
+dbg_msg("unknown\n");
+	if (INFO_TYPE_DISABLED == infoIs) {
+		selectedRole = ROLE_DISABLED;
+#ifdef DBG_STP_PORT_FLUSH
+		x->role_ = ROLE_DISABLED;
+		x->downPriority.port_id.num = 0;
+		x->learn_jiffies = 0;
+#endif
+	}
+	if (msti() && (INFO_TYPE_RECEIVED == CISTinfoIs) && !infoInternal) {
+		prio = 0;
+		if (ROLE_ROOT == CISTselectedRole) {
+			selectedRole = ROLE_MASTER;
+			prio = 1;
+		} else if (ROLE_ALTERNATE == CISTselectedRole) {
+			selectedRole = ROLE_ALTERNATE;
+			prio = 1;
+		}
+		if (prio) {
+			prio = CMP(designatedPriority, portPriority);
+			time = CMP(designatedTimes, portTimes);
+			if (prio || time)
+				updtInfo = TRUE;
+		}
+	} else {
+		if (INFO_TYPE_AGED == infoIs) {
+			updtInfo = TRUE;
+			selectedRole = ROLE_DESIGNATED;
+#ifdef DBG_STP_PORT_FLUSH
+			if (ROLE_DISABLED == x->role_)
+				x->role_ = ROLE_DESIGNATED;
+#endif
+#ifdef DBG_STP_RX
+p->dbg_rx = 3;
+#endif
+#ifdef DBG_STP_TX
+p->dbg_tx = 2;
+#endif
+		} else if (INFO_TYPE_MINE == infoIs) {
+			selectedRole = ROLE_DESIGNATED;
+			prio = CMP(designatedPriority, portPriority);
+			time = CMP(designatedTimes, portTimes);
+			if (prio || time)
+				updtInfo = TRUE;
+#ifdef DBG_STP_PORT_FLUSH
+			if (ROLE_DISABLED == x->role_)
+				x->role_ = ROLE_DESIGNATED;
+#endif
+#ifdef DBG_STP_RX
+p->dbg_rx = 3;
+#endif
+#ifdef DBG_STP_TX
+p->dbg_tx = 1;
+#endif
+		} else if (INFO_TYPE_RECEIVED == infoIs) {
+			if (priority == &portPriority) {
+				selectedRole = ROLE_ROOT;
+				updtInfo = FALSE;
+			} else {
+				prio = CMP(designatedPriority, portPriority);
+				if (prio >= 0) {
+					id = CMP(portPriority.bridge_id.addr,
+						BridgeIdentifier.addr);
+					if (id)
+						selectedRole = ROLE_ALTERNATE;
+					else
+						selectedRole = ROLE_BACKUP;
+					updtInfo = FALSE;
+#ifdef DBG_STP_PORT_FLUSH
+					x->role_ = ROLE_DESIGNATED;
+#endif
+				} else {
+					selectedRole = ROLE_DESIGNATED;
+					updtInfo = TRUE;
+				}
+			}
+#ifdef DBG_STP_RX
+p->dbg_rx = 3;
+#endif
+#ifdef DBG_STP_TX
+p->dbg_tx = 3;
+#endif
+		}
+	}
+#ifdef DBG_STP_ROLE
+if (!p->off)
+dbg_msg("  %s %d:%d %d\n", __func__, p->port_index, p->MSTI, selectedRole);
+#endif
+}
+
+static void updtRolesTree_(struct ksz_stp_bridge *br)
+{
+	int better;
+	int prio;
+	struct ksz_stp_port *p = &br->ports[0];
+	struct mstp_vector root_path;
+	struct mstp_vector best_path;
+	struct mstp_prio prevPriority;
+	struct mstp_prio *root_Priority = NULL;
+	struct _port_id prevPortId;
+	struct _port_id root_PortId;
+	struct ksz_stp_port *q = NULL;
+
+	COPY(root_PortId, BridgePriority.port_id);
+
+	/* Find out the best path from all ports. */
+	FOREACH_P_IN_T(
+
+		/* Check only after receiving new BPDU. */
+		if (infoIs != INFO_TYPE_RECEIVED)
+			continue;
+
+		/* Not from self. */
+		prio = CMP(portPriority.bridge_id.addr, BridgeIdentifier.addr);
+		if (!prio)
+			continue;
+
+		if (restrictedRole)
+			continue;
+
+		/* Ports may receive same BPDU when coming through a hub. */
+		COPY_PRIO(root_path, portPriority, portId);
+		if (!infoInternal) {
+			root_path.prio.internal_root_path_cost = 0;
+			COPY(root_path.prio.regional_root, BridgeIdentifier);
+			root_path.prio.root_path_cost = add_path_cost(
+				root_path.prio.root_path_cost,
+				ExternalPortPathCost);
+		} else
+			root_path.prio.internal_root_path_cost = add_path_cost(
+				root_path.prio.internal_root_path_cost,
+				InternalPortPathCost);
+
+		better = FALSE;
+		if (!root_Priority)
+			better = TRUE;
+		else if (betterVector(&root_path, &best_path))
+			better = TRUE;
+		if (better) {
+			COPY(best_path, root_path);
+			root_Priority = &portPriority;
+			COPY(root_PortId, portId);
+			q = p;
+		}
+	)
+#ifdef DBG_STP_ROLE
+if (root_Priority) {
+dbg_msg(" best \n");
+dbgPriority(&best_path.prio, &best_path.port_id);
+#if 0
+dbg_msg(" root prio\n");
+dbgPriority(root_Priority, &root_PortId);
+#endif
+}
+#endif
+
+	/* Compare with the bridge. */
+	better = FALSE;
+	if (!root_Priority)
+		better = TRUE;
+	else if (betterVector(&BridgePriority, &best_path))
+		better = TRUE;
+	if (better) {
+		COPY(best_path, BridgePriority);
+		root_Priority = NULL;
+		COPY(root_PortId, BridgePriority.port_id);
+	}
+
+	COPY(prevPriority, rootPriority);
+	COPY(prevPortId, rootPortId);
+
+	COPY(rootPriority, best_path.prio);
+	COPY(rootPortId, best_path.port_id);
+
+	if (root_Priority) {
+		struct ksz_stp_info *stp = br->parent;
+		struct ksz_sw *sw = stp->sw_dev;
+		uint num;
+
+		num = get_phy_port(sw, root_PortId.num);
+		p = &br->ports[num];
+		COPY(rootTimes, portTimes);
+		if (!rcvdInternal)
+			rootTimes.message_age += 1;
+		else
+			rootTimes.max_hops -= 1;
+	} else {
+		COPY(rootTimes, BridgeTimes);
+	}
+
+	if (cist() && (CMP(prevPriority, rootPriority) ||
+	    CMP(prevPortId, rootPortId)) &&
+	    CMP(prevPriority.regional_root, rootPriority.regional_root) &&
+	    (prevPriority.root_path_cost || rootPriority.root_path_cost))
+		syncMaster();
+
+	FOREACH_P_IN_T(
+		updtRole(p, root_Priority);
+	)
+}
+
+#define updtRolesTree()			updtRolesTree_(br)
+
+
+enum {
+	STP_BEGIN,
+	STP_LAST
+};
+
+enum {
+	STP_PortTimers_ONE_SECOND = STP_LAST,
+	STP_PortTimers_TICK,
+};
+
+enum {
+	STP_PortReceive_DISCARD = STP_LAST,
+	STP_PortReceive_RECEIVE,
+};
+
+enum {
+	STP_PortProtoMigr_CHECKING_RSTP = STP_LAST,
+	STP_PortProtoMigr_SELECTING_STP,
+	STP_PortProtoMigr_SENSING,
+};
+
+enum {
+	STP_BridgeDetection_EDGE = STP_LAST,
+	STP_BridgeDetection_NOT_EDGE,
+	STP_BridgeDetection_ISOLATED,
+};
+
+enum {
+	STP_PortTransmit_INIT = STP_LAST,
+	STP_PortTransmit_PERIODIC,
+	STP_PortTransmit_CONFIG,
+	STP_PortTransmit_TCN,
+	STP_PortTransmit_RSTP,
+	STP_PortTransmit_IDLE,
+};
+
+enum {
+	STP_PortInfo_DISABLED = STP_LAST,
+	STP_PortInfo_AGED,
+	STP_PortInfo_UPDATE,
+	STP_PortInfo_SUPERIOR_DESIGNATED,
+	STP_PortInfo_REPEATED_DESIGNATED,
+	STP_PortInfo_INFERIOR_DESIGNATED,
+	STP_PortInfo_NOT_DESIGNATED,
+	STP_PortInfo_OTHER,
+	STP_PortInfo_CURRENT,
+	STP_PortInfo_RECEIVE,
+};
+
+enum {
+	STP_PortRoleSel_INIT_BRIDGE = STP_LAST,
+	STP_PortRoleSel_ROLE_SELECTION,
+};
+
+enum {
+	STP_PortRoleTrans_INIT_PORT = STP_LAST,
+	STP_PortRoleTrans_DISABLE_PORT,
+	STP_PortRoleTrans_DISABLED_PORT,
+	STP_PortRoleTrans_MASTER_PROPOSED,
+	STP_PortRoleTrans_MASTER_AGREED,
+	STP_PortRoleTrans_MASTER_SYNCED,
+	STP_PortRoleTrans_MASTER_RETIRED,
+	STP_PortRoleTrans_MASTER_DISCARD,
+	STP_PortRoleTrans_MASTER_LEARN,
+	STP_PortRoleTrans_MASTER_FORWARD,
+	STP_PortRoleTrans_MASTER_PORT,
+	STP_PortRoleTrans_ROOT_PROPOSED,
+	STP_PortRoleTrans_ROOT_AGREED,
+	STP_PortRoleTrans_ROOT_SYNCED,
+	STP_PortRoleTrans_ROOT_RETIRED,
+	STP_PortRoleTrans_ROOT_DISCARD,
+	STP_PortRoleTrans_REROOT,
+	STP_PortRoleTrans_REROOTED,
+	STP_PortRoleTrans_ROOT_LEARN,
+	STP_PortRoleTrans_ROOT_FORWARD,
+	STP_PortRoleTrans_ROOT_PORT,
+	STP_PortRoleTrans_DESIGNATED_PROPOSE,
+	STP_PortRoleTrans_DESIGNATED_AGREED,
+	STP_PortRoleTrans_DESIGNATED_SYNCED,
+	STP_PortRoleTrans_DESIGNATED_RETIRED,
+	STP_PortRoleTrans_DESIGNATED_DISCARD,
+	STP_PortRoleTrans_DESIGNATED_LEARN,
+	STP_PortRoleTrans_DESIGNATED_FORWARD,
+	STP_PortRoleTrans_DESIGNATED_PORT,
+	STP_PortRoleTrans_BLOCK_PORT,
+	STP_PortRoleTrans_ALTERNATE_PROPOSED,
+	STP_PortRoleTrans_ALTERNATE_AGREED,
+	STP_PortRoleTrans_BACKUP_PORT,
+	STP_PortRoleTrans_ALTERNATE_PORT,
+};
+
+enum {
+	STP_PortStateTrans_DISCARDING = STP_LAST,
+	STP_PortStateTrans_LEARNING,
+	STP_PortStateTrans_FORWARDING,
+};
+
+enum {
+	STP_TopologyChange_INACTIVE = STP_LAST,
+	STP_TopologyChange_LEARNING,
+	STP_TopologyChange_DETECTED,
+	STP_TopologyChange_ACKNOWLEDGED,
+	STP_TopologyChange_PROPAGATING,
+	STP_TopologyChange_NOTIFIED_TC,
+	STP_TopologyChange_NOTIFIED_TCN,
+	STP_TopologyChange_ACTIVE,
+};
+
+enum {
+	STP_PortTimers,
+	STP_PortReceive,
+	STP_PortProtoMigr,
+	STP_BridgeDetection,
+	STP_PortInfo,
+	STP_PortRoleTrans,
+	STP_PortStateTrans,
+	STP_TopologyChange,
+	STP_PortTransmit,
+
+	STP_PortRoleSel,
+};
+
+#ifdef DBG_STP_STATE
+static char *PortTimers_names[] = {
+	"ONE_SECOND",
+	"TICK",
+};
+
+static char *PortReceive_names[] = {
+	"DISCARD,"
+	"RECEIVE",
+};
+
+static char *PortProtoMigr_names[] = {
+	"CHECKING_RSTP",
+	"SELECTING_STP",
+	"SENSING",
+};
+
+static char *BridgeDetection_names[] = {
+	"EDGE",
+	"NOT_EDGE",
+	"ISOLATED",
+};
+
+static char *PortTransmit_names[] = {
+	"INIT",
+	"PERIODIC",
+	"CONFIG",
+	"TCN",
+	"RSTP",
+	"IDLE",
+};
+
+static char *PortInfo_names[] = {
+	"DISABLED",
+	"AGED",
+	"UPDATE",
+	"SUPERIOR_DESIGNATED",
+	"REPEATED_DESIGNATED",
+	"INFERIOR_DESIGNATED",
+	"NOT_DESIGNATED",
+	"OTHER",
+	"CURRENT",
+	"RECEIVE",
+};
+
+static char *PortRoleSel_names[] = {
+	"INIT_BRIDGE",
+	"ROLE_SELECTION",
+};
+
+static char *PortRoleTrans_names[] = {
+	"INIT_PORT",
+	"DISABLE_PORT",
+	"DISABLED_PORT",
+	"MASTER_PROPOSED",
+	"MASTER_AGREED",
+	"MASTER_SYNCED",
+	"MASTER_RETIRED",
+	"MASTER_DISCARD",
+	"MASTER_LEARN",
+	"MASTER_FORWARD",
+	"MASTER_PORT",
+	"ROOT_PROPOSED",
+	"ROOT_AGREED",
+	"ROOT_SYNCED",
+	"ROOT_RETIRED",
+	"ROOT_DISCARD",
+	"REROOT",
+	"REROOTED",
+	"ROOT_LEARN",
+	"ROOT_FORWARD",
+	"ROOT_PORT",
+	"DESIGNATED_PROPOSE",
+	"DESIGNATED_AGREED",
+	"DESIGNATED_SYNCED",
+	"DESIGNATED_RETIRED",
+	"DESIGNATED_DISCARD",
+	"DESIGNATED_LEARN",
+	"DESIGNATED_FORWARD",
+	"DESIGNATED_PORT",
+	"BLOCK_PORT",
+	"ALTERNATE_PROPOSED",
+	"ALTERNATE_AGREED",
+	"BACKUP_PORT",
+	"ALTERNATE_PORT",
+};
+
+static char *PortStateTrans_names[] = {
+	"DISCARDING",
+	"LEARNING",
+	"FORWARDING",
+};
+
+static char *TopologyChange_names[] = {
+	"INACTIVE",
+	"LEARNING",
+	"DETECTED",
+	"ACKNOWLEDGED",
+	"PROPAGATING",
+	"NOTIFIED_TC",
+	"NOTIFIED_TCN",
+	"ACTIVE",
+};
+
+static char **stp_state_names[] = {
+	PortTimers_names,
+	PortReceive_names,
+	PortProtoMigr_names,
+	BridgeDetection_names,
+	PortInfo_names,
+	PortRoleTrans_names,
+	PortStateTrans_names,
+	TopologyChange_names,
+	PortTransmit_names,
+
+	PortRoleSel_names,
+};
+#endif
+
+
+struct ksz_stp_state {
+	int index;
+	int change;
+	int new_state;
+};
+
+static int stp_proc_state(struct ksz_stp_port *p, struct ksz_stp_state *state,
+	void (*state_init)(struct ksz_stp_port *p),
+	void (*state_next)(struct ksz_stp_port *p, struct ksz_stp_state *state))
+{
+	if (state->new_state) {
+		state->new_state = 0;
+		state_init(p);
+	}
+	state_next(p, state);
+	return 0;
+}  /* stp_proc_state */
+
+static void stp_change_state(struct ksz_stp_state *state, int cond, int new)
+{
+	if (!cond)
+		return;
+	if (state->new_state) {
+#ifdef DBG_STP_STATE
+		if (state->change) {
+			char **names = stp_state_names[state->index];
+			char *last;
+			char *next;
+
+			if (state->new_state != STP_BEGIN)
+				last = names[state->new_state - STP_LAST];
+			else
+				last = "BEGIN";
+			if (new != STP_BEGIN)
+				next = names[new - STP_LAST];
+			else
+				next = "BEGIN";
+dbg_msg("  %s %d %s %d %s\n", __func__, state->new_state, last, new, next);
+		}
+#endif
+		return;
+	}
+	state->new_state = new;
+}  /* stp_change_state */
+
+#if defined(DBG_STP_STATE) || defined(DBG)
+static void dbg_stp(struct ksz_stp_port *p, const char *msg, int rx)
+{
+	if (rx && !p->dbg_rx)
+		return;
+	dbg_msg(" %d:%d=", p->port_index, p->MSTI);
+	switch (role) {
+	case ROLE_DISABLED:
+		dbg_msg("Z");
+		break;
+	case ROLE_DESIGNATED:
+		dbg_msg("D");
+		break;
+	case ROLE_ROOT:
+		dbg_msg("R");
+		break;
+	case ROLE_ALTERNATE:
+		dbg_msg("A");
+		break;
+	case ROLE_BACKUP:
+		dbg_msg("B");
+		break;
+	case ROLE_MASTER:
+		dbg_msg("M");
+		break;
+	default:
+		dbg_msg(".");
+	}
+	switch (selectedRole) {
+	case ROLE_DISABLED:
+		dbg_msg("z");
+		break;
+	case ROLE_DESIGNATED:
+		dbg_msg("d");
+		break;
+	case ROLE_ROOT:
+		dbg_msg("r");
+		break;
+	case ROLE_ALTERNATE:
+		dbg_msg("a");
+		break;
+	case ROLE_BACKUP:
+		dbg_msg("b");
+		break;
+	case ROLE_MASTER:
+		dbg_msg("m");
+		break;
+	default:
+		dbg_msg(".");
+	}
+	dbg_msg(" ");
+	switch (infoIs) {
+	case INFO_TYPE_DISABLED:
+		dbg_msg("Z");
+		break;
+	case INFO_TYPE_MINE:
+		dbg_msg("M");
+		break;
+	case INFO_TYPE_AGED:
+		dbg_msg("A");
+		break;
+	case INFO_TYPE_RECEIVED:
+		dbg_msg("R");
+		break;
+	default:
+		dbg_msg(".");
+	}
+	dbg_msg(" ");
+	dbg_msg("%c", agree ? 'A' : '.');
+	dbg_msg("%c", agreed ? 'a' : '.');
+	dbg_msg("%c", master ? 'M' : '.');
+	dbg_msg("%c", mastered ? 'm' : '.');
+	dbg_msg("%c", proposing ? 'P' : '.');
+	dbg_msg("%c", proposed ? 'p' : '.');
+	dbg_msg("%c", sync ? 'S' : '.');
+	dbg_msg("%c", synced ? 's' : '.');
+	dbg_msg("%c", learn ? 'L' : '.');
+	dbg_msg("%c", learning ? 'l' : '.');
+	dbg_msg("%c", forward ? 'F' : '.');
+	dbg_msg("%c", forwarding ? 'f' : '.');
+	dbg_msg("%c", disputed ? 'D' : '.');
+	dbg_msg("%c", isolate ? 'I' : '.');
+	dbg_msg("%c", operEdge ? 'E' : '.');
+	dbg_msg("%c", reRoot ? 'R' : '.');
+	dbg_msg("%c", tcProp ? 'T' : '.');
+	dbg_msg("%c", rcvdTc ? 'C' : '.');
+	dbg_msg("  ");
+	for (rx = 0; rx < NUM_OF_PORT_STATE_MACHINES; rx++)
+		dbg_msg("%2d", p->states[rx][p->MSTI]);
+	dbg_msg("  %s\n", msg);
+}  /* dbg_stp */
+
+static void d_stp_states(struct ksz_stp_bridge *br)
+{
+	int i;
+	struct ksz_stp_port *p;
+
+	dbg_msg("\n");
+	for (i = 0; i < br->port_cnt; i++) {
+		p = &br->ports[i];
+		if (p->off)
+			continue;
+		p->MSTI = 0;
+		FOREACH_T_IN_P(
+			dbg_stp(p, "", false);
+		)
+	}
+}
+#endif
+
+
+static void stp_one_sec_init(struct ksz_stp_port *p)
+{
+	tick = FALSE;
+}  /* stp_one_sec_init */
+
+static void stp_one_sec_next(struct ksz_stp_port *p,
+	struct ksz_stp_state *state)
+{
+	stp_change_state(state, (tick), STP_PortTimers_TICK);
+}  /* stp_one_sec_next */
+
+#define dec_timer(x)	if (x) x -= STP_TIMER_TICK
+#define dec(x)		if (x) x--
+
+static void stp_tick_init(struct ksz_stp_port *p)
+{
+	int i;
+
+	for (i = 0; i < NUM_OF_PORT_TIMERS; i++)
+		dec_timer(p->vars.timers[i]);
+	FOREACH_T_IN_P(
+		for (i = 0; i < NUM_OF_MSTI_TIMERS; i++)
+			dec_timer(p->mvars[p->MSTI].timers[i]);
+		p->br->MSTI = p->MSTI;
+		if (!tcWhile)
+			isTC &= ~(1 << p->port_index);
+		if (!p->port_index && !(helloWhen % STP_TIMER_SCALE))
+			timeSinceTC++;
+	)
+	p->br->MSTI = 0;
+	if (!(helloWhen % STP_TIMER_SCALE))
+		dec(txCount);
+}  /* stp_tick_init */
+
+static void stp_tick_next(struct ksz_stp_port *p,
+	struct ksz_stp_state *state)
+{
+	stp_change_state(state, TRUE, STP_PortTimers_ONE_SECOND);
+}  /* stp_tick_next */
+
+static int PortTimers(struct ksz_stp_port *p)
+{
+	struct ksz_stp_state state_info;
+	u8 *state = &p->states[p->state_index][0];
+	int ret = 0;
+
+	state_info.index = STP_PortTimers;
+	state_info.change = 1;
+	state_info.new_state = 0;
+	if (STP_BEGIN == *state) {
+		state_info.new_state = STP_PortTimers_ONE_SECOND;
+	}
+	do {
+
+		/* There is a new state. */
+		if (state_info.new_state) {
+			*state = state_info.new_state;
+			ret = state_info.change;
+			ret = 1;
+		}
+		switch (*state) {
+		case STP_PortTimers_ONE_SECOND:
+			if (stp_proc_state(p, &state_info,
+			    stp_one_sec_init, stp_one_sec_next))
+				goto done;
+			break;
+		case STP_PortTimers_TICK:
+			if (stp_proc_state(p, &state_info,
+			    stp_tick_init, stp_tick_next))
+				goto done;
+			break;
+		}
+	} while (state_info.new_state);
+
+done:
+	return ret;
+}  /* PortTimers */
+
+static void stp_rx_discard_init(struct ksz_stp_port *p)
+{
+#ifdef DBG_STP_STATE_RX
+	dbg_stp(p, __func__, false);
+#endif
+	rcvdBPDU = rcvdRSTP = rcvdSTP = FALSE;
+	clearAllRcvdMsgs();
+	edgeDelayWhile = MigrateTime;
+}  /* stp_rx_discard_init */
+
+static void stp_rx_discard_next(struct ksz_stp_port *p,
+	struct ksz_stp_state *state)
+{
+	stp_change_state(state,
+		(rcvdBPDU && portEnabled && enableBPDUrx),
+		STP_PortReceive_RECEIVE);
+}  /* stp_rx_discard_next */
+
+static void stp_rx_receive_init(struct ksz_stp_port *p)
+{
+#ifdef DBG_STP_STATE_RX
+	dbg_stp(p, __func__, true);
+#endif
+	updtBPDUVersion();
+	rcvdInternal = fromSameRegion();
+	rcvMsgs();
+	isolate =
+	operEdge = rcvdBPDU = FALSE;
+	edgeDelayWhile = MigrateTime;
+}  /* stp_rx_receive_init */
+
+static void stp_rx_receive_next(struct ksz_stp_port *p,
+	struct ksz_stp_state *state)
+{
+	stp_change_state(state,
+		(rcvdBPDU && portEnabled && enableBPDUrx && !rcvdAnyMsg()),
+		STP_PortReceive_RECEIVE);
+}  /* stp_rx_receive_next */
+
+static void stp_rx_next(struct ksz_stp_port *p,
+	struct ksz_stp_state *state)
+{
+	/* Reset rcvdBPDU and edgeDelayWhile if !portEnabled. */
+	stp_change_state(state,
+		((rcvdBPDU || NEQ(edgeDelayWhile, MigrateTime)) &&
+		!portEnabled),
+		STP_PortReceive_DISCARD);
+}  /* stp_rx_next */
+
+static int PortReceive(struct ksz_stp_port *p)
+{
+	struct ksz_stp_state state_info;
+	u8 *state = &p->states[p->state_index][0];
+	int ret = 0;
+
+	state_info.index = STP_PortReceive;
+	state_info.change = 1;
+	state_info.new_state = 0;
+	if (STP_BEGIN == *state) {
+		state_info.new_state = STP_PortReceive_DISCARD;
+	} else
+		stp_rx_next(p, &state_info);
+	do {
+
+		/* There is a new state. */
+		if (state_info.new_state) {
+			*state = state_info.new_state;
+			ret = state_info.change;
+			ret = 1;
+		}
+		switch (*state) {
+		case STP_PortReceive_DISCARD:
+			if (stp_proc_state(p, &state_info,
+			    stp_rx_discard_init, stp_rx_discard_next))
+				goto done;
+			break;
+		case STP_PortReceive_RECEIVE:
+			if (stp_proc_state(p, &state_info,
+			    stp_rx_receive_init, stp_rx_receive_next))
+				goto done;
+			break;
+		}
+	} while (state_info.new_state);
+
+done:
+	return ret;
+}  /* PortReceive */
+
+static void stp_proto_check_init(struct ksz_stp_port *p)
+{
+#ifdef DBG_STP_STATE_PROTO
+	dbg_stp(p, __func__, false);
+#endif
+	mcheck = FALSE;
+	sendRSTP = rstpVersion;
+	mdelayWhile = MigrateTime;
+}  /* stp_proto_check_init */
+
+static void stp_proto_check_next(struct ksz_stp_port *p,
+	struct ksz_stp_state *state)
+{
+	stp_change_state(state,
+		(0 == mdelayWhile),
+		STP_PortProtoMigr_SENSING);
+
+	/* Reset mdelayWhile if !portEnabled */
+	stp_change_state(state,
+		(NEQ(mdelayWhile, MigrateTime) && !portEnabled),
+		STP_PortProtoMigr_CHECKING_RSTP);
+}  /* stp_proto_check_next */
+
+static void stp_proto_select_init(struct ksz_stp_port *p)
+{
+#ifdef DBG_STP_STATE
+	dbg_stp(p, __func__, false);
+#endif
+	sendRSTP = FALSE;
+	mdelayWhile = MigrateTime;
+}  /* stp_proto_select_init */
+
+static void stp_proto_select_next(struct ksz_stp_port *p,
+	struct ksz_stp_state *state)
+{
+	stp_change_state(state,
+		((0 == mdelayWhile) || !portEnabled || mcheck),
+		STP_PortProtoMigr_SENSING);
+}  /* stp_proto_select_next */
+
+static void stp_proto_sense_init(struct ksz_stp_port *p)
+{
+#ifdef DBG_STP_STATE
+	dbg_stp(p, __func__, false);
+#endif
+	rcvdRSTP = rcvdSTP = FALSE;
+}  /* stp_proto_sense_init */
+
+static void stp_proto_sense_next(struct ksz_stp_port *p,
+	struct ksz_stp_state *state)
+{
+	stp_change_state(state,
+		(!portEnabled || mcheck || (rstpVersion && !sendRSTP &&
+		rcvdRSTP)),
+		STP_PortProtoMigr_CHECKING_RSTP);
+	stp_change_state(state,
+		(sendRSTP && rcvdSTP),
+		STP_PortProtoMigr_SELECTING_STP);
+}  /* stp_proto_sense_next */
+
+static int PortProtocolMigration(struct ksz_stp_port *p)
+{
+	struct ksz_stp_state state_info;
+	u8 *state = &p->states[p->state_index][0];
+	int ret = 0;
+
+	state_info.index = STP_PortProtoMigr;
+	state_info.change = 1;
+	state_info.new_state = 0;
+	if (STP_BEGIN == *state) {
+		state_info.new_state = STP_PortProtoMigr_CHECKING_RSTP;
+	}
+	do {
+
+		/* There is a new state. */
+		if (state_info.new_state) {
+			*state = state_info.new_state;
+			ret = state_info.change;
+			ret = 1;
+		}
+		switch (*state) {
+		case STP_PortProtoMigr_CHECKING_RSTP:
+			if (stp_proc_state(p, &state_info,
+			    stp_proto_check_init, stp_proto_check_next))
+				goto done;
+			break;
+		case STP_PortProtoMigr_SELECTING_STP:
+			if (stp_proc_state(p, &state_info,
+			    stp_proto_select_init, stp_proto_select_next))
+				goto done;
+			break;
+		case STP_PortProtoMigr_SENSING:
+			if (stp_proc_state(p, &state_info,
+			    stp_proto_sense_init, stp_proto_sense_next))
+				goto done;
+			break;
+		}
+	} while (state_info.new_state);
+
+done:
+	return ret;
+}  /* PortProtocolMigration */
+
+static void stp_br_det_edge_init(struct ksz_stp_port *p)
+{
+#ifdef DBG_STP_STATE
+	dbg_stp(p, __func__, false);
+#endif
+	operEdge = TRUE;
+	isolate = FALSE;
+do {
+	struct ksz_stp_dbg_times *x = &p->dbg_times[p->MSTI];
+
+if (x->block_jiffies)
+dbg_msg(" b: %d:%d=%ld %s\n", p->port_index, p->MSTI,
+jiffies - x->block_jiffies, __func__);
+} while (0);
+}  /* stp_br_det_edge_init */
+
+static void stp_br_det_edge_next(struct ksz_stp_port *p,
+	struct ksz_stp_state *state)
+{
+	stp_change_state(state,
+		(((!portEnabled || !AutoEdge) && !AdminEdge) || !operEdge),
+		STP_BridgeDetection_NOT_EDGE);
+}  /* stp_br_det_edge_next */
+
+static void stp_br_det_not_init(struct ksz_stp_port *p)
+{
+#ifdef DBG_STP_STATE_BR_DET
+	dbg_stp(p, __func__, false);
+#endif
+	operEdge = FALSE;
+	isolate = FALSE;
+}  /* stp_br_det_not_init */
+
+static void stp_br_det_not_next(struct ksz_stp_port *p,
+	struct ksz_stp_state *state)
+{
+	stp_change_state(state,
+		((!portEnabled && AdminEdge) ||
+		((0 == edgeDelayWhile) && AutoEdge && sendRSTP && proposing)),
+		STP_BridgeDetection_EDGE);
+	if (ForceProtocolVersion <= 2)
+		return;
+
+	/* Get in isolated mode where the port stays closed. */
+	stp_change_state(state,
+		((0 == edgeDelayWhile) && !AdminEdge && !AutoEdge && sendRSTP
+		&& proposing && operPointToPointMAC),
+		STP_BridgeDetection_ISOLATED);
+}  /* stp_br_det_not_next */
+
+static void stp_br_det_isolate_init(struct ksz_stp_port *p)
+{
+#ifdef DBG_STP_STATE_BR_DET
+	dbg_stp(p, __func__, false);
+#endif
+	operEdge = FALSE;
+	isolate = TRUE;
+}  /* stp_br_det_isolate_init */
+
+static void stp_br_det_isolate_next(struct ksz_stp_port *p,
+	struct ksz_stp_state *state)
+{
+	stp_change_state(state,
+		(AdminEdge || AutoEdge || !isolate || !operPointToPointMAC),
+		STP_BridgeDetection_NOT_EDGE);
+}  /* stp_br_det_isolate_next */
+
+static int BridgeDetection(struct ksz_stp_port *p)
+{
+	struct ksz_stp_state state_info;
+	u8 *state = &p->states[p->state_index][0];
+	int ret = 0;
+
+	state_info.index = STP_BridgeDetection;
+	state_info.change = 1;
+	state_info.new_state = 0;
+	if (STP_BEGIN == *state) {
+		state_info.new_state = AdminEdge ?
+			STP_BridgeDetection_EDGE :
+			STP_BridgeDetection_NOT_EDGE;
+	}
+	do {
+
+		/* There is a new state. */
+		if (state_info.new_state) {
+			*state = state_info.new_state;
+			ret = state_info.change;
+			ret = 1;
+		}
+		switch (*state) {
+		case STP_BridgeDetection_EDGE:
+			if (stp_proc_state(p, &state_info,
+			    stp_br_det_edge_init, stp_br_det_edge_next))
+				goto done;
+			break;
+		case STP_BridgeDetection_NOT_EDGE:
+			if (stp_proc_state(p, &state_info,
+			    stp_br_det_not_init, stp_br_det_not_next))
+				goto done;
+			break;
+		case STP_BridgeDetection_ISOLATED:
+			if (stp_proc_state(p, &state_info,
+			    stp_br_det_isolate_init, stp_br_det_isolate_next))
+				goto done;
+			break;
+		}
+	} while (state_info.new_state);
+
+done:
+	return ret;
+}  /* BridgeDetection */
+
+static void stp_tx_init_init(struct ksz_stp_port *p)
+{
+#ifdef DBG_STP_STATE_TX
+	dbg_stp(p, __func__, false);
+#endif
+#if 0
+
+	/*
+	 * Not sure why newInfo is set here.  For STP BPUDs are sent only
+	 * when the port is Designated or Root.  For RSTP the txRstp routine
+	 * can be called even when the port role is not defined!
+	 */
+#if 0
+	/* Send RSTP after initialization when the port is disabled? */
+	if (!sendRSTP)
+#endif
+	newInfo = newInfoMsti = TRUE;
+#endif
+	txCount = 0;
+}  /* stp_tx_init_init */
+
+static void stp_tx_init_next(struct ksz_stp_port *p,
+	struct ksz_stp_state *state)
+{
+	stp_change_state(state,
+		TRUE, STP_PortTransmit_IDLE);
+}  /* stp_tx_init_next */
+
+static void stp_tx_periodic_init(struct ksz_stp_port *p)
+{
+#ifdef DBG_STP_STATE_TX_0
+	dbg_stp(p, __func__, false);
+#endif
+	newInfo = newInfo || (cistDesignatedPort() ||
+		(cistRootPort() && (tcWhile != 0)));
+	newInfoMsti = newInfoMsti || mstiDesignatedOrTCpropagatingRootPort();
+}  /* stp_tx_periodic_init */
+
+static void stp_tx_periodic_next(struct ksz_stp_port *p,
+	struct ksz_stp_state *state)
+{
+	stp_change_state(state,
+		TRUE, STP_PortTransmit_IDLE);
+}  /* stp_tx_periodic_next */
+
+static void stp_tx_config_init(struct ksz_stp_port *p)
+{
+#ifdef DBG_STP_STATE_TX
+	dbg_stp(p, __func__, false);
+#endif
+	newInfo = FALSE;
+	txConfig();
+	txCount++;
+	tcAck = FALSE;
+}  /* stp_tx_config_init */
+
+static void stp_tx_config_next(struct ksz_stp_port *p,
+	struct ksz_stp_state *state)
+{
+	stp_change_state(state,
+		TRUE, STP_PortTransmit_IDLE);
+}  /* stp_tx_config_next */
+
+static void stp_tx_tcn_init(struct ksz_stp_port *p)
+{
+#ifdef DBG_STP_STATE_TX
+	dbg_stp(p, __func__, false);
+#endif
+	newInfo = FALSE;
+	txTcn();
+	txCount++;
+}  /* stp_tx_tcn_init */
+
+static void stp_tx_tcn_next(struct ksz_stp_port *p,
+	struct ksz_stp_state *state)
+{
+	stp_change_state(state,
+		TRUE, STP_PortTransmit_IDLE);
+}  /* stp_tx_tcn_next */
+
+static void stp_tx_rstp_init(struct ksz_stp_port *p)
+{
+#ifdef DBG_STP_STATE_TX
+	dbg_stp(p, __func__, false);
+#endif
+	txRstp();
+	newInfo = newInfoMsti = FALSE;
+	txCount++;
+	tcAck = FALSE;
+}  /* stp_tx_rstp_init */
+
+static void stp_tx_rstp_next(struct ksz_stp_port *p,
+	struct ksz_stp_state *state)
+{
+	stp_change_state(state,
+		TRUE, STP_PortTransmit_IDLE);
+}  /* stp_tx_rstp_next */
+
+static void stp_tx_idle_init(struct ksz_stp_port *p)
+{
+#ifdef DBG_STP_STATE_TX_0
+	dbg_stp(p, __func__, false);
+#endif
+	helloWhen = HelloTime;
+}  /* stp_tx_idle_init */
+
+static void stp_tx_idle_next(struct ksz_stp_port *p,
+	struct ksz_stp_state *state)
+{
+	stp_change_state(state,
+		((0 == helloWhen) && allTransmitReady()),
+		STP_PortTransmit_PERIODIC);
+	stp_change_state(state,
+		((sendRSTP && (newInfo || (newInfoMsti && !mstiMasterPort())))
+		&& allTransmitReady() && canSend),
+		STP_PortTransmit_RSTP);
+	stp_change_state(state,
+		((!sendRSTP && newInfo && cistRootPort()) &&
+		allTransmitReady() && canSend),
+		STP_PortTransmit_TCN);
+	stp_change_state(state,
+		((!sendRSTP && newInfo && cistDesignatedPort()) &&
+		allTransmitReady() && canSend),
+		STP_PortTransmit_CONFIG);
+}  /* stp_tx_idle_next */
+
+static int PortTransmit(struct ksz_stp_port *p)
+{
+	struct ksz_stp_state state_info;
+	u8 *state = &p->states[p->state_index][0];
+	int ret = 0;
+
+	state_info.index = STP_PortTransmit;
+	state_info.change = 1;
+	state_info.new_state = 0;
+	if (STP_BEGIN == *state) {
+		state_info.new_state = STP_PortTransmit_INIT;
+	}
+	do {
+
+		/* There is a new state. */
+		if (state_info.new_state) {
+			*state = state_info.new_state;
+			ret = state_info.change;
+			ret = 1;
+		}
+		switch (*state) {
+		case STP_PortTransmit_INIT:
+			if (stp_proc_state(p, &state_info,
+			    stp_tx_init_init, stp_tx_init_next))
+				goto done;
+			break;
+		case STP_PortTransmit_PERIODIC:
+			if (stp_proc_state(p, &state_info,
+			    stp_tx_periodic_init, stp_tx_periodic_next))
+				goto done;
+			break;
+		case STP_PortTransmit_CONFIG:
+			if (stp_proc_state(p, &state_info,
+			    stp_tx_config_init, stp_tx_config_next))
+				goto done;
+			break;
+		case STP_PortTransmit_TCN:
+			if (stp_proc_state(p, &state_info,
+			    stp_tx_tcn_init, stp_tx_tcn_next))
+				goto done;
+			break;
+		case STP_PortTransmit_RSTP:
+			if (stp_proc_state(p, &state_info,
+			    stp_tx_rstp_init, stp_tx_rstp_next))
+				goto done;
+			break;
+		case STP_PortTransmit_IDLE:
+			if (stp_proc_state(p, &state_info,
+			    stp_tx_idle_init, stp_tx_idle_next))
+				goto done;
+			break;
+		}
+	} while (state_info.new_state);
+
+done:
+	return ret;
+}  /* PortTransmit */
+
+static void stp_info_disable_init(struct ksz_stp_port *p)
+{
+#ifdef DBG_STP_STATE_INFO
+	dbg_stp(p, __func__, false);
+#endif
+	rcvdMsg = FALSE;
+	proposing = proposed = agree = agreed = FALSE;
+	rcvdInfoWhile = 0;
+
+	/* Change role in PortRoleSelection. */
+	infoIs = INFO_TYPE_DISABLED;
+	reselect = TRUE;
+	selected = FALSE;
+}  /* stp_info_disable_init */
+
+static void stp_info_disable_next(struct ksz_stp_port *p,
+	struct ksz_stp_state *state)
+{
+	stp_change_state(state,
+		(portEnabled),
+		STP_PortInfo_AGED);
+	stp_change_state(state,
+		(rcvdMsg),
+		STP_PortInfo_DISABLED);
+}  /* stp_info_disable_next */
+
+static void stp_info_age_init(struct ksz_stp_port *p)
+{
+#ifdef DBG_STP_STATE
+	dbg_stp(p, __func__, false);
+#endif
+	/* Change role in PortRoleSelection. */
+	infoIs = INFO_TYPE_AGED;
+	reselect = TRUE;
+	selected = FALSE;
+}  /* stp_info_age_init */
+
+static void stp_info_age_next(struct ksz_stp_port *p,
+	struct ksz_stp_state *state)
+{
+	stp_change_state(state,
+		(selected && updtInfo),
+		STP_PortInfo_UPDATE);
+}  /* stp_info_age_next */
+
+static void stp_info_update_init(struct ksz_stp_port *p)
+{
+#ifdef DBG_STP_STATE
+	dbg_stp(p, __func__, false);
+#endif
+	proposing = proposed = FALSE;
+	agreed = agreed && betterorsameInfo(INFO_TYPE_MINE);
+
+#if 1
+	/*
+	 * When switching back to Root Bridge because of timeout the port will
+	 * have inferior priority, resetting both agreed and synced.  The port
+	 * is still in forwarding state though.  When the port becomes
+	 * forwarding agreed is always set.  Now that agreed is reset synced
+	 * will not be set, making allSynced to always fail, which prevents
+	 * the Root Port from sending an Agreement, thus failing RSTP.op.5.3.
+	 */
+	if (forward)
+		agreed = sendRSTP;
+#endif
+	synced = synced && agreed;
+
+#if 0
+	/* agree is never turned off if priority is changed. */
+	agree = FALSE;
+#endif
+#ifdef DBG_STP_STATE_INFO
+dbg_msg("  %s %d %d\n", __func__, agreed, synced);
+#endif
+	COPY(portPriority, designatedPriority);
+	COPY(portTimes, designatedTimes);
+	updtInfo = FALSE;
+	infoIs = INFO_TYPE_MINE;
+	newInfoXst(TRUE);
+}  /* stp_info_update_init */
+
+static void stp_info_update_next(struct ksz_stp_port *p,
+	struct ksz_stp_state *state)
+{
+	stp_change_state(state,
+		TRUE, STP_PortInfo_CURRENT);
+}  /* stp_info_update_next */
+
+static void stp_info_superior_init(struct ksz_stp_port *p)
+{
+#ifdef DBG_STP_STATE
+	dbg_stp(p, __func__, false);
+#endif
+	infoInternal = rcvdInternal;
+	agreed = proposing = FALSE;
+	recordProposal();
+	setTcFlags();
+	agree = agree && betterorsameInfo(INFO_TYPE_RECEIVED);
+	recordAgreement();
+	synced = synced && agreed;
+	recordPriority();
+	recordTimes();
+
+	/* Keep from aged out. */
+	updtRcvdInfoWhile();
+
+	/* Change role in PortRoleSelection. */
+	infoIs = INFO_TYPE_RECEIVED;
+	reselect = TRUE;
+	selected = FALSE;
+
+	rcvdMsg = FALSE;
+}  /* stp_info_superior_init */
+
+static void stp_info_superior_next(struct ksz_stp_port *p,
+	struct ksz_stp_state *state)
+{
+	stp_change_state(state,
+		TRUE, STP_PortInfo_CURRENT);
+}  /* stp_info_superior_next */
+
+static void stp_info_repeat_init(struct ksz_stp_port *p)
+{
+#ifdef DBG_STP_STATE
+	dbg_stp(p, __func__, true);
+#endif
+	infoInternal = rcvdInternal;
+	recordProposal();
+	setTcFlags();
+	recordAgreement();
+
+	/* Keep from aged out. */
+	updtRcvdInfoWhile();
+	rcvdMsg = FALSE;
+}  /* stp_info_repeat_init */
+
+static void stp_info_repeat_next(struct ksz_stp_port *p,
+	struct ksz_stp_state *state)
+{
+	stp_change_state(state,
+		TRUE, STP_PortInfo_CURRENT);
+}  /* stp_info_repeat_next */
+
+static void stp_info_inferior_init(struct ksz_stp_port *p)
+{
+#ifdef DBG_STP_STATE
+	dbg_stp(p, __func__, false);
+#endif
+	recordDispute();
+
+#if 1
+	/*
+	 * Priority is not recorded, so in updtRolesTree this port is still
+	 * selected as Root Port!
+	 * This can happen in test tool as the bridge id and port id is changed
+	 * from the port!
+	 */
+	if (p->br->hack_5_2 && INFO_TYPE_RECEIVED == infoIs &&
+	    memcmp(msgPriority.bridge_id.addr, portPriority.bridge_id.addr,
+	    ETH_ALEN)) {
+dbg_msg(" bridge id changed!\n");
+		recordPriority();
+		recordTimes();
+	}
+#endif
+
+	/* Will age out if keep receiving inferior messages. */
+	rcvdMsg = FALSE;
+}  /* stp_info_inferior_init */
+
+static void stp_info_inferior_next(struct ksz_stp_port *p,
+	struct ksz_stp_state *state)
+{
+	stp_change_state(state,
+		TRUE, STP_PortInfo_CURRENT);
+}  /* stp_info_inferior_next */
+
+static void stp_info_not_init(struct ksz_stp_port *p)
+{
+#ifdef DBG_STP_STATE_INFO
+	dbg_stp(p, __func__, false);
+#endif
+	recordAgreement();
+#if 1
+	/*
+	 * This port is not yet designated but receives an Agreement, most
+	 * likely when running the test tool.  Prepare the portPriority so that
+	 * betterorsameInfo can return true.
+	 */
+	if (agreed && role != ROLE_DESIGNATED && role != ROLE_BACKUP) {
+dbgPriority(&rootPriority, &rootPortId);
+		COPY(portPriority, rootPriority);
+		COPY(portPriority.port_id, portId);
+		COPY(portTimes, rootTimes);
+	}
+#endif
+	setTcFlags();
+	rcvdMsg = FALSE;
+
+	if (role != ROLE_BACKUP)
+		stp_chk_flush(p);
+}  /* stp_info_not_init */
+
+static void stp_info_not_next(struct ksz_stp_port *p,
+	struct ksz_stp_state *state)
+{
+	stp_change_state(state,
+		TRUE, STP_PortInfo_CURRENT);
+}  /* stp_info_not_next */
+
+static void stp_info_other_init(struct ksz_stp_port *p)
+{
+#ifdef DBG_STP_STATE
+	dbg_stp(p, __func__, false);
+#endif
+	rcvdMsg = FALSE;
+}  /* stp_info_other_init */
+
+static void stp_info_other_next(struct ksz_stp_port *p,
+	struct ksz_stp_state *state)
+{
+	stp_change_state(state,
+		TRUE, STP_PortInfo_CURRENT);
+}  /* stp_info_other_next */
+
+static void stp_info_cur_init(struct ksz_stp_port *p)
+{
+#ifdef DBG_STP_STATE_INFO
+	dbg_stp(p, __func__, true);
+#endif
+}  /* stp_info_cur_init */
+
+static void stp_info_cur_next(struct ksz_stp_port *p,
+	struct ksz_stp_state *state)
+{
+	stp_change_state(state,
+		(selected && updtInfo),
+		STP_PortInfo_UPDATE);
+	stp_change_state(state,
+		((infoIs == INFO_TYPE_RECEIVED) && (0 == rcvdInfoWhile) &&
+		!updtInfo && !rcvdXstMsg()),
+		STP_PortInfo_AGED);
+	stp_change_state(state,
+		(rcvdXstMsg() && !updtXstInfo()),
+		STP_PortInfo_RECEIVE);
+}  /* stp_info_cur_next */
+
+static void stp_info_receive_init(struct ksz_stp_port *p)
+{
+	rcvdInfo = rcvInfo();
+	recordMastered();
+#ifdef DBG_STP_RX
+if (p->dbg_rx)
+dbg_msg("  %s:%u:%d=%d\n", __func__, p->port_index, p->MSTI, rcvdInfo);
+#endif
+}  /* stp_info_receive_init */
+
+static void stp_info_receive_next(struct ksz_stp_port *p,
+	struct ksz_stp_state *state)
+{
+	stp_change_state(state,
+		(rcvdInfo == INFO_SUPERIOR_DESIGNATED),
+		STP_PortInfo_SUPERIOR_DESIGNATED);
+	stp_change_state(state,
+		(rcvdInfo == INFO_REPEATED_DESIGNATED),
+		STP_PortInfo_REPEATED_DESIGNATED);
+	stp_change_state(state,
+		(rcvdInfo == INFO_INFERIOR_DESIGNATED),
+		STP_PortInfo_INFERIOR_DESIGNATED);
+	stp_change_state(state,
+		(rcvdInfo == INFO_INFERIOR_ROOT_ALT),
+		STP_PortInfo_NOT_DESIGNATED);
+	stp_change_state(state,
+		(rcvdInfo == INFO_OTHER),
+		STP_PortInfo_OTHER);
+}  /* stp_info_receive_next */
+
+static void stp_info_next(struct ksz_stp_port *p,
+	struct ksz_stp_state *state)
+{
+	/* Notify port is disabled. */
+	stp_change_state(state,
+		(!portEnabled && (infoIs != INFO_TYPE_DISABLED)),
+		STP_PortInfo_DISABLED);
+}  /* stp_info_next */
+
+static int PortInformation(struct ksz_stp_port *p)
+{
+	struct ksz_stp_state state_info;
+	u8 *state = &p->states[p->state_index][p->MSTI];
+	int ret = 0;
+
+	state_info.index = STP_PortInfo;
+	state_info.change = 1;
+	state_info.new_state = 0;
+	if (STP_BEGIN == *state) {
+		state_info.new_state = STP_PortInfo_DISABLED;
+	} else
+		stp_info_next(p, &state_info);
+	do {
+
+		/* There is a new state. */
+		if (state_info.new_state) {
+			*state = state_info.new_state;
+			ret = state_info.change;
+			ret = 1;
+		}
+		switch (*state) {
+		case STP_PortInfo_DISABLED:
+			if (stp_proc_state(p, &state_info,
+			    stp_info_disable_init, stp_info_disable_next))
+				goto done;
+			break;
+		case STP_PortInfo_AGED:
+			if (stp_proc_state(p, &state_info,
+			    stp_info_age_init, stp_info_age_next))
+				goto done;
+			break;
+		case STP_PortInfo_UPDATE:
+			if (stp_proc_state(p, &state_info,
+			    stp_info_update_init, stp_info_update_next))
+				goto done;
+			break;
+		case STP_PortInfo_SUPERIOR_DESIGNATED:
+			if (stp_proc_state(p, &state_info,
+			    stp_info_superior_init, stp_info_superior_next))
+				goto done;
+			break;
+		case STP_PortInfo_REPEATED_DESIGNATED:
+			if (stp_proc_state(p, &state_info,
+			    stp_info_repeat_init, stp_info_repeat_next))
+				goto done;
+			break;
+		case STP_PortInfo_INFERIOR_DESIGNATED:
+			if (stp_proc_state(p, &state_info,
+			    stp_info_inferior_init, stp_info_inferior_next))
+				goto done;
+			break;
+		case STP_PortInfo_NOT_DESIGNATED:
+			if (stp_proc_state(p, &state_info,
+			    stp_info_not_init, stp_info_not_next))
+				goto done;
+			break;
+		case STP_PortInfo_OTHER:
+			if (stp_proc_state(p, &state_info,
+			    stp_info_other_init, stp_info_other_next))
+				goto done;
+			break;
+		case STP_PortInfo_CURRENT:
+			if (stp_proc_state(p, &state_info,
+			    stp_info_cur_init, stp_info_cur_next))
+				goto done;
+			break;
+		case STP_PortInfo_RECEIVE:
+			if (stp_proc_state(p, &state_info,
+			    stp_info_receive_init, stp_info_receive_next))
+				goto done;
+			break;
+		}
+	} while (state_info.new_state);
+
+done:
+	return ret;
+}  /* PortInformation */
+
+static void stp_role_sel_br_init(struct ksz_stp_port *p)
+{
+	struct ksz_stp_bridge *br = p->br;
+
+#ifdef DBG_STP_STATE_ROLE_SEL
+dbg_msg("  %s\n", __func__);
+#endif
+	updtRoleDisabledTree();
+}  /* stp_role_sel_br_init */
+
+static void stp_role_sel_br_next(struct ksz_stp_port *p,
+	struct ksz_stp_state *state)
+{
+	stp_change_state(state,
+		TRUE, STP_PortRoleSel_ROLE_SELECTION);
+}  /* stp_role_sel_br_next */
+
+static void stp_role_sel_role_init(struct ksz_stp_port *p)
+{
+	struct ksz_stp_bridge *br = p->br;
+
+#ifdef DBG_STP_STATE_ROLE_SEL
+dbg_msg("  %s\n", __func__);
+#endif
+	clearReselectTree();
+	updtRolesTree();
+	setSelectedTree();
+}  /* stp_role_sel_role_init */
+
+static void stp_role_sel_role_next(struct ksz_stp_port *p,
+	struct ksz_stp_state *state)
+{
+	FOREACH_P_IN_T(
+		if (reselect) {
+			state->new_state = STP_PortRoleSel_ROLE_SELECTION;
+			break;
+		}
+	)
+	if (state->new_state && !p->br->MSTI) {
+		FOREACH_P_IN_T(
+			FOREACH_M_IN_P(
+				reselect = TRUE;
+			)
+		)
+	}
+}  /* stp_role_sel_role_next */
+
+static int PortRoleSelection(struct ksz_stp_bridge *br)
+{
+	struct ksz_stp_state state_info;
+	struct ksz_stp_port *p = &br->ports[0];
+	u8 *state = &br->states[br->state_index][br->MSTI];
+	int ret = 0;
+
+	state_info.index = STP_PortRoleSel;
+	state_info.change = 1;
+	state_info.new_state = 0;
+	if (STP_BEGIN == *state) {
+		state_info.new_state = STP_PortRoleSel_INIT_BRIDGE;
+	}
+	do {
+
+		/* There is a new state. */
+		if (state_info.new_state) {
+			*state = state_info.new_state;
+			ret = state_info.change;
+			ret = 1;
+		}
+		switch (*state) {
+		case STP_PortRoleSel_INIT_BRIDGE:
+			if (stp_proc_state(p, &state_info,
+			    stp_role_sel_br_init, stp_role_sel_br_next))
+				goto done;
+			break;
+		case STP_PortRoleSel_ROLE_SELECTION:
+			if (stp_proc_state(p, &state_info,
+			    stp_role_sel_role_init, stp_role_sel_role_next))
+				goto done;
+			break;
+		}
+	} while (state_info.new_state);
+
+done:
+	return ret;
+}  /* PortRoleSelection */
+
+static void stp_role_tr_init_init(struct ksz_stp_port *p)
+{
+#ifdef DBG_STP_STATE_ROLE_TR
+	dbg_stp(p, __func__, false);
+#endif
+	role = ROLE_DISABLED;
+	learn = forward = FALSE;
+	synced = FALSE;
+	sync = reRoot = TRUE;
+	rrWhile = FwdDelay;
+	fdWhile = MaxAge;
+	rbWhile = 0;
+}  /* stp_role_tr_init_init */
+
+static void stp_role_tr_init_next(struct ksz_stp_port *p,
+	struct ksz_stp_state *state)
+{
+	int cond = canChange;
+
+#if 1
+	cond = TRUE;
+#endif
+	stp_change_state(state,
+		cond, STP_PortRoleTrans_DISABLE_PORT);
+}  /* stp_role_tr_init_next */
+
+static void stp_role_tr_dis_init(struct ksz_stp_port *p)
+{
+#ifdef DBG_STP_STATE
+	dbg_stp(p, __func__, false);
+#endif
+	role = ROLE_DISABLED;
+	learn = forward = FALSE;
+
+#if 1
+	/* Port is disabled.  Do not send. */
+#ifdef DBG_STP_STATE
+if (newInfo)
+dbg_msg("  %s clear newInfo\n", __func__);
+if (newInfoMsti)
+dbg_msg("  %s clear newInfoMsti\n", __func__);
+#endif
+#if 1
+	newInfo = newInfoMsti = FALSE;
+#endif
+#endif
+}  /* stp_role_tr_dis_init */
+
+static void stp_role_tr_dis_next(struct ksz_stp_port *p,
+	struct ksz_stp_state *state)
+{
+	stp_change_state(state,
+		((!learning && !forwarding) && canChange),
+		STP_PortRoleTrans_DISABLED_PORT);
+}  /* stp_role_tr_dis_next */
+
+static void stp_role_tr_disd_init(struct ksz_stp_port *p)
+{
+#ifdef DBG_STP_STATE_ROLE_TR
+	dbg_stp(p, __func__, false);
+#endif
+	fdWhile = MaxAge;
+#ifdef DBG_STP_STATE
+if (!synced)
+dbg_msg(" %s:%u\n", __func__, p->port_index);
+#endif
+	synced = TRUE;
+	rrWhile = 0;
+	sync = reRoot = FALSE;
+}  /* stp_role_tr_disd_init */
+
+static void stp_role_tr_disd_next(struct ksz_stp_port *p,
+	struct ksz_stp_state *state)
+{
+	/* Reset if !updtInfo. */
+	stp_change_state(state,
+		((NEQ(fdWhile, MaxAge) || sync || reRoot || !synced) &&
+		canChange),
+		STP_PortRoleTrans_DISABLED_PORT);
+}  /* stp_role_tr_disd_next */
+
+static void stp_role_tr_m_proposed_init(struct ksz_stp_port *p)
+{
+#ifdef DBG_STP_STATE
+	dbg_stp(p, __func__, false);
+#endif
+	setSyncTree();
+	proposed = FALSE;
+}  /* stp_role_tr_m_proposed_init */
+
+static void stp_role_tr_m_proposed_next(struct ksz_stp_port *p,
+	struct ksz_stp_state *state)
+{
+	stp_change_state(state,
+		TRUE, STP_PortRoleTrans_MASTER_PORT);
+}  /* stp_role_tr_m_proposed_next */
+
+static void stp_role_tr_m_agreed_init(struct ksz_stp_port *p)
+{
+#ifdef DBG_STP_STATE
+	dbg_stp(p, __func__, false);
+#endif
+	proposed = sync = FALSE;
+	agree = TRUE;
+}  /* stp_role_tr_m_agreed_init */
+
+static void stp_role_tr_m_agreed_next(struct ksz_stp_port *p,
+	struct ksz_stp_state *state)
+{
+	stp_change_state(state,
+		TRUE, STP_PortRoleTrans_MASTER_PORT);
+}  /* stp_role_tr_m_agreed_next */
+
+static void stp_role_tr_m_s_init(struct ksz_stp_port *p)
+{
+#ifdef DBG_STP_STATE
+	dbg_stp(p, __func__, false);
+#endif
+	rrWhile = 0;
+	synced = TRUE;
+	sync = FALSE;
+}  /* stp_role_tr_m_s_init */
+
+static void stp_role_tr_m_s_next(struct ksz_stp_port *p,
+	struct ksz_stp_state *state)
+{
+	stp_change_state(state,
+		TRUE, STP_PortRoleTrans_MASTER_PORT);
+}  /* stp_role_tr_m_s_next */
+
+static void stp_role_tr_m_retired_init(struct ksz_stp_port *p)
+{
+#ifdef DBG_STP_STATE
+	dbg_stp(p, __func__, false);
+#endif
+	reRoot = FALSE;
+}  /* stp_role_tr_m_retired_init */
+
+static void stp_role_tr_m_retired_next(struct ksz_stp_port *p,
+	struct ksz_stp_state *state)
+{
+	stp_change_state(state,
+		TRUE, STP_PortRoleTrans_MASTER_PORT);
+}  /* stp_role_tr_m_retired_next */
+
+static void stp_role_tr_m_discard_init(struct ksz_stp_port *p)
+{
+#ifdef DBG_STP_STATE
+	dbg_stp(p, __func__, false);
+#endif
+	learn = forward = disputed = FALSE;
+	fdWhile = forwardDelay();
+}  /* stp_role_tr_m_discard_init */
+
+static void stp_role_tr_m_discard_next(struct ksz_stp_port *p,
+	struct ksz_stp_state *state)
+{
+	stp_change_state(state,
+		TRUE, STP_PortRoleTrans_MASTER_PORT);
+}  /* stp_role_tr_m_discard_next */
+
+static void stp_role_tr_m_l_init(struct ksz_stp_port *p)
+{
+#ifdef DBG_STP_STATE
+	dbg_stp(p, __func__, false);
+#endif
+	fdWhile = forwardDelay();
+	learn = TRUE;
+}  /* stp_role_tr_m_l_init */
+
+static void stp_role_tr_m_l_next(struct ksz_stp_port *p,
+	struct ksz_stp_state *state)
+{
+	stp_change_state(state,
+		TRUE, STP_PortRoleTrans_MASTER_PORT);
+}  /* stp_role_tr_m_l_next */
+
+static void stp_role_tr_m_f_init(struct ksz_stp_port *p)
+{
+#ifdef DBG_STP_STATE
+	dbg_stp(p, __func__, false);
+#endif
+	fdWhile = 0;
+	forward = TRUE;
+	agreed = sendRSTP;
+}  /* stp_role_tr_m_f_init */
+
+static void stp_role_tr_m_f_next(struct ksz_stp_port *p,
+	struct ksz_stp_state *state)
+{
+	stp_change_state(state,
+		TRUE, STP_PortRoleTrans_MASTER_PORT);
+}  /* stp_role_tr_m_f_next */
+
+static void stp_role_tr_m_p_init(struct ksz_stp_port *p)
+{
+#ifdef DBG_STP_STATE
+	if (role != ROLE_ROOT)
+		dbg_stp(p, __func__, false);
+#endif
+	role = ROLE_MASTER;
+}  /* stp_role_tr_m_p_init */
+
+static void stp_role_tr_m_p_next(struct ksz_stp_port *p,
+	struct ksz_stp_state *state)
+{
+	stp_change_state(state,
+		((proposed && !agree) && canChange),
+		STP_PortRoleTrans_MASTER_PROPOSED);
+	stp_change_state(state,
+		(((allSynced() && !agree) || (proposed && agree)) && canChange),
+		STP_PortRoleTrans_MASTER_AGREED);
+	stp_change_state(state,
+		(((!learning && !forwarding && !synced) ||
+		(agreed && !synced) || (operEdge && !synced) ||
+		(sync && synced)) && canChange),
+		STP_PortRoleTrans_MASTER_SYNCED);
+	stp_change_state(state,
+		((reRoot && (0 == rrWhile)) && canChange),
+		STP_PortRoleTrans_MASTER_RETIRED);
+	stp_change_state(state,
+		((((sync && !synced) || (reRoot && (rrWhile != 0)) || disputed)
+		&& !operEdge && (learn || forward)) && canChange),
+		STP_PortRoleTrans_MASTER_DISCARD);
+	stp_change_state(state,
+		((((0 == fdWhile) || allSynced()) && !learn) && canChange),
+		STP_PortRoleTrans_MASTER_LEARN);
+	stp_change_state(state,
+		((((0 == fdWhile) || allSynced()) && (learn && !forward)) &&
+		canChange),
+		STP_PortRoleTrans_MASTER_FORWARD);
+}  /* stp_role_tr_m_p_next */
+
+static void stp_role_tr_reroot_init(struct ksz_stp_port *p)
+{
+#ifdef DBG_STP_STATE
+	dbg_stp(p, __func__, false);
+#endif
+	setReRootTree();
+}  /* stp_role_tr_reroot_init */
+
+static void stp_role_tr_reroot_next(struct ksz_stp_port *p,
+	struct ksz_stp_state *state)
+{
+	stp_change_state(state,
+		TRUE, STP_PortRoleTrans_ROOT_PORT);
+}  /* stp_role_tr_reroot_next */
+
+static void stp_role_tr_rerooted_init(struct ksz_stp_port *p)
+{
+#ifdef DBG_STP_STATE
+	dbg_stp(p, __func__, false);
+#endif
+	reRoot = FALSE;
+}  /* stp_role_tr_rerooted_init */
+
+static void stp_role_tr_rerooted_next(struct ksz_stp_port *p,
+	struct ksz_stp_state *state)
+{
+	stp_change_state(state,
+		TRUE, STP_PortRoleTrans_ROOT_PORT);
+}  /* stp_role_tr_rerooted_next */
+
+static void stp_role_tr_root_proposed_init(struct ksz_stp_port *p)
+{
+#ifdef DBG_STP_STATE
+	dbg_stp(p, __func__, false);
+#endif
+	setSyncTree();
+	proposed = FALSE;
+}  /* stp_role_tr_root_proposed_init */
+
+static void stp_role_tr_root_proposed_next(struct ksz_stp_port *p,
+	struct ksz_stp_state *state)
+{
+	stp_change_state(state,
+		TRUE, STP_PortRoleTrans_ROOT_PORT);
+}  /* stp_role_tr_root_proposed_next */
+
+static void stp_role_tr_root_agreed_init(struct ksz_stp_port *p)
+{
+#ifdef DBG_STP_STATE
+	dbg_stp(p, __func__, false);
+#endif
+	proposed = sync = FALSE;
+	agree = TRUE;
+	newInfoXst(TRUE);
+}  /* stp_role_tr_root_agreed_init */
+
+static void stp_role_tr_root_agreed_next(struct ksz_stp_port *p,
+	struct ksz_stp_state *state)
+{
+	stp_change_state(state,
+		TRUE, STP_PortRoleTrans_ROOT_PORT);
+}  /* stp_role_tr_root_agreed_next */
+
+static void stp_role_tr_root_s_init(struct ksz_stp_port *p)
+{
+#ifdef DBG_STP_STATE
+	dbg_stp(p, __func__, false);
+#endif
+	synced = TRUE;
+	sync = FALSE;
+}  /* stp_role_tr_root_s_init */
+
+static void stp_role_tr_root_s_next(struct ksz_stp_port *p,
+	struct ksz_stp_state *state)
+{
+	stp_change_state(state,
+		TRUE, STP_PortRoleTrans_ROOT_PORT);
+}  /* stp_role_tr_root_s_next */
+
+static void stp_role_tr_root_discard_init(struct ksz_stp_port *p)
+{
+#ifdef DBG_STP_STATE
+	dbg_stp(p, __func__, false);
+#endif
+	if (disputed)
+		rbWhile = 3 * HelloTime;
+	learn = forward = disputed = FALSE;
+	fdWhile = FwdDelay;
+}  /* stp_role_tr_root_discard_init */
+
+static void stp_role_tr_root_discard_next(struct ksz_stp_port *p,
+	struct ksz_stp_state *state)
+{
+	stp_change_state(state,
+		TRUE, STP_PortRoleTrans_ROOT_PORT);
+}  /* stp_role_tr_root_discard_next */
+
+static void stp_role_tr_root_l_init(struct ksz_stp_port *p)
+{
+#ifdef DBG_STP_STATE
+	dbg_stp(p, __func__, false);
+#endif
+	fdWhile = forwardDelay();
+	learn = TRUE;
+}  /* stp_role_tr_root_l_init */
+
+static void stp_role_tr_root_l_next(struct ksz_stp_port *p,
+	struct ksz_stp_state *state)
+{
+	stp_change_state(state,
+		TRUE, STP_PortRoleTrans_ROOT_PORT);
+}  /* stp_role_tr_root_l_next */
+
+static void stp_role_tr_root_f_init(struct ksz_stp_port *p)
+{
+#ifdef DBG_STP_STATE
+	dbg_stp(p, __func__, false);
+#endif
+	fdWhile = 0;
+	forward = TRUE;
+}  /* stp_role_tr_root_f_init */
+
+static void stp_role_tr_root_f_next(struct ksz_stp_port *p,
+	struct ksz_stp_state *state)
+{
+	stp_change_state(state,
+		TRUE, STP_PortRoleTrans_ROOT_PORT);
+}  /* stp_role_tr_root_f_next */
+
+static void stp_role_tr_root_p_init(struct ksz_stp_port *p)
+{
+#ifdef DBG_STP_STATE
+	if (role != ROLE_ROOT)
+		dbg_stp(p, __func__, false);
+#endif
+	do {
+		struct ksz_stp_info *stp = p->br->parent;
+		struct ksz_sw *sw = stp->sw_dev;
+
+		if (DesignatedPort)
+			sw->ops->from_designated(sw, p->port_index, false);
+		else if (!RootPort)
+			sw->ops->from_backup(sw, p->port_index);
+	} while (0);
+	role = ROLE_ROOT;
+	rrWhile = FwdDelay;
+}  /* stp_role_tr_root_p_init */
+
+static void stp_role_tr_root_p_next(struct ksz_stp_port *p,
+	struct ksz_stp_state *state)
+{
+	stp_change_state(state,
+		((proposed && !agree) && canChange),
+		STP_PortRoleTrans_ROOT_PROPOSED);
+	stp_change_state(state,
+		(((allSynced() && !agree) || (proposed && agree)) && canChange),
+		STP_PortRoleTrans_ROOT_AGREED);
+	stp_change_state(state,
+		(((agreed && !synced) || (sync && synced)) && canChange),
+		STP_PortRoleTrans_ROOT_SYNCED);
+	stp_change_state(state,
+		((!forward && (0 == rbWhile) && !reRoot) && canChange),
+		STP_PortRoleTrans_REROOT);
+	stp_change_state(state,
+		((reRoot && forward) && canChange),
+		STP_PortRoleTrans_REROOTED);
+	stp_change_state(state,
+		((((0 == fdWhile) || ((reRooted() && (0 == rbWhile)) &&
+		rstpVersion)) && !learn) && canChange),
+		STP_PortRoleTrans_ROOT_LEARN);
+	stp_change_state(state,
+		((((0 == fdWhile) || ((reRooted() && (0 == rbWhile)) &&
+		rstpVersion)) && learn && !forward) && canChange),
+		STP_PortRoleTrans_ROOT_FORWARD);
+	stp_change_state(state,
+		((disputed) && canChange),
+		STP_PortRoleTrans_ROOT_DISCARD);
+	stp_change_state(state,
+		(NEQ(rrWhile, FwdDelay) && canChange),
+		STP_PortRoleTrans_ROOT_PORT);
+}  /* stp_role_tr_root_p_next */
+
+static void stp_role_tr_desg_r_init(struct ksz_stp_port *p)
+{
+	int delay = EdgeDelay();
+
+	if (AutoEdge && p->br->hack_4_1)
+		delay = 0;
+
+#ifdef DBG_STP_STATE
+	dbg_stp(p, __func__, false);
+#endif
+	proposing = TRUE;
+	if (cist())
+		edgeDelayWhile = delay;
+	newInfoXst(TRUE);
+}  /* stp_role_tr_desg_r_init */
+
+static void stp_role_tr_desg_r_next(struct ksz_stp_port *p,
+	struct ksz_stp_state *state)
+{
+	stp_change_state(state,
+		TRUE, STP_PortRoleTrans_DESIGNATED_PORT);
+}  /* stp_role_tr_desg_r_next */
+
+static void stp_role_tr_desg_a_init(struct ksz_stp_port *p)
+{
+#ifdef DBG_STP_STATE
+	dbg_stp(p, __func__, false);
+#endif
+	proposed = sync = FALSE;
+	agree = TRUE;
+	newInfoXst(TRUE);
+}  /* stp_role_tr_desg_a_init */
+
+static void stp_role_tr_desg_a_next(struct ksz_stp_port *p,
+	struct ksz_stp_state *state)
+{
+	stp_change_state(state,
+		TRUE, STP_PortRoleTrans_DESIGNATED_PORT);
+}  /* stp_role_tr_desg_a_next */
+
+static void stp_role_tr_desg_s_init(struct ksz_stp_port *p)
+{
+#ifdef DBG_STP_STATE
+	dbg_stp(p, __func__, false);
+#endif
+	rrWhile = 0;
+	synced = TRUE;
+	sync = FALSE;
+}  /* stp_role_tr_desg_s_init */
+
+static void stp_role_tr_desg_s_next(struct ksz_stp_port *p,
+	struct ksz_stp_state *state)
+{
+	stp_change_state(state,
+		TRUE, STP_PortRoleTrans_DESIGNATED_PORT);
+}  /* stp_role_tr_desg_s_next */
+
+static void stp_role_tr_desg_retired_init(struct ksz_stp_port *p)
+{
+#ifdef DBG_STP_STATE
+	dbg_stp(p, __func__, false);
+#endif
+	reRoot = FALSE;
+}  /* stp_role_tr_desg_retired_init */
+
+static void stp_role_tr_desg_retired_next(struct ksz_stp_port *p,
+	struct ksz_stp_state *state)
+{
+	stp_change_state(state,
+		TRUE, STP_PortRoleTrans_DESIGNATED_PORT);
+}  /* stp_role_tr_desg_retired_next */
+
+static void stp_role_tr_desg_discard_init(struct ksz_stp_port *p)
+{
+#ifdef DBG_STP_STATE
+	dbg_stp(p, __func__, false);
+#endif
+	learn = forward = disputed = FALSE;
+	fdWhile = forwardDelay();
+}  /* stp_role_tr_desg_discard_init */
+
+static void stp_role_tr_desg_discard_next(struct ksz_stp_port *p,
+	struct ksz_stp_state *state)
+{
+	stp_change_state(state,
+		TRUE, STP_PortRoleTrans_DESIGNATED_PORT);
+}  /* stp_role_tr_desg_discard_next */
+
+static void stp_role_tr_desg_l_init(struct ksz_stp_port *p)
+{
+#ifdef DBG_STP_STATE
+	dbg_stp(p, __func__, false);
+#endif
+	learn = TRUE;
+	fdWhile = forwardDelay();
+do {
+	struct ksz_stp_dbg_times *x = &p->dbg_times[p->MSTI];
+
+if (x->block_jiffies)
+dbg_msg(" b: %d:%d=%ld %d %s\n", p->port_index, p->MSTI,
+jiffies - x->block_jiffies, fdWhile,
+__func__);
+} while (0);
+}  /* stp_role_tr_desg_l_init */
+
+static void stp_role_tr_desg_l_next(struct ksz_stp_port *p,
+	struct ksz_stp_state *state)
+{
+	stp_change_state(state,
+		TRUE, STP_PortRoleTrans_DESIGNATED_PORT);
+}  /* stp_role_tr_desg_l_next */
+
+static void stp_role_tr_desg_f_init(struct ksz_stp_port *p)
+{
+#ifdef DBG_STP_STATE
+	dbg_stp(p, __func__, false);
+#endif
+	forward = TRUE;
+	fdWhile = 0;
+	agreed = sendRSTP;
+
+#if 1
+	/*
+	 * proposing will be reset when infoIs is changed and betterorsameInfo
+	 * is called, even though the forwarding state is not changed.  Why
+	 * not just reset here?
+	 */
+	if (proposing)
+		proposing = FALSE;
+#endif
+#ifdef DBG_STP_STATE_
+	do {
+		char buf[40];
+
+		sprintf(buf, " %s", __func__);
+	dbg_stp(p, buf, false);
+	} while (0);
+#endif
+}  /* stp_role_tr_desg_f_init */
+
+static void stp_role_tr_desg_f_next(struct ksz_stp_port *p,
+	struct ksz_stp_state *state)
+{
+	stp_change_state(state,
+		TRUE, STP_PortRoleTrans_DESIGNATED_PORT);
+}  /* stp_role_tr_desg_f_next */
+
+static void stp_role_tr_desg_p_init(struct ksz_stp_port *p)
+{
+#ifdef DBG_STP_STATE
+	if (role != ROLE_DESIGNATED)
+		dbg_stp(p, __func__, false);
+#endif
+do {
+	struct ksz_stp_dbg_times *x = &p->dbg_times[p->MSTI];
+
+if (x->block_jiffies && role != ROLE_DESIGNATED)
+dbg_msg(" b: %d:%d=%ld %d %s\n", p->port_index, p->MSTI,
+jiffies - x->block_jiffies, fdWhile,
+__func__);
+} while (0);
+	if (RootPort || AlternatePort) {
+		struct ksz_stp_info *stp = p->br->parent;
+		struct ksz_sw *sw = stp->sw_dev;
+
+		sw->ops->to_designated(sw, p->port_index);
+	}
+	if (BackupPort) {
+		struct ksz_stp_info *stp = p->br->parent;
+		struct ksz_sw *sw = stp->sw_dev;
+
+		sw->ops->from_backup(sw, p->port_index);
+	}
+	role = ROLE_DESIGNATED;
+	if (cist()) {
+		proposing = proposing ||
+			(!AdminEdge && !AutoEdge && AutoIsolate &&
+			operPointToPointMAC);
+	}
+}  /* stp_role_tr_desg_p_init */
+
+static void stp_role_tr_desg_p_next(struct ksz_stp_port *p,
+	struct ksz_stp_state *state)
+{
+	stp_change_state(state,
+		((!forward && !agreed && !proposing && !operEdge) &&
+		canChange),
+		STP_PortRoleTrans_DESIGNATED_PROPOSE);
+	stp_change_state(state,
+		((allSynced() && (proposed || !agree)) && canChange),
+		STP_PortRoleTrans_DESIGNATED_AGREED);
+	stp_change_state(state,
+		(((!learning && !forwarding && !synced) ||
+		(agreed && !synced) || (operEdge && !synced) ||
+		(sync && synced)) && canChange),
+		STP_PortRoleTrans_DESIGNATED_SYNCED);
+	stp_change_state(state,
+		((reRoot && (0 == rrWhile)) && canChange),
+		STP_PortRoleTrans_DESIGNATED_RETIRED);
+	stp_change_state(state,
+		((((sync && !synced) || (reRoot && (rrWhile != 0)) || disputed
+		|| isolate) && !operEdge && (learn || forward)) && canChange),
+		STP_PortRoleTrans_DESIGNATED_DISCARD);
+	stp_change_state(state,
+		((((0 == fdWhile) || agreed || operEdge) &&
+		((0 == rrWhile) || !reRoot) && !sync && !learn && !isolate) &&
+		canChange),
+		STP_PortRoleTrans_DESIGNATED_LEARN);
+	stp_change_state(state,
+		((((0 == fdWhile) || agreed || operEdge) &&
+		((0 == rrWhile) || !reRoot) && !sync && (learn && !forward) &&
+		!isolate) && canChange),
+		STP_PortRoleTrans_DESIGNATED_FORWARD);
+}  /* stp_role_tr_desg_p_next */
+
+static void stp_role_tr_block_init(struct ksz_stp_port *p)
+{
+#ifdef DBG_STP_STATE
+	dbg_stp(p, __func__, false);
+#endif
+	do {
+		struct ksz_stp_info *stp = p->br->parent;
+		struct ksz_sw *sw = stp->sw_dev;
+
+		if (DesignatedPort && selectedRole == ROLE_ALTERNATE)
+			sw->ops->from_designated(sw, p->port_index, true);
+		else
+			sw->ops->to_backup(sw, p->port_index);
+	} while (0);
+	role = selectedRole;
+	learn = forward = FALSE;
+}  /* stp_role_tr_block_init */
+
+static void stp_role_tr_block_next(struct ksz_stp_port *p,
+	struct ksz_stp_state *state)
+{
+	stp_change_state(state,
+		((!learning && !forwarding) && canChange),
+		STP_PortRoleTrans_ALTERNATE_PORT);
+}  /* stp_role_tr_block_next */
+
+static void stp_role_tr_backup_init(struct ksz_stp_port *p)
+{
+#ifdef DBG_STP_STATE
+	dbg_stp(p, __func__, true);
+#endif
+	rbWhile = 2 * HelloTime;
+}  /* stp_role_tr_backup_init */
+
+static void stp_role_tr_backup_next(struct ksz_stp_port *p,
+	struct ksz_stp_state *state)
+{
+	stp_change_state(state,
+		TRUE, STP_PortRoleTrans_ALTERNATE_PORT);
+}  /* stp_role_tr_backup_next */
+
+static void stp_role_tr_alt_proposed_init(struct ksz_stp_port *p)
+{
+#ifdef DBG_STP_STATE
+	dbg_stp(p, __func__, false);
+#endif
+	setSyncTree();
+	proposed = FALSE;
+}  /* stp_role_tr_alt_proposed_init */
+
+static void stp_role_tr_alt_proposed_next(struct ksz_stp_port *p,
+	struct ksz_stp_state *state)
+{
+	stp_change_state(state,
+		TRUE, STP_PortRoleTrans_ALTERNATE_PORT);
+}  /* stp_role_tr_alt_proposed_next */
+
+static void stp_role_tr_alt_a_init(struct ksz_stp_port *p)
+{
+#ifdef DBG_STP_STATE
+	dbg_stp(p, __func__, false);
+#endif
+	proposed = FALSE;
+	agree = TRUE;
+	newInfoXst(TRUE);
+}  /* stp_role_tr_alt_a_init */
+
+static void stp_role_tr_alt_a_next(struct ksz_stp_port *p,
+	struct ksz_stp_state *state)
+{
+	stp_change_state(state,
+		TRUE, STP_PortRoleTrans_ALTERNATE_PORT);
+}  /* stp_role_tr_alt_a_next */
+
+static void stp_role_tr_alt_p_init(struct ksz_stp_port *p)
+{
+	int delay = forwardDelay();
+
+	if (p->br->hack_4_2)
+		delay = FwdDelay;
+
+#ifdef DBG_STP_STATE_ROLE_TR
+	dbg_stp(p, __func__, true);
+#endif
+	fdWhile = delay;
+	synced = TRUE;
+	rrWhile = 0;
+	sync = reRoot = FALSE;
+}  /* stp_role_tr_alt_p_init */
+
+static void stp_role_tr_alt_p_next(struct ksz_stp_port *p,
+	struct ksz_stp_state *state)
+{
+	int delay = forwardDelay();
+
+	if (p->br->hack_4_2)
+		delay = FwdDelay;
+
+	stp_change_state(state,
+		((proposed && !agree) && canChange),
+		STP_PortRoleTrans_ALTERNATE_PROPOSED);
+	stp_change_state(state,
+		(((allSynced() && !agree) || (proposed && agree)) && canChange),
+		STP_PortRoleTrans_ALTERNATE_AGREED);
+	state->change = 0;
+	stp_change_state(state,
+		((NEQ(rbWhile, 2 * HelloTime) && BackupPort) && canChange),
+		STP_PortRoleTrans_BACKUP_PORT);
+	stp_change_state(state,
+		((NEQ(fdWhile, delay) || sync || reRoot || !synced) &&
+		canChange),
+		STP_PortRoleTrans_ALTERNATE_PORT);
+	state->change = 1;
+}  /* stp_role_tr_alt_p_next */
+
+static void stp_role_tr_next(struct ksz_stp_port *p,
+	struct ksz_stp_state *state)
+{
+	stp_change_state(state,
+		(((selectedRole == ROLE_DISABLED) && (role != selectedRole)) &&
+		canChange),
+		STP_PortRoleTrans_DISABLE_PORT);
+	stp_change_state(state,
+		(((selectedRole == ROLE_MASTER) && (role != selectedRole)) &&
+		canChange),
+		STP_PortRoleTrans_MASTER_PORT);
+	stp_change_state(state,
+		(((selectedRole == ROLE_ROOT) && (role != selectedRole)) &&
+		canChange),
+		STP_PortRoleTrans_ROOT_PORT);
+	stp_change_state(state,
+		(((selectedRole == ROLE_DESIGNATED) && (role != selectedRole))
+		&& canChange),
+		STP_PortRoleTrans_DESIGNATED_PORT);
+	stp_change_state(state,
+		((((selectedRole == ROLE_ALTERNATE) ||
+		(selectedRole == ROLE_BACKUP)) && (role != selectedRole)) &&
+		canChange),
+		STP_PortRoleTrans_BLOCK_PORT);
+}  /* stp_role_tr_next */
+
+static int PortRoleTransitions(struct ksz_stp_port *p)
+{
+	struct ksz_stp_state state_info;
+	u8 *state = &p->states[p->state_index][p->MSTI];
+	int ret = 0;
+
+	state_info.index = STP_PortRoleTrans;
+	state_info.change = 1;
+	state_info.new_state = 0;
+	if (STP_BEGIN == *state) {
+		state_info.new_state = STP_PortRoleTrans_INIT_PORT;
+	} else
+		stp_role_tr_next(p, &state_info);
+	do {
+
+		/* There is a new state. */
+		if (state_info.new_state) {
+			*state = state_info.new_state;
+			ret = state_info.change;
+			ret = 1;
+		}
+		switch (*state) {
+		case STP_PortRoleTrans_INIT_PORT:
+			if (stp_proc_state(p, &state_info,
+			    stp_role_tr_init_init, stp_role_tr_init_next))
+				goto done;
+			break;
+		case STP_PortRoleTrans_DISABLE_PORT:
+			if (stp_proc_state(p, &state_info,
+			    stp_role_tr_dis_init, stp_role_tr_dis_next))
+				goto done;
+			break;
+		case STP_PortRoleTrans_DISABLED_PORT:
+			if (stp_proc_state(p, &state_info,
+			    stp_role_tr_disd_init, stp_role_tr_disd_next))
+				goto done;
+			break;
+		case STP_PortRoleTrans_MASTER_PROPOSED:
+			if (stp_proc_state(p, &state_info,
+			    stp_role_tr_m_proposed_init,
+			    stp_role_tr_m_proposed_next))
+				goto done;
+			break;
+		case STP_PortRoleTrans_MASTER_AGREED:
+			if (stp_proc_state(p, &state_info,
+			    stp_role_tr_m_agreed_init,
+			    stp_role_tr_m_agreed_next))
+				goto done;
+			break;
+		case STP_PortRoleTrans_MASTER_SYNCED:
+			if (stp_proc_state(p, &state_info,
+			    stp_role_tr_m_s_init, stp_role_tr_m_s_next))
+				goto done;
+			break;
+		case STP_PortRoleTrans_MASTER_RETIRED:
+			if (stp_proc_state(p, &state_info,
+			    stp_role_tr_m_retired_init,
+			    stp_role_tr_m_retired_next))
+				goto done;
+			break;
+		case STP_PortRoleTrans_MASTER_DISCARD:
+			if (stp_proc_state(p, &state_info,
+			    stp_role_tr_m_discard_init,
+			    stp_role_tr_m_discard_next))
+				goto done;
+			break;
+		case STP_PortRoleTrans_MASTER_LEARN:
+			if (stp_proc_state(p, &state_info,
+			    stp_role_tr_m_l_init, stp_role_tr_m_l_next))
+				goto done;
+			break;
+		case STP_PortRoleTrans_MASTER_FORWARD:
+			if (stp_proc_state(p, &state_info,
+			    stp_role_tr_m_f_init, stp_role_tr_m_f_next))
+				goto done;
+			break;
+		case STP_PortRoleTrans_MASTER_PORT:
+			if (stp_proc_state(p, &state_info,
+			    stp_role_tr_m_p_init, stp_role_tr_m_p_next))
+				goto done;
+			break;
+		case STP_PortRoleTrans_ROOT_PROPOSED:
+			if (stp_proc_state(p, &state_info,
+			    stp_role_tr_root_proposed_init,
+			    stp_role_tr_root_proposed_next))
+				goto done;
+			break;
+		case STP_PortRoleTrans_ROOT_AGREED:
+			if (stp_proc_state(p, &state_info,
+			    stp_role_tr_root_agreed_init,
+			    stp_role_tr_root_agreed_next))
+				goto done;
+			break;
+		case STP_PortRoleTrans_ROOT_SYNCED:
+			if (stp_proc_state(p, &state_info,
+			    stp_role_tr_root_s_init, stp_role_tr_root_s_next))
+				goto done;
+			break;
+		case STP_PortRoleTrans_REROOT:
+			if (stp_proc_state(p, &state_info,
+			    stp_role_tr_reroot_init, stp_role_tr_reroot_next))
+				goto done;
+			break;
+		case STP_PortRoleTrans_REROOTED:
+			if (stp_proc_state(p, &state_info,
+			    stp_role_tr_rerooted_init,
+			    stp_role_tr_rerooted_next))
+				goto done;
+			break;
+		case STP_PortRoleTrans_ROOT_DISCARD:
+			if (stp_proc_state(p, &state_info,
+			    stp_role_tr_root_discard_init,
+			    stp_role_tr_root_discard_next))
+				goto done;
+			break;
+		case STP_PortRoleTrans_ROOT_LEARN:
+			if (stp_proc_state(p, &state_info,
+			    stp_role_tr_root_l_init, stp_role_tr_root_l_next))
+				goto done;
+			break;
+		case STP_PortRoleTrans_ROOT_FORWARD:
+			if (stp_proc_state(p, &state_info,
+			    stp_role_tr_root_f_init, stp_role_tr_root_f_next))
+				goto done;
+			break;
+		case STP_PortRoleTrans_ROOT_PORT:
+			if (stp_proc_state(p, &state_info,
+			    stp_role_tr_root_p_init, stp_role_tr_root_p_next))
+				goto done;
+			break;
+		case STP_PortRoleTrans_DESIGNATED_PROPOSE:
+			if (stp_proc_state(p, &state_info,
+			    stp_role_tr_desg_r_init, stp_role_tr_desg_r_next))
+				goto done;
+			break;
+		case STP_PortRoleTrans_DESIGNATED_AGREED:
+			if (stp_proc_state(p, &state_info,
+			    stp_role_tr_desg_a_init, stp_role_tr_desg_a_next))
+				goto done;
+			break;
+		case STP_PortRoleTrans_DESIGNATED_SYNCED:
+			if (stp_proc_state(p, &state_info,
+			    stp_role_tr_desg_s_init, stp_role_tr_desg_s_next))
+				goto done;
+			break;
+		case STP_PortRoleTrans_DESIGNATED_RETIRED:
+			if (stp_proc_state(p, &state_info,
+			    stp_role_tr_desg_retired_init,
+			    stp_role_tr_desg_retired_next))
+				goto done;
+			break;
+		case STP_PortRoleTrans_DESIGNATED_DISCARD:
+			if (stp_proc_state(p, &state_info,
+			    stp_role_tr_desg_discard_init,
+			    stp_role_tr_desg_discard_next))
+				goto done;
+			break;
+		case STP_PortRoleTrans_DESIGNATED_LEARN:
+			if (stp_proc_state(p, &state_info,
+			    stp_role_tr_desg_l_init, stp_role_tr_desg_l_next))
+				goto done;
+			break;
+		case STP_PortRoleTrans_DESIGNATED_FORWARD:
+			if (stp_proc_state(p, &state_info,
+			    stp_role_tr_desg_f_init, stp_role_tr_desg_f_next))
+				goto done;
+			break;
+		case STP_PortRoleTrans_DESIGNATED_PORT:
+			if (stp_proc_state(p, &state_info,
+			    stp_role_tr_desg_p_init, stp_role_tr_desg_p_next))
+				goto done;
+			break;
+		case STP_PortRoleTrans_BLOCK_PORT:
+			if (stp_proc_state(p, &state_info,
+			    stp_role_tr_block_init, stp_role_tr_block_next))
+				goto done;
+			break;
+		case STP_PortRoleTrans_ALTERNATE_PROPOSED:
+			if (stp_proc_state(p, &state_info,
+			    stp_role_tr_alt_proposed_init,
+			    stp_role_tr_alt_proposed_next))
+				goto done;
+			break;
+		case STP_PortRoleTrans_ALTERNATE_AGREED:
+			if (stp_proc_state(p, &state_info,
+			    stp_role_tr_alt_a_init, stp_role_tr_alt_a_next))
+				goto done;
+			break;
+		case STP_PortRoleTrans_BACKUP_PORT:
+			if (stp_proc_state(p, &state_info,
+			    stp_role_tr_backup_init, stp_role_tr_backup_next))
+				goto done;
+			break;
+		case STP_PortRoleTrans_ALTERNATE_PORT:
+			if (stp_proc_state(p, &state_info,
+			    stp_role_tr_alt_p_init, stp_role_tr_alt_p_next))
+				goto done;
+			break;
+		}
+	} while (state_info.new_state);
+
+done:
+	return ret;
+}  /* PortRoleTransitions */
+
+static void stp_discarding_init(struct ksz_stp_port *p)
+{
+#ifdef DBG_STP_STATE_TR
+	dbg_stp(p, __func__, false);
+#endif
+	disableLearning();
+	learning = FALSE;
+	disableForwarding();
+	forwarding = FALSE;
+}  /* stp_discarding_init */
+
+static void stp_discarding_next(struct ksz_stp_port *p,
+	struct ksz_stp_state *state)
+{
+	stp_change_state(state,
+		(learn),
+		STP_PortStateTrans_LEARNING);
+}  /* stp_discarding_next */
+
+static void stp_learning_init(struct ksz_stp_port *p)
+{
+#ifdef DBG_STP_STATE_TR
+	dbg_stp(p, __func__, false);
+#endif
+	enableLearning();
+	learning = TRUE;
+}  /* stp_learning_init */
+
+static void stp_learning_next(struct ksz_stp_port *p,
+	struct ksz_stp_state *state)
+{
+	stp_change_state(state,
+		(!learn),
+		STP_PortStateTrans_DISCARDING);
+	stp_change_state(state,
+		(forward),
+		STP_PortStateTrans_FORWARDING);
+}  /* stp_learning_next */
+
+static void stp_forwarding_init(struct ksz_stp_port *p)
+{
+#ifdef DBG_STP_STATE_TR
+	dbg_stp(p, __func__, false);
+#endif
+	enableForwarding();
+	forwarding = TRUE;
+}  /* stp_forwarding_init */
+
+static void stp_forwarding_next(struct ksz_stp_port *p,
+	struct ksz_stp_state *state)
+{
+	stp_change_state(state,
+		(!forward),
+		STP_PortStateTrans_DISCARDING);
+}  /* stp_forwarding_next */
+
+static int PortStateTransition(struct ksz_stp_port *p)
+{
+	struct ksz_stp_state state_info;
+	u8 *state = &p->states[p->state_index][p->MSTI];
+	int ret = 0;
+
+	state_info.index = STP_PortStateTrans;
+	state_info.change = 1;
+	state_info.new_state = 0;
+	if (STP_BEGIN == *state) {
+		state_info.new_state = STP_PortStateTrans_DISCARDING;
+	}
+	do {
+
+		/* There is a new state. */
+		if (state_info.new_state) {
+			*state = state_info.new_state;
+			ret = state_info.change;
+			ret = 1;
+		}
+		switch (*state) {
+		case STP_PortStateTrans_DISCARDING:
+			if (stp_proc_state(p, &state_info,
+			    stp_discarding_init, stp_discarding_next))
+				goto done;
+			break;
+		case STP_PortStateTrans_LEARNING:
+			if (stp_proc_state(p, &state_info,
+			    stp_learning_init, stp_learning_next))
+				goto done;
+			break;
+		case STP_PortStateTrans_FORWARDING:
+			if (stp_proc_state(p, &state_info,
+			    stp_forwarding_init, stp_forwarding_next))
+				goto done;
+			break;
+		}
+	} while (state_info.new_state);
+
+done:
+	return ret;
+}  /* PortStateTransition */
+
+static void stp_top_inactive_init(struct ksz_stp_port *p)
+{
+#ifdef DBG_STP_STATE
+	dbg_stp(p, __func__, false);
+#endif
+	fdbFlush = TRUE;
+	doFlush();
+
+	/* Stop sending topology change. */
+	tcDetected = 0;
+	tcWhile = 0;
+	tcPropWhile = 0;
+	if (cist())
+		tcAck = FALSE;
+}  /* stp_top_inactive_init */
+
+static void stp_top_inactive_next(struct ksz_stp_port *p,
+	struct ksz_stp_state *state)
+{
+	stp_change_state(state,
+		(learn && !fdbFlush),
+		STP_TopologyChange_LEARNING);
+}  /* stp_top_inactive_next */
+
+static void stp_top_learn_init(struct ksz_stp_port *p)
+{
+#ifdef DBG_STP_STATE_TOPOLOGY
+	dbg_stp(p, __func__, false);
+#endif
+	if (cist())
+		rcvdTc = rcvdTcn = rcvdTcAck = FALSE;
+	rcvdTc = FALSE;
+	tcProp = FALSE;
+}  /* stp_top_learn_init */
+
+static void stp_top_learn_next(struct ksz_stp_port *p,
+	struct ksz_stp_state *state)
+{
+	stp_change_state(state,
+		(ForwardPort && forward && !operEdge),
+		STP_TopologyChange_DETECTED);
+	stp_change_state(state,
+		(!(ForwardPort) && !(learn || learning) &&
+		!(rcvdTc || rcvdTcn || rcvdTcAck || tcProp)),
+		STP_TopologyChange_INACTIVE);
+
+	/*
+	 * LEARNING will be called after DETECTED because the other port is
+	 * in forwarding while this port is going forwarding.
+	 */
+	stp_change_state(state,
+		(rcvdTc || rcvdTcn || rcvdTcAck || tcProp),
+		STP_TopologyChange_LEARNING);
+}  /* stp_top_learn_next */
+
+static void stp_top_det_init(struct ksz_stp_port *p)
+{
+#ifdef DBG_STP_STATE_TOPOLOGY
+	dbg_stp(p, __func__, false);
+#endif
+	newTcWhile();
+	setTcPropTree();
+	newTcDetected();
+	newInfoXst(TRUE);
+}  /* stp_top_det_init */
+
+static void stp_top_det_next(struct ksz_stp_port *p,
+	struct ksz_stp_state *state)
+{
+	stp_change_state(state,
+		TRUE, STP_TopologyChange_ACTIVE);
+}  /* stp_top_det_next */
+
+static void stp_top_ack_init(struct ksz_stp_port *p)
+{
+#ifdef DBG_STP_STATE
+	dbg_stp(p, __func__, false);
+#endif
+	/* Stop sending topology change. */
+	tcWhile = 0;
+	tcPropWhile = 0;
+	rcvdTcAck = FALSE;
+}  /* stp_top_ack_init */
+
+static void stp_top_ack_next(struct ksz_stp_port *p,
+	struct ksz_stp_state *state)
+{
+	stp_change_state(state,
+		TRUE, STP_TopologyChange_ACTIVE);
+}  /* stp_top_ack_next */
+
+static void stp_top_prop_init(struct ksz_stp_port *p)
+{
+#ifdef DBG_STP_STATE
+	dbg_stp(p, __func__, false);
+#endif
+	newTcWhile();
+	fdbFlush = TRUE;
+	doFlush();
+	tcProp = FALSE;
+}  /* stp_top_prop_init */
+
+static void stp_top_prop_next(struct ksz_stp_port *p,
+	struct ksz_stp_state *state)
+{
+	stp_change_state(state,
+		TRUE, STP_TopologyChange_ACTIVE);
+}  /* stp_top_prop_next */
+
+static void stp_top_tc_init(struct ksz_stp_port *p)
+{
+#ifdef DBG_STP_STATE_TOPOLOGY
+	dbg_stp(p, __func__, false);
+#endif
+	rcvdTc = FALSE;
+	if (cist()) {
+		rcvdTcn = FALSE;
+		if (role == ROLE_DESIGNATED)
+			tcAck = TRUE;
+	}
+	setTcPropTree();
+}  /* stp_top_tc_init */
+
+static void stp_top_tc_next(struct ksz_stp_port *p,
+	struct ksz_stp_state *state)
+{
+	stp_change_state(state,
+		TRUE, STP_TopologyChange_ACTIVE);
+}  /* stp_top_tc_next */
+
+static void stp_top_tcn_init(struct ksz_stp_port *p)
+{
+#ifdef DBG_STP_STATE
+	dbg_stp(p, __func__, false);
+#endif
+	newTcWhile();
+}  /* stp_top_tcn_init */
+
+static void stp_top_tcn_next(struct ksz_stp_port *p,
+	struct ksz_stp_state *state)
+{
+	stp_change_state(state,
+		TRUE, STP_TopologyChange_NOTIFIED_TC);
+}  /* stp_top_tcn_next */
+
+static void stp_top_active_init(struct ksz_stp_port *p)
+{
+#ifdef DBG_STP_STATE_TOPOLOGY
+	dbg_stp(p, __func__, false);
+#endif
+}  /* stp_top_active_init */
+
+static void stp_top_active_next(struct ksz_stp_port *p,
+	struct ksz_stp_state *state)
+{
+	/*
+	 * rcvd* variables are reset in LEARNING, so it does have higher
+	 * precedence.
+	 */
+	stp_change_state(state,
+		(!(ForwardPort) || operEdge),
+		STP_TopologyChange_LEARNING);
+	stp_change_state(state,
+		(rcvdTcAck),
+		STP_TopologyChange_ACKNOWLEDGED);
+	stp_change_state(state,
+		(tcProp && !operEdge),
+		STP_TopologyChange_PROPAGATING);
+	stp_change_state(state,
+		(rcvdTc),
+		STP_TopologyChange_NOTIFIED_TC);
+	stp_change_state(state,
+		(rcvdTcn),
+		STP_TopologyChange_NOTIFIED_TCN);
+}  /* stp_top_active_next */
+
+static int TopologyChange(struct ksz_stp_port *p)
+{
+	struct ksz_stp_state state_info;
+	u8 *state = &p->states[p->state_index][p->MSTI];
+	int ret = 0;
+
+	state_info.index = STP_TopologyChange;
+	state_info.change = 1;
+	state_info.new_state = 0;
+	if (STP_BEGIN == *state) {
+		state_info.new_state = STP_TopologyChange_INACTIVE;
+	}
+	do {
+
+		/* There is a new state. */
+		if (state_info.new_state) {
+			*state = state_info.new_state;
+			ret = state_info.change;
+			ret = 1;
+		}
+		switch (*state) {
+		case STP_TopologyChange_INACTIVE:
+			if (stp_proc_state(p, &state_info,
+			    stp_top_inactive_init, stp_top_inactive_next))
+				goto done;
+			break;
+		case STP_TopologyChange_LEARNING:
+			if (stp_proc_state(p, &state_info,
+			    stp_top_learn_init, stp_top_learn_next))
+				goto done;
+			break;
+		case STP_TopologyChange_DETECTED:
+			if (stp_proc_state(p, &state_info,
+			    stp_top_det_init, stp_top_det_next))
+				goto done;
+			break;
+		case STP_TopologyChange_ACKNOWLEDGED:
+			if (stp_proc_state(p, &state_info,
+			    stp_top_ack_init, stp_top_ack_next))
+				goto done;
+			break;
+		case STP_TopologyChange_PROPAGATING:
+			if (stp_proc_state(p, &state_info,
+			    stp_top_prop_init, stp_top_prop_next))
+				goto done;
+			break;
+		case STP_TopologyChange_NOTIFIED_TC:
+			if (stp_proc_state(p, &state_info,
+			    stp_top_tc_init, stp_top_tc_next))
+				goto done;
+			break;
+		case STP_TopologyChange_NOTIFIED_TCN:
+			if (stp_proc_state(p, &state_info,
+			    stp_top_tcn_init, stp_top_tcn_next))
+				goto done;
+			break;
+		case STP_TopologyChange_ACTIVE:
+			if (stp_proc_state(p, &state_info,
+			    stp_top_active_init, stp_top_active_next))
+				goto done;
+			break;
+		}
+	} while (state_info.new_state);
+
+done:
+	return ret;
+}  /* TopologyChange */
+
+
+static
+int (*bridge_stms[NUM_OF_BRIDGE_STATE_MACHINES])(struct ksz_stp_bridge *) = {
+	PortRoleSelection,
+};
+
+/* The bridge state machine needs to be run between the port state machines. */
+#define PORT_STATE_MACHINES_CIST	4
+#define PORT_STATE_MACHINES_BREAK	5
+#define PORT_STATE_MACHINES_TX		8
+
+static
+int (*port_stms[NUM_OF_PORT_STATE_MACHINES])(struct ksz_stp_port *) = {
+	PortTimers,
+	PortReceive,
+	PortProtocolMigration,
+	BridgeDetection,
+	PortInformation,
+
+	PortRoleTransitions,
+	PortStateTransition,
+	TopologyChange,
+	PortTransmit,
+};
+
+
+static void stp_state_machines(struct ksz_stp_bridge *br)
+{
+	int changed;
+	int update;
+	int c;
+	int i;
+	int m;
+	uint p;
+	struct ksz_stp_port *port;
+
+	do {
+		changed = update = 0;
+		m = 0;
+		for (p = 0; p < br->port_cnt; p++) {
+			br->MSTI = m;
+			port = &br->ports[p];
+			if (port->off)
+				continue;
+			for (i = 0; i < PORT_STATE_MACHINES_CIST; i++) {
+				port->state_index = i;
+				port->MSTI = m;
+				changed = port_stms[i](port);
+				update |= (changed << (1 + i));
+			}
+		}
+		for (p = 0; p < br->port_cnt; p++) {
+			port = &br->ports[p];
+			if (port->off)
+				continue;
+			for (i = PORT_STATE_MACHINES_CIST;
+			     i < PORT_STATE_MACHINES_BREAK; i++) {
+				port->state_index = i;
+				for (c = 0, m = 0; c < br->MSTI_cnt; c++, m++) {
+					m = next_msti(br, m);
+					br->MSTI = m;
+					port->MSTI = m;
+					changed = port_stms[i](port);
+					update |= (changed << (1 + i));
+				}
+			}
+#ifdef DEBUG_MSG
+	dbg_print_work(&db.dbg_print);
+#endif
+		}
+		for (i = 0; i < NUM_OF_BRIDGE_STATE_MACHINES; i++) {
+			br->state_index = i;
+			for (c = 0, m = 0; c < br->MSTI_cnt; c++, m++) {
+				m = next_msti(br, m);
+				br->MSTI = m;
+				for (p = 0; p < br->port_cnt; p++) {
+					port = &br->ports[p];
+					port->MSTI = m;
+				}
+				changed = bridge_stms[i](br);
+				update |= (changed << 0);
+			}
+		}
+		for (p = 0; p < br->port_cnt; p++) {
+			port = &br->ports[p];
+			if (port->off)
+				continue;
+			for (i = PORT_STATE_MACHINES_BREAK;
+			     i < PORT_STATE_MACHINES_TX; i++) {
+				port->state_index = i;
+				for (c = 0, m = 0; c < br->MSTI_cnt; c++, m++) {
+					m = next_msti(br, m);
+					br->MSTI = m;
+					port->MSTI = m;
+					changed = port_stms[i](port);
+					update |= (changed << (1 + i));
+				}
+#ifdef DEBUG_MSG
+	dbg_print_work(&db.dbg_print);
+#endif
+			}
+		}
+
+		/*
+		 * Cannot send if all received BPDU are not processed in case
+		 * they are looped back to the bridge.
+		 */
+		if (br->skip_tx)
+			break;
+		m = 0;
+		for (p = 0; p < br->port_cnt; p++) {
+			br->MSTI = m;
+			port = &br->ports[p];
+			if (port->off)
+				continue;
+			for (i = PORT_STATE_MACHINES_TX;
+			     i < NUM_OF_PORT_STATE_MACHINES; i++) {
+				port->state_index = i;
+				port->MSTI = m;
+				changed = port_stms[i](port);
+				update |= (changed << (1 + i));
+			}
+		}
+#ifdef DEBUG_MSG
+	dbg_print_work(&db.dbg_print);
+#endif
+	} while (update);
+}  /* stp_state_machines */
+
+static void proc_state_machines(struct work_struct *work)
+{
+	struct ksz_stp_info *stp =
+		container_of(work, struct ksz_stp_info, state_machine);
+
+	stp->machine_running = true;
+	mutex_lock(&stp->br.lock);
+	stp_state_machines(&stp->br);
+	mutex_unlock(&stp->br.lock);
+	stp->machine_running = false;
+	if (stp->br.skip_tx)
+		schedule_work(&stp->rx_proc);
+}  /* proc_state_machines */
+
+static void invoke_state_machines(struct ksz_stp_bridge *br)
+{
+	struct ksz_stp_info *stp = br->parent;
+
+	if (br->bridgeEnabled && stp->timer_tick)
+		schedule_work(&stp->state_machine);
+}  /* invoke_state_machines */
+
+static void stp_br_msti_init(struct ksz_stp_port *p)
+{
+	BridgeIdentifier.prio = htons(0x8000 + p->br->MSTI);
+	BridgePriority.prio.bridge_id.prio = BridgeIdentifier.prio;
+	if (!p->br->MSTI)
+		BridgePriority.prio.root.prio = BridgeIdentifier.prio;
+
+	BridgeMaxHops = 20;
+	BridgeHelloTime = 2;
+	BridgeMaxAge = 20;
+	BridgeFwdDelay = 15;
+}
+
+static void stp_br_init(struct ksz_stp_port *p)
+{
+	struct ksz_stp_bridge *br = p->br;
+
+	/* MigrateTime is only used internally. */
+	MigrateTime = to_stp_timer(3);
+	TxHoldCount = 6;
+	ForceProtocolVersion = 3;
+#if 0
+	ForceProtocolVersion = 2;
+#endif
+#if 0
+	ForceProtocolVersion = 0;
+#endif
+
+	for (br->MSTI = 0; br->MSTI < NUM_OF_MSTI; br->MSTI++)
+		stp_br_msti_init(p);
+	br->MSTI = 0;
+	br->MSTI_enabled = 1;
+	br->MSTI_cnt = 1;
+#if 0
+	if (ForceProtocolVersion >= 3) {
+		br->MSTI_enabled |= (1 << 1);
+		br->MSTI_cnt++;
+#if 0
+		br->MSTI_enabled |= (1 << 7);
+		br->MSTI_cnt++;
+#endif
+	}
+#endif
+	MstConfigId.name[0] = 'K';
+	MstConfigId.name[1] = 'S';
+	MstConfigId.name[2] = 'Z';
+	recalc_cfg_digest(p);
+
+	p->br->hack_4_1 = 0;
+	p->br->hack_4_2 = 0;
+	p->br->hack_5_2 = 0;
+}  /* stp_br_init */
+
+static void stp_port_msti_init(struct ksz_stp_port *p)
+{
+	portId.prio = 0x80;
+}  /* stp_port_msti_init */
+
+static void stp_port_init(struct ksz_stp_port *p)
+{
+	adminPointToPointMAC = ADMIN_P2P_AUTO;
+	AdminPortPathCost = 0;
+	AdminEdge = FALSE;
+	AutoEdge = FALSE;
+#if 1
+	AutoEdge = TRUE;
+#endif
+#if 0
+	AutoIsolate = TRUE;
+#endif
+	enableBPDUrx = enableBPDUtx = TRUE;
+}  /* stp_port_init */
+
+static void stp_state_init(struct ksz_stp_bridge *br)
+{
+	int i;
+	int m;
+	uint port;
+	struct ksz_stp_port *p;
+	struct ksz_stp_info *stp = br->parent;
+
+	stp->timer_tick = 1000;
+	for (i = 0; i < NUM_OF_BRIDGE_STATE_MACHINES; i++) {
+		br->state_index = i;
+		for (m = 0; m < NUM_OF_MSTI; m++)
+			br->states[br->state_index][m] = STP_BEGIN;
+	}
+	for (port = 0; port < br->port_cnt; port++) {
+		p = &br->ports[port];
+		for (i = 0; i < NUM_OF_PORT_STATE_MACHINES; i++) {
+			p->state_index = i;
+			for (m = 0; m < NUM_OF_MSTI; m++)
+				p->states[p->state_index][m] = STP_BEGIN;
+		}
+
+		memset(p->vars.timers, 0, sizeof(p->vars.timers));
+		for (m = 0; m < NUM_OF_MSTI; m++) {
+			memset(p->mvars[m].timers, 0,
+				sizeof(p->mvars[m].timers));
+			p->MSTI = m;
+			selected = FALSE;
+		}
+	}
+	invoke_state_machines(br);
+}  /* stp_state_init */
+
+static int stp_cfg_port(struct ksz_stp_port *p, int link, int speed,
+	int duplex)
+{
+	int change = 0;
+
+	if (link) {
+		if (!portEnabled) {
+			struct ksz_stp_dbg_times *x = &p->dbg_times[p->MSTI];
+
+			portEnabled = TRUE;
+if (x->block_jiffies) {
+dbg_msg("%s %d %lu\n", __func__, p->port_index, jiffies - x->block_jiffies);
+x->block_jiffies = jiffies;
+}
+			change = 1;
+		}
+		if (p->speed != speed) {
+			p->speed = speed;
+			p->MSTI = 0;
+			if (checkPathCost(p))
+				change = 1;
+		}
+		if (p->duplex != duplex) {
+			p->duplex = duplex;
+			if (checkP2P(p))
+				change = 1;
+		}
+	} else {
+		if (portEnabled) {
+			portEnabled = FALSE;
+			change = 1;
+		}
+	}
+	return change;
+}  /* stp_cfg_port */
+
+static void stp_disable_port(struct ksz_stp_info *stp, uint port)
+{
+	struct ksz_stp_bridge *br = &stp->br;
+	struct ksz_stp_port *p = &br->ports[port];
+
+	portEnabled = FALSE;
+	p->link = 0;
+}  /* stp_disable_port */
+
+static void stp_enable_port(struct ksz_stp_info *stp, uint port, u8 *state)
+{
+	struct ksz_stp_bridge *br = &stp->br;
+	struct ksz_stp_port *p = &br->ports[port];
+
+	if (br->bridgeEnabled && !p->off)
+		*state = STP_STATE_DISABLED;
+}  /* stp_enable_port */
+
+#ifdef DBG_STP_RX
+static int AllDesignatedPort(struct ksz_stp_port *p)
+{
+	int ret = true;
+
+	p->MSTI = 0;
+	FOREACH_T_IN_P(
+		if (!DesignatedPort) {
+			ret = false;
+			break;
+		}
+	)
+	return ret;
+}
+#endif
+
+static void stp_proc_rx(struct ksz_stp_port *p, struct bpdu *bpdu, u16 len)
+{
+	u16 size;
+#ifdef DBG_STP_RX
+	int cmp;
+	int bpdu_len = len;
+
+	if (bpdu_len > sizeof(struct bpdu_v3))
+		bpdu_len = sizeof(struct bpdu_v3);
+	if (AllDesignatedPort(p)) {
+	cmp = memcmp(&p->rx_bpdu0, bpdu, bpdu_len);
+	if (cmp || ((bpdu->flags & (PORT_ROLE_DESIGNATED << PORT_ROLE_S)) !=
+	    (PORT_ROLE_DESIGNATED << PORT_ROLE_S) &&
+	    BPDU_TYPE_CONFIG_RSTP == bpdu->type)) {
+		if (p->dbg_rx < 2)
+			p->dbg_rx = 2;
+	}
+	}
+	if (p->dbg_rx)
+		p->dbg_rx--;
+	if (p->dbg_rx && (bpdu->version <= 2 ||
+	    (bpdu->version >= 3 && len >= sizeof(struct bpdu)))) {
+dbg_msg("[R] %d: ", p->port_index);
+		disp_bpdu(bpdu);
+	}
+	memcpy(&p->rx_bpdu0, bpdu, bpdu_len);
+#endif
+
+	/* Reject bad BPDU. */
+	if (bpdu->protocol != 0)
+		return;
+	size = sizeof(struct bpdu_v1);
+	if (BPDU_TYPE_CONFIG == bpdu->type)
+		size--;
+	else if (BPDU_TYPE_TCN == bpdu->type)
+		size = 7;
+	else if (BPDU_TYPE_CONFIG_RSTP != bpdu->type)
+		size = 10000;
+	if (len < size)
+		return;
+	if (3 == bpdu->version) {
+		size = len - sizeof(struct bpdu) + sizeof(struct mstp_bpdu);
+		size -= 2;
+
+		/* Decode as RST BPDU. */
+		if (bpdu->version_1_length ||
+		    size != ntohs(bpdu->mstp.version_3_length))
+			bpdu->version = 2;
+	}
+	p->MSTI = 0;
+	bpduVersion = bpdu->version <= 3 ? bpdu->version : 3;
+	bpduType = bpdu->type;
+	bpduFlags = 0;
+	bpduRole = (bpdu->flags >> PORT_ROLE_S) & PORT_ROLE_DESIGNATED;
+	if (BPDU_TYPE_CONFIG_RSTP == bpdu->type)
+		bpduFlags = bpdu->flags;
+	else if (BPDU_TYPE_TCN != bpdu->type)
+		bpduFlags = bpdu->flags &
+			(TOPOLOGY_CHANGE_ACK | TOPOLOGY_CHANGE);
+	if (BPDU_TYPE_TCN != bpdu->type) {
+		copy_prio(&bpduPriority, &bpdu->root);
+		bpduTimes.message_age = get_bpdu_time(bpdu->message_age);
+		bpduTimes.max_age = get_bpdu_time(bpdu->max_age);
+		bpduTimes.hello_time = get_bpdu_time(bpdu->hello_time);
+		bpduTimes.forward_delay = get_bpdu_time(bpdu->forward_delay);
+		bpduTimes.max_hops = BridgeMaxHops;
+		if ((BPDU_TYPE_CONFIG == bpdu->type &&
+		    bpduTimes.message_age >= bpduTimes.max_age) ||
+		    bpduTimes.message_age > bpduTimes.max_age)
+			return;
+		if (3 <= bpdu->version && ForceProtocolVersion >= 3) {
+			struct msti_cfg *cfg;
+			struct _port_id port_id = bpduPriority.port_id;
+			struct mstp_times times;
+			struct mstp_bpdu *mstp = &bpdu->mstp;
+			int len = ntohs(mstp->version_3_length);
+
+			COPY(times, bpduTimes);
+			memcpy(&bpduMCID, &mstp->MCID,
+				sizeof(struct mst_cfg_id));
+			memcpy(&bpduPriority.bridge_id, &mstp->id,
+				sizeof(struct _bridge_id));
+			bpduPriority.internal_root_path_cost =
+				mstp->internal_root_path_cost;
+			bpduTimes.max_hops = mstp->max_hops;
+
+			/*
+			 * Initialize bpudFlags in case MSTI is not reported so
+			 * that rcvdMsg will not be set.
+			 */
+			FOREACH_M_IN_P(
+				bpduFlags = 0;
+			)
+			len += 2;
+			len -= sizeof(struct mstp_bpdu);
+			cfg = (struct msti_cfg *)(mstp + 1);
+			while (len >= sizeof(struct msti_cfg)) {
+				p->MSTI = ntohs(cfg->root.prio) & 0x0fff;
+				if (!(p->br->MSTI_enabled & (1 << p->MSTI)))
+					goto next;
+				bpduFlags = cfg->flags;
+				bpduRole = (cfg->flags >> PORT_ROLE_S) &
+					PORT_ROLE_DESIGNATED;
+				memcpy(&bpduPriority.regional_root,
+					&cfg->root, sizeof(struct _bridge_id));
+				bpduPriority.internal_root_path_cost =
+					cfg->root_path_cost;
+				memcpy(&bpduPriority.bridge_id, &mstp->id,
+					sizeof(struct _bridge_id));
+				bpduPriority.bridge_id.prio =
+					htons((cfg->br_prio << 8) | p->MSTI);
+				bpduPriority.port_id = port_id;
+				bpduPriority.port_id.prio = cfg->port_prio;
+				COPY(bpduTimes, times);
+				bpduTimes.max_hops = cfg->max_hops;
+
+next:
+				cfg++;
+				len -= sizeof(struct msti_cfg);
+			}
+			p->MSTI = 0;
+		} else {
+			bpduPriority.internal_root_path_cost = 0;
+			memcpy(&bpduPriority.bridge_id,
+				&bpduPriority.regional_root,
+				sizeof(struct _bridge_id));
+		}
+	} else {
+		ZERO(bpduPriority);
+		ZERO(bpduTimes);
+	}
+	rcvdBPDU = TRUE;
+}  /* stp_proc_rx */
+
+static void stp_proc_tick(struct ksz_stp_bridge *br)
+{
+	uint i;
+	struct ksz_stp_port *p;
+
+	for (i = 0; i < br->port_cnt; i++) {
+		p = &br->ports[i];
+		tick = TRUE;
+if (p->dbg_rx)
+	p->dbg_rx--;
+	}
+	invoke_state_machines(br);
+}  /* stp_proc_tick */
+
+static void proc_rx(struct work_struct *work)
+{
+	struct ksz_stp_info *stp =
+		container_of(work, struct ksz_stp_info, rx_proc);
+	struct ksz_stp_bridge *br = &stp->br;
+	struct ksz_stp_port *p;
+	uint i;
+	bool not_empty;
+	bool last;
+	struct sk_buff *skb;
+
+	if (mutex_is_locked(&stp->br.lock)) {
+		schedule_work(&stp->rx_proc);
+		return;
+	}
+	not_empty = false;
+	for (i = 0; i < br->port_cnt; i++) {
+		p = &br->ports[i];
+		if (rcvdBPDU)
+			continue;
+		last = skb_queue_empty(&p->rxq);
+		not_empty |= !last;
+		if (!last) {
+			skb = skb_dequeue(&p->rxq);
+			last = skb_queue_empty(&p->rxq);
+			if (skb) {
+				struct bpdu *bpdu;
+				u16 len = 0;
+
+				bpdu = chk_bpdu(skb->data, &len);
+				if (bpdu)
+					stp_proc_rx(p, bpdu, len);
+				dev_kfree_skb_irq(skb);
+			}
+		} else
+			br->port_rx &= ~(1 << p->port_index);
+	}
+	if (not_empty || br->port_rx) {
+		br->skip_tx = true;
+		invoke_state_machines(br);
+	} else if (!br->port_rx && br->skip_tx) {
+		br->skip_tx = false;
+		invoke_state_machines(br);
+	}
+}  /* proc_rx */
+
+static int stp_rcv(struct ksz_stp_info *stp, struct sk_buff *skb, uint port)
+{
+	struct bpdu *bpdu;
+	u16 len = 0;
+
+	bpdu = chk_bpdu(skb->data, &len);
+	if (bpdu) {
+		struct ksz_stp_bridge *br = &stp->br;
+		struct ksz_stp_port *p = &br->ports[port];
+
+		if (stp->machine_running || rcvdBPDU || br->port_rx) {
+			skb_queue_tail(&p->rxq, skb);
+			br->port_rx |= (1 << port);
+			schedule_work(&stp->rx_proc);
+		} else {
+			stp_proc_rx(p, bpdu, len);
+			dev_kfree_skb_irq(skb);
+
+			invoke_state_machines(br);
+		}
+		return 0;
+	}
+	return 1;
+}  /* stp_rcv */
+
+static void port_timer_monitor(struct timer_list *t)
+{
+	struct ksz_timer_info *info = from_timer(info, t, timer);
+	struct ksz_stp_info *stp = info->dev;
+
+	stp_proc_tick(&stp->br);
+
+	ksz_update_timer(&stp->port_timer_info);
+}  /* port_timer_monitor */
+
+static void reselectAll(struct ksz_stp_bridge *br)
+{
+	struct ksz_stp_port *p = &br->ports[0];
+
+	FOREACH_P_IN_T(
+		FOREACH_T_IN_P(
+			reselect = TRUE;
+			selected = FALSE;
+		)
+		p->rx_bpdu0.protocol = 0xff;
+		p->tx_bpdu0.protocol = 0xff;
+	)
+}
+
+static const u8 *stp_br_id(struct ksz_stp_info *stp)
+{
+	return (u8 *) &stp->br.mvars[0].br_id_;
+}
+
+static int stp_change_addr(struct ksz_stp_info *stp, u8 *addr)
+{
+	int diff;
+	struct ksz_stp_bridge *br = &stp->br;
+	struct ksz_stp_port *p = &br->ports[0];
+
+	mutex_lock(&br->lock);
+	br->MSTI = 0;
+	diff = memcmp(BridgeIdentifier.addr, addr, ETH_ALEN);
+	for (br->MSTI = 0; br->MSTI < NUM_OF_MSTI; br->MSTI++) {
+		memcpy(BridgeIdentifier.addr, addr, ETH_ALEN);
+
+		if (!br->MSTI)
+			COPY(BridgePriority.prio.root, BridgeIdentifier);
+		COPY(BridgePriority.prio.regional_root, BridgeIdentifier);
+		COPY(BridgePriority.prio.bridge_id, BridgeIdentifier);
+	}
+	br->MSTI = 0;
+	mutex_unlock(&br->lock);
+
+	memcpy(&stp->tx_frame[6], addr, ETH_ALEN);
+	if (diff) {
+		reselectAll(br);
+		invoke_state_machines(br);
+	}
+	return diff;
+}  /* stp_change_addr */
+
+static void stp_set_addr(struct ksz_stp_info *stp, u8 *addr)
+{
+	struct ksz_stp_bridge *br = &stp->br;
+	struct ksz_stp_port *p = &br->ports[0];
+
+	memcpy(BridgeIdentifier.addr, addr, ETH_ALEN);
+}  /* stp_set_addr */
+
+static void stp_link_change(struct ksz_stp_info *stp, int update)
+{
+	uint i;
+	int m;
+	int duplex;
+	int speed;
+	u8 state;
+	int change = 0;
+	struct ksz_port_info *info;
+	struct ksz_stp_port *p;
+	struct ksz_sw *sw = stp->sw_dev;
+	struct ksz_stp_bridge *br = &stp->br;
+
+	if (!br->bridgeEnabled)
+		return;
+	mutex_lock(&br->lock);
+	for (i = 0; i < br->port_cnt; i++) {
+		if (!(sw->dev_ports & (1 << i)))
+			continue;
+		p = &br->ports[i];
+		if (p->off)
+			continue;
+		info = get_port_info(sw, i);
+		if (p->link != (media_connected == info->state)) {
+			p->link = (media_connected == info->state);
+			if (p->link)
+				state = STP_STATE_BLOCKED;
+			else
+				state = STP_STATE_DISABLED;
+#ifdef CONFIG_KSZ_MRP
+			if (!p->link && (sw->features & MRP_SUPPORT)) {
+				struct mrp_info *mrp = &sw->mrp;
+
+				mrp_close_port(mrp, i);
+			}
+#endif
+			sw->ops->acquire(sw);
+			for (m = 0; m < NUM_OF_MSTI; m++) {
+				port_cfg_mstp(sw, i, m);
+				port_set_stp_state(sw, i, state);
+			}
+			sw->ops->release(sw);
+			duplex = (2 == info->duplex);
+			speed = info->tx_rate / TX_RATE_UNIT;
+			change |= stp_cfg_port(p, p->link, speed, duplex);
+		}
+	}
+	mutex_unlock(&br->lock);
+	if (change && update)
+		invoke_state_machines(br);
+}  /* stp_link_change */
+
+static int stp_get_tcDetected(struct ksz_stp_info *stp, int i)
+{
+	struct ksz_stp_port *p;
+	struct ksz_stp_bridge *br = &stp->br;
+
+	p = &br->ports[i];
+	return tcDetected != 0;
+}  /* stp_get_tcDetected */
+
+static void stp_start(struct ksz_stp_info *stp)
+{
+	uint i;
+	uint n;
+	struct ksz_stp_port *p;
+	struct ksz_sw *sw = stp->sw_dev;
+	struct ksz_stp_bridge *br = &stp->br;
+
+	stp->dev = sw->main_dev;
+	sw->ops->acquire(sw);
+	for (n = 1; n <= sw->mib_port_cnt; n++) {
+		i = get_phy_port(sw, n);
+		p = &br->ports[i];
+		if (p->off)
+			continue;
+		FOREACH_T_IN_P(
+			port_cfg_mstp(sw, i, p->MSTI);
+			port_set_stp_state(sw, i, STP_STATE_DISABLED);
+		)
+	}
+	sw->ops->release(sw);
+	do {
+		struct ksz_vlan_table entry;
+		int x;
+		int y;
+
+		memset(&entry, 0, sizeof(struct ksz_vlan_table));
+		for (i = 2; i <= NUM_OF_VID; i++) {
+			x = i / VID_IN_DATA;
+			y = (VID_IN_DATA - 1) - (i % VID_IN_DATA);
+			if (!(sw->info->vid[x] & (1 << y)))
+				continue;
+			x = sw->info->vid2fid[i];
+			entry.fid = (u8) x;
+			entry.untag = 0;
+			entry.ports = sw->PORT_MASK;
+			entry.mstp = sw->info->fid2mstid[x];
+			entry.valid = 1;
+			sw_w_vlan_table(sw, i, &entry);
+		}
+	} while (0);
+	stp->timer_tick = 1000;
+	ksz_start_timer(&stp->port_timer_info, stp->port_timer_info.period);
+	stp_link_change(stp, false);
+	if (!stp_change_addr(stp, sw->info->mac_addr))
+		stp_state_init(&stp->br);
+}  /* stp_start */
+
+static void stp_stop(struct ksz_stp_info *stp, int hw_access)
+{
+	uint i;
+	uint n;
+	struct ksz_stp_port *p;
+	struct ksz_sw *sw = stp->sw_dev;
+	struct ksz_stp_bridge *br = &stp->br;
+
+	if (hw_access) {
+		sw->ops->acquire(sw);
+		for (n = 1; n <= sw->mib_port_cnt; n++) {
+			i = get_phy_port(sw, n);
+			p = &br->ports[i];
+			if (p->off)
+				continue;
+			FOREACH_T_IN_P(
+				port_cfg_mstp(sw, i, p->MSTI);
+				port_set_stp_state(sw, i, STP_STATE_FORWARDING);
+			)
+		}
+		sw->ops->release(sw);
+	}
+	ksz_stop_timer(&stp->port_timer_info);
+	stp->timer_tick = 0;
+	flush_work(&stp->rx_proc);
+	flush_work(&stp->state_machine);
+
+	p = &br->ports[0];
+	stp_br_init(p);
+	for (i = 0; i < br->port_cnt; i++) {
+		p = &br->ports[i];
+		stp_port_init(p);
+		FOREACH_T_IN_P(
+			stp_port_msti_init(p);
+		)
+	}
+}  /* stp_stop */
+
+static void stp_br_test_setup(struct ksz_stp_bridge *br)
+{
+	uint i;
+	struct ksz_stp_port *p;
+
+	p = &br->ports[0];
+	stp_br_init(p);
+	for (i = 0; i < br->port_cnt; i++) {
+		p = &br->ports[i];
+		stp_port_init(p);
+
+		/* Switch back to RSTP if port is in STP. */
+		mcheck = TRUE;
+		AdminPortPathCost = 200000;
+		checkPathCost(p);
+		checkP2P(p);
+		p->rx_bpdu0.protocol = 0xff;
+		FOREACH_T_IN_P(
+			stp_port_msti_init(p);
+		)
+	}
+}  /* stp_br_test_setup */
+
+#define BR_ID_FMT  "%04x.%02x%02x%02x%02x%02x%02x"
+
+#define BR_ID_ARGS(x)  \
+	ntohs(x.prio), x.addr[0], x.addr[1], x.addr[2], \
+	x.addr[3], x.addr[4], x.addr[5]
+
+#define BOOL_STR(x)  ((x) ? "yes" : "no")
+
+static char *get_admin_p2p_str(int p2p)
+{
+	switch (p2p) {
+	case ADMIN_P2P_FORCE_FALSE:
+		return "no";
+	case ADMIN_P2P_FORCE_TRUE:
+		return "yes";
+	case ADMIN_P2P_AUTO:
+		return "auto";
+	}
+	return "unk";
+}
+
+static char *get_port_state_str(int state)
+{
+	switch (state) {
+	case STP_PortStateTrans_DISCARDING:
+		return "discarding";
+	case STP_PortStateTrans_LEARNING:
+		return "learning";
+	case STP_PortStateTrans_FORWARDING:
+		return "forwarding";
+	}
+	return "unknown";
+}
+
+static char *get_port_role_str(int _role)
+{
+	switch (_role) {
+	case ROLE_ROOT:
+		return "root";
+	case ROLE_DESIGNATED:
+		return "designated";
+	case ROLE_ALTERNATE:
+		return "alternate";
+	case ROLE_BACKUP:
+		return "backup";
+	case ROLE_DISABLED:
+		return "disabled";
+	case ROLE_MASTER:
+		return "master";
+	}
+	return "unknown";
+}
+
+static ssize_t sysfs_stp_read(struct ksz_sw *sw, int proc_num, ssize_t len,
+	char *buf)
+{
+	struct ksz_stp_info *stp = &sw->info->rstp;
+	struct ksz_stp_port *p;
+	struct _bridge_id root;
+	char name[36];
+	uint path_cost;
+	int enabled;
+	int i;
+	int chk = 0;
+	int type = SHOW_HELP_NUM;
+	char note[40];
+
+	note[0] = '\0';
+	if (!(sw->features & STP_SUPPORT))
+		return 0;
+	mutex_lock(&stp->br.lock);
+	p = &stp->br.ports[0];
+	p->MSTI = p->br->MSTI = p->br->MSTI_index;
+	switch (proc_num) {
+	case PROC_GET_STP_BR_INFO:
+		enabled = stp->br.bridgeEnabled;
+		if (enabled && !(stp->br.MSTI_enabled & (1 << p->MSTI)))
+			enabled = false;
+		len += sprintf(buf + len,
+			" enabled\t\t%4s\n",
+			BOOL_STR(enabled));
+		len += sprintf(buf + len,
+			" bridge id\t\t" BR_ID_FMT "\n",
+			BR_ID_ARGS(BridgeIdentifier));
+		if (p->MSTI) {
+			COPY(root, rootPriority.regional_root);
+			path_cost = rootPriority.internal_root_path_cost;
+		} else {
+			COPY(root, rootPriority.root);
+			path_cost = rootPriority.root_path_cost;
+		}
+		len += sprintf(buf + len,
+			" designated root\t" BR_ID_FMT "",
+			BR_ID_ARGS(root));
+		len += sprintf(buf + len,
+			"\tpath cost\t%12u\n",
+			ntohl(path_cost));
+		if (ForceProtocolVersion >= 3 && !p->MSTI) {
+			COPY(root, rootPriority.regional_root);
+			path_cost = rootPriority.internal_root_path_cost;
+			len += sprintf(buf + len,
+				" regional root\t\t" BR_ID_FMT "",
+				BR_ID_ARGS(root));
+			len += sprintf(buf + len,
+				"\tpath cost\t%12u\n",
+				ntohl(path_cost));
+		}
+		len += sprintf(buf + len,
+			" root port\t\t%4u\n",
+			rootPortId.num);
+		if (!p->MSTI) {
+			len += sprintf(buf + len,
+				" max age\t\t%4u",
+				rootTimes.max_age);
+			len += sprintf(buf + len,
+				"\t\t\tbridge max age\t\t%4u\n",
+				BridgeMaxAge);
+			len += sprintf(buf + len,
+				" hello time\t\t%4u",
+				rootTimes.hello_time);
+			len += sprintf(buf + len,
+				"\t\t\tbridge hello time\t%4u\n",
+				BridgeHelloTime);
+			len += sprintf(buf + len,
+				" forward delay\t\t%4u",
+				rootTimes.forward_delay);
+			len += sprintf(buf + len,
+				"\t\t\tbridge forward delay\t%4u\n",
+				BridgeFwdDelay);
+			len += sprintf(buf + len,
+				" tx hold count\t\t%4u",
+				TxHoldCount);
+			len += sprintf(buf + len,
+				"\t\t\tprotocol version\t%4u\n",
+				ForceProtocolVersion);
+		}
+		if (ForceProtocolVersion >= 3) {
+			len += sprintf(buf + len,
+				" maximum hops\t\t%4u",
+				rootTimes.max_hops);
+			len += sprintf(buf + len,
+				"\t\t\tbridge maximum hops\t%4u\n",
+				BridgeMaxHops);
+		}
+		len += sprintf(buf + len,
+			" time since topology change\t%9u\n",
+			timeSinceTC);
+		len += sprintf(buf + len,
+			" topology change count\t\t%9u\n",
+			cntTC);
+		len += sprintf(buf + len,
+			" topology change\t\t%9s\n",
+			BOOL_STR(isTC));
+		type = SHOW_HELP_NONE;
+		break;
+	case PROC_SET_STP_BR_ON:
+		chk = p->br->bridgeEnabled;
+		type = SHOW_HELP_ON_OFF;
+#if defined(DBG_STP_STATE) || defined(DBG)
+		d_stp_states(p->br);
+#endif
+		break;
+	case PROC_SET_STP_BR_PRIO:
+		chk = ntohs(BridgeIdentifier.prio);
+		type = SHOW_HELP_HEX;
+		break;
+	case PROC_SET_STP_BR_FWD_DELAY:
+		chk = BridgeFwdDelay;
+		break;
+	case PROC_SET_STP_BR_MAX_AGE:
+		chk = BridgeMaxAge;
+		break;
+	case PROC_SET_STP_BR_HELLO_TIME:
+		chk = BridgeHelloTime;
+		break;
+	case PROC_SET_STP_BR_TX_HOLD:
+		chk = TxHoldCount;
+		break;
+	case PROC_SET_STP_VERSION:
+		chk = ForceProtocolVersion;
+		if (sw->verbose) {
+			switch (chk) {
+			case 3:
+				strcpy(note, " (mstp)");
+				break;
+			case 2:
+				strcpy(note, " (rstp)");
+				break;
+			case 0:
+				strcpy(note, " (stp)");
+				break;
+			default:
+				strcpy(note, " (unknown)");
+			}
+		}
+		break;
+	case PROC_SET_STP_BR_MAX_HOPS:
+		chk = BridgeMaxHops;
+		break;
+	case PROC_SET_STP_MSTI:
+		chk = p->br->MSTI_index;
+		break;
+	case PROC_SET_STP_MSTI_VID:
+		chk = p->br->MSTI_index;
+		enabled = 0;
+		for (i = 0; i < VLAN_TABLE_ENTRIES; i++) {
+			if ((i % (32 * 8)) == 0)
+				len += sprintf(buf + len,
+					"%03x: ", i);
+			enabled <<= 1;
+			if (chk == sw->info->fid2mstid[sw->info->vid2fid[i]])
+				enabled |= 1;
+			if ((i % 32) == 31) {
+				enabled &= sw->info->vid[i / VID_IN_DATA];
+				len += sprintf(buf + len,
+					"%08x ", enabled);
+				enabled = 0;
+			}
+			if ((i % (32 * 8)) == (32 * 8 - 1))
+				len += sprintf(buf + len,
+					"\n");
+		}
+		type = SHOW_HELP_NONE;
+		break;
+	case PROC_SET_STP_MSTP_CFG:
+		len += sprintf(buf + len,
+			"%02x  %04x\n", MstConfigId.selector,
+			htons(MstConfigId.revision));
+		strncpy(name, MstConfigId.name, 32);
+		name[32] = '\0';
+		len += sprintf(buf + len,
+			"%s\n", name);
+		for (i = 0; i < 16; i++)
+			len += sprintf(buf + len,
+				"%02X", MstConfigId.digest[i]);
+		len += sprintf(buf + len, "\n");
+		type = SHOW_HELP_NONE;
+		break;
+	case PROC_SET_STP_MSTP_NAME:
+		strncpy(name, MstConfigId.name, 32);
+		name[32] = '\0';
+		len += sprintf(buf + len,
+			"%s\n", name);
+		type = SHOW_HELP_NONE;
+		break;
+	default:
+		type = SHOW_HELP_NONE;
+	}
+	p->MSTI = p->br->MSTI = 0;
+	mutex_unlock(&stp->br.lock);
+	return sysfs_show(len, buf, type, chk, note, sw->verbose);
+}  /* sysfs_stp_read */
+
+static int sysfs_stp_write(struct ksz_sw *sw, int proc_num, int num,
+	const char *buf)
+{
+	struct ksz_stp_info *stp = &sw->info->rstp;
+	struct ksz_stp_bridge *br = &stp->br;
+	struct ksz_stp_port *p;
+	char name[36];
+	uint i;
+	int set;
+	int change = 0;
+	int processed = true;
+
+	if (!(sw->features & STP_SUPPORT))
+		return false;
+	mutex_lock(&stp->br.lock);
+	p = &br->ports[0];
+	p->MSTI = p->br->MSTI = p->br->MSTI_index;
+	switch (proc_num) {
+	case PROC_SET_STP_BR_ON:
+		set = !!num;
+		if (set != br->bridgeEnabled) {
+			br->bridgeEnabled = set;
+			if (br->bridgeEnabled) {
+				mutex_unlock(&stp->br.lock);
+				stp_start(stp);
+				mutex_lock(&stp->br.lock);
+			} else
+				stp_stop(stp, true);
+		}
+		break;
+	case PROC_SET_STP_BR_PRIO:
+		if ('0' != buf[0] || 'x' != buf[1])
+			sscanf(buf, "%x", &num);
+		if ((0 <= num && num <= 0xf000) && !(num & ~0xf000)) {
+			u16 prio = ntohs(BridgeIdentifier.prio);
+
+			if (num != prio) {
+				prio = num | p->br->MSTI;
+				BridgeIdentifier.prio = htons(prio);
+				BridgePriority.prio.regional_root.prio =
+				BridgePriority.prio.bridge_id.prio =
+					BridgeIdentifier.prio;
+				if (!p->br->MSTI)
+					BridgePriority.prio.root.prio =
+						BridgeIdentifier.prio;
+				change = 1;
+			}
+		}
+		break;
+	case PROC_SET_STP_BR_FWD_DELAY:
+		if (checkParameters(BridgeHelloTime, BridgeMaxAge, num)) {
+			if (num != BridgeFwdDelay) {
+				BridgeFwdDelay = num;
+				change = 1;
+			}
+		}
+		break;
+	case PROC_SET_STP_BR_MAX_AGE:
+		if (checkParameters(BridgeHelloTime, num, BridgeFwdDelay)) {
+			if (num != BridgeMaxAge) {
+				BridgeMaxAge = num;
+				change = 1;
+			}
+		}
+		break;
+	case PROC_SET_STP_BR_HELLO_TIME:
+		if (num == 2) {
+			if (num != BridgeHelloTime) {
+				BridgeHelloTime = num;
+				change = 1;
+			}
+		}
+		break;
+	case PROC_SET_STP_BR_TX_HOLD:
+		if (1 <= num && num <= 10) {
+			if (num != TxHoldCount) {
+				TxHoldCount = num;
+				for (i = 0; i < br->port_cnt; i++) {
+					p = &br->ports[i];
+					txCount = 0;
+				}
+				change = 1;
+			}
+		}
+		break;
+	case PROC_SET_STP_VERSION:
+		if (0 == num || 2 == num) {
+			ForceProtocolVersion = num;
+			change = 2;
+		}
+		if (1 == num) {
+			stp_br_test_setup(br);
+			change = 1;
+		}
+		if (41 == num)
+			br->hack_4_1 = 1;
+		if (42 == num)
+			br->hack_4_2 = 1;
+		if (52 == num)
+			br->hack_5_2 = 1;
+		break;
+	case PROC_SET_STP_BR_MAX_HOPS:
+		if (10 <= num && num <= 20) {
+			if (num != BridgeMaxHops) {
+				BridgeMaxHops = num;
+				change = 1;
+			}
+		}
+		break;
+	case PROC_SET_STP_MSTI:
+		if (0 <= num && num < NUM_OF_MSTI &&
+		    ForceProtocolVersion >= 3) {
+			p->br->MSTI_index = num;
+			sw->ops->acquire(sw);
+			for (i = 0; i < br->port_cnt; i++) {
+				p = &br->ports[i];
+				if (p->off)
+					continue;
+				port_cfg_mstp(sw, i, num);
+			}
+			sw->ops->release(sw);
+		}
+		break;
+	case PROC_SET_STP_MSTP_NAME:
+		strncpy(name, buf, 32);
+		name[32] = '\0';
+		i = strlen(name);
+		if ('\n' == name[i - 1])
+			name[i - 1] = '\0';
+		if (strncmp(name, MstConfigId.name, 32)) {
+			strncpy(MstConfigId.name, name, 32);
+			change = 2;
+		}
+		if (sw->info->fid_updated) {
+			sw->info->fid_updated = 0;
+			recalc_cfg_digest(p);
+			change = 2;
+		}
+		break;
+	default:
+		processed = false;
+		break;
+	}
+	p->MSTI = p->br->MSTI = 0;
+	mutex_unlock(&stp->br.lock);
+	if (change && br->bridgeEnabled) {
+		reselectAll(br);
+		if (2 == change)
+			stp_state_init(br);
+		else
+			invoke_state_machines(br);
+	}
+	return processed;
+}  /* sysfs_stp_write */
+
+static ssize_t sysfs_stp_port_read(struct ksz_sw *sw, int proc_num, uint port,
+	ssize_t len, char *buf)
+{
+	struct ksz_stp_info *stp = &sw->info->rstp;
+	struct ksz_stp_port *p;
+	struct _bridge_id root;
+	uint cost;
+	uint path_cost;
+	char *state_str;
+	int enabled;
+	int chk = 0;
+	int type = SHOW_HELP_NONE;
+	char note[40];
+
+	note[0] = '\0';
+	if (!(sw->features & STP_SUPPORT))
+		return 0;
+	port = get_sysfs_port(sw, port);
+	if (port == sw->HOST_PORT)
+		return 0;
+	mutex_lock(&stp->br.lock);
+	p = &stp->br.ports[port];
+	p->MSTI = p->br->MSTI = p->br->MSTI_index;
+	switch (proc_num) {
+	case PROC_GET_STP_INFO:
+		if (p->off)
+			break;
+		enabled = portEnabled;
+		if (enabled && !(stp->br.MSTI_enabled & (1 << p->MSTI)))
+			enabled = false;
+		state_str = get_port_role_str(role);
+		len += sprintf(buf + len,
+			" enabled\t\t%4s",
+			BOOL_STR(enabled));
+		len += sprintf(buf + len,
+			"\t\t\trole\t\t%12s\n",
+			state_str);
+		state_str = get_port_state_str(p->states[6][p->MSTI]);
+		len += sprintf(buf + len,
+			" port id\t\t%02x%02x\t\t\tstate\t\t%12s\n",
+		       portId.prio, portId.num, state_str);
+		if (p->MSTI) {
+			path_cost = ExternalPortPathCost;
+			COPY(root, designatedPriority.regional_root);
+			cost = designatedPriority.internal_root_path_cost;
+		} else {
+			path_cost = InternalPortPathCost;
+			COPY(root, designatedPriority.root);
+			cost = designatedPriority.root_path_cost;
+		}
+		len += sprintf(buf + len,
+			" path cost\t%12u\t\t\tadmin path cost\t%12u\n",
+			path_cost, AdminPortPathCost);
+		len += sprintf(buf + len,
+			" designated root\t" BR_ID_FMT,
+			BR_ID_ARGS(root));
+		len += sprintf(buf + len,
+			"\tdesignated cost\t%12u\n",
+			ntohl(cost));
+		len += sprintf(buf + len,
+			" designated bridge\t" BR_ID_FMT,
+			BR_ID_ARGS(designatedPriority.bridge_id));
+		len += sprintf(buf + len,
+			"\tdesignated port\t\t%02x%02x\n",
+			designatedPriority.port_id.prio,
+			designatedPriority.port_id.num);
+		if (ForceProtocolVersion >= 3 && !p->MSTI) {
+			COPY(root, designatedPriority.regional_root);
+			cost = designatedPriority.internal_root_path_cost;
+			len += sprintf(buf + len,
+				" regional root\t\t" BR_ID_FMT,
+				BR_ID_ARGS(root));
+			len += sprintf(buf + len,
+				"\tinternal cost\t%12u\n",
+				ntohl(cost));
+		}
+		len += sprintf(buf + len,
+			" admin edge port\t%4s",
+			BOOL_STR(AdminEdge));
+		len += sprintf(buf + len,
+			"\t\t\tauto edge port\t\t%4s\n",
+			BOOL_STR(AutoEdge));
+		len += sprintf(buf + len,
+			" oper edge port\t\t%4s",
+			BOOL_STR(operEdge));
+		len += sprintf(buf + len,
+			"\t\t\ttopology change ack\t%4s\n",
+			BOOL_STR(tcAck));
+		len += sprintf(buf + len,
+			" point to point\t\t%4s",
+			BOOL_STR(operPointToPointMAC));
+		len += sprintf(buf + len,
+			"\t\t\tadmin point to point\t%4s\n",
+			get_admin_p2p_str(adminPointToPointMAC));
+		type = SHOW_HELP_NONE;
+		break;
+	case PROC_SET_STP_ON:
+		chk = portEnabled;
+		type = SHOW_HELP_ON_OFF;
+		break;
+	case PROC_SET_STP_PRIO:
+		chk = portId.prio;
+		type = SHOW_HELP_HEX;
+		break;
+	case PROC_SET_STP_ADMIN_PATH_COST:
+		chk = AdminPortPathCost;
+		type = SHOW_HELP_NUM;
+		break;
+	case PROC_SET_STP_PATH_COST:
+		chk = ExternalPortPathCost;
+		type = SHOW_HELP_NUM;
+		break;
+	case PROC_SET_STP_ADMIN_EDGE:
+		chk = AdminEdge;
+		type = SHOW_HELP_ON_OFF;
+		break;
+	case PROC_SET_STP_AUTO_EDGE:
+		chk = AutoEdge;
+		type = SHOW_HELP_ON_OFF;
+		break;
+	case PROC_SET_STP_MCHECK:
+		chk = mcheck;
+		type = SHOW_HELP_ON_OFF;
+		break;
+	case PROC_SET_STP_ADMIN_P2P:
+		chk = adminPointToPointMAC;
+		if (sw->verbose) {
+			switch (chk) {
+			case ADMIN_P2P_AUTO:
+				strcpy(note, " (auto)");
+				break;
+			case ADMIN_P2P_FORCE_TRUE:
+				strcpy(note, " (force true)");
+				break;
+			case ADMIN_P2P_FORCE_FALSE:
+				strcpy(note, " (force false)");
+				break;
+			}
+		}
+		type = SHOW_HELP_SPECIAL;
+		break;
+	case PROC_SET_STP_AUTO_ISOLATE:
+		chk = AutoIsolate;
+		type = SHOW_HELP_ON_OFF;
+		break;
+	}
+	p->MSTI = p->br->MSTI = 0;
+	mutex_unlock(&stp->br.lock);
+	return sysfs_show(len, buf, type, chk, note, sw->verbose);
+}  /* sysfs_stp_port_read */
+
+static int sysfs_stp_port_write(struct ksz_sw *sw, int proc_num, uint port,
+	int num, const char *buf)
+{
+	struct ksz_stp_info *stp = &sw->info->rstp;
+	struct ksz_stp_port *p;
+	int set;
+	int change = 0;
+	int processed = true;
+
+	if (!(sw->features & STP_SUPPORT))
+		return false;
+	port = get_sysfs_port(sw, port);
+	if (port == sw->HOST_PORT)
+		return false;
+	mutex_lock(&stp->br.lock);
+	p = &stp->br.ports[port];
+	p->MSTI = p->br->MSTI = p->br->MSTI_index;
+	switch (proc_num) {
+	case PROC_SET_STP_ON:
+		set = !!num;
+		if (set != portEnabled) {
+			portEnabled = set;
+			if (portEnabled && p->br->hack_4_2)
+				mdelayWhile = 0;
+			change = 1;
+		}
+		break;
+	case PROC_SET_STP_PRIO:
+		if ('0' != buf[0] || 'x' != buf[1])
+			sscanf(buf, "%x", &num);
+		if ((0 <= num && num <= 0xf0) && !(num & ~0xf0)) {
+			u8 prio = portId.prio;
+
+			if (num != prio) {
+				portId.prio = num;
+				reselect = TRUE;
+				selected = FALSE;
+				change = 1;
+			}
+		}
+		break;
+	case PROC_SET_STP_ADMIN_PATH_COST:
+		if (0 <= num && num <= PATH_COST * 10) {
+			AdminPortPathCost = num;
+			change = checkPathCost(p);
+		}
+		break;
+	case PROC_SET_STP_PATH_COST:
+		if (1 <= num && num <= PATH_COST * 10) {
+			AdminPortPathCost = num;
+			change = checkPathCost(p);
+		}
+		break;
+	case PROC_SET_STP_ADMIN_EDGE:
+		set = !!num;
+		if (set != AdminEdge) {
+			AdminEdge = set;
+			change = 1;
+		}
+		break;
+	case PROC_SET_STP_AUTO_EDGE:
+		set = !!num;
+		if (set != AutoEdge) {
+			AutoEdge = set;
+			change = 1;
+		}
+		break;
+	case PROC_SET_STP_MCHECK:
+		set = !!num;
+		if (set != mcheck) {
+			mcheck = set;
+			change = 1;
+		}
+		break;
+	case PROC_SET_STP_ADMIN_P2P:
+		if (ADMIN_P2P_FORCE_FALSE <= num && num <= ADMIN_P2P_AUTO) {
+			if (num != adminPointToPointMAC) {
+				adminPointToPointMAC = num;
+				change = checkP2P(p);
+			}
+		}
+		break;
+	case PROC_SET_STP_AUTO_ISOLATE:
+		set = !!num;
+		if (set != AutoIsolate) {
+			AutoIsolate = set;
+			change = 1;
+		}
+		break;
+	default:
+		processed = false;
+		break;
+	}
+	p->MSTI = p->br->MSTI = 0;
+	mutex_unlock(&stp->br.lock);
+	if (change) {
+		p->rx_bpdu0.protocol = 0xff;
+		invoke_state_machines(p->br);
+	}
+	return processed;
+}  /* sysfs_stp_port_write */
+
+static u8 MAC_ADDR_STP[] = { 0x01, 0x80, 0xC2, 0x00, 0x00, 0x00 };
+
+static void prep_stp_mcast(struct net_device *dev)
+{
+	char addr[MAX_ADDR_LEN];
+
+	memcpy(addr, MAC_ADDR_STP, ETH_ALEN);
+	dev_mc_add(dev, addr);
+}  /* prep_stp_mcast */
+
+static void leave_stp(struct ksz_stp_info *stp)
+{
+	if (stp->dev)
+		dev_mc_del(stp->dev, MAC_ADDR_STP);
+}  /* leave_stp */
+
+static struct stp_ops stp_ops = {
+	.change_addr		= stp_change_addr,
+	.link_change		= stp_link_change,
+
+	.get_tcDetected		= stp_get_tcDetected,
+};
+
+static void ksz_stp_exit(struct ksz_stp_info *stp)
+{
+	/* stp_stop should be called before. */
+}  /* ksz_stp_exit */
+
+static void ksz_stp_init(struct ksz_stp_info *stp, struct ksz_sw *sw)
+{
+	struct ksz_stp_bridge *br;
+	struct ksz_stp_port *p;
+	uint i;
+	int j;
+	int num;
+	struct llc *llc;
+#ifdef DBG_STP_PORT_FLUSH
+	struct ksz_stp_dbg_times *x;
+#endif
+
+	stp->sw_dev = sw;
+	ksz_init_timer(&stp->port_timer_info, STP_TIMER_TICK * HZ / 1000,
+		port_timer_monitor, stp);
+	INIT_WORK(&stp->state_machine, proc_state_machines);
+	INIT_WORK(&stp->rx_proc, proc_rx);
+	mutex_init(&stp->br.lock);
+	stp->ops = &stp_ops;
+
+	memcpy(stp->tx_frame, MAC_ADDR_STP, ETH_ALEN);
+	llc = (struct llc *) &stp->tx_frame[12];
+	llc->dsap = 0x42;
+	llc->ssap = 0x42;
+	llc->ctrl = 0x03;
+	stp->bpdu = (struct bpdu *)(llc + 1);
+
+	br = &stp->br;
+	br->parent = stp;
+	if (sw->stp)
+		br->bridgeEnabled = TRUE;
+
+	br->port_cnt = sw->port_cnt;
+	if (br->port_cnt > SWITCH_PORT_NUM)
+		br->port_cnt = SWITCH_PORT_NUM;
+
+	/* Can turn off ports.  Useful for using one port for telnet. */
+	num = sw->stp;
+	if (1 == num)
+		num = sw->PORT_MASK;
+	num &= ~sw->HOST_MASK;
+
+	br->MSTI = 0;
+	for (i = 0; i < SWITCH_PORT_NUM; i++) {
+		p = &br->ports[i];
+		p->MSTI = br->MSTI;
+		if (!(num & (1 << i)))
+			p->off = TRUE;
+		skb_queue_head_init(&p->rxq);
+		p->port_index = i;
+		p->br = br;
+		stp_port_init(p);
+	}
+
+	for (br->MSTI = 0; br->MSTI < NUM_OF_MSTI; br->MSTI++) {
+		for (i = 0; i < br->port_cnt; i++) {
+			p = &br->ports[i];
+			p->MSTI = br->MSTI;
+			if (p->off) {
+				selectedRole = ROLE_DISABLED;
+				role = ROLE_DISABLED;
+				infoIs = INFO_TYPE_DISABLED;
+				synced = TRUE;
+				continue;
+			}
+			portId.num = get_log_port(sw, i);
+			stp_port_msti_init(p);
+
+#ifdef DBG_STP_PORT_FLUSH
+			x = &p->dbg_times[p->MSTI];
+
+			/* No Root Port connected to port yet. */
+			x->role_ = ROLE_DISABLED;
+			x->downPriority.port_id.num = 0;
+#endif
+		}
+
+		p = &br->ports[0];
+		ZERO(BridgePriority);
+	}
+	stp_br_init(p);
+	for (i = 0; i < br->port_cnt; i++) {
+		p = &br->ports[i];
+		p->MSTI = br->MSTI;
+	}
+
+	sw->info->vid2fid[0] = sw->info->vid2fid[NUM_OF_VID + 1] = 0;
+	for (i = 1; i < NUM_OF_VID; i++) {
+		sw->info->vid2fid[i] = ((i - 1) % (FID_ENTRIES - 1)) + 1;
+	}
+	for (i = 1; i <= NUM_OF_MSTI; i++) {
+		num = i / VID_IN_DATA;
+		j = (VID_IN_DATA - 1) - (i % VID_IN_DATA);
+		sw->info->vid[num] |= (1 << j);
+	}
+#if 0
+	for (i = 0; i < FID_ENTRIES; i++)
+		sw->info->fid2mstid[i] = 0;
+#endif
+#if 0
+	for (i = 0; i < FID_ENTRIES; i++)
+		sw->info->fid2mstid[i] = 1;
+#endif
+#if 0
+	for (i = 0; i < FID_ENTRIES; i++)
+		sw->info->fid2mstid[i] = (i % 32) + 1;
+#endif
+	for (i = 0; i < FID_ENTRIES; i++)
+		sw->info->fid2mstid[i] = (i % NUM_OF_MSTI);
+	recalc_cfg_digest(p);
+}  /* ksz_stp_init */
+
+
+#undef forward
+#undef forwarding
+#undef learn
+#undef learning
+#undef master
+#undef mastered
+#undef proposed
+#undef proposing
+#undef reselect
+#undef selected
+#undef sync
+#undef synced
+
diff -Napur linux-linux4microchip-2022.10-orig/drivers/net/ethernet/micrel/ksz_mstp.h linux-linux4microchip-2022.10/drivers/net/ethernet/micrel/ksz_mstp.h
--- linux-linux4microchip-2022.10-orig/drivers/net/ethernet/micrel/ksz_mstp.h	1969-12-31 16:00:00.000000000 -0800
+++ linux-linux4microchip-2022.10/drivers/net/ethernet/micrel/ksz_mstp.h	2023-04-25 16:13:55.044163479 -0700
@@ -0,0 +1,454 @@
+/**
+ * Microchip MSTP driver header
+ *
+ * Copyright (c) 2016-2017 Microchip Technology Inc.
+ *	Tristram Ha <Tristram.Ha@microchip.com>
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ */
+
+
+#ifndef KSZ_MSTP_H
+#define KSZ_MSTP_H
+
+
+#define STP_TAG_TYPE			0x1080
+
+
+struct llc {
+	u16 len;
+	u8 dsap;
+	u8 ssap;
+	u8 ctrl;
+} __packed;
+
+struct _bridge_id {
+	u16 prio;
+	u8 addr[6];
+} __packed;
+
+struct _port_id {
+	u8 prio;
+	u8 num;
+} __packed;
+
+struct mst_cfg_id {
+	u8 selector;
+	u8 name[32];
+	u16 revision;
+	u8 digest[16];
+} __packed;
+
+struct msti_cfg {
+	u8 flags;
+	struct _bridge_id root;
+	u32 root_path_cost;
+	u8 br_prio;
+	u8 port_prio;
+	u8 max_hops;
+} __packed;
+
+struct mstp_bpdu {
+	u16 version_3_length;
+	struct mst_cfg_id MCID;
+	u32 internal_root_path_cost;
+	struct _bridge_id id;
+	u8 max_hops;
+} __packed;
+
+struct bpdu_v1 {
+	u16 protocol;
+	u8 version;
+	u8 type;
+	u8 flags;
+	struct _bridge_id root;
+	u32 root_path_cost;
+	struct _bridge_id bridge_id;
+	struct _port_id port_id;
+	u16 message_age;
+	u16 max_age;
+	u16 hello_time;
+	u16 forward_delay;
+	u8 version_1_length;
+} __packed;
+
+struct bpdu {
+	u16 protocol;
+	u8 version;
+	u8 type;
+	u8 flags;
+	struct _bridge_id root;
+	u32 root_path_cost;
+	struct _bridge_id bridge_id;
+	struct _port_id port_id;
+	u16 message_age;
+	u16 max_age;
+	u16 hello_time;
+	u16 forward_delay;
+	u8 version_1_length;
+	struct mstp_bpdu mstp;
+} __packed;
+
+struct bpdu_v3 {
+	u16 protocol;
+	u8 version;
+	u8 type;
+	u8 flags;
+	struct _bridge_id root;
+	u32 root_path_cost;
+	struct _bridge_id bridge_id;
+	struct _port_id port_id;
+	u16 message_age;
+	u16 max_age;
+	u16 hello_time;
+	u16 forward_delay;
+	u8 version_1_length;
+	struct mstp_bpdu mstp;
+	struct msti_cfg msti_cfgs[64];
+} __packed;
+
+
+struct stp_prio {
+	struct _bridge_id root;
+	u32 root_path_cost;
+	struct _bridge_id bridge_id;
+	struct _port_id port_id;
+} __packed;
+
+struct stp_vector {
+	struct stp_prio prio;
+	struct _port_id port_id;
+} __packed;
+
+struct mstp_prio {
+	struct _bridge_id root;
+	u32 root_path_cost;
+	struct _bridge_id regional_root;
+	u32 internal_root_path_cost;
+	struct _bridge_id bridge_id;
+	struct _port_id port_id;
+} __packed;
+
+struct mstp_vector {
+	struct mstp_prio prio;
+	struct _port_id port_id;
+} __packed;
+
+struct mstp_times {
+	u16 message_age;
+	u16 max_age;
+	u16 hello_time;
+	u16 forward_delay;
+	u8 max_hops;
+};
+
+
+enum {
+	ADMIN_P2P_FORCE_FALSE,
+	ADMIN_P2P_FORCE_TRUE,
+	ADMIN_P2P_AUTO,
+};
+
+enum {
+	ROLE_UNKNOWN,
+	ROLE_ALT_BACKUP,
+	ROLE_ROOT,
+	ROLE_DESIGNATED,
+	ROLE_ALTERNATE,
+	ROLE_BACKUP,
+	ROLE_DISABLED,
+	ROLE_MASTER,
+};
+
+enum {
+	INFO_SUPERIOR_DESIGNATED,
+	INFO_REPEATED_DESIGNATED,
+	INFO_INFERIOR_DESIGNATED,
+	INFO_INFERIOR_ROOT_ALT,
+	INFO_OTHER,
+};
+
+enum {
+	INFO_TYPE_UNKNOWN,
+	INFO_TYPE_MINE,
+	INFO_TYPE_AGED,
+	INFO_TYPE_RECEIVED,
+	INFO_TYPE_DISABLED,
+};
+
+struct stp_br_vars {
+	uint MigrateTime_;
+	uint TxHoldCount_;
+	u8 ForceProtocolVersion_;
+	struct mst_cfg_id MstConfigId_;
+};
+
+struct mstp_br_vars {
+	struct _bridge_id br_id_;
+	struct mstp_vector bridgePrio_;
+	struct mstp_prio rootPrio_;
+	struct mstp_times bridgeTimes_;
+	struct mstp_times rootTimes_;
+	struct _port_id rootPortId_;
+	u32 TC_sec_;
+	u32 TC_cnt_;
+	u32 TC_set_;
+};
+
+struct ksz_stp_timers {
+	/*
+	 * Edge Delay timer is time remaining before port is identified as an
+	 * operEdgePort.
+	 */
+	uint edgeDelayWhile_;
+	/*
+	 * Hello timer is used to ensure that at least one BPDU is sent by a
+	 * Desginated Port in each HelloTime period.
+	 */
+	uint helloWhen_;
+	/*
+	 * Migration Delay timer is used to allow time for another RSTP Bridge
+	 * to synchronize its migration state before the receipt of a BPDU can
+	 * cause this Port to change the BPDU type it sends.
+	 */
+	uint mdelayWhile_;
+};
+
+struct ksz_mstp_timers {
+	/*
+	 * Forward Delay timer is used to delay Port State transitions until
+	 * other Bridges have received spanning tree information.
+	 */
+	uint fdWhile_;
+	/*
+	 * Recent Backup timer is maintained at twice HelloTime while the Port
+	 * is a Backup Port.
+	 */
+	uint rbWhile_;
+	/*
+	 * Received Info timer is time remaining before spanning tree
+	 * inforamtion is aged out.
+	 */
+	uint rcvdInfoWhile_;
+	/*
+	 * Recent Root timer.
+	 */
+	uint rrWhile_;
+	/*
+	 * Topology Change timer is used to send TCN Messages.
+	 */
+	uint tcWhile_;
+	/*
+	 * Topology Change timer for MRP.
+	 */
+	uint tcDetected_;
+};
+
+struct stp_admin_vars {
+	u32 AdminEdgePort_:1;
+	u32 AutoEdgePort_:1;
+	u32 AutoIsolate_:1;
+	u32 enableBPDUrx_:1;
+	u32 enableBPDUtx_:1;
+	u32 operPointToPointMAC_:1;
+
+	u8 adminPointToPointMAC_;
+	uint adminPortPathCost_;
+};
+
+struct stp_vars {
+	u32 infoInternal_;
+	u32 isolate_:1;
+	u32 master_:1;
+	u32 mastered_:1;
+	u32 mcheck_:1;
+	u32 newInfo_:1;
+	u32 newInfoMsti_:1;
+	u32 operEdge_:1;
+	u32 portEnabled_:1;
+	u32 rcvdBPDU_:1;
+	u32 rcvdInternal_;
+	u32 rcvdRSTP_:1;
+	u32 rcvdSTP_:1;
+	u32 rcvdTcAck_:1;
+	u32 rcvdTcn_:1;
+	u32 restrictedRole_;
+	u32 restrictedTcn_;
+	u32 sendRSTP_:1;
+	u32 tcAck_:1;
+	u32 tick_:1;
+
+	uint ageingTime_;
+
+	uint txCount_;
+	uint PortPathCost_;
+};
+
+struct mstp_vars {
+	u32 agree_:1;
+	u32 agreed_:1;
+	u32 disputed_:1;
+	u32 fdbFlush_:1;
+	u32 forward_:1;
+	u32 forwarding_:1;
+	u32 learn_:1;
+	u32 learning_:1;
+	u32 proposed_:1;
+	u32 proposing_:1;
+	u32 rcvdMsg_:1;
+	u32 rcvdTc_:1;
+	u32 reRoot_:1;
+	u32 reselect_:1;
+	u32 selected_:1;
+	u32 sync_:1;
+	u32 synced_:1;
+	u32 tcProp_:1;
+	u32 updtInfo_:1;
+
+	u8 infoIs_;
+	u8 rcvdInfo_;
+	u8 role_;
+	u8 selectedRole_;
+
+	struct _port_id portId_;
+	uint PortPathCost_;
+
+	struct mstp_prio desgPrio_;
+	struct mstp_prio msgPrio_;
+	struct mstp_prio portPrio_;
+	struct mstp_times desgTimes_;
+	struct mstp_times msgTimes_;
+	struct mstp_times portTimes_;
+};
+
+#define NUM_OF_PORT_TIMERS		3
+
+struct stp_port_vars {
+	uint timers[NUM_OF_PORT_TIMERS];
+	struct stp_admin_vars admin_var;
+	struct stp_vars stp_var;
+
+	u8 bpduVersion_;
+	u8 bpduType_;
+	struct mst_cfg_id bpduMCID_;
+};
+
+#define NUM_OF_MSTI_TIMERS		(6 + 1)
+
+struct mstp_port_vars {
+	uint timers[NUM_OF_MSTI_TIMERS];
+	struct mstp_vars stp_var;
+
+	u8 bpduFlags_;
+	u8 bpduRole_;
+	struct mstp_prio bpduPrio_;
+	struct mstp_times bpduTimes_;
+};
+
+struct ksz_stp_bridge;
+
+#define NUM_OF_PORT_STATE_MACHINES	9
+
+struct ksz_stp_dbg_times {
+	struct mstp_prio downPriority;
+	struct mstp_prio lastPriority;
+	unsigned long alt_jiffies;
+	unsigned long learn_jiffies;
+	unsigned long block_jiffies;
+	u8 role_;
+};
+
+struct ksz_stp_port {
+	struct stp_port_vars vars;
+	struct mstp_port_vars mvars[NUM_OF_MSTI];
+	int MSTI;
+
+	u8 states[NUM_OF_PORT_STATE_MACHINES][NUM_OF_MSTI];
+	u8 state_index;
+	u8 port_index;
+	int off;
+	int link;
+	int duplex;
+	int speed;
+	struct sk_buff_head rxq;
+
+	struct bpdu_v3 rx_bpdu0;
+	struct bpdu_v3 tx_bpdu0;
+	int dbg_rx;
+	int dbg_tx;
+
+	struct ksz_stp_dbg_times dbg_times[NUM_OF_MSTI];
+
+	struct ksz_stp_bridge *br;
+};
+
+#define NUM_OF_BRIDGE_STATE_MACHINES	1
+
+struct ksz_stp_bridge {
+	struct stp_br_vars vars;
+	struct mstp_br_vars mvars[NUM_OF_MSTI];
+	u16 vid2mstid[NUM_OF_VID + 2];
+	int MSTI;
+	u32 MSTI_enabled;
+	u8 MSTI_cnt;
+	u8 MSTI_index;
+
+	u8 bridgeEnabled;
+
+	struct ksz_stp_port ports[SWITCH_PORT_NUM];
+	u8 port_cnt;
+	u8 skip_tx;
+	u16 port_rx;
+
+	u8 states[NUM_OF_BRIDGE_STATE_MACHINES][NUM_OF_MSTI];
+	u8 state_index;
+
+	void *parent;
+	struct mutex lock;
+
+	u32 hack_4_1:1;
+	u32 hack_4_2:1;
+	u32 hack_5_2:1;
+};
+
+
+struct ksz_stp_info;
+
+struct stp_ops {
+	int (*change_addr)(struct ksz_stp_info *stp, u8 *addr);
+	void (*link_change)(struct ksz_stp_info *stp, int update);
+
+	int (*dev_req)(struct ksz_stp_info *stp, char *arg, void *info);
+
+	int (*get_tcDetected)(struct ksz_stp_info *info, int p);
+};
+
+struct ksz_stp_info {
+	struct ksz_stp_bridge br;
+
+	void *sw_dev;
+	struct net_device *dev;
+	struct ksz_timer_info port_timer_info;
+	uint timer_tick;
+	struct work_struct state_machine;
+	bool machine_running;
+	struct work_struct rx_proc;
+
+	u8 tx_frame[sizeof(struct bpdu_v3) + 12 + sizeof(struct llc) + 9];
+	struct bpdu *bpdu;
+	int len;
+
+	struct sw_dev_info *dev_info;
+	uint notifications;
+
+	const struct stp_ops *ops;
+};
+
+#endif
diff -Napur linux-linux4microchip-2022.10-orig/drivers/net/ethernet/micrel/ksz_ptp_9897.c linux-linux4microchip-2022.10/drivers/net/ethernet/micrel/ksz_ptp_9897.c
--- linux-linux4microchip-2022.10-orig/drivers/net/ethernet/micrel/ksz_ptp_9897.c	1969-12-31 16:00:00.000000000 -0800
+++ linux-linux4microchip-2022.10/drivers/net/ethernet/micrel/ksz_ptp_9897.c	2023-11-16 18:54:51.000000000 -0800
@@ -0,0 +1,8420 @@
+/**
+ * Microchip PTP common code
+ *
+ * Copyright (c) 2015-2023 Microchip Technology Inc.
+ *	Tristram Ha <Tristram.Ha@microchip.com>
+ *
+ * Copyright (c) 2009-2015 Micrel, Inc.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ */
+
+
+#define CURRENT_UTC_OFFSET  37 /* 1 Jan 2017 */
+
+
+#if 0
+#define NO_PPS_DETECT
+#endif
+#if 0
+#define DBG_MSG_DROP
+#endif
+#if 0
+#define TEST_SW_EXEC
+#endif
+
+#if 1
+#define USE_2_STEP_WORKAROUND
+#endif
+
+static int mhz_gpo;
+static int pps_gpo = DEFAULT_PPS_GPO + 1;
+
+
+#define FMT_NSEC_SIZE			12
+
+static char *format_nsec(char *str, u32 nsec)
+{
+	u32 nsec0;
+	u32 nsec1;
+	u32 nsec2;
+	char str0[4];
+
+	nsec0 = nsec % 1000;
+	nsec1 = (nsec / 1000) % 1000;
+	nsec2 = (nsec / 1000000) % 1000;
+	sprintf(str0, "%03u", nsec0);
+	if (nsec2)
+		sprintf(str, "%3u.%03u.%s", nsec2, nsec1, str0);
+	else if (nsec1)
+		sprintf(str, "    %3u.%s", nsec1, str0);
+	else
+		sprintf(str, "        %3u", nsec0);
+	return str;
+}  /* format_nsec */
+
+struct pseudo_iphdr {
+	__u8 ttl;
+	__u8 protocol;
+	__be16 tot_len;
+	__be32 saddr;
+	__be32 daddr;
+};
+
+struct pseudo_ip6hdr {
+	__be16 payload_len;
+	__u8 hop_limit;
+	__u8 nexthdr;
+	struct in6_addr saddr;
+	struct in6_addr daddr;
+};
+
+static u32 timestamp_val(u32 timestamp, u8 *sec)
+{
+	*sec = timestamp >> 30;
+	timestamp <<= 2;
+	timestamp >>= 2;
+	return timestamp;
+}  /* timestamp_val */
+
+static void calc_diff(struct ksz_ptp_time *prev, struct ksz_ptp_time *cur,
+	struct ksz_ptp_time *result)
+{
+	struct ksz_ptp_time diff;
+	int prev_nsec = prev->nsec;
+	int cur_nsec = cur->nsec;
+
+	if (prev->sec < 0)
+		prev_nsec = -prev_nsec;
+	if (cur->sec < 0)
+		cur_nsec = -cur_nsec;
+	diff.sec = cur->sec - prev->sec;
+	diff.nsec = cur_nsec - prev_nsec;
+	if (diff.nsec >= NANOSEC_IN_SEC) {
+		diff.nsec -= NANOSEC_IN_SEC;
+		diff.sec++;
+	} else if (diff.nsec <= -NANOSEC_IN_SEC) {
+		diff.nsec += NANOSEC_IN_SEC;
+		diff.sec--;
+	}
+	if (diff.sec > 0 && diff.nsec < 0) {
+		diff.nsec += NANOSEC_IN_SEC;
+		diff.sec--;
+	} else if (diff.sec < 0 && diff.nsec > 0) {
+		diff.nsec -= NANOSEC_IN_SEC;
+		diff.sec++;
+	}
+	if (diff.nsec < 0 && diff.sec < 0)
+		diff.nsec = -diff.nsec;
+	result->sec = diff.sec;
+	result->nsec = diff.nsec;
+}  /* calc_diff */
+
+static void calc_udiff(struct ptp_utime *prev, struct ptp_utime *cur,
+	struct ksz_ptp_time *result)
+{
+	struct ksz_ptp_time t1;
+	struct ksz_ptp_time t2;
+
+	if (prev->sec > (1UL << 31) || cur->sec > (1UL << 31)) {
+		s64 t3;
+		s64 t4;
+		s64 diff;
+		s32 rem;
+
+		t3 = (u64) prev->sec * NANOSEC_IN_SEC + prev->nsec;
+		t4 = (u64) cur->sec * NANOSEC_IN_SEC + cur->nsec;
+		diff = t4 - t3;
+		t3 = div_s64_s32_rem(diff, NANOSEC_IN_SEC, &rem);
+		result->sec = (s32) t3;
+		result->nsec = rem;
+		return;
+	}
+	t1.sec = prev->sec;
+	t1.nsec = prev->nsec;
+	t2.sec = cur->sec;
+	t2.nsec = cur->nsec;
+	calc_diff(&t1, &t2, result);
+}  /* calc_udiff */
+
+static u32 ptp_unit_index(struct ptp_info *ptp, int shift, u8 unit)
+{
+	u32 index;
+#ifndef USE_OLD_PTP_UNIT_INDEX
+	struct ksz_sw *sw = ptp->parent;
+
+	index = sw->cached.ptp_unit_index;
+	index &= ~(PTP_UNIT_M << shift);
+	index |= (u32) unit << shift;
+	sw->cached.ptp_unit_index = index;
+#else
+	index = unit;
+#endif
+	return index;
+}  /* ptp_unit_index */
+
+static void ptp_write_index(struct ptp_info *ptp, int shift, u8 unit)
+{
+	struct ksz_sw *sw = ptp->parent;
+	u32 index = ptp_unit_index(ptp, shift, unit);
+
+	sw->reg->w32(sw, REG_PTP_UNIT_INDEX__4, index);
+}  /* ptp_write_index */
+
+static void add_large_nsec(u32 *sec, u32 *nsec, u64 delay)
+{
+	u64 ns = *nsec;
+	u32 rem;
+
+	ns += delay;
+	ns = div_u64_u32_rem(ns, NANOSEC_IN_SEC, &rem);
+	*sec += (u32)ns;
+	*nsec = rem;
+}  /* add_large_nsec */
+
+static void add_nsec(struct ptp_utime *t, u32 nsec)
+{
+	t->nsec += nsec;
+	if (t->nsec >= NANOSEC_IN_SEC) {
+		t->nsec -= NANOSEC_IN_SEC;
+		t->sec++;
+	}
+}  /* add_nsec */
+
+static void sub_nsec(struct ptp_utime *t, u32 nsec)
+{
+	if (t->nsec < nsec) {
+		t->nsec += NANOSEC_IN_SEC;
+		t->sec--;
+	}
+	t->nsec -= nsec;
+}  /* sub_nsec */
+
+static void update_ts(struct ptp_ts *ts, u32 cur_sec)
+{
+	int sec;
+	u8 sec_chk;
+
+	ts->t.nsec = timestamp_val(ts->timestamp, &sec_chk);
+	if (ts->timestamp)
+		sec = (cur_sec - sec_chk) & 3;
+	else
+		sec = 0;
+	if (sec >= 2)
+		sec -= 4;
+	ts->t.sec = cur_sec - sec;
+}  /* update_ts */
+
+#define INIT_NSEC			40
+/* Need at least 24 ns for level to go back to initial. */
+#define MIN_CYCLE_NSEC			40
+#define MIN_EDGE_NSEC			8
+#define MIN_GAP_NSEC			40
+#define PULSE_NSEC			8
+
+/* 5 ms */
+#define MIN_TIME_BEFORE_TRIGGER  5000000
+
+static int check_gap(struct ptp_info *ptp, struct ptp_output *a,
+		     struct ptp_output *b)
+{
+	int gap = MIN_GAP_NSEC;
+
+	/* Driver is doing cascade mode. */
+	if (ptp->cascade_sw_each)
+		return 0;
+
+	/* Only check signals with count. */
+	if (a->event < TRIG_NEG_PERIOD || b->event < TRIG_NEG_PERIOD)
+		return gap;
+	if (a->event == b->event &&
+	    (a->event == TRIG_POS_PERIOD ||
+	     b->event == TRIG_REG_OUTPUT)) {
+		/* 2 combinations. */
+		gap = 0;
+	} else if (a->event == TRIG_NEG_PERIOD &&
+		   b->event == TRIG_NEG_PERIOD) {
+		/* 1 combination. */
+		gap = 8;
+	} else {
+		/* 6 combinations. */
+		gap = 0;
+	}
+	return gap;
+}
+
+static int check_cascade(struct ptp_info *ptp, int first, int total,
+	u16 *repeat, u32 sec, u32 nsec)
+{
+	struct ptp_output *cur;
+	struct ptp_output *next;
+	struct ptp_output *prev;
+	s64 diff;
+	int i;
+	int tso;
+	int min_cnt;
+	int min_gap;
+	int cnt;
+	int skip_repeat = 0;
+
+	tso = first;
+	cur = &ptp->outputs[tso];
+	next = &ptp->outputs[first + total];
+	next->start = cur->start;
+	next->event = cur->event;
+	add_large_nsec(&next->start.sec, &next->start.nsec, cur->iterate);
+	for (i = 0; i < total; i++, tso++) {
+		cur = &ptp->outputs[tso];
+		cur->stop = cur->start;
+		add_large_nsec(&cur->stop.sec, &cur->stop.nsec, cur->len);
+		next = &ptp->outputs[tso + 1];
+		min_gap = check_gap(ptp, cur, next);
+		add_nsec(&cur->stop, min_gap);
+		calc_udiff(&cur->stop, &next->start, &cur->gap);
+		if ((cur->gap.sec < 0 || (!cur->gap.sec && cur->gap.nsec < 0))
+				&& (i < total - 1 || 1 != *repeat)) {
+			printk(KERN_DEBUG "gap too small: %d=%d.%d"NL, i,
+				cur->gap.sec, cur->gap.nsec);
+			dbg_msg("gap too small: %d=%d.%d"NL, i,
+				cur->gap.sec, cur->gap.nsec);
+
+			/* Allow operation to run for testing verification. */
+			if (cur->event < TRIG_NEG_PERIOD ||
+			    next->event < TRIG_NEG_PERIOD)
+				return 1;
+		}
+	}
+	if (1 == *repeat)
+		return 0;
+
+	min_cnt = *repeat;
+	tso = first + 1;
+	for (i = 1; i < total; i++, tso++) {
+		cur = &ptp->outputs[tso];
+		prev = &ptp->outputs[tso - 1];
+		if (cur->iterate < prev->iterate) {
+			diff = prev->iterate - cur->iterate;
+			cnt = (int)(div_s64_s64(prev->gap.nsec, diff) + 1);
+		} else if (cur->iterate > prev->iterate) {
+			diff = cur->iterate - prev->iterate;
+			cnt = (int)(div_s64_s64(cur->gap.nsec, diff) + 1);
+		} else
+			cnt = *repeat;
+		if (min_cnt > cnt)
+			min_cnt = cnt;
+	}
+	if (*repeat > min_cnt)
+		*repeat = min_cnt;
+	min_cnt = *repeat;
+	prev = &ptp->outputs[first + tso];
+	for (cnt = 0; cnt < min_cnt; cnt++) {
+		if (skip_repeat && cnt == min_cnt - 1)
+			skip_repeat = 0;
+		else if (!skip_repeat && cnt == 3 && min_cnt >= 5)
+			skip_repeat = 1;
+		tso = first;
+		for (i = 0; i < total; i++, tso++) {
+			cur = &ptp->outputs[tso];
+			next = &ptp->outputs[tso + 1];
+			if (cur->stop.sec > next->start.sec ||
+			    (cur->stop.sec == next->start.sec &&
+			     cur->stop.nsec > next->start.nsec))
+				skip_repeat = 0;
+			if (!skip_repeat) {
+				dbg_msg("%d: %u:%9u %llu %d:%9d %llu: %u:%9u"NL,
+					i, cur->start.sec, cur->start.nsec,
+					cur->len, cur->gap.sec, cur->gap.nsec,
+					cur->iterate, cur->stop.sec,
+					cur->stop.nsec);
+				if (cur->stop.sec > next->start.sec ||
+				    (cur->stop.sec == next->start.sec &&
+				     cur->stop.nsec > next->start.nsec)) {
+					dbg_msg(" >> %d=%u:%9u %u:%9u"NL, i,
+						cur->stop.sec, cur->stop.nsec,
+						next->start.sec,
+						next->start.nsec);
+				}
+			}
+			add_large_nsec(&cur->start.sec, &cur->start.nsec,
+				cur->iterate);
+			cur->stop = cur->start;
+			add_large_nsec(&cur->stop.sec, &cur->stop.nsec,
+				cur->len);
+			min_gap = check_gap(ptp, cur, next);
+			add_nsec(&cur->stop, min_gap);
+			if (!i)
+				prev->start = cur->start;
+		}
+		if (!skip_repeat) {
+			dbg_msg("%u:%9u"NL, prev->start.sec, prev->start.nsec);
+		} else if (skip_repeat == 1) {
+			skip_repeat++;
+			dbg_msg("..."NL);
+		}
+	}
+	return 0;
+}  /* check_cascade */
+
+static void update_cascade_time(struct ptp_info *ptp, int first, int total,
+	u32 sec, u32 nsec)
+{
+	struct ptp_output *cur;
+	int i, tso;
+
+	tso = first;
+	cur = &ptp->outputs[tso];
+
+	for (i = 0; i < total; i++, tso++) {
+		cur = &ptp->outputs[tso];
+
+		/* Trigger time is relative instead of absolute. */
+		if (cur->trig.sec < sec) {
+			cur->trig.sec += sec;
+			add_nsec(&cur->trig, nsec);
+			cur->intr.sec += sec;
+			add_nsec(&cur->intr, nsec);
+		}
+
+		/* Save the trigger time and interrupt time. */
+		cur->start = cur->trig;
+		cur->stop = cur->intr;
+	}
+}  /* update_cascade_time */
+
+#define MAX_DRIFT_CORR			6250000
+#define LOW_DRIFT_CORR			2499981
+#define MAX_U32_S			32
+#define MAX_DIVIDER_S			31
+
+static u32 drift_in_sec(u32 abs_offset, u64 interval64)
+{
+	u64 drift64;
+
+	drift64 = abs_offset;
+	drift64 *= NANOSEC_IN_SEC;
+	drift64 = div_u64_u64(drift64, interval64);
+	return (u32) drift64;
+}
+
+static u32 clk_adjust_val(int diff, u32 interval)
+{
+	u32 adjust;
+	u64 adjust64;
+
+	if (0 == diff)
+		return 0;
+	if (diff < 0)
+		adjust = -diff;
+	else
+		adjust = diff;
+
+	/* 2^32 * adjust * 1000000000 / interval / 25000000 */
+	if (interval != NANOSEC_IN_SEC)
+		adjust = drift_in_sec(adjust, interval);
+
+	if (adjust >= MAX_DRIFT_CORR)
+		adjust = 0x3fffffff;
+	else {
+		adjust64 = 1LL << 32;
+		adjust64 *= adjust;
+		adjust64 = div_u64_u32(adjust64, 25000000);
+		adjust = (u32) adjust64;
+		if (adjust >= 0x40000000)
+			adjust = 0x3fffffff;
+	}
+	if (diff < 0)
+		adjust |= PTP_RATE_DIR;
+	return adjust;
+}  /* clk_adjust_val */
+
+static void ptp_tso_off(struct ptp_info *ptp, u8 tso, u16 tso_bit)
+{
+	ptp->reg->tx_off(ptp, tso);
+	ptp->tso_intr &= ~tso_bit;
+	ptp->tso_used &= ~tso_bit;
+	if (ptp->tso_sys & tso_bit) {
+		printk(KERN_INFO "tso %d off!"NL, tso);
+		ptp->tso_sys &= ~tso_bit;
+	}
+	tso_bit <<= 16;
+	ptp->tso_intr &= ~tso_bit;
+	ptp->tso_dev[tso] = NULL;
+	ptp->tso_cnt[tso] = 0;
+	ptp->tso_chk[tso] = 0;
+}  /* ptp_tso_off */
+
+static inline void ptp_tx_reset(struct ptp_info *ptp, u8 tso, u32 *ctrl_ptr)
+{
+	u32 ctrl;
+	struct ksz_sw *sw = ptp->parent;
+
+	if (!ctrl_ptr) {
+		ctrl_ptr = &ctrl;
+		ptp_write_index(ptp, PTP_TOU_INDEX_S, tso);
+		*ctrl_ptr = sw->reg->r32(sw, REG_PTP_CTRL_STAT__4);
+	}
+	*ctrl_ptr &= ~TS_RESET;
+	*ctrl_ptr |= TRIG_RESET;
+	sw->reg->w32(sw, REG_PTP_CTRL_STAT__4, *ctrl_ptr);
+	*ctrl_ptr &= ~TRIG_RESET;
+	sw->reg->w32(sw, REG_PTP_CTRL_STAT__4, *ctrl_ptr);
+}  /* ptp_tx_reset */
+
+static inline void ptp_gpo_reset(struct ptp_info *ptp, int gpo, u8 tso,
+	u32 *ctrl_ptr)
+{
+	ptp_tx_reset(ptp, tso, ctrl_ptr);
+	ptp->cascade_gpo[gpo].tso &= ~(1 << tso);
+}  /* ptp_gpo_reset */
+
+/* -------------------------------------------------------------------------- */
+
+static void ptp_acquire(struct ptp_info *ptp)
+{
+	struct ksz_sw *sw = ptp->parent;
+
+	sw->ops->acquire(sw);
+}  /* ptp_acquire */
+
+static void ptp_release(struct ptp_info *ptp)
+{
+	struct ksz_sw *sw = ptp->parent;
+
+	sw->ops->release(sw);
+}  /* ptp_release */
+
+static void get_ptp_time(struct ptp_info *ptp, struct ptp_utime *t)
+{
+	u16 data;
+	u16 subnsec;
+	struct ksz_sw *sw = ptp->parent;
+
+	data = sw->cached.ptp_clk_ctrl;
+	data |= PTP_READ_TIME;
+	sw->reg->w16(sw, REG_PTP_CLK_CTRL, data);
+	do {
+		u8 buf[12];
+		u16 *w_ptr;
+		u32 *d_ptr;
+
+		sw->reg->r(sw, REG_PTP_RTC_SUB_NANOSEC__2, buf, 10);
+		w_ptr = (u16 *) buf;
+		subnsec = be16_to_cpu(*w_ptr);
+		++w_ptr;
+		d_ptr = (u32 *) w_ptr;
+		t->nsec = be32_to_cpu(*d_ptr);
+		++d_ptr;
+		t->sec = be32_to_cpu(*d_ptr);
+	} while (0);
+	subnsec &= PTP_RTC_SUB_NANOSEC_M;
+	add_nsec(t, subnsec * 8);
+}  /* get_ptp_time */
+
+static void set_ptp_time(struct ptp_info *ptp, struct ptp_utime *t)
+{
+	u16 data;
+	struct ksz_sw *sw = ptp->parent;
+
+	data = sw->cached.ptp_clk_ctrl;
+	sw->reg->w16(sw, REG_PTP_RTC_SUB_NANOSEC__2, 0);
+	sw->reg->w32(sw, REG_PTP_RTC_NANOSEC, t->nsec);
+	sw->reg->w32(sw, REG_PTP_RTC_SEC, t->sec);
+	data |= PTP_LOAD_TIME;
+	sw->reg->w16(sw, REG_PTP_CLK_CTRL, data);
+}  /* set_ptp_time */
+
+static void adjust_ptp_time(struct ptp_info *ptp, int add, u32 sec, u32 nsec,
+	int adj_hack)
+{
+	u16 ctrl;
+	u16 adj = 0;
+	u32 val = nsec;
+	struct ksz_sw *sw = ptp->parent;
+
+	ctrl = sw->cached.ptp_clk_ctrl;
+	if (add)
+		ctrl |= PTP_STEP_DIR;
+	else
+		ctrl &= ~PTP_STEP_DIR;
+	sw->cached.ptp_clk_ctrl = ctrl;
+	if (adj_hack) {
+		adj = ctrl;
+		ctrl &= ~PTP_CLK_ADJ_ENABLE;
+	}
+	ctrl |= PTP_STEP_ADJ;
+	sw->reg->w32(sw, REG_PTP_RTC_SEC, sec);
+	do {
+		if (nsec > NANOSEC_IN_SEC - 1)
+			nsec = NANOSEC_IN_SEC - 1;
+		sw->reg->w32(sw, REG_PTP_RTC_NANOSEC, nsec);
+		sw->reg->w16(sw, REG_PTP_CLK_CTRL, ctrl);
+		val -= nsec;
+		nsec = val;
+	} while (val);
+	if (adj_hack && (adj & PTP_CLK_ADJ_ENABLE))
+		sw->reg->w16(sw, REG_PTP_CLK_CTRL, adj);
+#ifdef NO_PPS_DETECT
+	if (add && (sec || nsec >= 1000))
+		ptp->clk_add = 1;
+#endif
+}  /* adjust_ptp_time */
+
+static void adjust_sync_time(struct ptp_info *ptp, int diff, u32 interval,
+	u32 duration)
+{
+	u32 adjust;
+	struct ksz_sw *sw = ptp->parent;
+
+	adjust = clk_adjust_val(diff, interval);
+	adjust |= PTP_TMP_RATE_ENABLE;
+	sw->reg->w32(sw, REG_PTP_RATE_DURATION, duration);
+	sw->reg->w32(sw, REG_PTP_SUBNANOSEC_RATE, adjust);
+}  /* adjust_sync_time */
+
+static void ptp_rx_reset(struct ptp_info *ptp, u8 tsi, u32 *ctrl_ptr)
+{
+	u32 ctrl;
+	struct ksz_sw *sw = ptp->parent;
+
+	if (!ctrl_ptr) {
+		ctrl_ptr = &ctrl;
+		ptp_write_index(ptp, PTP_TSI_INDEX_S, tsi);
+		*ctrl_ptr = sw->reg->r32(sw, REG_PTP_CTRL_STAT__4);
+	}
+	*ctrl_ptr &= ~TRIG_RESET;
+	*ctrl_ptr |= TS_RESET;
+	sw->reg->w32(sw, REG_PTP_CTRL_STAT__4, *ctrl_ptr);
+	*ctrl_ptr &= ~TS_RESET;
+	sw->reg->w32(sw, REG_PTP_CTRL_STAT__4, *ctrl_ptr);
+}  /* ptp_rx_reset */
+
+static void ptp_rx_off(struct ptp_info *ptp, u8 tsi)
+{
+	u32 ctrl;
+	u16 tsi_bit = (1 << tsi);
+	u32 ts_intr = 0;
+	struct ksz_sw *sw = ptp->parent;
+
+	ptp_write_index(ptp, PTP_TSI_INDEX_S, tsi);
+	ctrl = sw->reg->r32(sw, REG_PTP_CTRL_STAT__4);
+	ctrl &= ~(TRIG_RESET | TS_RESET);
+
+	/* Disable previous timestamp interrupt. */
+	if (ptp->ts_intr & tsi_bit) {
+		ptp->ts_intr &= ~tsi_bit;
+		ctrl &= ~TS_INT_ENABLE;
+		ts_intr = tsi_bit;
+	}
+
+	/* Disable previous timestamp detection. */
+	ctrl &= ~TS_ENABLE;
+	sw->reg->w32(sw, REG_PTP_CTRL_STAT__4, ctrl);
+
+	/*
+	 * Need to turn off cascade mode if it is used previously; otherwise,
+	 * event counter keeps increasing.
+	 */
+	if (ptp->cascade_rx & tsi_bit) {
+		ptp_rx_reset(ptp, tsi, &ctrl);
+		sw->reg->w32(sw, REG_TS_CTRL_STAT__4, 0);
+		ptp->cascade_rx &= ~tsi_bit;
+	}
+	if (ts_intr)
+		sw->reg->w32(sw, REG_PTP_INT_STATUS__4, ts_intr);
+}  /* ptp_rx_off */
+
+static inline void ptp_rx_intr(struct ptp_info *ptp, u16 tsi_bit, u32 *ctrl)
+{
+	ptp->ts_intr |= tsi_bit;
+	*ctrl |= TS_INT_ENABLE;
+}  /* ptp_rx_intr */
+
+static inline void ptp_rx_on(struct ptp_info *ptp, u8 tsi, int intr)
+{
+	u32 ctrl;
+	struct ksz_sw *sw = ptp->parent;
+
+	ctrl = sw->reg->r32(sw, REG_PTP_CTRL_STAT__4);
+	ctrl &= ~(TRIG_RESET | TS_RESET);
+
+	/* Enable timestamp interrupt. */
+	if (intr)
+		ptp_rx_intr(ptp, (1 << tsi), &ctrl);
+
+	ctrl |= TS_ENABLE;
+	sw->reg->w32(sw, REG_PTP_CTRL_STAT__4, ctrl);
+}  /* ptp_rx_on */
+
+static void ptp_rx_restart(struct ptp_info *ptp, u8 tsi)
+{
+	u32 ctrl;
+	struct ksz_sw *sw = ptp->parent;
+
+	ptp_write_index(ptp, PTP_TSI_INDEX_S, tsi);
+	ctrl = sw->reg->r32(sw, REG_PTP_CTRL_STAT__4);
+	ctrl &= ~(TRIG_RESET | TS_RESET);
+	ctrl &= ~TS_ENABLE;
+	sw->reg->w32(sw, REG_PTP_CTRL_STAT__4, ctrl);
+	ctrl |= TS_ENABLE;
+	sw->reg->w32(sw, REG_PTP_CTRL_STAT__4, ctrl);
+}  /* ptp_rx_restart */
+
+static u32 ts_event_gpi(u8 gpi, u8 event)
+{
+	u32 ctrl;
+	u32 data;
+
+	ctrl = event;
+	ctrl <<= TS_DETECT_S;
+	data = gpi & TS_GPI_M;
+	data <<= TS_GPI_S;
+	ctrl |= data;
+	return ctrl;
+}
+
+static u32 ts_cascade(int prev)
+{
+	u32 ctrl;
+
+	ctrl = prev & TS_CASCADE_UPS_M;
+	ctrl <<= TS_CASCADE_UPS_S;
+	ctrl |= TS_CASCADE_ENABLE;
+	return ctrl;
+}
+
+static void ptp_rx_event(struct ptp_info *ptp, u8 tsi, u8 gpi, u8 event,
+	int intr)
+{
+	u32 ctrl;
+	struct ksz_sw *sw = ptp->parent;
+
+	/* Config pattern. */
+	ptp_write_index(ptp, PTP_TSI_INDEX_S, tsi);
+	ctrl = ts_event_gpi(gpi, event);
+	sw->reg->w32(sw, REG_TS_CTRL_STAT__4, ctrl);
+
+	/* Enable timestamp detection. */
+	ptp_rx_on(ptp, tsi, intr);
+}  /* ptp_rx_event */
+
+static void ptp_rx_cascade_event(struct ptp_info *ptp, u8 first, u8 total,
+	u8 gpi, u8 event, int intr)
+{
+	int last;
+	int tsi;
+	u32 ctrl;
+	u32 tail;
+	int i;
+	int prev;
+	struct ksz_sw *sw = ptp->parent;
+
+	last = (first + total - 1) % MAX_TIMESTAMP_UNIT;
+	tsi = last;
+	tail = TS_CASCADE_TAIL;
+	for (i = 1; i < total; i++) {
+		ptp_write_index(ptp, PTP_TSI_INDEX_S, tsi);
+		prev = tsi - 1;
+		if (prev < 0)
+			prev = MAX_TIMESTAMP_UNIT - 1;
+		ctrl = ts_event_gpi(gpi, event);
+		ctrl |= ts_cascade(prev);
+		ctrl |= tail;
+		ptp->cascade_rx |= (1 << tsi);
+		sw->reg->w32(sw, REG_TS_CTRL_STAT__4, ctrl);
+
+		/* Enable timestamp interrupt. */
+		if (intr) {
+			ctrl = sw->reg->r32(sw, REG_PTP_CTRL_STAT__4);
+			ptp_rx_intr(ptp, (1 << tsi), &ctrl);
+			sw->reg->w32(sw, REG_PTP_CTRL_STAT__4, ctrl);
+		}
+		--tsi;
+		if (tsi < 0)
+			tsi = MAX_TIMESTAMP_UNIT - 1;
+		tail = 0;
+	}
+	ptp_write_index(ptp, PTP_TSI_INDEX_S, first);
+	ctrl = ts_event_gpi(gpi, event);
+	ctrl |= ts_cascade(last);
+	ptp->cascade_rx |= (1 << first);
+	sw->reg->w32(sw, REG_TS_CTRL_STAT__4, ctrl);
+
+	/* Enable timestamp detection. */
+	ptp_rx_on(ptp, first, intr);
+}  /* ptp_rx_cascade_event */
+
+static u32 ptp_get_event_cnt(struct ptp_info *ptp, u8 tsi, void *ptr)
+{
+	struct ksz_sw *sw = ptp->parent;
+
+	ptp_write_index(ptp, PTP_TSI_INDEX_S, tsi);
+	return sw->reg->r32(sw, REG_TS_CTRL_STAT__4);
+}  /* ptp_get_event_cnt */
+
+static void ptp_get_events(struct ptp_info *ptp, u32 reg_ns, size_t len,
+	void *buf, void *ptr)
+{
+	int i;
+	u32 *data = buf;
+	struct ksz_sw *sw = ptp->parent;
+
+	sw->reg->r(sw, reg_ns, buf, len);
+	for (i = 0; i < len / sizeof(u32); i++)
+		data[i] = be32_to_cpu(data[i]);
+}  /* ptp_get_events */
+
+static void ptp_read_event_func(struct ptp_info *ptp, u8 tsi, void *ptr,
+	u32 (*get_event_cnt)(struct ptp_info *ptp, u8 tsi, void *ptr),
+	void (*get_events)(struct ptp_info *ptp, u32 reg_ns, size_t len,
+	void *buf, void *ptr))
+{
+	u32 ctrl;
+	u16 tsi_bit = (1 << tsi);
+	u8 buf[96];
+	u32 *d_ptr;
+	u32 reg_ns;
+	struct ptp_utime t;
+	u32 sub;
+	int max_ts;
+	int num;
+	int i;
+	int edge;
+	struct ptp_event *event = &ptp->events[tsi];
+	int last = event->num;
+
+	ctrl = get_event_cnt(ptp, tsi, ptr);
+	num = (ctrl >> TS_EVENT_DETECT_S) & TS_EVENT_DETECT_M;
+	max_ts = (num <= event->max) ? num : event->max;
+	if (event->num >= max_ts)
+		return;
+	i = event->num;
+
+	reg_ns = REG_TS_EVENT_0_NANOSEC + TS_EVENT_SAMPLE * i;
+	get_events(ptp, reg_ns, 12 * (max_ts - event->num), buf, ptr);
+	d_ptr = (u32 *) buf;
+	for (; i < max_ts; i++) {
+		t.nsec = (*d_ptr);
+		++d_ptr;
+		t.sec = (*d_ptr);
+		++d_ptr;
+		sub = (*d_ptr) & TS_EVENT_SUB_NANOSEC_M;
+		++d_ptr;
+		edge = ((t.nsec >> TS_EVENT_EDGE_S) & TS_EVENT_EDGE_M);
+		t.nsec &= TS_EVENT_NANOSEC_M;
+		add_nsec(&t, sub * 8);
+#if 1
+/*
+ * THa  2011/10/06
+ * Unit sometimes detects rising edge when it is configured to detect falling
+ * edge only.  This happens in the case of hooking up the output pin to an
+ * input pin and using two units running opposite cycle in cascade mode.  The
+ * 8 ns switch pulse before the cycle is too short to detect properly,
+ * resulting in missing edges.
+ * When detecting events directly from the output pin, the minimum pulse time
+ * is 24 ns for proper detection without missing any edge.
+ */
+		if (event->event < 2 && edge != event->event)
+			edge = event->event;
+#endif
+		event->edge |= edge << i;
+		event->t[i] = t;
+	}
+	event->num = max_ts;
+
+	/* Indicate there is new event. */
+	if (event->num > last)
+		ptp->ts_status |= tsi_bit;
+}  /* ptp_read_event_func */
+
+static void ptp_read_event(struct ptp_info *ptp, u8 tsi)
+{
+	ptp_read_event_func(ptp, tsi, NULL, ptp_get_event_cnt,
+		ptp_get_events);
+}  /* ptp_read_event */
+
+static u32 trig_cascade(int prev)
+{
+	u32 ctrl;
+
+	ctrl = prev & TRIG_CASCADE_UPS_M;
+	ctrl <<= TRIG_CASCADE_UPS_S;
+	return ctrl;
+}
+
+static void ptp_tx_off(struct ptp_info *ptp, u8 tso)
+{
+	u32 ctrl;
+	u16 tso_bit = (1 << tso);
+	struct ksz_sw *sw = ptp->parent;
+
+	ptp_write_index(ptp, PTP_TOU_INDEX_S, tso);
+
+	/* Disable previous trigger out if not already completed. */
+	ctrl = sw->reg->r32(sw, REG_PTP_CTRL_STAT__4);
+	ctrl &= ~(TRIG_RESET | TS_RESET);
+	if (ctrl & TRIG_ENABLE) {
+		ctrl &= ~TRIG_ENABLE;
+		sw->reg->w32(sw, REG_PTP_CTRL_STAT__4, ctrl);
+	}
+
+	/*
+	 * Using cascade mode previously need to reset the trigger output so
+	 * that an errorneous output will not be generated during next
+	 * cascade mode setup.
+	 */
+	if (ptp->cascade_tx & tso_bit) {
+		ptp_gpo_reset(ptp, ptp->outputs[tso].gpo, tso, &ctrl);
+		ptp->cascade_tx &= ~tso_bit;
+	}
+	ctrl = sw->reg->r32(sw, REG_TRIG_CTRL__4);
+	if (ctrl & TRIG_CASCADE_ENABLE) {
+		ctrl &= ~TRIG_CASCADE_ENABLE;
+		ctrl &= ~TRIG_CASCADE_TAIL;
+		ctrl |= trig_cascade(TRIG_CASCADE_UPS_M);
+		sw->reg->w32(sw, REG_TRIG_CTRL__4, ctrl);
+	}
+}  /* ptp_tx_off */
+
+static void ptp_tx_on(struct ptp_info *ptp, u8 tso)
+{
+	u32 ctrl;
+	struct ksz_sw *sw = ptp->parent;
+
+	ctrl = sw->reg->r32(sw, REG_PTP_CTRL_STAT__4);
+	ctrl &= ~(TRIG_RESET | TS_RESET);
+
+	/* Do a reset on last TOU in previous cascade operation. */
+	if (ptp->cascade_tx & (1 << (tso + 16))) {
+		ptp->cascade_tx &= ~(1 << (tso + 16));
+		ctrl |= TRIG_RESET;
+		ctrl |= TRIG_ENABLE;
+	}
+
+	/* Need to disable and enable for interrupt mechanism to work. */
+	if (ctrl & TRIG_ENABLE) {
+		ctrl &= ~TRIG_ENABLE;
+		sw->reg->w32(sw, REG_PTP_CTRL_STAT__4, ctrl);
+		ctrl &= ~TRIG_RESET;
+	}
+	ctrl |= TRIG_ENABLE;
+	sw->reg->w32(sw, REG_PTP_CTRL_STAT__4, ctrl);
+}  /* ptp_tx_on */
+
+static void ptp_tx_trigger_time(struct ptp_info *ptp, u8 tso, u32 sec, u32 nsec)
+{
+	struct ksz_sw *sw = ptp->parent;
+
+	sw->reg->w32(sw, REG_TRIG_TARGET_SEC, sec);
+	sw->reg->w32(sw, REG_TRIG_TARGET_NANOSEC, nsec);
+}  /* ptp_tx_trigger_time */
+
+static void ptp_tx_start(struct ptp_info *ptp, u8 tso, u32 sec, u32 nsec)
+{
+	/* Config trigger time. */
+	ptp_tx_trigger_time(ptp, tso, sec, nsec);
+
+	/* Enable trigger. */
+	ptp_tx_on(ptp, tso);
+}  /* ptp_tx_start */
+
+static void ptp_tx_restart(struct ptp_info *ptp, u8 tso, u32 ctrl, u32 sec,
+			   u32 nsec)
+{
+	ptp_write_index(ptp, PTP_TOU_INDEX_S, tso);
+	if (ctrl) {
+		struct ksz_sw *sw = ptp->parent;
+
+		sw->reg->w32(sw, REG_TRIG_CTRL__4, ctrl);
+	}
+	ptp_tx_start(ptp, tso, sec, nsec);
+}  /* ptp_tx_restart */
+
+static u32 trig_event_gpo(u8 gpo, u8 event)
+{
+	u32 ctrl;
+	u32 data;
+
+	ctrl = event & TRIG_PATTERN_M;
+	ctrl <<= TRIG_PATTERN_S;
+	data = gpo & TRIG_GPO_M;
+	data <<= TRIG_GPO_S;
+	ctrl |= data;
+	return ctrl;
+}
+
+static u8 determine_last_level(u32 pattern, u16 cnt)
+{
+	u8 level = 0;
+
+	if (cnt) {
+		u16 reg;
+
+		reg = cnt - 1;
+		reg %= 16;
+		while (reg) {
+			pattern >>= 1;
+			reg--;
+		}
+		if (pattern & 1)
+			level = 1;
+	}
+	return level;
+}  /* determine_last_level */
+
+static void ptp_setup_tx(struct ptp_info *ptp, u8 tso, u8 gpo, u8 event,
+	u32 pulse, u32 cycle, u16 cnt, u16 real_cnt, u32 sec, u32 nsec,
+	u64 iterate, int intr, int now, int opt,
+	u32 *ctrl, u32 *hw_pulse, u32 *hw_cycle, u32 *pattern)
+{
+	struct ptp_output *cur = &ptp->outputs[tso];
+	u16 tso_bit = (1 << tso);
+
+	*hw_pulse = 0;
+	*hw_cycle = 0;
+	*pattern = 0;
+
+	/* Remember the event. */
+	cur->event = event;
+
+	/* Config pattern. */
+	*ctrl = trig_event_gpo(gpo, event);
+	if (intr)
+		*ctrl |= TRIG_NOTIFY;
+	if (now)
+		*ctrl |= TRIG_NOW;
+	if (opt)
+		*ctrl |= TRIG_EDGE;
+	*ctrl |= trig_cascade(TRIG_CASCADE_UPS_M);
+
+	/* Config pulse width. */
+	if (TRIG_REG_OUTPUT == event) {
+		*pattern = pulse & TRIG_BIT_PATTERN_M;
+		cur->level = determine_last_level(pulse, cnt);
+		pulse = 0;
+	} else if (event >= TRIG_NEG_PULSE) {
+		if (0 == pulse)
+			pulse = 1;
+		else if (pulse > TRIG_PULSE_WIDTH_M)
+			pulse = TRIG_PULSE_WIDTH_M;
+		*hw_pulse = pulse;
+	}
+
+	/* Config cycle width. */
+	if (cur->event >= TRIG_NEG_PERIOD) {
+		u32 data = cnt;
+		int min_cycle = pulse * PULSE_NSEC + MIN_CYCLE_NSEC;
+
+		if (cycle < min_cycle)
+			cycle = min_cycle;
+
+		/* Config trigger count. */
+		data <<= TRIG_CYCLE_CNT_S;
+		*pattern |= data;
+		*hw_cycle = cycle;
+	}
+	cur->hw_pulse = *hw_pulse;
+	cur->hw_cycle = *hw_cycle;
+	cur->hw_pattern = *pattern;
+	cur->hw_ctrl = *ctrl;
+
+	cur->len = 0;
+	if (cur->event >= TRIG_NEG_PERIOD) {
+		u64 len = cycle;
+
+		len *= real_cnt;
+		if (real_cnt)
+			cur->len += len;
+		else
+			cur->len += 0xFFFFFFF0;
+	} else if (cur->event >= TRIG_NEG_PULSE)
+		cur->len += pulse * PULSE_NSEC;
+	else
+		cur->len += MIN_EDGE_NSEC;
+
+	/* Save the count users want in case the operation is restarted. */
+	cur->cnt = 0;
+	if (cnt != real_cnt)
+		cur->cnt = real_cnt - 1;
+
+	cur->start.sec = sec;
+	cur->start.nsec = nsec;
+	cur->iterate = iterate;
+	cur->cycle = cycle;
+	cur->trig = cur->start;
+	cur->stop = cur->start;
+	add_large_nsec(&cur->stop.sec, &cur->stop.nsec, cur->len);
+	pulse *= 8;
+	pulse += 450000;
+	if (ptp->tso_cnt[tso]) {
+		cur->intr = cur->start;
+		add_nsec(&cur->intr, pulse);
+	} else {
+		cur->intr = cur->stop;
+		sub_nsec(&cur->intr, cycle);
+		add_nsec(&cur->intr, pulse);
+	}
+	cur->gpo = gpo;
+
+	/* Indicate have interrupt notification for internal use. */
+	if (intr && cnt)
+		ptp->tso_intr |= (tso_bit << 16);
+
+	switch (event) {
+	case TRIG_POS_EDGE:
+	case TRIG_NEG_PULSE:
+	case TRIG_NEG_PERIOD:
+		cur->level = 1;
+		break;
+	case TRIG_REG_OUTPUT:
+		break;
+	default:
+		cur->level = 0;
+		break;
+	}
+
+	if (ptp->cascade)
+		return;
+
+	/*
+	 * Need to reset after completion.  Otherwise, this output pattern
+	 * does not behave consistently in cascade mode.
+	 */
+	if (TRIG_NEG_EDGE == cur->event)
+		ptp->cascade_tx |= tso_bit;
+
+	ptp->cascade_gpo[gpo].total = 0;
+	if (cur->level)
+		ptp->cascade_gpo[gpo].tso |= tso_bit;
+	else
+		ptp->cascade_gpo[gpo].tso &= ~tso_bit;
+}  /* ptp_setup_tx */
+
+static void ptp_tx_event(struct ptp_info *ptp, u8 tso, u32 ctrl, u32 pulse,
+	u32 cycle, u32 pattern, u32 sec, u32 nsec)
+{
+	struct ksz_sw *sw = ptp->parent;
+
+	ptp_write_index(ptp, PTP_TOU_INDEX_S, tso);
+	sw->reg->w32(sw, REG_TRIG_CTRL__4, ctrl);
+	if (pulse)
+		sw->reg->w24(sw, REG_TRIG_PULSE_WIDTH__4 + 1, pulse);
+	if (cycle) {
+		sw->reg->w32(sw, REG_TRIG_CYCLE_WIDTH, cycle);
+		sw->reg->w32(sw, REG_TRIG_CYCLE_CNT, pattern);
+	}
+
+	/* Trigger time is set later in cascade mode. */
+	if (sec)
+		ptp_tx_start(ptp, tso, sec, nsec);
+}  /* ptp_tx_event */
+
+static void ptp_core_tx_event(struct ptp_info *ptp, u8 tso, u8 gpo, u8 event,
+	u32 pulse, u32 cycle, u16 cnt, u32 sec, u32 nsec, u64 iterate,
+	int intr, int now, int opt, int sw_oper)
+{
+	struct ptp_output *cur = &ptp->outputs[tso];
+	u16 hw_cnt;
+	u32 hw_cycle;
+	u32 hw_pulse;
+	u32 pattern;
+	u32 ctrl;
+
+	/* Assume no need for software to execute the operation. */
+	ptp->tso_cnt[tso] = 0;
+	hw_cnt = cnt;
+
+	/* Hardware cannot handle repeat time of more than 1 second. */
+	if ((sw_oper || cnt != 1) &&
+	    (event >= TRIG_NEG_PERIOD &&
+	     (sw_oper || cycle > NANOSEC_IN_SEC))) {
+
+		/* Save count for cascade mode repeat. */
+		if (event != TRIG_REG_OUTPUT) {
+			ptp->tso_cnt[tso] = cnt - 1;
+			hw_cnt = 1;
+		}
+		intr = 1;
+		if (ptp->cascade) {
+			u8 level = 0;
+
+			if (event == TRIG_REG_OUTPUT)
+				level = determine_last_level(pulse, cnt);
+			ptp->cascade_sw_each = 1;
+
+			/* Level will stay high for any TOU doing it. */
+			if (event == TRIG_NEG_PERIOD ||
+			    (event == TRIG_REG_OUTPUT && level))
+				ptp->cascade_sw_only = 1;
+		}
+	}
+
+	/* Hardware immediately keeps level high on new GPIO if not reset. */
+	if (cur->level && gpo != cur->gpo)
+		ptp_gpo_reset(ptp, cur->gpo, tso, NULL);
+
+	ptp_setup_tx(ptp, tso, gpo, event, pulse, cycle, hw_cnt, cnt,
+		sec, nsec, iterate, intr, now, opt,
+		&ctrl, &hw_pulse, &hw_cycle, &pattern);
+
+	/* Start TOU later in cascade mode. */
+	if (ptp->cascade)
+		sec = 0;
+	ptp->reg->tx_event(ptp, tso, ctrl, hw_pulse, hw_cycle, pattern, sec,
+		nsec);
+}  /* ptp_core_tx_event */
+
+static void ptp_pps_event(struct ptp_info *ptp, u8 gpo, u32 sec)
+{
+	u32 pattern;
+	u32 ctrl;
+	u32 nsec;
+	u32 pulse = (20000000 / 8);	/* 20 ms */
+	u32 cycle = 1000000000;
+	u16 cnt = 0;
+	u8 tso = ptp->pps_tso;
+	u8 event = TRIG_POS_PERIOD;
+	struct ksz_sw *sw = ptp->parent;
+
+	ptp_tx_off(ptp, tso);
+
+	/* Config pattern. */
+	ctrl = trig_event_gpo(gpo, event);
+	ctrl |= TRIG_NOTIFY;
+	ctrl |= TRIG_NOW;
+	ctrl |= trig_cascade(TRIG_CASCADE_UPS_M);
+	sw->reg->w32(sw, REG_TRIG_CTRL__4, ctrl);
+
+	/* Config pulse width. */
+	if (pulse > TRIG_PULSE_WIDTH_M)
+		pulse = TRIG_PULSE_WIDTH_M;
+	sw->reg->w24(sw, REG_TRIG_PULSE_WIDTH__4 + 1, pulse);
+
+	/* Config cycle width. */
+	sw->reg->w32(sw, REG_TRIG_CYCLE_WIDTH, cycle);
+
+	/* Config trigger count. */
+	pattern = cnt;
+	pattern <<= TRIG_CYCLE_CNT_S;
+	sw->reg->w32(sw, REG_TRIG_CYCLE_CNT, pattern);
+
+	/* Config trigger time. */
+	if (ptp->pps_offset >= 0)
+		nsec = ptp->pps_offset;
+	else {
+		nsec = NANOSEC_IN_SEC + ptp->pps_offset;
+		sec--;
+	}
+	ptp_tx_start(ptp, tso, sec, nsec);
+}  /* ptp_pps_event */
+
+static void cfg_10MHz(struct ptp_info *ptp, u8 tso, u8 gpo, u32 sec, u32 nsec)
+{
+	u32 pattern;
+	u32 ctrl;
+	u32 pulse = 6;
+	u32 cycle = 200;
+	u16 cnt = 0;
+	u8 event = TRIG_POS_PERIOD;
+	struct ksz_sw *sw = ptp->parent;
+
+	/* Config pattern. */
+	ctrl = trig_event_gpo(gpo, event);
+	ctrl |= TRIG_NOTIFY;
+	if (1 == tso)
+		ctrl |= TRIG_EDGE;
+	ctrl |= trig_cascade(TRIG_CASCADE_UPS_M);
+	sw->reg->w32(sw, REG_TRIG_CTRL__4, ctrl);
+
+	/* Config pulse width. */
+	if (pulse > TRIG_PULSE_WIDTH_M)
+		pulse = TRIG_PULSE_WIDTH_M;
+	sw->reg->w32(sw, REG_TRIG_PULSE_WIDTH__4 + 0, pulse);
+
+	/* Config cycle width. */
+	sw->reg->w32(sw, REG_TRIG_CYCLE_WIDTH, cycle);
+
+	/* Config trigger count. */
+	pattern = cnt;
+	pattern <<= TRIG_CYCLE_CNT_S;
+	sw->reg->w32(sw, REG_TRIG_CYCLE_CNT, pattern);
+
+	ptp_tx_trigger_time(ptp, tso, sec, nsec);
+}  /* cfg_10MHz */
+
+static void ptp_10MHz(struct ptp_info *ptp, u8 tso, u8 gpo, u32 sec)
+{
+	int i;
+	u32 nsec;
+
+	/* Config trigger time. */
+	if (ptp->pps_offset >= 0)
+		nsec = ptp->pps_offset;
+	else {
+		nsec = NANOSEC_IN_SEC + ptp->pps_offset;
+		sec--;
+	}
+	for (i = 0; i < 2; i++) {
+		ptp_tx_off(ptp, tso);
+
+		cfg_10MHz(ptp, tso, gpo, sec, nsec);
+
+		/* Enable trigger. */
+		ptp_tx_on(ptp, tso);
+
+		tso = 1;
+		nsec += 12 * 8;
+	}
+}  /* ptp_10MHz */
+
+static void ptp_tx_cascade_cycle(struct ptp_info *ptp, u8 tso, u32 nsec)
+{
+	struct ksz_sw *sw = ptp->parent;
+
+	sw->reg->w32(sw, REG_TRIG_ITERATE_TIME, nsec);
+}  /* ptp_tx_cascade_cycle */
+
+static void ptp_tx_cascade_on(struct ptp_info *ptp, u8 tso, u8 first, u8 last,
+	u16 repeat)
+{
+	u32 ctrl;
+	struct ksz_sw *sw = ptp->parent;
+
+	ctrl = sw->reg->r32(sw, REG_TRIG_CTRL__4);
+	ctrl |= TRIG_CASCADE_ENABLE;
+	ctrl &= ~trig_cascade(TRIG_CASCADE_UPS_M);
+	if (tso == first)
+		ctrl |= trig_cascade(last);
+	else
+		ctrl |= trig_cascade(tso - 1);
+	if (repeat && tso == last) {
+		ctrl |= TRIG_CASCADE_TAIL;
+		ctrl |= repeat - 1;
+	}
+	sw->reg->w32(sw, REG_TRIG_CTRL__4, ctrl);
+}  /* ptp_tx_cascade_on */
+
+static int ptp_tx_cascade(struct ptp_info *ptp, u8 first, u8 total,
+	u16 repeat, u32 sec, u32 nsec, int intr)
+{
+	int i;
+	u8 tso;
+	u8 last;
+	struct ptp_output *cur;
+
+	last = first + total - 1;
+	if (last >= MAX_TRIG_UNIT)
+		return 1;
+	tso = last;
+	for (i = 0; i < total; i++, tso--) {
+		cur = &ptp->outputs[tso];
+		ptp_write_index(ptp, PTP_TOU_INDEX_S, tso);
+
+		/* Need reset on last TOU in previous cascade operation. */
+		if (ptp->cascade_tx & (1 << (tso + 16))) {
+			struct ksz_sw *sw = ptp->parent;
+			u32 ctrl;
+
+			ctrl = sw->reg->r32(sw, REG_PTP_CTRL_STAT__4);
+			ptp_tx_reset(ptp, tso, &ctrl);
+			ptp->cascade_tx &= ~(1 << (tso + 16));
+		}
+		ptp_tx_trigger_time(ptp, tso, cur->trig.sec,
+			cur->trig.nsec);
+		ptp_tx_cascade_cycle(ptp, tso, (u32)cur->iterate);
+		ptp_tx_cascade_on(ptp, tso, first, last, repeat);
+		ptp->cascade_tx |= (1 << tso);
+	}
+
+	ptp_tx_on(ptp, first);
+	return 0;
+}  /* ptp_tx_cascade */
+
+/* -------------------------------------------------------------------------- */
+
+static void ptp_tx_intr_enable(struct ptp_info *ptp)
+{
+	uint n;
+	uint p;
+	struct ksz_sw *sw = ptp->parent;
+
+	for (n = 1; n <= ptp->ports; n++) {
+		p = get_phy_port(sw, n);
+		sw->ops->p_w16(sw, p, REG_PTP_PORT_TX_INT_STATUS__2, 0xffff);
+		sw->ops->p_w16(sw, p, REG_PTP_PORT_TX_INT_ENABLE__2,
+			       ptp->tx_intr);
+	}
+}  /* ptp_tx_intr_enable */
+
+/* -------------------------------------------------------------------------- */
+
+static void set_ptp_domain(struct ptp_info *ptp, u8 domain)
+{
+	u16 ctrl;
+	struct ksz_sw *sw = ptp->parent;
+
+	ctrl = sw->reg->r16(sw, REG_PTP_DOMAIN_VERSION) & ~PTP_DOMAIN_M;
+	ctrl |= domain;
+	sw->reg->w16(sw, REG_PTP_DOMAIN_VERSION, ctrl);
+}  /* set_ptp_domain */
+
+static void set_ptp_mode(struct ptp_info *ptp, u16 mode)
+{
+	struct ksz_sw *sw = ptp->parent;
+	u16 tx_intr = ptp->tx_intr;
+	bool need_sync_tx_ts = false;
+	bool need_resp_tx_ts = false;
+
+	/* Workaround 2-step clock issues. */
+	if (ptp->need_1_step_clock_oper)
+		mode |= PTP_1STEP;
+
+	/* May use 2-step P2P while using 1-step Sync. */
+	if ((mode & PTP_1STEP) && (mode & PTP_TC_P2P) &&
+	    !(ptp->mode & PTP_TC_P2P) &&
+	    ((ptp->tx_en & 1) && !(ptp->tx_en & 4))) {
+dbg_msg("2-step P2P %d %d\n", ptp->need_resp_tx_ts, ptp->need_2_step_resp_help);
+		/* P2P mode should be only set once when running PTP stack. */
+		ptp->need_resp_tx_ts = true;
+		ptp->need_2_step_resp_help = true;
+	}
+	if (!(mode & PTP_1STEP) || ptp->need_1_step_clock_oper) {
+		need_sync_tx_ts = true;
+		need_resp_tx_ts = true;
+	} else {
+		if (ptp->need_sync_tx_ts)
+			need_sync_tx_ts = true;
+		if (ptp->need_resp_tx_ts)
+			need_resp_tx_ts = true;
+	}
+
+	/* Only enable certain transmit timestamps for efficiency. */
+	if (need_sync_tx_ts)
+		ptp->tx_intr |= PTP_PORT_SYNC_INT;
+	else
+		ptp->tx_intr &= ~PTP_PORT_SYNC_INT;
+	if (need_resp_tx_ts)
+		ptp->tx_intr |= PTP_PORT_PDELAY_RESP_INT;
+	else
+		ptp->tx_intr &= ~PTP_PORT_PDELAY_RESP_INT;
+
+	if (tx_intr != ptp->tx_intr)
+		ptp_tx_intr_enable(ptp);
+
+	dbg_msg("mode: %x %x; %x"NL, ptp->mode, mode, ptp->tx_intr);
+	if (mode != ptp->mode) {
+		sw->reg->w16(sw, REG_PTP_MSG_CONF1, mode);
+		ptp->mode = mode;
+		mode = htons(mode);
+		sw->ops->chk_regs(sw, REG_PTP_MSG_CONF1, (u8 *)&mode, 2);
+	}
+}  /* set_ptp_mode */
+
+static void synchronize_clk(struct ptp_info *ptp)
+{
+	u32 sec;
+	int inc;
+
+	if (ptp->adjust_offset < 0 || ptp->adjust_sec < 0) {
+		ptp->adjust_offset = -ptp->adjust_offset;
+		ptp->adjust_sec = -ptp->adjust_sec;
+		inc = false;
+	} else
+		inc = true;
+	sec = (u32) ptp->adjust_sec;
+	ptp->reg->adjust_time(ptp, inc, sec, ptp->adjust_offset,
+		ptp->features & PTP_ADJ_HACK);
+	ptp->offset_changed = ptp->adjust_offset;
+	ptp->adjust_offset = 0;
+	ptp->adjust_sec = 0;
+}  /* synchronize_clk */
+
+static void set_ptp_adjust(struct ptp_info *ptp, u32 adjust)
+{
+	struct ksz_sw *sw = ptp->parent;
+
+	sw->reg->w32(sw, REG_PTP_SUBNANOSEC_RATE, adjust);
+}  /* set_ptp_adjust */
+
+static inline void unsyntonize_clk(struct ptp_info *ptp)
+{
+	u16 ctrl;
+	struct ksz_sw *sw = ptp->parent;
+
+	ctrl = sw->cached.ptp_clk_ctrl;
+	ctrl &= ~PTP_CLK_ADJ_ENABLE;
+	sw->cached.ptp_clk_ctrl = ctrl;
+	sw->reg->w16(sw, REG_PTP_CLK_CTRL, ctrl);
+}  /* unsyntonize_clk */
+
+static void syntonize_clk(struct ptp_info *ptp)
+{
+	u16 ctrl;
+	struct ksz_sw *sw = ptp->parent;
+
+	ctrl = sw->cached.ptp_clk_ctrl;
+	ctrl |= PTP_CLK_ADJ_ENABLE;
+	sw->cached.ptp_clk_ctrl = ctrl;
+	sw->reg->w16(sw, REG_PTP_CLK_CTRL, ctrl);
+}  /* syntonize_clk */
+
+static u16 get_ptp_delay(struct ptp_info *ptp, uint p, u32 reg)
+{
+	struct ksz_sw *sw = ptp->parent;
+	u16 data;
+
+	sw->ops->p_r16(sw, p, reg, &data);
+	return data;
+}  /* get_ptp_delay */
+
+static void set_ptp_delay(struct ptp_info *ptp, uint p, u32 reg, u16 nsec)
+{
+	struct ksz_sw *sw = ptp->parent;
+
+	sw->ops->p_w16(sw, p, reg, nsec);
+}  /* set_ptp_delay */
+
+static u16 get_ptp_ingress(struct ptp_info *ptp, uint p)
+{
+	return get_ptp_delay(ptp, p, REG_PTP_PORT_RX_DELAY__2);
+}
+
+static u16 get_ptp_egress(struct ptp_info *ptp, uint p)
+{
+	return get_ptp_delay(ptp, p, REG_PTP_PORT_TX_DELAY__2);
+}
+
+static short get_ptp_asym(struct ptp_info *ptp, uint p)
+{
+	short val;
+
+	val = get_ptp_delay(ptp, p, REG_PTP_PORT_ASYM_DELAY__2);
+	if (val & 0x8000)
+		val = -(val & ~0x8000);
+	return val;
+}
+
+static u32 get_ptp_link(struct ptp_info *ptp, uint p)
+{
+	struct ksz_sw *sw = ptp->parent;
+	u32 data;
+
+	sw->ops->p_r32(sw, p, REG_PTP_PORT_LINK_DELAY__4, &data);
+	return data;
+}
+
+static void set_ptp_ingress(struct ptp_info *ptp, uint p, u16 nsec)
+{
+	set_ptp_delay(ptp, p, REG_PTP_PORT_RX_DELAY__2, nsec);
+}
+
+static void set_ptp_egress(struct ptp_info *ptp, uint p, u16 nsec)
+{
+	set_ptp_delay(ptp, p, REG_PTP_PORT_TX_DELAY__2, nsec);
+}
+
+static void set_ptp_asym(struct ptp_info *ptp, uint p, short nsec)
+{
+	if (nsec < 0)
+		nsec = -nsec | 0x8000;
+	set_ptp_delay(ptp, p, REG_PTP_PORT_ASYM_DELAY__2, nsec);
+}
+
+static void set_ptp_link(struct ptp_info *ptp, uint p, u32 nsec)
+{
+	struct ksz_sw *sw = ptp->parent;
+
+	sw->ops->p_w32(sw, p, REG_PTP_PORT_LINK_DELAY__4, nsec);
+}
+
+static inline void dbp_tx_ts(char *name, u8 port, u32 timestamp)
+{
+	u8 overflow;
+	char ts[FMT_NSEC_SIZE];
+
+	timestamp = timestamp_val(timestamp, &overflow);
+	format_nsec(ts, timestamp);
+	dbg_msg("%s p:%d c:%u %08x:%s"NL, name, port, overflow, timestamp, ts);
+}  /* dbp_tx_ts */
+
+static void ptp_tsm_resp(void *data, void *param)
+{
+	struct tsm_db *db = (struct tsm_db *) data;
+	struct ptp_ts *ts = param;
+	u32 timestamp;
+	u8 sec_chk;
+
+	db->cmd |= TSM_CMD_RESP;
+	db->cur_sec = htonl(ts->t.sec);
+	db->cur_nsec = htonl(ts->t.nsec);
+	timestamp = timestamp_val(ts->timestamp, &sec_chk);
+	db->timestamp = htonl(timestamp);
+	db->cur_nsec = db->timestamp;
+}  /* ptp_tsm_resp */
+
+static void ptp_tsm_get_time_resp(void *data, void *param)
+{
+	struct tsm_get_time *get = (struct tsm_get_time *) data;
+	struct ptp_utime *t = param;
+
+	get->cmd |= TSM_CMD_GET_TIME_RESP;
+	get->sec = htonl(t->sec);
+	get->nsec = htonl(t->nsec);
+}  /* ptp_tsm_get_time_resp */
+
+static void mmedian_reset(struct mmedian *m)
+{
+	m->len = MMEDIAN_LEN;
+	m->cnt = 0;
+	m->index = 0;
+}
+
+static s64 filter_sample(struct mmedian *m, s64 delay)
+{
+	s64 median;
+	int i;
+
+	m->data[m->index].delays = delay;
+	if (m->cnt < m->len) {
+		m->cnt++;
+	} else {
+		for (i = 0; i < m->cnt; i++) {
+			if (m->data[i].order == m->index)
+				break;
+		}
+		for (; i + 1 < m->cnt; i++)
+			m->data[i].order = m->data[i + 1].order;
+	}
+	for (i = m->cnt - 1; i > 0; i--) {
+		if (m->data[m->data[i - 1].order].delays <=
+		    m->data[m->index].delays)
+			break;
+		m->data[i].order = m->data[i - 1].order;
+	}
+	m->data[i].order = m->index;
+	m->index = (1 + m->index) % m->len;
+	i = m->cnt / 2;
+	if (m->cnt % 2) {
+		median = m->data[m->data[i].order].delays;
+	} else {
+		median = m->data[m->data[i - 1].order].delays +
+			 m->data[m->data[i].order].delays;
+		median /= 2;
+	}
+	return median;
+}  /* filter_sample */
+
+static void ptp_calc_peer_delay(struct ptp_info *ptp, u8 port)
+{
+	struct ptp_peer_delay_ts *d = &ptp->peer_delay_info[port];
+	struct ptp_filter *f = &d->filter;
+	s64 delay;
+
+	if (d->resp_seqid != d->req_seqid) {
+dbg_msg(" wrong order %04x %04x\n", d->resp_seqid, d->req_seqid);
+		return;
+	}
+	delay = d->t4 - d->t1 - d->corr + 1;
+	delay /= 2;
+	if (delay <= 0)
+		delay = 1;
+	d->t1 = d->t2 = d->t3 = d->t4 = d->corr = 0;
+	f->delay = filter_sample(&f->median, delay);
+	f->delay_valid = true;
+
+	/* Update port peer delay if necessary. */
+	if (!ptp->have_first_drift_set && f->median.cnt == f->median.len)
+		ptp->have_first_drift_set = true;
+	if (ptp->have_first_drift_set)
+		schedule_work(&ptp->set_peer_delay);
+}  /* ptp_calc_peer_delay */
+
+static void ptp_save_peer_delay(struct ptp_info *ptp, u8 port,
+				struct ptp_msg *msg)
+{
+	struct ptp_peer_delay_ts *d = &ptp->peer_delay_info[port];
+	s64 corr;
+
+	/* Record the seqid and reset timestamps when sending. */
+	if (msg->hdr.messageType == PDELAY_REQ_MSG) {
+		d->req_seqid = ntohs(msg->hdr.sequenceId);
+		d->t1 = d->t2 = d->t3 = d->t4 = d->corr = 0;
+		return;
+	}
+
+	corr = htonl(msg->hdr.correctionField.scaled_nsec_hi);
+	corr <<= 32;
+	corr |= htonl(msg->hdr.correctionField.scaled_nsec_lo);
+	corr >>= 16;
+	if (msg->hdr.messageType == PDELAY_RESP_MSG) {
+		struct ptp_msg_info *rx_msg;
+
+		/* This contains the received timestamp of PTP message. */
+		rx_msg = &ptp->rx_msg_info[7];
+		d->t4 = rx_msg->data.ts.t.sec;
+		d->t4 *= NANOSEC_IN_SEC;
+		d->t4 += rx_msg->data.ts.t.nsec;
+		d->resp_seqid = ntohs(msg->hdr.sequenceId);
+		d->t2 = ntohl(msg->data.pdelay_resp.
+			requestReceiptTimestamp.sec.lo);
+		d->t2 *= NANOSEC_IN_SEC;
+		d->t2 += ntohl(msg->data.pdelay_resp.
+			requestReceiptTimestamp.nsec);
+		if (!msg->hdr.flagField.flag.twoStepFlag) {
+			d->corr = corr;
+			if (d->t1)
+				ptp_calc_peer_delay(ptp, port);
+		} else {
+			d->corr = 0;
+			d->t2 -= corr;
+
+			/* Unlikely to receive out-of-order. */
+			if (d->t3 && d->resp_seqid == d->fup_seqid)
+				d->corr = (d->t3 - d->t2);
+		}
+	} else if (msg->hdr.messageType == PDELAY_RESP_FOLLOW_UP_MSG) {
+		d->fup_seqid = ntohs(msg->hdr.sequenceId);
+		d->t3 = ntohl(msg->data.pdelay_resp_follow_up.
+			responseOriginTimestamp.sec.lo);
+		d->t3 *= NANOSEC_IN_SEC;
+		d->t3 += ntohl(msg->data.pdelay_resp_follow_up.
+			responseOriginTimestamp.nsec);
+		d->t3 += corr;
+		if (d->t2 && d->fup_seqid == d->resp_seqid)
+			d->corr = (d->t3 - d->t2);
+		if (d->t1 && d->corr)
+			ptp_calc_peer_delay(ptp, port);
+	}
+}  /* ptp_save_peer_delay */
+
+static void add_tx_delay(struct ptp_ts *ts, int delay, u32 cur_sec)
+{
+	update_ts(ts, cur_sec);
+
+	/*
+	 * Save timestamp without transmit latency for PTP stack that adjusts
+	 * transmit latency itself.
+	 */
+	ts->r = ts->t;
+	add_nsec(&ts->t, delay);
+	ts->timestamp = ts->t.nsec;
+}  /* add_tx_delay */
+
+static void save_tx_ts(struct ptp_info *ptp, struct ptp_tx_ts *tx,
+	struct ptp_hw_ts *htx, int delay, uint port)
+{
+	unsigned long diff = 0;
+
+	add_tx_delay(&htx->ts, delay, ptp->cur_time.sec);
+	if (ptp->need_peer_delay_set_help && htx == &ptp->hw_dreq[port]) {
+		struct ptp_peer_delay_ts *d = &ptp->peer_delay_info[port];
+
+		d->t1 = htx->ts.t.sec;
+		d->t1 *= NANOSEC_IN_SEC;
+		d->t1 += htx->ts.t.nsec;
+		if (d->corr)
+			ptp_calc_peer_delay(ptp, port);
+	}
+	if (ptp->overrides & PTP_CHECK_PATH_DELAY) {
+		if (ptp->last_rx_ts.t.sec) {
+			struct ksz_ptp_time diff;
+
+			calc_udiff(&htx->ts.t, &ptp->last_rx_ts.t, &diff);
+			dbg_msg("pd: %d"NL, diff.nsec);
+		} else
+			ptp->last_tx_ts = htx->ts;
+	}
+	tx->ts = htx->ts;
+	if (tx->data.len) {
+		struct tsm_db *db = (struct tsm_db *) tx->data.buf;
+		u8 msg = tx->data.buf[0] & 3;
+
+		tx->resp_time = jiffies;
+		if (tx->req_time)
+			diff = tx->resp_time - tx->req_time;
+		if (diff < 4 * ptp->delay_ticks) {
+			if (tx->missed) {
+				if (diff > 2 * ptp->delay_ticks)
+					dbg_msg("  caught: %d, %lu; %x=%04x"NL,
+						port, diff, msg,
+						ntohs(db->seqid));
+				if (tx->dev) {
+					file_dev_setup_msg(tx->dev,
+						tx->data.buf, tx->data.len,
+						ptp_tsm_resp, &tx->ts);
+					tx->dev = NULL;
+				}
+
+				/* Invalidate the timestamp. */
+				tx->ts.timestamp = 0;
+				tx->req_time = 0;
+			}
+		} else {
+			dbg_msg("  new: %d, %lu; %x=%04x"NL, port, diff,
+				msg, ntohs(db->seqid));
+		}
+		tx->missed = false;
+	}
+	if (tx->skb) {
+		int len;
+		u64 ns;
+		struct skb_shared_hwtstamps shhwtstamps;
+		struct ksz_sw *sw = ptp->parent;
+
+		if (ptp->tx_en & (1 << 8))
+			ns = (u64) tx->ts.t.sec * NANOSEC_IN_SEC +
+				tx->ts.t.nsec;
+		else
+			ns = (u64) tx->ts.r.sec * NANOSEC_IN_SEC +
+				tx->ts.r.nsec;
+		memset(&shhwtstamps, 0, sizeof(shhwtstamps));
+		shhwtstamps.hwtstamp = ns_to_ktime(ns);
+
+		/* Indicate which port message is sent out.
+		 * Can only report physical port.
+		 */
+		tx->msg->hdr.reserved2 = get_log_port(sw, port);
+		len = (unsigned char *) tx->msg - tx->skb->data;
+		__skb_pull(tx->skb, len);
+		skb_tstamp_tx(tx->skb, &shhwtstamps);
+
+		dev_kfree_skb_irq(tx->skb);
+		tx->skb = NULL;
+	}
+	htx->sending = false;
+}  /* save_tx_ts */
+
+static int get_speed_index(struct ptp_info *ptp, uint port)
+{
+	int index;
+	struct ksz_sw *sw = ptp->parent;
+	struct ksz_port_info *info = get_port_info(sw, port);
+	int speed = (media_connected == info->state) ?
+		info->tx_rate / TX_RATE_UNIT : 0;
+
+	if (speed == 1000)
+		index = 0;
+	else if (speed == 100)
+		index = 1;
+	else
+		index = 2;
+	return index;
+}  /* get_speed_index */
+
+static int get_tx_time(struct ptp_info *ptp, uint port, uint p, u16 status)
+{
+	int delay;
+	int index;
+	u32 reg;
+	u32 *timestamp = NULL;
+	struct ptp_tx_ts *tx = NULL;
+	struct ptp_hw_ts *htx = NULL;
+	struct ksz_sw *sw = ptp->parent;
+
+	index = get_speed_index(ptp, port);
+	while (status) {
+		delay = ptp->tx_latency[port][index];
+		if (status & PTP_PORT_XDELAY_REQ_INT) {
+			reg = REG_PTP_PORT_XDELAY_TS;
+			tx = &ptp->tx_dreq[port];
+			htx = &ptp->hw_dreq[port];
+			status &= ~PTP_PORT_XDELAY_REQ_INT;
+		} else if (status & PTP_PORT_SYNC_INT) {
+			reg = REG_PTP_PORT_SYNC_TS;
+			tx = &ptp->tx_sync[port];
+			htx = &ptp->hw_sync[port];
+			status &= ~PTP_PORT_SYNC_INT;
+		} else {
+			reg = REG_PTP_PORT_PDRESP_TS;
+			tx = &ptp->tx_resp[port];
+			htx = &ptp->hw_resp[port];
+			status &= ~PTP_PORT_PDELAY_RESP_INT;
+			timestamp = &ptp->pdelay_resp_timestamp[port];
+		}
+		sw->ops->p_r32(sw, p, reg, &htx->ts.timestamp);
+		if (timestamp && *timestamp) {
+			htx->ts.timestamp = *timestamp;
+			delay = 0;
+			*timestamp = 0;
+		}
+		save_tx_ts(ptp, tx, htx, delay, port);
+	}
+	if (!htx)
+		return false;
+
+	return true;
+}  /* get_tx_time */
+
+static void ptp_tx_done(struct ptp_info *ptp, int tso)
+{
+	int first;
+	int last;
+	int prev;
+	u32 data;
+	struct ksz_sw *sw = ptp->parent;
+
+	ptp_write_index(ptp, PTP_TOU_INDEX_S, tso);
+	data = sw->reg->r32(sw, REG_TRIG_CTRL__4);
+	last = tso;
+	if (data & TRIG_CASCADE_ENABLE) {
+		do {
+			--tso;
+			ptp_write_index(ptp, PTP_TOU_INDEX_S, tso);
+			data = sw->reg->r32(sw, REG_TRIG_CTRL__4);
+			prev = (data >> TRIG_CASCADE_UPS_S) &
+				TRIG_CASCADE_UPS_M;
+			if (prev == last)
+				break;
+		} while (tso > 0);
+		first = tso;
+		for (tso = last; tso > first; tso--)
+			ptp_tso_off(ptp, tso, (1 << tso));
+	}
+	ptp_tso_off(ptp, tso, (1 << tso));
+
+	prev = 0;
+	data = sw->reg->r32(sw, REG_PTP_CTRL_STAT__4);
+	if (ptp->outputs[last].level != !!(data & GPIO_IN)) {
+		dbg_msg(" wrong level: %d=%d %d\n",
+			last, ptp->outputs[last].level, !!(data & GPIO_IN));
+		prev = 1;
+	}
+	if (ptp->cascade_total &&
+	    last >= ptp->cascade_first &&
+	    last < ptp->cascade_first + ptp->cascade_total) {
+		last = ptp->cascade_first + ptp->cascade_total - 1;
+		if (prev && ptp->outputs[last].level == !!(data & GPIO_IN)) {
+			dbg_msg(" correct level: %d=%d\n",
+				last, ptp->outputs[last].level);
+		}
+		ptp->cascade_total = 0;
+	}
+}  /* ptp_tx_done */
+
+static u64 get_factor_unit(u32 unit, u8 factor)
+{
+	u64 num = unit;
+
+	while (factor) {
+		num *= 10;
+		factor--;
+	}
+	return num;
+}
+
+static u64 get_iterate(struct ptp_tso_options *cmd)
+{
+	u64 iterate;
+
+	iterate = get_factor_unit(cmd->iterate, cmd->reserved[0]);
+	return iterate;
+}
+
+/* 10 ms */
+#define MIN_NEXT_TRIG_TIME  10000000
+
+static void update_next_trigger(struct ptp_utime *t, struct ptp_utime *trig,
+				u32 nsec)
+{
+	struct ksz_ptp_time d;
+	struct ptp_utime s;
+
+	s.sec = trig->sec;
+	s.nsec = trig->nsec;
+	calc_udiff(t, &s, &d);
+
+	/* When operation is first started. */
+	if (!nsec) {
+		nsec = NANOSEC_IN_SEC;
+		if (s.sec == t->sec && s.nsec < t->nsec)
+			add_nsec(&s, nsec);
+	} else {
+		/* Move the trigger time closer to current time. */
+		if (d.sec > 1 || d.sec <= -1) {
+			s.sec = t->sec;
+		}
+
+		/* Move the trigger time quickly if the interval is small. */
+		if (nsec < 100000000 && s.nsec < t->nsec) {
+			u32 diff = t->nsec - s.nsec;
+
+			diff /= nsec;
+			diff *= nsec;
+			add_nsec(&s, diff);
+		}
+	}
+
+	/* Find the next trigger time that is long enough to program. */
+	do {
+		calc_udiff(t, &s, &d);
+
+		/* Not enough time to start the operation. */
+		if (d.sec == 0 && d.nsec < MIN_NEXT_TRIG_TIME)
+			add_large_nsec(&s.sec, &s.nsec, nsec);
+	} while (d.sec == 0 && d.nsec < MIN_NEXT_TRIG_TIME);
+	*trig = s;
+}  /* update_next_trigger */
+
+static void update_trig_and_intr(struct ptp_utime *t, struct ptp_utime *r,
+				 struct ptp_utime *i, u32 cycle)
+{
+	struct ksz_ptp_time d;
+
+	calc_udiff(r, i, &d);
+	update_next_trigger(t, r, cycle);
+	*i = *r;
+	add_nsec(i, d.nsec);
+}  /* update_trig_and_intr */
+
+static bool ptp_core_tx_cascade(struct ptp_info *ptp, u8 tso, u8 total,
+	u16 cnt, u32 sec, u32 nsec, int intr)
+{
+	struct ptp_output *cur, *cascade;
+	struct ptp_utime *trig;
+	u16 orig_cnt;
+	u8 i, j;
+
+	/* Sanity check on tso parameter. */
+	j = tso + total - 1;
+	if (j >= MAX_TRIG_UNIT)
+		return false;
+
+	ptp->cascade_first = tso;
+	ptp->cascade_total = total;
+	ptp->cascade = 0;
+
+	/* Set the trigger time of cascaded operation. */
+	trig = &ptp->outputs[MAX_TRIG_UNIT].trig;
+	trig->sec = sec;
+	trig->nsec = nsec;
+
+	/* Assume software needed to repeat operations. */
+	ptp->tso_cnt[MAX_TRIG_UNIT] = cnt - 1;
+dbg_msg("cascade_sw: %d %d\n", ptp->cascade_sw_each, ptp->cascade_sw_only);
+
+	/* Need software to execute each TOU operation individually. */
+	if (ptp->cascade_sw_each) {
+		ptp->ops->acquire(ptp);
+		ptp->cascade_tso = tso;
+
+		/* Prepare the trigger time for each TOU operation. */
+		cur = &ptp->outputs[tso];
+		trig = &cur->trig;
+		if (cur->level)
+			ptp->cascade_gpo[cur->gpo].tso |= (1 << tso);
+		else
+			ptp->cascade_gpo[cur->gpo].tso &= ~(1 << tso);
+		ptp->reg->tx_restart(ptp, tso, 0, trig->sec, trig->nsec);
+		ptp->ops->release(ptp);
+		return true;
+	}
+
+	/* Repeat may not work reliably if iterate is more than 1 second. */
+	orig_cnt = cnt;
+	if (cnt != 1) {
+		j = tso;
+		for (i = 0; i < total; i++, j++) {
+			cur = &ptp->outputs[tso];
+			if (cur->iterate > NANOSEC_IN_SEC) {
+
+				/* Hardware does it once. */
+				cnt = 1;
+				intr = 1;
+				break;
+			}
+		}
+	}
+
+	/* Hardware can execute repeat operation. */
+	if (orig_cnt == cnt)
+		ptp->tso_cnt[MAX_TRIG_UNIT] = 0;
+	ptp->ops->acquire(ptp);
+	ptp->reg->tx_cascade(ptp, tso, total, cnt, sec, nsec, intr);
+
+	/* Do not reset last unit to keep level high. */
+	j = tso + total - 1;
+	if (ptp->outputs[j].level) {
+		ptp->cascade_tx &= ~(1 << j);
+		ptp->cascade_tx |= (1 << (j + 16));
+		ptp->cascade_gpo[ptp->outputs[j].gpo].tso |= (1 << j);
+	} else {
+		ptp->cascade_gpo[ptp->outputs[j].gpo].tso &= ~(1 << j);
+	}
+	ptp->ops->release(ptp);
+
+	/* Remember the count in case the operation needs to be restarted. */
+	cascade = &ptp->outputs[MAX_TRIG_UNIT];
+	cascade->cnt = cnt;
+	j = tso + total - 1;
+	cur = &ptp->outputs[j];
+	cascade->intr = cur->intr;
+	return true;
+}  /* ptp_core_tx_cascade */
+
+static void update_new_cascade_time(struct ptp_info *ptp, u8 tso,
+				    struct ptp_utime *t, struct ptp_utime *r)
+{
+	struct ksz_ptp_time d_start[MAX_TRIG_UNIT];
+	struct ksz_ptp_time d_stop[MAX_TRIG_UNIT];
+	struct ksz_ptp_time d_intr, d_trig;
+	struct ptp_output *first, *cur, *next, *out;
+	struct ptp_utime s, *n;
+	u32 nsec = 1000000000;
+	u8 i;
+
+	cur = &ptp->outputs[tso];
+	tso = ptp->cascade_first;
+	first = &ptp->outputs[tso];
+	if (first != cur) {
+		calc_udiff(&first->start, &cur->start, &d_trig);
+		calc_udiff(&first->stop, &cur->stop, &d_intr);
+	}
+
+	/* Calculate differences between start and stop times. */
+	for (i = 0; i < ptp->cascade_total; i++, tso++) {
+		out = &ptp->outputs[tso];
+		calc_udiff(&out->start, &out->stop, &d_stop[i]);
+		if (i == ptp->cascade_total - 1)
+			break;
+		next = &ptp->outputs[tso + 1];
+		calc_udiff(&out->start, &next->start, &d_start[i]);
+	}
+
+	n = &cur->start;
+	s.sec = t->sec;
+	s.nsec = n->nsec;
+	if (cur->iterate < 4000000000UL)
+		nsec = (u32)cur->iterate;
+	update_next_trigger(t, &s, nsec);
+	*n = s;
+	*r = s;
+
+	/* Current operating TOU may not be the first. */
+	if (first != cur) {
+		s.sec -= d_trig.sec;
+		sub_nsec(&s, d_trig.nsec);
+		first->start = s;
+		s = first->stop;
+		s.sec -= d_intr.sec;
+		sub_nsec(&s, d_intr.nsec);
+		first->stop = s;
+	}
+
+	tso = ptp->cascade_first;
+	for (i = 0; i < ptp->cascade_total; i++, tso++) {
+		out = &ptp->outputs[tso];
+		if (i > 0) {
+			u8 j = i - 1;
+
+			next = &ptp->outputs[tso - 1];
+			out->start.sec = next->start.sec + d_start[j].sec;
+			out->start.nsec = next->start.nsec;
+			add_nsec(&out->start, d_start[j].nsec);
+		}
+		out->stop = out->start;
+		out->stop.sec += d_stop[i].sec;
+		add_nsec(&out->stop, d_stop[i].nsec);
+		out->trig = out->start;
+		out->intr = out->stop;
+	}
+}  /* update_new_cascade_time */
+
+static bool chk_next_trig(struct ptp_info *ptp, struct ptp_utime *t,
+			  struct ptp_output *cur)
+{
+	struct ptp_utime *trig = &cur->trig;
+	struct ksz_ptp_time d;
+	bool rc = false;
+
+	calc_udiff(t, trig, &d);
+	if (d.sec < 0 || (d.sec == 0 && d.nsec < MIN_TIME_BEFORE_TRIGGER)) {
+dbg_msg("%x:%9u %x:%9u %d\n",
+	t->sec & 0xf, t->nsec, trig->sec & 0xf, trig->nsec, d.nsec);
+		rc = true;
+	}
+	if (rc && !ptp->clk_change)
+		rc = false;
+	return rc;
+}  /* chk_next_trig */
+
+static bool ptp_do_repeat(struct ptp_info *ptp, u8 tso, u16 done,
+			  struct ptp_utime *t)
+{
+	struct ptp_output *cur;
+	struct ptp_utime *trig;
+	u8 tso_acting = tso;
+
+	if (ptp->cascade_sw_only) {
+		tso_acting = ptp->cascade_tso;
+	}
+	cur = &ptp->outputs[tso_acting];
+	trig = &cur->trig;
+	add_large_nsec(&trig->sec, &trig->nsec, cur->cycle);
+	add_large_nsec(&cur->intr.sec, &cur->intr.nsec, cur->cycle);
+
+	if (chk_next_trig(ptp, t, cur)) {
+dbg_msg(" mt0 %x:%9u %x:%9u\n",
+	t->sec & 0xf, t->nsec, trig->sec & 0xf, trig->nsec);
+		if (ptp->tso_cnt[tso]) {
+			update_trig_and_intr(t, &cur->trig, &cur->intr,
+					     cur->cycle);
+		}
+	}
+
+	ptp->reg->tx_restart(ptp, tso, 0, trig->sec, trig->nsec);
+
+	/* In case this is done for cascade mode. */
+	if (ptp->tso_cnt[MAX_TRIG_UNIT])
+		ptp->outputs[MAX_TRIG_UNIT].intr = cur->intr;
+
+	/* Not forever. */
+	if (ptp->tso_cnt[tso] != 0xFFFF)
+		ptp->tso_cnt[tso]--;
+	ptp->tso_chk[tso] = 0;
+	return true;
+}  /* ptp_do_repeat */
+
+static bool ptp_do_cascade_each(struct ptp_info *ptp, u8 tso, u16 done,
+				struct ptp_utime *t)
+{
+	struct ptp_output *cur;
+	struct ptp_utime *trig;
+	u8 tso_next = tso + 1;
+	u8 i, j;
+
+	j = ptp->cascade_first + ptp->cascade_total;
+	ptp->cascade_tso++;
+	if (ptp->cascade_sw_only) {
+		tso = ptp->cascade_tso - 1;
+		tso_next = ptp->cascade_tso;
+	}
+	if (ptp->cascade_tso == j) {
+		ptp->cascade_tso = ptp->cascade_first;
+	}
+
+	/* Last TOU is done in cascaded TOU. */
+	if (tso_next == j) {
+		if (ptp->tso_cnt[MAX_TRIG_UNIT]) {
+			/* Not forever. */
+			if (ptp->tso_cnt[MAX_TRIG_UNIT] != 0xFFFF)
+				ptp->tso_cnt[MAX_TRIG_UNIT]--;
+		} else {
+			j = ptp->cascade_first;
+			if (ptp->cascade_sw_only)
+				j++;
+			ptp->cascade_sw_each = 0;
+			ptp->cascade_sw_only = 0;
+			for (i = 0; i < ptp->cascade_total - 1; i++, j++)
+				ptp_tso_off(ptp, j, (1 << j));
+			return false;
+		}
+		j = ptp->cascade_first;
+		for (i = 0; i < ptp->cascade_total; i++, j++) {
+			cur = &ptp->outputs[j];
+			add_large_nsec(&cur->start.sec, &cur->start.nsec,
+				       cur->iterate);
+			add_large_nsec(&cur->stop.sec, &cur->stop.nsec,
+				       cur->iterate);
+		}
+	}
+
+	if (tso_next >= ptp->cascade_first + ptp->cascade_total)
+		tso_next = ptp->cascade_first;
+
+	/* Get trigger time for next TOU. */
+	tso = tso_next;
+	cur = &ptp->outputs[tso];
+	trig = &cur->trig;
+	*trig = cur->start;
+	cur->intr = cur->stop;
+
+	if (chk_next_trig(ptp, t, cur)) {
+dbg_msg(" %x:%9u %x:%9u; ", cur->start.sec & 0xf, cur->start.nsec,
+		cur->stop.sec & 0xf, cur->stop.nsec);
+dbg_msg(" mt1: %d %x:%9u\n", tso, t->sec & 0xf, t->nsec);
+		update_new_cascade_time(ptp, tso, t, trig);
+	}
+
+	if (ptp->cascade_sw_only) {
+		tso = ptp->cascade_first;
+		ptp->reg->tx_event(ptp, tso, cur->hw_ctrl, cur->hw_pulse,
+				   cur->hw_cycle, cur->hw_pattern, trig->sec,
+				   trig->nsec);
+		if (cur->level)
+			ptp->cascade_gpo[cur->gpo].tso |= (1 << tso);
+		else
+			ptp->cascade_gpo[cur->gpo].tso &= ~(1 << tso);
+	} else {
+		ptp->reg->tx_restart(ptp, tso, 0, trig->sec, trig->nsec);
+	}
+
+	/* May need software to execute the repetition. */
+	if (cur->event != TRIG_REG_OUTPUT)
+		ptp->tso_cnt[tso] = cur->cnt;
+
+	/* This is done for cascade mode. */
+	ptp->outputs[MAX_TRIG_UNIT].intr = cur->intr;
+
+	/* Reset check. */
+	tso = ptp->cascade_first;
+	for (i = 0; i < ptp->cascade_total; i++, tso++) {
+		ptp->tso_chk[tso] = 0;
+	}
+	ptp->tso_chk[MAX_TRIG_UNIT] = 0;
+	return true;
+}  /* ptp_do_cascade_each */
+
+static bool ptp_do_cascade_repeat(struct ptp_info *ptp)
+{
+	struct ptp_output *cur, *cascade;
+	struct ptp_utime *trig;
+	u8 tso;
+	u8 i;
+
+	cascade = &ptp->outputs[MAX_TRIG_UNIT];
+	tso = ptp->cascade_first + ptp->cascade_total - 1;
+	cur = &ptp->outputs[tso];
+	add_large_nsec(&cur->intr.sec, &cur->intr.nsec, cur->iterate);
+	cascade->intr = cur->intr;
+	for (i = 0; i < ptp->cascade_total; i++, tso--) {
+		cur = &ptp->outputs[tso];
+		trig = &cur->trig;
+		add_large_nsec(&trig->sec, &trig->nsec, cur->iterate);
+		ptp_write_index(ptp, PTP_TOU_INDEX_S, tso);
+		ptp_tx_trigger_time(ptp, tso, trig->sec, trig->nsec);
+	}
+	ptp_tx_on(ptp, ptp->cascade_first);
+
+	/* Not forever. */
+	if (ptp->tso_cnt[MAX_TRIG_UNIT] != 0xFFFF)
+		ptp->tso_cnt[MAX_TRIG_UNIT]--;
+
+	/* Reset check. */
+	tso = ptp->cascade_first;
+	for (i = 0; i < ptp->cascade_total; i++, tso++) {
+		ptp->tso_chk[tso] = 0;
+	}
+	ptp->tso_chk[MAX_TRIG_UNIT] = 0;
+	return true;
+}  /* ptp_do_cascade_repeat */
+
+static void ptp_chk_hw_repeat(struct ptp_info *ptp)
+{
+	struct ptp_output *cur;
+	struct ptp_utime s, t;
+	u8 i;
+
+	t.sec = 0;
+	for (i = 0; i < MAX_TRIG_UNIT; i++) {
+
+		/* Check only user executed operations. */
+		if (!(ptp->tso_used & (1 << i)) ||
+		    (ptp->tso_sys & (1 << i)))
+			continue;
+		if (!ptp->tso_chk[i])
+			continue;
+
+		/* Process only hardware repeat forever operations here. */
+		cur = &ptp->outputs[i];
+		if (ptp->tso_intr & (1 << (i + 16)) || cur->len != 0xFFFFFFF0)
+			continue;
+		if (!t.sec)
+			ptp->reg->get_time(ptp, &t);
+		s.sec = t.sec;
+		s.nsec = cur->start.nsec;
+		update_next_trigger(&t, &s, cur->cycle);
+		ptp->reg->tx_restart(ptp, i, 0, s.sec, s.nsec);
+		ptp->tso_chk[i] = 0;
+	}
+	if (ptp->cascade_total && !ptp->cascade_sw_each &&
+	    !ptp->tso_chk[MAX_TRIG_UNIT]) {
+		cur = &ptp->outputs[MAX_TRIG_UNIT];
+
+		/* Turn off hardware executed cascade operation. */
+		if (!cur->cnt) {
+			u8 tso = ptp->cascade_first;
+			struct ptp_output *first = &ptp->outputs[tso];
+			u32 nsec = 1000000000;
+
+			if (first->iterate < 1000000000)
+				nsec = (u32)first->iterate;
+			for (i = 0; i < ptp->cascade_total; i++, tso++) {
+				ptp->reg->tx_off(ptp, tso);
+				cur = &ptp->outputs[tso];
+
+				/* Reset so that times can be updated correctly
+				 * in update_cascade_time().
+				 */
+				cur->trig.sec = 0;
+			}
+			if (!t.sec)
+				ptp->reg->get_time(ptp, &t);
+			s.sec = t.sec;
+			s.nsec = first->start.nsec;
+			update_next_trigger(&t, &s, nsec);
+			update_cascade_time(ptp, ptp->cascade_first,
+					    ptp->cascade_total, s.sec, s.nsec);
+			ptp->ops->release(ptp);
+			ptp_core_tx_cascade(ptp, ptp->cascade_first,
+					    ptp->cascade_total, 0,
+					    s.sec, s.nsec, 1);
+			ptp->ops->acquire(ptp);
+		}
+	}
+}  /* ptp_chk_hw_repeat */
+
+static void generate_tx_event(struct ptp_info *ptp, int gpo)
+{
+	struct ptp_utime t;
+	struct ptp_utime s;
+
+	if (gpo >= MAX_GPIO)
+		return;
+	ptp->first_sec = 0;
+	ptp->intr_sec = 0;
+	ptp->update_sec_jiffies = jiffies;
+	ptp->reg->get_time(ptp, &t);
+	s = t;
+	t.sec += 1;
+	if (t.nsec >= (NANOSEC_IN_SEC - 5000000))
+		t.sec += 1;
+	if (pps_gpo && (ptp->tso_sys & (1 << ptp->pps_tso)))
+		ptp->reg->pps_event(ptp, gpo, t.sec);
+	if (mhz_gpo && (ptp->tso_sys & (1 << ptp->mhz_tso)))
+		ptp->reg->ptp_10MHz(ptp, ptp->mhz_tso, ptp->mhz_gpo, t.sec);
+	ptp_chk_hw_repeat(ptp);
+	schedule_delayed_work(&ptp->update_sec, (1000000 - t.nsec / 1000) * HZ
+		/ 1000000);
+	ptp->clk_add = 0;
+}  /* generate_tx_event */
+
+static void ptp_check_pps(struct work_struct *work)
+{
+	struct delayed_work *dwork = to_delayed_work(work);
+	struct ptp_info *ptp =
+		container_of(dwork, struct ptp_info, check_pps);
+
+#ifdef CONFIG_KSZ_IBA
+	if (iba_stopped(ptp->parent))
+		return;
+#endif
+
+	if (ptp->update_sec_jiffies) {
+		ptp->ops->acquire(ptp);
+		generate_tx_event(ptp, ptp->pps_gpo);
+		ptp->ops->release(ptp);
+	}
+}  /* ptp_check_pps */
+
+static void ptp_prepare_pps_check(struct ptp_info *ptp)
+{
+	bool sec_jump = false;
+	int i;
+
+	if (ptp->sec_changed > 1 || ptp->sec_changed < -1)
+		sec_jump = true;
+	if (!sec_jump)
+		ptp->clk_change = 1;
+	if (ptp->tso_cnt[MAX_TRIG_UNIT]) {
+		ptp->tso_chk[MAX_TRIG_UNIT] = 1;
+		ptp->outputs[MAX_TRIG_UNIT].cnt = 0;
+		if (ptp->tso_cnt[MAX_TRIG_UNIT] != 0xffff)
+			ptp->outputs[MAX_TRIG_UNIT].cnt =
+				ptp->tso_cnt[MAX_TRIG_UNIT];
+		if (sec_jump) {
+			u8 tso;
+
+			ptp->tso_chk[MAX_TRIG_UNIT] = 2;
+			tso = ptp->cascade_first;
+			for (i = 0; i < ptp->cascade_total; i++, tso++)
+				ptp->reg->tx_off(ptp, tso);
+		}
+		return;
+	} else if (ptp->cascade_total) {
+		return;
+	}
+	for (i = 0; i < MAX_TRIG_UNIT; i++) {
+		if (!(ptp->tso_used & (1 << i)) ||
+		    (ptp->tso_sys & (1 << i)))
+			continue;
+		ptp->tso_chk[i] = 1;
+		if (sec_jump) {
+			ptp->tso_chk[i] = 2;
+			ptp->reg->tx_off(ptp, i);
+		}
+	}
+}  /* ptp_prepare_pps_check */
+
+static void ptp_enable_pps_check(struct ptp_info *ptp)
+{
+#ifndef NO_PPS_DETECT
+	ptp->update_sec_jiffies = jiffies;
+	schedule_delayed_work(&ptp->check_pps,
+			      msecs_to_jiffies(1200));
+#endif
+}  /* ptp_enable_pps_check */
+
+static void prepare_gps(struct ptp_info *ptp)
+{
+	ptp->ops->acquire(ptp);
+	ptp->tsi_used |= (1 << ptp->gps_tsi);
+	ptp->events[ptp->gps_tsi].event = 1;
+	ptp->events[ptp->gps_tsi].timeout = 0;
+	ptp->reg->rx_event(ptp, ptp->gps_tsi, ptp->gps_gpi, DETECT_RISE, true);
+	ptp->ops->release(ptp);
+}  /* prepare_gps */
+
+static void prepare_pps(struct ptp_info *ptp)
+{
+	if (ptp->pps_gpo >= MAX_GPIO)
+		return;
+	ptp->ops->acquire(ptp);
+	if (pps_gpo) {
+		ptp->tso_used |= (1 << ptp->pps_tso);
+		ptp->tso_sys |= (1 << ptp->pps_tso);
+	}
+	if (mhz_gpo) {
+		ptp->tso_used |= (1 << ptp->mhz_tso);
+		ptp->tso_used |= (1 << 1);
+		ptp->tso_sys |= (1 << ptp->mhz_tso);
+		ptp->tso_sys |= (1 << 1);
+	}
+	generate_tx_event(ptp, ptp->pps_gpo);
+#ifndef NO_PPS_DETECT
+	if (pps_gpo) {
+		ptp->tsi_used |= (1 << ptp->pps_tsi);
+		ptp->tsi_sys |= (1 << ptp->pps_tsi);
+		ptp->events[ptp->pps_tsi].num = 0;
+		ptp->events[ptp->pps_tsi].event = 1;
+		ptp->events[ptp->pps_tsi].edge = 0;
+		ptp->events[ptp->pps_tsi].expired = 0;
+		ptp->reg->rx_event(ptp, ptp->pps_tsi, ptp->pps_gpo,
+				   DETECT_RISE, true);
+	}
+#endif
+	ptp->ops->release(ptp);
+}  /* prepare_pps */
+
+/* -------------------------------------------------------------------------- */
+
+static int ptp_poll_event(struct ptp_info *ptp, u8 tsi)
+{
+	int max_ts;
+	int num;
+	u32 status;
+	u16 tsi_bit = (1 << tsi);
+	struct ptp_event *event = &ptp->events[tsi];
+	struct ksz_sw *sw = ptp->parent;
+
+	ptp->ops->acquire(ptp);
+	ptp_write_index(ptp, PTP_TSI_INDEX_S, tsi);
+	status = sw->reg->r32(sw, REG_TS_CTRL_STAT__4);
+	ptp->ops->release(ptp);
+	num = (status >> TS_EVENT_DETECT_S) & TS_EVENT_DETECT_M;
+	max_ts = (num <= event->max) ? num : event->max;
+	if (max_ts > event->num) {
+		ptp->ops->acquire(ptp);
+		status = sw->reg->r32(sw, REG_PTP_INT_STATUS__4);
+		if (status & tsi_bit)
+			sw->reg->w32(sw, REG_PTP_INT_STATUS__4, tsi_bit);
+		ptp->reg->read_event(ptp, tsi);
+		ptp->ts_status = 0;
+		ptp->ops->release(ptp);
+		return true;
+	}
+	return false;
+}  /* ptp_poll_event */
+
+static void convert_scaled_nsec(s64 scaled_nsec, int s, s64 *sec, int *nsec)
+{
+	int sign;
+	u64 quot;
+	u32 rem;
+
+	/* Convert to positive number first. */
+	if (scaled_nsec < 0) {
+		sign = -1;
+		scaled_nsec = -scaled_nsec;
+	} else
+		sign = 1;
+	scaled_nsec >>= s;
+	quot = div_u64_u32_rem(scaled_nsec, NANOSEC_IN_SEC, &rem);
+	*sec = quot;
+	*nsec = (int) rem;
+
+	/* Positive number means clock is faster. */
+	if (1 == sign) {
+		*sec = -*sec;
+		*nsec = -*nsec;
+	}
+}  /* convert_scaled_nsec */
+
+static void adj_cur_time(struct ptp_info *ptp)
+{
+	if (ptp->adjust_offset || ptp->adjust_sec) {
+		synchronize_clk(ptp);
+		ptp_prepare_pps_check(ptp);
+		if (ptp->sec_changed || ptp->clk_add)
+			generate_tx_event(ptp, ptp->pps_gpo);
+		else
+			ptp_enable_pps_check(ptp);
+	}
+	if (ptp->sec_changed) {
+		struct timespec64 ts;
+		struct ptp_utime cur;
+
+		ptp->reg->get_time(ptp, &ptp->cur_time);
+		ts = ktime_to_timespec64(ktime_get_real());
+		cur.sec = ts.tv_sec;
+		cur.nsec = ts.tv_nsec;
+		calc_udiff(&ptp->cur_time, &cur, &ptp->time_diff);
+		ptp->sec_changed = 0;
+	}
+}  /* adj_cur_time */
+
+static void set_before_adj(struct ptp_info *ptp, struct ptp_utime *cur)
+{
+	ptp->adjust_offset += cur->nsec;
+	ptp->adjust_offset += ptp->set_delay;
+	ptp->adjust_offset += ptp->get_delay;
+	cur->nsec = 0;
+	if (ptp->adjust_offset > NANOSEC_IN_SEC) {
+		ptp->adjust_offset -= NANOSEC_IN_SEC;
+		cur->sec++;
+	}
+	ptp->reg->set_time(ptp, cur);
+}   /* set_before_adj */
+
+static void set_cur_time(struct ptp_info *ptp, struct ptp_ts *ts)
+{
+	struct ptp_utime cur;
+	s64 diff_sec;
+	int diff_nsec;
+
+	ptp->adjust_offset = ts->t.nsec - ts->timestamp;
+	ptp->reg->get_time(ptp, &cur);
+	diff_nsec = ts->t.nsec - ts->timestamp;
+	diff_sec = (s64) ts->t.sec - cur.sec;
+	if (ptp->features & PTP_ADJ_SEC) {
+		if (diff_sec) {
+			s64 nsec;
+
+			nsec = diff_sec;
+			nsec *= NANOSEC_IN_SEC;
+			nsec += diff_nsec;
+			convert_scaled_nsec(-nsec, 0, &ptp->adjust_sec,
+				&ptp->adjust_offset);
+		} else {
+			ptp->adjust_offset = diff_nsec;
+			ptp->adjust_sec = 0;
+		}
+		ptp->sec_changed = ptp->adjust_sec;
+	} else {
+		if (abs(diff_sec) <= 1) {
+			diff_nsec += diff_sec * NANOSEC_IN_SEC;
+			if (abs(diff_nsec) < NANOSEC_IN_SEC) {
+				ptp->adjust_offset = diff_nsec;
+				diff_sec = 0;
+			}
+		}
+		if (diff_sec) {
+			cur.sec = ts->t.sec;
+			set_before_adj(ptp, &cur);
+			ptp->sec_changed = diff_sec;
+		}
+		ptp->adjust_sec = 0;
+	}
+	adj_cur_time(ptp);
+}  /* set_cur_time */
+
+static void adj_clock(struct work_struct *work)
+{
+	struct ptp_info *ptp = container_of(work, struct ptp_info, adj_clk);
+	struct ptp_utime cur;
+
+#ifdef CONFIG_KSZ_IBA
+	if (iba_stopped(ptp->parent))
+		return;
+#endif
+
+	ptp->ops->acquire(ptp);
+
+	ptp->sec_changed = ptp->adjust_sec;
+	if (!(ptp->features & PTP_ADJ_SEC)) {
+
+		/* Need to adjust second. */
+		if (abs(ptp->adjust_sec) > 1) {
+			ptp->reg->get_time(ptp, &cur);
+			cur.sec += ptp->adjust_sec;
+			set_before_adj(ptp, &cur);
+		} else
+			ptp->adjust_offset += ptp->adjust_sec * NANOSEC_IN_SEC;
+		ptp->adjust_sec = 0;
+	}
+	adj_cur_time(ptp);
+	ptp->ops->release(ptp);
+}  /* adj_clock */
+
+static void set_latency(struct work_struct *work)
+{
+	struct ptp_info *ptp = container_of(work, struct ptp_info, set_latency);
+	struct ksz_sw *sw = ptp->parent;
+	int index;
+	uint n;
+	uint p;
+
+#ifdef CONFIG_KSZ_IBA
+	if (iba_stopped(sw))
+		return;
+#endif
+
+	ptp->ops->acquire(ptp);
+	for (n = 1; n <= ptp->ports; n++) {
+		p = get_phy_port(sw, n);
+		if (ptp->link_ports & (1 << p)) {
+			index = get_speed_index(ptp, p);
+			set_ptp_ingress(ptp, p, ptp->rx_latency[p][index]);
+			set_ptp_egress(ptp, p, ptp->tx_latency[p][index]);
+		}
+	}
+	ptp->ops->release(ptp);
+	ptp->link_ports = 0;
+}  /* set_latency */
+
+static void ptp_set_p2p(struct work_struct *work)
+{
+	struct ptp_info *ptp = container_of(work, struct ptp_info, set_p2p);
+
+#ifdef CONFIG_KSZ_IBA
+	if (iba_stopped(ptp->parent))
+		return;
+#endif
+
+	ptp->ops->acquire(ptp);
+	set_ptp_mode(ptp, ptp->mode | PTP_TC_P2P);
+	ptp->ops->release(ptp);
+}  /* ptp_set_p2p */
+
+static void ptp_set_peer_delay(struct work_struct *work)
+{
+	struct ptp_info *ptp =
+		container_of(work, struct ptp_info, set_peer_delay);
+	struct ptp_peer_delay_ts *d;
+	u32 nsec;
+	int i;
+
+#ifdef CONFIG_KSZ_IBA
+	if (iba_stopped(ptp->parent))
+		return;
+#endif
+
+	ptp->ops->acquire(ptp);
+	for (i = 0; i < ptp->ports; i++) {
+		d = &ptp->peer_delay_info[i];
+		if (d->filter.delay_valid) {
+			nsec = (u32)d->filter.delay;
+			set_ptp_link(ptp, i, nsec);
+			d->filter.delay_valid = false;
+		}
+	}
+	ptp->ops->release(ptp);
+}  /* ptp_set_peer_delay */
+
+static void execute(struct ptp_info *ptp, struct work_struct *work)
+{
+	queue_work(ptp->access, work);
+}  /* execute */
+
+static void ptp_hw_disable(struct ptp_info *ptp)
+{
+	int i;
+	u32 ctrl;
+	struct ksz_sw *sw = ptp->parent;
+
+	for (i = 0; i < 2; i++) {
+		sw->cached.ptp_unit_index =
+			(i << PTP_TSI_INDEX_S) |
+			(i << PTP_TOU_INDEX_S);
+		sw->reg->w32(sw, REG_PTP_UNIT_INDEX__4,
+			sw->cached.ptp_unit_index);
+		ctrl = TS_RESET | TRIG_RESET;
+		sw->reg->w32(sw, REG_PTP_CTRL_STAT__4, ctrl);
+		sw->reg->w32(sw, REG_PTP_CTRL_STAT__4, 0);
+		sw->reg->w32(sw, REG_TRIG_CTRL__4, 0);
+		sw->reg->w32(sw, REG_TS_CTRL_STAT__4, 0);
+	}
+	sw->cached.ptp_unit_index = (i << PTP_TOU_INDEX_S);
+	sw->reg->w32(sw, REG_PTP_UNIT_INDEX__4, sw->cached.ptp_unit_index);
+	ctrl = TRIG_RESET;
+	sw->reg->w32(sw, REG_PTP_CTRL_STAT__4, ctrl);
+	sw->reg->w32(sw, REG_PTP_CTRL_STAT__4, 0);
+	sw->reg->w32(sw, REG_TRIG_CTRL__4, 0);
+	sw->reg->w32(sw, REG_PTP_SUBNANOSEC_RATE, 0);
+}  /* ptp_hw_disable */
+
+static void ptp_hw_enable(struct ptp_info *ptp)
+{
+	uint n;
+	uint p;
+	int index;
+	u16 data;
+	struct ksz_sw *sw = ptp->parent;
+
+	ptp->ops->acquire(ptp);
+	ptp_hw_disable(ptp);
+#ifndef ACL_TEST
+	data = sw->reg->r16(sw, REG_PTP_MSG_CONF1);
+dbg_msg("msg_conf1: %x"NL, data);
+	data = ptp->mode;
+	sw->reg->w16(sw, REG_PTP_MSG_CONF1, data);
+	if ((data & PTP_ENABLE))
+		sw->overrides |= PTP_TAG;
+#endif
+	data = sw->reg->r16(sw, REG_PTP_MSG_CONF2);
+dbg_msg("msg_conf2: %x"NL, data);
+	sw->reg->w16(sw, REG_PTP_MSG_CONF2, ptp->cfg);
+	data = sw->reg->r16(sw, REG_PTP_CLK_CTRL);
+dbg_msg("clk_ctrl: %x"NL, data);
+	data |= PTP_CLK_ENABLE;
+	data &= ~PTP_CLK_ADJ_ENABLE;
+	sw->cached.ptp_clk_ctrl = data;
+#if 1
+	if (!(sw->features & NEW_CAP))
+		data |= PTP_CLK_RESET;
+#endif
+	sw->reg->w16(sw, REG_PTP_CLK_CTRL, data);
+
+	sw->reg->w16(sw, REG_SW_HSR_TPID__2, 0x892F);
+
+	for (n = 1; n <= ptp->ports; n++) {
+		p = get_phy_port(sw, n);
+		index = get_speed_index(ptp, p);
+		set_ptp_ingress(ptp, p, ptp->rx_latency[p][index]);
+		set_ptp_egress(ptp, p, ptp->tx_latency[p][index]);
+	}
+
+	/* PTP stack is still running while device is reset. */
+	if (ptp->drift_set) {
+		ptp->drift = ptp->drift_set;
+		ptp->adjust = clk_adjust_val(ptp->drift, NANOSEC_IN_SEC);
+		set_ptp_adjust(ptp, ptp->adjust);
+		syntonize_clk(ptp);
+		ptp->ptp_synt = true;
+	}
+
+	ptp->ops->release(ptp);
+}  /* ptp_hw_enable */
+
+static void init_tx_ts(struct ptp_tx_ts *ts)
+{
+	ts->ts.timestamp = 0;
+	ts->req_time = 0;
+	ts->resp_time = 0;
+	ts->missed = false;
+	ts->hdr.messageType = 7;
+}  /* init_tx_ts */
+
+static void ptp_init_hw(struct ptp_info *ptp)
+{
+	uint n;
+	uint p;
+	uint port;
+	struct ksz_sw *sw = ptp->parent;
+
+	ptp->ops->acquire(ptp);
+	for (n = 1; n <= ptp->ports; n++) {
+		int index;
+
+		port = get_phy_port(sw, n);
+		p = port;
+		ptp->hw_sync[port].ts.timestamp = 0;
+		ptp->hw_sync[port].sending = false;
+		ptp->hw_dreq[port].ts.timestamp = 0;
+		ptp->hw_dreq[port].sending = false;
+		ptp->hw_resp[port].ts.timestamp = 0;
+		ptp->hw_resp[port].sending = false;
+		init_tx_ts(&ptp->tx_sync[port]);
+		init_tx_ts(&ptp->tx_dreq[port]);
+		init_tx_ts(&ptp->tx_resp[port]);
+		sw->ops->p_r32(sw, p, REG_PTP_PORT_XDELAY_TS,
+			       &ptp->xdelay_ts[port]);
+		sw->ops->p_r32(sw, p, REG_PTP_PORT_PDRESP_TS,
+			       &ptp->pdresp_ts[port]);
+		index = get_speed_index(ptp, port);
+		ptp->rx_latency[port][index] = get_ptp_ingress(ptp, p);
+		ptp->tx_latency[port][index] = get_ptp_egress(ptp, p);
+		ptp->asym_delay[port][index] = get_ptp_asym(ptp, p);
+		ptp->peer_delay[port] = get_ptp_link(ptp, p);
+		dbg_msg("%d = %d %d %d; %u"NL, port,
+			ptp->rx_latency[port][index],
+			ptp->tx_latency[port][index],
+			ptp->asym_delay[port][index],
+			ptp->peer_delay[port]);
+		set_ptp_link(ptp, p, 0);
+		ptp->peer_delay[port] = 0;
+	}
+	ptp->ops->release(ptp);
+}  /* ptp_init_hw */
+
+static void ptp_check(struct ptp_info *ptp)
+{
+	struct ptp_utime cur;
+	struct ptp_utime now;
+	struct ksz_sw *sw = ptp->parent;
+
+	ptp->features |= PTP_ADJ_HACK;
+	ptp->ops->acquire(ptp);
+	ptp->reg->get_time(ptp, &cur);
+	ptp->reg->adjust_time(ptp, true, 10, 0, true);
+	ptp->reg->get_time(ptp, &now);
+	if (now.sec - cur.sec >= 10) {
+		ptp->features &= ~PTP_ADJ_HACK;
+		ptp->features |= PTP_ADJ_SEC;
+		ptp->reg->adjust_time(ptp, false, 10, 0, true);
+		ptp->version = 1;
+	}
+/*
+ * THa  2013/01/08
+ * The Rev. D chip has a problem of decrementing nanosecond that is bigger than
+ * the current nanosecond when continual clock adjustment is enabled.  The
+ * workaround is to use the PTP_ADJ_HACK code although the actual problem
+ * avoided is now different.
+ */
+	if (!(ptp->features & PTP_ADJ_HACK)) {
+		u16 data;
+
+		data = sw->cached.ptp_clk_ctrl;
+		sw->cached.ptp_clk_ctrl |= PTP_CLK_ADJ_ENABLE;
+		sw->reg->w16(sw, REG_PTP_CLK_CTRL, sw->cached.ptp_clk_ctrl);
+		if (cur.sec < 1)
+			cur.sec = 1;
+		cur.nsec = 0;
+		ptp->reg->set_time(ptp, &cur);
+		ptp->reg->adjust_time(ptp, false, 0, 800000000, false);
+		ptp->reg->get_time(ptp, &now);
+		dbg_msg("%x:%u %x:%u"NL, cur.sec, cur.nsec, now.sec, now.nsec);
+		if (abs(now.sec - cur.sec) > 2) {
+			ptp->reg->get_time(ptp, &now);
+			dbg_msg("! %x:%u"NL, now.sec, now.nsec);
+			ptp->features |= PTP_ADJ_HACK;
+			sw->reg->w16(sw, REG_PTP_CLK_CTRL, data);
+
+			sw->reg->w16(sw, REG_PTP_CLK_CTRL,
+				data | PTP_CLK_ADJ_ENABLE);
+			ptp->reg->set_time(ptp, &cur);
+			ptp->reg->adjust_time(ptp, false, 0, 800000000, true);
+			ptp->reg->get_time(ptp, &now);
+			dbg_msg("ok %x:%u"NL, now.sec, now.nsec);
+		}
+		sw->cached.ptp_clk_ctrl = data;
+		sw->reg->w16(sw, REG_PTP_CLK_CTRL, data);
+	}
+	ptp->version = 2;
+	ptp->ops->release(ptp);
+}  /* ptp_check */
+
+static void ptp_start(struct ptp_info *ptp, int init)
+{
+	struct ksz_sw *sw = ptp->parent;
+	struct ksz_iba_info *iba = &sw->info->iba;
+	u32 ctrl;
+	u16 val;
+	struct timespec64 ts;
+	struct ptp_utime t;
+
+	if (!ptp->version) {
+		ptp_hw_enable(ptp);
+		ptp_check(ptp);
+		if (ptp->test_access_time)
+			ptp->test_access_time(ptp);
+		ptp_init_hw(ptp);
+	} else {
+		if (init && (sw->features & NEW_CAP))
+			ptp_hw_enable(ptp);
+
+		/* Update access time calculated with SPI. */
+		if (iba->use_iba && ptp->get_delay > 80000)
+			ptp->get_delay = 80000;
+	}
+	ptp->ops->acquire(ptp);
+	ctrl = sw->reg->r16(sw, REG_PTP_MSG_CONF1);
+	if (ctrl == ptp->mode) {
+		ptp->cfg = sw->reg->r16(sw, REG_PTP_MSG_CONF2);
+		ptp->domain = sw->reg->r16(sw, REG_PTP_DOMAIN_VERSION) &
+			PTP_DOMAIN_M;
+		if (!init) {
+			ptp->ops->release(ptp);
+			return;
+		}
+	} else if (!init)
+		ptp->mode = ctrl;
+	if (ptp->mode != ptp->def_mode) {
+		dbg_msg("mode changed: %04x %04x; %04x %04x"NL,
+			ptp->mode, ptp->def_mode, ptp->cfg, ptp->def_cfg);
+		ptp->mode = ptp->def_mode;
+		ptp->cfg = ptp->def_cfg;
+		ptp->ptp_synt = false;
+	}
+	dbg_msg("ptp_start: %04x %04x"NL,
+		ptp->mode, ptp->cfg);
+	sw->reg->w16(sw, REG_PTP_MSG_CONF1, ptp->mode);
+	val = htons(ptp->mode);
+	sw->ops->chk_regs(sw, REG_PTP_MSG_CONF1, (u8 *)&val, 2);
+	sw->reg->w16(sw, REG_PTP_MSG_CONF2, ptp->cfg);
+	sw->reg->w32(sw, REG_PTP_INT_STATUS__4, 0xffffffff);
+	ptp->tx_intr = PTP_PORT_XDELAY_REQ_INT;
+	ptp_tx_intr_enable(ptp);
+	ptp->ops->release(ptp);
+
+	ts = ktime_to_timespec64(ktime_get_real());
+	t.sec = ts.tv_sec;
+	t.nsec = ts.tv_nsec;
+
+	/* Adjust for leap seconds. */
+	t.sec += ptp->utc_offset;
+	ptp->ops->acquire(ptp);
+	set_ptp_time(ptp, &t);
+	ptp->cur_time = t;
+	ptp->ops->release(ptp);
+
+	prepare_pps(ptp);
+	ptp->started = true;
+	if (sw->dev_offset)
+		ptp->forward |= FWD_STP_DEV;
+	else if (sw->features & VLAN_PORT_TAGGING)
+		ptp->forward |= FWD_VLAN_DEV;
+	ptp->def_forward = ptp->forward;
+}  /* ptp_start */
+
+static void save_msg_info(struct ptp_info *ptp, struct ptp_msg_info *info,
+	struct ptp_msg_hdr *hdr, u32 port, u32 timestamp)
+{
+	struct ptp_msg_options *data = &info->data;
+
+	memcpy(&data->id, &hdr->sourcePortIdentity,
+		sizeof(struct ptp_port_identity));
+	data->seqid = hdr->sequenceId;
+	data->domain = hdr->domainNumber;
+	data->port = port;
+	data->ts.timestamp = timestamp;
+	update_ts(&data->ts, ptp->cur_time.sec);
+	info->sec = ptp->sec_lo;
+}  /* save_msg_info */
+
+static void exit_msg_info(struct ptp_msg_info info[])
+{
+	struct ptp_msg_info *msg;
+	struct ptp_msg_info *prev;
+	int msg_type;
+
+	for (msg_type = SYNC_MSG; msg_type <= MANAGEMENT_MSG; msg_type++) {
+		prev = &info[msg_type];
+		msg = prev->next;
+		while (msg) {
+			prev->next = msg->next;
+			kfree(msg);
+			msg = prev->next;
+		}
+		info[msg_type].data.port = 0;
+	}
+}  /* exit_msg_info */
+
+static void init_msg_info(struct ptp_msg_info info[], spinlock_t *lock)
+{
+	int msg_type;
+
+	spin_lock_init(lock);
+	for (msg_type = SYNC_MSG; msg_type <= MANAGEMENT_MSG; msg_type++) {
+		info[msg_type].data.port = 0;
+		info[msg_type].next = NULL;
+	}
+}  /* init_msg_info */
+
+static void check_expired_msg(struct ptp_info *ptp, struct ptp_msg_info info[],
+	spinlock_t *lock, int *cnt)
+{
+	struct ptp_msg_info *msg;
+	struct ptp_msg_info *prev;
+	int msg_type;
+	int diff;
+	unsigned long flags;
+
+	spin_lock_irqsave(lock, flags);
+	for (msg_type = SYNC_MSG; msg_type <= MANAGEMENT_MSG; msg_type++) {
+		prev = &info[msg_type];
+		msg = prev->next;
+		while (msg) {
+			diff = abs(ptp->sec_lo - msg->sec);
+			if (diff >= 4) {
+				if (cnt && *cnt > 0)
+					(*cnt)--;
+				prev->next = msg->next;
+				kfree(msg);
+				msg = prev;
+			}
+			prev = msg;
+			msg = msg->next;
+		}
+	}
+	spin_unlock_irqrestore(lock, flags);
+}  /* check_expired_msg */
+
+static int find_msg_info(struct ptp_msg_info *msg_info, spinlock_t *lock,
+	struct ptp_msg_hdr *hdr, struct ptp_port_identity *id, int remove,
+	struct ptp_msg_options *info)
+{
+	struct ptp_msg_info *rx_msg = msg_info;
+	struct ptp_msg_info *prev;
+	struct ptp_msg_options *data;
+	int ret = false;
+	unsigned long flags;
+
+	spin_lock_irqsave(lock, flags);
+	prev = rx_msg;
+	rx_msg = rx_msg->next;
+	while (rx_msg) {
+		data = &rx_msg->data;
+		if (!memcmp(&data->id, id, sizeof(struct ptp_port_identity))
+				&& data->seqid == hdr->sequenceId &&
+				data->domain == hdr->domainNumber) {
+			info->port = data->port;
+			info->ts = data->ts;
+			if (remove) {
+				prev->next = rx_msg->next;
+				kfree(rx_msg);
+			}
+			ret = true;
+			break;
+		}
+		prev = rx_msg;
+		rx_msg = rx_msg->next;
+	}
+	spin_unlock_irqrestore(lock, flags);
+	return ret;
+}  /* find_msg_info */
+
+static int ptp_stop(struct ptp_info *ptp, int hw_access)
+{
+	flush_work(&ptp->adj_clk);
+	flush_work(&ptp->set_latency);
+	flush_work(&ptp->set_p2p);
+	flush_work(&ptp->set_peer_delay);
+	cancel_delayed_work_sync(&ptp->check_pps);
+	cancel_delayed_work_sync(&ptp->update_sec);
+	flush_workqueue(ptp->access);
+	ptp->update_sec_jiffies = 0;
+	exit_msg_info(ptp->rx_msg_info);
+	exit_msg_info(ptp->tx_msg_info);
+
+	/* S2 chip can be reset. */
+	ptp->ptp_synt = false;
+
+	/* Stop processing PTP interrupts. */
+	ptp->started = false;
+	ptp->first_drift = ptp->drift = 0;
+	ptp->tx_intr = 0;
+
+	/* Stop triggered outputs and timestamp inputs. */
+	if (hw_access) {
+		ptp->ops->acquire(ptp);
+		ptp_hw_disable(ptp);
+		ptp_tx_intr_enable(ptp);
+		ptp->ops->release(ptp);
+	}
+
+#ifdef DEBUG_MSG
+	dbg_print_work(&db.dbg_print);
+#endif
+	return false;
+}  /* ptp_stop */
+
+static struct file_dev_info *find_minor_dev(struct file_dev_info *info)
+{
+	struct ptp_info *ptp = info->dev;
+	struct file_dev_info *dev;
+	struct file_dev_info *prev;
+
+	dev = ptp->dev[info->minor ^ 1];
+	prev = ptp->dev[info->minor];
+	while (prev != info && prev && dev) {
+		prev = prev->next;
+		dev = dev->next;
+	}
+	if (prev != info)
+		dev = NULL;
+	return dev;
+}  /* find_minor_dev */
+
+static void ptp_init_state(struct ptp_info *ptp)
+{
+	struct ptp_utime t;
+	struct ptp_msg_info *tx_msg;
+
+	if (ptp->op_state) {
+		ptp->op_state++;
+		return;
+	}
+	mutex_lock(&ptp->lock);
+	tx_msg = &ptp->tx_msg_info[7];
+	tx_msg->data.port = 0;
+	tx_msg->data.ts.timestamp = 0;
+	ptp->tx_msg_cnt = 0;
+	ptp->overrides &= ~PTP_USE_DEFAULT_PORT;
+
+	/* Support using 1-step Pdelay_Resp by remembering Pdelay_Req receive
+	 * timestamp.
+	 */
+	ptp->need_1_step_resp_help = true;
+	ptp->need_peer_delay_set_help = true;
+	do {
+		struct ptp_peer_delay_ts *d;
+		int i;
+
+		for (i = 0; i < ptp->ports; i++) {
+			d = &ptp->peer_delay_info[i];
+			mmedian_reset(&d->filter.median);
+			d->filter.delay_valid = false;
+		}
+	} while (0);
+	mutex_unlock(&ptp->lock);
+
+	if (!ptp->started)
+		return;
+	ptp->reg->start(ptp, false);
+
+	ptp_init_hw(ptp);
+
+	ptp->ops->acquire(ptp);
+	ptp->adjust_offset = 0;
+	ptp->offset_changed = 0;
+	memset(&ptp->last_rx_ts, 0, sizeof(struct ptp_ts));
+	memset(&ptp->last_tx_ts, 0, sizeof(struct ptp_ts));
+
+#ifdef DBG_MSG_DROP
+	memset(ptp->seqid_sync, 0, sizeof(u16) * MAX_PTP_PORT);
+	memset(ptp->seqid_fup, 0, sizeof(u16) * MAX_PTP_PORT);
+	memset(ptp->seqid_pdelay_req, 0, sizeof(u16) * MAX_PTP_PORT);
+	memset(ptp->seqid_pdelay_resp, 0, sizeof(u16) * MAX_PTP_PORT);
+	memset(ptp->seqid_pdelay_resp_fup, 0, sizeof(u16) * MAX_PTP_PORT);
+#endif
+
+	if (!ptp->ptp_synt) {
+		syntonize_clk(ptp);
+		ptp->ptp_synt = true;
+	}
+	ptp->reg->get_time(ptp, &t);
+	ptp->cur_time = t;
+	ptp->op_state = 1;
+
+	/* Do not try to adjust drift automatically. */
+	if (!ptp->first_drift)
+		ptp->first_drift = 1;
+	ptp->ops->release(ptp);
+}  /* ptp_init_state */
+
+#ifdef DBG_PROC_SYNC
+static struct ptp_utime last_rcv;
+static s64 first_sync;
+static s64 first_recv;
+#endif
+
+static void ptp_exit_state(struct ptp_info *ptp)
+{
+	if (ptp->op_state > 1) {
+		ptp->op_state--;
+		return;
+	}
+dbg_msg(" def: %x %x\n", ptp->def_mode, ptp->mode);
+	if (ptp->mode != ptp->def_mode) {
+		struct ksz_sw *sw = ptp->parent;
+
+		ptp->ops->acquire(ptp);
+		ptp->mode = ptp->def_mode;
+		sw->reg->w16(sw, REG_PTP_MSG_CONF1, ptp->mode);
+		ptp->ops->release(ptp);
+	}
+	ptp->adjust_offset = 0;
+	ptp->offset_changed = 0;
+	ptp->tx_msg_cnt = 0;
+	ptp->overrides &= ~PTP_USE_DEFAULT_PORT;
+	ptp->tx_en = ptp->rx_en = 0;
+	ptp->tx_en_ports = ptp->rx_en_ports = 0;
+	ptp->need_sync_tx_ts = false;
+	ptp->need_resp_tx_ts = false;
+	ptp->need_1_step_resp_help = false;
+	ptp->need_2_step_resp_help = false;
+	ptp->need_1_step_clock_oper = false;
+	ptp->need_peer_delay_set_help = false;
+	ptp->have_first_drift_set = false;
+	ptp->use_own_api = false;
+	ptp->overrides &= ~(PTP_VERIFY_TIMESTAMP |
+			    PTP_UPDATE_PDELAY_RESP_TIME);
+	ptp->cap = 0;
+	ptp->op_mode = 0;
+	ptp->op_state = 0;
+	ptp->forward = ptp->def_forward;
+#ifdef DBG_PROC_SYNC
+	last_rcv.sec = 0;
+	first_recv = 0;
+	first_sync = 0;
+#endif
+
+	/* Indicate drift is not being set by PTP stack. */
+	ptp->drift_set = 0;
+}  /* ptp_exit_state */
+
+static struct ptp_msg *check_ptp_msg(u8 *data, u16 **udp_check_ptr)
+{
+	struct ethhdr *eth = (struct ethhdr *) data;
+	struct vlan_ethhdr *vlan = (struct vlan_ethhdr *) data;
+	struct iphdr *iph = NULL;
+	struct ipv6hdr *ip6h = NULL;
+	struct udphdr *udp;
+	int ipv6;
+	struct ptp_msg *msg;
+
+	if (eth->h_proto == htons(0x88F7)) {
+		msg = (struct ptp_msg *)(eth + 1);
+		goto check_ptp_version;
+	}
+
+	if (eth->h_proto == htons(ETH_P_8021Q)) {
+		if (vlan->h_vlan_encapsulated_proto == htons(ETH_P_8021Q)) {
+			unsigned char *ptr = (unsigned char *) vlan;
+
+			ptr += VLAN_HLEN;
+			vlan = (struct vlan_ethhdr *) ptr;
+		}
+		if (vlan->h_vlan_encapsulated_proto == htons(0x88F7)) {
+			msg = (struct ptp_msg *)(vlan + 1);
+			goto check_ptp_version;
+		}
+		ipv6 = vlan->h_vlan_encapsulated_proto == htons(ETH_P_IPV6);
+		if (vlan->h_vlan_encapsulated_proto != htons(ETH_P_IP) &&
+				!ipv6)
+			return NULL;
+		ip6h = (struct ipv6hdr *)(vlan + 1);
+		iph = (struct iphdr *)(vlan + 1);
+	} else {
+		ipv6 = eth->h_proto == htons(ETH_P_IPV6);
+		if (eth->h_proto != htons(ETH_P_IP) && !ipv6)
+			return NULL;
+		ip6h = (struct ipv6hdr *)(eth + 1);
+		iph = (struct iphdr *)(eth + 1);
+	}
+
+	if (ipv6) {
+		if (ip6h->nexthdr != IPPROTO_UDP)
+			return NULL;
+
+		udp = (struct udphdr *)(ip6h + 1);
+		if (udp_check_ptr)
+			*udp_check_ptr = &udp->check;
+	} else {
+		if (iph->protocol != IPPROTO_UDP)
+			return NULL;
+		if (ntohs(iph->frag_off) & IP_OFFSET)
+			return NULL;
+
+		udp = (struct udphdr *)(iph + 1);
+		if (udp_check_ptr)
+			*udp_check_ptr = &udp->check;
+	}
+
+	if (udp->dest != htons(319) && udp->dest != htons(320))
+		return NULL;
+
+	msg = (struct ptp_msg *)(udp + 1);
+
+check_ptp_version:
+	if (msg->hdr.versionPTP >= 2)
+		return msg;
+	return NULL;
+}  /* check_ptp_msg */
+
+static struct ptp_msg *check_ptp_event(u8 *data)
+{
+	struct ptp_msg *msg;
+
+	msg = check_ptp_msg(data, NULL);
+	if (!msg)
+		return NULL;
+	switch (msg->hdr.messageType) {
+	case SYNC_MSG:
+		break;
+	case DELAY_REQ_MSG:
+		break;
+	case PDELAY_REQ_MSG:
+		break;
+	case PDELAY_RESP_MSG:
+		break;
+	default:
+		msg = NULL;
+		break;
+	}
+	return msg;
+}
+
+/**
+ * update_ptp_msg - Update PTP message
+ * @data:	The PTP frame data.
+ * @port:	Buffer to hold the tx port, or hoding the rx port.
+ * @timestamp:	Buffer to hold the tx timestamp, or holding the rx timestamp.
+ * @overrides:	Parameter to do something with the reserved fields.
+ *
+ * This function serves two purposes so that driver code is compatible to the
+ * PTP stack using the first generation PTP operation behavior.
+ * The PTP header reserved fieids were used to store the port and timestamp.
+ * When receiving and if necessary those field are filled with the rx port
+ * and timestamp so that old PTP stack can retrieve those information.
+ * When transmiting the tx port and timestamp are retrieved before those fields
+ * are filled with zero.
+ */
+static struct ptp_msg *update_ptp_msg(u8 *data, u8 *port, u32 *timestamp,
+	u32 overrides)
+{
+	struct ptp_msg *msg;
+	u16 *udp_check_loc = NULL;
+	int udp_check = 0;
+
+	msg = check_ptp_msg(data, &udp_check_loc);
+	if (!msg)
+		return NULL;
+	if (msg->hdr.reserved2 != *port) {
+		u8 data = msg->hdr.reserved2;
+
+		/*
+		 * Hardware automatically updates the port number to the
+		 * actual port sending the Pdelay_Req message.
+		 */
+		if ((overrides & PTP_UPDATE_PDELAY_RESP_PORT) &&
+		    PDELAY_RESP_MSG == msg->hdr.messageType) {
+			u16 reply = ntohs(msg->data.pdelay_resp.
+				requestingPortIdentity.port);
+
+			if (reply - 1 == *port) {
+				udp_check += reply;
+				msg->data.pdelay_resp.requestingPortIdentity.
+					port = htons(*port);
+				udp_check -= *port;
+			}
+		}
+		if (overrides & PTP_ZERO_RESERVED_FIELD) {
+			udp_check += data;
+			msg->hdr.reserved2 = *port;
+			udp_check -= *port;
+		}
+		*port = data;
+	}
+	if (msg->hdr.reserved3 != htonl(*timestamp)) {
+		u32 tmp = ntohl(msg->hdr.reserved3);
+
+		if (overrides & PTP_ZERO_RESERVED_FIELD) {
+			int i;
+			u16 *data = (u16 *) &msg->hdr.reserved3;
+
+			for (i = 0; i < 2; i++)
+				udp_check += ntohs(data[i]);
+			msg->hdr.reserved3 = htonl(*timestamp);
+			for (i = 0; i < 2; i++)
+				udp_check -= ntohs(data[i]);
+		}
+		*timestamp = tmp;
+	}
+	if ((overrides & PTP_VERIFY_TIMESTAMP) &&
+			PDELAY_RESP_MSG == msg->hdr.messageType &&
+			msg->hdr.flagField.flag.twoStepFlag) {
+		struct ptp_utime rx;
+
+		rx.nsec = ntohl(msg->data.pdelay_resp.requestReceiptTimestamp.
+			nsec);
+		rx.sec = ntohl(msg->data.pdelay_resp.requestReceiptTimestamp.
+			sec.lo);
+		*timestamp = (rx.sec << 30) | rx.nsec;
+	}
+	if (udp_check && udp_check_loc) {
+		u16 check;
+
+		check = ntohs(*udp_check_loc);
+		udp_check += check;
+		udp_check = (udp_check >> 16) + (udp_check & 0xffff);
+		udp_check += (udp_check >> 16);
+		check = (u16) udp_check;
+		if (!check)
+			check = -1;
+		*udp_check_loc = htons(check);
+	}
+	return msg;
+}  /* update_ptp_msg */
+
+static void get_rx_tstamp(void *ptr, struct sk_buff *skb)
+{
+	struct ptp_info *ptp = ptr;
+	struct ptp_msg *msg;
+	struct ptp_msg_options *rx_msg;
+	struct ptp_ts ts;
+	u64 ns;
+	struct skb_shared_hwtstamps *shhwtstamps = skb_hwtstamps(skb);
+
+	if (!shhwtstamps)
+		return;
+
+	/* Received PTP messages are saved in database. */
+	if (ptp->op_mode > 1) {
+
+		/* Use previously parsed PTP message if available. */
+		msg = ptp->rx_msg;
+		if (!msg)
+			msg = check_ptp_msg(skb->data, NULL);
+		if (!msg || msg->hdr.messageType & 0x8)
+			return;
+	}
+
+	rx_msg = &ptp->rx_msg_info[7].data;
+	ts = rx_msg->ts;
+
+	ns = (u64) ts.t.sec * NANOSEC_IN_SEC + ts.t.nsec;
+	memset(shhwtstamps, 0, sizeof(*shhwtstamps));
+	shhwtstamps->hwtstamp = ns_to_ktime(ns);
+}  /* get_rx_tstamp */
+
+static void get_tx_tstamp(struct ptp_info *ptp, struct sk_buff *skb)
+{
+	struct ksz_sw *sw = ptp->parent;
+	int cnt;
+	uint m;
+	uint n;
+	uint p;
+	struct ptp_msg *msg;
+	u32 ports;
+	u32 intr;
+	bool dest;
+	struct ptp_tx_ts *tx;
+	struct ptp_tx_ts *xtx;
+	struct sk_buff *orig_skb = skb;
+
+	if (ptp->tx_msg_parsed)
+		msg = ptp->tx_msg;
+	else
+		msg = check_ptp_msg(skb->data, NULL);
+	ptp->tx_msg_parsed = false;
+	if (!msg || msg->hdr.messageType & 0x8)
+		return;
+
+	dest = false;
+	if (ptp->tx_ports & sw->PORT_MASK)
+		dest = true;
+	m = sw->PORT_MASK & ~sw->HOST_MASK;
+	if (!dest)
+		ports = m;
+	else
+		ports = ptp->tx_ports & m;
+	if (SYNC_MSG == msg->hdr.messageType) {
+		xtx = ptp->tx_sync;
+		intr = PTP_PORT_SYNC_INT;
+	} else if (PDELAY_RESP_MSG == msg->hdr.messageType) {
+		xtx = ptp->tx_resp;
+		intr = PTP_PORT_PDELAY_RESP_INT;
+	} else {
+		xtx = ptp->tx_dreq;
+		intr = PTP_PORT_XDELAY_REQ_INT;
+	}
+	if (!(ptp->tx_intr & intr))
+		return;
+
+	/* Only accept socket buffer from application. */
+	if (!orig_skb->sk)
+		return;
+	cnt = 0;
+	for (n = 1; n <= ptp->ports; n++) {
+		p = get_phy_port(sw, n);
+		if (!(ports & (1 << p)))
+			continue;
+		tx = &xtx[p];
+		if (tx->skb) {
+			dev_kfree_skb_irq(tx->skb);
+			tx->skb = NULL;
+		}
+		if (!cnt) {
+			skb = skb_clone_sk(orig_skb);
+			if (!skb)
+				break;
+		}
+
+		/* Need to create socket buffer for more than 1 port. */
+		if (cnt++) {
+			skb = skb_copy(orig_skb, GFP_ATOMIC);
+			if (!skb)
+				break;
+			skb->sk = orig_skb->sk;
+			msg = check_ptp_event(skb->data);
+		}
+		tx->skb = skb;
+		tx->msg = msg;
+		skb_shinfo(skb)->tx_flags |= SKBTX_IN_PROGRESS;
+	}
+}  /* get_tx_tstamp */
+
+static int ptp_hwtstamp_ioctl(struct ptp_info *ptp, struct ifreq *ifr,
+			      u16 ports)
+{
+	struct hwtstamp_config config;
+
+	if (copy_from_user(&config, ifr->ifr_data, sizeof(config)))
+		return -EFAULT;
+
+	/* reserved for future extensions */
+	if (config.flags)
+		return -EINVAL;
+
+	switch (config.tx_type) {
+	case HWTSTAMP_TX_OFF:
+		ptp->tx_en_ports &= ~ports;
+		if (!ptp->tx_en_ports)
+			ptp->tx_en &= ~7;
+		if (!(ptp->tx_en & 1))
+			ptp->tx_en &= ~(1 << 8);
+		break;
+	case HWTSTAMP_TX_ONESTEP_P2P:
+		ptp->tx_en |= 6;
+		break;
+	case HWTSTAMP_TX_ONESTEP_SYNC:
+		ptp->tx_en |= 2;
+		break;
+	case HWTSTAMP_TX_ON:
+		break;
+	default:
+		return -ERANGE;
+	}
+	if (config.tx_type != HWTSTAMP_TX_OFF) {
+
+		/* PTP stack can use PTP driver API to setup mode. */
+		if (!ptp->use_own_api && !(ptp->tx_en & 1)) {
+			u16 mode = ptp->mode;
+
+			mode &= ~(PTP_1STEP | PTP_TC_P2P | PTP_MASTER);
+			mode |= PTP_MASTER;
+			if (ptp->tx_en & 2) {
+				mode |= PTP_1STEP;
+				if (ptp->tx_en & 4)
+					mode |= PTP_TC_P2P;
+			} else {
+#ifdef USE_2_STEP_WORKAROUND
+				/* Workaround for 2-step clock issues. */
+				ptp->need_1_step_clock_oper = true;
+				ptp->need_2_step_resp_help = true;
+#endif
+
+				/* Assume stack will forward everything. */
+				mode |= PTP_802_1AS;
+			}
+			ptp_acquire(ptp);
+			set_ptp_mode(ptp, mode);
+			ptp->op_mode = 1;
+			ptp_release(ptp);
+		}
+
+		/* Default is to include tx latency in tx timestamp. */
+		if (!(ptp->tx_en & 1))
+			ptp->tx_en |= (1 << 8);
+		ptp->tx_en_ports |= ports;
+		ptp->tx_en |= 1;
+	}
+
+	switch (config.rx_filter) {
+	case HWTSTAMP_FILTER_NONE:
+		ptp->rx_en_ports &= ~ports;
+		if (!ptp->rx_en_ports)
+			ptp->rx_en &= ~1;
+		if (!(ptp->rx_en & 1))
+			ptp->rx_en &= ~(1 << 8);
+		ptp_exit_state(ptp);
+		break;
+	case HWTSTAMP_FILTER_PTP_V2_L2_SYNC:
+	case HWTSTAMP_FILTER_PTP_V2_L4_SYNC:
+		ptp_acquire(ptp);
+		set_ptp_mode(ptp, ptp->mode & ~PTP_MASTER);
+		ptp_release(ptp);
+		break;
+	case HWTSTAMP_FILTER_PTP_V2_L2_DELAY_REQ:
+	case HWTSTAMP_FILTER_PTP_V2_L4_DELAY_REQ:
+		ptp_acquire(ptp);
+		set_ptp_mode(ptp, ptp->mode | PTP_MASTER);
+		ptp_release(ptp);
+		break;
+	case HWTSTAMP_FILTER_ALL:
+	case HWTSTAMP_FILTER_PTP_V2_EVENT:
+	default:
+		ptp_init_state(ptp);
+
+		/* Default is to include rx latency in rx timestamp. */
+		if (!(ptp->rx_en & 1))
+			ptp->rx_en |= (1 << 8);
+		ptp->rx_en_ports |= ports;
+		ptp->rx_en |= 1;
+		break;
+	}
+
+	return copy_to_user(ifr->ifr_data, &config, sizeof(config)) ?
+		-EFAULT : 0;
+}
+
+static int ptp_chk_rx_msg(struct ptp_info *ptp, u8 *data, uint port)
+{
+	struct ptp_msg *msg;
+	struct ksz_sw *sw = ptp->parent;
+
+	/* Use previously parsed PTP message if available. */
+	msg = ptp->rx_msg;
+	if (!msg)
+		msg = check_ptp_msg(data, NULL);
+	if (!msg)
+		return false;
+	ptp->rx_msg = msg;
+
+	if ((sw->features & USE_802_1X_AUTH) &&
+	    !(sw->on_ports & (1 << port))) {
+		return true;
+	}
+	return false;
+}  /* ptp_chk_rx_msg */
+
+static int ptp_drop_pkt(struct ptp_info *ptp, struct sk_buff *skb, u32 vlan_id,
+	int *tag, int *ptp_tag, int *forward)
+{
+	struct ksz_sw *sw = ptp->parent;
+
+	/* Not PTP message. */
+	if (!get_rx_tag_ptp(&sw->tag))
+		return false;
+	do {
+		u16 vid;
+		u16 *protocol;
+
+		if (!(ptp->vid))
+			break;
+		if (vlan_get_tag(skb, &vid))
+			break;
+		vid &= VLAN_VID_MASK;
+		protocol = (u16 *) &skb->data[VLAN_ETH_HLEN - 2];
+
+		if (!vid)
+			break;
+		if (*protocol == ntohs(0x88F7) && vid != ptp->vid)
+			return true;
+	} while (0);
+	*ptp_tag = get_rx_tag_ports(&sw->tag);
+	ptp->ops->get_rx_info(ptp, skb->data, *ptp_tag, sw->tag.timestamp);
+	*forward = ptp->forward;
+	if (!ptp->op_state && !(ptp->rx_en & 1)) {
+		*ptp_tag = 0;
+		return false;
+	}
+	if (ptp_chk_rx_msg(ptp, skb->data, *ptp_tag))
+		return true;
+	if (ptp->rx_en & 1)
+		ptp->ops->get_rx_tstamp(ptp, skb);
+	(*ptp_tag)++;
+	return false;
+}  /* ptp_drop_pkt */
+
+static void set_msg_info(struct ptp_info *ptp, struct ptp_msg_hdr *hdr,
+	u32 port, u32 timestamp)
+{
+	struct ptp_msg_info *tx_msg;
+	struct ptp_msg_info *info;
+
+	tx_msg = &ptp->tx_msg_info[hdr->messageType];
+	info = kzalloc(sizeof(struct ptp_msg_info), GFP_KERNEL);
+	if (info) {
+		unsigned long flags;
+
+		spin_lock_irqsave(&ptp->tx_msg_lock, flags);
+		save_msg_info(ptp, info, hdr, port, timestamp);
+		info->next = tx_msg->next;
+		tx_msg->next = info;
+		if (ptp->tx_msg_cnt >= 0)
+			ptp->tx_msg_cnt++;
+		spin_unlock_irqrestore(&ptp->tx_msg_lock, flags);
+	}
+}  /* set_msg_info */
+
+static int proc_ptp_hw_access(struct ptp_info *ptp, int cmd, int subcmd,
+	int option, void *data, size_t len, struct file_dev_info *info,
+	int *output, int wait);
+
+#ifdef DBG_PROC_SYNC
+static void handle_sync(struct ptp_info *ptp, struct ptp_msg *msg)
+{
+	struct ptp_utime recv;
+	struct ptp_utime sync;
+	struct ksz_ptp_time drift;
+	struct ksz_ptp_time interval;
+	struct ksz_ptp_time offset;
+	s64 corr;
+	u64 nsec;
+	s64 drift_per_sec;
+	s64 avg;
+	s64 cur_recv;
+	s64 cur_sync;
+static s64 sync_corr;
+static struct ptp_ts ts;
+static struct ptp_utime last_sync;
+static struct ksz_ptp_time last_offset;
+
+	sync.sec = sync.nsec = 0;
+	if (SYNC_MSG == msg->hdr.messageType) {
+		struct ptp_msg_info *rx_msg;
+
+		rx_msg = &ptp->rx_msg_info[7];
+		ts = rx_msg->data.ts;
+
+		corr = htonl(msg->hdr.correctionField.scaled_nsec_hi);
+		corr <<= 32;
+		corr |= htonl(msg->hdr.correctionField.scaled_nsec_lo);
+		corr >>= 16;
+		sync_corr = corr;
+
+		/* This is one-step Sync. */
+		if (!msg->hdr.flagField.flag.twoStepFlag) {
+			sync.sec = ntohl(msg->data.sync.
+				originTimestamp.sec.lo);
+			sync.nsec = ntohl(msg->data.sync.
+				originTimestamp.nsec);
+		}
+	} else if (FOLLOW_UP_MSG == msg->hdr.messageType) {
+		corr = htonl(msg->hdr.correctionField.scaled_nsec_hi);
+		corr <<= 32;
+		corr |= htonl(msg->hdr.correctionField.scaled_nsec_lo);
+		corr >>= 16;
+		sync_corr += corr;
+		sync.sec = ntohl(msg->data.follow_up.
+			preciseOriginTimestamp.sec.lo);
+		sync.nsec = ntohl(msg->data.follow_up.
+			preciseOriginTimestamp.nsec);
+	}
+
+	/* Sync transmit timestamp not received. */
+	if (!sync.sec)
+		return;
+	if (sync_corr) {
+		s32 rem;
+
+		corr = ts.t.sec;
+		corr *= NANOSEC_IN_SEC;
+		corr += ts.t.nsec;
+		corr -= sync_corr;
+		corr = div_s64_s32_rem(corr, NANOSEC_IN_SEC, &rem);
+		ts.t.sec = (u32) corr;
+		ts.t.nsec = rem;
+	}
+#if 0
+	ts.t.nsec += 5;
+	ts.t.nsec /= 10;
+	ts.t.nsec *= 10;
+	sync.nsec += 5;
+	sync.nsec /= 10;
+	sync.nsec *= 10;
+#endif
+	calc_udiff(&sync, &ts.t, &offset);
+	cur_recv = cur_sync = avg = 0;
+	if (first_sync) {
+		nsec = sync.sec;
+		nsec *= NANOSEC_IN_SEC;
+		nsec += sync.nsec;
+		cur_sync = nsec;
+		nsec = ts.t.sec;
+		nsec *= NANOSEC_IN_SEC;
+		nsec += ts.t.nsec;
+		cur_recv = nsec;
+		cur_sync -= first_sync;
+		cur_recv -= first_recv;
+		cur_recv -= cur_sync;
+		cur_recv = abs(cur_recv);
+		cur_recv *= NANOSEC_IN_SEC;
+	} else {
+		nsec = sync.sec;
+		nsec *= NANOSEC_IN_SEC;
+		nsec += sync.nsec;
+		first_sync = nsec;
+		nsec = ts.t.sec;
+		nsec *= NANOSEC_IN_SEC;
+		nsec += ts.t.nsec;
+		first_recv = nsec;
+	}
+	nsec = 0;
+	drift_per_sec = 0;
+	if (last_rcv.sec) {
+		calc_udiff(&last_sync, &sync, &interval);
+		calc_diff(&last_offset, &offset, &drift);
+		nsec = interval.sec;
+		nsec *= NANOSEC_IN_SEC;
+		nsec += interval.nsec;
+		drift_per_sec = abs(drift.sec);
+		drift_per_sec *= NANOSEC_IN_SEC;
+		drift_per_sec += abs(drift.nsec);
+		drift_per_sec *= NANOSEC_IN_SEC;
+		drift_per_sec = div_s64_s64(drift_per_sec, nsec);
+		avg = div_s64_s64(cur_recv, cur_sync);
+	}
+	if (sync_corr) {
+		cur_recv = avg * nsec;
+		cur_recv = div_s64_u32(cur_recv, NANOSEC_IN_SEC);
+		cur_recv += nsec;
+		corr = last_rcv.sec;
+		corr *= NANOSEC_IN_SEC;
+		corr += last_rcv.nsec;
+		cur_recv += corr;
+		corr = recv.sec;
+		corr *= NANOSEC_IN_SEC;
+		corr += recv.nsec;
+		corr -= cur_recv;
+printk(" corr: %lld %lld %lld"NL, sync_corr, corr, corr - sync_corr);
+		sync_corr = 0;
+	}
+dbg_msg("sync: %x:%9u %x:%9u p:%10lld d:%lld a:%lld %lld"NL,
+ts.t.sec, ts.t.nsec,
+sync.sec, sync.nsec, nsec, drift_per_sec, avg, drift_per_sec - avg);
+dbg_msg("o: %d"NL, offset.nsec);
+#if 0
+	if (nsec && nsec < 5000000000) {
+		struct ptp_clk_options clk_opt;
+		int drift_set;
+		int output;
+		int err;
+
+		output = 1;
+		clk_opt.sec = clk_opt.nsec = 0;
+
+		drift_set = (int) drift_per_sec;
+		if (drift_set < 5 && abs(offset.nsec) > 1000) {
+			clk_opt.sec = offset.sec;
+			clk_opt.nsec = offset.nsec;
+			if (offset.nsec < 0)
+				output = 2;
+			last_rcv.sec = 0;
+			ts.t.sec = 0;
+			first_recv = 0;
+			first_sync = 0;
+		}
+
+		if (drift.nsec < 0)
+			drift_set = -drift_set;
+		drift_set += ptp->drift;
+
+		clk_opt.drift = drift_set;
+		clk_opt.interval = NANOSEC_IN_SEC;
+		err = proc_ptp_hw_access(ptp,
+			DEV_CMD_PUT, DEV_PTP_CLK, output,
+			&clk_opt, sizeof(clk_opt), NULL, &output,
+			false);
+	}
+#endif
+	last_sync = sync;
+	last_rcv = ts.t;
+	last_offset = offset;
+}  /* handle_sync */
+#endif
+
+static struct ptp_msg *ptp_set_rx_info(struct ptp_info *ptp, u8 *data, u8 port,
+	u32 timestamp)
+{
+	struct ptp_msg *msg;
+#ifdef DBG_MSG_DROP
+	u16 *seqid;
+#endif
+	u32 overrides = ptp->overrides;
+
+#ifndef DBG_MSG_DROP
+	/* Do not need to parse PTP message except for PDELAY_REQ_MSG. */
+	if (1 == ptp->op_mode &&
+	    (ptp->need_1_step_resp_help ||
+	     ptp->need_peer_delay_set_help)) {
+		msg = check_ptp_msg(data, NULL);
+		if (!msg)
+			return NULL;
+		if (ptp->need_1_step_resp_help &&
+		    msg->hdr.messageType == PDELAY_REQ_MSG)
+			return msg;
+		if (msg->hdr.messageType == PDELAY_RESP_MSG ||
+		    msg->hdr.messageType == PDELAY_RESP_FOLLOW_UP_MSG) {
+			if (ptp->need_peer_delay_set_help)
+				ptp_save_peer_delay(ptp, port, msg);
+		}
+		return NULL;
+	}
+#endif
+
+	/* Set receive port and timestamp inside the PTP message. */
+	if (0 == ptp->op_mode && ptp->op_state) {
+		overrides |= PTP_ZERO_RESERVED_FIELD;
+		overrides |= PTP_UPDATE_PDELAY_RESP_PORT;
+	} else
+		overrides &= ~PTP_ZERO_RESERVED_FIELD;
+	msg = update_ptp_msg(data, &port, &timestamp, overrides);
+
+#ifdef DBG_PROC_SYNC
+	if (ptp->overrides & PTP_CHECK_SYNC_TIME)
+		handle_sync(ptp, msg);
+#endif
+#ifdef DBG_MSG_DROP
+	switch (msg->hdr.messageType) {
+	case SYNC_MSG:
+		seqid = &ptp->seqid_sync[port];
+		break;
+	case FOLLOW_UP_MSG:
+		seqid = &ptp->seqid_fup[port];
+		break;
+	case PDELAY_REQ_MSG:
+		seqid = &ptp->seqid_pdelay_req[port];
+		break;
+	case PDELAY_RESP_MSG:
+		seqid = &ptp->seqid_pdelay_resp[port];
+		break;
+	case PDELAY_RESP_FOLLOW_UP_MSG:
+		seqid = &ptp->seqid_pdelay_resp_fup[port];
+		break;
+	default:
+		seqid = NULL;
+		break;
+	}
+	if (seqid) {
+		if (((*seqid + 1) & 0xffff) != ntohs(msg->hdr.sequenceId) &&
+		    *seqid)
+			printk(KERN_INFO " %d=%x:%04x %04x"NL, port,
+			       msg->hdr.messageType, *seqid,
+			       ntohs(msg->hdr.sequenceId));
+		*seqid = ntohs(msg->hdr.sequenceId);
+	}
+#endif
+
+	/* Do not need to save PTP messages. */
+	if (ptp->op_mode <= 1)
+		msg = NULL;
+	return msg;
+}  /* ptp_set_rx_info */
+
+#if 1
+/* Used for 802.1BA test tool to accept Announce messages. */
+static int ba_hack;
+#endif
+
+static struct ptp_msg *ptp_get_tx_info(struct ptp_info *ptp, u8 *data,
+	u32 *tx_port, u32 *tx_timestamp)
+{
+	struct ptp_msg *msg;
+	u32 overrides = ptp->overrides;
+	u32 timestamp = 0;
+	u8 port = 0;
+
+	/* Need to know about PTP message for queue assignment. */
+#if 0
+#if 1
+	if (!ba_hack)
+#endif
+	if ((1 == ptp->op_mode || 2 == ptp->op_mode) && !ptp->tx_msg_cnt) {
+		/*
+		 * This packet is not parsed and will be checked again if
+		 * necessary.
+		 */
+		ptp->tx_msg_parsed = false;
+		return NULL;
+	}
+#endif
+
+	/* Get receive port and timestamp inside the PTP message. */
+	if (0 == ptp->op_mode)
+		overrides |= PTP_ZERO_RESERVED_FIELD;
+	msg = update_ptp_msg(data, &port, &timestamp, overrides);
+	if (msg) {
+		/* Get transmit port and timestamp inside the PTP message. */
+		if (0 == ptp->op_mode) {
+			if (port)
+				*tx_port = port;
+			if (timestamp)
+				*tx_timestamp = timestamp;
+		}
+
+		/* Simulate passing transmit information from application. */
+		if (ptp->overrides & PTP_TEST_TX_INFO) {
+			u32 tx_ports = 0;
+
+			if (port)
+				tx_ports = port;
+			set_msg_info(ptp, &msg->hdr, tx_ports, timestamp);
+		}
+	}
+	return msg;
+}  /* ptp_get_tx_info */
+
+static void ptp_get_rx_info(struct ptp_info *ptp, u8 *data, u8 port,
+	u32 timestamp)
+{
+	int index;
+	struct ptp_msg *msg;
+	struct ptp_msg_info *rx_msg;
+	struct ptp_msg_info *info = NULL;
+
+	/* Indicate PTP message is not parsed yet. */
+	ptp->rx_msg = NULL;
+
+	/* Entry is not used for PTP message. */
+	rx_msg = &ptp->rx_msg_info[7];
+	rx_msg->data.port = port;
+	rx_msg->data.ts.timestamp = timestamp;
+	update_ts(&rx_msg->data.ts, ptp->cur_time.sec);
+
+	index = get_speed_index(ptp, port);
+	sub_nsec(&rx_msg->data.ts.t, ptp->rx_latency[port][index]);
+	timestamp = (rx_msg->data.ts.t.sec << 30) | rx_msg->data.ts.t.nsec;
+	if (ptp->overrides & PTP_CHECK_PATH_DELAY) {
+		if (ptp->last_tx_ts.t.sec) {
+			struct ksz_ptp_time diff;
+
+			calc_udiff(&ptp->last_tx_ts.t, &rx_msg->data.ts.t,
+				&diff);
+			dbg_msg("pd: %d"NL, diff.nsec);
+		} else
+			ptp->last_rx_ts = rx_msg->data.ts;
+	}
+
+	msg = ptp_set_rx_info(ptp, data, port, timestamp);
+	if (!msg)
+		return;
+
+	/* Indicate PTP message is parsed. */
+	ptp->rx_msg = msg;
+
+	rx_msg = &ptp->rx_msg_info[msg->hdr.messageType];
+	switch (msg->hdr.messageType) {
+	case SYNC_MSG:
+		if (!memcmp(&msg->hdr.sourcePortIdentity, &ptp->masterIdentity,
+		    sizeof(struct ptp_clock_identity))) {
+			rx_msg->data.port = port;
+			rx_msg->data.ts.timestamp = timestamp;
+		}
+		info = kzalloc(sizeof(struct ptp_msg_info), GFP_KERNEL);
+		break;
+
+	/* General messages that do not need tracking. */
+	case DELAY_RESP_MSG:
+	case ANNOUNCE_MSG:
+	case SIGNALING_MSG:
+	case PDELAY_RESP_FOLLOW_UP_MSG:
+		break;
+	case MANAGEMENT_MSG:
+	{
+		u8 action = msg->data.management.b.actionField;
+
+		/* No need to track management response message. */
+		if (MANAGEMENT_RESPONSE == action ||
+				MANAGEMENT_ACKNOWLEDGE == action)
+			break;
+		fallthrough;
+	}
+
+	default:
+		info = kzalloc(sizeof(struct ptp_msg_info), GFP_KERNEL);
+		break;
+	}
+	if (info) {
+		unsigned long flags;
+
+		spin_lock_irqsave(&ptp->rx_msg_lock, flags);
+		save_msg_info(ptp, info, &msg->hdr, port, timestamp);
+		info->next = rx_msg->next;
+		rx_msg->next = info;
+		spin_unlock_irqrestore(&ptp->rx_msg_lock, flags);
+	}
+}  /* ptp_get_rx_info */
+
+static uint get_port_from_bits(u32 bits)
+{
+	uint port;
+
+	port = 0;
+	while (bits) {
+		if ((bits & 1) && bits != 1) {
+			port = 0;
+			break;
+		}
+		++port;
+		bits >>= 1;
+	}
+	return port;
+}
+
+static void ptp_set_tx_info(struct ptp_info *ptp, u8 *data, void *ptr)
+{
+	struct ksz_sw *sw = ptp->parent;
+	int found;
+	bool dest;
+	bool prio;
+	struct ptp_msg *msg;
+	struct ptp_msg_options tx_msg;
+	struct ksz_sw_tx_tag *tag = ptr;
+
+	tx_msg.port = 0;
+	tx_msg.ts.timestamp = 0;
+
+	/* Assume packet will be parsed to determine PTP message type. */
+	ptp->tx_msg_parsed = true;
+	ptp->tx_msg = ptp_get_tx_info(ptp, data, &tx_msg.port,
+		&tx_msg.ts.timestamp);
+	dest = false;
+	if (get_tx_tag_ports(sw, tag))
+		dest = true;
+	prio = dest;
+	if (!ptp->tx_msg) {
+
+		/* Block PTP messages for blocked ports. */
+		if ((sw->features & USE_802_1X_AUTH) && !dest) {
+			if (!ptp->tx_msg_parsed) {
+				ptp->tx_msg = check_ptp_msg(data, NULL);
+				ptp->tx_msg_parsed = true;
+			}
+			if (ptp->tx_msg)
+				set_tx_tag_ports(tag, sw->on_ports);
+		}
+
+		/* Remember transmit ports for transmit timestamp report. */
+		ptp->tx_ports = get_tx_tag_ports(sw, tag);
+		return;
+	}
+	msg = ptp->tx_msg;
+
+	/* Check whether application sets transmit information using API. */
+	found = find_msg_info(&ptp->tx_msg_info[msg->hdr.messageType],
+		&ptp->tx_msg_lock, &msg->hdr, &msg->hdr.sourcePortIdentity,
+		true, &tx_msg);
+	if (found) {
+		uint port;
+		u32 bits;
+		struct ptp_tx_ts *tx = NULL;
+
+		if (tx_msg.port)
+			bits = tx_msg.port;
+		else
+			bits = get_tx_tag_ports(sw, tag);
+		bits &= sw->PORT_MASK;
+		port = get_port_from_bits(bits);
+		switch (msg->hdr.messageType) {
+		case SYNC_MSG:
+			if (port)
+				tx = &ptp->tx_sync[port - 1];
+			break;
+		case DELAY_REQ_MSG:
+			if (port)
+				tx = &ptp->tx_dreq[port - 1];
+			break;
+		case PDELAY_REQ_MSG:
+			if (port)
+				tx = &ptp->tx_dreq[port - 1];
+			break;
+		case PDELAY_RESP_MSG:
+			if (port) {
+				tx = &ptp->tx_resp[port - 1];
+				if (ptp->need_2_step_resp_help &&
+				    msg->hdr.flagField.flag.twoStepFlag)
+					ptp->pdelay_resp_timestamp[port - 1] =
+						tx_msg.ts.timestamp;
+			}
+			break;
+		default:
+			tx = NULL;
+		}
+		if (tx) {
+			tx->ts.timestamp = 0;
+			tx->req_time = 0;
+			tx->hdr.messageType = 7;
+		}
+		found = 2;
+		if (ptp->tx_msg_cnt > 0)
+			ptp->tx_msg_cnt--;
+	}
+
+	/* Check whether a default port is set.  Only used in testing. */
+	if (!found && (ptp->overrides & PTP_USE_DEFAULT_PORT)) {
+		tx_msg.port = ptp->tx_msg_info[7].data.port;
+		tx_msg.ts.timestamp = ptp->tx_msg_info[7].data.ts.timestamp;
+		found = 2;
+	}
+	if (PDELAY_REQ_MSG == msg->hdr.messageType) {
+		if (!(ptp->mode & PTP_TC_P2P)) {
+			/* Not fast enough to receive the response. */
+			schedule_work(&ptp->set_p2p);
+		}
+		if (ptp->need_peer_delay_set_help) {
+			uint port;
+			u32 bits;
+
+			bits = get_tx_tag_ports(sw, tag);
+			bits &= sw->PORT_MASK;
+			port = get_port_from_bits(bits);
+			if (port)
+				ptp_save_peer_delay(ptp, port - 1, msg);
+		}
+	}
+
+	/* Only PDELAY_RESP_MSG requires timestamp in transmission. */
+	if (!found && PDELAY_RESP_MSG == msg->hdr.messageType) {
+		int two_step = msg->hdr.flagField.flag.twoStepFlag;
+		struct ptp_msg_pdelay_resp *resp = &msg->data.pdelay_resp;
+
+		found = find_msg_info(&ptp->rx_msg_info[PDELAY_REQ_MSG],
+			&ptp->rx_msg_lock, &msg->hdr,
+			&resp->requestingPortIdentity, !two_step, &tx_msg);
+
+		/* Sending 2-step Pdelay_Resp in 1-step clock needs to fill in
+		 * correct received Pdelay_Req timestamp.
+		 */
+		if (ptp->need_2_step_resp_help)
+			two_step = false;
+
+		/* Need to specify timestamp in 1-step mode. */
+		if (!two_step && found) {
+			struct ptp_ts ts;
+
+			/* Calculate timestamp automatically. */
+			ts = tx_msg.ts;
+			tx_msg.ts.timestamp = (ts.t.sec << 30) | ts.t.nsec;
+		}
+
+		/* Simulate two-step operation while using 1-step clock. */
+		if (ptp->need_2_step_resp_help &&
+		    msg->hdr.flagField.flag.twoStepFlag) {
+			uint port;
+			u32 bits;
+
+			bits = get_tx_tag_ports(sw, tag);
+			bits &= sw->PORT_MASK;
+			port = get_port_from_bits(bits);
+			if (port)
+				ptp->pdelay_resp_timestamp[port - 1] =
+					tx_msg.ts.timestamp;
+		}
+	}
+	if (ba_hack) {
+		if (msg->hdr.messageType == ANNOUNCE_MSG &&
+		    memcmp(&data[6], sw->info->mac_addr, ETH_ALEN))
+			memcpy(&data[6], sw->info->mac_addr, ETH_ALEN);
+	}
+
+	tag->timestamp = tx_msg.ts.timestamp;
+
+	/* Specific ports are specified. */
+	if (dest)
+		goto set_tx_info_done;
+
+	/* No need to set outgoing port for unicast message. */
+	if (msg->hdr.flagField.flag.unicastFlag && !(data[0] & 1))
+		goto set_tx_info_done;
+
+	if (found || tx_msg.port) {
+		if (tx_msg.port) {
+			uint ports;
+
+			if (1 == found)
+				ports = (1 << tx_msg.port);
+			else
+				ports = tx_msg.port;
+			if (ports)
+				prio = true;
+			set_tx_tag_ports(tag, ports);
+		}
+		goto set_tx_info_done;
+	} else if (ptp->op_mode != 3)
+		goto set_tx_info_done;
+
+	/* Automatically find a port to send. */
+	switch (msg->hdr.messageType) {
+	case DELAY_REQ_MSG:
+		if (ptp->rx_msg_info[SYNC_MSG].data.ts.timestamp) {
+			tx_msg.port = ptp->rx_msg_info[SYNC_MSG].data.port;
+			found = true;
+		}
+		break;
+	case PDELAY_RESP_MSG:
+		/* Already determined from previous code. */
+		break;
+	case PDELAY_RESP_FOLLOW_UP_MSG:
+		found = find_msg_info(&ptp->rx_msg_info[PDELAY_REQ_MSG],
+			&ptp->rx_msg_lock, &msg->hdr, &msg->data.
+			pdelay_resp_follow_up.requestingPortIdentity,
+			true, &tx_msg);
+		break;
+	case DELAY_RESP_MSG:
+		found = find_msg_info(&ptp->rx_msg_info[DELAY_REQ_MSG],
+			&ptp->rx_msg_lock, &msg->hdr, &msg->data.
+			delay_resp.requestingPortIdentity,
+			true, &tx_msg);
+		break;
+	case MANAGEMENT_MSG:
+	{
+		u8 action = msg->data.management.b.actionField;
+
+		/* Not response to management request message. */
+		if (MANAGEMENT_GET == action || MANAGEMENT_SET == action ||
+		    MANAGEMENT_COMMAND == action)
+			break;
+
+		found = find_msg_info(&ptp->rx_msg_info[MANAGEMENT_MSG],
+			&ptp->rx_msg_lock, &msg->hdr, &msg->data.
+			management.b.targetPortIdentity,
+			true, &tx_msg);
+		break;
+	}
+	default:
+		break;
+	}
+
+	if (found)
+		set_tx_tag_ports(tag, (1 << tx_msg.port));
+dbg_msg("  tx m:%x f:%d p:%x"NL, msg->hdr.messageType, found,
+get_tx_tag_ports(sw, tag));
+
+set_tx_info_done:
+	if ((sw->features & USE_802_1X_AUTH) && !dest) {
+		set_tx_tag_ports(tag, sw->on_ports);
+	}
+
+	/* Remember transmit ports for transmit timestamp report. */
+	ptp->tx_ports = get_tx_tag_ports(sw, tag);
+
+	/* Need destination ports for queue assignment to work. */
+	if (prio)
+		set_tx_tag_queue(sw, tag, sw->ctrl_queue);
+
+	do {
+		uint m;
+		uint n;
+		uint p;
+		u32 ports;
+		u32 intr;
+		struct ptp_tx_ts *tx;
+		struct ptp_tx_ts *xtx;
+
+		/* Not PTP event message. */
+		if (msg->hdr.messageType & 0x8)
+			break;
+		m = sw->PORT_MASK & ~sw->HOST_MASK;
+		if (!dest)
+			ports = m;
+		else
+			ports = ptp->tx_ports & m;
+		if (SYNC_MSG == msg->hdr.messageType) {
+			xtx = ptp->tx_sync;
+			intr = PTP_PORT_SYNC_INT;
+		} else if (PDELAY_RESP_MSG == msg->hdr.messageType) {
+			xtx = ptp->tx_resp;
+			intr = PTP_PORT_PDELAY_RESP_INT;
+		} else {
+			xtx = ptp->tx_dreq;
+			intr = PTP_PORT_XDELAY_REQ_INT;
+		}
+
+		/* Transmit timestamps are not retrieved. */
+		if (!(ptp->tx_intr & intr))
+			break;
+		for (n = 1; n <= ptp->ports; n++) {
+			p = get_phy_port(sw, n);
+			if (!(ports & (1 << p)))
+				continue;
+			tx = &xtx[p];
+			memcpy(&tx->hdr, &msg->hdr,
+				sizeof(struct ptp_msg_hdr));
+
+			/* Clear previous timestamp if any exists. */
+			tx->ts.timestamp = 0;
+		}
+	} while (0);
+}  /* ptp_set_tx_info */
+
+static void proc_ptp_get_cfg(struct ptp_info *ptp, u8 *data)
+{
+	struct ptp_cfg_options *cmd = (struct ptp_cfg_options *) data;
+	struct ksz_sw *sw = ptp->parent;
+
+	ptp->ops->acquire(ptp);
+	ptp->mode = sw->reg->r16(sw, REG_PTP_MSG_CONF1);
+	ptp->cfg = sw->reg->r16(sw, REG_PTP_MSG_CONF2);
+	ptp->domain = sw->reg->r16(sw, REG_PTP_DOMAIN_VERSION) & PTP_DOMAIN_M;
+	cmd->priority = !!sw->ops->chk(sw, REG_PTP_EVENT_PRIO_CTRL,
+				       PTP_PRIO_ENABLE);
+	ptp->ops->release(ptp);
+
+	/* Check mode in case the switch is reset outside of driver control. */
+	if (!(ptp->tx_en & 1) && ptp->mode != ptp->def_mode && ptp->started) {
+		dbg_msg("mode mismatched: %04x %04x; %04x %04x"NL,
+			ptp->mode, ptp->def_mode, ptp->cfg, ptp->def_cfg);
+		ptp->mode = ptp->def_mode;
+		ptp->cfg = ptp->def_cfg;
+		ptp->reg->start(ptp, false);
+	}
+	cmd->two_step = (ptp->mode & PTP_1STEP) ? 0 : 1;
+	cmd->master = (ptp->mode & PTP_MASTER) ? 1 : 0;
+	cmd->p2p = (ptp->mode & PTP_TC_P2P) ? 1 : 0;
+	cmd->as = (ptp->mode & PTP_802_1AS) ? 1 : 0;
+	cmd->unicast = (ptp->cfg & PTP_UNICAST_ENABLE) ? 1 : 0;
+	cmd->alternate = (ptp->cfg & PTP_ALTERNATE_MASTER) ? 1 : 0;
+	cmd->domain_check = (ptp->cfg & PTP_DOMAIN_CHECK) ? 1 : 0;
+	cmd->udp_csum = (ptp->cfg & PTP_UDP_CHECKSUM) ? 1 : 0;
+	cmd->delay_assoc = (ptp->cfg & PTP_DELAY_CHECK) ? 1 : 0;
+	cmd->pdelay_assoc = (ptp->cfg & PTP_PDELAY_CHECK) ? 1 : 0;
+	cmd->sync_assoc = (ptp->cfg & PTP_SYNC_CHECK) ? 1 : 0;
+	cmd->drop_sync = (ptp->cfg & PTP_DROP_SYNC_DELAY_REQ) ? 1 : 0;
+	cmd->reserved = ptp->started;
+	cmd->domain = ptp->domain;
+	cmd->access_delay = ptp->get_delay;
+}  /* proc_ptp_get_cfg */
+
+static int proc_ptp_set_cfg(struct ptp_info *ptp, u8 *data)
+{
+	struct ptp_cfg_options *cmd = (struct ptp_cfg_options *) data;
+	u16 cfg;
+	u16 mode;
+	u8 domain;
+	struct ksz_sw *sw = ptp->parent;
+
+	mode = ptp->mode;
+	cfg = ptp->cfg;
+	domain = ptp->domain;
+	if (cmd->domain_set) {
+		domain = cmd->domain;
+	} else {
+		if (cmd->two_step_set) {
+			if (cmd->two_step)
+				ptp->mode &= ~PTP_1STEP;
+			else
+				ptp->mode |= PTP_1STEP;
+		}
+		if (cmd->master_set) {
+			if (cmd->master)
+				ptp->mode |= PTP_MASTER;
+			else
+				ptp->mode &= ~PTP_MASTER;
+		}
+		if (cmd->p2p_set) {
+			if (cmd->p2p)
+				ptp->mode |= PTP_TC_P2P;
+			else
+				ptp->mode &= ~PTP_TC_P2P;
+		}
+		if (cmd->as_set) {
+			if (cmd->as)
+				ptp->mode |= PTP_802_1AS;
+			else
+				ptp->mode &= ~PTP_802_1AS;
+		}
+		if (cmd->unicast_set) {
+			if (cmd->unicast)
+				ptp->cfg |= PTP_UNICAST_ENABLE;
+			else
+				ptp->cfg &= ~PTP_UNICAST_ENABLE;
+		}
+		if (cmd->alternate_set) {
+			if (cmd->alternate)
+				ptp->cfg |= PTP_ALTERNATE_MASTER;
+			else
+				ptp->cfg &= ~PTP_ALTERNATE_MASTER;
+		}
+		if (cmd->domain_check_set) {
+			if (cmd->domain_check)
+				ptp->cfg |= PTP_DOMAIN_CHECK;
+			else
+				ptp->cfg &= ~PTP_DOMAIN_CHECK;
+		}
+		if (cmd->udp_csum_set) {
+			if (cmd->udp_csum)
+				ptp->cfg |= PTP_UDP_CHECKSUM;
+			else
+				ptp->cfg &= ~PTP_UDP_CHECKSUM;
+		}
+		if (cmd->delay_assoc_set) {
+			if (cmd->delay_assoc)
+				ptp->cfg |= PTP_DELAY_CHECK;
+			else
+				ptp->cfg &= ~PTP_DELAY_CHECK;
+		}
+		if (cmd->pdelay_assoc_set) {
+			if (!(ptp->mode & PTP_1STEP))
+				cmd->pdelay_assoc = 0;
+			if (cmd->pdelay_assoc)
+				ptp->cfg |= PTP_PDELAY_CHECK;
+			else
+				ptp->cfg &= ~PTP_PDELAY_CHECK;
+		}
+		if (cmd->sync_assoc_set) {
+			if (cmd->sync_assoc)
+				ptp->cfg |= PTP_SYNC_CHECK;
+			else
+				ptp->cfg &= ~PTP_SYNC_CHECK;
+		}
+		if (cmd->drop_sync_set) {
+			if (cmd->drop_sync)
+				ptp->cfg |= PTP_DROP_SYNC_DELAY_REQ;
+			else
+				ptp->cfg &= ~PTP_DROP_SYNC_DELAY_REQ;
+		}
+		if (cmd->priority_set) {
+			ptp->ops->acquire(ptp);
+			sw->ops->cfg(sw, REG_PTP_EVENT_PRIO_CTRL,
+				     PTP_PRIO_ENABLE, cmd->priority);
+			sw->ops->cfg(sw, REG_PTP_GENERAL_PRIO_CTRL,
+				     PTP_PRIO_ENABLE, cmd->priority);
+			ptp->ops->release(ptp);
+		}
+	}
+	ptp->ops->acquire(ptp);
+	if (mode != ptp->mode) {
+		u16 old = mode;
+
+		/* Switch variables so that the new one is set. */
+		mode = ptp->mode;
+		ptp->mode = old;
+		set_ptp_mode(ptp, mode);
+		ptp->def_mode = mode;
+	}
+	if (cfg != ptp->cfg) {
+		dbg_msg("cfg: %x %x"NL, cfg, ptp->cfg);
+		sw->reg->w16(sw, REG_PTP_MSG_CONF2, ptp->cfg);
+	}
+	if (domain != ptp->domain) {
+		ptp->domain = domain;
+		set_ptp_domain(ptp, ptp->domain);
+	}
+	ptp->ops->release(ptp);
+	return 0;
+}  /* proc_ptp_set_cfg */
+
+static void cancel_rx_unit(struct ptp_info *ptp, int tsi)
+{
+	int first;
+	int last;
+	int tsi_bit;
+	struct ptp_event *events;
+
+	ptp->ops->acquire(ptp);
+	first = tsi;
+	events = &ptp->events[tsi];
+	if (events->last) {
+		first = events->first;
+		last = events->last;
+	} else
+		last = first + 1;
+	tsi = first;
+	ptp->events[tsi].timeout = 0;
+	do {
+		if (tsi >= MAX_TIMESTAMP_UNIT)
+			tsi = 0;
+		events = &ptp->events[tsi];
+		events->first = 0;
+		events->last = 0;
+		tsi_bit = 1 << tsi;
+		if (ptp->tsi_used & tsi_bit) {
+			if (events->num < events->max) {
+				ptp->reg->read_event(ptp, tsi);
+				ptp->ts_status = 0;
+			}
+			ptp->reg->rx_off(ptp, tsi);
+			ptp->tsi_intr &= ~tsi_bit;
+			ptp->tsi_used &= ~tsi_bit;
+			if (ptp->tsi_sys & tsi_bit) {
+				printk(KERN_INFO "tsi %d off!"NL, tsi);
+				ptp->tsi_sys &= ~tsi_bit;
+				ptp->update_sec_jiffies = jiffies;
+				schedule_delayed_work(&ptp->update_sec,
+						      msecs_to_jiffies(1000));
+			}
+			ptp->tsi_dev[tsi] = NULL;
+		}
+		++tsi;
+	} while (tsi != last);
+	ptp->ops->release(ptp);
+}  /* cancel_rx_unit */
+
+static int check_expired_rx_unit(struct ptp_info *ptp, int tsi)
+{
+	int first;
+	u32 expired;
+	struct ptp_event *events;
+	struct ksz_ptp_time diff;
+	struct ptp_utime t;
+
+	events = &ptp->events[tsi];
+	first = tsi;
+	if (events->last)
+		first = events->first;
+	events = &ptp->events[first];
+	if (events->num && events->timeout) {
+		ptp->ops->acquire(ptp);
+		ptp->reg->get_time(ptp, &t);
+		ptp->ops->release(ptp);
+		calc_udiff(events->t, &t, &diff);
+		if (diff.sec >= 0 && diff.nsec >= 0) {
+			expired = diff.sec * 1000 + diff.nsec / 1000000;
+			expired = expired * HZ / 1000;
+			if (expired > events->timeout) {
+				cancel_rx_unit(ptp, first);
+				return 1;
+			}
+		}
+	}
+	return 0;
+}  /* check_expired_rx_unit */
+
+static int proc_dev_rx_event(struct file_dev_info *info, u8 *data)
+{
+	struct ptp_info *ptp = info->dev;
+	struct ptp_tsi_options *cmd = (struct ptp_tsi_options *) data;
+	u8 event;
+	int first;
+	int i;
+	int intr;
+	int tsi;
+	int avail;
+	int total;
+	int last;
+	int tsi_bit;
+	struct ptp_event *events;
+
+	tsi = cmd->tsi;
+	total = cmd->total;
+	if (!total)
+		total = 1;
+	first = tsi;
+
+	/* Cancel operation. */
+	if ((cmd->flags & PTP_CMD_CANCEL_OPER)) {
+		if (tsi >= MAX_TIMESTAMP_UNIT)
+			return DEV_IOC_UNIT_UNAVAILABLE;
+		cancel_rx_unit(ptp, tsi);
+		goto proc_ptp_rx_cascade_event_done;
+	}
+	if (tsi >= MAX_TIMESTAMP_UNIT) {
+		first = 0;
+		do {
+			for (tsi = first; tsi < MAX_TIMESTAMP_UNIT; tsi++)
+				if (!(ptp->tsi_used & (1 << tsi)) &&
+						!ptp->events[tsi].last)
+					break;
+			if (tsi >= MAX_TIMESTAMP_UNIT)
+				return DEV_IOC_UNIT_UNAVAILABLE;
+			first = tsi;
+			avail = 1;
+			for (i = 1; i < total; i++)
+				if (!(ptp->tsi_used & (1 << tsi)) &&
+						!ptp->events[tsi].last) {
+					++avail;
+					++tsi;
+					if (tsi >= MAX_TIMESTAMP_UNIT)
+						tsi = 0;
+				} else {
+					++first;
+					break;
+				}
+		} while (avail < total);
+	} else {
+		for (i = 0; i < total; i++) {
+			if (ptp->tsi_used & (1 << tsi) ||
+					ptp->events[tsi].last)
+				if (!check_expired_rx_unit(ptp, tsi))
+					return DEV_IOC_UNIT_USED;
+			++tsi;
+			if (tsi >= MAX_TIMESTAMP_UNIT)
+				tsi = 0;
+		}
+	}
+	if (cmd->gpi >= MAX_GPIO)
+		return -EINVAL;
+	if (0 == cmd->event)
+		event = DETECT_FALL;
+	else if (1 == cmd->event)
+		event = DETECT_RISE;
+	else {
+		event = DETECT_RISE | DETECT_FALL;
+		cmd->event = 2;
+	}
+	tsi = first;
+	last = first + total;
+	if (last > MAX_TIMESTAMP_UNIT)
+		last -= MAX_TIMESTAMP_UNIT;
+	intr = cmd->flags & PTP_CMD_INTR_OPER;
+	ptp->ops->acquire(ptp);
+	for (i = 0; i < total; i++) {
+		tsi_bit = 1 << tsi;
+		ptp->tsi_used |= tsi_bit;
+
+		/* Interrupt means notification. */
+		if (intr) {
+
+			/* Not silent means every notification. */
+			if (!(cmd->flags & PTP_CMD_SILENT_OPER))
+				ptp->tsi_intr |= tsi_bit;
+			if (cmd->timeout || (ptp->tsi_intr & tsi_bit))
+				ptp->tsi_dev[tsi] = info;
+		}
+		events = &ptp->events[tsi];
+		events->num = 0;
+		events->event = cmd->event;
+		events->edge = 0;
+		events->expired = 0;
+		if (total > 1) {
+			events->first = first;
+			events->last = last;
+		}
+		++tsi;
+		if (tsi >= MAX_TIMESTAMP_UNIT)
+			tsi = 0;
+	}
+	tsi = first;
+	ptp->events[tsi].timeout = cmd->timeout * HZ / 1000;
+
+	/* Zero timeout means repeatable. */
+	if (!ptp->events[tsi].timeout && cmd->timeout)
+		ptp->events[tsi].timeout = 1;
+	if (total > 1)
+		ptp->reg->rx_cascade_event(ptp, tsi, total, cmd->gpi, event,
+			intr);
+	else
+		ptp->reg->rx_event(ptp, tsi, cmd->gpi, event, intr);
+	ptp->ops->release(ptp);
+
+proc_ptp_rx_cascade_event_done:
+	*data = tsi;
+	return 0;
+}  /* proc_dev_rx_event */
+
+static int find_avail_tx_unit(struct ptp_info *ptp, int total, int *unit)
+{
+	int avail;
+	int first;
+	int i;
+	int tso;
+
+	first = 0;
+	do {
+		for (tso = first; tso < MAX_TRIG_UNIT; tso++)
+			if (!(ptp->tso_used & (1 << tso)))
+				break;
+		if (tso >= MAX_TRIG_UNIT)
+			return DEV_IOC_UNIT_UNAVAILABLE;
+		first = tso++;
+		avail = 1;
+		for (i = 1; i < total; i++) {
+			if (tso >= MAX_TRIG_UNIT)
+				return DEV_IOC_UNIT_UNAVAILABLE;
+			if (!(ptp->tso_used & (1 << tso))) {
+				++avail;
+				++tso;
+			} else {
+				++first;
+				break;
+			}
+		}
+	} while (avail < total);
+	*unit = first;
+	return 0;
+}  /* find_avail_tx_unit */
+
+/* Actual time should be more than 80 ms in situations where driver needs to
+ * report event times and they are displayed and thus affect the driver
+ * execution to process the interrupt in timely fashion.
+ * Driver execution is only needed for more than 1 second duration anyway.
+ * Exception is doing TRIG_NEG_PERIOD in cascade mode as users want 20/40.
+ */
+#ifndef TEST_SW_EXEC
+#define MIN_TIME_LEFT_IN_CYCLE_IBA  80000000
+#define MIN_TIME_LEFT_IN_CYCLE_SPI  80000000
+#else
+/* 10 ms */
+#define MIN_TIME_LEFT_IN_CYCLE_IBA  10000000
+/* 15 ms */
+#define MIN_TIME_LEFT_IN_CYCLE_SPI  15000000
+#endif
+#define MIN_TIME_LEFT_IN_CYCLE      20000000
+
+static int proc_dev_tx_event(struct file_dev_info *info, u8 *data)
+{
+	struct ptp_info *ptp = info->dev;
+	struct ptp_tso_options *cmd = (struct ptp_tso_options *) data;
+	int gpo;
+	int sw_oper;
+	int intr;
+	int tso;
+	int tso_bit;
+	struct ptp_utime t;
+	u64 iterate;
+	u16 active;
+	u32 status;
+	int err = 0;
+	struct ksz_sw *sw = ptp->parent;
+
+	gpo = cmd->gpo;
+	if (gpo >= MAX_GPIO)
+		return -EINVAL;
+	if (cmd->event > TRIG_REG_OUTPUT)
+		return -EINVAL;
+	tso = cmd->tso;
+	tso_bit = 1 << tso;
+
+	/* Cancel operation. */
+	if ((cmd->flags & PTP_CMD_CANCEL_OPER)) {
+		if (tso >= MAX_TRIG_UNIT)
+			return DEV_IOC_UNIT_UNAVAILABLE;
+		ptp->ops->acquire(ptp);
+
+		/* Reset the tso. */
+		ptp->cascade_tx |= tso_bit;
+		ptp_tso_off(ptp, tso, tso_bit);
+		ptp->ops->release(ptp);
+		goto proc_dev_tx_event_done;
+	}
+	if (ptp->cascade && (tso < ptp->cascade_gpo[gpo].first ||
+			tso >= ptp->cascade_gpo[gpo].first +
+			ptp->cascade_gpo[gpo].total))
+		return DEV_IOC_UNIT_UNAVAILABLE;
+
+	/* Find available unit for use. */
+	if (tso >= MAX_TRIG_UNIT) {
+		int rc = find_avail_tx_unit(ptp, 1, &tso);
+
+		if (rc)
+			return rc;
+	} else if (!ptp->cascade && (ptp->tso_used & tso_bit)) {
+
+		/* See whether previous operation is completed. */
+		ptp->ops->acquire(ptp);
+		ptp_write_index(ptp, PTP_TOU_INDEX_S, tso);
+		active = sw->reg->r32(sw, REG_PTP_CTRL_STAT__4);
+		status = sw->reg->r32(sw, REG_PTP_TRIG_STATUS__4);
+		ptp->ops->release(ptp);
+		if (active & TRIG_ACTIVE) {
+			u16 error = (status >> TRIG_ERROR_S) &
+				PTP_TRIG_UNIT_M;
+
+			if (!(error & tso_bit))
+				return DEV_IOC_UNIT_USED;
+			dbg_msg("trig err: %d"NL, tso);
+		}
+		if (!(active & TRIG_ACTIVE)) {
+			u16 done = status & PTP_TRIG_UNIT_M;
+
+			if (!(done & tso_bit)) {
+				/* Reset the unit. */
+				ptp->cascade_tx |= tso_bit;
+				dbg_msg(" !? trig done: %d"NL, tso);
+			}
+		}
+		ptp->ops->acquire(ptp);
+		ptp_tso_off(ptp, tso, tso_bit);
+		ptp->ops->release(ptp);
+	}
+	ptp->ops->acquire(ptp);
+	if (!ptp->cascade && (cmd->flags & PTP_CMD_REL_TIME) &&
+			cmd->sec < 100) {
+		struct ptp_utime s;
+
+		ptp->reg->get_time(ptp, &t);
+		s.sec = cmd->sec + t.sec;
+		s.nsec = cmd->nsec;
+		update_next_trigger(&t, &s, 0);
+		cmd->sec = s.sec;
+	}
+	intr = cmd->flags & PTP_CMD_INTR_OPER;
+	if (intr & !(cmd->flags & PTP_CMD_SILENT_OPER)) {
+		ptp->tso_intr |= tso_bit;
+		ptp->tso_dev[tso] = info;
+	}
+
+	/* Want software to simulate period signal. */
+	sw_oper = cmd->flags & PTP_CMD_SW_OPER;
+	if (sw_oper && cmd->event >= TRIG_NEG_PERIOD) {
+		int min = MIN_TIME_LEFT_IN_CYCLE_IBA;
+		u32 pulse = cmd->pulse * 8;
+		int diff;
+
+		/* No pulse for bit pattern. */
+		if (cmd->event == TRIG_REG_OUTPUT)
+			pulse = 0;
+		diff = cmd->cycle - pulse;
+		if (ptp->delay_ticks > 10)
+			min = MIN_TIME_LEFT_IN_CYCLE_SPI;
+		if (ptp->cascade && cmd->event == TRIG_NEG_PERIOD &&
+		    min > MIN_TIME_LEFT_IN_CYCLE)
+			min = MIN_TIME_LEFT_IN_CYCLE;
+		if (diff < min)
+			sw_oper = 0;
+dbg_msg(" sw_oper: %d\n", !!sw_oper);
+	}
+	ptp->tso_used |= tso_bit;
+
+	iterate = get_iterate(cmd);
+	ptp_core_tx_event(ptp, tso, cmd->gpo, cmd->event, cmd->pulse,
+		cmd->cycle, cmd->cnt, cmd->sec, cmd->nsec, iterate, intr,
+		!(cmd->flags & PTP_CMD_ON_TIME),
+		(cmd->flags & PTP_CMD_CLK_OPT), sw_oper);
+	if (cmd->flags & PTP_CMD_ON_TIME) {
+		status = sw->reg->r32(sw, REG_PTP_TRIG_STATUS__4);
+		status = (status >> TRIG_ERROR_S) & PTP_TRIG_UNIT_M;
+		if (status & tso_bit)
+			err = DEV_IOC_UNIT_ERROR;
+	}
+	ptp->ops->release(ptp);
+
+proc_dev_tx_event_done:
+	*data = tso;
+	return err;
+}  /* proc_dev_tx_event */
+
+static int proc_ptp_tx_cascade_init(struct ptp_info *ptp, u8 *data)
+{
+	struct ptp_tso_options *cmd = (struct ptp_tso_options *) data;
+	int first;
+	int gpo;
+	int i;
+	int tso;
+	int total;
+	u32 status;
+	struct ksz_sw *sw = ptp->parent;
+
+	/* Sanity check on parameters. */
+	tso = cmd->tso;
+	gpo = cmd->gpo;
+	total = cmd->total;
+	if (!total)
+		return -EINVAL;
+	i = tso + total - 1;
+	if (i >= MAX_TRIG_UNIT)
+		return -EINVAL;
+	if (gpo >= MAX_GPIO)
+		return -EINVAL;
+	first = tso;
+
+	/* Cancel operation. */
+	if ((cmd->flags & PTP_CMD_CANCEL_OPER)) {
+		if (tso >= MAX_TRIG_UNIT)
+			return DEV_IOC_UNIT_UNAVAILABLE;
+		if (first != ptp->cascade_gpo[gpo].first ||
+		    (total != ptp->cascade_gpo[gpo].total &&
+		     ptp->cascade_gpo[gpo].total)) {
+			first = ptp->cascade_gpo[gpo].first;
+			total = ptp->cascade_gpo[gpo].total;
+		}
+
+		/* Reset the last unit in case it is used to raise the level. */
+		if (!ptp->cascade_sw_only)
+			first = first + total - 1;
+		if (ptp->outputs[first].level) {
+			ptp->cascade_tx |= (1 << first);
+			ptp->tso_used |= (1 << first);
+			ptp->outputs[first].level = 0;
+		}
+		ptp->ops->acquire(ptp);
+		for (i = 0; i < total; i++, tso++) {
+			if (ptp->tso_used & (1 << tso))
+				ptp_tso_off(ptp, tso, (1 << tso));
+		}
+		tso = total;
+		ptp->cascade = 0;
+		ptp->cascade_sw_each = 0;
+		ptp->cascade_sw_only = 0;
+		ptp->cascade_total = 0;
+		ptp->tso_cnt[MAX_TRIG_UNIT] = 0;
+		ptp->ops->release(ptp);
+		goto proc_ptp_tx_cascade_init_done;
+	}
+
+	if (ptp->cascade)
+		return DEV_IOC_UNIT_UNAVAILABLE;
+
+	/* Only support one software cascade execution. */
+	if (ptp->tso_cnt[MAX_TRIG_UNIT] || ptp->cascade_sw_each)
+		return DEV_IOC_UNIT_UNAVAILABLE;
+
+	/* Find available units for use. */
+	if (tso >= MAX_TRIG_UNIT) {
+		int rc = find_avail_tx_unit(ptp, total, &first);
+
+		if (rc)
+			return rc;
+	} else {
+		for (i = 0; i < total; i++) {
+			if (tso >= MAX_TRIG_UNIT)
+				return DEV_IOC_UNIT_UNAVAILABLE;
+			if (ptp->tso_used & (1 << tso))
+				return DEV_IOC_UNIT_USED;
+			++tso;
+		}
+	}
+
+	if ((cmd->flags & PTP_CMD_CASCADE_RESET_OPER))
+		goto proc_ptp_tx_cascade_init_set;
+
+	/* Last operation was not in cascade mode. */
+	if (!ptp->cascade_gpo[gpo].total)
+		goto proc_ptp_tx_cascade_init_set;
+
+	/* previous last unit. */
+	i = ptp->cascade_gpo[gpo].first + ptp->cascade_gpo[gpo].total - 1;
+
+	/* current last unit. */
+	tso = first + total - 1;
+
+	/* Probably using software to execute cascade operation. */
+	if (!!(ptp->cascade_tx & (1 << (i + 16))) != ptp->outputs[i].level)
+dbg_msg("cascade_tx: %d %x %d\n", i, ptp->cascade_tx, ptp->outputs[i].level);
+
+	/* Last operation not ended high. */
+	if (!ptp->outputs[i].level ||
+	    !(ptp->cascade_tx & (1 << (i + 16))))
+		goto proc_ptp_tx_cascade_init_set;
+
+	/* Always need reset for last unit in cascade mode unless it is used
+	 * as the first one in new cascade mode.
+	 */
+	ptp->ops->acquire(ptp);
+	ptp_write_index(ptp, PTP_GPIO_INDEX_S, gpo);
+	status = sw->reg->r32(sw, REG_PTP_CTRL_STAT__4);
+
+	/* Current level is high. */
+	if (status & GPIO_IN) {
+		if (i == first) {
+			/* Do not reset unit when started. */
+			ptp->cascade_tx &= ~(1 << (i + 16));
+		} else if (total > 1) {
+			bool again = false;
+
+			/* Cannot use the last unit to hold the signal. */
+			if (tso == i) {
+				tso = first;
+				again = true;
+			}
+
+			/* Set unit to hold the level high. */
+			/* Need at least 1 second to execute the command. */
+			ptp_core_tx_event(ptp, tso, gpo, TRIG_POS_EDGE,
+					  0, 0, 0, 1, 0, 0,
+					  PTP_CMD_INTR_OPER, 1, 0, 0);
+
+			/* Release the signal from the previous last unit. */
+			ptp_gpo_reset(ptp, ptp->outputs[i].gpo, i, NULL);
+			if (again) {
+				ptp_core_tx_event(ptp, i, gpo, TRIG_POS_EDGE,
+						  0, 0, 0, 1, 0, 0,
+						  PTP_CMD_INTR_OPER, 1, 0, 0);
+			}
+		}
+	}
+	ptp->ops->release(ptp);
+
+proc_ptp_tx_cascade_init_set:
+	ptp->cascade = 1;
+	ptp->cascade_gpo[gpo].first = first;
+	ptp->cascade_gpo[gpo].total = total;
+	tso = first;
+
+proc_ptp_tx_cascade_init_done:
+	*data = tso;
+	return 0;
+}  /* proc_ptp_tx_cascade_init */
+
+static int proc_ptp_tx_cascade(struct ptp_info *ptp, u8 *data)
+{
+	struct ptp_tso_options *cmd = (struct ptp_tso_options *) data;
+	struct ptp_utime t;
+	int rc = 0;
+	int gpo;
+	int tso;
+	int total;
+
+	gpo = cmd->gpo;
+	if (gpo >= MAX_GPIO)
+		return -EINVAL;
+	tso = cmd->tso;
+	total = cmd->total;
+	if (!ptp->cascade || tso != ptp->cascade_gpo[gpo].first ||
+			total != ptp->cascade_gpo[gpo].total)
+		return DEV_IOC_UNIT_UNAVAILABLE;
+
+	/* Cancel operation. */
+	if ((cmd->flags & PTP_CMD_CANCEL_OPER)) {
+		proc_ptp_tx_cascade_init(ptp, data);
+		goto proc_ptp_tx_cascade_done;
+	}
+
+	ptp->ops->acquire(ptp);
+	if ((cmd->flags & PTP_CMD_REL_TIME) && cmd->sec < 100) {
+		struct ptp_output *cur = &ptp->outputs[tso];
+		struct ptp_utime s;
+
+		ptp->reg->get_time(ptp, &t);
+		s.sec = cmd->sec + cur->trig.sec + t.sec;
+		s.nsec = cmd->nsec + cur->trig.nsec;
+		update_next_trigger(&t, &s, 0);
+		cmd->sec = s.sec - cur->trig.sec;
+	}
+
+	/* Need to reset the last TOU as it may not be used. */
+	if (ptp->cascade_sw_only && ptp->cascade_gpo[gpo].total) {
+		u8 last = ptp->cascade_gpo[gpo].first +
+			  ptp->cascade_gpo[gpo].total - 1;
+
+		if (last < MAX_TRIG_UNIT &&
+		    ptp->cascade_gpo[gpo].tso & (1 << last)) {
+			ptp->cascade_tx |= (1 << last);
+			ptp_tso_off(ptp, last, (1 << last));
+		}
+	}
+	ptp->ops->release(ptp);
+
+	/* Help check the interval and limit the repeat time. */
+	if (check_cascade(ptp, tso, total, &cmd->cnt, cmd->sec, cmd->nsec)) {
+		printk(KERN_INFO
+		       "cascade timing is not right"NL);
+		return DEV_IOC_INVALID_CMD;
+	}
+	update_cascade_time(ptp, tso, total, cmd->sec, cmd->nsec);
+
+	/* Operation may not be executed but will not happen here. */
+	if (!ptp_core_tx_cascade(ptp, tso, total, cmd->cnt, cmd->sec,
+				 cmd->nsec, cmd->flags & PTP_CMD_INTR_OPER))
+		rc = DEV_IOC_INVALID_CMD;
+
+proc_ptp_tx_cascade_done:
+	*data = tso;
+	return rc;
+}  /* proc_ptp_tx_cascade */
+
+static void proc_tsm_get_gps(struct ptp_info *ptp, u8 *data)
+{
+	struct tsm_get_gps *get = (struct tsm_get_gps *) data;
+
+	if (!ptp->gps_dev)
+		return;
+
+	get->cmd |= TSM_CMD_GET_TIME_RESP;
+	get->seqid = htons(ptp->gps_seqid);
+	get->sec = htonl(ptp->gps_time.sec);
+	get->nsec = htonl(ptp->gps_time.nsec);
+	file_dev_setup_msg(ptp->gps_dev, data, sizeof(struct tsm_get_gps),
+		NULL, NULL);
+	ptp->gps_dev = NULL;
+}  /* proc_tsm_get_gps */
+
+static int proc_dev_get_event(struct file_dev_info *info, u8 *data)
+{
+	struct ptp_info *ptp = info->dev;
+	int len;
+	struct ptp_tsi_info *in = (struct ptp_tsi_info *) data;
+	u8 buf[sizeof(struct ptp_utime) * MAX_TIMESTAMP_EVENT_UNIT +
+		sizeof(struct ptp_tsi_info)];
+	struct ptp_tsi_info *out = (struct ptp_tsi_info *) buf;
+
+	if (in->unit >= MAX_TIMESTAMP_UNIT)
+		return -EINVAL;
+	if (!ptp->events[in->unit].num)
+		return DEV_IOC_UNIT_UNAVAILABLE;
+	out->cmd = in->cmd | PTP_CMD_RESP;
+	out->unit = in->unit;
+	out->event = ptp->events[in->unit].event;
+	out->num = ptp->events[in->unit].num;
+	out->edge = ptp->events[in->unit].edge;
+	len = sizeof(struct ptp_utime) * out->num;
+	memcpy(out->t, ptp->events[in->unit].t, len);
+	len += sizeof(struct ptp_tsi_info);
+	file_dev_setup_msg(info, buf, len, NULL, NULL);
+	return 0;
+}  /* proc_dev_get_event */
+
+static int proc_ptp_get_event(struct ptp_info *ptp, u8 *data)
+{
+	struct ptp_tsi_info *in = (struct ptp_tsi_info *) data;
+	int ret = -1;
+
+	if (ptp->tsi_dev[in->unit])
+		ret = proc_dev_get_event(ptp->tsi_dev[in->unit], data);
+	return ret;
+}  /* proc_ptp_get_event */
+
+static int proc_ptp_get_trig(struct ptp_info *ptp, u8 *data, u16 done,
+	u16 error)
+{
+	int len;
+	struct ptp_tsi_info *in = (struct ptp_tsi_info *) data;
+	u8 buf[sizeof(struct ptp_utime) + sizeof(struct ptp_tsi_info)];
+	struct ptp_tsi_info *out = (struct ptp_tsi_info *) buf;
+	struct ptp_output *cur;
+	int tso = in->unit;
+	int tso_bit = (1 << tso);
+
+	out->cmd = in->cmd | PTP_CMD_RESP;
+	out->unit = in->unit;
+	cur = &ptp->outputs[tso];
+	if (error & tso_bit)
+		out->event = 1;
+	else if (!(done & tso_bit))
+		out->event = 2;
+	else
+		out->event = 0;
+	out->num = 1;
+	len = sizeof(struct ptp_utime) * out->num;
+	memcpy(out->t, &cur->stop, len);
+	len += sizeof(struct ptp_tsi_info);
+	if (ptp->tso_dev[tso]) {
+		file_dev_setup_msg(ptp->tso_dev[tso], buf, len, NULL, NULL);
+		return 0;
+	}
+	return -1;
+}  /* proc_ptp_get_trig */
+
+static int proc_dev_poll_event(struct file_dev_info *info, u8 *data)
+{
+	struct ptp_info *ptp = info->dev;
+	struct ptp_tsi_info *in = (struct ptp_tsi_info *) data;
+
+	if (in->unit >= MAX_TIMESTAMP_UNIT)
+		return -EINVAL;
+	if (!ptp_poll_event(ptp, in->unit))
+		return DEV_IOC_UNIT_UNAVAILABLE;
+	return proc_dev_get_event(info, data);
+}  /* proc_dev_poll_event */
+
+static int proc_dev_get_event_info(struct file_dev_info *info, u8 *data)
+{
+	struct ptp_tsi_info *in = (struct ptp_tsi_info *) data;
+
+	in->unit = MAX_TIMESTAMP_UNIT;
+	in->event = MAX_TIMESTAMP_EVENT_UNIT;
+	in->num = 0;
+	return 0;
+}  /* proc_dev_get_event_info */
+
+static int proc_ptp_get_output(struct ptp_info *ptp, u8 *data)
+{
+	int *output = (int *) data;
+	struct ptp_tso_options *in = (struct ptp_tso_options *) data;
+
+	if (in->gpo >= MAX_GPIO)
+		return -EINVAL;
+	*output = ptp->cascade_gpo[in->gpo].tso;
+	return 0;
+}  /* proc_ptp_get_output */
+
+static void proc_ptp_get_clk(struct ptp_info *ptp, u8 *data)
+{
+	struct ptp_utime t;
+	struct ptp_clk_options *cmd = (struct ptp_clk_options *) data;
+
+	ptp->ops->acquire(ptp);
+	ptp->reg->get_time(ptp, &t);
+	ptp->ops->release(ptp);
+	cmd->sec = t.sec;
+	cmd->nsec = t.nsec;
+}  /* proc_ptp_get_clk */
+
+static int proc_ptp_set_clk(struct ptp_info *ptp, u8 *data)
+{
+	struct ptp_utime t;
+	struct ptp_clk_options *cmd = (struct ptp_clk_options *) data;
+	struct timespec64 ts;
+	struct ptp_utime sys_time;
+
+	t.sec = cmd->sec;
+	t.nsec = cmd->nsec;
+	ptp->ops->acquire(ptp);
+	ts = ktime_to_timespec64(ktime_get_real());
+	sys_time.sec = ts.tv_sec;
+	sys_time.nsec = ts.tv_nsec;
+	ptp->reg->set_time(ptp, &t);
+	ptp->cur_time = t;
+	calc_udiff(&ptp->cur_time, &sys_time, &ptp->time_diff);
+	generate_tx_event(ptp, ptp->pps_gpo);
+	ptp->ops->release(ptp);
+	dbg_msg(" set clk: %x:%09u"NL, cmd->sec, cmd->nsec);
+	return 0;
+}  /* proc_ptp_set_clk */
+
+static int proc_ptp_adj_clk(struct ptp_info *ptp, u8 *data, int adjust)
+{
+	struct ptp_clk_options *cmd = (struct ptp_clk_options *) data;
+
+	adjust--;
+	if (cmd->sec > 1) {
+		ptp->adjust_sec = cmd->sec;
+		ptp->adjust_offset = cmd->nsec;
+		if (!adjust) {
+			ptp->adjust_sec = -ptp->adjust_sec;
+			ptp->adjust_offset = -ptp->adjust_offset;
+		}
+		dbg_msg("adj clk: %d %u:%09u"NL, adjust, cmd->sec, cmd->nsec);
+		cmd->sec = cmd->nsec = 0;
+		ptp->adj_clk.func(&ptp->adj_clk);
+	}
+	ptp->ops->acquire(ptp);
+	if (cmd->nsec || cmd->sec) {
+		ptp->sec_changed = cmd->sec;
+		if (!(ptp->features & PTP_ADJ_SEC)) {
+			cmd->nsec += cmd->sec * NANOSEC_IN_SEC;
+			cmd->sec = 0;
+		}
+		ptp->reg->adjust_time(ptp, adjust, cmd->sec, cmd->nsec,
+			ptp->features & PTP_ADJ_HACK);
+		ptp->offset_changed = cmd->nsec;
+		if (!adjust)
+			ptp->offset_changed = -cmd->nsec;
+		ptp_prepare_pps_check(ptp);
+		if (ptp->sec_changed || ptp->clk_add)
+			generate_tx_event(ptp, ptp->pps_gpo);
+		else
+			ptp_enable_pps_check(ptp);
+		if (ptp->sec_changed) {
+			if (adjust)
+				ptp->cur_time.sec += cmd->sec;
+			else
+				ptp->cur_time.sec -= cmd->sec;
+			ptp->sec_changed = 0;
+		}
+		if (adjust)
+			add_nsec(&ptp->cur_time, cmd->nsec);
+		else
+			sub_nsec(&ptp->cur_time, cmd->nsec);
+		if (cmd->sec)
+		dbg_msg(" adj clk: %d %u:%09u"NL, adjust, cmd->sec, cmd->nsec);
+	}
+	if (cmd->interval) {
+		ptp->drift = cmd->drift;
+		if (ptp->drift > MAX_DRIFT_CORR)
+			ptp->drift = MAX_DRIFT_CORR;
+		else if (ptp->drift < -MAX_DRIFT_CORR)
+			ptp->drift = -MAX_DRIFT_CORR;
+		ptp->drift_set = ptp->drift;
+		ptp->adjust = clk_adjust_val(ptp->drift, cmd->interval);
+		set_ptp_adjust(ptp, ptp->adjust);
+		if (!ptp->ptp_synt) {
+			syntonize_clk(ptp);
+			ptp->ptp_synt = true;
+		}
+
+		/* Start updating port peer delay when the clock is being
+		 * synchronized.  Note master clock will eventually update
+		 * peer delay, but the delay only matters to 1-step TC.
+		 */
+		if (ptp->need_peer_delay_set_help &&
+		    !ptp->have_first_drift_set && ptp->drift_set) {
+			ptp->have_first_drift_set = true;
+		}
+if (!ptp->first_drift && ptp->drift_set)
+dbg_msg("first drift: %d"NL, ptp->drift_set);
+		if (!ptp->first_drift)
+			ptp->first_drift = ptp->drift_set;
+#if 0
+		dbg_msg(" adj drift: %d"NL, cmd->drift);
+#endif
+	}
+	ptp->ops->release(ptp);
+	return 0;
+}  /* proc_ptp_adj_clk */
+
+static int proc_ptp_get_delay(struct ptp_info *ptp, uint port, u8 *data)
+{
+	struct ptp_delay_values *delay = (struct ptp_delay_values *) data;
+	struct ksz_sw *sw = ptp->parent;
+
+	if (port > ptp->ports)
+		return DEV_IOC_INVALID_CMD;
+	port = get_phy_port(sw, port);
+	ptp->ops->acquire(ptp);
+	delay->rx_latency = get_ptp_ingress(ptp, port);
+	delay->tx_latency = get_ptp_egress(ptp, port);
+	delay->asym_delay = get_ptp_asym(ptp, port);
+	ptp->ops->release(ptp);
+	return 0;
+}  /* proc_ptp_get_delay */
+
+static int proc_ptp_set_delay(struct ptp_info *ptp, uint port, u8 *data)
+{
+	struct ptp_delay_values *delay = (struct ptp_delay_values *) data;
+	struct ksz_sw *sw = ptp->parent;
+	int index;
+
+	if (port > ptp->ports)
+		return DEV_IOC_INVALID_CMD;
+	port = get_phy_port(sw, port);
+	ptp->ops->acquire(ptp);
+	set_ptp_ingress(ptp, port, delay->rx_latency);
+	set_ptp_egress(ptp, port, delay->tx_latency);
+	set_ptp_asym(ptp, port, delay->asym_delay);
+	index = get_speed_index(ptp, port);
+	ptp->rx_latency[port][index] = delay->rx_latency;
+	ptp->tx_latency[port][index] = delay->tx_latency;
+	ptp->asym_delay[port][index] = delay->asym_delay;
+	ptp->ops->release(ptp);
+	dbg_msg("set delay: %d = %d %d %d"NL, port,
+		ptp->rx_latency[port][index],
+		ptp->tx_latency[port][index],
+		ptp->asym_delay[port][index]);
+	return 0;
+}  /* proc_ptp_set_delay */
+
+static int proc_ptp_get_peer_delay(struct ptp_info *ptp, uint port, u8 *data)
+{
+	struct ptp_delay_values *delay = (struct ptp_delay_values *) data;
+	struct ksz_sw *sw = ptp->parent;
+	u32 link;
+
+	if (port > ptp->ports)
+		return DEV_IOC_INVALID_CMD;
+	port = get_phy_port(sw, port);
+	ptp->ops->acquire(ptp);
+	delay->rx_latency = 0;
+	delay->tx_latency = 0;
+	delay->asym_delay = 0;
+	link = get_ptp_link(ptp, port);
+	delay->reserved = link;
+	delay->rx_latency = link >> 16;
+	ptp->ops->release(ptp);
+	return 0;
+}  /* proc_ptp_get_peer_delay */
+
+static int proc_ptp_set_peer_delay(struct ptp_info *ptp, uint port, u8 *data)
+{
+	struct ptp_delay_values *delay = (struct ptp_delay_values *) data;
+	struct ksz_sw *sw = ptp->parent;
+	u32 link;
+
+	if (port > ptp->ports)
+		return DEV_IOC_INVALID_CMD;
+	port = get_phy_port(sw, port);
+	ptp->ops->acquire(ptp);
+	link = delay->rx_latency;
+	link <<= 16;
+	link |= delay->reserved;
+	if (link != ptp->peer_delay[port]) {
+		set_ptp_link(ptp, port, link);
+		ptp->peer_delay[port] = link;
+		if (abs(link - ptp->peer_delay[port]) > 5)
+			dbg_msg("set delay: %d = %d"NL, port,
+				ptp->peer_delay[port]);
+	}
+	ptp->ops->release(ptp);
+	return 0;
+}  /* proc_ptp_set_peer_delay */
+
+static int proc_ptp_get_port_cfg(struct ptp_info *ptp, uint port, u8 *data)
+{
+	struct ptp_delay_values *delay = (struct ptp_delay_values *) data;
+	struct ksz_sw *sw = ptp->parent;
+	struct ksz_port_cfg *cfg;
+
+	if (port > ptp->ports)
+		return DEV_IOC_INVALID_CMD;
+	port = get_phy_port(sw, port);
+	cfg = get_port_cfg(sw, port);
+	delay->reserved = 0;
+	if (cfg->ptp_enabled)
+		delay->reserved |= PTP_PORT_ENABLED;
+	if (cfg->asCapable)
+		delay->reserved |= PTP_PORT_ASCAPABLE;
+	return 0;
+}  /* proc_ptp_get_port_cfg */
+
+static int proc_ptp_set_port_cfg(struct ptp_info *ptp, uint port, u8 *data)
+{
+	struct ptp_delay_values *delay = (struct ptp_delay_values *) data;
+	struct ksz_sw *sw = ptp->parent;
+	struct ksz_port_cfg *cfg;
+
+	if (port > ptp->ports)
+		return DEV_IOC_INVALID_CMD;
+	port = get_phy_port(sw, port);
+	cfg = get_port_cfg(sw, port);
+	cfg->ptp_enabled = !!(delay->reserved & PTP_PORT_ENABLED);
+	if (cfg->ptp_enabled)
+		cfg->asCapable = !!(delay->reserved & PTP_PORT_ASCAPABLE);
+	else
+		cfg->asCapable = false;
+
+#ifdef CONFIG_KSZ_MSRP
+	if (sw->features & MRP_SUPPORT) {
+		struct mrp_info *mrp = &sw->mrp;
+
+dbg_msg("as: %d=%d"NL, port, cfg->asCapable);
+		mrp->ops->chk_talker(mrp, port);
+	}
+#endif
+	return 0;
+}  /* proc_ptp_set_port_cfg */
+
+static struct ptp_tx_ts *proc_get_ts(struct ptp_info *ptp, uint port, u8 msg,
+	u16 seqid, u8 *mac, struct file_dev_info *info, int len)
+{
+	struct ptp_tx_ts *tx;
+	int from_stack = false;
+	u8 *data = NULL;
+	struct ksz_sw *sw = ptp->parent;
+	bool linked = (media_connected == sw->port_info[port].state);
+
+	if (info)
+		data = info->write_buf;
+	if (SYNC_MSG == msg)
+		tx = &ptp->tx_sync[port];
+	else if (PDELAY_RESP_MSG == msg)
+		tx = &ptp->tx_resp[port];
+	else
+		tx = &ptp->tx_dreq[port];
+	if (seqid || mac[0] || mac[1])
+		from_stack = true;
+	if (data && tx->req_time && linked && port < 5)
+		dbg_msg("  last %x=%04x: p=%d, j=%lu"NL, msg, seqid, port,
+			jiffies - tx->req_time);
+	tx->missed = false;
+	tx->req_time = jiffies;
+	if (tx->ts.timestamp && from_stack) {
+		unsigned long diff = tx->req_time - tx->resp_time;
+
+		/* The timestamp is not valid. */
+		if (diff >= 4 * ptp->delay_ticks) {
+			dbg_msg("  invalid: %x=%04x: %d, %lu"NL,
+				msg, seqid, port, diff);
+			tx->ts.timestamp = 0;
+		} else if (diff > 2 * ptp->delay_ticks)
+			dbg_msg("  ready? %x=%04x: %d, %lu"NL,
+				msg, seqid, port, diff);
+	}
+	if (!tx->ts.timestamp && linked && data) {
+		int rc = wait_event_interruptible_timeout(ptp->wait_ts[port],
+			0 != tx->ts.timestamp, ptp->delay_ticks);
+
+		if (rc < 0)
+			return NULL;
+	}
+	if (!tx->ts.timestamp) {
+		if (from_stack && data) {
+			tx->missed = true;
+			memcpy(tx->data.buf, data, len);
+			tx->data.len = len;
+			tx->dev = info;
+		}
+		if (linked && port < 5)
+			dbg_msg("  missed %x=%04x: p=%d, j=%lu"NL,
+				msg, seqid, port, jiffies - tx->req_time);
+		tx = NULL;
+	}
+	return tx;
+}  /* proc_get_ts */
+
+static int proc_ptp_get_timestamp(struct ptp_info *ptp, u8 *data,
+	struct file_dev_info *info)
+{
+	struct ptp_ts_options *opt = (struct ptp_ts_options *) data;
+
+	if (opt->timestamp) {
+		struct ptp_ts ts;
+
+		ts.timestamp = opt->timestamp;
+		update_ts(&ts, ptp->cur_time.sec);
+		opt->sec = ts.t.sec;
+		opt->nsec = ts.t.nsec;
+	} else {
+		struct ptp_tx_ts *tx;
+		struct tsm_db *db;
+		uint port;
+		struct ksz_sw *sw = ptp->parent;
+
+		if (opt->port > ptp->ports)
+			return DEV_IOC_INVALID_CMD;
+
+		/* Save timestamp information for later reporting. */
+		if (info) {
+			db = (struct tsm_db *) info->write_buf;
+			db->cmd = opt->msg;
+			db->cmd |= TSM_CMD_DB_GET;
+			db->index = opt->port << 1;
+			db->seqid = htons(opt->seqid);
+			db->mac[0] = opt->mac[0];
+			db->mac[1] = opt->mac[1];
+		}
+		port = get_phy_port(sw, opt->port);
+		tx = proc_get_ts(ptp, port, opt->msg,
+			opt->seqid, opt->mac, info, sizeof(struct tsm_db));
+		if (!tx)
+			return -EAGAIN;
+		if (ptp->cap & PTP_KNOW_ABOUT_LATENCY) {
+			opt->sec = tx->ts.r.sec;
+			opt->nsec = tx->ts.r.nsec;
+		} else {
+			opt->sec = tx->ts.t.sec;
+			opt->nsec = tx->ts.t.nsec;
+		}
+		tx->ts.timestamp = 0;
+		tx->req_time = 0;
+	}
+	return 0;
+}  /* proc_ptp_get_timestamp */
+
+static struct ptp_tx_ts *proc_get_ts_port(struct ptp_info *ptp, u8 msg,
+	uint *port, int *more)
+{
+	uint m;
+	uint p;
+	struct ptp_tx_ts *tx;
+	struct ptp_tx_ts *xts = NULL;
+	struct ksz_sw *sw = ptp->parent;
+	struct ksz_port_info *info;
+	bool linked;
+
+	*more = false;
+	for (m = 1; m <= ptp->ports; m++) {
+		p = get_phy_port(sw, m);
+		if (SYNC_MSG == msg)
+			tx = &ptp->tx_sync[p];
+		else if (PDELAY_RESP_MSG == msg)
+			tx = &ptp->tx_resp[p];
+		else
+			tx = &ptp->tx_dreq[p];
+
+		/* Same type of event message has been sent. */
+		if (tx->hdr.messageType != msg)
+			continue;
+
+		*more = true;
+
+		/* Have all information ready. */
+		if (xts)
+			break;
+
+		info = get_port_info(sw, p);
+		linked = (media_connected == info->state);
+		if (!tx->ts.timestamp && linked) {
+			int rc = wait_event_interruptible_timeout(
+				ptp->wait_ts[p], 0 != tx->ts.timestamp,
+				ptp->delay_ticks);
+
+			if (rc < 0)
+				return NULL;
+		}
+
+		/* Transmit timestamp ready. */
+		if (tx->ts.timestamp) {
+			xts = tx;
+			*port = p;
+			*more = false;
+			tx->ts.timestamp = 0;
+			tx->req_time = 0;
+			tx->hdr.messageType = 7;
+		}
+	}
+	return xts;
+}  /* proc_get_ts_port */
+
+static int proc_ptp_get_msg_info(struct ptp_info *ptp, u8 *data,
+	struct file_dev_info *info, int *tx)
+{
+	struct ksz_sw *sw = ptp->parent;
+	struct ptp_msg_options *opt = (struct ptp_msg_options *) data;
+
+	if (!*tx) {
+		struct ptp_msg_hdr hdr;
+		struct ptp_msg_options tx_msg;
+		int found;
+
+		memcpy(&hdr.sourcePortIdentity, &opt->id,
+			sizeof(struct ptp_port_identity));
+		hdr.messageType = opt->msg;
+		hdr.sequenceId = opt->seqid;
+		hdr.domainNumber = opt->domain;
+		found = find_msg_info(&ptp->rx_msg_info[hdr.messageType],
+			&ptp->rx_msg_lock, &hdr, &hdr.sourcePortIdentity,
+			true, &tx_msg);
+		if (found) {
+			opt->port = get_log_port(sw, tx_msg.port);
+			opt->ts = tx_msg.ts;
+		} else
+			return DEV_IOC_UNIT_UNAVAILABLE;
+	} else {
+		struct ptp_tx_ts *xts;
+		uint port = 0;
+
+		/* Not event message. */
+		if (opt->msg & 0x8)
+			return DEV_IOC_INVALID_CMD;
+
+		xts = proc_get_ts_port(ptp, opt->msg, &port, tx);
+		if (xts) {
+			if (ptp->cap & PTP_KNOW_ABOUT_LATENCY) {
+				opt->ts.t.sec = xts->ts.r.sec;
+				opt->ts.t.nsec = xts->ts.r.nsec;
+			} else {
+				opt->ts.t.sec = xts->ts.t.sec;
+				opt->ts.t.nsec = xts->ts.t.nsec;
+			}
+			opt->port = get_log_port(sw, port);
+		} else
+			return -EAGAIN;
+	}
+	return 0;
+}  /* proc_ptp_get_msg_info */
+
+static int proc_ptp_set_msg_info(struct ptp_info *ptp, u8 *data,
+	struct file_dev_info *info)
+{
+	struct ksz_sw *sw = ptp->parent;
+	struct ptp_msg_options *opt = (struct ptp_msg_options *) data;
+
+	/* Used for testing. */
+	if (7 == opt->msg) {
+		struct ptp_msg_info *tx_msg;
+		u32 ports = opt->port;
+
+		if (ports >= (1 << ptp->ports))
+			return DEV_IOC_INVALID_CMD;
+
+		ports = sw->ops->get_phy_mask_from_log(sw, ports);
+
+		/* Do not allow sending to host port. */
+		if (ports == sw->HOST_MASK)
+			return DEV_IOC_INVALID_CMD;
+
+		/* Host port is not sending. */
+		ports &= ~sw->HOST_MASK;
+		tx_msg = &ptp->tx_msg_info[opt->msg];
+		tx_msg->data.port = ports;
+		tx_msg->data.ts.timestamp = opt->ts.timestamp;
+		if (opt->port) {
+			ptp->tx_msg_cnt = -1;
+			ptp->overrides |= PTP_USE_DEFAULT_PORT;
+		} else {
+			ptp->tx_msg_cnt = 0;
+			ptp->overrides &= ~PTP_USE_DEFAULT_PORT;
+		}
+	} else {
+		struct ptp_msg_hdr hdr;
+		u32 ports = opt->port;
+
+		if (ports >= (1 << ptp->ports))
+			return DEV_IOC_INVALID_CMD;
+
+		ports = sw->ops->get_phy_mask_from_log(sw, ports);
+
+		/* Do not allow sending to host port. */
+		if (ports == sw->HOST_MASK)
+			return DEV_IOC_INVALID_CMD;
+
+		/* Host port is not sending. */
+		ports &= ~sw->HOST_MASK;
+		memcpy(&hdr.sourcePortIdentity, &opt->id,
+			sizeof(struct ptp_port_identity));
+		hdr.messageType = opt->msg;
+		hdr.sequenceId = opt->seqid;
+		hdr.domainNumber = opt->domain;
+		opt->ts.timestamp = (opt->ts.t.sec << 30) |
+			opt->ts.t.nsec;
+		set_msg_info(ptp, &hdr, ports, opt->ts.timestamp);
+	}
+	return 0;
+}  /* proc_ptp_set_msg_info */
+
+static int parse_tsm_msg(struct file_dev_info *info, int len)
+{
+	struct ptp_info *ptp = info->dev;
+	u8 *data = info->write_buf;
+	u8 cmd = data[0] & 0xf0;
+	u8 msg = data[0] & 0x03;
+	int result = 0;
+
+	switch (cmd) {
+	case TSM_CMD_DB_GET_TIME:
+	{
+		struct tsm_get_time *get = (struct tsm_get_time *) data;
+		struct ptp_ts ts;
+
+		ts.timestamp = ntohl(get->nsec);
+		if (ts.timestamp) {
+			update_ts(&ts, ptp->cur_time.sec);
+		} else {
+			ptp->ops->acquire(ptp);
+			ptp->reg->get_time(ptp, &ts.t);
+			ptp->ops->release(ptp);
+		}
+		file_dev_setup_msg(info, data, len, ptp_tsm_get_time_resp,
+			&ts.t);
+		break;
+	}
+	case TSM_CMD_DB_GET:
+	{
+		struct ksz_sw *sw = ptp->parent;
+		struct tsm_db *db = (struct tsm_db *) data;
+
+		if (db->index <= (1 << 7)) {
+			struct ptp_tx_ts *tx;
+			uint port = db->index >> 1;
+
+			if (port > ptp->ports)
+				break;
+			port = get_phy_port(sw, port);
+			tx = proc_get_ts(ptp, port, msg, ntohs(db->seqid),
+				db->mac, info, len);
+			if (tx) {
+				file_dev_setup_msg(info, data, len,
+					ptp_tsm_resp, &tx->ts);
+				tx->ts.timestamp = 0;
+				tx->req_time = 0;
+			}
+		}
+		break;
+	}
+	case TSM_CMD_GET_GPS_TS:
+	{
+		/* First time to get GPS timestamp. */
+		if (MAX_TIMESTAMP_UNIT == ptp->gps_tsi) {
+			ptp->gps_tsi = DEFAULT_GPS_TSI;
+			if (ptp->tsi_used & (1 << ptp->gps_tsi))
+				ptp->reg->rx_off(ptp, ptp->gps_tsi);
+			prepare_gps(ptp);
+			ptp->gps_seqid = 0;
+		}
+		ptp->gps_req_time = jiffies;
+		ptp->gps_dev = info;
+		if (ptp->gps_resp_time) {
+			unsigned long diff = ptp->gps_req_time -
+				ptp->gps_resp_time;
+
+			/* The timestamp is not valid. */
+			if (diff >= 2 * ptp->delay_ticks) {
+				dbg_msg("  invalid gps: %lu"NL, diff);
+				ptp->gps_time.sec = 0;
+			}
+		}
+		if (ptp->gps_time.sec) {
+			proc_tsm_get_gps(ptp, data);
+			ptp->gps_time.sec = 0;
+			ptp->gps_req_time = 0;
+		} else
+			dbg_msg("  missed gps"NL);
+		break;
+	}
+	case TSM_CMD_CNF_SET:
+	{
+		struct tsm_cfg *cfg = (struct tsm_cfg *) data;
+		u32 ingress = htonl(cfg->ingress_delay);
+
+		ptp->ops->acquire(ptp);
+		if (0xFF == cfg->port) {
+			u16 mode = ptp->mode;
+
+			if (cfg->enable & 0x04)
+				mode |= PTP_TC_P2P;
+			else
+				mode &= ~PTP_TC_P2P;
+			set_ptp_mode(ptp, mode);
+			ptp->def_mode = mode;
+		} else {
+			u8 n = cfg->port - 1;
+			u8 port = n;
+
+			if ((cfg->enable & 0x10) && n < ptp->ports &&
+					ptp->peer_delay[port] != ingress) {
+				ptp->peer_delay[port] = ingress;
+				set_ptp_link(ptp, port, ingress);
+			}
+		}
+		ptp->ops->release(ptp);
+		break;
+	}
+	case TSM_CMD_CLOCK_SET:
+	{
+		struct tsm_clock_set *clk = (struct tsm_clock_set *) data;
+		struct ptp_ts ts;
+
+		ts.t.sec = ntohl(clk->sec);
+		ts.t.nsec = ntohl(clk->nsec);
+		ts.timestamp = ntohl(clk->timestamp);
+		ptp->ops->acquire(ptp);
+		set_cur_time(ptp, &ts);
+		ptp->ops->release(ptp);
+		ptp->state = 2;
+		break;
+	}
+	case TSM_CMD_CLOCK_CORRECT:
+	{
+		struct tsm_clock_correct *clk = (struct tsm_clock_correct *)
+			data;
+		u32 drift;
+		u32 nsec;
+		int ptp_offset;
+
+		drift = ntohl(clk->drift);
+		nsec = ntohl(clk->nsec);
+		ptp_offset = ntohl(clk->offset);
+		if (2 == (clk->add >> 4))
+			break;
+
+		ptp->ops->acquire(ptp);
+		if (nsec) {
+			ptp->reg->adjust_time(ptp, !ptp_offset, 0, nsec,
+				ptp->features & PTP_ADJ_HACK);
+			ptp->offset_changed = nsec;
+			if (ptp_offset)
+				ptp->offset_changed = -nsec;
+			else {
+				ptp_prepare_pps_check(ptp);
+				ptp_enable_pps_check(ptp);
+			}
+		}
+		if (clk->add & 1)
+			ptp->drift = drift;
+		else
+			ptp->drift = -drift;
+		if (ptp->drift > MAX_DRIFT_CORR)
+			ptp->drift = MAX_DRIFT_CORR;
+		else if (ptp->drift < -MAX_DRIFT_CORR)
+			ptp->drift = -MAX_DRIFT_CORR;
+		ptp->drift_set = ptp->drift;
+		ptp->adjust = clk_adjust_val(ptp->drift,
+			NANOSEC_IN_SEC);
+		set_ptp_adjust(ptp, ptp->adjust);
+if (!ptp->first_drift)
+dbg_msg("  first drift: %d"NL, ptp->drift_set);
+		if (!ptp->first_drift)
+			ptp->first_drift = ptp->drift_set;
+		ptp->ops->release(ptp);
+		break;
+	}
+	default:
+		dbg_msg("tsm cmd: %02X, %d"NL, cmd, len);
+	}
+	return result;
+}  /* parse_tsm_msg */
+
+static int ptp_get_port_info(struct ptp_info *ptp, u8 *data, int *output)
+{
+	struct net_device *netdev;
+	struct ksz_port *netport;
+	u32 mask;
+	int len;
+	int c;
+	int i;
+	int n;
+	int p;
+	int phys_port;
+	int virt_port;
+	char devname[40];
+	char *dot;
+	char *dot2;
+	char *name = (char *)data;
+	bool matched = false;
+	unsigned int vlan = 0;
+	int result = DEV_IOC_OK;
+	struct ksz_sw *sw = ptp->parent;
+	int dev_count = sw->dev_count + sw->dev_offset;
+
+	len = strnlen(name, sizeof(devname));
+	strncpy(devname, name, len);
+	devname[len] = '\0';
+	dot = strchr(devname, '.');
+	if (dot) {
+		++dot;
+		n = sscanf(dot, "%u", &vlan);
+		dot2 = strchr(dot, '.');
+		if (dot2) {
+			*dot2 = '\0';
+			len = strnlen(devname, sizeof(devname));
+		}
+	}
+
+	/* Check real network device. */
+	for (n = 0; n < dev_count; n++) {
+		netdev = sw->netdev[n];
+		if (!strncmp(netdev->name, devname, len) &&
+		    sw->netport[n]) {
+			netport = sw->netport[n];
+			virt_port = netport->first_port;
+			phys_port = get_phy_port(sw, virt_port);
+			mask = 0;
+			for (c = 0, i = netport->first_port;
+			     c < netport->port_cnt; c++, i++) {
+				p = get_phy_port(sw, i);
+				if (p == sw->HOST_PORT)
+					continue;
+				mask |= (1 << p);
+			}
+			matched = true;
+			break;
+		}
+	}
+
+	/* Check virtual VLAN device. */
+	if (!matched && vlan > VLAN_PORT_START &&
+	    vlan <= sw->mib_port_cnt + VLAN_PORT_START) {
+		--dot;
+		*dot = '\0';
+		--vlan;
+		for (n = 0; n < dev_count; n++) {
+			netdev = sw->netdev[n];
+			if (!strncmp(netdev->name, devname, len) &&
+			    sw->netport[n]) {
+				netport = sw->netport[n];
+				virt_port = netport->first_port;
+				virt_port += vlan - VLAN_PORT_START;
+				phys_port = get_phy_port(sw, virt_port);
+				mask = (1 << phys_port);
+				matched = true;
+				break;
+			}
+		}
+	}
+	if (matched) {
+		data[0] = 'M';
+		data[1] = 'i';
+		data[2] = 'c';
+		data[3] = 'r';
+		data[4] = phys_port + 1;
+		data[5] = virt_port;
+		*output = mask;
+	} else {
+		result = DEV_IOC_INVALID_CMD;
+	}
+	return result;
+}  /* ptp_get_port_info */
+
+static int execute_wait(struct ptp_work *work)
+{
+	int rc = 0;
+
+	execute(work->ptp, &work->work);
+	wait_for_completion(&work->done);
+	return rc;
+}  /* execute_wait */
+
+static void proc_ptp_work(struct work_struct *work)
+{
+	struct ptp_work *parent =
+		container_of(work, struct ptp_work, work);
+	struct ptp_info *ptp = parent->ptp;
+	struct file_dev_info *info = parent->dev_info;
+	u8 *data = parent->param.data;
+	uint port;
+	u32 reg;
+	u32 val;
+	size_t width;
+	int result = DEV_IOC_OK;
+	struct ksz_sw *sw = ptp->parent;
+
+#ifdef CONFIG_KSZ_IBA
+	if (iba_stopped(sw))
+		return;
+#endif
+
+	parent->output = parent->option;
+	switch (parent->cmd) {
+	case DEV_CMD_INFO:
+		switch (parent->subcmd) {
+		case DEV_INFO_INIT:
+
+			/* This is called before by another process. */
+			if (ptp->op_state && (ptp->cap || ptp->op_mode > 1))
+				goto skip;
+			ptp->cap = ptp->op_mode = 0;
+			if ('1' == data[0] &&
+                            '5' == data[1] &&
+                            '8' == data[2] &&
+                            '8' == data[3] &&
+                            'v' == data[4] &&
+                            '2' == data[5]) {
+				int cap = parent->option;
+
+/*
+ * op_mode 0	Original mode of using reserved fields to specify port.
+ * op_mode 1	Have multiple devices and use standard Linux PTP API to receive
+ * 		timestamps.  Do not need to know about PTP messages except the
+ * 		only case of sending 1-step Pdelay_Resp timestamp.  The
+ * 		destination port is already known by other means.
+ * op_mode 2	The destination port is communicated well before sending the
+ * 		PTP message.
+ * op_mode 3	Use single device and do not know about multiple ports.
+ */
+				ptp->forward = FWD_MAIN_DEV;
+				if (cap & PTP_HAVE_MULT_DEVICES)
+					ptp->forward = FWD_VLAN_DEV |
+						       FWD_STP_DEV;
+				if ((cap & PTP_HAVE_MULT_DEVICES) &&
+				    (cap & PTP_CAN_RX_TIMESTAMP)) {
+					ptp->op_mode = 1;
+				} else if (cap & PTP_HAVE_MULT_DEVICES) {
+					ptp->op_mode = 2;
+				} else if (!(cap & (PTP_KNOW_ABOUT_MULT_PORTS |
+						    PTP_HAVE_MULT_PORTS))) {
+					ptp->op_mode = 3;
+				} else if (cap & PTP_USE_RESERVED_FIELDS) {
+					ptp->op_mode = 0;
+				} else {
+					ptp->op_mode = 2;
+				}
+				if (cap & PTP_CAN_RX_TIMESTAMP) {
+					if (cap & PTP_KNOW_ABOUT_LATENCY) {
+						ptp->rx_en &= ~(1 << 8);
+						ptp->tx_en &= ~(1 << 8);
+					} else {
+						ptp->rx_en |= (1 << 8);
+						ptp->tx_en |= (1 << 8);
+					}
+				}
+
+				/* Hardware supports 1-step P2P. */
+				if (ptp->tx_en & 2)
+					ptp->tx_en |= 4;
+
+				/* Want to use 1-step clock while using 2-step
+				 * operation.
+				 */
+				if (cap & PTP_USE_ONE_STEP) {
+					ptp->need_1_step_clock_oper = true;
+					ptp->need_2_step_resp_help = true;
+				} else {
+					ptp->need_1_step_clock_oper = false;
+					ptp->need_2_step_resp_help = false;
+				}
+
+				/* Do not need to remember Pdelay_Req receive
+				 * timestamp.
+				 */
+				if (cap & PTP_KNOW_ABOUT_MULT_PORTS) {
+					ptp->need_1_step_resp_help = false;
+					ptp->need_peer_delay_set_help = false;
+				}
+				ptp->use_own_api = true;
+				ptp->cap = cap;
+dbg_msg("op_mode: %x %d; %x %08x %x:%x"NL, ptp->cap, ptp->op_mode,
+	ptp->forward, ptp->overrides, ptp->rx_en, ptp->tx_en);
+			}
+
+skip:
+			ptp_init_state(ptp);
+			parent->output = ptp->drift;
+			break;
+		case DEV_INFO_EXIT:
+			ptp_exit_state(ptp);
+			break;
+		case DEV_INFO_RESET:
+			reg = parent->option;
+			break;
+		case DEV_INFO_PORT:
+			result = ptp_get_port_info(ptp, data, &parent->output);
+			break;
+		default:
+			result = DEV_IOC_INVALID_CMD;
+			break;
+		}
+		break;
+	case DEV_CMD_PUT:
+		switch (parent->subcmd) {
+		case DEV_PTP_CFG:
+			result = proc_ptp_set_cfg(ptp, data);
+			break;
+		case DEV_PTP_TEVT:
+			result = proc_dev_rx_event(info, data);
+			parent->output = *data;
+			break;
+		case DEV_PTP_TOUT:
+			result = proc_dev_tx_event(info, data);
+			parent->output = *data;
+			break;
+		case DEV_PTP_CASCADE:
+			if (parent->option)
+				result = proc_ptp_tx_cascade(ptp, data);
+			else
+				result = proc_ptp_tx_cascade_init(ptp, data);
+			parent->output = *data;
+			break;
+		case DEV_PTP_CLK:
+			parent->option &= 0xffff;
+			if (parent->option)
+				result = proc_ptp_adj_clk(ptp, data,
+					parent->option);
+			else
+				result = proc_ptp_set_clk(ptp, data);
+			break;
+		case DEV_PTP_DELAY:
+			port = parent->option;
+			result = proc_ptp_set_delay(ptp, port, data);
+			break;
+		case DEV_PTP_REG:
+			reg = parent->option;
+			if (ptp->op_mode > 0 || ptp->cap) {
+				u32 *param = (u32 *) data;
+
+				switch (param[0]) {
+				case 4:
+					width = 4;
+					break;
+				case 1:
+					width = 1;
+					break;
+				default:
+					width = 2;
+					break;
+				}
+				reg = param[1];
+				val = param[2];
+			} else {
+				val = reg >> 16;
+				reg &= 0xffff;
+				width = 2;
+			}
+			if (width > 1) {
+				int align = reg & (width - 1);
+
+				if (align) {
+					if (align & 1) {
+						width = 1;
+						val &= 0xff;
+					} else {
+						width = 2;
+						val &= 0xffff;
+					}
+				}
+			}
+			ptp->ops->acquire(ptp);
+			switch (width) {
+			case 4:
+				sw->reg->w32(sw, reg, val);
+				val = htonl(val);
+				break;
+			case 1:
+				sw->reg->w8(sw, reg, val);
+				break;
+			default:
+				sw->reg->w16(sw, reg, val);
+				if (reg == REG_PTP_MSG_CONF1) {
+					ptp->mode = val;
+					ptp->def_mode = val;
+				}
+				val = htons(val);
+				break;
+			}
+			sw->ops->chk_regs(sw, reg, (u8 *)&val, width);
+			ptp->ops->release(ptp);
+			break;
+		case DEV_PTP_PEER_DELAY:
+			port = parent->option;
+			result = proc_ptp_set_peer_delay(ptp, port, data);
+			break;
+		case DEV_PTP_PORT_CFG:
+			port = parent->option;
+			result = proc_ptp_set_port_cfg(ptp, port, data);
+			break;
+		default:
+			result = DEV_IOC_INVALID_CMD;
+			break;
+		}
+		break;
+	case DEV_CMD_GET:
+		switch (parent->subcmd) {
+		case DEV_PTP_CFG:
+			proc_ptp_get_cfg(ptp, data);
+			break;
+		case DEV_PTP_TEVT:
+			if (2 == parent->option)
+				result = proc_dev_get_event_info(info, data);
+			else if (1 == parent->option)
+				result = proc_dev_poll_event(info, data);
+
+			/* Not actually used. */
+			else
+				result = proc_dev_get_event(info, data);
+			break;
+		case DEV_PTP_TOUT:
+			break;
+		case DEV_PTP_CLK:
+			parent->option &= 0xffff;
+			if ((ptp->op_mode > 0 || ptp->cap) && parent->option) {
+				parent->output = ptp->drift;
+				break;
+			}
+			proc_ptp_get_clk(ptp, data);
+			break;
+		case DEV_PTP_DELAY:
+			port = parent->option;
+			result = proc_ptp_get_delay(ptp, port, data);
+			break;
+		case DEV_PTP_REG:
+			reg = parent->option;
+			width = 2;
+			if (ptp->op_mode > 0 || ptp->cap) {
+				u32 *param = (u32 *) data;
+
+				switch (param[0]) {
+				case 4:
+					width = 4;
+					break;
+				case 1:
+					width = 1;
+					break;
+				default:
+					width = 2;
+					break;
+				}
+				reg = param[1];
+				val = param[2];
+			}
+			ptp->ops->acquire(ptp);
+			switch (width) {
+			case 4:
+				parent->output = sw->reg->r32(sw, reg);
+				break;
+			case 1:
+				parent->output = sw->reg->r8(sw, reg);
+				break;
+			default:
+				parent->output = sw->reg->r16(sw, reg);
+			}
+			ptp->ops->release(ptp);
+			break;
+		case DEV_PTP_PEER_DELAY:
+			port = parent->option;
+			result = proc_ptp_get_peer_delay(ptp, port, data);
+			break;
+		case DEV_PTP_PORT_CFG:
+			port = parent->option;
+			result = proc_ptp_get_port_cfg(ptp, port, data);
+			break;
+		}
+		break;
+	default:
+		result = DEV_IOC_INVALID_CMD;
+		break;
+	}
+	parent->result = result;
+	parent->used = false;
+	complete(&parent->done);
+}  /* proc_ptp_work */
+
+static int proc_ptp_hw_access(struct ptp_info *ptp, int cmd, int subcmd,
+	int option, void *data, size_t len, struct file_dev_info *info,
+	int *output, int wait)
+{
+	struct ptp_access *access;
+	struct ptp_work *work;
+	int ret = 0;
+
+	access = &ptp->hw_access;
+	mutex_lock(&access->lock);
+	work = &access->works[access->index];
+	if (work->used) {
+		pr_alert("work full"NL);
+		mutex_unlock(&access->lock);
+		return -EFAULT;
+	}
+	work->cmd = cmd;
+	work->subcmd = subcmd;
+	work->option = option;
+	memcpy(work->param.data, data, len);
+	work->dev_info = info;
+	work->wait = wait;
+	work->used = true;
+	access->index++;
+	access->index &= PTP_WORK_LAST;
+	init_completion(&work->done);
+	if (!wait) {
+		execute(ptp, &work->work);
+		goto hw_access_end;
+	}
+	ret = execute_wait(work);
+
+	/* Cannot continue if ERESTARTSYS. */
+	if (ret < 0)
+		goto hw_access_end;
+
+	ret = work->result;
+	if (DEV_IOC_OK == ret && (DEV_CMD_GET == work->cmd ||
+	    (work->cmd == DEV_CMD_INFO && work->subcmd == DEV_INFO_PORT)))
+		memcpy(data, work->param.data, len);
+	*output = work->output;
+
+hw_access_end:
+	mutex_unlock(&access->lock);
+	return ret;
+}  /* proc_ptp_hw_access */
+
+static void exit_ptp_work(struct ptp_info *ptp)
+{
+	struct ptp_access *access;
+	struct ptp_work *work;
+	int i;
+
+	access = &ptp->hw_access;
+	for (i = 0; i < PTP_WORK_NUM; i++) {
+		work = &access->works[i];
+		flush_work(&work->work);
+	}
+}  /* exit_ptp_work */
+
+static void init_ptp_work(struct ptp_info *ptp)
+{
+	struct ptp_access *access;
+	struct ptp_work *work;
+	int i;
+
+	access = &ptp->hw_access;
+	mutex_init(&access->lock);
+	for (i = 0; i < PTP_WORK_NUM; i++) {
+		work = &access->works[i];
+		work->ptp = ptp;
+		INIT_WORK(&work->work, proc_ptp_work);
+		init_completion(&work->done);
+	}
+}  /* init_ptp_work */
+
+#ifdef CONFIG_PTP_1588_CLOCK
+#include "micrel_ptp.c"
+#endif
+
+static void ptp_chk_rx_events(struct ptp_info *ptp)
+{
+	struct ptp_event *event;
+	u16 tsi_bit;
+	int i;
+
+	for (i = 0; i < MAX_TIMESTAMP_UNIT; i++) {
+		int stop;
+
+		stop = false;
+		tsi_bit = 1 << i;
+		event = &ptp->events[i];
+		if (ptp->tsi_used & tsi_bit) {
+
+			/* At least one event. */
+			if (event->num || event->expired) {
+				if (event->num >= event->max)
+					stop = true;
+				else if (event->expired &&
+					 time_after_eq(jiffies,
+					 event->expired)) {
+					ptp->reg->read_event(ptp, i);
+					stop = true;
+				}
+			}
+		}
+		if ((ptp->ts_status & ptp->ts_intr) & tsi_bit) {
+			bool report = event->num >= event->max;
+			u8 data[24];
+
+			/* Delay reporting events until stopped. */
+			if (event->timeout && event->num < event->max) {
+				ptp->tsi_intr |= (tsi_bit << 16);
+			}
+			if (ptp->tsi_intr & tsi_bit)
+				report = true;
+			if (report) {
+				ptp->tsi_intr &= ~(tsi_bit << 16);
+				data[0] = PTP_CMD_GET_EVENT;
+				data[1] = i;
+				proc_ptp_get_event(ptp, data);
+			}
+			if (i == ptp->gps_tsi && ptp->gps_req_time) {
+				unsigned long diff = jiffies -
+					ptp->gps_req_time;
+
+				if (diff < 2 * ptp->delay_ticks) {
+					data[0] = TSM_CMD_GET_GPS_TS;
+					proc_tsm_get_gps(ptp, data);
+					ptp->gps_time.sec = 0;
+				}
+				ptp->gps_req_time = 0;
+			}
+
+			/* Not used in cascade mode. */
+			if (!event->timeout && !event->last) {
+				event->num = 0;
+				event->edge = 0;
+				ptp->reg->rx_restart(ptp, i);
+				stop = false;
+			}
+		}
+		if (stop) {
+			ptp->reg->rx_off(ptp, i);
+			if (ptp->tsi_intr & (tsi_bit << 16)) {
+				u8 data[24];
+
+				ptp->tsi_intr &= ~(tsi_bit << 16);
+				data[0] = PTP_CMD_GET_EVENT;
+				data[1] = i;
+				proc_ptp_get_event(ptp, data);
+			}
+			ptp->tsi_intr &= ~tsi_bit;
+			ptp->tsi_used &= ~tsi_bit;
+			ptp->tsi_dev[i] = NULL;
+			ptp->events[i].timeout = 0;
+			if (i + 1 == event->last) {
+				int tsi;
+				int last;
+
+				tsi = event->first;
+				last = event->last;
+				do {
+					if (tsi >= MAX_TIMESTAMP_UNIT)
+						tsi = 0;
+					ptp->events[tsi].first = 0;
+					ptp->events[tsi].last = 0;
+					++tsi;
+				} while (tsi != last);
+			}
+		}
+	}
+}  /* ptp_chk_rx_events */
+
+static void ptp_chk_stopped_repeat(struct ptp_info *ptp)
+{
+	struct ptp_output *out;
+	struct ptp_utime *cur, *intr, *trig;
+	struct ptp_utime t;
+	u8 tso;
+	u8 i;
+
+	t.sec = 0;
+	cur = &ptp->cur_time;
+	if (ptp->tso_chk[MAX_TRIG_UNIT]) {
+		out = &ptp->outputs[MAX_TRIG_UNIT];
+		intr = &out->intr;
+		if ((ptp->tso_chk[MAX_TRIG_UNIT] > 1) || cur->sec > intr->sec ||
+		    (cur->sec == intr->sec && cur->nsec > intr->nsec)) {
+			struct ptp_output *first, *next;
+
+			tso = ptp->cascade_first;
+			first = &ptp->outputs[tso];
+			trig = &first->start;
+			ptp->reg->get_time(ptp, &t);
+			update_new_cascade_time(ptp, tso, &t, trig);
+
+			tso = ptp->cascade_first + ptp->cascade_total - 1;
+			next = &ptp->outputs[tso];
+			*intr = next->intr;
+
+			/* Reset the repeat count for first TOU. */
+			tso = ptp->cascade_first;
+			if (ptp->cascade_sw_each) {
+				ptp->tso_cnt[tso] = first->cnt;
+			}
+			for (i = 0; i < ptp->cascade_total; i++, tso++) {
+				ptp->reg->tx_off(ptp, tso);
+			}
+			ptp->tso_chk[MAX_TRIG_UNIT] = 0;
+			out = &ptp->outputs[MAX_TRIG_UNIT];
+			ptp->ops->release(ptp);
+			ptp_core_tx_cascade(ptp, ptp->cascade_first,
+					    ptp->cascade_total, out->cnt,
+					    trig->sec, trig->nsec, 1);
+			ptp->ops->acquire(ptp);
+		}
+		return;
+	}
+	for (i = 0; i < MAX_TRIG_UNIT; i++) {
+		if (!ptp->tso_chk[i])
+			continue;
+		out = &ptp->outputs[i];
+		intr = &out->intr;
+		if (!((ptp->tso_chk[i] > 1) || cur->sec > intr->sec ||
+		    (cur->sec == intr->sec && cur->nsec > intr->nsec)))
+			continue;
+		if (ptp->tso_cnt[i]) {
+			ptp->reg->tx_off(ptp, i);
+			trig = &out->trig;
+			if (!t.sec)
+				ptp->reg->get_time(ptp, &t);
+			update_trig_and_intr(&t, &out->trig, &out->intr,
+					     out->cycle);
+
+			ptp->reg->tx_restart(ptp, i, 0, trig->sec, trig->nsec);
+dbg_msg(" r: %x:%9u %x:%9u\n",
+	t.sec & 0xf, t.nsec, trig->sec & 0xf, trig->nsec);
+			continue;
+		}
+	}
+}  /* ptp_chk_stopped_repeat */
+
+static void ptp_update_sec(struct work_struct *work)
+{
+	struct delayed_work *dwork = to_delayed_work(work);
+	struct ptp_info *ptp =
+		container_of(dwork, struct ptp_info, update_sec);
+
+#ifdef CONFIG_KSZ_IBA
+	if (iba_stopped(ptp->parent))
+		return;
+#endif
+
+	if (ptp->update_sec_jiffies) {
+		struct ptp_utime *cur_time = &ptp->cur_time;
+		struct ksz_ptp_time diff;
+		int msec;
+
+		static struct ptp_utime last_time;
+		static int last_msec = 1000;
+
+		/* Try to time the clock to trigger near the next second. */
+		msec = last_msec;
+
+		last_time = *cur_time;
+		ptp->ops->acquire(ptp);
+		ptp->reg->get_time(ptp, &ptp->cur_time);
+		ptp->ops->release(ptp);
+		calc_udiff(&last_time, cur_time, &diff);
+		if ((diff.sec == 1 ||
+		    (diff.sec == 0 && diff.nsec > 800000000))) {
+			if (diff.sec == 1 && diff.nsec > 24000000)
+				last_msec -= 10;
+			else if (diff.sec == 0 && diff.nsec < 960000000)
+				last_msec += 10;
+			if (!diff.sec && cur_time->nsec < 850000000)
+				last_msec += 50;
+			if (last_msec > 1000)
+				last_msec = 1000;
+			msec = last_msec;
+			if (cur_time->nsec < 900000000)
+				msec += 100;
+			else if (cur_time->nsec > 970000000)
+				msec -= 50;
+		}
+		schedule_delayed_work(&ptp->update_sec, msecs_to_jiffies(msec));
+#ifdef DBG_MSEC_SYNC
+dbg_msg(" msec: %x:%9u %d.%9d %d %d\n",
+	cur_time->sec, cur_time->nsec,
+	diff.sec, diff.nsec, last_msec, msec);
+#endif
+		if (ptp->cur_time.nsec >= 990000000) {
+			ptp->cur_time.nsec = 0;
+			ptp->cur_time.sec++;
+		}
+		ptp->sec_lo++;
+		if (!(ptp->sec_lo & 3)) {
+			check_expired_msg(ptp, ptp->rx_msg_info,
+				&ptp->rx_msg_lock, NULL);
+			check_expired_msg(ptp, ptp->tx_msg_info,
+				&ptp->tx_msg_lock, &ptp->tx_msg_cnt);
+		}
+		ptp->ops->acquire(ptp);
+		ptp_chk_rx_events(ptp);
+		ptp_chk_stopped_repeat(ptp);
+		ptp->ops->release(ptp);
+		ptp->clk_change = 0;
+	}
+}  /* ptp_update_sec */
+
+static u32 _get_clk_cnt(void)
+{
+	return 0;
+}
+
+#define ACCESS_VAL			1000
+
+static void test_get_time(struct ptp_info *ptp, struct ptp_utime *cur,
+	struct ptp_utime *now, u32 *cur_cnt, u32 *now_cnt)
+{
+	ptp->reg->get_time(ptp, cur);
+	*cur_cnt = ptp->get_clk_cnt();
+	ptp->reg->get_time(ptp, now);
+	*now_cnt = ptp->get_clk_cnt();
+}
+
+static void test_set_time(struct ptp_info *ptp, struct ptp_utime *cur,
+	struct ptp_utime *now, u32 *cur_cnt, u32 *now_cnt)
+{
+	*cur_cnt = ptp->get_clk_cnt();
+	ptp->reg->set_time(ptp, cur);
+	*now_cnt = ptp->get_clk_cnt();
+	ptp->reg->get_time(ptp, now);
+}
+
+static u32 test_avg_time(struct ptp_info *ptp,
+	void (*test_time)(struct ptp_info *ptp, struct ptp_utime *cur,
+	struct ptp_utime *now, u32 *cur_cnt, u32 *now_cnt))
+{
+	struct ptp_utime cur;
+	struct ptp_utime now;
+	struct ksz_ptp_time diff;
+	int i;
+	int clk_delay[6];
+	u32 cur_cnt;
+	u32 now_cnt;
+	u32 hw_delay[6];
+	u64 clk;
+
+	cur.sec = 5;
+	cur.nsec = 0x12345678;
+	ptp->ops->acquire(ptp);
+	for (i = 0; i < 5; i++) {
+		test_time(ptp, &cur, &now, &cur_cnt, &now_cnt);
+		calc_udiff(&cur, &now, &diff);
+		clk_delay[i] = (diff.nsec + (ACCESS_VAL / 2)) / ACCESS_VAL *
+			ACCESS_VAL;
+		hw_delay[i] = now_cnt - cur_cnt;
+	}
+	ptp->ops->release(ptp);
+	clk_delay[5] = 20000000;
+	hw_delay[5] = 50000000;
+	for (i = 0; i < 5; i++) {
+		clk = hw_delay[i];
+		clk *= 1000000;
+		if (ptp->clk_divider)
+			clk = div_u64_u32(clk, ptp->clk_divider);
+		dbg_msg(" %u %u=%llu"NL, clk_delay[i], hw_delay[i], clk);
+		if (clk_delay[i] < clk_delay[5])
+			clk_delay[5] = clk_delay[i];
+		if (hw_delay[i] < hw_delay[5])
+			hw_delay[5] = hw_delay[i];
+	}
+	clk = hw_delay[5];
+	clk *= 1000000;
+	if (ptp->clk_divider)
+		clk = div_u64_u32(clk, ptp->clk_divider);
+	dbg_msg("%u %llu"NL, clk_delay[5], clk);
+	return clk_delay[5];
+}
+
+static void _test_access_time(struct ptp_info *ptp)
+{
+	ptp->get_delay = test_avg_time(ptp, test_get_time);
+	ptp->set_delay = test_avg_time(ptp, test_set_time);
+	if (ptp->get_delay < 10000)
+		ptp->delay_ticks = 10 * HZ / 1000;
+	else if (ptp->get_delay < 12000000)
+		ptp->delay_ticks = 20 * HZ / 1000;
+	else
+		ptp->delay_ticks = 30 * HZ / 1000;
+	dbg_msg("delay_ticks: %lu"NL, ptp->delay_ticks);
+}  /* test_access_time */
+
+#ifndef NO_PPS_DETECT
+static void set_ptp_drift(struct ptp_info *ptp, int drift)
+{
+	drift /= 100;
+	drift *= 100;
+	drift = -drift;
+	ptp->first_drift = ptp->drift = drift;
+	ptp->first_sec = 0;
+	ptp->adjust = clk_adjust_val(drift, NANOSEC_IN_SEC);
+	set_ptp_adjust(ptp, ptp->adjust);
+	syntonize_clk(ptp);
+	ptp->ptp_synt = true;
+	dbg_msg("drift: %d"NL, drift);
+}  /* set_ptp_drift */
+
+static void check_sys_time(struct ptp_info *ptp, unsigned long cur_jiffies,
+	ktime_t cur_ktime)
+{
+	int diff;
+	int interval;
+	u32 cur_clk_cnt;
+
+	cur_clk_cnt = ptp->get_clk_cnt();
+	if (!ptp->first_drift) {
+		interval = 8;
+		diff = ptp->cur_time.sec - ptp->intr_sec;
+
+		/*
+		 * The second interval is not accurate after first setting up
+		 * the clock until later.
+		 */
+		if (diff < 6)
+			ptp->first_sec = 0;
+	} else
+		interval = 10;
+
+	if (!ptp->first_sec) {
+		ptp->last_clk_cnt = cur_clk_cnt;
+		ptp->total_clk_cnt = 0;
+		ptp->last_jiffies = cur_jiffies;
+		ptp->total_jiffies = 0;
+		ptp->first_ktime = cur_ktime;
+		ptp->first_sec = ptp->cur_time.sec;
+		return;
+	}
+
+	diff = ptp->cur_time.sec - ptp->first_sec;
+
+	if (diff >= 1 && !(diff % interval)) {
+		u64 clk;
+		u64 clk_cnt;
+		s64 drift_clk;
+		s64 drift_jiffies;
+		s64 drift_ktime;
+		u32 passed_sec;
+		u64 passed_usec;
+		u64 passed_nsec;
+		u32 cnt;
+
+		cnt = cur_clk_cnt - ptp->last_clk_cnt;
+		ptp->total_clk_cnt += cnt;
+		ptp->last_clk_cnt = cur_clk_cnt;
+
+		passed_sec = ptp->cur_time.sec - ptp->first_sec;
+		passed_usec = passed_sec;
+		passed_usec *= 1000000;
+		passed_nsec = passed_usec;
+		passed_nsec *= 1000;
+
+		cnt = cur_jiffies - ptp->last_jiffies;
+		ptp->total_jiffies += cnt;
+		ptp->last_jiffies = cur_jiffies;
+
+		clk = ptp->total_jiffies * (1000000 / HZ);
+		drift_jiffies = clk - passed_usec;
+		drift_jiffies *= 1000;
+		drift_jiffies = div_s64_u32(drift_jiffies, passed_sec);
+
+		cur_ktime -= ptp->first_ktime;
+		drift_ktime = cur_ktime - passed_nsec;
+		drift_ktime = div_s64_u32(drift_ktime, passed_sec);
+
+		if (!ptp->clk_divider) {
+			if (!ptp->first_drift)
+				set_ptp_drift(ptp, (int) drift_ktime);
+			else
+				printk(KERN_INFO "%lld %lld"NL,
+					drift_jiffies, drift_ktime);
+			return;
+		}
+
+		clk_cnt = div_u64_u32(ptp->total_clk_cnt, passed_sec);
+
+		clk = ptp->total_clk_cnt * 1000000;
+		clk = div_u64_u32(clk, ptp->clk_divider);
+		drift_clk = clk;
+		if (drift_clk < 0)
+			ptp->overrides &= ~PTP_CHECK_SYS_TIME;
+		drift_clk -= passed_nsec;
+		drift_clk = div_s64_u32(drift_clk, passed_sec);
+
+		if (!ptp->first_drift)
+			set_ptp_drift(ptp, (int) drift_clk);
+		else
+			printk(KERN_INFO "%10llu %lld %lld %lld"NL,
+				clk_cnt, drift_clk, drift_jiffies, drift_ktime);
+	}
+}  /* check_sys_time */
+#endif
+
+static bool ptp_chk_repeat(struct ptp_info *ptp, u8 tso, u16 error, u16 done,
+			   struct ptp_utime *t)
+{
+	/* Repeat within regular TOU operation. */
+	if (ptp->tso_cnt[tso]) {
+		return ptp_do_repeat(ptp, tso, done, t);
+	}
+
+	/* Repeat manually for each TOU in cascade mode. */
+	if (ptp->cascade_sw_each) {
+		return ptp_do_cascade_each(ptp, tso, done, t);
+	}
+
+	/* Repeat within regular TOU cascade operation. */
+	if (ptp->tso_cnt[MAX_TRIG_UNIT]) {
+		return ptp_do_cascade_repeat(ptp);
+	}
+	return false;
+}
+
+static void proc_ptp_intr(struct ptp_info *ptp)
+{
+	struct ptp_event *event;
+	struct ptp_utime t;
+	u16 done;
+	u16 error;
+	u16 status;
+	u32 trig_status;
+	u32 int_status;
+	u16 tsi_bit;
+	u8 data[24];
+	int i;
+	int tsi;
+	ktime_t cur_ktime;
+	struct timespec64 ts;
+	struct ksz_sw *sw = ptp->parent;
+
+	cur_ktime = ktime_get_real();
+	ts = ktime_to_timespec64(cur_ktime);
+
+proc_chk_trig_intr:
+	t.sec = 0;
+	int_status = sw->reg->r32(sw, REG_PTP_INT_STATUS__4);
+	if (!int_status)
+		goto proc_ptp_intr_done;
+
+	sw->reg->w32(sw, REG_PTP_INT_STATUS__4, int_status);
+
+	status = (int_status >> TRIG_INT_S) & PTP_TRIG_UNIT_M;
+	if (!status)
+		goto proc_chk_ts_intr;
+
+	trig_status = sw->reg->r32(sw, REG_PTP_TRIG_STATUS__4);
+	error = (trig_status >> TRIG_ERROR_S) & PTP_TRIG_UNIT_M;
+	done = trig_status & PTP_TRIG_UNIT_M;
+	for (i = 0; i < MAX_TRIG_UNIT; i++) {
+		if (status & (1 << i)) {
+			if (!t.sec)
+				ptp->reg->get_time(ptp, &t);
+#if 1
+			if (error && ptp->clk_change) {
+				dbg_msg(" done: %x:%9u %u %x %x\n",
+					t.sec & 0xf, t.nsec, i, error, done);
+			}
+#endif
+			if (ptp_chk_repeat(ptp, i, error, done, &t))
+				continue;
+			if (ptp->tso_intr & (1 << i)) {
+				struct ptp_output *cur = &ptp->outputs[i];
+
+				if (!t.sec)
+					ptp->reg->get_time(ptp, &t);
+				cur->stop = t;
+				data[0] = PTP_CMD_GET_OUTPUT;
+				data[1] = i;
+				proc_ptp_get_trig(ptp, data, done, error);
+			}
+			ptp_tx_done(ptp, i);
+		}
+	}
+	if (trig_status)
+		sw->reg->w32(sw, REG_PTP_TRIG_STATUS__4, trig_status);
+
+proc_chk_ts_intr:
+	status = (int_status >> TS_INT_S) & PTP_TS_UNIT_M;
+	if (!status)
+		goto proc_ptp_port_intr;
+
+	for (i = 0; i < MAX_TIMESTAMP_UNIT; i++) {
+		tsi_bit = 1 << i;
+		if (!(status & tsi_bit))
+			continue;
+		ptp->reg->read_event(ptp, i);
+		event = &ptp->events[i];
+		if (event->timeout &&
+		    (event->num < event->max || event->last)) {
+			unsigned long expired;
+
+			expired = jiffies + event->timeout;
+			if (0 == expired)
+				expired = 1;
+			event->expired = expired;
+			if (event->last) {
+				tsi = i + 1;
+				while (tsi != event->last) {
+					if (tsi >= MAX_TIMESTAMP_UNIT)
+						tsi = 0;
+					ptp->events[tsi].expired = expired;
+
+					/* Extend timeout for next unit. */
+					ptp->events[tsi].timeout =
+						event->timeout;
+					++tsi;
+				}
+			}
+		}
+		if (event->last && i != event->first) {
+			tsi = i - 1;
+			if (tsi < 0)
+				tsi = MAX_TIMESTAMP_UNIT - 1;
+			if (ptp->tsi_used & (1 << tsi))
+				ptp->events[tsi].expired = jiffies;
+		}
+
+		/* For system use only. */
+		if (!(ptp->tsi_sys & tsi_bit))
+			continue;
+#ifndef NO_PPS_DETECT
+		if (i == ptp->pps_tsi) {
+			struct ptp_utime sys_time;
+
+			ptp->cur_time.sec = event->t[0].sec;
+			ptp->cur_time.nsec = event->t[0].nsec;
+			ptp->update_sec_jiffies = 0;
+			ptp->clk_change = 0;
+			ptp->sec_lo++;
+			sys_time.sec = ts.tv_sec;
+			sys_time.nsec = ts.tv_nsec;
+			calc_udiff(&ptp->cur_time, &sys_time, &ptp->time_diff);
+			if (!ptp->intr_sec)
+				ptp->intr_sec = ptp->cur_time.sec;
+#if 1
+			if ((ptp->overrides & PTP_CHECK_SYS_TIME) ||
+					!ptp->first_drift)
+				check_sys_time(ptp, jiffies, cur_ktime);
+#endif
+#ifdef CONFIG_PTP_1588_CLOCK
+			if (ptp->clock_events & (1 << 0))
+				ptp_event_trigger(ptp->clock_info, 0,
+					ptp->cur_time.sec, ptp->cur_time.nsec);
+			if (ptp->clock_events & (1 << 31))
+				ptp_event_pps(ptp->clock_info);
+#endif
+			if (!(ptp->sec_lo & 3)) {
+				check_expired_msg(ptp, ptp->rx_msg_info,
+					&ptp->rx_msg_lock, NULL);
+				check_expired_msg(ptp, ptp->tx_msg_info,
+					&ptp->tx_msg_lock, &ptp->tx_msg_cnt);
+			}
+		} else if (i == ptp->gps_tsi) {
+			ptp->gps_time.sec = event->t[0].sec;
+			ptp->gps_time.nsec = event->t[0].nsec;
+			++ptp->gps_seqid;
+			ptp->gps_resp_time = jiffies;
+		}
+#endif
+	}
+	ptp_chk_rx_events(ptp);
+	ptp->ts_status = 0;
+
+proc_ptp_port_intr:
+
+	goto proc_chk_trig_intr;
+
+proc_ptp_intr_done:
+	return;
+}  /* proc_ptp_intr */
+
+static int ptp_get_ts_info(struct ptp_info *ptp, struct net_device *dev,
+	struct ethtool_ts_info *info)
+{
+	int ptp_clock = false;
+	struct ksz_sw *sw = ptp->parent;
+	int ret = -ENODEV;
+
+#ifdef CONFIG_PTP_1588_CLOCK
+	ptp_clock = true;
+#endif
+	if (!(sw->features & PTP_HW) || !ptp_clock)
+		return ethtool_op_get_ts_info(dev, info);
+#ifdef CONFIG_PTP_1588_CLOCK
+	if (ptp->clock_info)
+		ret = micrel_ptp_get_ts_info(ptp, info);
+#endif
+	return ret;
+}  /* ptp_get_ts_info */
+
+static void proc_ptp_tx_intr(struct ptp_info *ptp, uint port)
+{
+	uint p = port;
+	u16 status;
+	struct ksz_sw *sw = ptp->parent;
+
+	sw->ops->p_r16(sw, p, REG_PTP_PORT_TX_INT_STATUS__2, &status);
+	if (status) {
+		sw->ops->p_w16(sw, p, REG_PTP_PORT_TX_INT_STATUS__2, status);
+		status &= ptp->tx_intr;
+		if (get_tx_time(ptp, port, p, status))
+			wake_up_interruptible(&ptp->wait_ts[port]);
+	}
+}  /* proc_ptp_tx_intr */
+
+#define PTP_ENABLE_TXTS		SIOCDEVPRIVATE
+#define PTP_DISABLE_TXTS	(SIOCDEVPRIVATE + 1)
+#define PTP_ENABLE_RXTS		(SIOCDEVPRIVATE + 2)
+#define PTP_DISABLE_RXTS	(SIOCDEVPRIVATE + 3)
+#define PTP_GET_TX_TIMESTAMP	(SIOCDEVPRIVATE + 4)
+#define PTP_GET_RX_TIMESTAMP	(SIOCDEVPRIVATE + 5)
+#define PTP_SET_TIME		(SIOCDEVPRIVATE + 6)
+#define PTP_GET_TIME		(SIOCDEVPRIVATE + 7)
+#define PTP_SET_FIPER_ALARM	(SIOCDEVPRIVATE + 8)
+#define PTP_SET_ADJ		(SIOCDEVPRIVATE + 9)
+#define PTP_GET_ADJ		(SIOCDEVPRIVATE + 10)
+#define PTP_CLEANUP_TS		(SIOCDEVPRIVATE + 11)
+#define PTP_ADJ_TIME		(SIOCDEVPRIVATE + 12)
+
+struct ixxat_ptp_time {
+	/* just 48 bit used */
+	u64 sec;
+	u32 nsec;
+};
+
+struct ixxat_ptp_ident {
+	u8 vers;
+	u8 mType;
+	u16 netwProt;
+	u16 seqId;
+	struct ptp_port_identity portId;
+} __packed;
+
+/* needed for timestamp data over ioctl */
+struct ixxat_ptp_data {
+	struct ixxat_ptp_ident ident;
+	struct ixxat_ptp_time ts;
+};
+
+static int ixxat_ptp_ioctl(struct ptp_info *ptp, unsigned int cmd,
+	struct ifreq *ifr)
+{
+	struct ixxat_ptp_time ptp_time;
+	struct ixxat_ptp_data ptp_data;
+	struct ptp_clk_options clk_opt;
+	int output;
+	s64 scaled_nsec;
+	struct ptp_ts ts;
+	struct ptp_tx_ts *tx;
+	int drift;
+	int err = 0;
+	uint port;
+	struct ksz_sw *sw = ptp->parent;
+
+	switch (cmd) {
+	case PTP_ENABLE_TXTS:
+		ptp->tx_en |= 2;
+		break;
+	case PTP_DISABLE_TXTS:
+		ptp->tx_en &= ~2;
+		break;
+	case PTP_ENABLE_RXTS:
+		ptp->rx_en |= 2;
+		break;
+	case PTP_DISABLE_RXTS:
+		ptp->rx_en &= ~2;
+		break;
+	case PTP_GET_TX_TIMESTAMP:
+		if (copy_from_user(&ptp_data, ifr->ifr_data, sizeof(ptp_data)))
+			return -EFAULT;
+
+		err = -EINVAL;
+		port = htons(ptp_data.ident.portId.port);
+		if (port < 1 || port > ptp->ports)
+			break;
+		port = get_phy_port(sw, port);
+		tx = proc_get_ts(ptp, port, ptp_data.ident.mType,
+			ptp_data.ident.seqId,
+			ptp_data.ident.portId.clockIdentity.addr,
+			NULL, 0);
+		if (!tx)
+			break;
+		ptp_data.ts.sec = tx->ts.r.sec;
+		ptp_data.ts.nsec = tx->ts.r.nsec;
+		tx->ts.timestamp = 0;
+		tx->req_time = 0;
+		err = copy_to_user(ifr->ifr_data, &ptp_data, sizeof(ptp_data));
+		break;
+	case PTP_GET_RX_TIMESTAMP:
+		if (copy_from_user(&ptp_data, ifr->ifr_data, sizeof(ptp_data)))
+			return -EFAULT;
+
+		ts.timestamp = ptp_data.ts.nsec;
+		if (ts.timestamp)
+			update_ts(&ts, ptp->cur_time.sec);
+		else {
+			struct ptp_msg_hdr hdr;
+			struct ptp_msg_options tx_msg;
+			int found;
+
+			ts.t.sec = ts.t.nsec = 0;
+			memcpy(&hdr.sourcePortIdentity, &ptp_data.ident.portId,
+				sizeof(struct ptp_port_identity));
+			hdr.messageType = ptp_data.ident.mType;
+			hdr.sequenceId = htons(ptp_data.ident.seqId);
+			hdr.domainNumber = ptp_data.ident.vers;
+			found = find_msg_info(&ptp->rx_msg_info[
+				hdr.messageType],
+				&ptp->rx_msg_lock, &hdr,
+				&hdr.sourcePortIdentity,
+				PDELAY_REQ_MSG != hdr.messageType, &tx_msg);
+			if (found) {
+				ts.t = tx_msg.ts.t;
+			}
+		}
+		ptp_data.ts.sec = ts.t.sec;
+		ptp_data.ts.nsec = ts.t.nsec;
+		err = copy_to_user(ifr->ifr_data, &ptp_data, sizeof(ptp_data));
+		break;
+	case PTP_GET_TIME:
+	{
+		struct timespec64 ts;
+		struct ksz_ptp_time cur_time;
+		struct ksz_ptp_time sys_time;
+
+		ts = ktime_to_timespec64(ktime_get_real());
+		sys_time.sec = ts.tv_sec;
+		sys_time.nsec = ts.tv_nsec;
+		calc_diff(&ptp->time_diff, &sys_time, &cur_time);
+		ptp_time.sec = cur_time.sec;
+		ptp_time.nsec = cur_time.nsec;
+		err = proc_ptp_hw_access(ptp,
+			DEV_CMD_GET, DEV_PTP_CLK, 0,
+			&clk_opt, sizeof(clk_opt), NULL, &output,
+			true);
+		if (err)
+			break;
+		ptp_time.sec = clk_opt.sec;
+		ptp_time.nsec = clk_opt.nsec;
+		err = copy_to_user(ifr->ifr_data, &ptp_time, sizeof(ptp_time));
+		break;
+	}
+	case PTP_SET_TIME:
+		if (copy_from_user(&ptp_time, ifr->ifr_data, sizeof(ptp_time)))
+			return -EFAULT;
+		output = 0;
+		clk_opt.sec = (u32) ptp_time.sec;
+		clk_opt.nsec = ptp_time.nsec;
+		err = proc_ptp_hw_access(ptp,
+			DEV_CMD_PUT, DEV_PTP_CLK, output,
+			&clk_opt, sizeof(clk_opt), NULL, &output,
+			true);
+		break;
+	case PTP_ADJ_TIME:
+		if (copy_from_user(&scaled_nsec, ifr->ifr_data, sizeof(s64)))
+			return -EFAULT;
+		convert_scaled_nsec(scaled_nsec, SCALED_NANOSEC_S,
+			&ptp->adjust_sec, &ptp->adjust_offset);
+		if (ptp->adjust_offset < 0 || ptp->adjust_sec < 0) {
+			output = 1;
+			ptp->adjust_sec = -ptp->adjust_sec;
+			ptp->adjust_offset = -ptp->adjust_offset;
+		} else
+			output = 2;
+		clk_opt.sec = (u32) ptp->adjust_sec;
+		clk_opt.nsec = ptp->adjust_offset;
+		clk_opt.interval = 0;
+		ptp->adjust_sec = 0;
+		ptp->adjust_offset = 0;
+		err = proc_ptp_hw_access(ptp,
+			DEV_CMD_PUT, DEV_PTP_CLK, output,
+			&clk_opt, sizeof(clk_opt), NULL, &output,
+			true);
+		break;
+	case PTP_SET_ADJ:
+		if (copy_from_user(&drift, ifr->ifr_data, sizeof(drift)))
+			return -EFAULT;
+		output = 1;
+		clk_opt.sec = clk_opt.nsec = 0;
+		clk_opt.drift = drift;
+		clk_opt.interval = NANOSEC_IN_SEC;
+		err = proc_ptp_hw_access(ptp,
+			DEV_CMD_PUT, DEV_PTP_CLK, output,
+			&clk_opt, sizeof(clk_opt), NULL, &output,
+			true);
+		break;
+	case PTP_GET_ADJ:
+		drift = ptp->drift;
+		err = copy_to_user(ifr->ifr_data, &drift, sizeof(drift));
+		break;
+	case PTP_CLEANUP_TS:
+		break;
+	case PTP_SET_FIPER_ALARM:
+		break;
+	default:
+		err = -EOPNOTSUPP;
+	}
+	return err;
+}
+
+static int ptp_dev_req(struct ptp_info *ptp, char *arg,
+	struct file_dev_info *info)
+{
+	struct ksz_request *req = (struct ksz_request *) arg;
+	int len;
+	int maincmd;
+	int req_size;
+	int subcmd;
+	int output;
+	u8 data[PARAM_DATA_SIZE];
+	struct file_dev_info *dev;
+	int err = 0;
+	int result = 0;
+	int v2 = 0;
+
+	/* Assume success. */
+	result = DEV_IOC_OK;
+
+	/* Check request size. */
+	__get_user(req_size, &req->size);
+	if (chk_ioctl_size(req_size, SIZEOF_ksz_request, 0, &req_size,
+			&result, NULL, NULL))
+		goto dev_ioctl_resp;
+
+	err = 0;
+	__get_user(maincmd, &req->cmd);
+	__get_user(subcmd, &req->subcmd);
+	__get_user(output, &req->output);
+	len = req_size - SIZEOF_ksz_request;
+	switch (maincmd) {
+	case DEV_CMD_INFO:
+		switch (subcmd) {
+		case DEV_INFO_INIT:
+		{
+			struct ksz_sw *sw = ptp->parent;
+			int p = ptp->ports;
+
+			if (chk_ioctl_size(len, 6,
+					SIZEOF_ksz_request,
+					&req_size, &result, &req->param, data))
+				goto dev_ioctl_resp;
+
+			if (len >= 8 &&
+			    'v' == data[4] && '2' == data[5])
+				v2 = 1;
+			result = proc_ptp_hw_access(ptp,
+				maincmd, subcmd, output,
+				data, len, info, &output,
+				true);
+			__put_user(output, &req->output);
+			data[0] = 'M';
+			data[1] = 'i';
+			data[2] = 'c';
+			data[3] = 'r';
+			data[4] = ptp->version;
+			if (v2) {
+				++p;
+				data[6] = sw->HOST_PORT + 1;
+				data[7] = 0;
+			}
+			data[5] = p;
+			if (!access_ok(req->param.data, len) ||
+			    copy_to_user(req->param.data, data, len)) {
+				err = -EFAULT;
+				goto dev_ioctl_done;
+			}
+			break;
+		}
+		case DEV_INFO_EXIT:
+			result = proc_ptp_hw_access(ptp,
+				maincmd, subcmd, output,
+				data, 0, info, &output,
+				true);
+			fallthrough;
+
+		case DEV_INFO_QUIT:
+			if (!info)
+				break;
+			data[0] = 0xF0;
+			dev = find_minor_dev(info);
+			if (dev)
+				file_dev_setup_msg(dev, data, 4, NULL, NULL);
+			file_dev_setup_msg(info, data, 4, NULL, NULL);
+			break;
+		case DEV_INFO_RESET:
+			if (output < 3) {
+				result = proc_ptp_hw_access(ptp,
+					maincmd, subcmd, output,
+					data, 0, info, &output,
+					false);
+			} else
+				result = -EINVAL;
+			break;
+		case DEV_INFO_PORT:
+			if (len < 6) {
+				result = DEV_IOC_INVALID_LEN;
+				break;
+			}
+			if (chk_ioctl_size(len, len,
+					SIZEOF_ksz_request,
+					&req_size, &result, &req->param, data))
+				goto dev_ioctl_resp;
+
+			result = proc_ptp_hw_access(ptp,
+				maincmd, subcmd, output,
+				data, len, info, &output,
+				true);
+			if (!result) {
+				len = 6;
+				__put_user(output, &req->output);
+				if (!access_ok(req->param.data,
+				    len) ||
+				    copy_to_user(req->param.data, data, len)) {
+					err = -EFAULT;
+					goto dev_ioctl_done;
+				}
+			}
+			break;
+		default:
+			result = DEV_IOC_INVALID_CMD;
+			break;
+		}
+		break;
+	case DEV_CMD_PUT:
+		switch (subcmd) {
+		case DEV_PTP_CFG:
+			if (chk_ioctl_size(len, sizeof(struct ptp_cfg_options),
+					SIZEOF_ksz_request,
+					&req_size, &result, &req->param, data))
+				goto dev_ioctl_resp;
+			result = proc_ptp_hw_access(ptp,
+				maincmd, subcmd, 0,
+				data, len, info, &output,
+				false);
+			break;
+		case DEV_PTP_TEVT:
+			if (chk_ioctl_size(len, sizeof(struct ptp_tsi_options),
+					SIZEOF_ksz_request,
+					&req_size, &result, &req->param, data))
+				goto dev_ioctl_resp;
+			if (!info) {
+				err = -EFAULT;
+				goto dev_ioctl_done;
+			}
+			result = proc_ptp_hw_access(ptp,
+				maincmd, subcmd, output,
+				data, len, info, &output,
+				true);
+			__put_user(output, &req->output);
+			break;
+		case DEV_PTP_TOUT:
+			if (chk_ioctl_size(len, sizeof(struct ptp_tso_options),
+					SIZEOF_ksz_request,
+					&req_size, &result, &req->param, data))
+				goto dev_ioctl_resp;
+			if (!info) {
+				err = -EFAULT;
+				goto dev_ioctl_done;
+			}
+			result = proc_ptp_hw_access(ptp,
+				maincmd, subcmd, output,
+				data, len, info, &output,
+				true);
+			__put_user(output, &req->output);
+			break;
+		case DEV_PTP_CASCADE:
+			if (chk_ioctl_size(len, sizeof(struct ptp_tso_options),
+					SIZEOF_ksz_request,
+					&req_size, &result, &req->param, data))
+				goto dev_ioctl_resp;
+			result = proc_ptp_hw_access(ptp,
+				maincmd, subcmd, output,
+				data, len, info, &output,
+				true);
+			__put_user(output, &req->output);
+			break;
+		case DEV_PTP_CLK:
+			if (chk_ioctl_size(len, sizeof(struct ptp_clk_options),
+					SIZEOF_ksz_request,
+					&req_size, &result, &req->param, data))
+				goto dev_ioctl_resp;
+			result = proc_ptp_hw_access(ptp,
+				maincmd, subcmd, output,
+				data, len, info, &output,
+				true);
+			break;
+		case DEV_PTP_DELAY:
+			if (chk_ioctl_size(len,
+					sizeof(struct ptp_delay_values),
+					SIZEOF_ksz_request,
+					&req_size, &result, &req->param, data))
+				goto dev_ioctl_resp;
+			result = proc_ptp_hw_access(ptp,
+				maincmd, subcmd, output,
+				data, len, info, &output,
+				true);
+			break;
+		case DEV_PTP_REG:
+			if ((ptp->op_mode > 0 || ptp->cap) &&
+			    chk_ioctl_size(len,
+					sizeof(u32) * 3,
+					SIZEOF_ksz_request,
+					&req_size, &result, &req->param, data))
+				goto dev_ioctl_resp;
+			result = proc_ptp_hw_access(ptp,
+				maincmd, subcmd, output,
+				data, len, info, &output,
+				false);
+			break;
+		case DEV_PTP_IDENTITY:
+			if (chk_ioctl_size(len,
+					sizeof(struct ptp_clock_identity),
+					SIZEOF_ksz_request,
+					&req_size, &result, &req->param, data))
+				goto dev_ioctl_resp;
+			if ((ptp->op_mode > 0 || ptp->cap) && output) {
+				memcpy(&ptp->masterIdentity, data,
+					sizeof(struct ptp_clock_identity));
+				break;
+			}
+			memcpy(&ptp->clockIdentity, data,
+				sizeof(struct ptp_clock_identity));
+			break;
+		case DEV_PTP_PEER_DELAY:
+			if (chk_ioctl_size(len,
+					sizeof(struct ptp_delay_values),
+					SIZEOF_ksz_request,
+					&req_size, &result, &req->param, data))
+				goto dev_ioctl_resp;
+			result = proc_ptp_hw_access(ptp,
+				maincmd, subcmd, output,
+				data, len, info, &output,
+				true);
+			break;
+		case DEV_PTP_UTC_OFFSET:
+			ptp->utc_offset = output;
+			break;
+		case DEV_PTP_MSG:
+			if (chk_ioctl_size(len,
+					sizeof(struct ptp_msg_options),
+					SIZEOF_ksz_request,
+					&req_size, &result, &req->param, data))
+				goto dev_ioctl_resp;
+			result = proc_ptp_set_msg_info(ptp, data, info);
+			break;
+		case DEV_PTP_PORT_CFG:
+			if (chk_ioctl_size(len,
+					sizeof(struct ptp_delay_values),
+					SIZEOF_ksz_request,
+					&req_size, &result, &req->param, data))
+				goto dev_ioctl_resp;
+			result = proc_ptp_hw_access(ptp,
+				maincmd, subcmd, output,
+				data, len, info, &output,
+				true);
+			break;
+		default:
+			result = DEV_IOC_INVALID_CMD;
+			break;
+		}
+		break;
+	case DEV_CMD_GET:
+		switch (subcmd) {
+		case DEV_PTP_CFG:
+			if (chk_ioctl_size(len, sizeof(struct ptp_cfg_options),
+					SIZEOF_ksz_request,
+					&req_size, &result, NULL, NULL))
+				goto dev_ioctl_resp;
+			result = proc_ptp_hw_access(ptp,
+				maincmd, subcmd, 0,
+				data, len, info, &output,
+				true);
+			if (!access_ok(req->param.data,
+					sizeof(struct ptp_cfg_options)) ||
+					copy_to_user(req->param.data, data,
+					sizeof(struct ptp_cfg_options))) {
+				err = -EFAULT;
+				goto dev_ioctl_done;
+			}
+			break;
+		case DEV_PTP_TEVT:
+			if (chk_ioctl_size(len, sizeof(struct ptp_tsi_info),
+					SIZEOF_ksz_request,
+					&req_size, &result, &req->param, data))
+				goto dev_ioctl_resp;
+			if (!info) {
+				err = -EFAULT;
+				goto dev_ioctl_done;
+			}
+			if (output) {
+				int wait = false;
+
+				if (2 == output)
+					wait = true;
+				result = proc_ptp_hw_access(ptp,
+					maincmd, subcmd, output,
+					data, len, info, &output,
+					wait);
+				if (!wait)
+					break;
+				if (!access_ok(req->param.data,
+				    len) || copy_to_user(req->param.data,
+				    data, len)) {
+					err = -EFAULT;
+					goto dev_ioctl_done;
+				}
+			} else
+				result = proc_dev_get_event(info, data);
+			break;
+		case DEV_PTP_TOUT:
+			if (chk_ioctl_size(len, sizeof(struct ptp_tso_options),
+					SIZEOF_ksz_request,
+					&req_size, &result, &req->param, data))
+				goto dev_ioctl_resp;
+			result = proc_ptp_get_output(ptp, data);
+			output = *((int *) data);
+			__put_user(output, &req->output);
+			break;
+		case DEV_PTP_CLK:
+			if (chk_ioctl_size(len, sizeof(struct ptp_clk_options),
+					SIZEOF_ksz_request,
+					&req_size, &result, NULL, NULL))
+				goto dev_ioctl_resp;
+			if (0 == ptp->op_mode && !ptp->cap)
+				output = 0;
+			result = proc_ptp_hw_access(ptp,
+				maincmd, subcmd, output,
+				data, len, info, &output,
+				true);
+			__put_user(output, &req->output);
+			if (!access_ok(req->param.data,
+					sizeof(struct ptp_clk_options)) ||
+					copy_to_user(req->param.data, data,
+					sizeof(struct ptp_clk_options))) {
+				err = -EFAULT;
+				goto dev_ioctl_done;
+			}
+			break;
+		case DEV_PTP_DELAY:
+			if (chk_ioctl_size(len,
+					sizeof(struct ptp_delay_values),
+					SIZEOF_ksz_request,
+					&req_size, &result, NULL, NULL))
+				goto dev_ioctl_resp;
+			result = proc_ptp_hw_access(ptp,
+				maincmd, subcmd, output,
+				data, len, info, &output,
+				true);
+			if (!access_ok(req->param.data,
+					sizeof(struct ptp_delay_values)) ||
+					copy_to_user(req->param.data, data,
+					sizeof(struct ptp_delay_values))) {
+				err = -EFAULT;
+				goto dev_ioctl_done;
+			}
+			break;
+		case DEV_PTP_REG:
+			if ((ptp->op_mode > 0 || ptp->cap) &&
+			    chk_ioctl_size(len,
+					sizeof(u32) * 3,
+					SIZEOF_ksz_request,
+					&req_size, &result, &req->param, data))
+				goto dev_ioctl_resp;
+			result = proc_ptp_hw_access(ptp,
+				maincmd, subcmd, output,
+				data, len, info, &output,
+				true);
+			__put_user(output, &req->output);
+			break;
+		case DEV_PTP_IDENTITY:
+		{
+			struct ptp_clock_identity *id = &ptp->clockIdentity;
+
+			if ((ptp->op_mode > 0 || ptp->cap) && output)
+				id = &ptp->masterIdentity;
+			if (!access_ok(req->param.data,
+					sizeof(struct ptp_clock_identity)) ||
+					copy_to_user(req->param.data, id,
+					sizeof(struct ptp_clock_identity))) {
+				err = -EFAULT;
+				goto dev_ioctl_done;
+			}
+			break;
+		}
+		case DEV_PTP_PEER_DELAY:
+			if (chk_ioctl_size(len,
+					sizeof(struct ptp_delay_values),
+					SIZEOF_ksz_request,
+					&req_size, &result, NULL, NULL))
+				goto dev_ioctl_resp;
+			result = proc_ptp_hw_access(ptp,
+				maincmd, subcmd, output,
+				data, len, info, &output,
+				true);
+			if (!access_ok(req->param.data,
+					sizeof(struct ptp_delay_values)) ||
+					copy_to_user(req->param.data, data,
+					sizeof(struct ptp_delay_values))) {
+				err = -EFAULT;
+				goto dev_ioctl_done;
+			}
+			break;
+		case DEV_PTP_UTC_OFFSET:
+			__put_user(ptp->utc_offset, &req->output);
+			break;
+		case DEV_PTP_TIMESTAMP:
+			if (chk_ioctl_size(len,
+					sizeof(struct ptp_ts_options),
+					SIZEOF_ksz_request,
+					&req_size, &result, &req->param, data))
+				goto dev_ioctl_resp;
+			result = proc_ptp_get_timestamp(ptp, data, info);
+			if (result)
+				goto dev_ioctl_resp;
+			if (!access_ok(req->param.data,
+					sizeof(struct ptp_ts_options)) ||
+					copy_to_user(req->param.data, data,
+					sizeof(struct ptp_ts_options))) {
+				err = -EFAULT;
+				goto dev_ioctl_done;
+			}
+			break;
+		case DEV_PTP_MSG:
+			if (chk_ioctl_size(len,
+					sizeof(struct ptp_msg_options),
+					SIZEOF_ksz_request,
+					&req_size, &result, &req->param, data))
+				goto dev_ioctl_resp;
+			result = proc_ptp_get_msg_info(ptp, data, info,
+				&output);
+			__put_user(output, &req->output);
+			if (result)
+				goto dev_ioctl_resp;
+			if (!access_ok(req->param.data,
+					sizeof(struct ptp_msg_options)) ||
+					copy_to_user(req->param.data, data,
+					sizeof(struct ptp_msg_options))) {
+				err = -EFAULT;
+				goto dev_ioctl_done;
+			}
+			break;
+		case DEV_PTP_PORT_CFG:
+			if (chk_ioctl_size(len,
+					sizeof(struct ptp_delay_values),
+					SIZEOF_ksz_request,
+					&req_size, &result, NULL, NULL))
+				goto dev_ioctl_resp;
+			result = proc_ptp_hw_access(ptp,
+				maincmd, subcmd, output,
+				data, len, info, &output,
+				true);
+			if (!access_ok(req->param.data,
+					sizeof(struct ptp_delay_values)) ||
+					copy_to_user(req->param.data, data,
+					sizeof(struct ptp_delay_values))) {
+				err = -EFAULT;
+				goto dev_ioctl_done;
+			}
+			break;
+		default:
+			result = DEV_IOC_INVALID_CMD;
+			break;
+		}
+		break;
+	default:
+		result = DEV_IOC_INVALID_CMD;
+		break;
+	}
+
+dev_ioctl_resp:
+	__put_user(req_size, &req->size);
+	__put_user(result, &req->result);
+
+	/* Return ERESTARTSYS so that the system call is called again. */
+	if (result < 0)
+		err = result;
+
+dev_ioctl_done:
+	return err;
+}  /* ptp_dev_req */
+
+static long ptp_dev_ioctl(struct file *filp, unsigned int cmd,
+	unsigned long arg)
+{
+	struct file_dev_info *info = (struct file_dev_info *)
+		filp->private_data;
+	struct ptp_info *ptp = info->dev;
+	int err = 0;
+
+	if (_IOC_TYPE(cmd) != DEV_IOC_MAGIC)
+		return -ENOTTY;
+	if (_IOC_NR(cmd) > DEV_IOC_MAX)
+		return -ENOTTY;
+	if (_IOC_DIR(cmd) & _IOC_READ)
+		err = !access_ok((void *) arg, _IOC_SIZE(cmd));
+	else if (_IOC_DIR(cmd) & _IOC_WRITE)
+		err = !access_ok((void *) arg, _IOC_SIZE(cmd));
+	if (err) {
+		printk(KERN_ALERT "err fault"NL);
+		return -EFAULT;
+	}
+	if (down_interruptible(&info->sem))
+		return -ERESTARTSYS;
+
+	err = ptp_dev_req(ptp, (char *) arg, info);
+	up(&info->sem);
+	return err;
+}  /* ptp_dev_ioctl */
+
+static ssize_t ptp_dev_read(struct file *filp, char *buf, size_t count,
+	loff_t *offp)
+{
+	struct file_dev_info *info = (struct file_dev_info *)
+		filp->private_data;
+	ssize_t result = 0;
+	int rc;
+
+	if (!info->read_len) {
+		*offp = 0;
+		rc = wait_event_interruptible(info->wait_msg,
+			0 != info->read_len);
+
+		/* Cannot continue if ERESTARTSYS. */
+		if (rc < 0)
+			return 0;
+	}
+
+	if (down_interruptible(&info->sem))
+		return -ERESTARTSYS;
+
+	mutex_lock(&info->lock);
+	if (*offp >= info->read_len) {
+		info->read_len = 0;
+		count = 0;
+		*offp = 0;
+		goto dev_read_done;
+	}
+
+	if (*offp + count > info->read_len) {
+		count = info->read_len - *offp;
+		info->read_len = 0;
+	}
+
+	if (copy_to_user(buf, &info->read_buf[*offp], count)) {
+		result = -EFAULT;
+		goto dev_read_done;
+	}
+	if (info->read_len)
+		*offp += count;
+	else
+		*offp = 0;
+	result = count;
+
+dev_read_done:
+	mutex_unlock(&info->lock);
+	up(&info->sem);
+	return result;
+}  /* ptp_dev_read */
+
+static ssize_t ptp_dev_write(struct file *filp, const char *buf, size_t count,
+	loff_t *offp)
+{
+	struct file_dev_info *info = (struct file_dev_info *)
+		filp->private_data;
+	ssize_t result = 0;
+	size_t size;
+	int rc;
+	u8 cmd;
+
+	if (!count)
+		return result;
+
+	if (down_interruptible(&info->sem))
+		return -ERESTARTSYS;
+
+	if (*offp >= info->write_len) {
+		result = -ENOSPC;
+		goto dev_write_done;
+	}
+	if (*offp + count > info->write_len)
+		count = info->write_len - *offp;
+	if (copy_from_user(info->write_buf, buf, count)) {
+		result = -EFAULT;
+		goto dev_write_done;
+	}
+	cmd = info->write_buf[0] & 0xf0;
+	switch (cmd) {
+	case TSM_CMD_GET_GPS_TS:
+		size = sizeof(struct tsm_get_gps);
+		break;
+	case TSM_CMD_DB_GET_TIME:
+		size = sizeof(struct tsm_get_time);
+		break;
+	case TSM_CMD_DB_GET:
+		size = sizeof(struct tsm_db);
+		break;
+	case TSM_CMD_CNF_SET:
+		size = sizeof(struct tsm_cfg);
+		break;
+	case TSM_CMD_CLOCK_SET:
+		size = sizeof(struct tsm_clock_set);
+		break;
+	case TSM_CMD_CLOCK_CORRECT:
+		size = sizeof(struct tsm_clock_correct);
+		break;
+	default:
+		dbg_msg("tsm: %x"NL, info->write_buf[0]);
+		result = count;
+		goto dev_write_done;
+	}
+	if (count < size) {
+		result = -EFAULT;
+		goto dev_write_done;
+	}
+	result = size;
+	rc = parse_tsm_msg(info, count);
+	if (rc)
+		result = rc;
+
+dev_write_done:
+	up(&info->sem);
+	return result;
+}  /* ptp_dev_write */
+
+static struct ksz_dev_major ptp_majors[MAX_SW_DEVICES];
+
+static struct file_dev_info *alloc_dev_info(struct ptp_info *ptp, uint minor)
+{
+	struct file_dev_info *info;
+
+	info = kzalloc(sizeof(struct file_dev_info), GFP_KERNEL);
+	if (info) {
+		info->dev = ptp;
+		sema_init(&info->sem, 1);
+		mutex_init(&info->lock);
+		init_waitqueue_head(&info->wait_msg);
+		info->read_max = 60000;
+		info->read_tmp = MAX_TSM_UDP_LEN;
+		info->read_buf = kzalloc(info->read_max + info->read_tmp,
+			GFP_KERNEL);
+		info->read_in = &info->read_buf[info->read_max];
+		info->write_len = 1000;
+		info->write_buf = kzalloc(info->write_len, GFP_KERNEL);
+
+		info->minor = minor;
+		info->next = ptp->dev[minor];
+		ptp->dev[minor] = info;
+	}
+	return info;
+}  /* alloc_dev_info */
+
+static void free_dev_info(struct file_dev_info *info)
+{
+	if (info) {
+		int i;
+		struct ptp_info *ptp = info->dev;
+		uint minor = info->minor;
+
+		for (i = 0; i < MAX_TIMESTAMP_UNIT; i++) {
+			if (ptp->tsi_dev[i] == info) {
+				cancel_rx_unit(ptp, i);
+			}
+		}
+		for (i = 0; i < MAX_TRIG_UNIT; i++) {
+			if (ptp->tso_dev[i] == info) {
+				ptp->ops->acquire(ptp);
+				ptp_tso_off(ptp, i, (1 << i));
+				ptp->ops->release(ptp);
+			}
+		}
+		if (ptp->gps_dev == info)
+			ptp->gps_dev = NULL;
+		file_gen_dev_release(info, &ptp->dev[minor]);
+	}
+}  /* free_dev_info */
+
+static int ptp_dev_open(struct inode *inode, struct file *filp)
+{
+	struct file_dev_info *info = (struct file_dev_info *)
+		filp->private_data;
+	uint minor = MINOR(inode->i_rdev);
+	uint major = MAJOR(inode->i_rdev);
+	struct ptp_info *ptp = NULL;
+	int i;
+
+	if (minor > 1)
+		return -ENODEV;
+	for (i = 0; i < MAX_SW_DEVICES; i++) {
+		if (ptp_majors[i].major == major) {
+			ptp = ptp_majors[i].dev;
+			break;
+		}
+	}
+	if (!ptp)
+		return -ENODEV;
+	if (!info) {
+		info = alloc_dev_info(ptp, minor);
+		if (info)
+			filp->private_data = info;
+		else
+			return -ENOMEM;
+	}
+	return 0;
+}  /* ptp_dev_open */
+
+static int ptp_dev_release(struct inode *inode, struct file *filp)
+{
+	struct file_dev_info *info = (struct file_dev_info *)
+		filp->private_data;
+
+	free_dev_info(info);
+	filp->private_data = NULL;
+	return 0;
+}  /* ptp_dev_release */
+
+static const struct file_operations ptp_dev_fops = {
+	.read		= ptp_dev_read,
+	.write		= ptp_dev_write,
+	.unlocked_ioctl	= ptp_dev_ioctl,
+	.open		= ptp_dev_open,
+	.release	= ptp_dev_release,
+};
+
+static struct class *ptp_class[MAX_SW_DEVICES];
+
+static int init_ptp_device(int id, int dev_major, char *dev_name,
+			   char *minor_name)
+{
+	int result;
+
+	result = register_chrdev(dev_major, dev_name, &ptp_dev_fops);
+	if (result < 0) {
+		printk(KERN_WARNING "%s: can't get major %d"NL, dev_name,
+			dev_major);
+		return result;
+	}
+	if (0 == dev_major)
+		dev_major = result;
+	ptp_class[id] = class_create(THIS_MODULE, dev_name);
+	if (IS_ERR(ptp_class[id])) {
+		unregister_chrdev(dev_major, dev_name);
+		return -ENODEV;
+	}
+	device_create(ptp_class[id], NULL, MKDEV(dev_major, 0), NULL,
+		      dev_name);
+	device_create(ptp_class[id], NULL, MKDEV(dev_major, 1), NULL,
+		      minor_name);
+	return dev_major;
+}  /* init_ptp_device */
+
+static void exit_ptp_device(int id, int dev_major, char *dev_name)
+{
+	device_destroy(ptp_class[id], MKDEV(dev_major, 1));
+	device_destroy(ptp_class[id], MKDEV(dev_major, 0));
+	class_destroy(ptp_class[id]);
+	unregister_chrdev(dev_major, dev_name);
+}  /* exit_ptp_device */
+
+static void ptp_set_identity(struct ptp_info *ptp, u8 *addr)
+{
+	memcpy(&ptp->clockIdentity.addr[0], &addr[0], 3);
+	ptp->clockIdentity.addr[3] = 0xFF;
+	ptp->clockIdentity.addr[4] = 0xFE;
+	memcpy(&ptp->clockIdentity.addr[5], &addr[3], 3);
+}  /* ptp_set_identity */
+
+static void ptp_init(struct ptp_info *ptp, u8 *mac_addr)
+{
+	uint i;
+	uint n;
+	int latency[2][3];
+	struct ksz_port_cfg *cfg;
+	struct ksz_sw *sw = ptp->parent;
+
+	ptp->utc_offset = CURRENT_UTC_OFFSET;
+	ptp->get_delay = 100000;
+	ptp->set_delay = 100000;
+	ptp->delay_ticks = 20 * HZ / 1000;
+	ptp->access = create_singlethread_workqueue("ptp_access");
+	init_ptp_work(ptp);
+	mutex_init(&ptp->lock);
+	for (n = 0; n < MAX_PTP_PORT; n++)
+		init_waitqueue_head(&ptp->wait_ts[n]);
+	init_waitqueue_head(&ptp->wait_intr);
+	INIT_WORK(&ptp->adj_clk, adj_clock);
+	INIT_WORK(&ptp->set_latency, set_latency);
+	INIT_WORK(&ptp->set_p2p, ptp_set_p2p);
+	INIT_WORK(&ptp->set_peer_delay, ptp_set_peer_delay);
+	INIT_DELAYED_WORK(&ptp->check_pps, ptp_check_pps);
+	INIT_DELAYED_WORK(&ptp->update_sec, ptp_update_sec);
+	ptp_set_identity(ptp, mac_addr);
+
+#ifndef ACL_TEST
+	ptp->mode = PTP_ENABLE |
+		PTP_IPV4_UDP_ENABLE |
+		PTP_1STEP;
+	ptp->mode |= PTP_IPV6_UDP_ENABLE;
+	ptp->mode |= PTP_ETH_ENABLE;
+#else
+	ptp->mode = PTP_1STEP;
+#endif
+	ptp->cfg = 0;
+	ptp->cfg |= PTP_UNICAST_ENABLE;
+	ptp->cfg |= PTP_UDP_CHECKSUM;
+#if 1
+	ptp->cfg |= PTP_DOMAIN_CHECK;
+
+	/* Pdelay_Req association does not work in 2-step mode anymore. */
+	ptp->cfg |= PTP_DELAY_CHECK;
+	ptp->cfg |= PTP_SYNC_CHECK;
+#endif
+	ptp->def_mode = ptp->mode;
+	ptp->def_cfg = ptp->cfg;
+	ptp->trig_intr = PTP_TRIG_UNIT_M;
+
+	if (!ptp->ports)
+		ptp->ports = MAX_PTP_PORT - 1;
+
+	/* KSZ8463: 574 = 415 + 45 + 114 */
+	/* KSZ9567 S1: 1246; 616 */
+	/* KSZ8463: 70m=908, 100m=1090; 1m=6.07 ns, 483; 582 */
+	/* KSZ9567 S1: 70m=1211, 100m = 1369; 1m=5.27 ns, 842; 866 */
+	/* KSZ9567 S1: 360 ns */
+	/* KSZ9567 S1: 70m=921, 100m = 1072; 1m=5.03 ns, 569; 569 */
+	/* KSZ9567 S1: 72 ns */
+	/* KSZ9567 S3: 70m=1968, 100m = 2124; 1m=5.20 ns, 1604; 1622 */
+	/* KSZ9567 S3: 1080 ns */
+	/* KSZ9567 S3: 70m=1012, 100m = 1164; 1m=5.07 ns, 657; 660 */
+	/* KSZ9567 S3: 152 ns */
+
+	/* Meinberg LANTIME M1000: 500 ns */
+	latency[0][0] = 480;
+	latency[1][0] = 150;
+	/* Meinberg SyncBox: 800 ns */
+	latency[0][1] = 690;
+	latency[1][1] = 830;
+	/* Meinberg LANTIME M1000: 760 ns */
+	latency[0][1] = 720;
+	latency[1][1] = 870;
+	latency[0][2] = latency[0][1] + 1000;
+	latency[1][2] = latency[1][1] + 1000;
+	for (n = 1; n <= ptp->ports; n++) {
+		i = get_phy_port(sw, n);
+		cfg = get_port_cfg(sw, i);
+		cfg->ptp_enabled = true;
+		if ((sw->features & AVB_SUPPORT) &&
+		    !(sw->features & MRP_SUPPORT))
+			cfg->asCapable_set = true;
+		ptp->rx_latency[i][0] = latency[0][0];
+		ptp->tx_latency[i][0] = latency[1][0];
+		ptp->rx_latency[i][1] = latency[0][1];
+		ptp->tx_latency[i][1] = latency[1][1];
+		ptp->rx_latency[i][2] = latency[0][2];
+		ptp->tx_latency[i][2] = latency[1][2];
+	}
+	cfg = get_port_cfg(sw, sw->HOST_PORT);
+	cfg->asCapable = true;
+
+	if (!ptp->get_clk_cnt)
+		ptp->get_clk_cnt = _get_clk_cnt;
+	if (!ptp->test_access_time)
+		ptp->test_access_time = _test_access_time;
+
+	ptp->gps_tsi = MAX_TIMESTAMP_UNIT;
+	ptp->gps_gpi = DEFAULT_GPS_GPI;
+	ptp->pps_gpo = DEFAULT_PPS_GPO;
+	if (pps_gpo > MAX_GPIO)
+		pps_gpo = 0;
+	if (pps_gpo)
+		ptp->pps_gpo = pps_gpo - 1;
+	ptp->pps_tsi = DEFAULT_PPS_TSI;
+	ptp->pps_tso = DEFAULT_PPS_TSO;
+	ptp->mhz_gpo = DEFAULT_MHZ_GPO;
+	ptp->mhz_tso = DEFAULT_MHZ_TSO;
+	if (mhz_gpo > MAX_GPIO)
+		mhz_gpo = 0;
+	if (mhz_gpo)
+		ptp->mhz_gpo = mhz_gpo - 1;
+
+	for (n = 0; n < MAX_TIMESTAMP_UNIT; n++)
+		ptp->events[n].max = MAX_TIMESTAMP_EVENT_UNIT;
+
+	init_msg_info(ptp->rx_msg_info, &ptp->rx_msg_lock);
+	init_msg_info(ptp->tx_msg_info, &ptp->tx_msg_lock);
+
+	sprintf(ptp->dev_name[0], "ptp_dev");
+	sprintf(ptp->dev_name[1], "ptp_event");
+	if (sw->id) {
+		sprintf(ptp->dev_name[0], "ptp_dev_%u", sw->id);
+		sprintf(ptp->dev_name[1], "ptp_event_%u", sw->id);
+	}
+	ptp->dev_major = init_ptp_device(sw->id, 0, ptp->dev_name[0],
+		ptp->dev_name[1]);
+	ptp_majors[sw->id].dev = ptp;
+	ptp_majors[sw->id].major = ptp->dev_major;
+
+#ifdef CONFIG_PTP_1588_CLOCK_
+/* THa  05/30/2018
+ * I2C driver is invoked before PPS and PTP, so the clock will not be created.
+ */
+	micrel_ptp_probe(ptp);
+#endif
+}  /* ptp_init */
+
+static void ptp_exit(struct ptp_info *ptp)
+{
+	struct ksz_sw *sw = ptp->parent;
+
+	exit_ptp_work(ptp);
+	flush_work(&ptp->adj_clk);
+	flush_work(&ptp->set_latency);
+	flush_work(&ptp->set_p2p);
+	flush_work(&ptp->set_peer_delay);
+	cancel_delayed_work_sync(&ptp->check_pps);
+	cancel_delayed_work_sync(&ptp->update_sec);
+	if (ptp->access) {
+		destroy_workqueue(ptp->access);
+		ptp->access = NULL;
+	}
+	if (ptp->dev_major >= 0)
+		exit_ptp_device(sw->id, ptp->dev_major, ptp->dev_name[0]);
+
+#ifdef CONFIG_PTP_1588_CLOCK
+	if (ptp->clock_info)
+		micrel_ptp_remove(ptp);
+#endif
+}  /* ptp_exit */
+
+enum {
+	PROC_SET_PTP_FEATURES,
+	PROC_SET_PTP_OVERRIDES,
+	PROC_SET_PTP_VID,
+	PROC_SET_PTP_GPIO_1,
+	PROC_SET_PTP_GPIO_2,
+};
+
+static ssize_t sysfs_ptp_read(struct ptp_info *ptp, int proc_num, ssize_t len,
+	char *buf)
+{
+	switch (proc_num) {
+	case PROC_SET_PTP_FEATURES:
+		len += sprintf(buf + len, "%08x:"NL, ptp->features);
+		len += sprintf(buf + len, "\t%08x = adjust hack"NL,
+			PTP_ADJ_HACK);
+		len += sprintf(buf + len, "\t%08x = adjust sec"NL,
+			PTP_ADJ_SEC);
+		break;
+	case PROC_SET_PTP_OVERRIDES:
+		len += sprintf(buf + len, "%08x:"NL, ptp->overrides);
+		len += sprintf(buf + len, "\t%08x = PTP port forwarding"NL,
+			(uint)PTP_PORT_FORWARD);
+		len += sprintf(buf + len, "\t%08x = PTP port TX forwarding"NL,
+			(uint)PTP_PORT_TX_FORWARD);
+		len += sprintf(buf + len, "\t%08x = PTP check path delay"NL,
+			(uint)PTP_CHECK_PATH_DELAY);
+		len += sprintf(buf + len, "\t%08x = PTP verify timestamp"NL,
+			(uint)PTP_VERIFY_TIMESTAMP);
+		len += sprintf(buf + len, "\t%08x = PTP zero reserved field"NL,
+			(uint)PTP_ZERO_RESERVED_FIELD);
+		len += sprintf(buf + len, "\t%08x = PTP update pdelay_resp"NL,
+			(uint)PTP_UPDATE_PDELAY_RESP_TIME);
+		len += sprintf(buf + len, "\t%08x = PTP check system time"NL,
+			(uint)PTP_CHECK_SYS_TIME);
+		len += sprintf(buf + len, "\t%08x = PTP check sync time"NL,
+			(uint)PTP_CHECK_SYNC_TIME);
+		break;
+	case PROC_SET_PTP_VID:
+		len += sprintf(buf + len, "0x%04x"NL, ptp->vid);
+		break;
+	case PROC_SET_PTP_GPIO_1:
+	case PROC_SET_PTP_GPIO_2:
+	{
+		u32 reg;
+		int gpo = (proc_num == PROC_SET_PTP_GPIO_1) ? 0 : 1;
+		struct ksz_sw *sw = ptp->parent;
+
+		ptp->ops->acquire(ptp);
+		ptp_write_index(ptp, PTP_GPIO_INDEX_S, gpo);
+		reg = sw->reg->r32(sw, REG_PTP_CTRL_STAT__4);
+		ptp->ops->release(ptp);
+		len += sprintf(buf + len, "%d"NL, !(reg & GPIO_MODE_IN));
+		break;
+	}
+	}
+	return len;
+}
+
+static void sysfs_ptp_write(struct ptp_info *ptp, int proc_num, int num,
+	const char *buf)
+{
+	int changes;
+
+	switch (proc_num) {
+	case PROC_SET_PTP_FEATURES:
+		if ('0' != buf[0] || 'x' != buf[1])
+			sscanf(buf, "%x", &num);
+		changes = ptp->features ^ num;
+		ptp->features = num;
+		break;
+	case PROC_SET_PTP_OVERRIDES:
+		if ('0' != buf[0] || 'x' != buf[1])
+			sscanf(buf, "%x", &num);
+		changes = ptp->overrides ^ num;
+		if ((changes & PTP_CHECK_SYS_TIME) &&
+				(ptp->overrides & PTP_CHECK_SYS_TIME))
+			ptp->first_sec = 0;
+#ifdef DBG_PROC_SYNC
+		if ((changes & PTP_CHECK_SYNC_TIME) &&
+				(ptp->overrides & PTP_CHECK_SYNC_TIME)) {
+			last_rcv.sec = 0;
+			first_recv = 0;
+			first_sync = 0;
+		}
+#endif
+		ptp->overrides = num;
+		break;
+	case PROC_SET_PTP_VID:
+		ptp->vid = num;
+		break;
+	case PROC_SET_PTP_GPIO_1:
+	case PROC_SET_PTP_GPIO_2:
+	{
+		u32 reg;
+		int gpo = (proc_num == PROC_SET_PTP_GPIO_1) ? 0 : 1;
+		struct ksz_sw *sw = ptp->parent;
+
+		ptp_write_index(ptp, PTP_GPIO_INDEX_S, gpo);
+		reg = sw->reg->r32(sw, REG_PTP_CTRL_STAT__4);
+		if (num)
+			reg &= ~GPIO_MODE_IN;
+		else
+			reg |= GPIO_MODE_IN;
+		sw->reg->w32(sw, REG_PTP_CTRL_STAT__4, reg);
+		break;
+	}
+	}
+}
+
+static struct ptp_reg_ops ptp_reg_ops = {
+	.get_time		= get_ptp_time,
+	.set_time		= set_ptp_time,
+	.adjust_time		= adjust_ptp_time,
+	.adjust_sync_time	= adjust_sync_time,
+
+	.rx_off			= ptp_rx_off,
+	.rx_reset		= ptp_rx_reset,
+	.rx_restart		= ptp_rx_restart,
+	.rx_event		= ptp_rx_event,
+	.rx_cascade_event	= ptp_rx_cascade_event,
+	.read_event		= ptp_read_event,
+
+	.tx_off			= ptp_tx_off,
+	.tx_restart		= ptp_tx_restart,
+	.tx_event		= ptp_tx_event,
+	.pps_event		= ptp_pps_event,
+	.ptp_10MHz		= ptp_10MHz,
+	.tx_cascade		= ptp_tx_cascade,
+
+	.start			= ptp_start,
+};
+
+#ifdef CONFIG_KSZ_IBA
+#include "ksz_ptp_iba.c"
+#endif
+
+static void ptp_use_iba(struct ptp_info *ptp, bool iba)
+{
+#ifdef CONFIG_KSZ_IBA
+	static int last_delay_ticks = 20 * HZ / 1000;
+
+	if (iba) {
+		ptp->reg = &ptp_iba_ops;
+		last_delay_ticks = ptp->delay_ticks;
+		ptp->delay_ticks = 10 * HZ / 1000;
+	} else {
+		ptp->reg = &ptp_reg_ops;
+		ptp->delay_ticks = last_delay_ticks;
+	}
+#endif
+}
+
+static struct ptp_ops ptp_ops = {
+	.acquire		= ptp_acquire,
+	.release		= ptp_release,
+	.use_iba		= ptp_use_iba,
+
+	.init			= ptp_init,
+	.exit			= ptp_exit,
+
+	.stop			= ptp_stop,
+	.set_identity		= ptp_set_identity,
+
+	.check_msg		= check_ptp_msg,
+	.get_rx_tstamp		= get_rx_tstamp,
+	.get_tx_tstamp		= get_tx_tstamp,
+	.hwtstamp_ioctl		= ptp_hwtstamp_ioctl,
+	.ixxat_ioctl		= ixxat_ptp_ioctl,
+	.dev_req		= ptp_dev_req,
+	.proc_intr		= proc_ptp_intr,
+	.proc_tx_intr		= proc_ptp_tx_intr,
+	.get_ts_info		= ptp_get_ts_info,
+
+	.sysfs_read		= sysfs_ptp_read,
+	.sysfs_write		= sysfs_ptp_write,
+
+	.drop_pkt		= ptp_drop_pkt,
+	.get_rx_info		= ptp_get_rx_info,
+	.set_tx_info		= ptp_set_tx_info,
+};
+
diff -Napur linux-linux4microchip-2022.10-orig/drivers/net/ethernet/micrel/ksz_ptp_9897.h linux-linux4microchip-2022.10/drivers/net/ethernet/micrel/ksz_ptp_9897.h
--- linux-linux4microchip-2022.10-orig/drivers/net/ethernet/micrel/ksz_ptp_9897.h	1969-12-31 16:00:00.000000000 -0800
+++ linux-linux4microchip-2022.10/drivers/net/ethernet/micrel/ksz_ptp_9897.h	2023-11-16 18:55:14.000000000 -0800
@@ -0,0 +1,1187 @@
+/**
+ * Microchip PTP common header
+ *
+ * Copyright (c) 2015-2023 Microchip Technology Inc.
+ *	Tristram Ha <Tristram.Ha@microchip.com>
+ *
+ * Copyright (c) 2010-2015 Micrel, Inc.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ */
+
+#ifndef KSZ_PTP_H
+#define KSZ_PTP_H
+
+#ifndef __KERNEL__
+typedef unsigned char u8;
+typedef unsigned short u16;
+typedef unsigned int u32;
+typedef short s16;
+typedef long long s64;
+typedef unsigned long long u64;
+#endif
+
+struct ksz_ptp_time {
+	int sec;
+	int nsec;
+};
+
+struct ptp_utime {
+	u32 sec;
+	u32 nsec;
+};
+
+struct ptp_ts {
+	struct ptp_utime r;
+	struct ptp_utime t;
+	u32 timestamp;
+};
+
+struct ptp_second {
+	u16 hi;
+	u32 lo;
+} __packed;
+
+struct ptp_timestamp {
+	struct ptp_second sec;
+	u32 nsec;
+} __packed;
+
+#define SCALED_NANOSEC_S		16
+#define SCALED_NANOSEC_MULT		(1 << SCALED_NANOSEC_S)
+
+struct ptp_scaled_ns {
+	int hi;
+	s64 lo;
+} __packed;
+
+struct ptp_correction {
+	int scaled_nsec_hi;
+	int scaled_nsec_lo;
+} __packed;
+
+struct ptp_clock_identity {
+	u8 addr[8];
+};
+
+struct ptp_port_identity {
+	struct ptp_clock_identity clockIdentity;
+	u16 port;
+} __packed;
+
+struct ptp_clock_quality {
+	u8 clockClass;
+	u8 clockAccuracy;
+	u16 offsetScaledLogVariance;
+} __packed;
+
+struct ptp_port_address {
+	u16 networkProtocol;
+	u16 addressLength;
+	u8 addressField[1];
+} __packed;
+
+struct ptp_text {
+	u8 lengthField;
+	u8 textField[1];
+} __packed;
+
+#define SYNC_MSG			0x0
+#define DELAY_REQ_MSG			0x1
+#define PDELAY_REQ_MSG			0x2
+#define PDELAY_RESP_MSG			0x3
+#define FOLLOW_UP_MSG			0x8
+#define DELAY_RESP_MSG			0x9
+#define PDELAY_RESP_FOLLOW_UP_MSG	0xA
+#define ANNOUNCE_MSG			0xB
+#define SIGNALING_MSG			0xC
+#define MANAGEMENT_MSG			0xD
+
+struct ptp_msg_hdr {
+#ifdef __BIG_ENDIAN_BITFIELD
+	u8 transportSpecific:4;
+	u8 messageType:4;
+	u8 reserved1:4;
+	u8 versionPTP:4;
+#else
+	u8 messageType:4;
+	u8 transportSpecific:4;
+	u8 versionPTP:4;
+	u8 reserved1:4;
+#endif
+	u16 messageLength;
+	u8 domainNumber;
+	u8 reserved2;
+	union {
+		struct {
+#ifdef __BIG_ENDIAN_BITFIELD
+			u8 reservedFlag7:1;
+			u8 profileSpecific1:1;
+			u8 profileSpecific2:1;
+			u8 reservedFlag4:1;
+			u8 reservedFlag3:1;
+			u8 unicastFlag:1;
+			u8 twoStepFlag:1;
+			u8 alternateMasterFlag:1;
+			u8 reservedFlag6:1;
+			u8 reservedFlag5:1;
+			u8 frequencyTraceable:1;
+			u8 timeTraceable:1;
+			u8 ptpTimescale:1;
+			u8 utcOffsetValid:1;
+			u8 leap59:1;
+			u8 leap61:1;
+#else
+			u8 alternateMasterFlag:1;
+			u8 twoStepFlag:1;
+			u8 unicastFlag:1;
+			u8 reservedFlag3:1;
+			u8 reservedFlag4:1;
+			u8 profileSpecific1:1;
+			u8 profileSpecific2:1;
+			u8 reservedFlag7:1;
+			u8 leap61:1;
+			u8 leap59:1;
+			u8 utcOffsetValid:1;
+			u8 ptpTimescale:1;
+			u8 timeTraceable:1;
+			u8 frequencyTraceable:1;
+			u8 reservedFlag5:1;
+			u8 reservedFlag6:1;
+#endif
+		} __packed flag;
+		u16 data;
+	} __packed flagField;
+	struct ptp_correction correctionField;
+	u32 reserved3;
+	struct ptp_port_identity sourcePortIdentity;
+	u16 sequenceId;
+	u8 controlField;
+	char logMessageInterval;
+} __packed;
+
+struct ptp_msg_sync {
+	struct ptp_timestamp originTimestamp;
+} __packed;
+
+struct ptp_msg_follow_up {
+	struct ptp_timestamp preciseOriginTimestamp;
+} __packed;
+
+struct ptp_msg_delay_resp {
+	struct ptp_timestamp receiveTimestamp;
+	struct ptp_port_identity requestingPortIdentity;
+} __packed;
+
+struct ptp_msg_pdelay_req {
+	struct ptp_timestamp originTimestamp;
+	struct ptp_port_identity reserved;
+} __packed;
+
+struct ptp_msg_pdelay_resp {
+	struct ptp_timestamp requestReceiptTimestamp;
+	struct ptp_port_identity requestingPortIdentity;
+} __packed;
+
+struct ptp_msg_pdelay_resp_follow_up {
+	struct ptp_timestamp responseOriginTimestamp;
+	struct ptp_port_identity requestingPortIdentity;
+} __packed;
+
+#define TLV_MANAGEMENT					0x0001
+#define TLV_MANAGEMENT_ERROR_STATUS			0x0002
+#define TLV_ORGANIZATION_EXTENSION			0x0003
+#define TLV_REQUEST_UNICAST_TRANSMISSION		0x0004
+#define TLV_GRANT_UNICAST_TRANSMISSION			0x0005
+#define TLV_CANCEL_UNICAST_TRANSMISSION			0x0006
+#define TLV_ACKNOWLEDGE_CANCEL_UNICAST_TRANSMISSION	0x0007
+#define TLV_PATH_TRACE					0x0008
+#define TLV_ALTERNATE_TIME_OFFSET_INDICATOR		0x0009
+
+struct ptp_tlv {
+	u16 tlvType;
+	u16 lengthField;
+} __packed;
+
+struct ptp_organization_ext_tlv {
+	struct ptp_tlv tlv;
+	u8 organizationId[3];
+	u8 organizationSubType[3];
+	u8 dataField[1];
+} __packed;
+
+struct IEEE_C37_238_data {
+	u16 grandmasterID;
+	u32 grandmasterTimeInaccuracy;
+	u32 networkTimeInaccuracy;
+	u16 reserved;
+} __packed;
+
+struct IEEE_802_1AS_data_1 {
+	int cumulativeScaledRateOffset;
+	u16 gmTimeBaseIndicator;
+	struct ptp_scaled_ns lastGmPhaseChange;
+	int scaledLastGmFreqChange;
+} __packed;
+
+struct IEEE_802_1AS_data_2 {
+	char linkDelayInterval;
+	char timeSyncInterval;
+	char announceInterval;
+	u8 flags;
+	u16 reserved;
+} __packed;
+
+struct ptp_request_unicast_tlv {
+	struct ptp_tlv tlv;
+#ifdef __BIG_ENDIAN_BITFIELD
+	u8 messageType:4;
+	u8 reserved1:4;
+#else
+	u8 reserved1:4;
+	u8 messageType:4;
+#endif
+	char logInterMessagePeriod;
+	u32 durationField;
+} __packed;
+
+struct ptp_grant_unicast_tlv {
+	struct ptp_tlv tlv;
+#ifdef __BIG_ENDIAN_BITFIELD
+	u8 messageType:4;
+	u8 reserved1:4;
+#else
+	u8 reserved1:4;
+	u8 messageType:4;
+#endif
+	char logInterMessagePeriod;
+	u32 durationField;
+	u8 reserved2;
+#ifdef __BIG_ENDIAN_BITFIELD
+	u8 reserved3:7;
+	u8 renewal:1;
+#else
+	u8 renewal:1;
+	u8 reserved3:7;
+#endif
+} __packed;
+
+struct ptp_cancel_unicast_tlv {
+	struct ptp_tlv tlv;
+#ifdef __BIG_ENDIAN_BITFIELD
+	u8 messageType:4;
+	u8 reserved1:4;
+#else
+	u8 reserved1:4;
+	u8 messageType:4;
+#endif
+	u8 reserved2;
+} __packed;
+
+struct ptp_alternate_time_offset_tlv {
+	struct ptp_tlv tlv;
+	u8 keyField;
+	int currentOffset;
+	int jumpSeconds;
+	struct ptp_second timeOfNextJump;
+	struct ptp_text displayName;
+} __packed;
+
+struct ptp_msg_signaling_base {
+	struct ptp_port_identity targetPortIdentity;
+} __packed;
+
+struct ptp_msg_signaling {
+	struct ptp_msg_signaling_base b;
+	union {
+		struct ptp_request_unicast_tlv request[1];
+		struct ptp_grant_unicast_tlv grant[1];
+		struct ptp_cancel_unicast_tlv cancel[1];
+	} tlv;
+} __packed;
+
+#define M_NULL_MANAGEMENT				0x0000
+#define M_CLOCK_DESCRIPTION				0x0001
+#define M_DEFAULT_DATA_SET				0x2000
+#define M_CURRENT_DATA_SET				0x2001
+#define M_PARENT_DATA_SET				0x2002
+#define M_PORT_DATA_SET					0x2004
+#define M_PRIORITY1					0x2005
+#define M_PRIORITY2					0x2006
+#define M_DOMAIN					0x2007
+#define M_SLAVE_ONLY					0x2008
+#define M_VERSION_NUMBER				0x200C
+#define M_ENABLE_PORT					0x200D
+#define M_DISABLE_PORT					0x200E
+#define M_TIME						0x200F
+#define M_UNICAST_NEGOTIATION_ENABLE			0x2014
+#define M_PATH_TRACE_LIST				0x2015
+#define M_PATH_TRACE_ENABLE				0x2016
+#define M_GRANDMASTER_CLUSTER_TABLE			0x2017
+#define M_UNICAST_MASTER_TABLE				0x2018
+#define M_UNICAST_MASTER_MAX_TABLE_SIZE			0x2019
+#define M_ACCEPTABLE_MASTER_TABLE			0x201A
+#define M_ACCEPTABLE_MASTER_TABLE_ENABLED		0x201B
+#define M_ACCEPTABLE_MASTER_MAX_TABLE_SIZE		0x201C
+#define M_ALTERNATE_MASTER				0x201D
+#define M_ALTERNATE_TIME_OFFSET_ENABLE			0x201E
+#define M_ALTERNATE_TIME_OFFSET_NAME			0x201F
+#define M_ALTERNATE_TIME_OFFSET_MAX_KEY			0x2020
+#define M_ALTERNATE_TIME_OFFSET_PROPERTIES		0x2021
+
+struct ptp_management_unicast_negotiation {
+#ifdef __BIG_ENDIAN_BITFIELD
+	u8 reserved1:7;
+	u8 enable:1;
+#else
+	u8 enable:1;
+	u8 reserved1:7;
+#endif
+	u8 reserved2;
+} __packed;
+
+struct ptp_management_unicast_master_table {
+	u8 logQueryInterval;
+	u16 tableSize;
+	struct ptp_port_address unicastMasterTable[1];
+} __packed;
+
+struct ptp_management_unicast_master_max_table_size {
+	u16 maxTableSize;
+} __packed;
+
+struct ptp_management_alternate_time_offset {
+	u8 keyField;
+#ifdef __BIG_ENDIAN_BITFIELD
+	u8 reserved1:7;
+	u8 enable:1;
+#else
+	u8 enable:1;
+	u8 reserved1:7;
+#endif
+} __packed;
+
+struct ptp_management_alternate_time_offset_name {
+	u8 keyField;
+	struct ptp_text displayName;
+} __packed;
+
+struct ptp_management_alternate_time_offset_max_key {
+	u8 keyField;
+	u8 reserved;
+} __packed;
+
+struct ptp_management_alternate_time_offset_properties {
+	u8 keyField;
+	int currentOffset;
+	int jumpSeconds;
+	struct ptp_second timeOfNextJump;
+	u8 reserved;
+} __packed;
+
+struct ptp_management_tlv {
+	struct ptp_tlv tlv;
+	u16 managementId;
+	u8 dataField[1];
+} __packed;
+
+#define M_RESPONSE_TOO_BIG		0x0001
+#define M_NO_SUCH_ID			0x0002
+#define M_WRONG_LENGTH			0x0003
+#define M_WRONG_VALUE			0x0004
+#define M_NOT_SETABLE			0x0005
+#define M_NOT_SUPPORTED			0x0006
+#define M_GENERAL_ERROR			0xFFFE
+
+struct ptp_management_error_tlv {
+	struct ptp_tlv tlv;
+	u16 managementErrorId;
+	u16 managementId;
+	u32 reserved1;
+	u8 data[1];
+} __packed;
+
+#define MANAGEMENT_GET			0
+#define MANAGEMENT_SET			1
+#define MANAGEMENT_RESPONSE		2
+#define MANAGEMENT_COMMAND		3
+#define MANAGEMENT_ACKNOWLEDGE		4
+
+struct ptp_msg_management_base {
+	struct ptp_port_identity targetPortIdentity;
+	u8 startingBoundaryHops;
+	u8 boundaryHops;
+#ifdef __BIG_ENDIAN_BITFIELD
+	u8 reserved1:4;
+	u8 actionField:4;
+#else
+	u8 actionField:4;
+	u8 reserved1:4;
+#endif
+	u8 reserved2;
+} __packed;
+
+struct ptp_msg_management {
+	struct ptp_msg_management_base b;
+	union {
+		struct ptp_management_tlv normal[1];
+		struct ptp_management_error_tlv error[1];
+	} tlv;
+} __packed;
+
+struct ptp_msg_announce {
+	struct ptp_timestamp originTimestamp;
+	s16 currentUtcOffset;
+	u8 reserved;
+	u8 grandmasterPriority1;
+	struct ptp_clock_quality grandmasterClockQuality;
+	u8 grandmasterPriority2;
+	struct ptp_clock_identity grandmasterIdentity;
+	u16 stepsRemoved;
+	u8 timeSource;
+} __packed;
+
+union ptp_msg_data {
+	struct ptp_msg_sync sync;
+	struct ptp_msg_follow_up follow_up;
+	struct ptp_msg_delay_resp delay_resp;
+	struct ptp_msg_pdelay_req pdelay_req;
+	struct ptp_msg_pdelay_resp pdelay_resp;
+	struct ptp_msg_pdelay_resp_follow_up pdelay_resp_follow_up;
+	struct ptp_msg_signaling signaling;
+	struct ptp_msg_management management;
+	struct ptp_msg_announce announce;
+	u8 data[8];
+} __packed;
+
+struct ptp_msg {
+	struct ptp_msg_hdr hdr;
+	union ptp_msg_data data;
+} __packed;
+
+
+struct ptp_id {
+	u16 seq;
+	struct ptp_clock_identity clock;
+	u8 mac[2];
+	u8 msg;
+	u8 port;
+};
+
+struct ptp_cfg_options {
+	u8 master:1;
+	u8 two_step:1;
+	u8 p2p:1;
+	u8 as:1;
+	u8 domain_check:1;
+	u8 udp_csum:1;
+	u8 unicast:1;
+	u8 alternate:1;
+	u8 delay_assoc:1;
+	u8 pdelay_assoc:1;
+	u8 sync_assoc:1;
+	u8 drop_sync:1;
+	u8 priority:1;
+	u8 reserved:3;
+	u8 master_set:1;
+	u8 two_step_set:1;
+	u8 p2p_set:1;
+	u8 as_set:1;
+	u8 domain_check_set:1;
+	u8 udp_csum_set:1;
+	u8 unicast_set:1;
+	u8 alternate_set:1;
+	u8 delay_assoc_set:1;
+	u8 pdelay_assoc_set:1;
+	u8 sync_assoc_set:1;
+	u8 drop_sync_set:1;
+	u8 priority_set:1;
+	u8 reserved_set:2;
+	u8 domain_set:1;
+	u8 domain;
+	u8 reserved3;
+	u32 access_delay;
+} __packed;
+
+#define PTP_CMD_RESP			0x01
+#define PTP_CMD_GET_MSG			0x00
+#define PTP_CMD_GET_OUTPUT		0xE0
+#define PTP_CMD_GET_EVENT		0xF0
+
+#define PTP_CMD_INTR_OPER		0x01
+#define PTP_CMD_SILENT_OPER		0x02
+#define PTP_CMD_ON_TIME			0x04
+#define PTP_CMD_REL_TIME		0x08
+#define PTP_CMD_CLK_OPT			0x10
+#define PTP_CMD_SW_OPER			0x20
+#define PTP_CMD_CASCADE_RESET_OPER	0x40
+#define PTP_CMD_CANCEL_OPER		0x80
+
+struct ptp_tsi_info {
+	u8 cmd;
+	u8 unit;
+	u8 event;
+	u8 num;
+	u32 edge;
+	struct ptp_utime t[0];
+} __packed;
+
+struct ptp_tsi_options {
+	u8 tsi;
+	u8 gpi;
+	u8 event;
+	u8 flags;
+	u8 total;
+	u8 reserved[3];
+	u32 timeout;
+} __packed;
+
+struct ptp_tso_options {
+	u8 tso;
+	u8 gpo;
+	u8 event;
+	u8 flags;
+	u8 total;
+	u8 reserved[1];
+	u16 cnt;
+	u32 pulse;
+	u32 cycle;
+	u32 sec;
+	u32 nsec;
+	u32 iterate;
+} __packed;
+
+struct ptp_clk_options {
+	u32 sec;
+	u32 nsec;
+	int drift;
+	u32 interval;
+} __packed;
+
+struct ptp_ts_options {
+	u32 timestamp;
+	u32 sec;
+	u32 nsec;
+	u8 msg;
+	u8 port;
+	u16 seqid;
+	u8 mac[2];
+} __packed;
+
+struct ptp_delay_values {
+	u16 rx_latency;
+	u16 tx_latency;
+	short asym_delay;
+	u16 reserved;
+} __packed;
+
+struct ptp_msg_options {
+	struct ptp_port_identity id;
+	u16 seqid;
+	u8 domain;
+	u8 msg;
+	u8 reserved[2];
+	u32 port;
+	struct ptp_ts ts;
+} __packed;
+
+struct ptp_udp_msg {
+	u16 len;
+	u8 data[0];
+} __packed;
+
+#ifdef __KERNEL__
+#define NANOSEC_IN_SEC			1000000000
+
+/* Host port can be any one of the ports. */
+#define MAX_PTP_PORT			SWITCH_PORT_NUM
+
+#define MAX_TSM_UDP_LEN			100
+#define MAX_TSM_UDP_CNT			(1 << 6)
+
+struct ptp_ltime {
+	s64 sec;
+	s64 nsec;
+};
+
+struct ptp_hw_ts {
+	struct ptp_id id;
+	struct ptp_ts ts;
+	int update;
+	int sending;
+};
+
+struct ptp_tx_ts {
+	struct ptp_id id;
+	struct ptp_ts ts;
+	int missed;
+	unsigned long req_time;
+	unsigned long resp_time;
+	struct {
+		u8 buf[MAX_TSM_UDP_LEN];
+		int len;
+	} data;
+	struct file_dev_info *dev;
+	struct sk_buff *skb;
+	struct ptp_msg *msg;
+	struct ptp_msg_hdr hdr;
+};
+
+struct ptp_event {
+	int max;
+	int num;
+	int event;
+	int first;
+	int last;
+	u32 edge;
+	struct ptp_utime t[MAX_TIMESTAMP_EVENT_UNIT];
+	u32 timeout;
+	unsigned long expired;
+};
+
+struct ptp_output {
+	struct ptp_utime trig;
+	struct ptp_utime intr;
+	struct ptp_utime start;
+	struct ptp_utime stop;
+	struct ksz_ptp_time gap;
+	u64 iterate;
+	u64 len;
+	u32 cycle;
+	u32 hw_ctrl;
+	u32 hw_pulse;
+	u32 hw_cycle;
+	u32 hw_pattern;
+	u16 cnt;
+	u8 event;
+	u8 gpo;
+	u8 level;
+};
+
+#define CLOCK_ENTRIES		2
+
+struct ptp_irig_info {
+	u32 pulse[100];
+	int index;
+	u8 tso[8];
+	int cur_tso;
+	int max_tso;
+	struct ptp_utime t;
+};
+
+struct ptp_msg_info {
+	struct ptp_msg_options data;
+	u32 sec;
+	struct ptp_msg_info *next;
+};
+
+#define MMEDIAN_LEN  10
+
+struct mmedian_data {
+	s64 delays;
+	int order;
+};
+
+struct mmedian {
+	struct mmedian_data data[MMEDIAN_LEN];
+	int index;
+	int cnt;
+	int len;
+};
+
+struct ptp_filter {
+	struct mmedian median;
+	u8 delay_valid;
+	s64 delay;
+};
+
+struct ptp_peer_delay_ts {
+	struct ptp_filter filter;
+	s64 corr;
+	s64 t1;
+	s64 t2;
+	s64 t3;
+	s64 t4;
+	u16 fup_seqid;
+	u16 req_seqid;
+	u16 resp_seqid;
+};
+
+struct ptp_info;
+
+struct ptp_work {
+	struct work_struct work;
+	struct completion done;
+	struct ptp_info *ptp;
+	int cmd;
+	int subcmd;
+	int option;
+	int output;
+	int result;
+	int used;
+	int wait;
+	union {
+		struct ptp_cfg_options cfg;
+		struct ptp_tsi_info tsi;
+		struct ptp_tsi_options tsi_opt;
+		struct ptp_tso_options tso_opt;
+		struct ptp_clk_options clk_opt;
+		struct ptp_ts_options ts_opt;
+		struct ptp_delay_values delay;
+		u8 data[8];
+	} param;
+	struct file_dev_info *dev_info;
+};
+
+#define PTP_WORK_NUM			(1 << 4)
+#define PTP_WORK_LAST			(PTP_WORK_NUM - 1)
+
+struct ptp_access {
+	struct mutex lock;
+	int index;
+	struct ptp_work works[PTP_WORK_NUM];
+};
+
+struct ptp_reg_ops {
+	void (*lock)(struct ptp_info *ptp);
+	void (*unlock)(struct ptp_info *ptp);
+
+	void (*get_time)(struct ptp_info *ptp, struct ptp_utime *t);
+	void (*set_time)(struct ptp_info *ptp, struct ptp_utime *t);
+	void (*adjust_time)(struct ptp_info *ptp, int add, u32 sec, u32 nsec,
+		int adj_hack);
+	void (*adjust_sync_time)(struct ptp_info *ptp, int diff, u32 interval,
+		u32 duration);
+
+	void (*rx_off)(struct ptp_info *ptp, u8 tsi);
+	void (*rx_reset)(struct ptp_info *ptp, u8 tsi, u32 *ctrl_ptr);
+	void (*rx_restart)(struct ptp_info *ptp, u8 tsi);
+	void (*rx_event)(struct ptp_info *ptp, u8 tsi, u8 gpi, u8 event,
+		int intr);
+	void (*rx_cascade_event)(struct ptp_info *ptp, u8 first, u8 total,
+		u8 gpi, u8 event, int intr);
+	void (*read_event)(struct ptp_info *ptp, u8 tsi);
+
+	void (*tx_off)(struct ptp_info *ptp, u8 tso);
+	void (*tx_restart)(struct ptp_info *ptp, u8 tso, u32 ctrl, u32 sec,
+		u32 nsec);
+	void (*tx_event)(struct ptp_info *ptp, u8 tso, u32 ctrl, u32 pulse,
+		u32 cycle, u32 pattern, u32 sec, u32 nsec);
+	void (*pps_event)(struct ptp_info *ptp, u8 gpo, u32 sec);
+	void (*ptp_10MHz)(struct ptp_info *ptp, u8 tso, u8 gpo, u32 sec);
+	int (*tx_cascade)(struct ptp_info *ptp, u8 first, u8 total,
+		u16 repeat, u32 sec, u32 nsec, int intr);
+
+	void (*start)(struct ptp_info *ptp, int init);
+};
+
+struct ptp_ops {
+	void (*acquire)(struct ptp_info *ptp);
+	void (*release)(struct ptp_info *ptp);
+	void (*use_iba)(struct ptp_info *ptp, bool iba);
+
+	void (*init)(struct ptp_info *ptp, u8 *mac_addr);
+	void (*exit)(struct ptp_info *ptp);
+	int (*stop)(struct ptp_info *ptp, int hw_access);
+	void (*set_identity)(struct ptp_info *ptp, u8 *addr);
+	struct ptp_msg *(*check_msg)(u8 *data, u16 **udp_check_ptr);
+	int (*update_msg)(u8 *data, u32 port, u32 overrides);
+	void (*get_rx_tstamp)(void *ptr, struct sk_buff *skb);
+	void (*get_tx_tstamp)(struct ptp_info *ptp, struct sk_buff *skb);
+	int (*hwtstamp_ioctl)(struct ptp_info *ptp, struct ifreq *ifr,
+			      u16 ports);
+	int (*ixxat_ioctl)(struct ptp_info *ptp, unsigned int cmd,
+		struct ifreq *ifr);
+	int (*dev_req)(struct ptp_info *ptp, char *arg,
+		struct file_dev_info *info);
+	void (*proc_intr)(struct ptp_info *ptp);
+	void (*proc_tx_intr)(struct ptp_info *ptp, uint port);
+	int (*get_ts_info)(struct ptp_info *ptp, struct net_device *dev,
+		struct ethtool_ts_info *info);
+
+	ssize_t (*sysfs_read)(struct ptp_info *ptp, int proc_num, ssize_t len,
+		char *buf);
+	void (*sysfs_write)(struct ptp_info *ptp, int proc_num, int num,
+		const char *buf);
+
+	int (*drop_pkt)(struct ptp_info *ptp, struct sk_buff *skb, u32 vlan_id,
+		int *tag, int *ptp_tag, int *forward);
+
+	void (*get_rx_info)(struct ptp_info *ptp, u8 *data, u8 port,
+		u32 timestamp);
+	void (*set_tx_info)(struct ptp_info *ptp, u8 *data, void *tag);
+};
+
+#define DEFAULT_GPS_GPI			1
+#define DEFAULT_GPS_TSI			1
+
+#define DEFAULT_PPS_TSI			1
+
+#if 1
+#define DEFAULT_MHZ_GPO			1
+#define DEFAULT_PPS_GPO			0
+#else
+#define DEFAULT_MHZ_GPO			0
+#define DEFAULT_PPS_GPO			1
+#endif
+
+/* TSO 1 is reserved if 10 MHz clock is used. */
+#define DEFAULT_MHZ_TSO			0
+#define DEFAULT_PPS_TSO			2
+
+/* Switch features and bug fixes. */
+#define PTP_ADJ_HACK			(1 << 0)
+#define PTP_ADJ_SEC			(1 << 1)
+#define PTP_PDELAY_HACK			(1 << 2)
+
+/* Software overrides. */
+
+#define PTP_PORT_FORWARD		(1 << 0)
+#define PTP_PORT_TX_FORWARD		(1 << 1)
+
+#define PTP_CHECK_PATH_DELAY		(1 << 7)
+#define PTP_VERIFY_TIMESTAMP		(1 << 8)
+#define PTP_ZERO_RESERVED_FIELD		(1 << 9)
+#define PTP_UPDATE_PDELAY_RESP_PORT	(1 << 10)
+#define PTP_UPDATE_PDELAY_RESP_TIME	(1 << 11)
+#define PTP_CHECK_SYS_TIME		(1 << 16)
+#define PTP_CHECK_SYNC_TIME		(1 << 24)
+#define PTP_TEST_TX_INFO		(1 << 28)
+#define PTP_USE_DEFAULT_PORT		(1 << 29)
+#define PTP_KEEP_DST_PORT		(1 << 30)
+#define PTP_UPDATE_DST_PORT		(1 << 31)
+
+struct ptp_info {
+	void *parent;
+	struct mutex lock;
+	struct ptp_access hw_access;
+
+	/* current system time. */
+	struct ptp_utime cur_time;
+	struct ptp_utime gps_time;
+	struct ksz_ptp_time time_diff;
+	u32 sec_hi;
+	u32 sec_lo;
+	struct delayed_work check_pps;
+	struct delayed_work update_sec;
+	unsigned long update_sec_jiffies;
+
+	u32 adjust;
+	int drift;
+	int drift_set;
+
+	int adjust_offset;
+	int offset_changed;
+	s64 adjust_sec;
+	s64 sec_changed;
+
+	struct ptp_utime time_set;
+
+	u32 adj_delay;
+	u32 get_delay;
+	u32 set_delay;
+	int pps_offset;
+	struct file_dev_info *gps_dev;
+	unsigned long gps_req_time;
+	unsigned long gps_resp_time;
+	u8 gps_gpi;
+	u8 gps_tsi;
+	u16 gps_seqid;
+	u8 pps_tsi;
+	u8 pps_tso;
+	u8 pps_gpo;
+	u8 mhz_tso;
+	u8 mhz_gpo;
+	u8 version;
+	u8 ports;
+	u8 started;
+
+	/* hardware register values. */
+	u16 rx_latency[MAX_PTP_PORT][3];
+	u16 tx_latency[MAX_PTP_PORT][3];
+	short asym_delay[MAX_PTP_PORT][3];
+	u32 peer_delay[MAX_PTP_PORT];
+	struct ptp_peer_delay_ts peer_delay_info[MAX_PTP_PORT];
+
+	spinlock_t rx_msg_lock;
+	spinlock_t tx_msg_lock;
+	struct ptp_msg_info rx_msg_info[MANAGEMENT_MSG + 1];
+	struct ptp_msg_info tx_msg_info[MANAGEMENT_MSG + 1];
+	u16 seqid_sync[MAX_PTP_PORT];
+	u16 seqid_fup[MAX_PTP_PORT];
+	u16 seqid_pdelay_req[MAX_PTP_PORT];
+	u16 seqid_pdelay_resp[MAX_PTP_PORT];
+	u16 seqid_pdelay_resp_fup[MAX_PTP_PORT];
+	struct ptp_msg *rx_msg;
+	struct ptp_msg *tx_msg;
+	int tx_msg_cnt;
+	int tx_msg_parsed;
+	u32 tx_ports;
+	int cap;
+	int def_forward;
+	int forward;
+	int op_mode;
+	int op_state;
+
+	/* used to remember tx timestamp to differentiate between pdelay_req
+	 * and pdelay_resp.
+	 */
+	u32 xdelay_ts[MAX_PTP_PORT];
+	u32 pdresp_ts[MAX_PTP_PORT];
+
+	/* tx timestamp */
+	struct ptp_hw_ts hw_sync[MAX_PTP_PORT];
+	struct ptp_hw_ts hw_dreq[MAX_PTP_PORT];
+	struct ptp_hw_ts hw_resp[MAX_PTP_PORT];
+	struct ptp_tx_ts tx_sync[MAX_PTP_PORT];
+	struct ptp_tx_ts tx_dreq[MAX_PTP_PORT];
+	struct ptp_tx_ts tx_resp[MAX_PTP_PORT];
+	u32 pdelay_resp_timestamp[MAX_PTP_PORT];
+	u16 link_ports;
+
+	int state;
+	u16 def_mode;
+	u16 def_cfg;
+	u16 mode;
+	u16 cfg;
+	u16 domain;
+	u16 vid;
+	int ptp_synt;
+	u16 trig_intr;
+	u16 ts_intr;
+	u16 tx_intr;
+
+	int tsi_intr;
+	int tsi_used;
+	int tsi_sys;
+	int tso_intr;
+	int tso_used;
+	int tso_sys;
+	int ts_status;
+	int cascade_rx;
+	int cascade_tx;
+	struct {
+		int first;
+		int total;
+		int tso;
+	} cascade_gpo[MAX_GPIO];
+	struct ptp_clock_identity clockIdentity;
+	struct ptp_clock_identity masterIdentity;
+	struct ptp_event events[MAX_TIMESTAMP_UNIT];
+	struct ptp_output outputs[MAX_TRIG_UNIT + 1];
+	u16 tso_cnt[MAX_TRIG_UNIT + 1];
+	u8 tso_chk[MAX_TRIG_UNIT + 1];
+	u8 cascade_first;
+	u8 cascade_total;
+	u8 cascade_tso;
+	int dev_major;
+	struct file_dev_info *dev[2];
+	struct file_dev_info *tsi_dev[MAX_TIMESTAMP_UNIT];
+	struct file_dev_info *tso_dev[MAX_TRIG_UNIT];
+	char dev_name[2][20];
+	wait_queue_head_t wait_ts[MAX_PTP_PORT];
+	wait_queue_head_t wait_intr;
+	unsigned long delay_ticks;
+	int rx_en;
+	int tx_en;
+	u16 rx_en_ports;
+	u16 tx_en_ports;
+	int utc_offset;
+
+	u32 clk_divider;
+	u32 (*get_clk_cnt)(void);
+	u32 last_clk_cnt;
+	u64 total_clk_cnt;
+	u32 first_sec;
+	u32 intr_sec;
+	unsigned long last_jiffies;
+	u64 total_jiffies;
+	ktime_t first_ktime;
+	int first_drift;
+	struct ptp_ts last_rx_ts;
+	struct ptp_ts last_tx_ts;
+
+	uint features;
+	uint overrides;
+
+	u32 need_sync_tx_ts:1;
+	u32 need_resp_tx_ts:1;
+	u32 need_1_step_resp_help:1;
+	u32 need_2_step_resp_help:1;
+	u32 need_1_step_clock_oper:1;
+	u32 need_peer_delay_set_help:1;
+	u32 have_first_drift_set:1;
+	u32 use_own_api:1;
+
+	u32 clk_add:1;
+	u32 clk_change:1;
+	u32 cascade:1;
+	u32 cascade_sw_each:1;
+	u32 cascade_sw_only:1;
+
+	struct work_struct adj_clk;
+	struct work_struct set_latency;
+	struct work_struct set_p2p;
+	struct work_struct set_peer_delay;
+
+	const struct ptp_ops *ops;
+	const struct ptp_reg_ops *reg;
+	void (*test_access_time)(struct ptp_info *ptp);
+
+	struct workqueue_struct *access;
+
+	struct device *dev_parent;
+#ifdef CONFIG_PTP_1588_CLOCK
+	void *clock_info;
+	u32 clock_events;
+#endif
+};
+
+struct ksz_ptp_sysfs {
+	struct ksz_dev_attr *ksz_clock_attrs[CLOCK_ENTRIES];
+	struct attribute **clock_attrs[CLOCK_ENTRIES];
+};
+#endif
+
+enum {
+	DEV_IOC_UNIT_UNAVAILABLE = DEV_IOC_LAST,
+	DEV_IOC_UNIT_USED,
+	DEV_IOC_UNIT_ERROR,
+};
+
+enum {
+	DEV_INFO_MSG = DEV_INFO_LAST,
+	DEV_INFO_RESET,
+};
+
+enum {
+	DEV_PTP_CFG,
+	DEV_PTP_TEVT,
+	DEV_PTP_TOUT,
+	DEV_PTP_CLK,
+	DEV_PTP_CASCADE,
+	DEV_PTP_DELAY,
+	DEV_PTP_REG,
+	DEV_PTP_IDENTITY,
+	DEV_PTP_PEER_DELAY,
+	DEV_PTP_UTC_OFFSET,
+	DEV_PTP_TIMESTAMP,
+	DEV_PTP_MSG,
+	DEV_PTP_PORT_CFG,
+};
+
+#ifndef TSM_CMD_CLOCK_SET
+#define TSM_CMD_RESP			0x04
+#define TSM_CMD_GET_TIME_RESP		0x08
+
+#define TSM_CMD_CLOCK_SET		0x10
+#define TSM_CMD_CLOCK_CORRECT		0x20
+#define TSM_CMD_DB_SET			0x30
+#define TSM_CMD_DB_GET			0x40
+#define TSM_CMD_STAT_CLEAR		0x50
+#define TSM_CMD_STAT_GET		0x60
+#define TSM_CMD_CNF_SET			0x70
+#define TSM_CMD_CNF_GET			0x80
+#define TSM_CMD_GPIO_SET		0x90
+#define TSM_CMD_GPIO_GET		0xA0
+#define	TSM_CMD_SET_SECONDS		0xB0
+/* todo */
+#define TSM_CMD_GET_GPS_TS		0xE0
+
+/* used for accessing reserved DB entry for a given port for SYNC or DELAY_REQ
+ * packets on egress
+ */
+#define TSM_CMD_DB_GET_RESRV1		0xB0
+/* used for accessing reserved DB entry for a given port for P2P PATH_DEL_REQ
+ * packets on egress
+ */
+#define TSM_CMD_DB_GET_RESRV2		0xC0
+/* used for getting time from TSM, no look-up of a DB entry with an ingress or
+ * egress time stamp
+ */
+#define TSM_CMD_DB_GET_TIME		0xD0
+#define TSM_CMD_DB_SET_TIME		0xF0
+#endif
+
+struct tsm_cfg {
+	u8 cmd;
+	u8 port;
+	u8 enable;
+	u8 gmp;
+	u32 ingress_delay;
+	u16 egress_delay;
+} __packed;
+
+struct tsm_clock_set {
+	u8 cmd;
+	u32 timestamp;
+	u32 nsec;
+	u32 sec;
+	u8 reserved[5];
+} __packed;
+
+struct tsm_clock_correct {
+	u8 cmd;
+	u8 add;
+	u32 sec;
+	u32 nsec;
+	u32 drift;
+	u32 offset;
+} __packed;
+
+struct tsm_db {
+	u8 cmd;
+	u8 index;
+	u16 seqid;
+	u8 mac[2];
+	u32 cur_sec;
+	u32 cur_nsec;
+	u32 timestamp;
+} __packed;
+
+struct tsm_get_gps {
+	u8 cmd;
+	u8 reserved[7];
+	u16 seqid;
+	u32 sec;
+	u32 nsec;
+} __packed;
+
+struct tsm_get_time {
+	u8 cmd;
+	u16 seqid;
+	u8 msg;
+	u32 sec;
+	u32 nsec;
+} __packed;
+
+#define PTP_CAN_RX_TIMESTAMP		BIT(0)
+#define PTP_KNOW_ABOUT_LATENCY		BIT(1)
+#define PTP_HAVE_MULT_DEVICES		BIT(2)
+#define PTP_HAVE_MULT_PORTS		BIT(3)
+#define PTP_KNOW_ABOUT_MULT_PORTS	BIT(4)
+#define PTP_USE_RESERVED_FIELDS		BIT(5)
+#define PTP_SEPARATE_PATHS		BIT(6)
+#define PTP_USE_ONE_STEP		BIT(7)
+
+#define PTP_PORT_ENABLED		BIT(0)
+#define PTP_PORT_ASCAPABLE		BIT(1)
+
+#ifdef __KERNEL__
+struct ptp_attributes {
+	int features;
+	int overrides;
+	int vid;
+	int gpio_1;
+	int gpio_2;
+};
+#endif
+
+#endif
diff -Napur linux-linux4microchip-2022.10-orig/drivers/net/ethernet/micrel/ksz_ptp.c linux-linux4microchip-2022.10/drivers/net/ethernet/micrel/ksz_ptp.c
--- linux-linux4microchip-2022.10-orig/drivers/net/ethernet/micrel/ksz_ptp.c	1969-12-31 16:00:00.000000000 -0800
+++ linux-linux4microchip-2022.10/drivers/net/ethernet/micrel/ksz_ptp.c	2023-04-25 16:13:55.052163480 -0700
@@ -0,0 +1,5221 @@
+/**
+ * Microchip PTP common code
+ *
+ * Copyright (c) 2015-2023 Microchip Technology Inc.
+ *	Tristram Ha <Tristram.Ha@microchip.com>
+ *
+ * Copyright (c) 2009-2015 Micrel, Inc.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ */
+
+
+#define CURRENT_UTC_OFFSET  37 /* 1 Jan 2017 */
+
+
+#if 1
+#define ENABLE_10_MHZ_CLK
+#endif
+
+
+#define FMT_NSEC_SIZE			12
+
+static char *format_nsec(char *str, u32 nsec)
+{
+	u32 nsec0;
+	u32 nsec1;
+	u32 nsec2;
+	char str0[4];
+
+	nsec0 = nsec % 1000;
+	nsec1 = (nsec / 1000) % 1000;
+	nsec2 = (nsec / 1000000) % 1000;
+	sprintf(str0, "%03u", nsec0);
+	if (nsec2)
+		sprintf(str, "%3u.%03u.%s", nsec2, nsec1, str0);
+	else if (nsec1)
+		sprintf(str, "    %3u.%s", nsec1, str0);
+	else
+		sprintf(str, "        %3u", nsec0);
+	return str;
+}  /* format_nsec */
+
+struct pseudo_iphdr {
+	__u8 ttl;
+	__u8 protocol;
+	__be16 tot_len;
+	__be32 saddr;
+	__be32 daddr;
+};
+
+struct pseudo_ip6hdr {
+	__be16 payload_len;
+	__u8 hop_limit;
+	__u8 nexthdr;
+	struct in6_addr saddr;
+	struct in6_addr daddr;
+};
+
+static u32 timestamp_val(u32 timestamp, u8 *sec)
+{
+	*sec = timestamp >> 30;
+	timestamp <<= 2;
+	timestamp >>= 2;
+	return timestamp;
+}  /* timestamp_val */
+
+static void calc_diff(struct ksz_ptp_time *prev, struct ksz_ptp_time *cur,
+	struct ksz_ptp_time *result)
+{
+	struct ksz_ptp_time diff;
+	int prev_nsec = prev->nsec;
+	int cur_nsec = cur->nsec;
+
+	if (prev->sec < 0)
+		prev_nsec = -prev_nsec;
+	if (cur->sec < 0)
+		cur_nsec = -cur_nsec;
+	diff.sec = cur->sec - prev->sec;
+	diff.nsec = cur_nsec - prev_nsec;
+	if (diff.nsec >= NANOSEC_IN_SEC) {
+		diff.nsec -= NANOSEC_IN_SEC;
+		diff.sec++;
+	} else if (diff.nsec <= -NANOSEC_IN_SEC) {
+		diff.nsec += NANOSEC_IN_SEC;
+		diff.sec--;
+	}
+	if (diff.sec > 0 && diff.nsec < 0) {
+		diff.nsec += NANOSEC_IN_SEC;
+		diff.sec--;
+	} else if (diff.sec < 0 && diff.nsec > 0) {
+		diff.nsec -= NANOSEC_IN_SEC;
+		diff.sec++;
+	}
+	if (diff.nsec < 0 && diff.sec < 0)
+		diff.nsec = -diff.nsec;
+	result->sec = diff.sec;
+	result->nsec = diff.nsec;
+}  /* calc_diff */
+
+static void calc_udiff(struct ptp_utime *prev, struct ptp_utime *cur,
+	struct ksz_ptp_time *result)
+{
+	struct ksz_ptp_time t1;
+	struct ksz_ptp_time t2;
+
+	if (prev->sec > (1UL << 31) || cur->sec > (1UL << 31)) {
+		s64 t3;
+		s64 t4;
+		s64 diff;
+		s32 rem;
+
+		t3 = (u64) prev->sec * NANOSEC_IN_SEC + prev->nsec;
+		t4 = (u64) cur->sec * NANOSEC_IN_SEC + cur->nsec;
+		diff = t4 - t3;
+		t3 = div_s64_rem(diff, NSEC_PER_SEC, &rem);
+		result->sec = (s32) t3;
+		result->nsec = rem;
+		return;
+	}
+	t1.sec = prev->sec;
+	t1.nsec = prev->nsec;
+	t2.sec = cur->sec;
+	t2.nsec = cur->nsec;
+	calc_diff(&t1, &t2, result);
+}  /* calc_udiff */
+
+#ifdef PTP_PROCESS
+static void calc_diff64(struct ptp_ltime *prev, struct ptp_ltime *cur,
+	struct ptp_ltime *result)
+{
+	struct ptp_ltime diff;
+	s64 prev_nsec = prev->nsec;
+	s64 cur_nsec = cur->nsec;
+	s64 scaled_nsec = (s64) NANOSEC_IN_SEC << SCALED_NANOSEC_S;
+
+	if (prev->sec < 0)
+		prev_nsec = -prev_nsec;
+	if (cur->sec < 0)
+		cur_nsec = -cur_nsec;
+	diff.sec = cur->sec - prev->sec;
+	diff.nsec = cur_nsec - prev_nsec;
+	if (diff.nsec >= scaled_nsec) {
+		diff.nsec -= scaled_nsec;
+		diff.sec++;
+	} else if (diff.nsec <= -scaled_nsec) {
+		diff.nsec += scaled_nsec;
+		diff.sec--;
+	}
+	if (diff.sec > 0 && diff.nsec < 0) {
+		diff.nsec += scaled_nsec;
+		diff.sec--;
+	} else if (diff.sec < 0 && diff.nsec > 0) {
+		diff.nsec -= scaled_nsec;
+		diff.sec++;
+	}
+	if (diff.nsec < 0 && diff.sec < 0)
+		diff.nsec = -diff.nsec;
+	result->sec = diff.sec;
+	result->nsec = diff.nsec;
+}  /* calc_diff64 */
+#endif
+
+static void add_nsec(struct ptp_utime *t, u32 nsec)
+{
+	t->nsec += nsec;
+	if (t->nsec >= NANOSEC_IN_SEC) {
+		t->nsec -= NANOSEC_IN_SEC;
+		t->sec++;
+	}
+}  /* add_nsec */
+
+static void sub_nsec(struct ptp_utime *t, u32 nsec)
+{
+	if (t->nsec < nsec) {
+		t->nsec += NANOSEC_IN_SEC;
+		t->sec--;
+	}
+	t->nsec -= nsec;
+}  /* sub_nsec */
+
+static void update_ts(struct ptp_ts *ts, u32 cur_sec)
+{
+	int sec;
+	u8 sec_chk;
+
+	ts->t.nsec = timestamp_val(ts->timestamp, &sec_chk);
+	if (ts->timestamp)
+		sec = (cur_sec - sec_chk) & 3;
+	else
+		sec = 0;
+	if (sec >= 2)
+		sec -= 4;
+	ts->t.sec = cur_sec - sec;
+}  /* update_ts */
+
+#define INIT_NSEC			40
+#define MIN_CYCLE_NSEC			8
+#define MIN_GAP_NSEC			120
+#define PULSE_NSEC			8
+
+static int check_cascade(struct ptp_info *ptp, int first, int total,
+	u16 *repeat, u32 sec, u32 nsec)
+{
+	struct ptp_output *cur;
+	struct ptp_output *next;
+	struct ptp_output *prev;
+	int diff;
+	int i;
+	int tso;
+	int min_cnt;
+	int cnt;
+
+	tso = first;
+	cur = &ptp->outputs[tso];
+	next = &ptp->outputs[first + total];
+	next->start = cur->start;
+	add_nsec(&next->start, cur->iterate);
+	for (i = 0; i < total; i++, tso++) {
+		cur = &ptp->outputs[tso];
+		cur->stop = cur->start;
+		add_nsec(&cur->stop, cur->len);
+		next = &ptp->outputs[tso + 1];
+		calc_udiff(&cur->stop, &next->start, &cur->gap);
+		if ((cur->gap.sec < 0 || (!cur->gap.sec && cur->gap.nsec < 0))
+				&& (i < total - 1 || 1 != *repeat)) {
+			dbg_msg("gap too small: %d=%d\n", i, cur->gap.nsec);
+			return 1;
+		}
+	}
+	if (1 == *repeat)
+		goto check_cascade_done;
+
+	min_cnt = *repeat;
+	tso = first + 1;
+	for (i = 1; i < total; i++, tso++) {
+		cur = &ptp->outputs[tso];
+		prev = &ptp->outputs[tso - 1];
+		if (cur->iterate < prev->iterate) {
+			diff = prev->iterate - cur->iterate;
+			cnt = prev->gap.nsec / diff + 1;
+		} else if (cur->iterate > prev->iterate) {
+			diff = cur->iterate - prev->iterate;
+			cnt = cur->gap.nsec / diff + 1;
+		} else
+			cnt = *repeat;
+		if (min_cnt > cnt)
+			min_cnt = cnt;
+	}
+	if (*repeat > min_cnt)
+		*repeat = min_cnt;
+	prev = &ptp->outputs[first + tso];
+	for (cnt = 0; cnt < *repeat; cnt++) {
+		tso = first;
+		for (i = 0; i < total; i++, tso++) {
+			cur = &ptp->outputs[tso];
+			next = &ptp->outputs[tso + 1];
+			dbg_msg("%d: %d:%9d %d %d:%9d %d: %d:%9d\n",
+				i, cur->start.sec, cur->start.nsec, cur->len,
+				cur->gap.sec, cur->gap.nsec, cur->iterate,
+				cur->stop.sec, cur->stop.nsec);
+			if (cur->stop.sec > next->start.sec ||
+					(cur->stop.sec == next->start.sec &&
+					cur->stop.nsec > next->stop.nsec))
+				dbg_msg("> %d %d:%9d %d:%9d\n", i,
+					cur->stop.sec, cur->stop.nsec,
+					next->start.sec, next->start.nsec);
+			add_nsec(&cur->start, cur->iterate);
+			cur->stop = cur->start;
+			add_nsec(&cur->stop, cur->len);
+			if (!i)
+				prev->start = cur->start;
+		}
+		dbg_msg("%d:%9d\n", prev->start.sec, prev->start.nsec);
+	}
+
+check_cascade_done:
+	tso = first;
+	cur = &ptp->outputs[tso];
+	if (cur->trig.sec >= sec)
+		return 0;
+
+	for (i = 0; i < total; i++, tso++) {
+		cur = &ptp->outputs[tso];
+		cur->trig.sec += sec;
+		add_nsec(&cur->trig, nsec);
+	}
+	return 0;
+}
+
+#define MAX_DRIFT_CORR			6250000
+#define LOW_DRIFT_CORR			2499981
+#define MAX_U32_SHIFT			32
+#define MAX_DIVIDER_SHIFT		31
+
+static u32 drift_in_sec(u32 abs_offset, u64 interval64)
+{
+	u64 drift64;
+
+	drift64 = abs_offset;
+	drift64 *= NANOSEC_IN_SEC;
+	drift64 = div64_u64(drift64, interval64);
+	return (u32) drift64;
+}
+
+static u32 clk_adjust_val(int diff, u32 interval)
+{
+	u32 adjust;
+	u32 rem;
+	u64 adjust64;
+
+	if (0 == diff)
+		return 0;
+	if (diff < 0)
+		adjust = -diff;
+	else
+		adjust = diff;
+
+	/* 2^32 * adjust * 1000000000 / interval / 25000000 */
+	if (interval != NANOSEC_IN_SEC)
+		adjust = drift_in_sec(adjust, interval);
+
+	if (adjust >= MAX_DRIFT_CORR)
+		adjust = 0x3fffffff;
+	else {
+		adjust64 = 1LL << 32;
+		adjust64 *= adjust;
+		adjust64 = div_u64_rem(adjust64, 25000000, &rem);
+		adjust = (u32) adjust64;
+		if (adjust >= 0x40000000)
+			adjust = 0x3fffffff;
+	}
+	if (diff < 0)
+		adjust |= PTP_RATE_DIR << 16;
+	return adjust;
+}  /* clk_adjust_val */
+
+static void ptp_tso_off(struct ptp_info *ptp, u8 tso, u16 tso_bit)
+{
+	ptp->reg->tx_off(ptp, tso);
+	ptp->tso_intr &= ~tso_bit;
+	ptp->tso_used &= ~tso_bit;
+	ptp->tso_dev[tso] = NULL;
+}  /* ptp_tso_off */
+
+static inline void ptp_tx_reset(struct ptp_info *ptp, u16 tso_bit)
+{
+	struct ksz_sw *sw = container_of(ptp, struct ksz_sw, ptp_hw);
+
+	sw->reg->w16(sw, TRIG_RESET, tso_bit);
+}  /* ptp_tx_reset */
+
+static inline void ptp_gpo_reset(struct ptp_info *ptp, int gpo, u16 tso_bit)
+{
+	ptp_tx_reset(ptp, tso_bit);
+	ptp->cascade_gpo[gpo].tso &= ~tso_bit;
+}  /* ptp_gpo_reset */
+
+/* -------------------------------------------------------------------------- */
+
+static void ptp_acquire(struct ptp_info *ptp)
+{
+	struct ksz_sw *sw = container_of(ptp, struct ksz_sw, ptp_hw);
+
+	mutex_lock(sw->hwlock);
+#ifdef PTP_SPI
+	mutex_lock(sw->reglock);
+#endif
+}  /* ptp_acquire */
+
+static void ptp_release(struct ptp_info *ptp)
+{
+	struct ksz_sw *sw = container_of(ptp, struct ksz_sw, ptp_hw);
+
+#ifdef PTP_SPI
+	mutex_unlock(sw->reglock);
+#endif
+	mutex_unlock(sw->hwlock);
+}  /* ptp_release */
+
+static void get_ptp_time(struct ptp_info *ptp, struct ptp_utime *t)
+{
+	u16 data;
+	u8 subnsec;
+	struct ksz_sw *sw = container_of(ptp, struct ksz_sw, ptp_hw);
+
+	data = sw->cached.ptp_clk_ctrl;
+	data |= PTP_READ_RTC;
+	sw_w16(sw, PTP_CLK_CTRL, data);
+	t->sec = sw_r32(sw, PTP_RTC_SEC_L);
+	t->nsec = sw_r32(sw, PTP_RTC_NANOSEC_L);
+	subnsec = sw_r8(sw, PTP_RTC_SUB_NANOSEC);
+	add_nsec(t, subnsec * 8);
+}  /* get_ptp_time */
+
+static void set_ptp_time(struct ptp_info *ptp, struct ptp_utime *t)
+{
+	u16 data;
+	struct ksz_sw *sw = container_of(ptp, struct ksz_sw, ptp_hw);
+
+	data = sw->cached.ptp_clk_ctrl;
+	sw_w8(sw, PTP_RTC_SUB_NANOSEC, 0);
+	sw_w32(sw, PTP_RTC_SEC_L, t->sec);
+	sw_w32(sw, PTP_RTC_NANOSEC_L, t->nsec);
+	data |= PTP_LOAD_TIME;
+	sw_w16(sw, PTP_CLK_CTRL, data);
+}  /* set_ptp_time */
+
+static void adjust_ptp_time(struct ptp_info *ptp, int add, u32 sec, u32 nsec,
+	int adj_hack)
+{
+	u16 ctrl;
+	u16 adj = 0;
+	u32 val = nsec;
+	struct ksz_sw *sw = container_of(ptp, struct ksz_sw, ptp_hw);
+
+	ctrl = sw->cached.ptp_clk_ctrl;
+	if (add)
+		ctrl |= PTP_STEP_DIR;
+	else
+		ctrl &= ~PTP_STEP_DIR;
+	sw->cached.ptp_clk_ctrl = ctrl;
+	if (adj_hack) {
+		adj = ctrl;
+		ctrl &= ~PTP_CLK_ADJ_ENABLE;
+	}
+	ctrl |= PTP_STEP_TIME;
+	sw_w32(sw, PTP_RTC_SEC_L, sec);
+	do {
+		if (nsec > NANOSEC_IN_SEC - 1)
+			nsec = NANOSEC_IN_SEC - 1;
+		sw_w32(sw, PTP_RTC_NANOSEC_L, nsec);
+		sw_w16(sw, PTP_CLK_CTRL, ctrl);
+		val -= nsec;
+		nsec = val;
+	} while (val);
+	if (adj_hack && (adj & PTP_CLK_ADJ_ENABLE))
+		sw_w16(sw, PTP_CLK_CTRL, adj);
+}  /* adjust_ptp_time */
+
+static void adjust_sync_time(struct ptp_info *ptp, int diff, u32 interval,
+	u32 duration)
+{
+	u32 adjust;
+	struct ksz_sw *sw = container_of(ptp, struct ksz_sw, ptp_hw);
+
+	adjust = clk_adjust_val(diff, interval);
+	adjust |= PTP_TMP_RATE_ENABLE << 16;
+	sw_w32(sw, PTP_RATE_DURATION_L, duration);
+	sw_w32(sw, PTP_SUBNANOSEC_RATE_L, adjust);
+}  /* adjust_sync_time */
+
+static inline void ptp_rx_reset(struct ptp_info *ptp, u16 tsi_bit)
+{
+	struct ksz_sw *sw = container_of(ptp, struct ksz_sw, ptp_hw);
+
+	sw_w16(sw, TS_RESET, tsi_bit);
+}  /* ptp_rx_reset */
+
+static void ptp_rx_off(struct ptp_info *ptp, u8 tsi)
+{
+	u16 ctrl;
+	u16 tsi_bit = (1 << tsi);
+	struct ksz_sw *sw = container_of(ptp, struct ksz_sw, ptp_hw);
+
+	/* Disable previous timestamp interrupt. */
+	if (ptp->ts_intr & tsi_bit) {
+		ptp->ts_intr &= ~tsi_bit;
+		sw_w16(sw, TS_INT_ENABLE, ptp->ts_intr);
+	}
+
+	/* Disable previous timestamp detection. */
+	ctrl = sw_r16(sw, TS_ENABLE);
+	if (ctrl & tsi_bit) {
+		ctrl &= ~tsi_bit;
+		sw_w16(sw, TS_ENABLE, ctrl);
+	}
+
+	/*
+	 * Need to turn off cascade mode if it is used previously; otherwise,
+	 * event counter keeps increasing.
+	 */
+	if (ptp->cascade_rx & tsi_bit) {
+		ptp_rx_reset(ptp, tsi_bit);
+		ptp->cascade_rx &= ~tsi_bit;
+	}
+}  /* ptp_rx_off */
+
+static inline void ptp_rx_intr(struct ptp_info *ptp, u16 tsi_bit)
+{
+	struct ksz_sw *sw = container_of(ptp, struct ksz_sw, ptp_hw);
+
+	ptp->ts_intr |= tsi_bit;
+	sw_w16(sw, TS_INT_ENABLE, ptp->ts_intr);
+}  /* ptp_rx_intr */
+
+static inline void ptp_rx_on(struct ptp_info *ptp, u16 tsi_bit)
+{
+	u16 ctrl;
+	struct ksz_sw *sw = container_of(ptp, struct ksz_sw, ptp_hw);
+
+	ctrl = sw_r16(sw, TS_ENABLE);
+	ctrl |= tsi_bit;
+	sw_w16(sw, TS_ENABLE, ctrl);
+}  /* ptp_rx_on */
+
+static void ptp_rx_restart(struct ptp_info *ptp, u16 tsi_bit)
+{
+	u16 ctrl;
+	struct ksz_sw *sw = container_of(ptp, struct ksz_sw, ptp_hw);
+
+	ctrl = sw_r16(sw, TS_ENABLE);
+	ctrl &= ~tsi_bit;
+	sw_w16(sw, TS_ENABLE, ctrl);
+	ctrl |= tsi_bit;
+	sw_w16(sw, TS_ENABLE, ctrl);
+}  /* ptp_rx_restart */
+
+static void ptp_rx_event(struct ptp_info *ptp, u8 tsi, u8 gpi, u8 event,
+	int intr)
+{
+	u32 reg;
+	u16 ctrl;
+	u16 tsi_bit = (1 << tsi);
+	struct ksz_sw *sw = container_of(ptp, struct ksz_sw, ptp_hw);
+
+	/* Config pattern. */
+	reg = TSn_CONF(tsi);
+	ctrl = event | ((gpi & 0xf) << 8);
+	sw_w16(sw, reg, ctrl);
+
+	/* Enable timestamp interrupt. */
+	if (intr)
+		ptp_rx_intr(ptp, tsi_bit);
+
+	/* Enable timestamp detection. */
+	ptp_rx_on(ptp, tsi_bit);
+}  /* ptp_rx_event */
+
+static void ptp_rx_cascade_event(struct ptp_info *ptp, u8 first, u8 total,
+	u8 gpi, u8 event, int intr)
+{
+	int last;
+	int tsi;
+	u32 reg;
+	u16 ctrl;
+	u16 tail;
+	int i;
+	int prev;
+	struct ksz_sw *sw = container_of(ptp, struct ksz_sw, ptp_hw);
+
+	last = (first + total - 1) % MAX_TIMESTAMP_UNIT;
+	tsi = last;
+	tail = TS_CASCADE_TAIL;
+	for (i = 1; i < total; i++) {
+		reg = TSn_CONF(tsi);
+		prev = tsi - 1;
+		if (prev < 0)
+			prev = MAX_TIMESTAMP_UNIT - 1;
+		ctrl = event | ((gpi & 0xf) << 8);
+		ctrl |= TS_CASCADE_EN | ((prev & 0xf) << 1);
+		ctrl |= tail;
+		ptp->cascade_rx |= (1 << tsi);
+		sw_w16(sw, reg, ctrl);
+
+		/* Enable timestamp interrupt. */
+		if (intr)
+			ptp->ts_intr |= (1 << tsi);
+		--tsi;
+		if (tsi < 0)
+			tsi = MAX_TIMESTAMP_UNIT - 1;
+		tail = 0;
+	}
+	reg = TSn_CONF(first);
+	ctrl = event | ((gpi & 0xf) << 8);
+	ctrl |= TS_CASCADE_EN | ((last & 0xf) << 1);
+	ptp->cascade_rx |= (1 << first);
+	sw_w16(sw, reg, ctrl);
+
+	/* Enable timestamp interrupt. */
+	if (intr)
+		ptp_rx_intr(ptp, (1 << first));
+
+	/* Enable timestamp detection. */
+	ptp_rx_on(ptp, (1 << first));
+}  /* ptp_rx_cascade_event */
+
+static void ptp_read_event(struct ptp_info *ptp, u8 tsi)
+{
+	u32 reg;
+	u16 ctrl;
+	u16 tsi_bit = (1 << tsi);
+
+	u32 reg_ns;
+	u32 reg_s;
+	u32 reg_sub;
+	struct ptp_utime t;
+	u16 sub;
+	int max_ts;
+	int num;
+	int i;
+	int edge;
+	struct ptp_event *event = &ptp->events[tsi];
+	int last = event->num;
+	struct ksz_sw *sw = container_of(ptp, struct ksz_sw, ptp_hw);
+
+	reg = TSn_EVENT_STATUS(tsi);
+	ctrl = sw_r16(sw, reg);
+	num = (ctrl & TS_NO_EVENT_DET_MASK) >> 1;
+	max_ts = (num <= event->max) ? num : event->max;
+	i = event->num;
+
+	reg_ns = TSn_0_EVENT_NANOSEC_L(tsi) + 0x10 * i;
+	reg_s = TSn_0_EVENT_SEC_L(tsi) + 0x10 * i;
+	reg_sub = TSn_0_EVENT_SUB_NANOSEC(tsi) + 0x10 * i;
+	for (; i < max_ts; i++) {
+		t.nsec = sw_r32(sw, reg_ns);
+		t.sec = sw_r32(sw, reg_s);
+		sub = sw_r16(sw, reg_sub);
+		edge = ((t.nsec >> 30) & 1);
+		t.nsec <<= 2;
+		t.nsec >>= 2;
+		add_nsec(&t, sub * 8);
+#if 1
+/*
+ * THa  2011/10/06
+ * Unit sometimes detects rising edge when it is configured to detect falling
+ * edge only.  This happens in the case of hooking up the output pin to an
+ * input pin and using two units running opposite cycle in cascade mode.  The
+ * 8 ns switch pulse before the cycle is too short to detect properly,
+ * resulting in missing edges.
+ * When detecting events directly from the output pin, the minimum pulse time
+ * is 24 ns for proper detection without missing any edge.
+ */
+		if (event->event < 2 && edge != event->event)
+			edge = event->event;
+#endif
+		event->edge |= edge << i;
+		event->t[i] = t;
+		reg_ns += 0x10;
+		reg_s += 0x10;
+		reg_sub += 0x10;
+	}
+	event->num = max_ts;
+
+	/* Indicate there is new event. */
+	if (event->num > last)
+		ptp->ts_status |= tsi_bit;
+}  /* ptp_read_event */
+
+static void ptp_tx_off(struct ptp_info *ptp, u8 tso)
+{
+	u16 ctrl;
+	u16 tso_bit = (1 << tso);
+	struct ksz_sw *sw = container_of(ptp, struct ksz_sw, ptp_hw);
+
+	/* Disable previous trigger out if not already completed. */
+	ctrl = sw_r16(sw, TRIG_EN);
+	if (ctrl & tso_bit) {
+		ctrl &= ~tso_bit;
+		sw_w16(sw, TRIG_EN, ctrl);
+	}
+
+	/*
+	 * Using cascade mode previously need to reset the trigger output so
+	 * that an errorneous output will not be generated during next
+	 * cascade mode setup.
+	 */
+	if (ptp->cascade_tx & tso_bit) {
+		ptp_gpo_reset(ptp, ptp->outputs[tso].gpo, tso_bit);
+		ptp->cascade_tx &= ~tso_bit;
+	} else {
+		u32 reg = TRIGn_CONF_1(tso);
+
+		ctrl = sw_r16(sw, reg);
+		if (ctrl & TRIG_CASCADE_EN) {
+			ctrl &= ~TRIG_CASCADE_EN;
+			ctrl &= ~TRIG_CASCADE_TAIL;
+			ctrl |= TRIG_CASCADE_UPS_MASK;
+			sw_w16(sw, reg, ctrl);
+		}
+	}
+}  /* ptp_tx_off */
+
+static void ptp_tx_on(struct ptp_info *ptp, u8 tso)
+{
+	u16 ctrl;
+	u16 tso_bit = (1 << tso);
+	struct ksz_sw *sw = container_of(ptp, struct ksz_sw, ptp_hw);
+
+	ctrl = sw_r16(sw, TRIG_EN);
+	ctrl |= tso_bit;
+	sw_w16(sw, TRIG_EN, ctrl);
+}  /* ptp_tx_on */
+
+static void ptp_tx_trigger_time(struct ptp_info *ptp, u8 tso, u32 sec, u32 nsec)
+{
+	u32 reg;
+	struct ksz_sw *sw = container_of(ptp, struct ksz_sw, ptp_hw);
+
+	reg = TRIGn_TARGET_SEC_L(tso);
+	sw_w32(sw, reg, sec);
+	reg = TRIGn_TARGET_NANOSEC_L(tso);
+	sw_w32(sw, reg, nsec);
+}  /* ptp_tx_trigger_time */
+
+static void ptp_tx_event(struct ptp_info *ptp, u8 tso, u8 gpo, u8 event,
+	u32 pulse, u32 cycle, u16 cnt, u32 sec, u32 nsec, u32 iterate,
+	int intr, int now, int opt)
+{
+	u32 reg;
+	u16 ctrl;
+	u16 tso_bit = (1 << tso);
+	struct ptp_output *cur = &ptp->outputs[tso];
+	struct ksz_sw *sw = container_of(ptp, struct ksz_sw, ptp_hw);
+
+	/* Hardware immediately keeps level high on new GPIO if not reset. */
+	if (cur->level && gpo != cur->gpo)
+		ptp_gpo_reset(ptp, cur->gpo, tso_bit);
+
+	/* Config pattern. */
+	reg = TRIGn_CONF_1(tso);
+	ctrl = ((event & 0x7) << 4);
+	ctrl |= (gpo & 0xf);
+	if (intr)
+		ctrl |= TRIG_NOTIFY;
+	if (now)
+		ctrl |= TRIG_NOW;
+	if (opt)
+		ctrl |= TRIG_CLK_OPT;
+	ctrl |= TRIG_CASCADE_UPS_MASK;
+	sw_w16(sw, reg, ctrl);
+
+	/* Config pulse width. */
+	if (TRIG_REG_OUTPUT == event) {
+		reg = TRIGn_BIT_PATTERN(tso);
+		sw_w16(sw, reg, (u16) pulse);
+		cur->level = 0;
+		if (cnt) {
+			reg = cnt - 1;
+			reg %= 16;
+			while (reg) {
+				pulse >>= 1;
+				reg--;
+			}
+			if (pulse & 1)
+				cur->level = 1;
+		}
+		pulse = 0;
+	} else if (event >= TRIG_NEG_PULSE) {
+		if (0 == pulse)
+			pulse = 1;
+		else if (tso != 11 && pulse > 0xffff)
+			pulse = 0xffff;
+		reg = TRIGn_PULSE_WIDTH(tso);
+		sw_w16(sw, reg, (u16) pulse);
+		if (11 == tso) {
+			if (pulse > 0xffffff)
+				pulse = 0xffffff;
+			ctrl = sw_r16(sw, TRIG_PPS_WS);
+			ctrl &= ~TRIG_PPS_WS_MASK;
+			ctrl |= ((pulse >> 16) & TRIG_PPS_WS_MASK);
+			sw_w16(sw, TRIG_PPS_WS, ctrl);
+		}
+	}
+
+	/* Config cycle width. */
+	if (event >= TRIG_NEG_PERIOD) {
+		int min_cycle = pulse * PULSE_NSEC + MIN_CYCLE_NSEC;
+
+		if (cycle < min_cycle)
+			cycle = min_cycle;
+		reg = TRIGn_CYCLE_WIDTH_L(tso);
+		sw_w32(sw, reg, cycle);
+
+		/* Config trigger count. */
+		reg = TRIGn_PER_OCCUR(tso);
+		sw_w16(sw, reg, cnt);
+	}
+
+	cur->len = 0;
+	if (event >= TRIG_NEG_PERIOD) {
+		if (cnt)
+			cur->len += cycle * cnt;
+		else
+			cur->len += 0xF0000000;
+	} else if (event >= TRIG_NEG_PULSE)
+		cur->len += pulse * PULSE_NSEC;
+	else
+		cur->len += MIN_CYCLE_NSEC;
+
+	cur->start.sec = sec;
+	cur->start.nsec = nsec;
+	cur->iterate = iterate;
+	cur->trig = cur->start;
+	cur->stop = cur->start;
+	add_nsec(&cur->stop, cur->len);
+	cur->gpo = gpo;
+
+	switch (event) {
+	case TRIG_POS_EDGE:
+	case TRIG_NEG_PULSE:
+	case TRIG_NEG_PERIOD:
+		cur->level = 1;
+		break;
+	case TRIG_REG_OUTPUT:
+		break;
+	default:
+		cur->level = 0;
+		break;
+	}
+
+	if (ptp->cascade)
+		return;
+
+	/*
+	 * Need to reset after completion.  Otherwise, this output pattern
+	 * does not behave consistently in cascade mode.
+	 */
+	if (TRIG_NEG_EDGE == event)
+		ptp->cascade_tx |= tso_bit;
+
+	ptp->cascade_gpo[gpo].total = 0;
+	if (cur->level)
+		ptp->cascade_gpo[gpo].tso |= tso_bit;
+	else
+		ptp->cascade_gpo[gpo].tso &= ~tso_bit;
+
+	/* Config trigger time. */
+	ptp_tx_trigger_time(ptp, tso, sec, nsec);
+
+	/* Enable trigger. */
+	ptp_tx_on(ptp, tso);
+}  /* ptp_tx_event */
+
+static void ptp_pps_event(struct ptp_info *ptp, u8 gpo, u32 sec)
+{
+	u32 reg;
+	u16 ctrl;
+	u32 nsec;
+	u32 pulse = (20000000 / 8);	/* 20 ms */
+	u32 cycle = 1000000000;
+	u16 cnt = 0;
+	u8 tso = ptp->pps_tso;
+	u8 event = TRIG_POS_PERIOD;
+	struct ksz_sw *sw = container_of(ptp, struct ksz_sw, ptp_hw);
+
+	ptp_tx_off(ptp, tso);
+
+	/* Config pattern. */
+	reg = TRIGn_CONF_1(tso);
+	ctrl = ((event & 0x7) << 4);
+	ctrl |= (gpo & 0xf);
+	ctrl |= TRIG_NOTIFY;
+	ctrl |= TRIG_NOW;
+	ctrl |= TRIG_CASCADE_UPS_MASK;
+	sw_w16(sw, reg, ctrl);
+
+	/* Config pulse width. */
+	reg = TRIGn_PULSE_WIDTH(tso);
+	if (11 != tso && pulse > 0xffff)
+		pulse = 0xffff;
+	sw_w16(sw, reg, (u16) pulse);
+	if (11 == tso) {
+		if (pulse > 0xffffff)
+			pulse = 0xffffff;
+		ctrl = sw_r16(sw, TRIG_PPS_WS);
+		ctrl &= ~TRIG_PPS_WS_MASK;
+		ctrl |= ((pulse >> 16) & TRIG_PPS_WS_MASK);
+		sw_w16(sw, TRIG_PPS_WS, ctrl);
+	}
+
+	/* Config cycle width. */
+	reg = TRIGn_CYCLE_WIDTH_L(tso);
+	sw_w32(sw, reg, cycle);
+
+	/* Config trigger count. */
+	reg = TRIGn_PER_OCCUR(tso);
+	sw_w16(sw, reg, cnt);
+
+	/* Config trigger time. */
+	if (ptp->pps_offset >= 0)
+		nsec = ptp->pps_offset;
+	else {
+		nsec = NANOSEC_IN_SEC + ptp->pps_offset;
+		sec--;
+	}
+	ptp_tx_trigger_time(ptp, tso, sec, nsec);
+
+	/* Enable trigger. */
+	ptp_tx_on(ptp, tso);
+}  /* ptp_pps_event */
+
+static void cfg_10MHz(struct ptp_info *ptp, u8 tso, u8 gpo, u32 sec, u32 nsec)
+{
+	u32 reg;
+	u16 ctrl;
+	u32 pulse = 6;
+	u32 cycle = 200;
+	u16 cnt = 0;
+	u8 event = TRIG_POS_PERIOD;
+	struct ksz_sw *sw = container_of(ptp, struct ksz_sw, ptp_hw);
+
+	/* Config pattern. */
+	reg = TRIGn_CONF_1(tso);
+	ctrl = ((event & 0x7) << 4);
+	ctrl |= (gpo & 0xf);
+	ctrl |= TRIG_NOTIFY;
+	ctrl |= TRIG_CASCADE_UPS_MASK;
+	if (1 == tso)
+		ctrl |= TRIG_CLK_OPT;
+	sw_w16(sw, reg, ctrl);
+
+	/* Config pulse width. */
+	reg = TRIGn_PULSE_WIDTH(tso);
+	if (11 != tso && pulse > 0xffff)
+		pulse = 0xffff;
+	sw_w16(sw, reg, (u16) pulse);
+	if (11 == tso) {
+		if (pulse > 0xffffff)
+			pulse = 0xffffff;
+		ctrl = sw_r16(sw, TRIG_PPS_WS);
+		ctrl &= ~TRIG_PPS_WS_MASK;
+		ctrl |= ((pulse >> 16) & TRIG_PPS_WS_MASK);
+		sw_w16(sw, TRIG_PPS_WS, ctrl);
+	}
+
+	/* Config cycle width. */
+	reg = TRIGn_CYCLE_WIDTH_L(tso);
+	sw_w32(sw, reg, cycle);
+
+	/* Config trigger count. */
+	reg = TRIGn_PER_OCCUR(tso);
+	sw_w16(sw, reg, cnt);
+
+	ptp_tx_trigger_time(ptp, tso, sec, nsec);
+}  /* cfg_10MHz */
+
+static void ptp_10MHz(struct ptp_info *ptp, u8 tso, u8 gpo, u32 sec)
+{
+	int i;
+	u32 nsec;
+
+	/* Config trigger time. */
+	if (ptp->pps_offset >= 0)
+		nsec = ptp->pps_offset;
+	else {
+		nsec = NANOSEC_IN_SEC + ptp->pps_offset;
+		sec--;
+	}
+	for (i = 0; i < 2; i++) {
+		ptp_tx_off(ptp, tso);
+
+		cfg_10MHz(ptp, tso, gpo, sec, nsec);
+
+		/* Enable trigger. */
+		ptp_tx_on(ptp, tso);
+
+		tso = 1;
+		nsec += 12 * 8;
+	}
+}  /* ptp_10MHz */
+
+static void ptp_tx_cascade_on(struct ptp_info *ptp, u8 tso, u8 first, u8 last,
+	u16 repeat)
+{
+	u32 reg;
+	u16 ctrl;
+	int repeat_reg = 0;
+	struct ksz_sw *sw = container_of(ptp, struct ksz_sw, ptp_hw);
+
+	reg = TRIGn_CONF_1(tso);
+	ctrl = sw_r16(sw, reg);
+	ctrl |= TRIG_CASCADE_EN;
+	ctrl &= ~TRIG_CASCADE_UPS_MASK;
+	if (tso == first)
+		ctrl |= ((last & 0xf) << 10);
+	else
+		ctrl |= (((tso - 1) & 0xf) << 10);
+	if (repeat && tso == last) {
+		ctrl |= TRIG_CASCADE_TAIL;
+		if (((ctrl >> 4) & 0xf) != TRIG_REG_OUTPUT)
+			repeat_reg = TRIGn_BIT_PATTERN(tso);
+		else
+			repeat_reg = TRIGn_PULSE_WIDTH(tso);
+	}
+	sw_w16(sw, reg, ctrl);
+	if (repeat_reg)
+		sw_w16(sw, repeat_reg, repeat - 1);
+}  /* ptp_tx_cascade_on */
+
+static void ptp_tx_cascade_cycle(struct ptp_info *ptp, u8 tso, u32 nsec)
+{
+	u32 reg;
+	struct ksz_sw *sw = container_of(ptp, struct ksz_sw, ptp_hw);
+
+	reg = TRIGn_ITERATE_TIME_L(tso);
+	sw_w32(sw, reg, nsec);
+}  /* ptp_tx_cascade_cycle */
+
+static int ptp_tx_cascade(struct ptp_info *ptp, u8 first, u8 total,
+	u16 repeat, u32 sec, u32 nsec, int intr)
+{
+	int i;
+	u8 tso;
+	u8 last;
+	struct ptp_output *cur;
+
+	last = first + total - 1;
+	if (last >= MAX_TRIG_UNIT)
+		return 1;
+	if (check_cascade(ptp, first, total, &repeat, sec, nsec)) {
+		dbg_msg("cascade repeat timing is not right\n");
+		return 1;
+	}
+	tso = first;
+	for (i = 0; i < total; i++, tso++) {
+		cur = &ptp->outputs[tso];
+		ptp_tx_trigger_time(ptp, tso, cur->trig.sec,
+			cur->trig.nsec);
+		ptp_tx_cascade_cycle(ptp, tso, cur->iterate);
+		ptp_tx_cascade_on(ptp, tso, first, last, repeat);
+		ptp->cascade_tx |= (1 << tso);
+	}
+
+	/* Do not reset last unit to keep level high. */
+	if (ptp->outputs[last].level) {
+		ptp->cascade_tx &= ~(1 << last);
+		ptp->cascade_gpo[ptp->outputs[last].gpo].tso |= (1 << last);
+	} else
+		ptp->cascade_gpo[ptp->outputs[last].gpo].tso &= ~(1 << last);
+	ptp_tx_on(ptp, first);
+	return 0;
+}  /* ptp_tx_cascade */
+
+/* -------------------------------------------------------------------------- */
+
+static void set_ptp_domain(struct ptp_info *ptp, u8 domain)
+{
+	u16 ctrl;
+	struct ksz_sw *sw = container_of(ptp, struct ksz_sw, ptp_hw);
+
+	ctrl = sw->reg->r16(sw, PTP_DOMAIN_VERSION) & ~PTP_DOMAIN_MASK;
+	ctrl |= domain;
+	sw->reg->w16(sw, PTP_DOMAIN_VERSION, ctrl);
+}  /* set_ptp_domain */
+
+static void set_ptp_mode(struct ptp_info *ptp, u16 mode)
+{
+	u16 val;
+	u16 sav;
+	struct ksz_sw *sw = container_of(ptp, struct ksz_sw, ptp_hw);
+
+	val = sw->reg->r16(sw, PTP_MSG_CONF1);
+	sav = val;
+	val &= ~(PTP_1STEP | PTP_TC_P2P | PTP_MASTER);
+	val |= mode;
+	if (val != sav)
+		sw->reg->w16(sw, PTP_MSG_CONF1, val);
+}  /* set_ptp_mode */
+
+static void synchronize_clk(struct ptp_info *ptp)
+{
+	u32 sec;
+	int inc;
+
+	if (ptp->adjust_offset < 0 || ptp->adjust_sec < 0) {
+		ptp->adjust_offset = -ptp->adjust_offset;
+		ptp->adjust_sec = -ptp->adjust_sec;
+		inc = false;
+	} else
+		inc = true;
+	sec = (u32) ptp->adjust_sec;
+	ptp->reg->adjust_time(ptp, inc, sec, ptp->adjust_offset,
+		ptp->features & PTP_ADJ_HACK);
+	ptp->offset_changed = ptp->adjust_offset;
+	ptp->adjust_offset = 0;
+	ptp->adjust_sec = 0;
+}  /* synchronize_clk */
+
+static void set_ptp_adjust(struct ptp_info *ptp, u32 adjust)
+{
+	struct ksz_sw *sw = container_of(ptp, struct ksz_sw, ptp_hw);
+
+	sw->reg->w32(sw, PTP_SUBNANOSEC_RATE_L, adjust);
+}  /* set_ptp_adjust */
+
+static inline void unsyntonize_clk(struct ptp_info *ptp)
+{
+	u16 ctrl;
+	struct ksz_sw *sw = container_of(ptp, struct ksz_sw, ptp_hw);
+
+	ctrl = sw->cached.ptp_clk_ctrl;
+	ctrl &= ~PTP_CLK_ADJ_ENABLE;
+	sw->cached.ptp_clk_ctrl = ctrl;
+	sw->reg->w16(sw, PTP_CLK_CTRL, ctrl);
+}  /* unsyntonize_clk */
+
+static void syntonize_clk(struct ptp_info *ptp)
+{
+	u16 ctrl;
+	struct ksz_sw *sw = container_of(ptp, struct ksz_sw, ptp_hw);
+
+	ctrl = sw->cached.ptp_clk_ctrl;
+	ctrl |= PTP_CLK_ADJ_ENABLE;
+	sw->cached.ptp_clk_ctrl = ctrl;
+	sw->reg->w16(sw, PTP_CLK_CTRL, ctrl);
+}  /* syntonize_clk */
+
+static u16 get_ptp_delay(struct ptp_info *ptp, int port, u32 reg)
+{
+	struct ksz_sw *sw = container_of(ptp, struct ksz_sw, ptp_hw);
+
+	reg += PTP_PORT_INTERVAL(port);
+	return sw->reg->r16(sw, reg);
+}  /* get_ptp_delay */
+
+static void set_ptp_delay(struct ptp_info *ptp, int port, u32 reg, u16 nsec)
+{
+	struct ksz_sw *sw = container_of(ptp, struct ksz_sw, ptp_hw);
+
+	reg += PTP_PORT_INTERVAL(port);
+	sw->reg->w16(sw, reg, nsec);
+}  /* set_ptp_delay */
+
+static u16 get_ptp_ingress(struct ptp_info *ptp, int port)
+{
+	return get_ptp_delay(ptp, port, PTP_PORT1_RX_MAC2PHY_DELAY);
+}
+
+static u16 get_ptp_egress(struct ptp_info *ptp, int port)
+{
+	return get_ptp_delay(ptp, port, PTP_PORT1_TX_MAC2PHY_DELAY);
+}
+
+static short get_ptp_asym(struct ptp_info *ptp, int port)
+{
+	short val;
+
+	val = get_ptp_delay(ptp, port, PTP_PORT1_ASYM_DELAY);
+	if (val & 0x8000)
+		val = -(val & ~0x8000);
+	return val;
+}
+
+static u16 get_ptp_link(struct ptp_info *ptp, int port)
+{
+	return get_ptp_delay(ptp, port, PTP_PORT1_LINK_DELAY);
+}
+
+static void set_ptp_ingress(struct ptp_info *ptp, int port, u16 nsec)
+{
+	set_ptp_delay(ptp, port, PTP_PORT1_RX_MAC2PHY_DELAY, nsec);
+}
+
+static void set_ptp_egress(struct ptp_info *ptp, int port, u16 nsec)
+{
+	set_ptp_delay(ptp, port, PTP_PORT1_TX_MAC2PHY_DELAY, nsec);
+}
+
+static void set_ptp_asym(struct ptp_info *ptp, int port, short nsec)
+{
+	if (nsec < 0)
+		nsec = -nsec | 0x8000;
+	set_ptp_delay(ptp, port, PTP_PORT1_ASYM_DELAY, nsec);
+}
+
+static void set_ptp_link(struct ptp_info *ptp, int port, u16 nsec)
+{
+	set_ptp_delay(ptp, port, PTP_PORT1_LINK_DELAY, nsec);
+}
+
+static inline void dbp_tx_ts(char *name, u8 port, u32 timestamp)
+{
+	u8 overflow;
+	char ts[FMT_NSEC_SIZE];
+
+	timestamp = timestamp_val(timestamp, &overflow);
+	format_nsec(ts, timestamp);
+	dbg_msg("%s p:%d c:%u %08x:%s\n", name, port, overflow, timestamp, ts);
+}  /* dbp_tx_ts */
+
+static void ptp_tsm_resp(void *data, void *param)
+{
+	struct tsm_db *db = (struct tsm_db *) data;
+	struct ptp_ts *ts = param;
+	u32 timestamp;
+	u8 sec_chk;
+
+	db->cmd |= TSM_CMD_RESP;
+	db->cur_sec = htonl(ts->t.sec);
+	db->cur_nsec = htonl(ts->t.nsec);
+	timestamp = timestamp_val(ts->timestamp, &sec_chk);
+	db->timestamp = htonl(timestamp);
+	db->cur_nsec = db->timestamp;
+}  /* ptp_tsm_resp */
+
+static void ptp_tsm_get_time_resp(void *data, void *param)
+{
+	struct tsm_get_time *get = (struct tsm_get_time *) data;
+	struct ptp_utime *t = param;
+
+	get->cmd |= TSM_CMD_GET_TIME_RESP;
+	get->sec = htonl(t->sec);
+	get->nsec = htonl(t->nsec);
+}  /* ptp_tsm_get_time_resp */
+
+static void add_tx_delay(struct ptp_ts *ts, int delay, u32 cur_sec)
+{
+	update_ts(ts, cur_sec);
+
+	/*
+	 * Save timestamp without transmit latency for PTP stack that adjusts
+	 * transmit latency itself.
+	 */
+	ts->r = ts->t;
+	add_nsec(&ts->t, delay);
+	ts->timestamp = ts->t.nsec;
+}  /* add_tx_delay */
+
+static void save_tx_ts(struct ptp_info *ptp, struct ptp_tx_ts *tx,
+	struct ptp_hw_ts *htx, int delay, uint port)
+{
+	unsigned long diff = 0;
+	struct ksz_sw *sw = container_of(ptp, struct ksz_sw, ptp_hw);
+
+	add_tx_delay(&htx->ts, delay, ptp->cur_time.sec);
+	if (ptp->overrides & PTP_CHECK_PATH_DELAY) {
+		if (ptp->last_rx_ts.t.sec) {
+			struct ksz_ptp_time diff;
+
+			calc_udiff(&htx->ts.t, &ptp->last_rx_ts.t, &diff);
+			dbg_msg("pd: %d\n", diff.nsec);
+		} else
+			ptp->last_tx_ts = htx->ts;
+	}
+	tx->ts = htx->ts;
+	if (tx->data.len) {
+		struct tsm_db *db = (struct tsm_db *) tx->data.buf;
+		u8 msg = tx->data.buf[0] & 3;
+
+		tx->resp_time = jiffies;
+		if (tx->req_time)
+			diff = tx->resp_time - tx->req_time;
+		if (diff < 4 * ptp->delay_ticks) {
+			if (tx->missed) {
+				if (diff > 2 * ptp->delay_ticks)
+					dbg_msg("  caught: %d, %lu; %x=%04x\n",
+						port, diff, msg,
+						ntohs(db->seqid));
+				if (tx->dev) {
+					file_dev_setup_msg(tx->dev,
+						tx->data.buf, tx->data.len,
+						ptp_tsm_resp, &tx->ts);
+					tx->dev = NULL;
+				}
+
+				/* Invalidate the timestamp. */
+				tx->ts.timestamp = 0;
+				tx->req_time = 0;
+			}
+		} else {
+			dbg_msg("  new: %d, %lu; %x=%04x\n", port, diff,
+				msg, ntohs(db->seqid));
+		}
+		tx->missed = false;
+	}
+	if (tx->skb) {
+		int len;
+		u64 ns;
+		struct skb_shared_hwtstamps shhwtstamps;
+
+		if (ptp->tx_en & (1 << 8))
+			ns = (u64) tx->ts.t.sec * NANOSEC_IN_SEC +
+				tx->ts.t.nsec;
+		else
+			ns = (u64) tx->ts.r.sec * NANOSEC_IN_SEC +
+				tx->ts.r.nsec;
+		memset(&shhwtstamps, 0, sizeof(shhwtstamps));
+		shhwtstamps.hwtstamp = ns_to_ktime(ns);
+
+		/* Indicate which port message is sent out. */
+		tx->msg->hdr.reserved2 = get_log_port(sw, port);
+		len = (unsigned char *) tx->msg - tx->skb->data;
+		__skb_pull(tx->skb, len);
+		skb_tstamp_tx(tx->skb, &shhwtstamps);
+
+		dev_kfree_skb_irq(tx->skb);
+		tx->skb = NULL;
+	}
+	htx->sending = false;
+}  /* save_tx_ts */
+
+static int get_tx_time(struct ptp_info *ptp, u16 status)
+{
+	u32 reg = 0;
+	int port;
+	int delay;
+	u32 xts;
+	u32 *pts;
+	struct ptp_tx_ts *tx = NULL;
+	struct ptp_hw_ts *htx = NULL;
+	struct ksz_sw *sw = container_of(ptp, struct ksz_sw, ptp_hw);
+
+	while (status) {
+		/* Do port 1 first. */
+		if (status & (TS_PORT1_INT_XDELAY | TS_PORT1_INT_SYNC))
+			port = 0;
+		else if (status & (TS_PORT2_INT_XDELAY | TS_PORT2_INT_SYNC))
+			port = 1;
+		else
+			break;
+		xts = 0;
+		pts = NULL;
+		delay = ptp->tx_latency[port];
+		if (status & TS_PORT1_INT_XDELAY) {
+			reg = PTP_PORT1_XDELAY_TIMESTAMP_L;
+			pts = &ptp->xdelay_ts[port];
+			tx = &ptp->tx_dreq[port];
+			htx = &ptp->hw_dreq[port];
+			status &= ~TS_PORT1_INT_XDELAY;
+		} else if (status & TS_PORT1_INT_SYNC) {
+			reg = PTP_PORT1_SYNC_TIMESTAMP_L;
+			tx = &ptp->tx_sync[port];
+			htx = &ptp->hw_sync[port];
+			status &= ~TS_PORT1_INT_SYNC;
+		} else if (status & TS_PORT2_INT_XDELAY) {
+			reg = PTP_PORT1_XDELAY_TIMESTAMP_L;
+			pts = &ptp->xdelay_ts[port];
+			tx = &ptp->tx_dreq[port];
+			htx = &ptp->hw_dreq[port];
+			status &= ~TS_PORT2_INT_XDELAY;
+		} else if (status & TS_PORT2_INT_SYNC) {
+			reg = PTP_PORT1_SYNC_TIMESTAMP_L;
+			tx = &ptp->tx_sync[port];
+			htx = &ptp->hw_sync[port];
+			status &= ~TS_PORT2_INT_SYNC;
+		}
+
+		/* PDELAY_REQ and PDELAY_RESP share same interrupt. */
+		if (pts) {
+			reg += PTP_PORT_INTERVAL(port);
+			xts = sw->reg->r32(sw, reg);
+
+			if (xts != *pts) {
+				*pts = xts;
+				htx->ts.timestamp = xts;
+				save_tx_ts(ptp, tx, htx, delay, port);
+			}
+
+			reg = PTP_PORT1_PDRESP_TIMESTAMP_L;
+			pts = &ptp->pdresp_ts[port];
+			tx = &ptp->tx_resp[port];
+			htx = &ptp->hw_resp[port];
+
+			reg += PTP_PORT_INTERVAL(port);
+			xts = sw->reg->r32(sw, reg);
+			if (xts != *pts) {
+				delay = ptp->tx_latency[port];
+				*pts = xts;
+				htx->ts.timestamp = xts;
+				save_tx_ts(ptp, tx, htx, delay, port);
+			}
+		} else {
+			reg += PTP_PORT_INTERVAL(port);
+			htx->ts.timestamp = sw->reg->r32(sw, reg);
+			save_tx_ts(ptp, tx, htx, delay, port);
+		}
+	}
+	if (!htx)
+		return false;
+
+	return true;
+}  /* get_tx_time */
+
+static void ptp_update_sec(struct work_struct *work)
+{
+	struct delayed_work *dwork = to_delayed_work(work);
+	struct ptp_info *ptp =
+		container_of(dwork, struct ptp_info, update_sec);
+
+	if (ptp->update_sec_jiffies) {
+		ptp->cur_time.sec++;
+		schedule_delayed_work(&ptp->update_sec, 1000 * HZ / 1000);
+	}
+}  /* ptp_update_sec */
+
+static void generate_tx_event(struct ptp_info *ptp, int gpo)
+{
+	struct ptp_utime t;
+
+	ptp->first_sec = 0;
+	ptp->intr_sec = 0;
+	ptp->update_sec_jiffies = jiffies;
+	ptp->reg->get_time(ptp, &t);
+	t.sec += 1;
+	if (t.nsec >= (NANOSEC_IN_SEC - ptp->delay_ticks * 50000000))
+		t.sec += 1;
+	ptp->reg->pps_event(ptp, gpo, t.sec);
+#ifdef ENABLE_10_MHZ_CLK
+	ptp->reg->ptp_10MHz(ptp, ptp->mhz_tso, ptp->mhz_gpo, t.sec);
+#endif
+	schedule_delayed_work(&ptp->update_sec, (1000000 - t.nsec / 1000) * HZ
+		/ 1000000);
+}  /* generate_tx_event */
+
+static void ptp_check_pps(struct work_struct *work)
+{
+	struct delayed_work *dwork = to_delayed_work(work);
+	struct ptp_info *ptp =
+		container_of(dwork, struct ptp_info, check_pps);
+
+	if (ptp->update_sec_jiffies) {
+		ptp->ops->acquire(ptp);
+		generate_tx_event(ptp, ptp->pps_gpo);
+		ptp->ops->release(ptp);
+	}
+}  /* ptp_check_pps */
+
+static void prepare_gps(struct ptp_info *ptp)
+{
+	ptp->ops->acquire(ptp);
+	ptp->tsi_used |= (1 << ptp->gps_tsi);
+	ptp->events[ptp->gps_tsi].event = 1;
+	ptp->events[ptp->gps_tsi].timeout = 0;
+	ptp->reg->rx_event(ptp, ptp->gps_tsi, ptp->gps_gpi, TS_DETECT_RISE,
+		true);
+	ptp->ops->release(ptp);
+}  /* prepare_gps */
+
+static void prepare_pps(struct ptp_info *ptp)
+{
+	ptp->ops->acquire(ptp);
+	ptp->tso_used |= (1 << ptp->pps_tso);
+#ifdef ENABLE_10_MHZ_CLK
+	ptp->tso_used |= (1 << ptp->mhz_tso);
+	ptp->tso_used |= (1 << 1);
+#endif
+	generate_tx_event(ptp, ptp->pps_gpo);
+	ptp->tsi_used |= (1 << ptp->pps_tsi);
+	ptp->events[ptp->pps_tsi].event = 1;
+	ptp->reg->rx_event(ptp, ptp->pps_tsi, ptp->pps_gpo, TS_DETECT_RISE,
+		true);
+	ptp->ops->release(ptp);
+}  /* prepare_pps */
+
+/* -------------------------------------------------------------------------- */
+
+static void ptp_check(struct ptp_info *ptp)
+{
+	struct ptp_utime cur;
+	struct ptp_utime now;
+	struct ksz_sw *sw = container_of(ptp, struct ksz_sw, ptp_hw);
+
+	ptp->features |= PTP_ADJ_HACK;
+	ptp->ops->acquire(ptp);
+	sw->cached.ptp_clk_ctrl = sw->reg->r16(sw, PTP_CLK_CTRL);
+	ptp->reg->get_time(ptp, &cur);
+	ptp->reg->adjust_time(ptp, true, 10, 0, true);
+	ptp->reg->get_time(ptp, &now);
+	if (now.sec - cur.sec >= 10) {
+		ptp->features &= ~PTP_ADJ_HACK;
+		ptp->features |= PTP_ADJ_SEC;
+		ptp->features |= PTP_PDELAY_HACK;
+		ptp->reg->adjust_time(ptp, false, 10, 0, true);
+		ptp->version = 1;
+	}
+/*
+ * THa  2013/01/08
+ * The Rev. D chip has a problem of decrementing nanosecond that is bigger than
+ * the current nanosecond when continual clock adjustment is enabled.  The
+ * workaround is to use the PTP_ADJ_HACK code although the actual problem
+ * avoided is now different.
+ */
+	if (!(ptp->features & PTP_ADJ_HACK)) {
+		u16 data;
+
+		data = sw->cached.ptp_clk_ctrl;
+		sw->cached.ptp_clk_ctrl |= PTP_CLK_ADJ_ENABLE;
+		sw->reg->w16(sw, PTP_CLK_CTRL, sw->cached.ptp_clk_ctrl);
+		if (cur.sec < 1)
+			cur.sec = 1;
+		cur.nsec = 0;
+		ptp->reg->set_time(ptp, &cur);
+		ptp->reg->adjust_time(ptp, false, 0, 800000000, false);
+		ptp->reg->get_time(ptp, &now);
+		dbg_msg("%x:%u %x:%u\n", cur.sec, cur.nsec, now.sec, now.nsec);
+		if (abs(now.sec - cur.sec) > 2) {
+			ptp->reg->get_time(ptp, &now);
+			dbg_msg("! %x:%u\n", now.sec, now.nsec);
+			ptp->features |= PTP_ADJ_HACK;
+			sw->reg->w16(sw, PTP_CLK_CTRL, data);
+
+			sw->reg->w16(sw, PTP_CLK_CTRL,
+				data | PTP_CLK_ADJ_ENABLE);
+			ptp->reg->set_time(ptp, &cur);
+			ptp->reg->adjust_time(ptp, false, 0, 800000000, true);
+			ptp->reg->get_time(ptp, &now);
+			dbg_msg("ok %x:%u\n", now.sec, now.nsec);
+		}
+		sw->cached.ptp_clk_ctrl = data;
+		sw->reg->w16(sw, PTP_CLK_CTRL, data);
+	}
+	ptp->ops->release(ptp);
+}  /* ptp_check */
+
+static void ptp_start(struct ptp_info *ptp, int init)
+{
+	u16 ctrl;
+	struct timespec64 ts;
+	struct ptp_utime t;
+	struct ksz_sw *sw = container_of(ptp, struct ksz_sw, ptp_hw);
+
+	if (!ptp->features) {
+		ptp_check(ptp);
+		if (ptp->test_access_time)
+			ptp->test_access_time(ptp);
+	}
+	ptp_acquire(ptp);
+	ctrl = sw_r16(sw, PTP_MSG_CONF1);
+	if (ctrl == ptp->mode) {
+		ptp->cfg = sw_r16(sw, PTP_MSG_CONF2);
+		ptp->domain = sw_r16(sw, PTP_DOMAIN_VERSION) &
+			PTP_DOMAIN_MASK;
+		if (!init) {
+			ptp_release(ptp);
+			return;
+		}
+	} else if (!init)
+		ptp->mode = ctrl;
+	if (ptp->mode != ptp->def_mode) {
+		dbg_msg("mode changed: %04x %04x; %04x %04x\n",
+			ptp->mode, ptp->def_mode, ptp->cfg, ptp->def_cfg);
+		ptp->mode = ptp->def_mode;
+		ptp->cfg = ptp->def_cfg;
+		ptp->ptp_synt = false;
+	}
+	dbg_msg("ptp_start: %04x %04x\n",
+		ptp->mode, ptp->cfg);
+	sw_w16(sw, PTP_MSG_CONF1, ptp->mode);
+	sw_w16(sw, PTP_MSG_CONF2, ptp->cfg);
+	sw_w16(sw, TRIG_INT_ENABLE, ptp->trig_intr);
+	sw_w16(sw, TS_INT_ENABLE, ptp->ts_intr);
+
+	/* PTP stack is still running while device is reset. */
+	if (ptp->drift_set) {
+		ptp->drift = ptp->drift_set;
+		ptp->adjust = clk_adjust_val(ptp->drift, NANOSEC_IN_SEC);
+		set_ptp_adjust(ptp, ptp->adjust);
+		syntonize_clk(ptp);
+		ptp->ptp_synt = true;
+	}
+
+	ptp_release(ptp);
+
+	ts = ktime_to_timespec64(ktime_get_real());
+	t.sec = ts.tv_sec;
+	t.nsec = ts.tv_nsec;
+
+	/* Adjust for leap seconds. */
+	t.sec += ptp->utc_offset;
+	ptp_acquire(ptp);
+	set_ptp_time(ptp, &t);
+	ptp->cur_time = t;
+	ptp_release(ptp);
+
+	prepare_pps(ptp);
+	ptp->started = true;
+}  /* ptp_start */
+
+/* -------------------------------------------------------------------------- */
+
+static int ptp_poll_event(struct ptp_info *ptp, u8 tsi)
+{
+	int max_ts;
+	int num;
+	u16 status;
+	u16 tsi_bit = (1 << tsi);
+	u32 reg = TSn_EVENT_STATUS(tsi);
+	struct ptp_event *event = &ptp->events[tsi];
+	struct ksz_sw *sw = container_of(ptp, struct ksz_sw, ptp_hw);
+
+	status = sw->reg->r16(sw, reg);
+	num = (status & TS_NO_EVENT_DET_MASK) >> 1;
+	max_ts = (num <= event->max) ? num : event->max;
+	if (max_ts > event->num) {
+		ptp->ops->acquire(ptp);
+		status = sw->reg->r16(sw, TS_INT_STATUS);
+		if (status & tsi_bit)
+			sw->reg->w16(sw, TS_INT_STATUS, tsi_bit);
+		ptp->reg->read_event(ptp, tsi);
+		ptp->ts_status = 0;
+		ptp->ops->release(ptp);
+		return true;
+	}
+	return false;
+}  /* ptp_poll_event */
+
+static void convert_scaled_nsec(s64 scaled_nsec, int s, s64 *sec, int *nsec)
+{
+	int sign;
+	u64 quot;
+	u32 rem;
+
+	/* Convert to positive number first. */
+	if (scaled_nsec < 0) {
+		sign = -1;
+		scaled_nsec = -scaled_nsec;
+	} else
+		sign = 1;
+	scaled_nsec >>= s;
+	quot = div_u64_rem(scaled_nsec, NSEC_PER_SEC, &rem);
+	*sec = quot;
+	*nsec = (int) rem;
+
+	/* Positive number means clock is faster. */
+	if (1 == sign) {
+		*sec = -*sec;
+		*nsec = -*nsec;
+	}
+}  /* convert_scaled_nsec */
+
+static void adj_cur_time(struct ptp_info *ptp)
+{
+	if (ptp->adjust_offset || ptp->adjust_sec) {
+		synchronize_clk(ptp);
+		if (ptp->sec_changed)
+			generate_tx_event(ptp, ptp->pps_gpo);
+		else {
+			ptp->update_sec_jiffies = jiffies;
+			schedule_delayed_work(&ptp->check_pps,
+					      msecs_to_jiffies(1200));
+		}
+	}
+	if (ptp->sec_changed) {
+		struct timespec64 ts;
+		struct ptp_utime cur;
+
+		ptp->reg->get_time(ptp, &ptp->cur_time);
+		ts = ktime_to_timespec64(ktime_get_real());
+		cur.sec = ts.tv_sec;
+		cur.nsec = ts.tv_nsec;
+		calc_udiff(&ptp->cur_time, &cur, &ptp->time_diff);
+		ptp->sec_changed = 0;
+	}
+}  /* adj_cur_time */
+
+static void set_before_adj(struct ptp_info *ptp, struct ptp_utime *cur)
+{
+	ptp->adjust_offset += cur->nsec;
+	ptp->adjust_offset += ptp->set_delay;
+	ptp->adjust_offset += ptp->get_delay;
+	cur->nsec = 0;
+	if (ptp->adjust_offset > NANOSEC_IN_SEC) {
+		ptp->adjust_offset -= NANOSEC_IN_SEC;
+		cur->sec++;
+	}
+	ptp->reg->set_time(ptp, cur);
+}   /* set_before_adj */
+
+static void set_cur_time(struct ptp_info *ptp, struct ptp_ts *ts)
+{
+	struct ptp_utime cur;
+	s64 diff_sec;
+	int diff_nsec;
+
+	ptp->adjust_offset = ts->t.nsec - ts->timestamp;
+	ptp->reg->get_time(ptp, &cur);
+	diff_nsec = ts->t.nsec - ts->timestamp;
+	diff_sec = (s64) ts->t.sec - cur.sec;
+	if (ptp->features & PTP_ADJ_SEC) {
+		if (diff_sec) {
+			s64 nsec;
+
+			nsec = diff_sec;
+			nsec *= NANOSEC_IN_SEC;
+			nsec += diff_nsec;
+			convert_scaled_nsec(-nsec, 0, &ptp->adjust_sec,
+				&ptp->adjust_offset);
+		} else {
+			ptp->adjust_offset = diff_nsec;
+			ptp->adjust_sec = 0;
+		}
+		ptp->sec_changed = ptp->adjust_sec;
+	} else {
+		if (abs(diff_sec) <= 1) {
+			diff_nsec += diff_sec * NANOSEC_IN_SEC;
+			if (abs(diff_nsec) < NANOSEC_IN_SEC) {
+				ptp->adjust_offset = diff_nsec;
+				diff_sec = 0;
+			}
+		}
+		if (diff_sec) {
+			cur.sec = ts->t.sec;
+			set_before_adj(ptp, &cur);
+			ptp->sec_changed = diff_sec;
+		}
+		ptp->adjust_sec = 0;
+	}
+	adj_cur_time(ptp);
+}  /* set_cur_time */
+
+static void adj_clock(struct work_struct *work)
+{
+	struct ptp_info *ptp = container_of(work, struct ptp_info, adj_clk);
+	struct ptp_utime cur;
+
+	ptp->ops->acquire(ptp);
+
+	ptp->sec_changed = ptp->adjust_sec;
+	if (!(ptp->features & PTP_ADJ_SEC)) {
+
+		/* Need to adjust second. */
+		if (abs(ptp->adjust_sec) > 1) {
+			ptp->reg->get_time(ptp, &cur);
+			cur.sec += ptp->adjust_sec;
+			set_before_adj(ptp, &cur);
+		} else
+			ptp->adjust_offset += ptp->adjust_sec * NANOSEC_IN_SEC;
+		ptp->adjust_sec = 0;
+	}
+	adj_cur_time(ptp);
+	ptp->ops->release(ptp);
+}  /* adj_clock */
+
+static void execute(struct ptp_info *ptp, struct work_struct *work)
+{
+#ifdef PTP_SPI
+	queue_work(ptp->access, work);
+#else
+	work->func(work);
+#endif
+}  /* execute */
+
+static int ptp_stop(struct ptp_info *ptp, int hw_access)
+{
+	struct ksz_sw *sw = container_of(ptp, struct ksz_sw, ptp_hw);
+
+#ifdef PTP_SPI
+	flush_work(&ptp->adj_clk);
+	flush_workqueue(ptp->access);
+#endif
+	cancel_delayed_work_sync(&ptp->check_pps);
+	cancel_delayed_work_sync(&ptp->update_sec);
+	ptp->update_sec_jiffies = 0;
+	ptp->ptp_synt = false;
+	ptp->started = false;
+	ptp->first_drift = ptp->drift = 0;
+	if (hw_access) {
+		ptp->ops->acquire(ptp);
+		sw->reg->w16(sw, REG_RESET_CTRL, PTP_SOFTWARE_RESET);
+		udelay(1);
+		sw->reg->w16(sw, REG_RESET_CTRL, 0);
+		sw->cached.ptp_clk_ctrl =
+			sw->reg->r16(sw, PTP_CLK_CTRL);
+		sw->reg->w16(sw, TRIG_INT_ENABLE, 0);
+		sw->reg->w16(sw, TS_INT_ENABLE, 0);
+		ptp->ops->release(ptp);
+	}
+	return false;
+}  /* ptp_stop */
+
+static void init_tx_ts(struct ptp_tx_ts *ts)
+{
+	ts->ts.timestamp = 0;
+	ts->req_time = 0;
+	ts->resp_time = 0;
+	ts->missed = false;
+}  /* init_tx_ts */
+
+static struct file_dev_info *find_minor_dev(struct file_dev_info *info)
+{
+	struct ptp_info *ptp = info->dev;
+	struct file_dev_info *dev;
+	struct file_dev_info *prev;
+
+	dev = ptp->dev[info->minor ^ 1];
+	prev = ptp->dev[info->minor];
+	while (prev != info && prev && dev) {
+		prev = prev->next;
+		dev = dev->next;
+	}
+	if (prev != info)
+		dev = NULL;
+	return dev;
+}  /* find_minor_dev */
+
+static void ptp_init_state(struct ptp_info *ptp)
+{
+	int port;
+	u32 reg;
+	struct ptp_utime t;
+	struct ksz_sw *sw = container_of(ptp, struct ksz_sw, ptp_hw);
+
+	if (ptp->op_state) {
+		ptp->op_state++;
+		return;
+	}
+	mutex_lock(&ptp->lock);
+	mutex_unlock(&ptp->lock);
+
+	if (!ptp->started)
+		return;
+	ptp->reg->start(ptp, false);
+
+	/* Stop automatic drift adjustment if PTP operation is started. */
+	if (!ptp->first_drift)
+		ptp->first_drift = 1;
+
+	ptp->ops->acquire(ptp);
+	for (port = 0; port < MAX_PTP_PORT; port++) {
+		ptp->hw_sync[port].ts.timestamp = 0;
+		ptp->hw_sync[port].sending = false;
+		ptp->hw_dreq[port].ts.timestamp = 0;
+		ptp->hw_dreq[port].sending = false;
+		ptp->hw_resp[port].ts.timestamp = 0;
+		ptp->hw_resp[port].sending = false;
+		init_tx_ts(&ptp->tx_sync[port]);
+		init_tx_ts(&ptp->tx_dreq[port]);
+		init_tx_ts(&ptp->tx_resp[port]);
+		reg = PTP_PORT1_XDELAY_TIMESTAMP_L + PTP_PORT_INTERVAL(port);
+		ptp->xdelay_ts[port] = sw->reg->r32(sw, reg);
+		reg = PTP_PORT1_PDRESP_TIMESTAMP_L + PTP_PORT_INTERVAL(port);
+		ptp->pdresp_ts[port] = sw->reg->r32(sw, reg);
+		ptp->rx_latency[port] = get_ptp_ingress(ptp, port);
+		ptp->tx_latency[port] = get_ptp_egress(ptp, port);
+		ptp->asym_delay[port] = get_ptp_asym(ptp, port);
+		ptp->peer_delay[port] = get_ptp_link(ptp, port);
+		set_ptp_link(ptp, port, 0);
+		dbg_msg("%d = %d %d %d; %d\n", port,
+			ptp->rx_latency[port],
+			ptp->tx_latency[port],
+			ptp->asym_delay[port],
+			ptp->peer_delay[port]);
+	}
+
+	ptp->adjust_offset = 0;
+	ptp->offset_changed = 0;
+	memset(&ptp->last_rx_ts, 0, sizeof(struct ptp_ts));
+	memset(&ptp->last_tx_ts, 0, sizeof(struct ptp_ts));
+
+	if (!ptp->ptp_synt) {
+		syntonize_clk(ptp);
+		ptp->ptp_synt = true;
+	}
+	ptp->reg->get_time(ptp, &t);
+	ptp->cur_time = t;
+	ptp->op_state = 1;
+
+	/* Do not try to adjust drift automatically. */
+	if (!ptp->first_drift)
+		ptp->first_drift = 1;
+	ptp->ops->release(ptp);
+}  /* ptp_init_state */
+
+static void ptp_exit_state(struct ptp_info *ptp)
+{
+	if (ptp->op_state > 1) {
+		ptp->op_state--;
+		return;
+	}
+	if (ptp->mode & PTP_MASTER) {
+		u16 data;
+		struct ksz_sw *sw = container_of(ptp, struct ksz_sw, ptp_hw);
+
+		ptp->ops->acquire(ptp);
+		data = sw->reg->r16(sw, PTP_MSG_CONF1);
+		data &= ~PTP_MASTER;
+		sw->reg->w16(sw, PTP_MSG_CONF1, data);
+		ptp->ops->release(ptp);
+		ptp->mode &= ~PTP_MASTER;
+		ptp->def_mode &= ~PTP_MASTER;
+	}
+	ptp->adjust_offset = 0;
+	ptp->offset_changed = 0;
+	ptp->cap = 0;
+	ptp->op_mode = 0;
+	ptp->op_state = 0;
+	ptp->forward = FWD_MAIN_DEV;
+
+	/* Indicate drift is not being set by PTP stack. */
+	ptp->drift_set = 0;
+}  /* ptp_exit_state */
+
+static struct ptp_msg *check_ptp_msg(u8 *data, u16 **udp_check_ptr)
+{
+	struct ethhdr *eth = (struct ethhdr *) data;
+	struct vlan_ethhdr *vlan = (struct vlan_ethhdr *) data;
+	struct iphdr *iph = NULL;
+	struct ipv6hdr *ip6h = NULL;
+	struct udphdr *udp;
+	int ipv6;
+	struct ptp_msg *msg;
+
+	if (eth->h_proto == htons(0x88F7)) {
+		msg = (struct ptp_msg *)(eth + 1);
+		goto check_ptp_version;
+	}
+
+	if (eth->h_proto == htons(ETH_P_8021Q)) {
+		if (vlan->h_vlan_encapsulated_proto == htons(ETH_P_8021Q)) {
+			unsigned char *ptr = (unsigned char *) vlan;
+
+			ptr += VLAN_HLEN;
+			vlan = (struct vlan_ethhdr *) ptr;
+		}
+		if (vlan->h_vlan_encapsulated_proto == htons(0x88F7)) {
+			msg = (struct ptp_msg *)(vlan + 1);
+			goto check_ptp_version;
+		}
+		ipv6 = vlan->h_vlan_encapsulated_proto == htons(ETH_P_IPV6);
+		if (vlan->h_vlan_encapsulated_proto != htons(ETH_P_IP) &&
+				!ipv6)
+			return NULL;
+		ip6h = (struct ipv6hdr *)(vlan + 1);
+		iph = (struct iphdr *)(vlan + 1);
+	} else {
+		ipv6 = eth->h_proto == htons(ETH_P_IPV6);
+		if (eth->h_proto != htons(ETH_P_IP) && !ipv6)
+			return NULL;
+		ip6h = (struct ipv6hdr *)(eth + 1);
+		iph = (struct iphdr *)(eth + 1);
+	}
+
+	if (ipv6) {
+		if (ip6h->nexthdr != IPPROTO_UDP)
+			return NULL;
+
+		udp = (struct udphdr *)(ip6h + 1);
+		if (udp_check_ptr)
+			*udp_check_ptr = &udp->check;
+	} else {
+		if (iph->protocol != IPPROTO_UDP)
+			return NULL;
+		if (ntohs(iph->frag_off) & IP_OFFSET)
+			return NULL;
+
+		udp = (struct udphdr *)(iph + 1);
+		if (udp_check_ptr)
+			*udp_check_ptr = &udp->check;
+	}
+
+	if (udp->dest != htons(319) && udp->dest != htons(320))
+		return NULL;
+
+	msg = (struct ptp_msg *)(udp + 1);
+
+check_ptp_version:
+	if (msg->hdr.versionPTP >= 2)
+		return msg;
+	return NULL;
+}  /* check_ptp_msg */
+
+static struct ptp_msg *check_ptp_event(u8 *data)
+{
+	struct ptp_msg *msg;
+
+	msg = check_ptp_msg(data, NULL);
+	if (!msg)
+		return NULL;
+	switch (msg->hdr.messageType) {
+	case SYNC_MSG:
+		break;
+	case DELAY_REQ_MSG:
+		break;
+	case PDELAY_REQ_MSG:
+		break;
+	case PDELAY_RESP_MSG:
+		break;
+	default:
+		msg = NULL;
+		break;
+	}
+	return msg;
+}
+
+static int update_ptp_msg(u8 *data, u32 port, u32 overrides)
+{
+	struct ethhdr *eth = (struct ethhdr *) data;
+	struct vlan_ethhdr *vlan = (struct vlan_ethhdr *) data;
+	struct iphdr *iph;
+	struct ipv6hdr *ip6h;
+	struct ptp_msg *msg;
+	u32 dst;
+	u8 src;
+	struct udphdr *udp = NULL;
+	int ipv6 = 0;
+	int udp_check = 0;
+
+	do {
+		if (eth->h_proto == htons(0x88F7)) {
+			msg = (struct ptp_msg *)(eth + 1);
+			break;
+		}
+
+		if (eth->h_proto == htons(ETH_P_8021Q)) {
+			if (vlan->h_vlan_encapsulated_proto == htons(0x88F7)) {
+				msg = (struct ptp_msg *)(vlan + 1);
+				break;
+			}
+			ipv6 = vlan->h_vlan_encapsulated_proto ==
+				htons(ETH_P_IPV6);
+			if (vlan->h_vlan_encapsulated_proto != htons(ETH_P_IP)
+					&& !ipv6)
+				return false;
+			ip6h = (struct ipv6hdr *)(vlan + 1);
+			iph = (struct iphdr *)(vlan + 1);
+		} else {
+			ipv6 = eth->h_proto == htons(ETH_P_IPV6);
+			if (eth->h_proto != htons(ETH_P_IP) && !ipv6)
+				return false;
+			ip6h = (struct ipv6hdr *)(eth + 1);
+			iph = (struct iphdr *)(eth + 1);
+		}
+
+		if (ipv6) {
+			if (ip6h->nexthdr != IPPROTO_UDP)
+				return false;
+
+			udp = (struct udphdr *)(ip6h + 1);
+		} else {
+			if (iph->protocol != IPPROTO_UDP)
+				return false;
+
+			udp = (struct udphdr *)(iph + 1);
+		}
+
+		if (udp->dest != htons(319) && udp->dest != htons(320))
+			return false;
+		msg = (struct ptp_msg *)(udp + 1);
+	} while (0);
+	if (msg->hdr.versionPTP < 2)
+		return false;
+	if ((overrides & PTP_VERIFY_TIMESTAMP) &&
+			PDELAY_RESP_MSG == msg->hdr.messageType &&
+			msg->hdr.flagField.flag.twoStepFlag) {
+		struct ptp_utime rx;
+		u32 timestamp;
+		int i;
+		u16 *data = (u16 *) &msg->hdr.reserved3;
+
+		rx.nsec = ntohl(msg->data.pdelay_resp.requestReceiptTimestamp.
+			nsec);
+		rx.sec = ntohl(msg->data.pdelay_resp.requestReceiptTimestamp.
+			sec.lo);
+		timestamp = (rx.sec << 30) | rx.nsec;
+		for (i = 0; i < 2; i++)
+			udp_check += ntohs(data[i]);
+		msg->hdr.reserved3 = htonl(timestamp);
+		for (i = 0; i < 2; i++)
+			udp_check -= ntohs(data[i]);
+	}
+	dst = port >> 16;
+	port &= 0xffff;
+	src = msg->hdr.reserved2;
+
+	/* No destination port specified, use whatever port assigned. */
+	if (!src)
+		src = port;
+
+	/* No change if all ports is specified, or all ports is assigned. */
+	else if (!(overrides & PTP_KEEP_DST_PORT) &&
+			src != 3 && port != 0 && port != 3)
+		src = port;
+	port = src;
+	if (overrides & PTP_UPDATE_DST_PORT) {
+		switch (msg->hdr.messageType) {
+		case PDELAY_REQ_MSG:
+		case PDELAY_RESP_MSG:
+		case PDELAY_RESP_FOLLOW_UP_MSG:
+
+			/* Ports are always open for Pdelay messages. */
+			dst = 3;
+
+			/* Force to all ports to send message. */
+			if (!src)
+				src = 3;
+			port = src & dst;
+			break;
+		default:
+			port = src & dst;
+
+			/*
+			 * Zero port still can send message through an open
+			 * port, which is not what the application wants.
+			 * Need to have the network driver not to send the
+			 * packet to workaround the problem.
+			 */
+			if (!port && src && src != 3)
+				return true;
+			break;
+		}
+	}
+	if (msg->hdr.reserved2 != port) {
+		u8 data = msg->hdr.reserved2;
+
+		udp_check += data;
+		msg->hdr.reserved2 = (u8) port;
+		udp_check -= port;
+	}
+	if ((overrides & PTP_ZERO_RESERVED_FIELD) && msg->hdr.reserved3 &&
+			(PDELAY_RESP_MSG != msg->hdr.messageType ||
+			msg->hdr.flagField.flag.twoStepFlag)) {
+		int i;
+		u16 *data = (u16 *) &msg->hdr.reserved3;
+
+		for (i = 0; i < 2; i++)
+			udp_check += ntohs(data[i]);
+		msg->hdr.reserved3 = 0;
+	}
+	if (udp_check) {
+		u16 check;
+
+		/* Zero checksum in IPv4. */
+		if (udp && !ipv6 && !udp->check)
+			udp = NULL;
+		if (udp) {
+			check = ntohs(udp->check);
+			udp_check += check;
+			udp_check = (udp_check >> 16) + (udp_check & 0xffff);
+			udp_check += (udp_check >> 16);
+			check = (u16) udp_check;
+			if (!check)
+				check = -1;
+			udp->check = htons(check);
+		}
+	}
+	return false;
+}  /* update_ptp_msg */
+
+static void get_rx_tstamp(void *ptr, struct sk_buff *skb)
+{
+	struct ptp_info *ptp = ptr;
+	struct ptp_msg *msg;
+	u8 port;
+	int delay;
+	struct ptp_ts ts;
+	u64 ns;
+	struct skb_shared_hwtstamps *shhwtstamps = skb_hwtstamps(skb);
+
+	if (!shhwtstamps)
+		return;
+	if (ptp->rx_msg_parsed)
+		msg = ptp->rx_msg;
+	else {
+		msg = check_ptp_event(skb->data);
+		ptp->rx_msg_parsed = true;
+		ptp->rx_msg = msg;
+	}
+	if (!msg)
+		return;
+
+	ts.timestamp = ntohl(msg->hdr.reserved3);
+	update_ts(&ts, ptp->cur_time.sec);
+	if (ptp->rx_en & (1 << 8)) {
+		port = msg->hdr.reserved2;
+		if (port)
+			port--;
+		delay = ptp->rx_latency[port];
+		sub_nsec(&ts.t, delay);
+	}
+
+	ns = (u64) ts.t.sec * NANOSEC_IN_SEC + ts.t.nsec;
+	memset(shhwtstamps, 0, sizeof(*shhwtstamps));
+	shhwtstamps->hwtstamp = ns_to_ktime(ns);
+}  /* get_rx_tstamp */
+
+static void get_tx_tstamp(struct ptp_info *ptp, struct sk_buff *skb)
+{
+	int cnt;
+	int p;
+	struct ptp_msg *msg;
+	u8 port;
+	struct ptp_tx_ts *tx;
+	struct sk_buff *orig_skb = skb;
+
+	msg = check_ptp_event(skb->data);
+	if (!msg)
+		return;
+
+	/* Only accept socket buffer from application. */
+	if (!orig_skb->sk)
+		return;
+	port = msg->hdr.reserved2;
+	if (!port)
+		port = (1 << MAX_PTP_PORT) - 1;
+	cnt = 0;
+	if (SYNC_MSG == msg->hdr.messageType)
+		tx = ptp->tx_sync;
+	else if (PDELAY_RESP_MSG == msg->hdr.messageType)
+		tx = ptp->tx_resp;
+	else
+		tx = ptp->tx_dreq;
+	for (p = 0; p < MAX_PTP_PORT; p++, tx++) {
+		if (!(port & (1 << p)))
+			continue;
+		if (tx->skb) {
+			dev_kfree_skb_irq(tx->skb);
+			tx->skb = NULL;
+		}
+		if (!cnt) {
+			skb = skb_clone_sk(orig_skb);
+			if (!skb)
+				break;
+		}
+
+		/* Need to create socket buffer for more than 1 port. */
+		if (cnt++) {
+			skb = skb_copy(orig_skb, GFP_ATOMIC);
+			if (!skb)
+				break;
+			skb->sk = orig_skb->sk;
+			msg = check_ptp_event(skb->data);
+		}
+		tx->skb = skb;
+		tx->msg = msg;
+		skb_shinfo(skb)->tx_flags |= SKBTX_IN_PROGRESS;
+	}
+}  /* get_tx_tstamp */
+
+static int ptp_hwtstamp_ioctl(struct ptp_info *ptp, struct ifreq *ifr,
+			      u16 ports)
+{
+	struct hwtstamp_config config;
+
+	if (copy_from_user(&config, ifr->ifr_data, sizeof(config)))
+		return -EFAULT;
+
+	/* reserved for future extensions */
+	if (config.flags)
+		return -EINVAL;
+
+	switch (config.tx_type) {
+	case HWTSTAMP_TX_OFF:
+		ptp->tx_en &= ~1;
+		break;
+	case HWTSTAMP_TX_ONESTEP_SYNC:
+	case HWTSTAMP_TX_ON:
+		ptp->tx_en |= 1;
+		break;
+	default:
+		return -ERANGE;
+	}
+
+	switch (config.rx_filter) {
+	case HWTSTAMP_FILTER_NONE:
+		if ((ptp->rx_en & 1) && (ptp->rx_en & (1 << 8))) {
+			ptp->tx_en &= ~(1 << 8);
+			ptp->rx_en &= ~(1 << 8);
+		}
+		ptp->rx_en &= ~1;
+		break;
+	case HWTSTAMP_FILTER_ALL:
+#if 0
+		ptp->rx_en |= 1;
+		break;
+#endif
+	default:
+		if (!(ptp->rx_en & 1) && (ptp->tx_en & 1)) {
+			ptp->tx_en |= (1 << 8);
+			ptp->rx_en |= (1 << 8);
+		}
+		ptp->rx_en |= 1;
+		break;
+	}
+
+	return copy_to_user(ifr->ifr_data, &config, sizeof(config)) ?
+		-EFAULT : 0;
+}
+
+static int ptp_drop_pkt(struct ptp_info *ptp, struct sk_buff *skb, u32 vlan_id,
+	int *tag, int *ptp_tag, int *forward)
+{
+	ptp->rx_msg_parsed = false;
+	ptp->rx_msg = NULL;
+	do {
+		u16 vid;
+		u16 *protocol;
+
+		if (!(ptp->vid))
+			break;
+		if (vlan_get_tag(skb, &vid))
+			break;
+		vid &= VLAN_VID_MASK;
+		protocol = (u16 *) &skb->data[VLAN_ETH_HLEN - 2];
+
+		if (!vid)
+			break;
+		if (*protocol == ntohs(0x88F7) && vid != ptp->vid)
+			return true;
+	} while (0);
+	if ((ptp->overrides & PTP_PORT_FORWARD) || (ptp->rx_en & 1)) {
+		struct ptp_msg *msg = check_ptp_msg(skb->data, NULL);
+
+		ptp->rx_msg_parsed = true;
+		ptp->rx_msg = msg;
+		if (msg) {
+			/* Indicate this is a PTP message. */
+			*ptp_tag = msg->hdr.reserved2;
+
+			if (ptp->overrides & PTP_CHECK_PATH_DELAY) {
+				struct ptp_ts ts;
+
+				ts.timestamp = ntohl(msg->hdr.reserved3);
+				update_ts(&ts, ptp->cur_time.sec);
+				if (ptp->last_tx_ts.t.sec) {
+					struct ksz_ptp_time diff;
+
+					calc_udiff(&ptp->last_tx_ts.t, &ts.t,
+						&diff);
+					dbg_msg("pd: %d\n", diff.nsec);
+				} else
+					ptp->last_rx_ts = ts;
+			}
+
+			/*
+			 * Tag is already updated to reflect VLAN forwarding if
+			 * tail tagging is used.  This is a case of it not
+			 * being used.
+			 */
+			if (!*tag) {
+				*tag = *ptp_tag;
+				if (!(vlan_id & (1 << *tag)))
+					*tag = 0;
+			}
+			if (ptp->rx_en & 1)
+				ptp->ops->get_rx_tstamp(ptp, skb);
+			*forward = ptp->forward;
+		}
+	}
+	if (!ptp->op_state)
+		*ptp_tag = 0;
+	return false;
+}  /* ptp_drop_pkt */
+
+static void proc_ptp_get_cfg(struct ptp_info *ptp, u8 *data)
+{
+	struct ptp_cfg_options *cmd = (struct ptp_cfg_options *) data;
+	struct ksz_sw *sw = container_of(ptp, struct ksz_sw, ptp_hw);
+
+	ptp->ops->acquire(ptp);
+	ptp->mode = sw->reg->r16(sw, PTP_MSG_CONF1);
+	ptp->cfg = sw->reg->r16(sw, PTP_MSG_CONF2);
+	ptp->domain = sw->reg->r16(sw, PTP_DOMAIN_VERSION) &
+		PTP_DOMAIN_MASK;
+	ptp->ops->release(ptp);
+	if (ptp->mode != ptp->def_mode && ptp->started) {
+		dbg_msg("mode mismatched: %04x %04x; %04x %04x\n",
+			ptp->mode, ptp->def_mode, ptp->cfg, ptp->def_cfg);
+		ptp->mode = ptp->def_mode;
+		ptp->cfg = ptp->def_cfg;
+		ptp->reg->start(ptp, false);
+	}
+	cmd->two_step = (ptp->mode & PTP_1STEP) ? 0 : 1;
+	cmd->master = (ptp->mode & PTP_MASTER) ? 1 : 0;
+	cmd->p2p = (ptp->mode & PTP_TC_P2P) ? 1 : 0;
+	cmd->as = (ptp->mode & PTP_FORWARD_TO_PORT3) ? 1 : 0;
+	cmd->unicast = (ptp->cfg & PTP_UNICAST_ENABLE) ? 1 : 0;
+	cmd->alternate = (ptp->cfg & PTP_ALTERNATE_MASTER) ? 1 : 0;
+	cmd->domain_check = (ptp->cfg & PTP_DOMAIN_CHECK) ? 1 : 0;
+	cmd->udp_csum = (ptp->cfg & PTP_UDP_CHECKSUM) ? 1 : 0;
+	cmd->delay_assoc = (ptp->cfg & PTP_DELAY_CHECK) ? 1 : 0;
+	cmd->pdelay_assoc = (ptp->cfg & PTP_PDELAY_CHECK) ? 1 : 0;
+	cmd->sync_assoc = (ptp->cfg & PTP_SYNC_CHECK) ? 1 : 0;
+	cmd->drop_sync = (ptp->cfg & PTP_DROP_SYNC_DELAY_REQ) ? 1 : 0;
+	cmd->priority = (ptp->cfg & PTP_ALL_HIGH_PRIORITY) ? 1 : 0;
+	cmd->reserved = ptp->started;
+	cmd->domain = ptp->domain;
+	cmd->access_delay = ptp->get_delay;
+}  /* proc_ptp_get_cfg */
+
+static int proc_ptp_set_cfg(struct ptp_info *ptp, u8 *data)
+{
+	struct ptp_cfg_options *cmd = (struct ptp_cfg_options *) data;
+	u16 cfg;
+	u16 mode;
+	u8 domain;
+	struct ksz_sw *sw = container_of(ptp, struct ksz_sw, ptp_hw);
+
+	mode = ptp->mode;
+	cfg = ptp->cfg;
+	domain = ptp->domain;
+	if (cmd->domain_set) {
+		domain = cmd->domain;
+	} else {
+		if (cmd->two_step_set) {
+			if (cmd->two_step)
+				ptp->mode &= ~PTP_1STEP;
+			else
+				ptp->mode |= PTP_1STEP;
+		}
+		if (cmd->master_set) {
+			if (cmd->master)
+				ptp->mode |= PTP_MASTER;
+			else
+				ptp->mode &= ~PTP_MASTER;
+		}
+		if (cmd->p2p_set) {
+			if (cmd->p2p)
+				ptp->mode |= PTP_TC_P2P;
+			else
+				ptp->mode &= ~PTP_TC_P2P;
+		}
+		if (cmd->as_set) {
+			if (cmd->as)
+				ptp->mode |= PTP_FORWARD_TO_PORT3;
+			else
+				ptp->mode &= ~PTP_FORWARD_TO_PORT3;
+		}
+		if (cmd->unicast_set) {
+			if (cmd->unicast)
+				ptp->cfg |= PTP_UNICAST_ENABLE;
+			else
+				ptp->cfg &= ~PTP_UNICAST_ENABLE;
+		}
+		if (cmd->alternate_set) {
+			if (cmd->alternate)
+				ptp->cfg |= PTP_ALTERNATE_MASTER;
+			else
+				ptp->cfg &= ~PTP_ALTERNATE_MASTER;
+		}
+		if (cmd->domain_check_set) {
+			if (cmd->domain_check)
+				ptp->cfg |= PTP_DOMAIN_CHECK;
+			else
+				ptp->cfg &= ~PTP_DOMAIN_CHECK;
+		}
+		if (cmd->udp_csum_set) {
+			if (cmd->udp_csum)
+				ptp->cfg |= PTP_UDP_CHECKSUM;
+			else
+				ptp->cfg &= ~PTP_UDP_CHECKSUM;
+		}
+		if (cmd->delay_assoc_set) {
+			if (cmd->delay_assoc)
+				ptp->cfg |= PTP_DELAY_CHECK;
+			else
+				ptp->cfg &= ~PTP_DELAY_CHECK;
+		}
+		if (cmd->pdelay_assoc_set) {
+			if (cmd->pdelay_assoc)
+				ptp->cfg |= PTP_PDELAY_CHECK;
+			else
+				ptp->cfg &= ~PTP_PDELAY_CHECK;
+		}
+		if (cmd->sync_assoc_set) {
+			if (cmd->sync_assoc)
+				ptp->cfg |= PTP_SYNC_CHECK;
+			else
+				ptp->cfg &= ~PTP_SYNC_CHECK;
+		}
+		if (cmd->drop_sync_set) {
+			if (cmd->drop_sync)
+				ptp->cfg |= PTP_DROP_SYNC_DELAY_REQ;
+			else
+				ptp->cfg &= ~PTP_DROP_SYNC_DELAY_REQ;
+		}
+		if (cmd->priority_set) {
+			if (cmd->priority)
+				ptp->cfg |= PTP_ALL_HIGH_PRIORITY;
+			else
+				ptp->cfg &= ~PTP_ALL_HIGH_PRIORITY;
+		}
+	}
+	ptp->ops->acquire(ptp);
+	if (mode != ptp->mode) {
+		u16 ts_intr = ptp->ts_intr;
+
+		if (ptp->mode & PTP_1STEP)
+			ptp->ts_intr &= ~(TS_PORT2_INT_SYNC |
+				TS_PORT1_INT_SYNC);
+		else
+			ptp->ts_intr |= (TS_PORT2_INT_SYNC |
+				TS_PORT1_INT_SYNC);
+		dbg_msg("mode: %x %x\n", mode, ptp->mode);
+		mode = ptp->mode;
+		if (ptp->overrides & PTP_VERIFY_TIMESTAMP)
+			mode |= PTP_1STEP;
+		sw->reg->w16(sw, PTP_MSG_CONF1, mode);
+		ptp->def_mode = ptp->mode;
+		if (ts_intr != ptp->ts_intr)
+			sw->reg->w16(sw, TS_INT_ENABLE, ptp->ts_intr);
+	}
+	if (cfg != ptp->cfg) {
+		dbg_msg("cfg: %x %x\n", cfg, ptp->cfg);
+		sw->reg->w16(sw, PTP_MSG_CONF2, ptp->cfg);
+	}
+	if (domain != ptp->domain) {
+		ptp->domain = domain;
+		set_ptp_domain(ptp, ptp->domain);
+	}
+	ptp->ops->release(ptp);
+	return 0;
+}  /* proc_ptp_set_cfg */
+
+static void cancel_rx_unit(struct ptp_info *ptp, int tsi)
+{
+	int first;
+	int last;
+	int tsi_bit;
+	struct ptp_event *events;
+
+	ptp->ops->acquire(ptp);
+	first = tsi;
+	events = &ptp->events[tsi];
+	if (events->last) {
+		first = events->first;
+		last = events->last;
+	} else
+		last = first + 1;
+	tsi = first;
+	ptp->events[tsi].timeout = 0;
+	do {
+		if (tsi >= MAX_TIMESTAMP_UNIT)
+			tsi = 0;
+		events = &ptp->events[tsi];
+		events->first = 0;
+		events->last = 0;
+		tsi_bit = 1 << tsi;
+		if (ptp->tsi_used & tsi_bit) {
+			if (events->num < events->max) {
+				ptp->reg->read_event(ptp, tsi);
+				ptp->ts_status = 0;
+			}
+			ptp->reg->rx_off(ptp, tsi);
+			ptp->tsi_intr &= ~tsi_bit;
+			ptp->tsi_used &= ~tsi_bit;
+			ptp->tsi_dev[tsi] = NULL;
+		}
+		++tsi;
+	} while (tsi != last);
+	ptp->ops->release(ptp);
+}  /* cancel_rx_unit */
+
+static int check_expired_rx_unit(struct ptp_info *ptp, int tsi)
+{
+	int first;
+	int last;
+	u32 expired;
+	struct ptp_event *events;
+	struct ksz_ptp_time diff;
+	struct ptp_utime t;
+
+	events = &ptp->events[tsi];
+	first = tsi;
+	if (events->last) {
+		first = events->first;
+		last = events->last;
+	} else
+		last = first + 1;
+	events = &ptp->events[first];
+	if (events->num && events->timeout) {
+		ptp->ops->acquire(ptp);
+		ptp->reg->get_time(ptp, &t);
+		ptp->ops->release(ptp);
+		calc_udiff(events->t, &t, &diff);
+		if (diff.sec >= 0 && diff.nsec >= 0) {
+			expired = diff.sec * 1000 + diff.nsec / 1000000;
+			expired = expired * HZ / 1000;
+			if (expired > events->timeout) {
+				cancel_rx_unit(ptp, first);
+				return 1;
+			}
+		}
+	}
+	return 0;
+}  /* check_expired_rx_unit */
+
+static int proc_dev_rx_event(struct file_dev_info *info, u8 *data)
+{
+	struct ptp_info *ptp = info->dev;
+	struct ptp_tsi_options *cmd = (struct ptp_tsi_options *) data;
+	u8 event;
+	int first;
+	int i;
+	int intr;
+	int tsi;
+	int avail;
+	int total;
+	int last;
+	int tsi_bit;
+	struct ptp_event *events;
+
+	tsi = cmd->tsi;
+	total = cmd->total;
+	if (!total)
+		total = 1;
+	first = tsi;
+
+	/* Cancel operation. */
+	if ((cmd->flags & PTP_CMD_CANCEL_OPER)) {
+		if (tsi >= MAX_TIMESTAMP_UNIT)
+			return DEV_IOC_UNIT_UNAVAILABLE;
+		cancel_rx_unit(ptp, tsi);
+		goto proc_ptp_rx_cascade_event_done;
+	}
+	if (tsi >= MAX_TIMESTAMP_UNIT) {
+		first = 0;
+		do {
+			for (tsi = first; tsi < MAX_TIMESTAMP_UNIT; tsi++)
+				if (!(ptp->tsi_used & (1 << tsi)) &&
+						!ptp->events[tsi].last)
+					break;
+			if (tsi >= MAX_TIMESTAMP_UNIT)
+				return DEV_IOC_UNIT_UNAVAILABLE;
+			first = tsi;
+			avail = 1;
+			for (i = 1; i < total; i++)
+				if (!(ptp->tsi_used & (1 << tsi)) &&
+						!ptp->events[tsi].last) {
+					++avail;
+					++tsi;
+					if (tsi >= MAX_TIMESTAMP_UNIT)
+						tsi = 0;
+				} else {
+					++first;
+					break;
+				}
+		} while (avail < total);
+	} else {
+		for (i = 0; i < total; i++) {
+			if (ptp->tsi_used & (1 << tsi) ||
+					ptp->events[tsi].last)
+				if (!check_expired_rx_unit(ptp, tsi))
+					return DEV_IOC_UNIT_USED;
+			++tsi;
+			if (tsi >= MAX_TIMESTAMP_UNIT)
+				tsi = 0;
+		}
+	}
+	if (cmd->gpi >= MAX_GPIO)
+		return -EINVAL;
+	if (0 == cmd->event)
+		event = TS_DETECT_FALL;
+	else if (1 == cmd->event)
+		event = TS_DETECT_RISE;
+	else {
+		event = TS_DETECT_RISE | TS_DETECT_FALL;
+		cmd->event = 2;
+	}
+	tsi = first;
+	last = first + total;
+	if (last > MAX_TIMESTAMP_UNIT)
+		last -= MAX_TIMESTAMP_UNIT;
+	intr = cmd->flags & PTP_CMD_INTR_OPER;
+	ptp->ops->acquire(ptp);
+	for (i = 0; i < total; i++) {
+		tsi_bit = 1 << tsi;
+		ptp->tsi_used |= tsi_bit;
+		if (intr & !(cmd->flags & PTP_CMD_SILENT_OPER)) {
+			ptp->tsi_intr |= tsi_bit;
+			ptp->tsi_dev[tsi] = info;
+		}
+		events = &ptp->events[tsi];
+		events->num = 0;
+		events->event = cmd->event;
+		events->edge = 0;
+		events->expired = 0;
+		if (total > 1) {
+			events->first = first;
+			events->last = last;
+		}
+		++tsi;
+		if (tsi >= MAX_TIMESTAMP_UNIT)
+			tsi = 0;
+	}
+	tsi = first;
+	ptp->events[tsi].timeout = cmd->timeout * HZ / 1000;
+
+	/* Zero timeout means repeatable. */
+	if (!ptp->events[tsi].timeout && cmd->timeout)
+		ptp->events[tsi].timeout = 1;
+	if (total > 1)
+		ptp->reg->rx_cascade_event(ptp, tsi, total, cmd->gpi, event,
+			intr);
+	else
+		ptp->reg->rx_event(ptp, tsi, cmd->gpi, event, intr);
+	ptp->ops->release(ptp);
+
+proc_ptp_rx_cascade_event_done:
+	*data = tsi;
+	return 0;
+}  /* proc_dev_rx_event */
+
+static int find_avail_tx_unit(struct ptp_info *ptp, int total, int *unit)
+{
+	int avail;
+	int first;
+	int i;
+	int tso;
+
+	first = 0;
+	do {
+		for (tso = first; tso < MAX_TRIG_UNIT; tso++)
+			if (!(ptp->tso_used & (1 << tso)))
+				break;
+		if (tso >= MAX_TRIG_UNIT)
+			return DEV_IOC_UNIT_UNAVAILABLE;
+		first = tso++;
+		avail = 1;
+		for (i = 1; i < total; i++) {
+			if (tso >= MAX_TRIG_UNIT)
+				return DEV_IOC_UNIT_UNAVAILABLE;
+			if (!(ptp->tso_used & (1 << tso))) {
+				++avail;
+				++tso;
+			} else {
+				++first;
+				break;
+			}
+		}
+	} while (avail < total);
+	*unit = first;
+	return 0;
+}  /* find_avail_tx_unit */
+
+static int proc_dev_tx_event(struct file_dev_info *info, u8 *data)
+{
+	struct ptp_info *ptp = info->dev;
+	struct ptp_tso_options *cmd = (struct ptp_tso_options *) data;
+	int gpo;
+	int intr;
+	int tso;
+	int tso_bit;
+	struct ptp_utime t;
+	u16 active;
+	u16 status;
+	int err = 0;
+	struct ksz_sw *sw = container_of(ptp, struct ksz_sw, ptp_hw);
+
+	gpo = cmd->gpo;
+	if (gpo >= MAX_GPIO)
+		return -EINVAL;
+	if (cmd->event > TRIG_REG_OUTPUT)
+		return -EINVAL;
+	tso = cmd->tso;
+	tso_bit = 1 << tso;
+
+	/* Cancel operation. */
+	if ((cmd->flags & PTP_CMD_CANCEL_OPER)) {
+		if (tso >= MAX_TRIG_UNIT)
+			return DEV_IOC_UNIT_UNAVAILABLE;
+		ptp->ops->acquire(ptp);
+
+		/* Reset the tso. */
+		ptp->cascade_tx |= tso_bit;
+		ptp_tso_off(ptp, tso, tso_bit);
+		ptp->ops->release(ptp);
+		goto proc_dev_tx_event_done;
+	}
+	if (ptp->cascade && (tso < ptp->cascade_gpo[gpo].first ||
+			tso >= ptp->cascade_gpo[gpo].first +
+			ptp->cascade_gpo[gpo].total))
+		return DEV_IOC_UNIT_UNAVAILABLE;
+
+	/* Find available unit for use. */
+	if (tso >= MAX_TRIG_UNIT) {
+		int rc = find_avail_tx_unit(ptp, 1, &tso);
+
+		if (rc)
+			return rc;
+	} else if (!ptp->cascade && (ptp->tso_used & tso_bit)) {
+
+		/* See whether previous operation is completed. */
+		ptp->ops->acquire(ptp);
+		active = sw->reg->r16(sw, TRIG_ACTIVE);
+		if (active & tso_bit) {
+			status = sw->reg->r16(sw, TRIG_ERROR);
+			if (!(status & tso_bit)) {
+				ptp->ops->release(ptp);
+				return DEV_IOC_UNIT_USED;
+			}
+			dbg_msg("trig err: %d\n", tso);
+		}
+		if (!(active & tso_bit)) {
+			status = sw->reg->r16(sw, TRIG_DONE);
+			if (!(status & tso_bit)) {
+				/* Reset the unit. */
+				ptp->cascade_tx |= tso_bit;
+				dbg_msg(" !? trig done: %d\n", tso);
+			}
+		}
+		ptp_tso_off(ptp, tso, tso_bit);
+		ptp->ops->release(ptp);
+	}
+	ptp->ops->acquire(ptp);
+	if (!ptp->cascade && (cmd->flags & PTP_CMD_REL_TIME) &&
+			cmd->sec < 100) {
+		ptp->reg->get_time(ptp, &t);
+		if (0 == cmd->sec) {
+			cmd->nsec += t.nsec;
+			cmd->nsec += 500;
+			cmd->nsec /= 1000;
+			cmd->nsec *= 1000;
+			if (cmd->nsec >= NANOSEC_IN_SEC) {
+				cmd->nsec -= NANOSEC_IN_SEC;
+				cmd->sec++;
+			}
+		}
+		cmd->sec += t.sec;
+	}
+	intr = cmd->flags & PTP_CMD_INTR_OPER;
+	if (intr & !(cmd->flags & PTP_CMD_SILENT_OPER)) {
+		ptp->tso_intr |= tso_bit;
+		ptp->tso_dev[tso] = info;
+	}
+	ptp->tso_used |= tso_bit;
+	ptp->reg->tx_event(ptp, tso, cmd->gpo, cmd->event, cmd->pulse,
+		cmd->cycle, cmd->cnt, cmd->sec, cmd->nsec, cmd->iterate, intr,
+		!(cmd->flags & PTP_CMD_ON_TIME),
+		(cmd->flags & PTP_CMD_CLK_OPT));
+	if (cmd->flags & PTP_CMD_ON_TIME) {
+		status = sw->reg->r16(sw, TRIG_ERROR);
+		if (status & tso_bit)
+			err = DEV_IOC_UNIT_ERROR;
+	}
+	ptp->ops->release(ptp);
+
+proc_dev_tx_event_done:
+	*data = tso;
+	return err;
+}  /* proc_dev_tx_event */
+
+static int proc_ptp_tx_cascade_init(struct ptp_info *ptp, u8 *data)
+{
+	struct ptp_tso_options *cmd = (struct ptp_tso_options *) data;
+	int first;
+	int gpo;
+	int i;
+	int tso;
+	int total;
+	u16 status;
+	struct ksz_sw *sw = container_of(ptp, struct ksz_sw, ptp_hw);
+
+	tso = cmd->tso;
+	gpo = cmd->gpo;
+	total = cmd->total;
+	if (!total)
+		return -EINVAL;
+	if (gpo >= MAX_GPIO)
+		return -EINVAL;
+	first = tso;
+
+	/* Cancel operation. */
+	if ((cmd->flags & PTP_CMD_CANCEL_OPER)) {
+		if (tso >= MAX_TRIG_UNIT)
+			return DEV_IOC_UNIT_UNAVAILABLE;
+		if (first != ptp->cascade_gpo[gpo].first ||
+				total != ptp->cascade_gpo[gpo].total) {
+			first = ptp->cascade_gpo[gpo].first;
+			total = ptp->cascade_gpo[gpo].total;
+		}
+
+		/* Reset the last unit in case it is used to raise the level. */
+		first = first + total - 1;
+		if (ptp->outputs[first].level) {
+			ptp->cascade_tx |= (1 << first);
+			ptp->tso_used |= (1 << first);
+		}
+		ptp->ops->acquire(ptp);
+		for (i = 0; i < total; i++, tso++) {
+			if (ptp->tso_used & (1 << tso))
+				ptp_tso_off(ptp, tso, (1 << tso));
+		}
+		tso = total;
+		ptp->cascade = false;
+		ptp->ops->release(ptp);
+		goto proc_ptp_tx_cascade_init_done;
+	}
+
+	if (ptp->cascade)
+		return DEV_IOC_UNIT_UNAVAILABLE;
+
+	/* Find available units for use. */
+	if (tso >= MAX_TRIG_UNIT) {
+		int rc = find_avail_tx_unit(ptp, total, &first);
+
+		if (rc)
+			return rc;
+	} else {
+		for (i = 0; i < total; i++) {
+			if (tso >= MAX_TRIG_UNIT)
+				return DEV_IOC_UNIT_UNAVAILABLE;
+			if (ptp->tso_used & (1 << tso))
+				return DEV_IOC_UNIT_USED;
+			++tso;
+		}
+	}
+
+	if ((cmd->flags & PTP_CMD_CASCADE_RESET_OPER))
+		goto proc_ptp_tx_cascade_init_set;
+
+	/* Last operation was not in cascade mode. */
+	if (!ptp->cascade_gpo[gpo].total)
+		goto proc_ptp_tx_cascade_init_set;
+
+	/* previous last unit. */
+	i = ptp->cascade_gpo[gpo].first + ptp->cascade_gpo[gpo].total - 1;
+
+	/* current last unit. */
+	tso = first + total - 1;
+
+	/* Last operation not ended high. */
+	if (tso == i || !ptp->outputs[i].level)
+		goto proc_ptp_tx_cascade_init_set;
+
+	ptp->ops->acquire(ptp);
+	status = sw->reg->r16(sw, GPIO_MONITOR);
+
+	/* Current level is high. */
+	if (status & (1 << gpo)) {
+
+		/* Set unit to hold the level high. */
+		ptp->reg->tx_event(ptp, tso, gpo, TRIG_POS_EDGE, 0, 0, 1, 0, 1,
+			0, PTP_CMD_INTR_OPER, 1, 0);
+
+		/* Release the signal from the previous last unit. */
+		ptp_gpo_reset(ptp, ptp->outputs[i].gpo, (1 << i));
+	}
+	ptp->ops->release(ptp);
+
+proc_ptp_tx_cascade_init_set:
+	ptp->cascade = true;
+	ptp->cascade_gpo[gpo].first = first;
+	ptp->cascade_gpo[gpo].total = total;
+	tso = first;
+
+proc_ptp_tx_cascade_init_done:
+	*data = tso;
+	return 0;
+}  /* proc_ptp_tx_cascade_init */
+
+static int proc_ptp_tx_cascade(struct ptp_info *ptp, u8 *data)
+{
+	struct ptp_tso_options *cmd = (struct ptp_tso_options *) data;
+	int first;
+	int gpo;
+	int tso;
+	int total;
+	struct ptp_utime t;
+
+	gpo = cmd->gpo;
+	if (gpo >= MAX_GPIO)
+		return -EINVAL;
+	tso = cmd->tso;
+	total = cmd->total;
+	first = tso;
+	if (!ptp->cascade || tso != ptp->cascade_gpo[gpo].first ||
+			total != ptp->cascade_gpo[gpo].total)
+		return DEV_IOC_UNIT_UNAVAILABLE;
+
+	/* Cancel operation. */
+	if ((cmd->flags & PTP_CMD_CANCEL_OPER)) {
+		proc_ptp_tx_cascade_init(ptp, data);
+		goto proc_ptp_tx_cascade_done;
+	}
+	ptp->ops->acquire(ptp);
+	if ((cmd->flags & PTP_CMD_REL_TIME) && cmd->sec < 100) {
+		ptp->reg->get_time(ptp, &t);
+		cmd->sec += t.sec;
+	}
+	total = ptp->reg->tx_cascade(ptp, tso, total, cmd->cnt, cmd->sec,
+		cmd->nsec, cmd->flags & PTP_CMD_INTR_OPER);
+	if (!total)
+		ptp->cascade = false;
+	ptp->ops->release(ptp);
+
+proc_ptp_tx_cascade_done:
+	*data = tso;
+	return 0;
+}  /* proc_ptp_tx_cascade */
+
+static void proc_tsm_get_gps(struct ptp_info *ptp, u8 *data)
+{
+	struct tsm_get_gps *get = (struct tsm_get_gps *) data;
+
+	if (!ptp->gps_dev)
+		return;
+
+	get->cmd |= TSM_CMD_GET_TIME_RESP;
+	get->seqid = htons(ptp->gps_seqid);
+	get->sec = htonl(ptp->gps_time.sec);
+	get->nsec = htonl(ptp->gps_time.nsec);
+	file_dev_setup_msg(ptp->gps_dev, data, sizeof(struct tsm_get_gps),
+		NULL, NULL);
+	ptp->gps_dev = NULL;
+}  /* proc_tsm_get_gps */
+
+static int proc_dev_get_event(struct file_dev_info *info, u8 *data)
+{
+	struct ptp_info *ptp = info->dev;
+	int len;
+	struct ptp_tsi_info *in = (struct ptp_tsi_info *) data;
+	u8 buf[sizeof(struct ptp_utime) * MAX_TIMESTAMP_EVENT_UNIT +
+		sizeof(struct ptp_tsi_info)];
+	struct ptp_tsi_info *out = (struct ptp_tsi_info *) buf;
+
+	if (in->unit >= MAX_TIMESTAMP_UNIT)
+		return -EINVAL;
+	if (!ptp->events[in->unit].num)
+		return DEV_IOC_UNIT_UNAVAILABLE;
+	out->cmd = in->cmd | PTP_CMD_RESP;
+	out->unit = in->unit;
+	out->event = ptp->events[in->unit].event;
+	out->num = ptp->events[in->unit].num;
+	out->edge = ptp->events[in->unit].edge;
+	len = sizeof(struct ptp_utime) * out->num;
+	memcpy(out->t, ptp->events[in->unit].t, len);
+	len += sizeof(struct ptp_tsi_info);
+	file_dev_setup_msg(info, buf, len, NULL, NULL);
+	return 0;
+}  /* proc_dev_get_event */
+
+static int proc_ptp_get_event(struct ptp_info *ptp, u8 *data)
+{
+	struct ptp_tsi_info *in = (struct ptp_tsi_info *) data;
+	int ret = -1;
+
+	if (ptp->tsi_dev[in->unit])
+		ret = proc_dev_get_event(ptp->tsi_dev[in->unit], data);
+	return ret;
+}  /* proc_ptp_get_event */
+
+static int proc_ptp_get_trig(struct ptp_info *ptp, u8 *data, u16 done,
+	u16 error)
+{
+	int len;
+	struct ptp_tsi_info *in = (struct ptp_tsi_info *) data;
+	u8 buf[sizeof(struct ptp_utime) + sizeof(struct ptp_tsi_info)];
+	struct ptp_tsi_info *out = (struct ptp_tsi_info *) buf;
+	struct ptp_output *cur;
+	int tso = in->unit;
+	int tso_bit = (1 << tso);
+
+	out->cmd = in->cmd | PTP_CMD_RESP;
+	out->unit = in->unit;
+	cur = &ptp->outputs[tso];
+	if (error & tso_bit)
+		out->event = 1;
+	else if (!(done & tso_bit))
+		out->event = 2;
+	else
+		out->event = 0;
+	out->num = 1;
+	len = sizeof(struct ptp_utime) * out->num;
+	memcpy(out->t, &cur->trig, len);
+	len += sizeof(struct ptp_tsi_info);
+	if (ptp->tso_dev[tso]) {
+		file_dev_setup_msg(ptp->tso_dev[tso], buf, len, NULL, NULL);
+		return 0;
+	}
+	return -1;
+}  /* proc_ptp_get_trig */
+
+static int proc_dev_poll_event(struct file_dev_info *info, u8 *data)
+{
+	struct ptp_info *ptp = info->dev;
+	struct ptp_tsi_info *in = (struct ptp_tsi_info *) data;
+
+	if (in->unit >= MAX_TIMESTAMP_UNIT)
+		return -EINVAL;
+	if (!ptp_poll_event(ptp, in->unit))
+		return DEV_IOC_UNIT_UNAVAILABLE;
+	return proc_dev_get_event(info, data);
+}  /* proc_dev_poll_event */
+
+static int proc_ptp_get_output(struct ptp_info *ptp, u8 *data)
+{
+	int *output = (int *) data;
+	struct ptp_tso_options *in = (struct ptp_tso_options *) data;
+
+	if (in->gpo >= MAX_GPIO)
+		return -EINVAL;
+	*output = ptp->cascade_gpo[in->gpo].tso;
+	return 0;
+}  /* proc_ptp_get_output */
+
+static void proc_ptp_get_clk(struct ptp_info *ptp, u8 *data)
+{
+	struct ptp_utime t;
+	struct ptp_clk_options *cmd = (struct ptp_clk_options *) data;
+
+	ptp->ops->acquire(ptp);
+	ptp->reg->get_time(ptp, &t);
+	ptp->ops->release(ptp);
+	cmd->sec = t.sec;
+	cmd->nsec = t.nsec;
+}  /* proc_ptp_get_clk */
+
+static int proc_ptp_set_clk(struct ptp_info *ptp, u8 *data)
+{
+	struct ptp_utime t;
+	struct ptp_clk_options *cmd = (struct ptp_clk_options *) data;
+	struct timespec64 ts;
+	struct ptp_utime sys_time;
+
+	t.sec = cmd->sec;
+	t.nsec = cmd->nsec;
+	ptp->ops->acquire(ptp);
+	ts = ktime_to_timespec64(ktime_get_real());
+	sys_time.sec = ts.tv_sec;
+	sys_time.nsec = ts.tv_nsec;
+	ptp->reg->set_time(ptp, &t);
+	ptp->cur_time = t;
+	calc_udiff(&ptp->cur_time, &sys_time, &ptp->time_diff);
+	generate_tx_event(ptp, ptp->pps_gpo);
+	ptp->ops->release(ptp);
+	dbg_msg(" set clk: %x:%09u\n", cmd->sec, cmd->nsec);
+	return 0;
+}  /* proc_ptp_set_clk */
+
+static int proc_ptp_adj_clk(struct ptp_info *ptp, u8 *data, int adjust)
+{
+	struct ptp_clk_options *cmd = (struct ptp_clk_options *) data;
+
+	adjust--;
+	if (cmd->sec > 1) {
+		ptp->adjust_sec = cmd->sec;
+		ptp->adjust_offset = cmd->nsec;
+		if (!adjust) {
+			ptp->adjust_sec = -ptp->adjust_sec;
+			ptp->adjust_offset = -ptp->adjust_offset;
+		}
+		cmd->sec = cmd->nsec = 0;
+		adj_clock(&ptp->adj_clk);
+	}
+	ptp->ops->acquire(ptp);
+	if (cmd->nsec || cmd->sec) {
+		ptp->sec_changed = cmd->sec;
+		if (!(ptp->features & PTP_ADJ_SEC)) {
+			cmd->nsec += cmd->sec * NANOSEC_IN_SEC;
+			cmd->sec = 0;
+		}
+		ptp->reg->adjust_time(ptp, adjust, cmd->sec, cmd->nsec,
+			ptp->features & PTP_ADJ_HACK);
+		ptp->offset_changed = cmd->nsec;
+		if (!adjust)
+			ptp->offset_changed = -cmd->nsec;
+		if (ptp->sec_changed)
+			generate_tx_event(ptp, ptp->pps_gpo);
+		else {
+			ptp->update_sec_jiffies = jiffies;
+			schedule_delayed_work(&ptp->check_pps,
+				1200 * HZ / 1000);
+		}
+		if (ptp->sec_changed) {
+			if (adjust)
+				ptp->cur_time.sec += cmd->sec;
+			else
+				ptp->cur_time.sec -= cmd->sec;
+			ptp->sec_changed = 0;
+		}
+		if (adjust)
+			add_nsec(&ptp->cur_time, cmd->nsec);
+		else
+			sub_nsec(&ptp->cur_time, cmd->nsec);
+		dbg_msg(" adj clk: %d %u:%09u\n", adjust, cmd->sec, cmd->nsec);
+	}
+	if (cmd->interval) {
+		ptp->drift = cmd->drift;
+		if (ptp->drift > MAX_DRIFT_CORR)
+			ptp->drift = MAX_DRIFT_CORR;
+		else if (ptp->drift < -MAX_DRIFT_CORR)
+			ptp->drift = -MAX_DRIFT_CORR;
+		ptp->drift_set = ptp->drift;
+		ptp->adjust = clk_adjust_val(ptp->drift, cmd->interval);
+		set_ptp_adjust(ptp, ptp->adjust);
+		if (!ptp->ptp_synt) {
+			syntonize_clk(ptp);
+			ptp->ptp_synt = true;
+		}
+		if (!ptp->first_drift)
+			ptp->first_drift = ptp->drift_set;
+		dbg_msg(" adj drift: %d\n", cmd->drift);
+	}
+	ptp->ops->release(ptp);
+	return 0;
+}  /* proc_ptp_adj_clk */
+
+static int proc_ptp_get_delay(struct ptp_info *ptp, int port, u8 *data)
+{
+	struct ptp_delay_values *delay = (struct ptp_delay_values *) data;
+
+	if (port >= MAX_PTP_PORT)
+		return DEV_IOC_INVALID_CMD;
+	ptp->ops->acquire(ptp);
+	delay->rx_latency = get_ptp_ingress(ptp, port);
+	delay->tx_latency = get_ptp_egress(ptp, port);
+	delay->asym_delay = get_ptp_asym(ptp, port);
+	ptp->ops->release(ptp);
+	return 0;
+}  /* proc_ptp_get_delay */
+
+static int proc_ptp_set_delay(struct ptp_info *ptp, int port, u8 *data)
+{
+	struct ptp_delay_values *delay = (struct ptp_delay_values *) data;
+
+	if (port >= MAX_PTP_PORT)
+		return DEV_IOC_INVALID_CMD;
+	ptp->ops->acquire(ptp);
+	set_ptp_ingress(ptp, port, delay->rx_latency);
+	set_ptp_egress(ptp, port, delay->tx_latency);
+	set_ptp_asym(ptp, port, delay->asym_delay);
+	ptp->rx_latency[port] = delay->rx_latency;
+	ptp->tx_latency[port] = delay->tx_latency;
+	ptp->asym_delay[port] = delay->asym_delay;
+	ptp->ops->release(ptp);
+	dbg_msg("set delay: %d = %d %d %d\n", port,
+		ptp->rx_latency[port],
+		ptp->tx_latency[port],
+		ptp->asym_delay[port]);
+	return 0;
+}  /* proc_ptp_set_delay */
+
+static int proc_ptp_get_peer_delay(struct ptp_info *ptp, int port, u8 *data)
+{
+	struct ptp_delay_values *delay = (struct ptp_delay_values *) data;
+
+	if (port >= MAX_PTP_PORT)
+		return DEV_IOC_INVALID_CMD;
+	ptp->ops->acquire(ptp);
+	delay->rx_latency = 0;
+	delay->tx_latency = 0;
+	delay->asym_delay = 0;
+	delay->reserved = get_ptp_link(ptp, port);
+	ptp->ops->release(ptp);
+	return 0;
+}  /* proc_ptp_get_peer_delay */
+
+static int proc_ptp_set_peer_delay(struct ptp_info *ptp, int port, u8 *data)
+{
+	struct ptp_delay_values *delay = (struct ptp_delay_values *) data;
+
+	if (port >= MAX_PTP_PORT)
+		return DEV_IOC_INVALID_CMD;
+	ptp->ops->acquire(ptp);
+	set_ptp_link(ptp, port, delay->reserved);
+	ptp->peer_delay[port] = delay->reserved;
+	ptp->ops->release(ptp);
+	dbg_msg("set delay: %d = %d\n", port,
+		ptp->peer_delay[port]);
+	return 0;
+}  /* proc_ptp_set_peer_delay */
+
+static void ptp_tx_done(struct ptp_info *ptp, int tso)
+{
+	int first;
+	int last;
+	int prev;
+	u32 reg;
+	u16 data;
+	struct ksz_sw *sw = container_of(ptp, struct ksz_sw, ptp_hw);
+
+	reg = TRIGn_CONF_1(tso);
+	data = sw->reg->r16(sw, reg);
+	if (data & TRIG_CASCADE_EN) {
+		last = tso;
+		do {
+			--tso;
+			reg = TRIGn_CONF_1(tso);
+			data = sw->reg->r16(sw, reg);
+			prev = (data & TRIG_CASCADE_UPS_MASK) >> 10;
+			if (prev == last)
+				break;
+		} while (tso > 0);
+		first = tso;
+		for (tso = last; tso > first; tso--)
+			ptp_tso_off(ptp, tso, (1 << tso));
+	}
+	ptp_tso_off(ptp, tso, (1 << tso));
+}  /* ptp_tx_done */
+
+static struct ptp_tx_ts *proc_get_ts(struct ptp_info *ptp, u8 port, u8 msg,
+	u16 seqid, u8 *mac, struct file_dev_info *info, int len)
+{
+	struct ptp_tx_ts *tx;
+	int from_stack = false;
+	u8 *data = NULL;
+	struct ksz_sw *sw = container_of(ptp, struct ksz_sw, ptp_hw);
+	bool linked = (media_connected == sw->port_info[port].state);
+
+	if (info)
+		data = info->write_buf;
+	if (SYNC_MSG == msg)
+		tx = &ptp->tx_sync[port];
+	else if (PDELAY_RESP_MSG == msg)
+		tx = &ptp->tx_resp[port];
+	else
+		tx = &ptp->tx_dreq[port];
+	if (seqid || mac[0] || mac[1])
+		from_stack = true;
+	if (data && tx->req_time && linked)
+		dbg_msg("  last %x=%04x: p=%d, j=%lu\n", msg, seqid, port,
+			jiffies - tx->req_time);
+	tx->missed = false;
+	tx->req_time = jiffies;
+	if (tx->ts.timestamp && from_stack) {
+		unsigned long diff = tx->req_time - tx->resp_time;
+
+		/* The timestamp is not valid. */
+		if (diff >= 4 * ptp->delay_ticks) {
+			dbg_msg("  invalid: %x=%04x: %d, %lu\n",
+				msg, seqid, port, diff);
+			tx->ts.timestamp = 0;
+		}
+#ifdef PTP_SPI
+		else if (diff > 2 * ptp->delay_ticks)
+			dbg_msg("  ready? %x=%04x: %d, %lu\n",
+				msg, seqid, port, diff);
+#endif
+	}
+	if (!tx->ts.timestamp && linked && data) {
+		int rc = wait_event_interruptible_timeout(ptp->wait_ts,
+			0 != tx->ts.timestamp, ptp->delay_ticks);
+
+		if (rc < 0)
+			return NULL;
+	}
+	if (!tx->ts.timestamp) {
+		if (from_stack && data) {
+			tx->missed = true;
+			memcpy(tx->data.buf, data, len);
+			tx->data.len = len;
+			tx->dev = info;
+		}
+#ifndef PTP_SPI
+		if (linked)
+			dbg_msg("  missed %x=%04x: p=%d, j=%lu\n",
+				msg, seqid, port, jiffies - tx->req_time);
+#endif
+		tx = NULL;
+	}
+	return tx;
+}  /* proc_get_ts */
+
+static int proc_ptp_get_timestamp(struct ptp_info *ptp, u8 *data,
+	struct file_dev_info *info)
+{
+	struct ptp_ts_options *opt = (struct ptp_ts_options *) data;
+
+	if (opt->timestamp) {
+		struct ptp_ts ts;
+
+		ts.timestamp = opt->timestamp;
+		update_ts(&ts, ptp->cur_time.sec);
+		opt->sec = ts.t.sec;
+		opt->nsec = ts.t.nsec;
+	} else {
+		struct ptp_tx_ts *tx;
+		struct tsm_db *db;
+
+		if (opt->port >= MAX_PTP_PORT)
+			return DEV_IOC_INVALID_CMD;
+
+		/* Save timestamp information for later reporting. */
+		if (info) {
+			db = (struct tsm_db *) info->write_buf;
+			db->cmd = opt->msg;
+			db->cmd |= TSM_CMD_DB_GET;
+			db->index = opt->port << 1;
+			db->seqid = htons(opt->seqid);
+			db->mac[0] = opt->mac[0];
+			db->mac[1] = opt->mac[1];
+		}
+		tx = proc_get_ts(ptp, opt->port, opt->msg,
+			opt->seqid, opt->mac, info, sizeof(struct tsm_db));
+		if (!tx)
+			return -EAGAIN;
+		opt->sec = tx->ts.r.sec;
+		opt->nsec = tx->ts.r.nsec;
+		tx->ts.timestamp = 0;
+		tx->req_time = 0;
+	}
+	return 0;
+}  /* proc_ptp_get_timestamp */
+
+static int parse_tsm_msg(struct file_dev_info *info, int len)
+{
+	struct ptp_info *ptp = info->dev;
+	u8 *data = info->write_buf;
+	u8 cmd = data[0] & 0xf0;
+	u8 msg = data[0] & 0x03;
+	int result = 0;
+
+	switch (cmd) {
+	case TSM_CMD_DB_GET_TIME:
+	{
+		struct tsm_get_time *get = (struct tsm_get_time *) data;
+		struct ptp_ts ts;
+
+		ts.timestamp = ntohl(get->nsec);
+		if (ts.timestamp) {
+			update_ts(&ts, ptp->cur_time.sec);
+		} else {
+			ptp->ops->acquire(ptp);
+			ptp->reg->get_time(ptp, &ts.t);
+			ptp->ops->release(ptp);
+		}
+		file_dev_setup_msg(info, data, len, ptp_tsm_get_time_resp,
+			&ts.t);
+		break;
+	}
+	case TSM_CMD_DB_GET:
+	{
+		struct tsm_db *db = (struct tsm_db *) data;
+
+		if (db->index <= 3) {
+			struct ptp_tx_ts *tx;
+			int port = db->index >> 1;
+
+			tx = proc_get_ts(ptp, port, msg, ntohs(db->seqid),
+				db->mac, info, len);
+			if (tx) {
+				file_dev_setup_msg(info, data, len,
+					ptp_tsm_resp, &tx->ts);
+				tx->ts.timestamp = 0;
+				tx->req_time = 0;
+			}
+		}
+		break;
+	}
+	case TSM_CMD_GET_GPS_TS:
+	{
+		/* First time to get GPS timestamp. */
+		if (MAX_TIMESTAMP_UNIT == ptp->gps_tsi) {
+			ptp->gps_tsi = DEFAULT_GPS_TSI;
+			if (ptp->tsi_used & (1 << ptp->gps_tsi))
+				ptp->reg->rx_off(ptp, ptp->gps_tsi);
+			prepare_gps(ptp);
+			ptp->gps_seqid = 0;
+		}
+		ptp->gps_req_time = jiffies;
+		ptp->gps_dev = info;
+		if (ptp->gps_resp_time) {
+			unsigned long diff = ptp->gps_req_time -
+				ptp->gps_resp_time;
+
+			/* The timestamp is not valid. */
+			if (diff >= 2 * ptp->delay_ticks) {
+				dbg_msg("  invalid gps: %lu\n", diff);
+				ptp->gps_time.sec = 0;
+			}
+		}
+		if (ptp->gps_time.sec) {
+			proc_tsm_get_gps(ptp, data);
+			ptp->gps_time.sec = 0;
+			ptp->gps_req_time = 0;
+		} else
+			dbg_msg("  missed gps\n");
+		break;
+	}
+	case TSM_CMD_CNF_SET:
+	{
+		struct tsm_cfg *cfg = (struct tsm_cfg *) data;
+		u32 ingress = htonl(cfg->ingress_delay);
+
+		ptp->ops->acquire(ptp);
+		if (0xFF == cfg->port) {
+			u16 mode;
+
+			if (cfg->enable & 0x04)
+				ptp->mode |= PTP_TC_P2P;
+			else
+				ptp->mode &= ~PTP_TC_P2P;
+			mode = ptp->mode;
+			if (ptp->overrides & PTP_VERIFY_TIMESTAMP)
+				mode |= PTP_1STEP;
+			set_ptp_mode(ptp, mode);
+			ptp->def_mode = ptp->mode;
+		} else {
+			u8 port = cfg->port - 1;
+
+			if ((cfg->enable & 0x10) && port < MAX_PTP_PORT &&
+					ptp->peer_delay[port] != ingress) {
+				ptp->peer_delay[port] = ingress;
+				set_ptp_link(ptp, port, ingress);
+			}
+		}
+		ptp->ops->release(ptp);
+		break;
+	}
+	case TSM_CMD_CLOCK_SET:
+	{
+		struct tsm_clock_set *clk = (struct tsm_clock_set *) data;
+		struct ptp_ts ts;
+
+		ts.t.sec = ntohl(clk->sec);
+		ts.t.nsec = ntohl(clk->nsec);
+		ts.timestamp = ntohl(clk->timestamp);
+		ptp->ops->acquire(ptp);
+		set_cur_time(ptp, &ts);
+		ptp->ops->release(ptp);
+		ptp->state = 2;
+		break;
+	}
+	case TSM_CMD_CLOCK_CORRECT:
+	{
+		struct tsm_clock_correct *clk = (struct tsm_clock_correct *)
+			data;
+		u32 drift;
+		u32 nsec;
+		int ptp_offset;
+
+		drift = ntohl(clk->drift);
+		nsec = ntohl(clk->nsec);
+		ptp_offset = ntohl(clk->offset);
+		if (2 == (clk->add >> 4))
+			break;
+
+		ptp->ops->acquire(ptp);
+		if (nsec) {
+			ptp->reg->adjust_time(ptp, !ptp_offset, 0, nsec,
+				ptp->features & PTP_ADJ_HACK);
+			ptp->offset_changed = nsec;
+			if (ptp_offset)
+				ptp->offset_changed = -nsec;
+			ptp->update_sec_jiffies = jiffies;
+			schedule_delayed_work(&ptp->check_pps,
+					      msecs_to_jiffies(1200));
+		}
+		if (clk->add & 1)
+			ptp->drift = drift;
+		else
+			ptp->drift = -drift;
+		if (ptp->drift > MAX_DRIFT_CORR)
+			ptp->drift = MAX_DRIFT_CORR;
+		else if (ptp->drift < -MAX_DRIFT_CORR)
+			ptp->drift = -MAX_DRIFT_CORR;
+		ptp->drift_set = ptp->drift;
+		ptp->adjust = clk_adjust_val(ptp->drift,
+			NANOSEC_IN_SEC);
+		set_ptp_adjust(ptp, ptp->adjust);
+		if (!ptp->first_drift)
+			ptp->first_drift = ptp->drift_set;
+		ptp->ops->release(ptp);
+		break;
+	}
+	default:
+		dbg_msg("tsm cmd: %02X, %d\n", cmd, len);
+	}
+	return result;
+}  /* parse_tsm_msg */
+
+static struct ksz_dev_major ptp_majors[MAX_SW_DEVICES];
+
+static struct file_dev_info *alloc_dev_info(struct ptp_info *ptp, uint minor)
+{
+	struct file_dev_info *info;
+
+	info = kzalloc(sizeof(struct file_dev_info), GFP_KERNEL);
+	if (info) {
+		info->dev = ptp;
+		sema_init(&info->sem, 1);
+		mutex_init(&info->lock);
+		init_waitqueue_head(&info->wait_msg);
+		info->read_max = 60000;
+		info->read_tmp = MAX_TSM_UDP_LEN;
+		info->read_buf = kzalloc(info->read_max + info->read_tmp,
+			GFP_KERNEL);
+		info->read_in = &info->read_buf[info->read_max];
+		info->write_len = 1000;
+		info->write_buf = kzalloc(info->write_len, GFP_KERNEL);
+
+		info->minor = minor;
+		info->next = ptp->dev[minor];
+		ptp->dev[minor] = info;
+	}
+	return info;
+}  /* alloc_dev_info */
+
+static void free_dev_info(struct file_dev_info *info)
+{
+	if (info) {
+		int i;
+		struct ptp_info *ptp = info->dev;
+		uint minor = info->minor;
+
+		for (i = 0; i < MAX_TIMESTAMP_UNIT; i++) {
+			if (ptp->tsi_dev[i] == info) {
+				cancel_rx_unit(ptp, i);
+			}
+		}
+		for (i = 0; i < MAX_TRIG_UNIT; i++) {
+			if (ptp->tso_dev[i] == info) {
+				ptp->ops->acquire(ptp);
+				ptp_tso_off(ptp, i, (1 << i));
+				ptp->ops->release(ptp);
+			}
+		}
+		if (ptp->gps_dev == info)
+			ptp->gps_dev = NULL;
+		file_gen_dev_release(info, &ptp->dev[minor]);
+	}
+}  /* free_dev_info */
+
+static int ptp_dev_open(struct inode *inode, struct file *filp)
+{
+	struct file_dev_info *info = (struct file_dev_info *)
+		filp->private_data;
+	uint minor = MINOR(inode->i_rdev);
+	uint major = MAJOR(inode->i_rdev);
+	struct ptp_info *ptp = NULL;
+	int i;
+
+	if (minor > 1)
+		return -ENODEV;
+	for (i = 0; i < MAX_SW_DEVICES; i++) {
+		if (ptp_majors[i].major == major) {
+			ptp = ptp_majors[i].dev;
+			break;
+		}
+	}
+	if (!ptp)
+		return -ENODEV;
+	if (!info) {
+		info = alloc_dev_info(ptp, minor);
+		if (info)
+			filp->private_data = info;
+		else
+			return -ENOMEM;
+	}
+	return 0;
+}  /* ptp_dev_open */
+
+static int ptp_dev_release(struct inode *inode, struct file *filp)
+{
+	struct file_dev_info *info = (struct file_dev_info *)
+		filp->private_data;
+
+	free_dev_info(info);
+	filp->private_data = NULL;
+	return 0;
+}  /* ptp_dev_release */
+
+static int execute_wait(struct ptp_work *work)
+{
+	int rc = 0;
+
+	execute(work->ptp, &work->work);
+#ifdef PTP_SPI
+	wait_for_completion(&work->done);
+#endif
+	return rc;
+}  /* execute_wait */
+
+static void proc_ptp_work(struct work_struct *work)
+{
+	struct ptp_work *parent =
+		container_of(work, struct ptp_work, work);
+	struct ptp_info *ptp = parent->ptp;
+	struct file_dev_info *info = parent->dev_info;
+	u8 *data = parent->param.data;
+	uint port;
+	u32 reg;
+	int result = DEV_IOC_OK;
+	struct ksz_sw *sw = container_of(ptp, struct ksz_sw, ptp_hw);
+
+	parent->output = parent->option;
+	switch (parent->cmd) {
+	case DEV_CMD_INFO:
+		switch (parent->subcmd) {
+		case DEV_INFO_INIT:
+		{
+#if 0
+			int cap = 0;
+#endif
+
+			if (ptp->op_state)
+				goto skip;
+			ptp->forward = FWD_MAIN_DEV;
+#if 0
+			if (cap & PTP_HAVE_MULTI_DEVICES)
+				ptp->forward = FWD_VLAN_DEV |
+					       FWD_STP_DEV;
+#endif
+
+skip:
+			ptp_init_state(ptp);
+			parent->output = ptp->drift;
+			break;
+		}
+		case DEV_INFO_EXIT:
+			ptp_exit_state(ptp);
+			break;
+		case DEV_INFO_RESET:
+			reg = parent->option;
+			ptp->ops->acquire(ptp);
+			sw->reg->w16(sw, REG_RESET_CTRL, 1 << reg);
+			udelay(1);
+			sw->reg->w16(sw, REG_RESET_CTRL, 0);
+			ptp->ops->release(ptp);
+			break;
+		default:
+			result = DEV_IOC_INVALID_CMD;
+			break;
+		}
+		break;
+	case DEV_CMD_PUT:
+		switch (parent->subcmd) {
+		case DEV_PTP_CFG:
+			result = proc_ptp_set_cfg(ptp, data);
+			break;
+		case DEV_PTP_TEVT:
+			result = proc_dev_rx_event(info, data);
+			parent->output = *data;
+			break;
+		case DEV_PTP_TOUT:
+			result = proc_dev_tx_event(info, data);
+			parent->output = *data;
+			break;
+		case DEV_PTP_CASCADE:
+			if (parent->option)
+				result = proc_ptp_tx_cascade(ptp, data);
+			else
+				result = proc_ptp_tx_cascade_init(ptp, data);
+			parent->output = *data;
+			break;
+		case DEV_PTP_CLK:
+			parent->option &= 0xffff;
+			if (parent->option)
+				result = proc_ptp_adj_clk(ptp, data,
+					parent->option);
+			else
+				result = proc_ptp_set_clk(ptp, data);
+			break;
+		case DEV_PTP_DELAY:
+			port = parent->option;
+			result = proc_ptp_set_delay(ptp, port, data);
+			break;
+		case DEV_PTP_REG:
+			reg = parent->option;
+			ptp->ops->acquire(ptp);
+			sw->reg->w16(sw, reg & 0xffff, reg >> 16);
+			ptp->ops->release(ptp);
+			break;
+		case DEV_PTP_PEER_DELAY:
+			port = parent->option;
+			result = proc_ptp_set_peer_delay(ptp, port, data);
+			break;
+		default:
+			result = DEV_IOC_INVALID_CMD;
+			break;
+		}
+		break;
+	case DEV_CMD_GET:
+		switch (parent->subcmd) {
+		case DEV_PTP_CFG:
+			proc_ptp_get_cfg(ptp, data);
+			break;
+		case DEV_PTP_TEVT:
+			if (parent->option)
+				result = proc_dev_poll_event(info, data);
+
+			/* Not actually used. */
+			else
+				result = proc_dev_get_event(info, data);
+			break;
+		case DEV_PTP_TOUT:
+			break;
+		case DEV_PTP_CLK:
+			proc_ptp_get_clk(ptp, data);
+			break;
+		case DEV_PTP_DELAY:
+			port = parent->option;
+			result = proc_ptp_get_delay(ptp, port, data);
+			break;
+		case DEV_PTP_REG:
+			reg = parent->option;
+			ptp->ops->acquire(ptp);
+			parent->output = sw->reg->r16(sw, reg);
+			ptp->ops->release(ptp);
+			break;
+		case DEV_PTP_PEER_DELAY:
+			port = parent->option;
+			result = proc_ptp_get_peer_delay(ptp, port, data);
+			break;
+		}
+		break;
+	default:
+		result = DEV_IOC_INVALID_CMD;
+		break;
+	}
+	parent->result = result;
+	parent->used = false;
+	complete(&parent->done);
+}  /* proc_ptp_work */
+
+static int proc_ptp_hw_access(struct ptp_info *ptp, int cmd, int subcmd,
+	int option, void *data, size_t len, struct file_dev_info *info,
+	int *output, int wait)
+{
+	struct ptp_access *access;
+	struct ptp_work *work;
+	int ret = 0;
+
+	access = &ptp->hw_access;
+	mutex_lock(&access->lock);
+	work = &access->works[access->index];
+	if (work->used) {
+		pr_alert("work full\n");
+		return -EFAULT;
+	}
+	work->cmd = cmd;
+	work->subcmd = subcmd;
+	work->option = option;
+	memcpy(work->param.data, data, len);
+	work->dev_info = info;
+	work->used = true;
+	access->index++;
+	access->index &= PTP_WORK_LAST;
+#ifdef PTP_SPI
+	init_completion(&work->done);
+#endif
+	if (!wait) {
+		execute(ptp, &work->work);
+		goto hw_access_end;
+	}
+	ret = execute_wait(work);
+
+	/* Cannot continue if ERESTARTSYS. */
+	if (ret < 0)
+		goto hw_access_end;
+
+	ret = work->result;
+	if (DEV_IOC_OK == ret && DEV_CMD_GET == work->cmd)
+		memcpy(data, work->param.data, len);
+	*output = work->output;
+
+hw_access_end:
+	mutex_unlock(&access->lock);
+	return ret;
+}  /* proc_ptp_hw_access */
+
+static void exit_ptp_work(struct ptp_info *ptp)
+{
+	struct ptp_access *access;
+	struct ptp_work *work;
+	int i;
+
+	access = &ptp->hw_access;
+	for (i = 0; i < PTP_WORK_NUM; i++) {
+		work = &access->works[i];
+		flush_work(&work->work);
+	}
+}  /* exit_ptp_work */
+
+static void init_ptp_work(struct ptp_info *ptp)
+{
+	struct ptp_access *access;
+	struct ptp_work *work;
+	int i;
+
+	access = &ptp->hw_access;
+	mutex_init(&access->lock);
+	for (i = 0; i < PTP_WORK_NUM; i++) {
+		work = &access->works[i];
+		work->ptp = ptp;
+		INIT_WORK(&work->work, proc_ptp_work);
+		init_completion(&work->done);
+	}
+}  /* init_ptp_work */
+
+#ifdef CONFIG_PTP_1588_CLOCK
+#include "micrel_ptp.c"
+#endif
+
+static u32 _get_clk_cnt(void)
+{
+	return 0;
+}
+
+#ifdef PTP_SPI
+#define ACCESS_VAL			1000
+#else
+#define ACCESS_VAL			100
+#endif
+
+static void test_get_time(struct ptp_info *ptp, struct ptp_utime *cur,
+	struct ptp_utime *now, u32 *cur_cnt, u32 *now_cnt)
+{
+	ptp->reg->get_time(ptp, cur);
+	*cur_cnt = ptp->get_clk_cnt();
+	ptp->reg->get_time(ptp, now);
+	*now_cnt = ptp->get_clk_cnt();
+}
+
+static void test_set_time(struct ptp_info *ptp, struct ptp_utime *cur,
+	struct ptp_utime *now, u32 *cur_cnt, u32 *now_cnt)
+{
+	*cur_cnt = ptp->get_clk_cnt();
+	ptp->reg->set_time(ptp, cur);
+	*now_cnt = ptp->get_clk_cnt();
+	ptp->reg->get_time(ptp, now);
+}
+
+static u32 test_avg_time(struct ptp_info *ptp,
+	void (*test_time)(struct ptp_info *ptp, struct ptp_utime *cur,
+	struct ptp_utime *now, u32 *cur_cnt, u32 *now_cnt))
+{
+	struct ptp_utime cur;
+	struct ptp_utime now;
+	struct ksz_ptp_time diff;
+	int i;
+	int clk_delay[6];
+	u32 cur_cnt;
+	u32 now_cnt;
+	u32 hw_delay[6];
+	u64 clk;
+	u32 rem;
+
+	cur.sec = 5;
+	cur.nsec = 0x12345678;
+	ptp->ops->acquire(ptp);
+	for (i = 0; i < 5; i++) {
+		test_time(ptp, &cur, &now, &cur_cnt, &now_cnt);
+		calc_udiff(&cur, &now, &diff);
+		clk_delay[i] = (diff.nsec + (ACCESS_VAL / 2)) / ACCESS_VAL *
+			ACCESS_VAL;
+		hw_delay[i] = now_cnt - cur_cnt;
+	}
+	ptp->ops->release(ptp);
+	clk_delay[5] = 20000000;
+	hw_delay[5] = 50000000;
+	for (i = 0; i < 5; i++) {
+		clk = hw_delay[i];
+		clk *= 1000000;
+		if (ptp->clk_divider)
+			clk = div_u64_rem(clk, ptp->clk_divider, &rem);
+		dbg_msg(" %u %u=%llu\n", clk_delay[i], hw_delay[i], clk);
+		if (clk_delay[i] < clk_delay[5])
+			clk_delay[5] = clk_delay[i];
+		if (hw_delay[i] < hw_delay[5])
+			hw_delay[5] = hw_delay[i];
+	}
+	clk = hw_delay[5];
+	clk *= 1000000;
+	if (ptp->clk_divider)
+		clk = div_u64_rem(clk, ptp->clk_divider, &rem);
+	dbg_msg("%u %llu\n", clk_delay[5], clk);
+	return clk_delay[5];
+}
+
+static void _test_access_time(struct ptp_info *ptp)
+{
+	ptp->get_delay = test_avg_time(ptp, test_get_time);
+	ptp->set_delay = test_avg_time(ptp, test_set_time);
+	if (ptp->get_delay < 10000)
+		ptp->delay_ticks = 10 * HZ / 1000;
+	else if (ptp->get_delay < 12000000)
+		ptp->delay_ticks = 20 * HZ / 1000;
+	else
+		ptp->delay_ticks = 30 * HZ / 1000;
+	dbg_msg("delay_ticks: %lu\n", ptp->delay_ticks);
+}  /* test_access_time */
+
+static void set_ptp_drift(struct ptp_info *ptp, int drift)
+{
+	drift /= 100;
+	drift *= 100;
+	drift = -drift;
+	ptp->first_drift = ptp->drift = drift;
+	ptp->first_sec = 0;
+	ptp->adjust = clk_adjust_val(drift, NANOSEC_IN_SEC);
+	set_ptp_adjust(ptp, ptp->adjust);
+	syntonize_clk(ptp);
+	ptp->ptp_synt = true;
+	dbg_msg("drift: %d\n", drift);
+}  /* set_ptp_drift */
+
+static void check_sys_time(struct ptp_info *ptp, unsigned long cur_jiffies,
+	ktime_t cur_ktime)
+{
+	int diff;
+	int interval;
+	u32 cur_clk_cnt;
+
+	cur_clk_cnt = ptp->get_clk_cnt();
+	if (!ptp->first_drift) {
+		interval = 8;
+		diff = ptp->cur_time.sec - ptp->intr_sec;
+
+		/*
+		 * The second interval is not accurate after first setting up
+		 * the clock until later.
+		 */
+		if (diff < 6)
+			ptp->first_sec = 0;
+	} else
+		interval = 10;
+
+	if (!ptp->first_sec) {
+		ptp->last_clk_cnt = cur_clk_cnt;
+		ptp->total_clk_cnt = 0;
+		ptp->last_jiffies = cur_jiffies;
+		ptp->total_jiffies = 0;
+		ptp->first_ktime = cur_ktime;
+		ptp->first_sec = ptp->cur_time.sec;
+		return;
+	}
+
+	diff = ptp->cur_time.sec - ptp->first_sec;
+
+	if (diff >= 1 && !(diff % interval)) {
+		u32 rem;
+		u64 clk;
+		u64 clk_cnt;
+		s64 drift_clk;
+		s64 drift_jiffies;
+		s64 drift_ktime;
+		u32 passed_sec;
+		u64 passed_usec;
+		u64 passed_nsec;
+		u32 cnt;
+
+		cnt = cur_clk_cnt - ptp->last_clk_cnt;
+		ptp->total_clk_cnt += cnt;
+		ptp->last_clk_cnt = cur_clk_cnt;
+
+		passed_sec = ptp->cur_time.sec - ptp->first_sec;
+		passed_usec = passed_sec;
+		passed_usec *= 1000000;
+		passed_nsec = passed_usec;
+		passed_nsec *= 1000;
+
+		cnt = cur_jiffies - ptp->last_jiffies;
+		ptp->total_jiffies += cnt;
+		ptp->last_jiffies = cur_jiffies;
+
+		clk = ptp->total_jiffies * (1000000 / HZ);
+		drift_jiffies = clk - passed_usec;
+		drift_jiffies *= 1000;
+		drift_jiffies = div_s64_rem(drift_jiffies, passed_sec, &rem);
+
+		cur_ktime -= ptp->first_ktime;
+		drift_ktime = cur_ktime - passed_nsec;
+		drift_ktime = div_s64_rem(drift_ktime, passed_sec, &rem);
+
+		if (!ptp->clk_divider) {
+			if (!ptp->first_drift)
+				set_ptp_drift(ptp, (int) drift_ktime);
+			else
+				printk(KERN_INFO "%lld %lld\n",
+					drift_jiffies, drift_ktime);
+			return;
+		}
+
+		clk_cnt = div_u64_rem(ptp->total_clk_cnt, passed_sec, &rem);
+
+		clk = ptp->total_clk_cnt * 1000000;
+		clk = div_u64_rem(clk, ptp->clk_divider, &rem);
+		drift_clk = clk;
+		if (drift_clk < 0)
+			ptp->overrides &= ~PTP_CHECK_SYS_TIME;
+		drift_clk -= passed_nsec;
+		drift_clk = div_s64_rem(drift_clk, passed_sec, &rem);
+
+		if (!ptp->first_drift)
+			set_ptp_drift(ptp, (int) drift_clk);
+		else
+			printk(KERN_INFO "%10llu %lld %lld %lld\n",
+				clk_cnt, drift_clk, drift_jiffies, drift_ktime);
+	}
+}  /* check_sys_time */
+
+static void proc_ptp_intr(struct ptp_info *ptp)
+{
+	struct ptp_event *event;
+	u16 done;
+	u16 error;
+	u16 status;
+	u16 tsi_bit;
+	u8 data[24];
+	int i;
+	int tsi;
+	int last;
+	ktime_t cur_ktime;
+	struct timespec64 ts;
+	struct ksz_sw *sw = container_of(ptp, struct ksz_sw, ptp_hw);
+
+	cur_ktime = ktime_get_real();
+	ts = ktime_to_timespec64(cur_ktime);
+
+proc_chk_trig_intr:
+	status = sw->reg->r16(sw, TRIG_INT_STATUS);
+	if (!status)
+		goto proc_chk_ts_intr;
+
+	sw->reg->w16(sw, TRIG_INT_STATUS, status);
+	done = sw->reg->r16(sw, TRIG_DONE);
+	error = sw->reg->r16(sw, TRIG_ERROR);
+	for (i = 0; i < MAX_TRIG_UNIT; i++) {
+		if (status & (1 << i)) {
+			if (ptp->tso_intr & (1 << i)) {
+				data[0] = PTP_CMD_GET_OUTPUT;
+				data[1] = i;
+				proc_ptp_get_trig(ptp, data, done, error);
+			}
+			ptp_tx_done(ptp, i);
+		}
+	}
+
+proc_chk_ts_intr:
+	status = sw->reg->r16(sw, TS_INT_STATUS);
+	if (!status)
+		goto proc_ptp_intr_done;
+
+	sw->reg->w16(sw, TS_INT_STATUS, status);
+	for (i = 0; i < MAX_TIMESTAMP_UNIT; i++) {
+		tsi_bit = 1 << i;
+		if (!(status & tsi_bit))
+			continue;
+		ptp->reg->read_event(ptp, i);
+		event = &ptp->events[i];
+		if (event->timeout && (event->num < event->max ||
+				event->last)) {
+			unsigned long expired;
+
+			expired = jiffies + event->timeout;
+			event->expired = expired;
+			if (event->last) {
+				tsi = i + 1;
+				do {
+					if (tsi >= MAX_TIMESTAMP_UNIT)
+						tsi = 0;
+					ptp->events[tsi].expired = expired;
+					++tsi;
+				} while (tsi != event->last);
+			}
+		} else if (event->last && i != event->first) {
+			tsi = i - 1;
+			if (tsi < 0)
+				tsi = MAX_TIMESTAMP_UNIT - 1;
+			if (ptp->tsi_used & (1 << tsi))
+				ptp->events[tsi].expired = jiffies;
+		}
+		if (i == ptp->pps_tsi) {
+			struct ptp_utime sys_time;
+
+			ptp->cur_time.sec = event->t[0].sec;
+			ptp->cur_time.nsec = event->t[0].nsec;
+			ptp->update_sec_jiffies = 0;
+			sys_time.sec = ts.tv_sec;
+			sys_time.nsec = ts.tv_nsec;
+			calc_udiff(&ptp->cur_time, &sys_time, &ptp->time_diff);
+			if (!ptp->intr_sec)
+				ptp->intr_sec = ptp->cur_time.sec;
+			if ((ptp->overrides & PTP_CHECK_SYS_TIME) ||
+					!ptp->first_drift)
+				check_sys_time(ptp, jiffies, cur_ktime);
+#ifdef CONFIG_PTP_1588_CLOCK
+			if (ptp->clock_events & (1 << 0))
+				ptp_event_trigger(ptp->clock_info, 0,
+					ptp->cur_time.sec, ptp->cur_time.nsec);
+			if (ptp->clock_events & (1 << 31))
+				ptp_event_pps(ptp->clock_info);
+#endif
+		} else if (i == ptp->gps_tsi) {
+			ptp->gps_time.sec = event->t[0].sec;
+			ptp->gps_time.nsec = event->t[0].nsec;
+			++ptp->gps_seqid;
+			ptp->gps_resp_time = jiffies;
+		}
+	}
+	for (i = 0; i < MAX_TIMESTAMP_UNIT; i++) {
+		int stop;
+
+		stop = false;
+		tsi_bit = 1 << i;
+		event = &ptp->events[i];
+		if (ptp->tsi_used & tsi_bit) {
+
+			/* At least one event. */
+			if (event->num || event->expired) {
+				if (event->num >= event->max)
+					stop = true;
+				else if (event->expired &&
+						jiffies >= event->expired) {
+					ptp->reg->read_event(ptp, i);
+					stop = true;
+				}
+			}
+		}
+		if ((ptp->ts_status & ptp->ts_intr) & tsi_bit) {
+			if (ptp->tsi_intr & tsi_bit) {
+				data[0] = PTP_CMD_GET_EVENT;
+				data[1] = i;
+				proc_ptp_get_event(ptp, data);
+			}
+			if (i == ptp->gps_tsi && ptp->gps_req_time) {
+				unsigned long diff = jiffies -
+					ptp->gps_req_time;
+
+				if (diff < 2 * ptp->delay_ticks) {
+					data[0] = TSM_CMD_GET_GPS_TS;
+					proc_tsm_get_gps(ptp, data);
+					ptp->gps_time.sec = 0;
+				}
+				ptp->gps_req_time = 0;
+			}
+
+			/* Not used in cascade mode. */
+			if (!event->timeout && !event->last) {
+				event->num = 0;
+				ptp->reg->rx_restart(ptp, tsi_bit);
+				stop = false;
+			}
+		}
+		if (stop) {
+			ptp->reg->rx_off(ptp, i);
+			ptp->tsi_intr &= ~tsi_bit;
+			ptp->tsi_used &= ~tsi_bit;
+			ptp->tsi_dev[i] = NULL;
+			tsi = i;
+			ptp->events[i].timeout = 0;
+			if (i + 1 == event->last) {
+				tsi = event->first;
+				last = event->last;
+				do {
+					if (tsi >= MAX_TIMESTAMP_UNIT)
+						tsi = 0;
+					ptp->events[tsi].first = 0;
+					ptp->events[tsi].last = 0;
+					++tsi;
+				} while (tsi != last);
+			}
+		}
+	}
+	ptp->ts_status = 0;
+	if (!(status & 0xF000))
+		goto proc_chk_trig_intr;
+
+	if (get_tx_time(ptp, status & 0xF000))
+		wake_up_interruptible(&ptp->wait_ts);
+	goto proc_chk_trig_intr;
+
+proc_ptp_intr_done:
+	return;
+}  /* proc_ptp_intr */
+
+static int ptp_get_ts_info(struct ptp_info *ptp, struct net_device *dev,
+	struct ethtool_ts_info *info)
+{
+	int ptp_clock = false;
+	struct ksz_sw *sw = container_of(ptp, struct ksz_sw, ptp_hw);
+	int ret = -ENODEV;
+
+#ifdef CONFIG_PTP_1588_CLOCK
+	ptp_clock = true;
+#endif
+	if (!(sw->features & PTP_HW) || !ptp_clock)
+		return ethtool_op_get_ts_info(dev, info);
+#ifdef CONFIG_PTP_1588_CLOCK
+	if (ptp->clock_info)
+		ret = micrel_ptp_get_ts_info(ptp, info);
+#endif
+	return ret;
+}  /* ptp_get_ts_info */
+
+#define PTP_ENABLE_TXTS		SIOCDEVPRIVATE
+#define PTP_DISABLE_TXTS	(SIOCDEVPRIVATE + 1)
+#define PTP_ENABLE_RXTS		(SIOCDEVPRIVATE + 2)
+#define PTP_DISABLE_RXTS	(SIOCDEVPRIVATE + 3)
+#define PTP_GET_TX_TIMESTAMP	(SIOCDEVPRIVATE + 4)
+#define PTP_GET_RX_TIMESTAMP	(SIOCDEVPRIVATE + 5)
+#define PTP_SET_TIME		(SIOCDEVPRIVATE + 6)
+#define PTP_GET_TIME		(SIOCDEVPRIVATE + 7)
+#define PTP_SET_FIPER_ALARM	(SIOCDEVPRIVATE + 8)
+#define PTP_SET_ADJ		(SIOCDEVPRIVATE + 9)
+#define PTP_GET_ADJ		(SIOCDEVPRIVATE + 10)
+#define PTP_CLEANUP_TS		(SIOCDEVPRIVATE + 11)
+#define PTP_ADJ_TIME		(SIOCDEVPRIVATE + 12)
+
+struct ixxat_ptp_time {
+	/* just 48 bit used */
+	u64 sec;
+	u32 nsec;
+};
+
+struct ixxat_ptp_ident {
+	u8 vers;
+	u8 mType;
+	u16 netwProt;
+	u16 seqId;
+	struct ptp_port_identity portId;
+} __packed;
+
+/* needed for timestamp data over ioctl */
+struct ixxat_ptp_data {
+	struct ixxat_ptp_ident ident;
+	struct ixxat_ptp_time ts;
+};
+
+static int ixxat_ptp_ioctl(struct ptp_info *ptp, unsigned int cmd,
+	struct ifreq *ifr)
+{
+	struct ixxat_ptp_time ptp_time;
+	struct ixxat_ptp_data ptp_data;
+	struct ptp_clk_options clk_opt;
+	int output;
+	s64 scaled_nsec;
+	struct ptp_ts ts;
+	struct ptp_tx_ts *tx;
+	int drift;
+	int err = 0;
+	uint port;
+
+	switch (cmd) {
+	case PTP_ENABLE_TXTS:
+		ptp->tx_en |= 2;
+		break;
+	case PTP_DISABLE_TXTS:
+		ptp->tx_en &= ~2;
+		break;
+	case PTP_ENABLE_RXTS:
+		ptp->rx_en |= 2;
+		break;
+	case PTP_DISABLE_RXTS:
+		ptp->rx_en &= ~2;
+		break;
+	case PTP_GET_TX_TIMESTAMP:
+		if (copy_from_user(&ptp_data, ifr->ifr_data, sizeof(ptp_data)))
+			return -EFAULT;
+
+		err = -EINVAL;
+		port = htons(ptp_data.ident.portId.port);
+		if (port < 1 || port > MAX_PTP_PORT)
+			break;
+		port--;
+		tx = proc_get_ts(ptp, port, ptp_data.ident.mType,
+			ptp_data.ident.seqId,
+			ptp_data.ident.portId.clockIdentity.addr,
+			NULL, 0);
+		if (!tx)
+			break;
+		ptp_data.ts.sec = tx->ts.r.sec;
+		ptp_data.ts.nsec = tx->ts.r.nsec;
+		tx->ts.timestamp = 0;
+		tx->req_time = 0;
+		err = copy_to_user(ifr->ifr_data, &ptp_data, sizeof(ptp_data));
+		break;
+	case PTP_GET_RX_TIMESTAMP:
+		if (copy_from_user(&ptp_data, ifr->ifr_data, sizeof(ptp_data)))
+			return -EFAULT;
+
+		ts.timestamp = ptp_data.ts.nsec;
+		if (ts.timestamp)
+			update_ts(&ts, ptp->cur_time.sec);
+		else {
+			ptp->ops->acquire(ptp);
+			ptp->reg->get_time(ptp, &ts.t);
+			ptp->ops->release(ptp);
+		}
+		ptp_data.ts.sec = ts.t.sec;
+		ptp_data.ts.nsec = ts.t.nsec;
+		err = copy_to_user(ifr->ifr_data, &ptp_data, sizeof(ptp_data));
+		break;
+	case PTP_GET_TIME:
+	{
+		struct timespec64 ts;
+		struct ksz_ptp_time cur_time;
+		struct ksz_ptp_time sys_time;
+
+		ts = ktime_to_timespec64(ktime_get_real());
+		sys_time.sec = ts.tv_sec;
+		sys_time.nsec = ts.tv_nsec;
+		calc_diff(&ptp->time_diff, &sys_time, &cur_time);
+		ptp_time.sec = cur_time.sec;
+		ptp_time.nsec = cur_time.nsec;
+		err = proc_ptp_hw_access(ptp,
+			DEV_CMD_GET, DEV_PTP_CLK, 0,
+			&clk_opt, sizeof(clk_opt), NULL, &output,
+			true);
+		if (err)
+			break;
+		ptp_time.sec = clk_opt.sec;
+		ptp_time.nsec = clk_opt.nsec;
+		err = copy_to_user(ifr->ifr_data, &ptp_time, sizeof(ptp_time));
+		break;
+	}
+	case PTP_SET_TIME:
+		if (copy_from_user(&ptp_time, ifr->ifr_data, sizeof(ptp_time)))
+			return -EFAULT;
+		output = 0;
+		clk_opt.sec = (u32) ptp_time.sec;
+		clk_opt.nsec = ptp_time.nsec;
+		err = proc_ptp_hw_access(ptp,
+			DEV_CMD_PUT, DEV_PTP_CLK, output,
+			&clk_opt, sizeof(clk_opt), NULL, &output,
+			true);
+		break;
+	case PTP_ADJ_TIME:
+		if (copy_from_user(&scaled_nsec, ifr->ifr_data, sizeof(s64)))
+			return -EFAULT;
+		convert_scaled_nsec(scaled_nsec, SCALED_NANOSEC_S,
+			&ptp->adjust_sec, &ptp->adjust_offset);
+		if (ptp->adjust_offset < 0 || ptp->adjust_sec < 0) {
+			output = 1;
+			ptp->adjust_sec = -ptp->adjust_sec;
+			ptp->adjust_offset = -ptp->adjust_offset;
+		} else
+			output = 2;
+		clk_opt.sec = (u32) ptp->adjust_sec;
+		clk_opt.nsec = ptp->adjust_offset;
+		clk_opt.interval = 0;
+		ptp->adjust_sec = 0;
+		ptp->adjust_offset = 0;
+		err = proc_ptp_hw_access(ptp,
+			DEV_CMD_PUT, DEV_PTP_CLK, output,
+			&clk_opt, sizeof(clk_opt), NULL, &output,
+			true);
+		break;
+	case PTP_SET_ADJ:
+		if (copy_from_user(&drift, ifr->ifr_data, sizeof(drift)))
+			return -EFAULT;
+		output = 1;
+		clk_opt.sec = clk_opt.nsec = 0;
+		clk_opt.drift = drift;
+		clk_opt.interval = NANOSEC_IN_SEC;
+		err = proc_ptp_hw_access(ptp,
+			DEV_CMD_PUT, DEV_PTP_CLK, output,
+			&clk_opt, sizeof(clk_opt), NULL, &output,
+			true);
+		break;
+	case PTP_GET_ADJ:
+		drift = ptp->drift;
+		err = copy_to_user(ifr->ifr_data, &drift, sizeof(drift));
+		break;
+	case PTP_CLEANUP_TS:
+		break;
+	case PTP_SET_FIPER_ALARM:
+		break;
+	default:
+		err = -EOPNOTSUPP;
+	}
+	return err;
+}
+
+static int ptp_dev_req(struct ptp_info *ptp, char *arg,
+	struct file_dev_info *info)
+{
+	struct ksz_request *req = (struct ksz_request *) arg;
+	int len;
+	int maincmd;
+	int port;
+	int req_size;
+	int subcmd;
+	int output;
+	u8 data[PARAM_DATA_SIZE];
+	struct file_dev_info *dev;
+	int err = 0;
+	int result = 0;
+
+	/* Assume success. */
+	result = DEV_IOC_OK;
+
+	/* Check request size. */
+	__get_user(req_size, &req->size);
+	if (chk_ioctl_size(req_size, SIZEOF_ksz_request, 0, &req_size,
+			&result, NULL, NULL))
+		goto dev_ioctl_resp;
+
+	err = 0;
+	__get_user(maincmd, &req->cmd);
+	__get_user(subcmd, &req->subcmd);
+	__get_user(output, &req->output);
+	len = req_size - SIZEOF_ksz_request;
+	switch (maincmd) {
+	case DEV_CMD_INFO:
+		switch (subcmd) {
+		case DEV_INFO_INIT:
+			req_size = SIZEOF_ksz_request + 4;
+			if (len >= 4) {
+				data[0] = 'M';
+				data[1] = 'i';
+				data[2] = 'c';
+				data[3] = 'r';
+				data[4] = ptp->version;
+				data[5] = ptp->ports;
+				if (!access_ok(req->param.data, 6) ||
+						copy_to_user(req->param.data,
+						data, 6)) {
+					err = -EFAULT;
+					goto dev_ioctl_done;
+				}
+				result = proc_ptp_hw_access(ptp,
+					maincmd, subcmd, 0,
+					data, 6, info, &output,
+					true);
+				__put_user(ptp->drift, &req->output);
+			} else
+				result = DEV_IOC_INVALID_LEN;
+			break;
+		case DEV_INFO_EXIT:
+			result = proc_ptp_hw_access(ptp,
+				maincmd, subcmd, 0,
+				data, 0, info, &output,
+				true);
+			fallthrough;
+
+		case DEV_INFO_QUIT:
+			if (!info)
+				break;
+			data[0] = 0xF0;
+			dev = find_minor_dev(info);
+			if (dev)
+				file_dev_setup_msg(dev, data, 4, NULL, NULL);
+			file_dev_setup_msg(info, data, 4, NULL, NULL);
+			break;
+		case DEV_INFO_RESET:
+			if (output < 3) {
+				result = proc_ptp_hw_access(ptp,
+					maincmd, subcmd, output,
+					data, 0, info, &output,
+					false);
+			} else
+				result = -EINVAL;
+			break;
+		default:
+			result = DEV_IOC_INVALID_CMD;
+			break;
+		}
+		break;
+	case DEV_CMD_PUT:
+		switch (subcmd) {
+		case DEV_PTP_CFG:
+			if (chk_ioctl_size(len, sizeof(struct ptp_cfg_options),
+					SIZEOF_ksz_request,
+					&req_size, &result, &req->param, data))
+				goto dev_ioctl_resp;
+			result = proc_ptp_hw_access(ptp,
+				maincmd, subcmd, 0,
+				data, len, info, &output,
+				false);
+			break;
+		case DEV_PTP_TEVT:
+			if (chk_ioctl_size(len, sizeof(struct ptp_tsi_options),
+					SIZEOF_ksz_request,
+					&req_size, &result, &req->param, data))
+				goto dev_ioctl_resp;
+			if (!info) {
+				err = -EFAULT;
+				goto dev_ioctl_done;
+			}
+			result = proc_ptp_hw_access(ptp,
+				maincmd, subcmd, 0,
+				data, len, info, &output,
+				true);
+			__put_user(output, &req->output);
+			break;
+		case DEV_PTP_TOUT:
+			if (chk_ioctl_size(len, sizeof(struct ptp_tso_options),
+					SIZEOF_ksz_request,
+					&req_size, &result, &req->param, data))
+				goto dev_ioctl_resp;
+			if (!info) {
+				err = -EFAULT;
+				goto dev_ioctl_done;
+			}
+			result = proc_ptp_hw_access(ptp,
+				maincmd, subcmd, 0,
+				data, len, info, &output,
+				true);
+			__put_user(output, &req->output);
+			break;
+		case DEV_PTP_CASCADE:
+			if (chk_ioctl_size(len, sizeof(struct ptp_tso_options),
+					SIZEOF_ksz_request,
+					&req_size, &result, &req->param, data))
+				goto dev_ioctl_resp;
+			result = proc_ptp_hw_access(ptp,
+				maincmd, subcmd, output,
+				data, len, info, &output,
+				true);
+			__put_user(output, &req->output);
+			break;
+		case DEV_PTP_CLK:
+			if (chk_ioctl_size(len, sizeof(struct ptp_clk_options),
+					SIZEOF_ksz_request,
+					&req_size, &result, &req->param, data))
+				goto dev_ioctl_resp;
+			result = proc_ptp_hw_access(ptp,
+				maincmd, subcmd, output,
+				data, len, info, &output,
+				true);
+			break;
+		case DEV_PTP_DELAY:
+			if (chk_ioctl_size(len,
+					sizeof(struct ptp_delay_values),
+					SIZEOF_ksz_request,
+					&req_size, &result, &req->param, data))
+				goto dev_ioctl_resp;
+			__get_user(port, &req->output);
+			result = proc_ptp_hw_access(ptp,
+				maincmd, subcmd, output,
+				data, len, info, &output,
+				false);
+			break;
+		case DEV_PTP_REG:
+			result = proc_ptp_hw_access(ptp,
+				maincmd, subcmd, output,
+				data, len, info, &output,
+				false);
+			break;
+		case DEV_PTP_IDENTITY:
+			if (chk_ioctl_size(len,
+					sizeof(struct ptp_clock_identity),
+					SIZEOF_ksz_request,
+					&req_size, &result, &req->param, data))
+				goto dev_ioctl_resp;
+			memcpy(&ptp->clockIdentity, data,
+				sizeof(struct ptp_clock_identity));
+			break;
+		case DEV_PTP_PEER_DELAY:
+			if (chk_ioctl_size(len,
+					sizeof(struct ptp_delay_values),
+					SIZEOF_ksz_request,
+					&req_size, &result, &req->param, data))
+				goto dev_ioctl_resp;
+			__get_user(port, &req->output);
+			result = proc_ptp_hw_access(ptp,
+				maincmd, subcmd, output,
+				data, len, info, &output,
+				false);
+			break;
+		case DEV_PTP_UTC_OFFSET:
+			ptp->utc_offset = output;
+			break;
+		default:
+			result = DEV_IOC_INVALID_CMD;
+			break;
+		}
+		break;
+	case DEV_CMD_GET:
+		switch (subcmd) {
+		case DEV_PTP_CFG:
+			if (chk_ioctl_size(len, sizeof(struct ptp_cfg_options),
+					SIZEOF_ksz_request,
+					&req_size, &result, NULL, NULL))
+				goto dev_ioctl_resp;
+			result = proc_ptp_hw_access(ptp,
+				maincmd, subcmd, 0,
+				data, len, info, &output,
+				true);
+			if (!access_ok(req->param.data,
+					sizeof(struct ptp_cfg_options)) ||
+					copy_to_user(req->param.data, data,
+					sizeof(struct ptp_cfg_options))) {
+				err = -EFAULT;
+				goto dev_ioctl_done;
+			}
+			break;
+		case DEV_PTP_TEVT:
+			if (chk_ioctl_size(len, sizeof(struct ptp_tsi_info),
+					SIZEOF_ksz_request,
+					&req_size, &result, &req->param, data))
+				goto dev_ioctl_resp;
+			if (!info) {
+				err = -EFAULT;
+				goto dev_ioctl_done;
+			}
+			if (output)
+				result = proc_ptp_hw_access(ptp,
+					maincmd, subcmd, output,
+					data, len, info, &output,
+					false);
+			else
+				result = proc_dev_get_event(info, data);
+			break;
+		case DEV_PTP_TOUT:
+			if (chk_ioctl_size(len, sizeof(struct ptp_tso_options),
+					SIZEOF_ksz_request,
+					&req_size, &result, &req->param, data))
+				goto dev_ioctl_resp;
+			result = proc_ptp_get_output(ptp, data);
+			output = *((int *) data);
+			__put_user(output, &req->output);
+			break;
+		case DEV_PTP_CLK:
+			if (chk_ioctl_size(len, sizeof(struct ptp_clk_options),
+					SIZEOF_ksz_request,
+					&req_size, &result, NULL, NULL))
+				goto dev_ioctl_resp;
+			result = proc_ptp_hw_access(ptp,
+				maincmd, subcmd, 0,
+				data, len, info, &output,
+				true);
+			if (!access_ok(req->param.data,
+					sizeof(struct ptp_clk_options)) ||
+					copy_to_user(req->param.data, data,
+					sizeof(struct ptp_clk_options))) {
+				err = -EFAULT;
+				goto dev_ioctl_done;
+			}
+			break;
+		case DEV_PTP_DELAY:
+			if (chk_ioctl_size(len,
+					sizeof(struct ptp_delay_values),
+					SIZEOF_ksz_request,
+					&req_size, &result, NULL, NULL))
+				goto dev_ioctl_resp;
+			__get_user(port, &req->output);
+			result = proc_ptp_hw_access(ptp,
+				maincmd, subcmd, output,
+				data, len, info, &output,
+				true);
+			if (!access_ok(req->param.data,
+					sizeof(struct ptp_delay_values)) ||
+					copy_to_user(req->param.data, data,
+					sizeof(struct ptp_delay_values))) {
+				err = -EFAULT;
+				goto dev_ioctl_done;
+			}
+			break;
+		case DEV_PTP_REG:
+			result = proc_ptp_hw_access(ptp,
+				maincmd, subcmd, output,
+				data, len, info, &output,
+				true);
+			__put_user(output, &req->output);
+			break;
+		case DEV_PTP_IDENTITY:
+			if (!access_ok(req->param.data,
+					sizeof(struct ptp_clock_identity)) ||
+					copy_to_user(req->param.data,
+					&ptp->clockIdentity,
+					sizeof(struct ptp_clock_identity))) {
+				err = -EFAULT;
+				goto dev_ioctl_done;
+			}
+			break;
+		case DEV_PTP_PEER_DELAY:
+			if (chk_ioctl_size(len,
+					sizeof(struct ptp_delay_values),
+					SIZEOF_ksz_request,
+					&req_size, &result, NULL, NULL))
+				goto dev_ioctl_resp;
+			__get_user(port, &req->output);
+			result = proc_ptp_hw_access(ptp,
+				maincmd, subcmd, output,
+				data, len, info, &output,
+				true);
+			if (!access_ok(req->param.data,
+					sizeof(struct ptp_delay_values)) ||
+					copy_to_user(req->param.data, data,
+					sizeof(struct ptp_delay_values))) {
+				err = -EFAULT;
+				goto dev_ioctl_done;
+			}
+			break;
+		case DEV_PTP_UTC_OFFSET:
+			__put_user(ptp->utc_offset, &req->output);
+			break;
+		case DEV_PTP_TIMESTAMP:
+			if (chk_ioctl_size(len,
+					sizeof(struct ptp_ts_options),
+					SIZEOF_ksz_request,
+					&req_size, &result, &req->param, data))
+				goto dev_ioctl_resp;
+			result = proc_ptp_get_timestamp(ptp, data, info);
+			if (result)
+				goto dev_ioctl_resp;
+			if (!access_ok(req->param.data,
+					sizeof(struct ptp_ts_options)) ||
+					copy_to_user(req->param.data, data,
+					sizeof(struct ptp_ts_options))) {
+				err = -EFAULT;
+				goto dev_ioctl_done;
+			}
+			break;
+		}
+		break;
+	default:
+		result = DEV_IOC_INVALID_CMD;
+		break;
+	}
+
+dev_ioctl_resp:
+	__put_user(req_size, &req->size);
+	__put_user(result, &req->result);
+
+	/* Return ERESTARTSYS so that the system call is called again. */
+	if (result < 0)
+		err = result;
+
+dev_ioctl_done:
+	return err;
+}  /* ptp_dev_req */
+
+static long ptp_dev_ioctl(struct file *filp, unsigned int cmd,
+	unsigned long arg)
+{
+	struct file_dev_info *info = (struct file_dev_info *)
+		filp->private_data;
+	struct ptp_info *ptp = info->dev;
+	int err = 0;
+
+	if (_IOC_TYPE(cmd) != DEV_IOC_MAGIC)
+		return -ENOTTY;
+	if (_IOC_NR(cmd) > DEV_IOC_MAX)
+		return -ENOTTY;
+	if (_IOC_DIR(cmd) & _IOC_READ)
+		err = !access_ok((void *) arg, _IOC_SIZE(cmd));
+	else if (_IOC_DIR(cmd) & _IOC_WRITE)
+		err = !access_ok((void *) arg, _IOC_SIZE(cmd));
+	if (err) {
+		printk(KERN_ALERT "err fault\n");
+		return -EFAULT;
+	}
+	if (down_interruptible(&info->sem))
+		return -ERESTARTSYS;
+
+	err = ptp_dev_req(ptp, (char *) arg, info);
+	up(&info->sem);
+	return err;
+}  /* ptp_dev_ioctl */
+
+static ssize_t ptp_dev_read(struct file *filp, char *buf, size_t count,
+	loff_t *offp)
+{
+	struct file_dev_info *info = (struct file_dev_info *)
+		filp->private_data;
+	ssize_t result = 0;
+	int rc;
+
+	if (!info->read_len) {
+		*offp = 0;
+		rc = wait_event_interruptible(info->wait_msg,
+			0 != info->read_len);
+
+		/* Cannot continue if ERESTARTSYS. */
+		if (rc < 0)
+			return 0;
+	}
+
+	if (down_interruptible(&info->sem))
+		return -ERESTARTSYS;
+
+	mutex_lock(&info->lock);
+	if (*offp >= info->read_len) {
+		info->read_len = 0;
+		count = 0;
+		*offp = 0;
+		goto dev_read_done;
+	}
+
+	if (*offp + count > info->read_len) {
+		count = info->read_len - *offp;
+		info->read_len = 0;
+	}
+
+	if (copy_to_user(buf, &info->read_buf[*offp], count)) {
+		result = -EFAULT;
+		goto dev_read_done;
+	}
+	if (info->read_len)
+		*offp += count;
+	else
+		*offp = 0;
+	result = count;
+
+dev_read_done:
+	mutex_unlock(&info->lock);
+	up(&info->sem);
+	return result;
+}  /* ptp_dev_read */
+
+static ssize_t ptp_dev_write(struct file *filp, const char *buf, size_t count,
+	loff_t *offp)
+{
+	struct file_dev_info *info = (struct file_dev_info *)
+		filp->private_data;
+	ssize_t result = 0;
+	size_t size;
+	int rc;
+	u8 cmd;
+
+	if (!count)
+		return result;
+
+	if (down_interruptible(&info->sem))
+		return -ERESTARTSYS;
+
+	if (*offp >= info->write_len) {
+		result = -ENOSPC;
+		goto dev_write_done;
+	}
+	if (*offp + count > info->write_len)
+		count = info->write_len - *offp;
+	if (copy_from_user(info->write_buf, buf, count)) {
+		result = -EFAULT;
+		goto dev_write_done;
+	}
+	cmd = info->write_buf[0] & 0xf0;
+	switch (cmd) {
+	case TSM_CMD_GET_GPS_TS:
+		size = sizeof(struct tsm_get_gps);
+		break;
+	case TSM_CMD_DB_GET_TIME:
+		size = sizeof(struct tsm_get_time);
+		break;
+	case TSM_CMD_DB_GET:
+		size = sizeof(struct tsm_db);
+		break;
+	case TSM_CMD_CNF_SET:
+		size = sizeof(struct tsm_cfg);
+		break;
+	case TSM_CMD_CLOCK_SET:
+		size = sizeof(struct tsm_clock_set);
+		break;
+	case TSM_CMD_CLOCK_CORRECT:
+		size = sizeof(struct tsm_clock_correct);
+		break;
+	default:
+		dbg_msg("tsm: %x\n", info->write_buf[0]);
+		result = count;
+		goto dev_write_done;
+	}
+	if (count < size) {
+		result = -EFAULT;
+		goto dev_write_done;
+	}
+	result = size;
+	rc = parse_tsm_msg(info, count);
+	if (rc)
+		result = rc;
+
+dev_write_done:
+	up(&info->sem);
+	return result;
+}  /* ptp_dev_write */
+
+static const struct file_operations ptp_dev_fops = {
+	.read		= ptp_dev_read,
+	.write		= ptp_dev_write,
+	.unlocked_ioctl	= ptp_dev_ioctl,
+	.open		= ptp_dev_open,
+	.release	= ptp_dev_release,
+};
+
+static struct class *ptp_class[MAX_SW_DEVICES];
+
+static int init_ptp_device(int id, int dev_major, char *dev_name,
+			   char *minor_name)
+{
+	int result;
+
+	result = register_chrdev(dev_major, dev_name, &ptp_dev_fops);
+	if (result < 0) {
+		printk(KERN_WARNING "%s: can't get major %d\n", dev_name,
+			dev_major);
+		return result;
+	}
+	if (0 == dev_major)
+		dev_major = result;
+	ptp_class[id] = class_create(THIS_MODULE, dev_name);
+	if (IS_ERR(ptp_class[id])) {
+		unregister_chrdev(dev_major, dev_name);
+		return -ENODEV;
+	}
+	device_create(ptp_class[id], NULL, MKDEV(dev_major, 0), NULL,
+		      dev_name);
+	device_create(ptp_class[id], NULL, MKDEV(dev_major, 1), NULL,
+		      minor_name);
+	return dev_major;
+}  /* init_ptp_device */
+
+static void exit_ptp_device(int id, int dev_major, char *dev_name)
+{
+	device_destroy(ptp_class[id], MKDEV(dev_major, 1));
+	device_destroy(ptp_class[id], MKDEV(dev_major, 0));
+	class_destroy(ptp_class[id]);
+	unregister_chrdev(dev_major, dev_name);
+}  /* exit_ptp_device */
+
+#if 0
+static void ptp_set_identity(struct ptp_info *ptp, u8 *addr)
+{
+	memcpy(&ptp->clockIdentity.addr[0], &addr[0], 3);
+	ptp->clockIdentity.addr[3] = 0xFF;
+	ptp->clockIdentity.addr[4] = 0xFE;
+	memcpy(&ptp->clockIdentity.addr[5], &addr[3], 3);
+}  /* ptp_set_identity */
+#endif
+
+static void ptp_init(struct ptp_info *ptp)
+{
+	struct ksz_sw *sw = container_of(ptp, struct ksz_sw, ptp_hw);
+	int i;
+
+	ptp->utc_offset = CURRENT_UTC_OFFSET;
+	ptp->get_delay = 100000;
+	ptp->set_delay = 100000;
+	ptp->delay_ticks = 1;
+#ifdef PTP_SPI
+	ptp->delay_ticks = 2;
+	ptp->access = create_singlethread_workqueue("ptp_access");
+#endif
+	init_ptp_work(ptp);
+	mutex_init(&ptp->lock);
+	init_waitqueue_head(&ptp->wait_ts);
+	init_waitqueue_head(&ptp->wait_intr);
+	INIT_WORK(&ptp->adj_clk, adj_clock);
+	INIT_DELAYED_WORK(&ptp->check_pps, ptp_check_pps);
+	INIT_DELAYED_WORK(&ptp->update_sec, ptp_update_sec);
+
+	ptp->ports = MAX_PTP_PORT;
+	if (!ptp->get_clk_cnt)
+		ptp->get_clk_cnt = _get_clk_cnt;
+	if (!ptp->test_access_time)
+		ptp->test_access_time = _test_access_time;
+
+	ptp->mode = PTP_ENABLE |
+		PTP_IPV4_UDP_ENABLE |
+		PTP_1STEP;
+	ptp->mode |= PTP_IPV6_UDP_ENABLE;
+	ptp->mode |= PTP_ETH_ENABLE;
+	ptp->cfg = 0;
+	ptp->cfg |= PTP_DOMAIN_CHECK;
+	ptp->cfg |= PTP_PDELAY_CHECK | PTP_DELAY_CHECK;
+	ptp->cfg |= PTP_UNICAST_ENABLE;
+	if (ptp->version >= 1) {
+		ptp->cfg |= PTP_UDP_CHECKSUM;
+		ptp->cfg |= PTP_SYNC_CHECK;
+	}
+	ptp->def_mode = ptp->mode;
+	ptp->def_cfg = ptp->cfg;
+	ptp->trig_intr = 0xfff;
+	ptp->ts_intr = (TS_PORT2_INT_XDELAY | TS_PORT1_INT_XDELAY);
+
+	ptp->gps_tsi = MAX_TIMESTAMP_UNIT;
+	ptp->gps_gpi = DEFAULT_GPS_GPI;
+	ptp->pps_gpo = DEFAULT_PPS_GPO;
+	ptp->pps_tsi = DEFAULT_PPS_TSI;
+	ptp->pps_tso = DEFAULT_PPS_TSO;
+	ptp->mhz_gpo = DEFAULT_MHZ_GPO;
+	ptp->mhz_tso = DEFAULT_MHZ_TSO;
+
+	for (i = 0; i < MAX_TIMESTAMP_UNIT - 1; i++)
+		ptp->events[i].max = 2;
+	ptp->events[i].max = MAX_TIMESTAMP_EVENT_UNIT;
+
+	sprintf(ptp->dev_name[0], "ptp_dev");
+	sprintf(ptp->dev_name[1], "ptp_event");
+	if (sw->id) {
+		sprintf(ptp->dev_name[0], "ptp_dev_%u", sw->id);
+		sprintf(ptp->dev_name[1], "ptp_event_%u", sw->id);
+	}
+	ptp->dev_major = init_ptp_device(sw->id, 0, ptp->dev_name[0],
+		ptp->dev_name[1]);
+	ptp_majors[sw->id].dev = ptp;
+	ptp_majors[sw->id].major = ptp->dev_major;
+
+#ifdef CONFIG_PTP_1588_CLOCK
+	micrel_ptp_probe(ptp);
+#endif
+}  /* ptp_init */
+
+static void ptp_exit(struct ptp_info *ptp)
+{
+	struct ksz_sw *sw = container_of(ptp, struct ksz_sw, ptp_hw);
+
+	exit_ptp_work(ptp);
+#ifdef PTP_SPI
+	if (ptp->access) {
+		destroy_workqueue(ptp->access);
+		ptp->access = NULL;
+	}
+#endif
+	if (ptp->dev_major >= 0)
+		exit_ptp_device(sw->id, ptp->dev_major, ptp->dev_name[0]);
+
+#ifdef CONFIG_PTP_1588_CLOCK
+	micrel_ptp_remove(ptp);
+#endif
+}  /* ptp_exit */
+
+enum {
+	PROC_SET_PTP_FEATURES,
+	PROC_SET_PTP_OVERRIDES,
+	PROC_SET_PTP_VID,
+	PROC_SET_PTP_GPIO_1,
+	PROC_SET_PTP_GPIO_2,
+};
+
+static ssize_t sysfs_ptp_read(struct ptp_info *ptp, int proc_num, ssize_t len,
+	char *buf)
+{
+	switch (proc_num) {
+	case PROC_SET_PTP_FEATURES:
+		len += sprintf(buf + len, "%08x:\n", ptp->features);
+		len += sprintf(buf + len, "\t%08x = adjust hack\n",
+			PTP_ADJ_HACK);
+		len += sprintf(buf + len, "\t%08x = adjust sec\n",
+			PTP_ADJ_SEC);
+		len += sprintf(buf + len, "\t%08x = pdelay hack\n",
+			PTP_PDELAY_HACK);
+		break;
+	case PROC_SET_PTP_OVERRIDES:
+		len += sprintf(buf + len, "%08x:\n", ptp->overrides);
+		len += sprintf(buf + len, "\t%08x = PTP port forwarding\n",
+			PTP_PORT_FORWARD);
+		len += sprintf(buf + len, "\t%08x = PTP port TX forwarding\n",
+			PTP_PORT_TX_FORWARD);
+		len += sprintf(buf + len, "\t%08x = PTP verify timestamp\n",
+			PTP_VERIFY_TIMESTAMP);
+		len += sprintf(buf + len, "\t%08x = PTP zero reserved field\n",
+			PTP_ZERO_RESERVED_FIELD);
+		len += sprintf(buf + len, "\t%08x = PTP check system time\n",
+			PTP_CHECK_SYS_TIME);
+		break;
+	case PROC_SET_PTP_VID:
+		len += sprintf(buf + len, "0x%04x\n", ptp->vid);
+		break;
+	}
+	return len;
+}
+
+static void sysfs_ptp_write(struct ptp_info *ptp, int proc_num, int num,
+	const char *buf)
+{
+	int changes;
+
+	switch (proc_num) {
+	case PROC_SET_PTP_FEATURES:
+		if ('0' != buf[0] || 'x' != buf[1])
+			sscanf(buf, "%x", &num);
+		changes = ptp->features ^ num;
+		ptp->features = num;
+#ifdef PTP_PROCESS
+		if ((changes & (PTP_SYNT | PTP_SIM_2_STEP))) {
+#ifdef PTP_2_STEP
+			if (num & PTP_SIM_2_STEP) {
+				ptp->sim_2_step = true;
+				ptp->mode &= ~PTP_1STEP;
+			} else {
+				ptp->sim_2_step = false;
+				ptp->mode |= PTP_1STEP;
+			}
+#endif
+			if (num & (PTP_SYNT | PTP_SIM_2_STEP)) {
+				ptp_init_state(ptp);
+				if (num & PTP_SYNT) {
+					ptp->sim = 1;
+					ptp->I = 0;
+					ptp->KP = 50;
+					ptp->KI = 5;
+				}
+			} else {
+				ptp_exit_state(ptp);
+				dbg_msg("exit ptp\n");
+			}
+		}
+#endif
+		break;
+	case PROC_SET_PTP_OVERRIDES:
+		if ('0' != buf[0] || 'x' != buf[1])
+			sscanf(buf, "%x", &num);
+		changes = ptp->overrides ^ num;
+		if ((changes & PTP_CHECK_SYS_TIME) &&
+				(ptp->overrides & PTP_CHECK_SYS_TIME))
+			ptp->first_sec = 0;
+		ptp->overrides = num;
+		break;
+	case PROC_SET_PTP_VID:
+		ptp->vid = num;
+		break;
+	}
+}
+
+static struct ptp_reg_ops ptp_reg_ops = {
+	.get_time		= get_ptp_time,
+	.set_time		= set_ptp_time,
+	.adjust_time		= adjust_ptp_time,
+	.adjust_sync_time	= adjust_sync_time,
+
+	.rx_off			= ptp_rx_off,
+	.rx_restart		= ptp_rx_restart,
+	.rx_event		= ptp_rx_event,
+	.rx_cascade_event	= ptp_rx_cascade_event,
+	.read_event		= ptp_read_event,
+
+	.tx_off			= ptp_tx_off,
+	.tx_event		= ptp_tx_event,
+	.pps_event		= ptp_pps_event,
+	.ptp_10MHz		= ptp_10MHz,
+	.tx_cascade		= ptp_tx_cascade,
+
+	.start			= ptp_start,
+};
+
+static struct ptp_ops ptp_ops = {
+	.acquire		= ptp_acquire,
+	.release		= ptp_release,
+
+	.init			= ptp_init,
+	.exit			= ptp_exit,
+
+	.stop			= ptp_stop,
+
+	.check_msg		= check_ptp_msg,
+	.update_msg		= update_ptp_msg,
+	.get_rx_tstamp		= get_rx_tstamp,
+	.get_tx_tstamp		= get_tx_tstamp,
+	.hwtstamp_ioctl		= ptp_hwtstamp_ioctl,
+	.ixxat_ioctl		= ixxat_ptp_ioctl,
+	.dev_req		= ptp_dev_req,
+	.proc_intr		= proc_ptp_intr,
+	.get_ts_info		= ptp_get_ts_info,
+
+	.sysfs_read		= sysfs_ptp_read,
+	.sysfs_write		= sysfs_ptp_write,
+
+	.drop_pkt		= ptp_drop_pkt,
+};
+
diff -Napur linux-linux4microchip-2022.10-orig/drivers/net/ethernet/micrel/ksz_ptp.h linux-linux4microchip-2022.10/drivers/net/ethernet/micrel/ksz_ptp.h
--- linux-linux4microchip-2022.10-orig/drivers/net/ethernet/micrel/ksz_ptp.h	1969-12-31 16:00:00.000000000 -0800
+++ linux-linux4microchip-2022.10/drivers/net/ethernet/micrel/ksz_ptp.h	2023-04-25 16:13:55.052163480 -0700
@@ -0,0 +1,1055 @@
+/**
+ * Microchip PTP common header
+ *
+ * Copyright (c) 2015-2019 Microchip Technology Inc.
+ *	Tristram Ha <Tristram.Ha@microchip.com>
+ *
+ * Copyright (c) 2010-2015 Micrel, Inc.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ */
+
+#ifndef KSZ_PTP_H
+#define KSZ_PTP_H
+
+#ifndef __KERNEL__
+typedef unsigned char u8;
+typedef unsigned short u16;
+typedef unsigned int u32;
+typedef short s16;
+typedef long long s64;
+typedef unsigned long long u64;
+#endif
+
+struct ksz_ptp_time {
+	int sec;
+	int nsec;
+};
+
+struct ptp_utime {
+	u32 sec;
+	u32 nsec;
+};
+
+struct ptp_ts {
+	struct ptp_utime r;
+	struct ptp_utime t;
+	u32 timestamp;
+};
+
+struct ptp_second {
+	u16 hi;
+	u32 lo;
+} __packed;
+
+struct ptp_timestamp {
+	struct ptp_second sec;
+	u32 nsec;
+} __packed;
+
+#define SCALED_NANOSEC_S		16
+#define SCALED_NANOSEC_MULT		(1 << SCALED_NANOSEC_S)
+
+struct ptp_scaled_ns {
+	int hi;
+	s64 lo;
+} __packed;
+
+struct ptp_correction {
+	int scaled_nsec_hi;
+	int scaled_nsec_lo;
+} __packed;
+
+struct ptp_clock_identity {
+	u8 addr[8];
+};
+
+struct ptp_port_identity {
+	struct ptp_clock_identity clockIdentity;
+	u16 port;
+} __packed;
+
+struct ptp_clock_quality {
+	u8 clockClass;
+	u8 clockAccuracy;
+	u16 offsetScaledLogVariance;
+} __packed;
+
+struct ptp_port_address {
+	u16 networkProtocol;
+	u16 addressLength;
+	u8 addressField[1];
+} __packed;
+
+struct ptp_text {
+	u8 lengthField;
+	u8 textField[1];
+} __packed;
+
+#define SYNC_MSG			0x0
+#define DELAY_REQ_MSG			0x1
+#define PDELAY_REQ_MSG			0x2
+#define PDELAY_RESP_MSG			0x3
+#define FOLLOW_UP_MSG			0x8
+#define DELAY_RESP_MSG			0x9
+#define PDELAY_RESP_FOLLOW_UP_MSG	0xA
+#define ANNOUNCE_MSG			0xB
+#define SIGNALING_MSG			0xC
+#define MANAGEMENT_MSG			0xD
+
+struct ptp_msg_hdr {
+#ifdef __BIG_ENDIAN_BITFIELD
+	u8 transportSpecific:4;
+	u8 messageType:4;
+	u8 reserved1:4;
+	u8 versionPTP:4;
+#else
+	u8 messageType:4;
+	u8 transportSpecific:4;
+	u8 versionPTP:4;
+	u8 reserved1:4;
+#endif
+	u16 messageLength;
+	u8 domainNumber;
+	u8 reserved2;
+	union {
+		struct {
+#ifdef __BIG_ENDIAN_BITFIELD
+			u8 reservedFlag7:1;
+			u8 profileSpecific1:1;
+			u8 profileSpecific2:1;
+			u8 reservedFlag4:1;
+			u8 reservedFlag3:1;
+			u8 unicastFlag:1;
+			u8 twoStepFlag:1;
+			u8 alternateMasterFlag:1;
+			u8 reservedFlag6:1;
+			u8 reservedFlag5:1;
+			u8 frequencyTraceable:1;
+			u8 timeTraceable:1;
+			u8 ptpTimescale:1;
+			u8 utcOffsetValid:1;
+			u8 leap59:1;
+			u8 leap61:1;
+#else
+			u8 alternateMasterFlag:1;
+			u8 twoStepFlag:1;
+			u8 unicastFlag:1;
+			u8 reservedFlag3:1;
+			u8 reservedFlag4:1;
+			u8 profileSpecific1:1;
+			u8 profileSpecific2:1;
+			u8 reservedFlag7:1;
+			u8 leap61:1;
+			u8 leap59:1;
+			u8 utcOffsetValid:1;
+			u8 ptpTimescale:1;
+			u8 timeTraceable:1;
+			u8 frequencyTraceable:1;
+			u8 reservedFlag5:1;
+			u8 reservedFlag6:1;
+#endif
+		} __packed flag;
+		u16 data;
+	} __packed flagField;
+	struct ptp_correction correctionField;
+	u32 reserved3;
+	struct ptp_port_identity sourcePortIdentity;
+	u16 sequenceId;
+	u8 controlField;
+	char logMessageInterval;
+} __packed;
+
+struct ptp_msg_sync {
+	struct ptp_timestamp originTimestamp;
+} __packed;
+
+struct ptp_msg_follow_up {
+	struct ptp_timestamp preciseOriginTimestamp;
+} __packed;
+
+struct ptp_msg_delay_resp {
+	struct ptp_timestamp receiveTimestamp;
+	struct ptp_port_identity requestingPortIdentity;
+} __packed;
+
+struct ptp_msg_pdelay_req {
+	struct ptp_timestamp originTimestamp;
+	struct ptp_port_identity reserved;
+} __packed;
+
+struct ptp_msg_pdelay_resp {
+	struct ptp_timestamp requestReceiptTimestamp;
+	struct ptp_port_identity requestingPortIdentity;
+} __packed;
+
+struct ptp_msg_pdelay_resp_follow_up {
+	struct ptp_timestamp responseOriginTimestamp;
+	struct ptp_port_identity requestingPortIdentity;
+} __packed;
+
+#define TLV_MANAGEMENT					0x0001
+#define TLV_MANAGEMENT_ERROR_STATUS			0x0002
+#define TLV_ORGANIZATION_EXTENSION			0x0003
+#define TLV_REQUEST_UNICAST_TRANSMISSION		0x0004
+#define TLV_GRANT_UNICAST_TRANSMISSION			0x0005
+#define TLV_CANCEL_UNICAST_TRANSMISSION			0x0006
+#define TLV_ACKNOWLEDGE_CANCEL_UNICAST_TRANSMISSION	0x0007
+#define TLV_PATH_TRACE					0x0008
+#define TLV_ALTERNATE_TIME_OFFSET_INDICATOR		0x0009
+
+struct ptp_tlv {
+	u16 tlvType;
+	u16 lengthField;
+} __packed;
+
+struct ptp_organization_ext_tlv {
+	struct ptp_tlv tlv;
+	u8 organizationId[3];
+	u8 organizationSubType[3];
+	u8 dataField[1];
+} __packed;
+
+struct IEEE_C37_238_data {
+	u16 grandmasterID;
+	u32 grandmasterTimeInaccuracy;
+	u32 networkTimeInaccuracy;
+	u16 reserved;
+} __packed;
+
+struct IEEE_802_1AS_data_1 {
+	int cumulativeScaledRateOffset;
+	u16 gmTimeBaseIndicator;
+	struct ptp_scaled_ns lastGmPhaseChange;
+	int scaledLastGmFreqChange;
+} __packed;
+
+struct IEEE_802_1AS_data_2 {
+	char linkDelayInterval;
+	char timeSyncInterval;
+	char announceInterval;
+	u8 flags;
+	u16 reserved;
+} __packed;
+
+struct ptp_request_unicast_tlv {
+	struct ptp_tlv tlv;
+#ifdef __BIG_ENDIAN_BITFIELD
+	u8 messageType:4;
+	u8 reserved1:4;
+#else
+	u8 reserved1:4;
+	u8 messageType:4;
+#endif
+	char logInterMessagePeriod;
+	u32 durationField;
+} __packed;
+
+struct ptp_grant_unicast_tlv {
+	struct ptp_tlv tlv;
+#ifdef __BIG_ENDIAN_BITFIELD
+	u8 messageType:4;
+	u8 reserved1:4;
+#else
+	u8 reserved1:4;
+	u8 messageType:4;
+#endif
+	char logInterMessagePeriod;
+	u32 durationField;
+	u8 reserved2;
+#ifdef __BIG_ENDIAN_BITFIELD
+	u8 reserved3:7;
+	u8 renewal:1;
+#else
+	u8 renewal:1;
+	u8 reserved3:7;
+#endif
+} __packed;
+
+struct ptp_cancel_unicast_tlv {
+	struct ptp_tlv tlv;
+#ifdef __BIG_ENDIAN_BITFIELD
+	u8 messageType:4;
+	u8 reserved1:4;
+#else
+	u8 reserved1:4;
+	u8 messageType:4;
+#endif
+	u8 reserved2;
+} __packed;
+
+struct ptp_alternate_time_offset_tlv {
+	struct ptp_tlv tlv;
+	u8 keyField;
+	int currentOffset;
+	int jumpSeconds;
+	struct ptp_second timeOfNextJump;
+	struct ptp_text displayName;
+} __packed;
+
+struct ptp_msg_signaling_base {
+	struct ptp_port_identity targetPortIdentity;
+} __packed;
+
+struct ptp_msg_signaling {
+	struct ptp_msg_signaling_base b;
+	union {
+		struct ptp_request_unicast_tlv request[1];
+		struct ptp_grant_unicast_tlv grant[1];
+		struct ptp_cancel_unicast_tlv cancel[1];
+	} tlv;
+} __packed;
+
+#define M_NULL_MANAGEMENT				0x0000
+#define M_CLOCK_DESCRIPTION				0x0001
+#define M_DEFAULT_DATA_SET				0x2000
+#define M_CURRENT_DATA_SET				0x2001
+#define M_PARENT_DATA_SET				0x2002
+#define M_PORT_DATA_SET					0x2004
+#define M_PRIORITY1					0x2005
+#define M_PRIORITY2					0x2006
+#define M_DOMAIN					0x2007
+#define M_SLAVE_ONLY					0x2008
+#define M_VERSION_NUMBER				0x200C
+#define M_ENABLE_PORT					0x200D
+#define M_DISABLE_PORT					0x200E
+#define M_TIME						0x200F
+#define M_UNICAST_NEGOTIATION_ENABLE			0x2014
+#define M_PATH_TRACE_LIST				0x2015
+#define M_PATH_TRACE_ENABLE				0x2016
+#define M_GRANDMASTER_CLUSTER_TABLE			0x2017
+#define M_UNICAST_MASTER_TABLE				0x2018
+#define M_UNICAST_MASTER_MAX_TABLE_SIZE			0x2019
+#define M_ACCEPTABLE_MASTER_TABLE			0x201A
+#define M_ACCEPTABLE_MASTER_TABLE_ENABLED		0x201B
+#define M_ACCEPTABLE_MASTER_MAX_TABLE_SIZE		0x201C
+#define M_ALTERNATE_MASTER				0x201D
+#define M_ALTERNATE_TIME_OFFSET_ENABLE			0x201E
+#define M_ALTERNATE_TIME_OFFSET_NAME			0x201F
+#define M_ALTERNATE_TIME_OFFSET_MAX_KEY			0x2020
+#define M_ALTERNATE_TIME_OFFSET_PROPERTIES		0x2021
+
+struct ptp_management_unicast_negotiation {
+#ifdef __BIG_ENDIAN_BITFIELD
+	u8 reserved1:7;
+	u8 enable:1;
+#else
+	u8 enable:1;
+	u8 reserved1:7;
+#endif
+	u8 reserved2;
+} __packed;
+
+struct ptp_management_unicast_master_table {
+	u8 logQueryInterval;
+	u16 tableSize;
+	struct ptp_port_address unicastMasterTable[1];
+} __packed;
+
+struct ptp_management_unicast_master_max_table_size {
+	u16 maxTableSize;
+} __packed;
+
+struct ptp_management_alternate_time_offset {
+	u8 keyField;
+#ifdef __BIG_ENDIAN_BITFIELD
+	u8 reserved1:7;
+	u8 enable:1;
+#else
+	u8 enable:1;
+	u8 reserved1:7;
+#endif
+} __packed;
+
+struct ptp_management_alternate_time_offset_name {
+	u8 keyField;
+	struct ptp_text displayName;
+} __packed;
+
+struct ptp_management_alternate_time_offset_max_key {
+	u8 keyField;
+	u8 reserved;
+} __packed;
+
+struct ptp_management_alternate_time_offset_properties {
+	u8 keyField;
+	int currentOffset;
+	int jumpSeconds;
+	struct ptp_second timeOfNextJump;
+	u8 reserved;
+} __packed;
+
+struct ptp_management_tlv {
+	struct ptp_tlv tlv;
+	u16 managementId;
+	u8 dataField[1];
+} __packed;
+
+#define M_RESPONSE_TOO_BIG		0x0001
+#define M_NO_SUCH_ID			0x0002
+#define M_WRONG_LENGTH			0x0003
+#define M_WRONG_VALUE			0x0004
+#define M_NOT_SETABLE			0x0005
+#define M_NOT_SUPPORTED			0x0006
+#define M_GENERAL_ERROR			0xFFFE
+
+struct ptp_management_error_tlv {
+	struct ptp_tlv tlv;
+	u16 managementErrorId;
+	u16 managementId;
+	u32 reserved1;
+	u8 data[1];
+} __packed;
+
+#define MANAGEMENT_GET			0
+#define MANAGEMENT_SET			1
+#define MANAGEMENT_RESPONSE		2
+#define MANAGEMENT_COMMAND		3
+#define MANAGEMENT_ACKNOWLEDGE		4
+
+struct ptp_msg_management_base {
+	struct ptp_port_identity targetPortIdentity;
+	u8 startingBoundaryHops;
+	u8 boundaryHops;
+#ifdef __BIG_ENDIAN_BITFIELD
+	u8 reserved1:4;
+	u8 actionField:4;
+#else
+	u8 actionField:4;
+	u8 reserved1:4;
+#endif
+	u8 reserved2;
+} __packed;
+
+struct ptp_msg_management {
+	struct ptp_msg_management_base b;
+	union {
+		struct ptp_management_tlv normal[1];
+		struct ptp_management_error_tlv error[1];
+	} tlv;
+} __packed;
+
+struct ptp_msg_announce {
+	struct ptp_timestamp originTimestamp;
+	s16 currentUtcOffset;
+	u8 reserved;
+	u8 grandmasterPriority1;
+	struct ptp_clock_quality grandmasterClockQuality;
+	u8 grandmasterPriority2;
+	struct ptp_clock_identity grandmasterIdentity;
+	u16 stepsRemoved;
+	u8 timeSource;
+} __packed;
+
+union ptp_msg_data {
+	struct ptp_msg_sync sync;
+	struct ptp_msg_follow_up follow_up;
+	struct ptp_msg_delay_resp delay_resp;
+	struct ptp_msg_pdelay_req pdelay_req;
+	struct ptp_msg_pdelay_resp pdelay_resp;
+	struct ptp_msg_pdelay_resp_follow_up pdelay_resp_follow_up;
+	struct ptp_msg_signaling signaling;
+	struct ptp_msg_management management;
+	struct ptp_msg_announce announce;
+	u8 data[8];
+} __packed;
+
+struct ptp_msg {
+	struct ptp_msg_hdr hdr;
+	union ptp_msg_data data;
+} __packed;
+
+
+struct ptp_id {
+	u16 seq;
+	struct ptp_clock_identity clock;
+	u8 mac[2];
+	u8 msg;
+	u8 port;
+};
+
+struct ptp_cfg_options {
+	u8 master:1;
+	u8 two_step:1;
+	u8 p2p:1;
+	u8 as:1;
+	u8 domain_check:1;
+	u8 udp_csum:1;
+	u8 unicast:1;
+	u8 alternate:1;
+	u8 delay_assoc:1;
+	u8 pdelay_assoc:1;
+	u8 sync_assoc:1;
+	u8 drop_sync:1;
+	u8 priority:1;
+	u8 reserved:3;
+	u8 master_set:1;
+	u8 two_step_set:1;
+	u8 p2p_set:1;
+	u8 as_set:1;
+	u8 domain_check_set:1;
+	u8 udp_csum_set:1;
+	u8 unicast_set:1;
+	u8 alternate_set:1;
+	u8 delay_assoc_set:1;
+	u8 pdelay_assoc_set:1;
+	u8 sync_assoc_set:1;
+	u8 drop_sync_set:1;
+	u8 priority_set:1;
+	u8 reserved_set:2;
+	u8 domain_set:1;
+	u8 domain;
+	u8 reserved3;
+	u32 access_delay;
+} __packed;
+
+#define PTP_CMD_RESP			0x01
+#define PTP_CMD_GET_MSG			0x00
+#define PTP_CMD_GET_OUTPUT		0xE0
+#define PTP_CMD_GET_EVENT		0xF0
+
+#define PTP_CMD_INTR_OPER		0x01
+#define PTP_CMD_SILENT_OPER		0x02
+#define PTP_CMD_ON_TIME			0x04
+#define PTP_CMD_REL_TIME		0x08
+#define PTP_CMD_CLK_OPT			0x10
+#define PTP_CMD_CASCADE_RESET_OPER	0x40
+#define PTP_CMD_CANCEL_OPER		0x80
+
+struct ptp_tsi_info {
+	u8 cmd;
+	u8 unit;
+	u8 event;
+	u8 num;
+	u32 edge;
+	struct ptp_utime t[0];
+} __packed;
+
+struct ptp_tsi_options {
+	u8 tsi;
+	u8 gpi;
+	u8 event;
+	u8 flags;
+	u8 total;
+	u8 reserved[3];
+	u32 timeout;
+} __packed;
+
+struct ptp_tso_options {
+	u8 tso;
+	u8 gpo;
+	u8 event;
+	u8 flags;
+	u8 total;
+	u8 reserved[1];
+	u16 cnt;
+	u32 pulse;
+	u32 cycle;
+	u32 sec;
+	u32 nsec;
+	u32 iterate;
+} __packed;
+
+struct ptp_clk_options {
+	u32 sec;
+	u32 nsec;
+	int drift;
+	u32 interval;
+} __packed;
+
+struct ptp_ts_options {
+	u32 timestamp;
+	u32 sec;
+	u32 nsec;
+	u8 msg;
+	u8 port;
+	u16 seqid;
+	u8 mac[2];
+} __packed;
+
+struct ptp_delay_values {
+	u16 rx_latency;
+	u16 tx_latency;
+	short asym_delay;
+	u16 reserved;
+} __packed;
+
+struct ptp_udp_msg {
+	u16 len;
+	u8 data[0];
+} __packed;
+
+#ifdef __KERNEL__
+#define NANOSEC_IN_SEC			1000000000
+
+#define MAX_PTP_PORT			SWITCH_PORT_NUM
+
+#define MAX_TSM_UDP_LEN			100
+#define MAX_TSM_UDP_CNT			(1 << 6)
+
+struct ptp_ltime {
+	s64 sec;
+	s64 nsec;
+};
+
+struct ptp_hw_ts {
+	struct ptp_id id;
+	struct ptp_ts ts;
+	int update;
+	int sending;
+};
+
+struct ptp_tx_ts {
+	struct ptp_id id;
+	struct ptp_ts ts;
+	int missed;
+	unsigned long req_time;
+	unsigned long resp_time;
+	struct {
+		u8 buf[MAX_TSM_UDP_LEN];
+		int len;
+	} data;
+	struct file_dev_info *dev;
+	struct sk_buff *skb;
+	struct ptp_msg *msg;
+};
+
+struct ptp_event {
+	int max;
+	int num;
+	int event;
+	int first;
+	int last;
+	u32 edge;
+	struct ptp_utime t[MAX_TIMESTAMP_EVENT_UNIT];
+	u32 timeout;
+	unsigned long expired;
+};
+
+struct ptp_output {
+	struct ptp_utime trig;
+	struct ptp_utime start;
+	struct ptp_utime stop;
+	struct ksz_ptp_time gap;
+	u32 iterate;
+	u32 len;
+	int gpo;
+	int level;
+};
+
+#define CLOCK_ENTRIES		2
+
+struct ptp_irig_info {
+	u32 pulse[100];
+	int index;
+	u8 tso[8];
+	int cur_tso;
+	int max_tso;
+	struct ptp_utime t;
+};
+
+struct ptp_info;
+
+struct ptp_work {
+	struct work_struct work;
+	struct completion done;
+	struct ptp_info *ptp;
+	int cmd;
+	int subcmd;
+	int option;
+	int output;
+	int result;
+	int used;
+	union {
+		struct ptp_cfg_options cfg;
+		struct ptp_tsi_info tsi;
+		struct ptp_tsi_options tsi_opt;
+		struct ptp_tso_options tso_opt;
+		struct ptp_clk_options clk_opt;
+		struct ptp_ts_options ts_opt;
+		struct ptp_delay_values delay;
+		u8 data[8];
+	} param;
+	struct file_dev_info *dev_info;
+};
+
+#define PTP_WORK_NUM			(1 << 4)
+#define PTP_WORK_LAST			(PTP_WORK_NUM - 1)
+
+struct ptp_access {
+	struct mutex lock;
+	int index;
+	struct ptp_work works[PTP_WORK_NUM];
+};
+
+struct ptp_reg_ops {
+	u32 (*read)(struct ptp_info *ptp, int addr, u32 reg);
+	void (*write)(struct ptp_info *ptp, int addr, u32 reg, u32 val);
+
+	void (*get_time)(struct ptp_info *ptp, struct ptp_utime *t);
+	void (*set_time)(struct ptp_info *ptp, struct ptp_utime *t);
+	void (*adjust_time)(struct ptp_info *ptp, int add, u32 sec, u32 nsec,
+		int adj_hack);
+	void (*adjust_sync_time)(struct ptp_info *ptp, int diff, u32 interval,
+		u32 duration);
+
+	void (*rx_off)(struct ptp_info *ptp, u8 tsi);
+	void (*rx_restart)(struct ptp_info *ptp, u16 tsi_bit);
+	void (*rx_event)(struct ptp_info *ptp, u8 tsi, u8 gpi, u8 event,
+		int intr);
+	void (*rx_cascade_event)(struct ptp_info *ptp, u8 first, u8 total,
+		u8 gpi, u8 event, int intr);
+	void (*read_event)(struct ptp_info *ptp, u8 tsi);
+
+	void (*tx_off)(struct ptp_info *ptp, u8 tso);
+	void (*tx_event)(struct ptp_info *ptp, u8 tso, u8 gpo, u8 event,
+		u32 pulse, u32 cycle, u16 cnt, u32 sec, u32 nsec, u32 iterate,
+		int intr, int now, int opt);
+	void (*pps_event)(struct ptp_info *ptp, u8 gpo, u32 sec);
+	void (*ptp_10MHz)(struct ptp_info *ptp, u8 tso, u8 gpo, u32 sec);
+	int (*tx_cascade)(struct ptp_info *ptp, u8 first, u8 total,
+		u16 repeat, u32 sec, u32 nsec, int intr);
+
+	void (*start)(struct ptp_info *ptp, int init);
+};
+
+struct ptp_ops {
+	void (*acquire)(struct ptp_info *ptp);
+	void (*release)(struct ptp_info *ptp);
+
+	void (*init)(struct ptp_info *ptp);
+	void (*exit)(struct ptp_info *ptp);
+	int (*stop)(struct ptp_info *ptp, int hw_access);
+	struct ptp_msg *(*check_msg)(u8 *data, u16 **udp_check_ptr);
+	int (*update_msg)(u8 *data, u32 port, u32 overrides);
+	void (*get_rx_tstamp)(void *ptr, struct sk_buff *skb);
+	void (*get_tx_tstamp)(struct ptp_info *ptp, struct sk_buff *skb);
+	int (*hwtstamp_ioctl)(struct ptp_info *ptp, struct ifreq *ifr,
+			      u16 ports);
+	int (*ixxat_ioctl)(struct ptp_info *ptp, unsigned int cmd,
+		struct ifreq *ifr);
+	int (*dev_req)(struct ptp_info *ptp, char *arg,
+		struct file_dev_info *info);
+	void (*proc_intr)(struct ptp_info *ptp);
+	int (*get_ts_info)(struct ptp_info *ptp, struct net_device *dev,
+		struct ethtool_ts_info *info);
+
+	ssize_t (*sysfs_read)(struct ptp_info *ptp, int proc_num, ssize_t len,
+		char *buf);
+	void (*sysfs_write)(struct ptp_info *ptp, int proc_num, int num,
+		const char *buf);
+
+	int (*drop_pkt)(struct ptp_info *ptp, struct sk_buff *skb, u32 vlan_id,
+		int *tag, int *ptp_tag, int *forward);
+};
+
+#define DEFAULT_GPS_GPI			6
+#define DEFAULT_GPS_TSI			9
+#define DEFAULT_PPS_TSI			10
+
+#define DEFAULT_MHZ_GPO			2
+#define DEFAULT_PPS_GPO			6
+
+/* TSO 1 is reserved if 10 MHz clock is used. */
+#define DEFAULT_MHZ_TSO			10
+#define DEFAULT_PPS_TSO			11
+
+/* Switch features and bug fixes. */
+#define PTP_ADJ_HACK			(1 << 0)
+#define PTP_ADJ_SEC			(1 << 1)
+#define PTP_PDELAY_HACK			(1 << 2)
+
+/* Software overrides. */
+
+#define PTP_PORT_FORWARD		(1 << 0)
+#define PTP_PORT_TX_FORWARD		(1 << 1)
+
+#define PTP_CHECK_PATH_DELAY		(1 << 7)
+#define PTP_VERIFY_TIMESTAMP		(1 << 8)
+#define PTP_ZERO_RESERVED_FIELD		(1 << 9)
+#define PTP_CHECK_SYS_TIME		(1 << 16)
+#define PTP_CHECK_SYNC_TIME		(1 << 24)
+#define PTP_KEEP_DST_PORT		(1 << 30)
+#define PTP_UPDATE_DST_PORT		(1 << 31)
+
+struct ptp_info {
+	struct mutex lock;
+	struct ptp_access hw_access;
+
+	/* current system time. */
+	struct ptp_utime cur_time;
+	struct ptp_utime gps_time;
+	struct ksz_ptp_time time_diff;
+	u32 sec_hi;
+	struct delayed_work check_pps;
+	struct delayed_work update_sec;
+	unsigned long update_sec_jiffies;
+
+	u32 adjust;
+	int drift;
+	int drift_set;
+
+	int adjust_offset;
+	int offset_changed;
+	s64 adjust_sec;
+	s64 sec_changed;
+
+	struct ptp_utime time_set;
+
+	u32 adj_delay;
+	u32 get_delay;
+	u32 set_delay;
+	int pps_offset;
+	struct file_dev_info *gps_dev;
+	unsigned long gps_req_time;
+	unsigned long gps_resp_time;
+	u8 gps_gpi;
+	u8 gps_tsi;
+	u16 gps_seqid;
+	u8 pps_tsi;
+	u8 pps_tso;
+	u8 pps_gpo;
+	u8 mhz_tso;
+	u8 mhz_gpo;
+	u8 version;
+	u8 ports;
+	u8 started;
+
+	/* hardware register values. */
+	u16 rx_latency[MAX_PTP_PORT];
+	u16 tx_latency[MAX_PTP_PORT];
+	short asym_delay[MAX_PTP_PORT];
+	u16 peer_delay[MAX_PTP_PORT];
+
+	struct ptp_msg *rx_msg;
+	int rx_msg_parsed;
+	u16 seqid_sync[MAX_PTP_PORT];
+	u16 seqid_fup[MAX_PTP_PORT];
+	u16 seqid_pdelay_req[MAX_PTP_PORT];
+	u16 seqid_pdelay_resp[MAX_PTP_PORT];
+	u16 seqid_pdelay_resp_fup[MAX_PTP_PORT];
+	int cap;
+	int forward;
+	int op_mode;
+	int op_state;
+
+	/* used to remember tx timestamp to differentiate between pdelay_req
+	 * and pdelay_resp.
+	 */
+	u32 xdelay_ts[MAX_PTP_PORT];
+	u32 pdresp_ts[MAX_PTP_PORT];
+
+	/* tx timestamp */
+	struct ptp_hw_ts hw_sync[MAX_PTP_PORT];
+	struct ptp_hw_ts hw_dreq[MAX_PTP_PORT];
+	struct ptp_hw_ts hw_resp[MAX_PTP_PORT];
+	struct ptp_tx_ts tx_sync[MAX_PTP_PORT];
+	struct ptp_tx_ts tx_dreq[MAX_PTP_PORT];
+	struct ptp_tx_ts tx_resp[MAX_PTP_PORT];
+
+	int state;
+	u16 def_mode;
+	u16 def_cfg;
+	u16 mode;
+	u16 cfg;
+	u16 domain;
+	u16 vid;
+	int ptp_synt;
+	u16 trig_intr;
+	u16 ts_intr;
+
+	int tsi_intr;
+	int tsi_used;
+	int tso_intr;
+	int tso_used;
+	int ts_status;
+	int cascade;
+	int cascade_rx;
+	int cascade_tx;
+	struct {
+		int first;
+		int total;
+		int tso;
+	} cascade_gpo[MAX_GPIO];
+	struct ptp_clock_identity clockIdentity;
+	struct ptp_event events[MAX_TIMESTAMP_UNIT];
+	struct ptp_output outputs[MAX_TRIG_UNIT + 1];
+	int dev_major;
+	struct file_dev_info *dev[2];
+	struct file_dev_info *tsi_dev[MAX_TIMESTAMP_UNIT];
+	struct file_dev_info *tso_dev[MAX_TRIG_UNIT];
+	char dev_name[2][20];
+	wait_queue_head_t wait_ts;
+	wait_queue_head_t wait_intr;
+	unsigned long delay_ticks;
+	int rx_en;
+	int tx_en;
+	int utc_offset;
+
+	u32 clk_divider;
+	u32 (*get_clk_cnt)(void);
+	u32 last_clk_cnt;
+	u64 total_clk_cnt;
+	u32 first_sec;
+	u32 intr_sec;
+	unsigned long last_jiffies;
+	u64 total_jiffies;
+	ktime_t first_ktime;
+	int first_drift;
+	struct ptp_ts last_rx_ts;
+	struct ptp_ts last_tx_ts;
+
+	uint features;
+	uint overrides;
+
+	struct work_struct adj_clk;
+
+	const struct ptp_ops *ops;
+	const struct ptp_reg_ops *reg;
+	void (*test_access_time)(struct ptp_info *ptp);
+
+#ifdef PTP_SPI
+	struct workqueue_struct *access;
+#endif
+
+	struct device *dev_parent;
+#ifdef CONFIG_PTP_1588_CLOCK
+	void *clock_info;
+	u32 clock_events;
+#endif
+};
+
+struct ksz_ptp_sysfs {
+	struct ksz_dev_attr *ksz_clock_attrs[CLOCK_ENTRIES];
+	struct attribute **clock_attrs[CLOCK_ENTRIES];
+};
+#endif
+
+enum {
+	DEV_IOC_UNIT_UNAVAILABLE = DEV_IOC_LAST,
+	DEV_IOC_UNIT_USED,
+	DEV_IOC_UNIT_ERROR,
+};
+
+enum {
+	DEV_INFO_MSG = DEV_INFO_LAST,
+	DEV_INFO_RESET,
+};
+
+enum {
+	DEV_PTP_CFG,
+	DEV_PTP_TEVT,
+	DEV_PTP_TOUT,
+	DEV_PTP_CLK,
+	DEV_PTP_CASCADE,
+	DEV_PTP_DELAY,
+	DEV_PTP_REG,
+	DEV_PTP_IDENTITY,
+	DEV_PTP_PEER_DELAY,
+	DEV_PTP_UTC_OFFSET,
+	DEV_PTP_TIMESTAMP,
+};
+
+#ifndef TSM_CMD_CLOCK_SET
+#define TSM_CMD_RESP			0x04
+#define TSM_CMD_GET_TIME_RESP		0x08
+
+#define TSM_CMD_CLOCK_SET		0x10
+#define TSM_CMD_CLOCK_CORRECT		0x20
+#define TSM_CMD_DB_SET			0x30
+#define TSM_CMD_DB_GET			0x40
+#define TSM_CMD_STAT_CLEAR		0x50
+#define TSM_CMD_STAT_GET		0x60
+#define TSM_CMD_CNF_SET			0x70
+#define TSM_CMD_CNF_GET			0x80
+#define TSM_CMD_GPIO_SET		0x90
+#define TSM_CMD_GPIO_GET		0xA0
+#define	TSM_CMD_SET_SECONDS		0xB0
+/* todo */
+#define TSM_CMD_GET_GPS_TS		0xE0
+
+/* used for accessing reserved DB entry for a given port for SYNC or DELAY_REQ
+ * packets on egress
+ */
+#define TSM_CMD_DB_GET_RESRV1		0xB0
+/* used for accessing reserved DB entry for a given port for P2P PATH_DEL_REQ
+ * packets on egress
+ */
+#define TSM_CMD_DB_GET_RESRV2		0xC0
+/* used for getting time from TSM, no look-up of a DB entry with an ingress or
+ * egress time stamp
+ */
+#define TSM_CMD_DB_GET_TIME		0xD0
+#define TSM_CMD_DB_SET_TIME		0xF0
+#endif
+
+struct tsm_cfg {
+	u8 cmd;
+	u8 port;
+	u8 enable;
+	u8 gmp;
+	u32 ingress_delay;
+	u16 egress_delay;
+} __packed;
+
+struct tsm_clock_set {
+	u8 cmd;
+	u32 timestamp;
+	u32 nsec;
+	u32 sec;
+	u8 reserved[5];
+} __packed;
+
+struct tsm_clock_correct {
+	u8 cmd;
+	u8 add;
+	u32 sec;
+	u32 nsec;
+	u32 drift;
+	u32 offset;
+} __packed;
+
+struct tsm_db {
+	u8 cmd;
+	u8 index;
+	u16 seqid;
+	u8 mac[2];
+	u32 cur_sec;
+	u32 cur_nsec;
+	u32 timestamp;
+} __packed;
+
+struct tsm_get_gps {
+	u8 cmd;
+	u8 reserved[7];
+	u16 seqid;
+	u32 sec;
+	u32 nsec;
+} __packed;
+
+struct tsm_get_time {
+	u8 cmd;
+	u16 seqid;
+	u8 msg;
+	u32 sec;
+	u32 nsec;
+} __packed;
+
+#ifdef __KERNEL__
+struct ptp_attributes {
+	int features;
+	int overrides;
+	int vid;
+	int gpio_1;
+	int gpio_2;
+};
+#endif
+
+#endif
diff -Napur linux-linux4microchip-2022.10-orig/drivers/net/ethernet/micrel/ksz_ptp_iba.c linux-linux4microchip-2022.10/drivers/net/ethernet/micrel/ksz_ptp_iba.c
--- linux-linux4microchip-2022.10-orig/drivers/net/ethernet/micrel/ksz_ptp_iba.c	1969-12-31 16:00:00.000000000 -0800
+++ linux-linux4microchip-2022.10/drivers/net/ethernet/micrel/ksz_ptp_iba.c	2023-04-25 16:13:55.052163480 -0700
@@ -0,0 +1,1102 @@
+/**
+ * Microchip PTP common code in IBA format
+ *
+ * Copyright (c) 2015-2022 Microchip Technology Inc.
+ *	Tristram Ha <Tristram.Ha@microchip.com>
+ *
+ * Copyright (c) 2009-2015 Micrel, Inc.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ */
+
+
+static void *get_time_pre(struct ksz_iba_info *info, void *in, void *obj)
+{
+	u32 *data = in;
+
+	iba_cmd_set(info, IBA_CMD_WRITE, IBA_CMD_16, REG_PTP_CLK_CTRL, *data);
+	iba_cmd_set(info, IBA_CMD_READ, IBA_CMD_32, REG_PTP_RTC_SEC, 0);
+	iba_cmd(info, IBA_CMD_READ, IBA_CMD_32, REG_PTP_RTC_NANOSEC);
+	iba_cmd(info, IBA_CMD_READ, IBA_CMD_16, REG_PTP_RTC_SUB_NANOSEC__2);
+	return info->fptr;
+}  /* get_time_pre */
+
+static int get_time_post(struct ksz_iba_info *info, void *out, void *obj)
+{
+	struct ptp_utime *t = obj;
+	int i = 0;
+	u16 subnsec = 0;
+
+	while (info->regs[i].cmd != (u32) -1) {
+		if (IBA_CMD_READ == (info->regs[i].cmd >> IBA_CMD_S)) {
+			u32 reg = (info->regs[i].cmd & IBA_CMD_ADDR_M);
+			u32 size = (info->regs[i].cmd & IBA_CMD_32);
+
+			switch (reg) {
+			case REG_PTP_RTC_SEC:
+				t->sec = info->ops->get_val(size,
+					info->regs[i].data[0]);
+				break;
+			case REG_PTP_RTC_NANOSEC:
+				t->nsec = info->ops->get_val(size,
+					info->regs[i].data[0]);
+				break;
+			case REG_PTP_RTC_SUB_NANOSEC__2:
+				subnsec = (u16) info->ops->get_val(size,
+					info->regs[i].data[0]);
+				break;
+			}
+		}
+		i++;
+	}
+
+	add_nsec(t, subnsec * 8);
+	return i;
+}  /* get_time_post */
+
+static void get_ptp_time_iba(struct ptp_info *ptp, struct ptp_utime *t)
+{
+	struct ksz_sw *sw = ptp->parent;
+	struct ksz_iba_info *info = &sw->info->iba;
+	u32 data[3];
+
+	data[0] = sw->cached.ptp_clk_ctrl;
+	data[0] |= PTP_READ_TIME;
+	info->ops->req(info, data, NULL, t, get_time_pre, get_time_post);
+}  /* get_ptp_time_iba */
+
+static void *set_time_pre(struct ksz_iba_info *info, void *in, void *obj)
+{
+	u32 *data = in;
+	struct ptp_utime *t = obj;
+
+	iba_cmd_set(info, IBA_CMD_WRITE, IBA_CMD_16,
+		REG_PTP_RTC_SUB_NANOSEC__2, 0);
+	iba_cmd_set(info, IBA_CMD_WRITE, IBA_CMD_32, REG_PTP_RTC_NANOSEC,
+		t->nsec);
+	iba_cmd_set(info, IBA_CMD_WRITE, IBA_CMD_32, REG_PTP_RTC_SEC,
+		t->sec);
+	iba_cmd_set(info, IBA_CMD_WRITE, IBA_CMD_16, REG_PTP_CLK_CTRL,
+		data[0]);
+	return info->fptr;
+}  /* set_time_pre */
+
+static void set_ptp_time_iba(struct ptp_info *ptp, struct ptp_utime *t)
+{
+	struct ksz_sw *sw = ptp->parent;
+	struct ksz_iba_info *info = &sw->info->iba;
+	u32 data[3];
+
+	data[0] = sw->cached.ptp_clk_ctrl;
+	data[0] |= PTP_LOAD_TIME;
+	info->ops->req(info, data, NULL, t, set_time_pre, NULL);
+}  /* set_ptp_time_iba */
+
+static void *adjust_time_pre(struct ksz_iba_info *info, void *in,
+	void *obj)
+{
+	u32 *data = in;
+	u32 nsec = data[4];
+	u32 val = nsec;
+
+	iba_cmd_set(info, IBA_CMD_WRITE, IBA_CMD_32, REG_PTP_RTC_SEC,
+		data[3]);
+	do {
+		if (nsec > NANOSEC_IN_SEC - 1)
+			nsec = NANOSEC_IN_SEC - 1;
+		iba_cmd_set(info, IBA_CMD_WRITE, IBA_CMD_32,
+			REG_PTP_RTC_NANOSEC, nsec);
+		iba_cmd_set(info, IBA_CMD_WRITE, IBA_CMD_16, REG_PTP_CLK_CTRL,
+			data[2]);
+		val -= nsec;
+		nsec = val;
+	} while (val);
+	if (data[5]) {
+		iba_cmd_set(info, IBA_CMD_WRITE, IBA_CMD_16, REG_PTP_CLK_CTRL,
+			data[5]);
+	}
+	return info->fptr;
+}  /* adjust_time_pre */
+
+static void adjust_ptp_time_iba(struct ptp_info *ptp, int add, u32 sec,
+	u32 nsec, int adj_hack)
+{
+	struct ksz_sw *sw = ptp->parent;
+	struct ksz_iba_info *info = &sw->info->iba;
+	u32 data[6];
+	u16 ctrl;
+	u16 adj = 0;
+
+	data[2] = sw->cached.ptp_clk_ctrl;
+	ctrl = data[2];
+	if (add)
+		ctrl |= PTP_STEP_DIR;
+	else
+		ctrl &= ~PTP_STEP_DIR;
+	sw->cached.ptp_clk_ctrl = ctrl;
+	if (adj_hack) {
+		adj = ctrl;
+		ctrl &= ~PTP_CLK_ADJ_ENABLE;
+	}
+	ctrl |= PTP_STEP_ADJ;
+	data[2] = ctrl;
+	data[3] = sec;
+	data[4] = nsec;
+	data[5] = 0;
+	if (adj_hack && (adj & PTP_CLK_ADJ_ENABLE))
+		data[5] = adj;
+	info->ops->req(info, data, NULL, NULL, adjust_time_pre, NULL);
+#ifdef NO_PPS_DETECT
+	if (add && (sec || nsec >= 1000))
+		ptp->clk_add = 1;
+#endif
+}  /* adjust_ptp_time_iba */
+
+static void *adjust_sync_time_pre(struct ksz_iba_info *info, void *in,
+	void *obj)
+{
+	u32 *data = in;
+
+	iba_cmd_set(info, IBA_CMD_WRITE, IBA_CMD_32, REG_PTP_RATE_DURATION,
+		data[0]);
+	iba_cmd_set(info, IBA_CMD_WRITE, IBA_CMD_32, REG_PTP_SUBNANOSEC_RATE,
+		data[1]);
+	return info->fptr;
+}
+
+static void adjust_sync_time_iba(struct ptp_info *ptp, int diff, u32 interval,
+	u32 duration)
+{
+	struct ksz_sw *sw = ptp->parent;
+	struct ksz_iba_info *info = &sw->info->iba;
+	u32 data[2];
+	u32 adjust;
+
+	adjust = clk_adjust_val(diff, interval);
+	adjust |= PTP_TMP_RATE_ENABLE;
+	data[0] = duration;
+	data[1] = adjust;
+	info->ops->req(info, data, NULL, NULL, adjust_sync_time_pre, NULL);
+}  /* adjust_sync_time_iba */
+
+static void *ptp_unit_index_pre(struct ksz_iba_info *info, void *in, void *obj)
+{
+	u32 *data = in;
+	int shift = data[0];
+	u8 unit = (u8) data[1];
+	struct ptp_info *ptp = obj;
+
+	iba_cmd_set(info, IBA_CMD_WRITE, IBA_CMD_32, REG_PTP_UNIT_INDEX__4,
+		ptp_unit_index(ptp, shift, unit));
+	return info->fptr;
+}  /* ptp_unit_index_pre */
+
+static void *rx_reset_pre(struct ksz_iba_info *info, void *in, void *obj)
+{
+	iba_cmd_set(info, IBA_CMD_READ, IBA_CMD_32, REG_PTP_CTRL_STAT__4, 0);
+	iba_cmd_set(info, IBA_CMD_WRITE_1, IBA_CMD_32, REG_PTP_CTRL_STAT__4,
+		TS_RESET);
+	iba_cmd_set(info, IBA_CMD_READ, IBA_CMD_32, REG_PTP_CTRL_STAT__4, 0);
+	iba_cmd_set(info, IBA_CMD_WRITE_0, IBA_CMD_32, REG_PTP_CTRL_STAT__4,
+		TS_ENABLE | TS_RESET);
+	return info->fptr;
+}  /* rx_reset_pre */
+
+static void ptp_rx_reset_iba(struct ptp_info *ptp, u8 tsi, u32 *ctrl_ptr)
+{
+	struct ksz_sw *sw = ptp->parent;
+	struct ksz_iba_info *info = &sw->info->iba;
+	int rc;
+	u32 buf[8];
+	void *func[3];
+	void *data_in[2];
+	u32 *data = buf;
+	int i = 0;
+
+	if (!ctrl_ptr) {
+		data_in[i] = data;
+		data[0] = PTP_TSI_INDEX_S;
+		data[1] = tsi;
+		data += 2;
+		func[i++] = ptp_unit_index_pre;
+	}
+
+	data_in[i] = data;
+	func[i++] = rx_reset_pre;
+
+	func[i] = NULL;
+	iba_assert(info, __func__, i, sizeof(func), buf, data, sizeof(buf));
+	rc = info->ops->reqs(info, data_in, NULL, ptp, func, NULL);
+}  /* ptp_rx_reset_iba */
+
+static void *rx_off_pre(struct ksz_iba_info *info, void *in, void *obj)
+{
+	iba_cmd_set(info, IBA_CMD_READ, IBA_CMD_32, REG_PTP_CTRL_STAT__4, 0);
+	iba_cmd_set(info, IBA_CMD_WRITE_0, IBA_CMD_32, REG_PTP_CTRL_STAT__4,
+		TS_INT_ENABLE | TS_ENABLE);
+	return info->fptr;
+}  /* rx_off_pre */
+
+static void ptp_rx_off_iba(struct ptp_info *ptp, u8 tsi)
+{
+	struct ksz_sw *sw = ptp->parent;
+	struct ksz_iba_info *info = &sw->info->iba;
+	u16 tsi_bit = (1 << tsi);
+	u32 ts_intr = 0;
+	int rc;
+	u32 buf[13];
+	void *func[6];
+	void *data_in[5];
+	u32 *data = buf;
+	u32 *data_out;
+	int i = 0;
+
+	data_in[i] = data;
+	data[0] = PTP_TSI_INDEX_S;
+	data[1] = tsi;
+	data += 2;
+	func[i++] = ptp_unit_index_pre;
+
+	/* Disable previous timestamp interrupt. */
+	if (ptp->ts_intr & tsi_bit) {
+		ptp->ts_intr &= ~tsi_bit;
+		ts_intr = tsi_bit;
+	}
+
+	data_in[i] = data;
+	func[i++] = rx_off_pre;
+
+	/*
+	 * Need to turn off cascade mode if it is used previously; otherwise,
+	 * event counter keeps increasing.
+	 */
+	if (ptp->cascade_rx & tsi_bit) {
+		data_in[i] = data;
+		func[i++] = rx_reset_pre;
+
+		data_in[i] = data;
+		data[0] = IBA_CMD_32;
+		data[1] = REG_TS_CTRL_STAT__4;
+		data[2] = 0;
+		data += 3;
+		func[i++] = info->ops->w_pre;
+		ptp->cascade_rx &= ~tsi_bit;
+	}
+	if (ts_intr) {
+		data_in[i] = data;
+		data[0] = IBA_CMD_32;
+		data[1] = REG_PTP_INT_STATUS__4;
+		data[2] = ts_intr;
+		data += 3;
+		func[i++] = info->ops->w_pre;
+	}
+
+	data_out = data;
+	data = iba_prepare(info, REG_PTP_CTRL_STAT__4, data);
+	data = iba_prepare(info, -1, data);
+
+	func[i] = NULL;
+	iba_assert(info, __func__, i, sizeof(func), buf, data, sizeof(buf));
+	rc = info->ops->reqs(info, data_in, data_out, ptp, func,
+		info->ops->r_post);
+}  /* ptp_rx_off_iba */
+
+static inline void rx_intr_iba(struct ptp_info *ptp, u16 tsi_bit, u32 *ctrl)
+{
+	ptp->ts_intr |= tsi_bit;
+	*ctrl |= TS_INT_ENABLE;
+}  /* rx_intr_iba */
+
+static void *rx_on_pre(struct ksz_iba_info *info, void *in, void *obj)
+{
+	u32 *data = in;
+	u8 tsi = (u8) data[0];
+	int intr = data[1];
+	struct ptp_info *ptp = obj;
+	u32 ctrl = 0;
+
+	iba_cmd_set(info, IBA_CMD_READ, IBA_CMD_32, REG_PTP_CTRL_STAT__4, 0);
+
+	/* Enable timestamp interrupt. */
+	if (intr)
+		ptp_rx_intr(ptp, (1 << tsi), &ctrl);
+
+	ctrl |= TS_ENABLE;
+	iba_cmd_set(info, IBA_CMD_WRITE_1, IBA_CMD_32, REG_PTP_CTRL_STAT__4,
+		ctrl);
+	return info->fptr;
+}  /* rx_on_pre */
+
+static void *rx_restart_pre(struct ksz_iba_info *info, void *in, void *obj)
+{
+	iba_cmd_set(info, IBA_CMD_READ, IBA_CMD_32, REG_PTP_CTRL_STAT__4, 0);
+	iba_cmd_set(info, IBA_CMD_WRITE_0, IBA_CMD_32, REG_PTP_CTRL_STAT__4,
+		TS_ENABLE);
+	iba_cmd_set(info, IBA_CMD_READ, IBA_CMD_32, REG_PTP_CTRL_STAT__4, 0);
+	iba_cmd_set(info, IBA_CMD_WRITE_1, IBA_CMD_32, REG_PTP_CTRL_STAT__4,
+		TS_ENABLE);
+	return info->fptr;
+}  /* rx_restart_pre */
+
+static void ptp_rx_restart_iba(struct ptp_info *ptp, u8 tsi)
+{
+	struct ksz_sw *sw = ptp->parent;
+	struct ksz_iba_info *info = &sw->info->iba;
+	int rc;
+	void *func[3];
+	void *data_in[2];
+	u32 buf[8];
+	u32 *data = buf;
+	int i = 0;
+
+	data_in[i] = data;
+	data[0] = PTP_TSI_INDEX_S;
+	data[1] = tsi;
+	data += 2;
+	func[i++] = ptp_unit_index_pre;
+
+	data_in[i] = data;
+	func[i++] = rx_restart_pre;
+
+	func[i] = NULL;
+	iba_assert(info, __func__, i, sizeof(func), buf, data, sizeof(buf));
+	rc = info->ops->reqs(info, data_in, NULL, ptp, func, NULL);
+}  /* ptp_rx_restart_iba */
+
+static void *rx_event_pre(struct ksz_iba_info *info, void *in, void *obj)
+{
+	u32 *data = in;
+	u8 tsi = data[0];
+	u8 gpi = data[1];
+	u8 event = data[2];
+	struct ptp_info *ptp = obj;
+	u32 ctrl;
+
+	iba_cmd_set(info, IBA_CMD_WRITE, IBA_CMD_32, REG_PTP_UNIT_INDEX__4,
+		ptp_unit_index(ptp, PTP_TSI_INDEX_S, tsi));
+
+	/* Config pattern. */
+	ctrl = ts_event_gpi(gpi, event);
+	iba_cmd_set(info, IBA_CMD_WRITE, IBA_CMD_32, REG_TS_CTRL_STAT__4,
+		ctrl);
+	return info->fptr;
+}  /* rx_event_pre */
+
+static void ptp_rx_event_iba(struct ptp_info *ptp, u8 tsi, u8 gpi, u8 event,
+	int intr)
+{
+	struct ksz_sw *sw = ptp->parent;
+	struct ksz_iba_info *info = &sw->info->iba;
+	int rc;
+	void *func[3];
+	void *data_in[2];
+	u32 buf[8];
+	u32 *data = buf;
+	int i = 0;
+
+	data_in[i] = data;
+	data[0] = tsi;
+	data[1] = gpi;
+	data[2] = event;
+	data += 3;
+	func[i++] = rx_event_pre;
+
+	data_in[i] = data;
+	data[0] = tsi;
+	data[1] = intr;
+	data += 2;
+	func[i++] = rx_on_pre;
+
+	func[i] = NULL;
+	iba_assert(info, __func__, i, sizeof(func), buf, data, sizeof(buf));
+	rc = info->ops->reqs(info, data_in, NULL, ptp, func, NULL);
+}  /* ptp_rx_event_iba */
+
+static void *rx_cascade_event_pre(struct ksz_iba_info *info, void *in,
+	void *obj)
+{
+	u32 *data = in;
+	u8 first = data[0];
+	u8 total = data[1];
+	u8 gpi = data[2];
+	u8 event = data[3];
+	int intr = data[4];
+	struct ptp_info *ptp = obj;
+	int last;
+	int tsi;
+	u32 ctrl;
+	u32 tail;
+	int i;
+	int prev;
+
+	last = (first + total - 1) % MAX_TIMESTAMP_UNIT;
+	tsi = last;
+	tail = TS_CASCADE_TAIL;
+	for (i = 1; i < total; i++) {
+		iba_cmd_set(info, IBA_CMD_WRITE, IBA_CMD_32,
+			REG_PTP_UNIT_INDEX__4,
+			ptp_unit_index(ptp, PTP_TSI_INDEX_S, tsi));
+
+		prev = tsi - 1;
+		if (prev < 0)
+			prev = MAX_TIMESTAMP_UNIT - 1;
+		ctrl = ts_event_gpi(gpi, event);
+		ctrl |= ts_cascade(prev);
+		ctrl |= tail;
+		ptp->cascade_rx |= (1 << tsi);
+		iba_cmd_set(info, IBA_CMD_WRITE, IBA_CMD_32,
+			REG_TS_CTRL_STAT__4, ctrl);
+
+		/* Enable timestamp interrupt. */
+		if (intr) {
+			ctrl = 0;
+			ptp_rx_intr(ptp, (1 << tsi), &ctrl);
+			iba_cmd_set(info, IBA_CMD_READ, IBA_CMD_32,
+				REG_PTP_CTRL_STAT__4, 0);
+			iba_cmd_set(info, IBA_CMD_WRITE_1, IBA_CMD_32,
+				REG_PTP_CTRL_STAT__4, ctrl);
+		}
+		--tsi;
+		if (tsi < 0)
+			tsi = MAX_TIMESTAMP_UNIT - 1;
+		tail = 0;
+	}
+	iba_cmd_set(info, IBA_CMD_WRITE, IBA_CMD_32, REG_PTP_UNIT_INDEX__4,
+		ptp_unit_index(ptp, PTP_TSI_INDEX_S, first));
+
+	ctrl = ts_event_gpi(gpi, event);
+	ctrl |= ts_cascade(last);
+	ptp->cascade_rx |= (1 << first);
+	iba_cmd_set(info, IBA_CMD_WRITE, IBA_CMD_32, REG_TS_CTRL_STAT__4,
+		ctrl);
+	return info->fptr;
+}  /* rx_cascade_event_pre */
+
+static void ptp_rx_cascade_event_iba(struct ptp_info *ptp, u8 first, u8 total,
+	u8 gpi, u8 event, int intr)
+{
+	struct ksz_sw *sw = ptp->parent;
+	struct ksz_iba_info *info = &sw->info->iba;
+	int rc;
+	void *func[3];
+	void *data_in[2];
+	u32 buf[8];
+	u32 *data = buf;
+	int i = 0;
+
+	data_in[i] = data;
+	data[0] = first;
+	data[1] = total;
+	data[2] = gpi;
+	data[3] = event;
+	data[4] = intr;
+	data += 5;
+	func[i++] = rx_cascade_event_pre;
+
+	data_in[i] = data;
+	data[0] = first;
+	data[1] = intr;
+	data += 2;
+	func[i++] = rx_on_pre;
+
+	func[i] = NULL;
+	iba_assert(info, __func__, i, sizeof(func), buf, data, sizeof(buf));
+	rc = info->ops->reqs(info, data_in, NULL, ptp, func, NULL);
+}  /* ptp_rx_cascade_event_iba */
+
+static u32 ptp_get_event_cnt_iba(struct ptp_info *ptp, u8 tsi, void *ptr)
+{
+	struct ksz_iba_info *info = ptr;
+	int rc;
+	void *func[3];
+	void *data_in[2];
+	u32 buf[8];
+	u32 *data = buf;
+	u32 *data_out;
+	int i = 0;
+
+	data_in[i] = data;
+	data[0] = PTP_TSI_INDEX_S;
+	data[1] = tsi;
+	data += 2;
+	func[i++] = ptp_unit_index_pre;
+
+	data_in[i] = data;
+	data[0] = IBA_CMD_32;
+	data[1] = REG_TS_CTRL_STAT__4;
+	data += 3;
+	func[i++] = info->ops->r_pre;
+
+	data_out = data;
+	data = iba_prepare(info, REG_TS_CTRL_STAT__4, data);
+	data = iba_prepare(info, -1, data);
+
+	func[i] = NULL;
+	iba_assert(info, __func__, i, sizeof(func), buf, data, sizeof(buf));
+	rc = info->ops->reqs(info, data_in, data_out, ptp, func,
+		info->ops->r_post);
+	return data_out[1];
+}  /* ptp_get_event_cnt_iba */
+
+static void ptp_get_events_iba(struct ptp_info *ptp, u32 reg_ns, size_t len,
+	void *buf, void *ptr)
+{
+	struct ksz_iba_info *info = ptr;
+
+	info->ops->burst(info, reg_ns, len, buf, 0,
+		info->ops->get_pre, info->ops->get_post_le);
+}  /* ptp_get_events_iba */
+
+static void ptp_read_event_iba(struct ptp_info *ptp, u8 tsi)
+{
+	struct ksz_sw *sw = ptp->parent;
+	struct ksz_iba_info *info = &sw->info->iba;
+
+	ptp_read_event_func(ptp, tsi, info, ptp_get_event_cnt_iba,
+		ptp_get_events_iba);
+}  /* ptp_read_event_iba */
+
+static void *tx_reset_pre(struct ksz_iba_info *info, void *in, void *obj)
+{
+	iba_cmd_set(info, IBA_CMD_READ, IBA_CMD_32, REG_PTP_CTRL_STAT__4, 0);
+	iba_cmd_set(info, IBA_CMD_WRITE_1, IBA_CMD_32, REG_PTP_CTRL_STAT__4,
+		TRIG_RESET);
+	iba_cmd_set(info, IBA_CMD_READ, IBA_CMD_32, REG_PTP_CTRL_STAT__4, 0);
+	iba_cmd_set(info, IBA_CMD_WRITE_0, IBA_CMD_32, REG_PTP_CTRL_STAT__4,
+		TRIG_ENABLE | TRIG_RESET);
+	return info->fptr;
+}  /* tx_reset_pre */
+
+static void *tx_off_pre(struct ksz_iba_info *info, void *in, void *obj)
+{
+	iba_cmd_set(info, IBA_CMD_READ, IBA_CMD_32, REG_PTP_CTRL_STAT__4, 0);
+	iba_cmd_set(info, IBA_CMD_WRITE_0, IBA_CMD_32, REG_PTP_CTRL_STAT__4,
+		TRIG_ENABLE);
+	return info->fptr;
+}  /* tx_off_pre */
+
+static void *tx_init_pre(struct ksz_iba_info *info, void *in, void *obj)
+{
+	iba_cmd_set(info, IBA_CMD_READ, IBA_CMD_32, REG_TRIG_CTRL__4, 0);
+	iba_cmd_set(info, IBA_CMD_WRITE_0, IBA_CMD_32, REG_TRIG_CTRL__4,
+		TRIG_CASCADE_ENABLE | TRIG_CASCADE_TAIL);
+	iba_cmd_set(info, IBA_CMD_READ, IBA_CMD_32, REG_TRIG_CTRL__4, 0);
+	iba_cmd_set(info, IBA_CMD_WRITE_1, IBA_CMD_32, REG_TRIG_CTRL__4,
+		trig_cascade(TRIG_CASCADE_UPS_M));
+	return info->fptr;
+}  /* tx_init_pre */
+
+static void ptp_tx_off_iba(struct ptp_info *ptp, u8 tso)
+{
+	struct ksz_sw *sw = ptp->parent;
+	struct ksz_iba_info *info = &sw->info->iba;
+	u16 tso_bit = (1 << tso);
+	int rc;
+	u32 buf[8 + 8];
+	void *func[5];
+	void *data_in[4];
+	u32 *data = buf;
+	u32 *data_out;
+	int i = 0;
+
+	data_in[i] = data;
+	data[0] = PTP_TOU_INDEX_S;
+	data[1] = tso;
+	data += 2;
+	func[i++] = ptp_unit_index_pre;
+
+	data_in[i] = data;
+	func[i++] = tx_off_pre;
+
+	/*
+	 * Using cascade mode previously need to reset the trigger output so
+	 * that an errorneous output will not be generated during next
+	 * cascade mode setup.
+	 */
+	if (ptp->cascade_tx & tso_bit) {
+		data_in[i] = data;
+		func[i++] = tx_reset_pre;
+
+		ptp->cascade_gpo[ptp->outputs[tso].gpo].tso &= ~tso_bit;
+		ptp->cascade_tx &= ~tso_bit;
+	}
+	data_in[i] = data;
+	func[i++] = tx_init_pre;
+
+	data_out = data;
+	data = iba_prepare(info, -1, data);
+
+	func[i] = NULL;
+	iba_assert(info, __func__, i, sizeof(func), buf, data, sizeof(buf));
+	rc = info->ops->reqs(info, data_in, data_out, ptp, func,
+		info->ops->r_post);
+}  /* ptp_tx_off_iba */
+
+static void *tx_on_pre(struct ksz_iba_info *info, void *in, void *obj)
+{
+	struct ptp_info *ptp = obj;
+	u32 *data = in;
+	u8 tso = (u8)data[0];
+
+	iba_cmd_set(info, IBA_CMD_READ, IBA_CMD_32, REG_PTP_CTRL_STAT__4, 0);
+
+	/* Do a reset on last TOU in previous cascade operation. */
+	if (ptp->cascade_tx & (1 << (tso + 16))) {
+		ptp->cascade_tx &= ~(1 << (tso + 16));
+		iba_cmd_set(info, IBA_CMD_WRITE_1, IBA_CMD_32,
+			REG_PTP_CTRL_STAT__4, TRIG_RESET);
+	}
+	iba_cmd_set(info, IBA_CMD_WRITE_0, IBA_CMD_32, REG_PTP_CTRL_STAT__4,
+		TRIG_ENABLE | TRIG_RESET);
+	iba_cmd_set(info, IBA_CMD_WRITE_1, IBA_CMD_32, REG_PTP_CTRL_STAT__4,
+		TRIG_ENABLE);
+	return info->fptr;
+}  /* tx_on_pre */
+
+static void *tx_trigger_time_iba(struct ksz_iba_info *info, u32 sec, u32 nsec)
+{
+	iba_cmd_set(info, IBA_CMD_WRITE, IBA_CMD_32, REG_TRIG_TARGET_SEC,
+		sec);
+	iba_cmd_set(info, IBA_CMD_WRITE, IBA_CMD_32, REG_TRIG_TARGET_NANOSEC,
+		nsec);
+	return info->fptr;
+}  /* tx_trigger_time_iba */
+
+static void *tx_restart_pre(struct ksz_iba_info *info, void *in, void *obj)
+{
+	u32 *data = in;
+	u32 ctrl = data[1];
+	u32 sec = data[2];
+	u32 nsec = data[3];
+
+	if (ctrl)
+		iba_cmd_set(info, IBA_CMD_WRITE, IBA_CMD_32, REG_TRIG_CTRL__4,
+			    ctrl);
+	tx_trigger_time_iba(info, sec, nsec);
+	tx_on_pre(info, in, obj);
+	return info->fptr;
+}  /* tx_restart_pre */
+
+static void ptp_tx_restart_iba(struct ptp_info *ptp, u8 tso, u32 ctrl, u32 sec,
+			       u32 nsec)
+{
+	struct ksz_sw *sw = ptp->parent;
+	struct ksz_iba_info *info = &sw->info->iba;
+	int rc;
+	void *func[3];
+	void *data_in[2];
+	u32 buf[8];
+	u32 *data = buf;
+	int i = 0;
+
+	data_in[i] = data;
+	data[0] = PTP_TOU_INDEX_S;
+	data[1] = tso;
+
+	/* Re-use tso for next command. */
+	data += 1;
+	func[i++] = ptp_unit_index_pre;
+
+	data_in[i] = data;
+	data[1] = ctrl;
+	data[2] = sec;
+	data[3] = nsec;
+	data += 4;
+	func[i++] = tx_restart_pre;
+
+	func[i] = NULL;
+	iba_assert(info, __func__, i, sizeof(func), buf, data, sizeof(buf));
+	rc = info->ops->reqs(info, data_in, NULL, ptp, func, NULL);
+}  /* ptp_tx_restart_iba */
+
+static void *tx_event_pre(struct ksz_iba_info *info, void *in, void *obj)
+{
+	u32 *data = in;
+	u32 ctrl = data[1];
+	u32 pulse = data[2];
+	u32 cycle = data[3];
+	u32 pattern = data[4];
+	u32 sec = data[5];
+	u32 nsec = data[6];
+
+	iba_cmd_set(info, IBA_CMD_WRITE, IBA_CMD_32, REG_TRIG_CTRL__4, ctrl);
+	if (pulse)
+		iba_cmd_set(info, IBA_CMD_WRITE, IBA_CMD_24,
+			REG_TRIG_PULSE_WIDTH__4 + 1, pulse);
+	if (cycle) {
+		iba_cmd_set(info, IBA_CMD_WRITE, IBA_CMD_32,
+			REG_TRIG_CYCLE_WIDTH, cycle);
+		iba_cmd_set(info, IBA_CMD_WRITE, IBA_CMD_32,
+			REG_TRIG_CYCLE_CNT, pattern);
+	}
+
+	/* Trigger time is set later in cascade mode. */
+	if (sec) {
+		tx_trigger_time_iba(info, sec, nsec);
+		tx_on_pre(info, in, obj);
+	}
+	return info->fptr;
+}  /* tx_event_pre */
+
+static void ptp_tx_event_iba(struct ptp_info *ptp, u8 tso, u32 ctrl, u32 pulse,
+	u32 cycle, u32 pattern, u32 sec, u32 nsec)
+{
+	struct ksz_sw *sw = ptp->parent;
+	struct ksz_iba_info *info = &sw->info->iba;
+	int rc;
+	void *func[3];
+	void *data_in[2];
+	u32 buf[20];
+	u32 *data = buf;
+	int i = 0;
+
+	data_in[i] = data;
+	data[0] = PTP_TOU_INDEX_S;
+	data[1] = tso;
+	data += 2;
+	func[i++] = ptp_unit_index_pre;
+
+	data_in[i] = data;
+	data[0] = tso;
+	data[1] = ctrl;
+	data[2] = pulse;
+	data[3] = cycle;
+	data[4] = pattern;
+	data[5] = sec;
+	data[6] = nsec;
+	data += 7;
+	func[i++] = tx_event_pre;
+
+	func[i] = NULL;
+	iba_assert(info, __func__, i, sizeof(func), buf, data, sizeof(buf));
+	rc = info->ops->reqs(info, data_in, NULL, ptp, func, NULL);
+}  /* ptp_tx_event_iba */
+
+static void *pps_event_pre(struct ksz_iba_info *info, void *in, void *obj)
+{
+	u32 *data = in;
+	u8 gpo = data[3];
+	u32 sec = data[4];
+	struct ptp_info *ptp = obj;
+	u32 pattern;
+	u32 ctrl;
+	u32 nsec;
+	u32 pulse = (20000000 / 8);	/* 20 ms */
+	u32 cycle = 1000000000;
+	u16 cnt = 0;
+	u8 event = TRIG_POS_PERIOD;
+
+	/* Config pattern. */
+	ctrl = trig_event_gpo(gpo, event);
+	ctrl |= TRIG_NOTIFY;
+	ctrl |= TRIG_NOW;
+	ctrl |= trig_cascade(TRIG_CASCADE_UPS_M);
+	iba_cmd_set(info, IBA_CMD_WRITE, IBA_CMD_32, REG_TRIG_CTRL__4, ctrl);
+
+	/* Config pulse width. */
+	if (pulse > TRIG_PULSE_WIDTH_M)
+		pulse = TRIG_PULSE_WIDTH_M;
+	iba_cmd_set(info, IBA_CMD_WRITE, IBA_CMD_24,
+		REG_TRIG_PULSE_WIDTH__4 + 1, pulse);
+
+	/* Config cycle width. */
+	iba_cmd_set(info, IBA_CMD_WRITE, IBA_CMD_32, REG_TRIG_CYCLE_WIDTH,
+		cycle);
+
+	/* Config trigger count. */
+	pattern = cnt;
+	pattern <<= TRIG_CYCLE_CNT_S;
+	iba_cmd_set(info, IBA_CMD_WRITE, IBA_CMD_32, REG_TRIG_CYCLE_CNT,
+		pattern);
+
+	/* Config trigger time. */
+	if (ptp->pps_offset >= 0) {
+		nsec = ptp->pps_offset;
+	} else {
+		nsec = NANOSEC_IN_SEC + ptp->pps_offset;
+		sec--;
+	}
+	tx_trigger_time_iba(info, sec, nsec);
+
+	return info->fptr;
+}  /* pps_event_pre */
+
+static void ptp_pps_event_iba(struct ptp_info *ptp, u8 gpo, u32 sec)
+{
+	struct ksz_sw *sw = ptp->parent;
+	struct ksz_iba_info *info = &sw->info->iba;
+	u8 tso = ptp->pps_tso;
+	int rc;
+	void *func[5];
+	void *data_in[4];
+	u32 buf[20];
+	u32 *data = buf;
+	int i = 0;
+
+	ptp_tx_off_iba(ptp, tso);
+
+	data_in[i] = data;
+	data[3] = gpo;
+	data[4] = sec;
+	data += 5;
+	func[i++] = pps_event_pre;
+
+	data_in[i] = data;
+	data[0] = tso;
+	data += 1;
+	func[i++] = tx_on_pre;
+
+	func[i] = NULL;
+	iba_assert(info, __func__, i, sizeof(func), buf, data, sizeof(buf));
+	rc = info->ops->reqs(info, data_in, NULL, ptp, func, NULL);
+}  /* ptp_pps_event */
+
+static void *ptp_10MHz_pre(struct ksz_iba_info *info, void *in, void *obj)
+{
+	u32 *data = in;
+	u8 tso = data[3];
+	u8 gpo = data[4];
+	u32 sec = data[5];
+	u32 nsec = data[6];
+	u32 pattern;
+	u32 ctrl;
+	u32 pulse = 6;
+	u32 cycle = 200;
+	u16 cnt = 0;
+	u8 event = TRIG_POS_PERIOD;
+
+	/* Config pattern. */
+	ctrl = trig_event_gpo(gpo, event);
+	ctrl |= TRIG_NOTIFY;
+	if (1 == tso)
+		ctrl |= TRIG_EDGE;
+	ctrl |= trig_cascade(TRIG_CASCADE_UPS_M);
+	iba_cmd_set(info, IBA_CMD_WRITE, IBA_CMD_32, REG_TRIG_CTRL__4, ctrl);
+
+	/* Config pulse width. */
+	if (pulse > TRIG_PULSE_WIDTH_M)
+		pulse = TRIG_PULSE_WIDTH_M;
+	iba_cmd_set(info, IBA_CMD_WRITE, IBA_CMD_32,
+		REG_TRIG_PULSE_WIDTH__4 + 0, pulse);
+
+	/* Config cycle width. */
+	iba_cmd_set(info, IBA_CMD_WRITE, IBA_CMD_32, REG_TRIG_CYCLE_WIDTH,
+		cycle);
+
+	/* Config trigger count. */
+	pattern = cnt;
+	pattern <<= TRIG_CYCLE_CNT_S;
+	iba_cmd_set(info, IBA_CMD_WRITE, IBA_CMD_32, REG_TRIG_CYCLE_CNT,
+		pattern);
+
+	tx_trigger_time_iba(info, sec, nsec);
+
+	return info->fptr;
+}  /* ptp_10MHz_pre */
+
+static void ptp_10MHz_iba(struct ptp_info *ptp, u8 tso, u8 gpo, u32 sec)
+{
+	struct ksz_sw *sw = ptp->parent;
+	struct ksz_iba_info *info = &sw->info->iba;
+	int n;
+	u32 nsec;
+	int rc;
+	void *func[5];
+	void *data_in[4];
+	u32 buf[20];
+	u32 *data = buf;
+	u32 *data_out;
+	int k = 0;
+
+	/* Config trigger time. */
+	if (ptp->pps_offset >= 0) {
+		nsec = ptp->pps_offset;
+	} else {
+		nsec = NANOSEC_IN_SEC + ptp->pps_offset;
+		sec--;
+	}
+
+	for (n = 0; n < 2; n++) {
+		ptp_tx_off_iba(ptp, tso);
+
+		k = 0;
+		data = buf;
+
+		data_in[k] = data;
+		data[3] = tso;
+		data[4] = gpo;
+		data[5] = sec;
+		data[6] = nsec;
+		data += 7;
+		func[k++] = ptp_10MHz_pre;
+
+		data_in[k] = data;
+		data[0] = tso;
+		data += 1;
+		func[k++] = tx_on_pre;
+
+		data_out = data;
+		data = iba_prepare(info, REG_PTP_CTRL_STAT__4, data);
+		data = iba_prepare(info, -1, data);
+
+		func[k] = NULL;
+		iba_assert(info, __func__, k, sizeof(func), buf, data,
+			sizeof(buf));
+		rc = info->ops->reqs(info, data_in, data_out, ptp, func,
+			info->ops->r_post);
+
+		tso = 1;
+		nsec += 12 * 8;
+	}
+}  /* ptp_10MHz_iba */
+
+static void *tx_cascade_cycle_iba(struct ksz_iba_info *info, u8 tso, u32 nsec)
+{
+	iba_cmd_set(info, IBA_CMD_WRITE, IBA_CMD_32, REG_TRIG_ITERATE_TIME,
+		nsec);
+	return info->fptr;
+}  /* tx_cascade_cycle_iba */
+
+static void *tx_cascade_on_pre(struct ksz_iba_info *info, void *in,
+	void *obj)
+{
+	u32 *data = in;
+	u8 tso = data[3];
+	u8 first = data[4];
+	u8 last = data[5];
+	u16 repeat = data[6];
+	struct ptp_output *cur = obj;
+	u32 ctrl;
+
+	tx_trigger_time_iba(info, cur->trig.sec, cur->trig.nsec);
+	tx_cascade_cycle_iba(info, tso, (u32)cur->iterate);
+
+	ctrl = data[2];
+	ctrl |= TRIG_CASCADE_ENABLE;
+	ctrl &= ~trig_cascade(TRIG_CASCADE_UPS_M);
+	if (tso == first)
+		ctrl |= trig_cascade(last);
+	else
+		ctrl |= trig_cascade(tso - 1);
+	if (repeat && tso == last) {
+		ctrl |= TRIG_CASCADE_TAIL;
+		ctrl |= repeat - 1;
+	}
+	iba_cmd_set(info, IBA_CMD_WRITE, IBA_CMD_32, REG_TRIG_CTRL__4, ctrl);
+	return info->fptr;
+}  /* tx_cascade_on_pre */
+
+static int ptp_tx_cascade_iba(struct ptp_info *ptp, u8 first, u8 total,
+	u16 repeat, u32 sec, u32 nsec, int intr)
+{
+	struct ksz_sw *sw = ptp->parent;
+	struct ksz_iba_info *info = &sw->info->iba;
+	int n;
+	u8 tso;
+	u8 last;
+	struct ptp_output *cur = NULL;
+	int rc;
+	void *func[5];
+	void *data_in[4];
+	u32 buf[20];
+	u32 *data = buf;
+	u32 *data_out;
+	int k = 0;
+
+	last = first + total - 1;
+	if (last >= MAX_TRIG_UNIT)
+		return 1;
+	tso = last;
+	for (n = 0; n < total; n++, tso--) {
+		cur = &ptp->outputs[tso];
+		data = buf;
+		k = 0;
+		data_in[k] = data;
+		data[0] = PTP_TOU_INDEX_S;
+		data[1] = tso;
+		data += 2;
+		func[k++] = ptp_unit_index_pre;
+
+		/* Need reset on last TOU in previous cascade operation. */
+		if (ptp->cascade_tx & (1 << (tso + 16))) {
+			data_in[k] = data;
+			func[k++] = tx_reset_pre;
+			ptp->cascade_tx &= ~(1 << (tso + 16));
+		}
+		data_in[k] = data;
+		data[0] = IBA_CMD_32;
+		data[1] = REG_TRIG_CTRL__4;
+		data += 3;
+		func[k++] = info->ops->r_pre;
+
+		data_out = data;
+		data = iba_prepare(info, REG_TRIG_CTRL__4, data);
+		data = iba_prepare(info, -1, data);
+
+		func[k] = NULL;
+		iba_assert(info, __func__, k, sizeof(func), buf, data,
+			sizeof(buf));
+		rc = info->ops->reqs(info, data_in, data_out, ptp, func,
+			info->ops->r_post);
+
+		data = buf;
+		k = 0;
+
+		data_in[k] = data;
+		data[2] = data_out[1];
+		data[3] = tso;
+		data[4] = first;
+		data[5] = last;
+		data[6] = repeat;
+		data += 7;
+		func[k++] = tx_cascade_on_pre;
+
+		func[k] = NULL;
+		iba_assert(info, __func__, k, sizeof(func), buf, data,
+			sizeof(buf));
+		if (tso != first)
+			rc = info->ops->reqs(info, data_in, NULL, cur, func,
+				NULL);
+		ptp->cascade_tx |= (1 << tso);
+	}
+
+	data_in[k] = data;
+	data[0] = last;
+	data += 1;
+	func[k++] = tx_on_pre;
+
+	func[k] = NULL;
+	iba_assert(info, __func__, k, sizeof(func), buf, data, sizeof(buf));
+	rc = info->ops->reqs(info, data_in, NULL, cur, func, NULL);
+	return 0;
+}  /* ptp_tx_cascade_iba */
+
+static struct ptp_reg_ops ptp_iba_ops = {
+	.get_time		= get_ptp_time_iba,
+	.set_time		= set_ptp_time_iba,
+	.adjust_time		= adjust_ptp_time_iba,
+	.adjust_sync_time	= adjust_sync_time_iba,
+
+	.rx_off			= ptp_rx_off_iba,
+	.rx_reset		= ptp_rx_reset_iba,
+	.rx_restart		= ptp_rx_restart_iba,
+	.rx_event		= ptp_rx_event_iba,
+	.rx_cascade_event	= ptp_rx_cascade_event_iba,
+	.read_event		= ptp_read_event_iba,
+
+	.tx_off			= ptp_tx_off_iba,
+	.tx_restart		= ptp_tx_restart_iba,
+	.tx_event		= ptp_tx_event_iba,
+	.pps_event		= ptp_pps_event_iba,
+	.ptp_10MHz		= ptp_10MHz_iba,
+	.tx_cascade		= ptp_tx_cascade_iba,
+
+	.start			= ptp_start,
+};
diff -Napur linux-linux4microchip-2022.10-orig/drivers/net/ethernet/micrel/ksz_ptp_sysfs.c linux-linux4microchip-2022.10/drivers/net/ethernet/micrel/ksz_ptp_sysfs.c
--- linux-linux4microchip-2022.10-orig/drivers/net/ethernet/micrel/ksz_ptp_sysfs.c	1969-12-31 16:00:00.000000000 -0800
+++ linux-linux4microchip-2022.10/drivers/net/ethernet/micrel/ksz_ptp_sysfs.c	2023-04-25 16:13:55.052163480 -0700
@@ -0,0 +1,130 @@
+/**
+ * Microchip PTP common sysfs code
+ *
+ * Copyright (c) 2015-2018 Microchip Technology Inc.
+ *	Tristram Ha <Tristram.Ha@microchip.com>
+ *
+ * Copyright (c) 2012-2013 Micrel, Inc.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ */
+
+
+static ssize_t ptp_show(struct device *d, struct device_attribute *attr,
+	char *buf, unsigned long offset)
+{
+	struct ksz_sw *sw;
+	struct semaphore *proc_sem;
+	struct ptp_info *ptp;
+	ssize_t len = -EINVAL;
+	int proc_num;
+
+	get_private_data(d, &proc_sem, &sw, NULL);
+	if (down_interruptible(proc_sem))
+		return -ERESTARTSYS;
+
+	ptp = &sw->ptp_hw;
+	len = 0;
+	proc_num = offset / sizeof(int);
+	len = ptp->ops->sysfs_read(ptp, proc_num, len, buf);
+	up(proc_sem);
+	return len;
+}
+
+static ssize_t ptp_store(struct device *d, struct device_attribute *attr,
+	const char *buf, size_t count, unsigned long offset)
+{
+	struct ksz_sw *sw;
+	struct semaphore *proc_sem;
+	struct ptp_info *ptp;
+	ssize_t ret = -EINVAL;
+	int num;
+	int proc_num;
+
+	num = get_num_val(buf);
+	get_private_data(d, &proc_sem, &sw, NULL);
+	if (down_interruptible(proc_sem))
+		return -ERESTARTSYS;
+
+	ptp = &sw->ptp_hw;
+	proc_num = offset / sizeof(int);
+	ret = count;
+	ptp->ops->acquire(ptp);
+	ptp->ops->sysfs_write(ptp, proc_num, num, buf);
+	ptp->ops->release(ptp);
+	up(proc_sem);
+	return ret;
+}
+
+#define PTP_ATTR(_name, _mode, _show, _store) \
+struct device_attribute ptp_attr_##_name = \
+	__ATTR(_name, _mode, _show, _store)
+
+/* generate a read-only attribute */
+#define PTP_RD_ENTRY(name)						\
+static ssize_t show_ptp_##name(struct device *d,			\
+	struct device_attribute *attr, char *buf)			\
+{									\
+	return ptp_show(d, attr, buf,					\
+		offsetof(struct ptp_attributes, name));			\
+}									\
+static PTP_ATTR(name, S_IRUGO, show_ptp_##name, NULL)
+
+/* generate a write-able attribute */
+#define PTP_WR_ENTRY(name)						\
+static ssize_t show_ptp_##name(struct device *d,			\
+	struct device_attribute *attr, char *buf)			\
+{									\
+	return ptp_show(d, attr, buf,					\
+		offsetof(struct ptp_attributes, name));			\
+}									\
+static ssize_t store_ptp_##name(struct device *d,			\
+	struct device_attribute *attr, const char *buf, size_t count)	\
+{									\
+	return ptp_store(d, attr, buf, count,				\
+		offsetof(struct ptp_attributes, name));			\
+}									\
+static PTP_ATTR(name, S_IRUGO | S_IWUSR, show_ptp_##name, store_ptp_##name)
+
+PTP_WR_ENTRY(features);
+PTP_WR_ENTRY(overrides);
+PTP_WR_ENTRY(vid);
+PTP_WR_ENTRY(gpio_1);
+PTP_WR_ENTRY(gpio_2);
+
+static struct attribute *ptp_attrs[] = {
+	&ptp_attr_features.attr,
+	&ptp_attr_overrides.attr,
+	&ptp_attr_vid.attr,
+	&ptp_attr_gpio_1.attr,
+	&ptp_attr_gpio_2.attr,
+	NULL
+};
+
+static struct attribute_group ptp_group = {
+	.name  = "ptpfs",
+	.attrs  = ptp_attrs,
+};
+
+static void exit_ptp_sysfs(struct ksz_ptp_sysfs *info, struct device *dev)
+{
+	sysfs_remove_group(&dev->kobj, &ptp_group);
+}
+
+static int init_ptp_sysfs(struct ksz_ptp_sysfs *info, struct device *dev)
+{
+	int err;
+
+	err = sysfs_create_group(&dev->kobj, &ptp_group);
+	if (err)
+		return err;
+	return err;
+}
+
diff -Napur linux-linux4microchip-2022.10-orig/drivers/net/ethernet/micrel/ksz_req.c linux-linux4microchip-2022.10/drivers/net/ethernet/micrel/ksz_req.c
--- linux-linux4microchip-2022.10-orig/drivers/net/ethernet/micrel/ksz_req.c	1969-12-31 16:00:00.000000000 -0800
+++ linux-linux4microchip-2022.10/drivers/net/ethernet/micrel/ksz_req.c	2023-04-25 16:13:55.052163480 -0700
@@ -0,0 +1,95 @@
+/**
+ * Microchip driver request common code
+ *
+ * Copyright (c) 2015-2019 Microchip Technology Inc.
+ *	Tristram Ha <Tristram.Ha@microchip.com>
+ *
+ * Copyright (c) 2009-2014 Micrel, Inc.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ */
+
+
+#include "ksz_req.h"
+
+
+#define PARAM_DATA_SIZE			80
+
+static void get_user_data(int *kernel, int *user, void *info)
+{
+	if (info)
+		__get_user(*kernel, user);
+	else
+		*kernel = *user;
+}  /* get_user_data */
+
+static void put_user_data(int *kernel, int *user, void *info)
+{
+	if (info)
+		__put_user(*kernel, user);
+	else
+		*user = *kernel;
+}  /* put_user_data */
+
+static int read_user_data(void *kernel, void *user, size_t size, void *info)
+{
+	if (info) {
+		if (!access_ok(user, size) ||
+		    copy_from_user(kernel, user, size))
+			return -EFAULT;
+	} else
+		memcpy(kernel, user, size);
+	return 0;
+}  /* read_user_data */
+
+static int write_user_data(void *kernel, void *user, size_t size, void *info)
+{
+	if (info) {
+		if (!access_ok(user, size) ||
+		    copy_to_user(user, kernel, size))
+			return -EFAULT;
+	} else
+		memcpy(user, kernel, size);
+	return 0;
+}  /* write_user_data */
+
+static int _chk_ioctl_size(int len, int size, int additional, int *req_size,
+	int *result, void *param, u8 *data, void *info)
+{
+	if (len < size) {
+		printk(KERN_INFO "wrong size: %d %d\n", len, size);
+		*req_size = size + additional;
+		*result = DEV_IOC_INVALID_LEN;
+		return -1;
+	}
+	if (size >= PARAM_DATA_SIZE) {
+		printk(KERN_INFO "large size: %d\n", size);
+		*result = -EFAULT;
+		return -1;
+	}
+	if (data) {
+		int err = read_user_data(data, param, size, info);
+
+		if (err) {
+			*result = -EFAULT;
+			return -1;
+		}
+	}
+	return 0;
+}  /* _chk_ioctl_size */
+
+static int chk_ioctl_size(int len, int size, int additional, int *req_size,
+	int *result, void *param, u8 *data)
+{
+	return _chk_ioctl_size(len, size, additional, req_size, result, param,
+		data, data);
+	return 0;
+}  /* chk_ioctl_size */
+
diff -Napur linux-linux4microchip-2022.10-orig/drivers/net/ethernet/micrel/ksz_req.h linux-linux4microchip-2022.10/drivers/net/ethernet/micrel/ksz_req.h
--- linux-linux4microchip-2022.10-orig/drivers/net/ethernet/micrel/ksz_req.h	1969-12-31 16:00:00.000000000 -0800
+++ linux-linux4microchip-2022.10/drivers/net/ethernet/micrel/ksz_req.h	2023-04-25 16:13:55.052163480 -0700
@@ -0,0 +1,110 @@
+/**
+ * Microchip driver request common header
+ *
+ * Copyright (c) 2015-2018 Microchip Technology Inc.
+ *	Tristram Ha <Tristram.Ha@microchip.com>
+ *
+ * Copyright (c) 2009-2014 Micrel, Inc.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ */
+
+
+#ifndef KSZ_REQ_H
+#define KSZ_REQ_H
+
+enum {
+	DEV_IOC_OK,
+	DEV_IOC_INVALID_SIZE,
+	DEV_IOC_INVALID_CMD,
+	DEV_IOC_INVALID_LEN,
+
+	DEV_IOC_LAST
+};
+
+enum {
+	DEV_CMD_INFO,
+	DEV_CMD_GET,
+	DEV_CMD_PUT,
+
+	DEV_CMD_LAST
+};
+
+enum {
+	DEV_INFO_INIT,
+	DEV_INFO_EXIT,
+	DEV_INFO_QUIT,
+	DEV_INFO_NOTIFY,
+	DEV_INFO_PORT,
+
+	DEV_INFO_LAST
+};
+
+struct ksz_request {
+	int size;
+	int cmd;
+	int subcmd;
+	int output;
+	int result;
+	union {
+		u8 data[1];
+		int num[1];
+	} param;
+};
+
+/* Some compilers in different OS cannot have zero number in array. */
+#define SIZEOF_ksz_request	(sizeof(struct ksz_request) - sizeof(int))
+
+/* Not used in the driver. */
+
+#ifndef MAX_REQUEST_SIZE
+#define MAX_REQUEST_SIZE	20
+#endif
+
+struct ksz_request_actual {
+	int size;
+	int cmd;
+	int subcmd;
+	int output;
+	int result;
+	union {
+		u8 data[MAX_REQUEST_SIZE];
+		int num[MAX_REQUEST_SIZE / sizeof(int)];
+	} param;
+};
+
+#define DEV_IOC_MAGIC			0x92
+
+#define DEV_IOC_MAX			1
+
+
+struct ksz_read_msg {
+	u16 len;
+	u8 data[0];
+} __packed;
+
+
+enum {
+	DEV_MOD_BASE,
+	DEV_MOD_PTP,
+	DEV_MOD_MRP,
+	DEV_MOD_DLR,
+	DEV_MOD_HSR,
+};
+
+struct ksz_resp_msg {
+	u16 module;
+	u16 cmd;
+	union {
+		u32 data[1];
+	} resp;
+} __packed;
+
+#endif
diff -Napur linux-linux4microchip-2022.10-orig/drivers/net/ethernet/micrel/ksz_spi_net.h linux-linux4microchip-2022.10/drivers/net/ethernet/micrel/ksz_spi_net.h
--- linux-linux4microchip-2022.10-orig/drivers/net/ethernet/micrel/ksz_spi_net.h	1969-12-31 16:00:00.000000000 -0800
+++ linux-linux4microchip-2022.10/drivers/net/ethernet/micrel/ksz_spi_net.h	2023-04-25 16:13:55.052163480 -0700
@@ -0,0 +1,195 @@
+/**
+ * Microchip SPI switch common header
+ *
+ * Copyright (c) 2015-2023 Microchip Technology Inc.
+ *	Tristram Ha <Tristram.Ha@microchip.com>
+ *
+ * Copyright (c) 2012-2015 Micrel, Inc.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ */
+
+
+#ifndef KSZ_SPI_NET_H
+#define KSZ_SPI_NET_H
+
+
+#if defined(_LINUX_I2C_H)
+/**
+ * struct i2c_hw_priv - I2C device private data structure
+ * @i2cdev:		Adapter device information.
+ * @rxd:		Buffer for receiving I2C data.
+ * @txd:		Buffer for transmitting I2C data.
+ */
+struct i2c_hw_priv {
+	struct i2c_client *i2cdev;
+
+	u8 rxd[8];
+	u8 txd[32];
+};
+#endif
+
+#if defined(__LINUX_SPI_H)
+/**
+ * struct spi_hw_priv - SPI device private data structure
+ * @spidev:		Adapter device information.
+ * @spi_msg1:		Used for SPI transfer with one message.
+ * @spi_msg2:		Used for SPI transfer with two messages.
+ * @spi_xfer1:		Used for SPI transfer with one message.
+ * @spi_xfer2:		Used for SPI transfer with two messages.
+ * @rx_1msg:		Flag to receive SPI data with single message.
+ * @rxd:		Buffer for receiving SPI data.
+ * @txd:		Buffer for transmitting SPI data.
+ */
+struct spi_hw_priv {
+	struct spi_device *spidev;
+	struct spi_message spi_msg1;
+	struct spi_message spi_msg2;
+	struct spi_transfer spi_xfer1;
+	struct spi_transfer spi_xfer2[2];
+	int rx_1msg;
+
+	u8 rxd[128];
+	u8 txd[128];
+};
+#endif
+
+struct smi_hw_priv {
+	struct mii_bus *bus;
+	int phyid;
+	int (*read)(struct mii_bus *bus, int phy_id, int regnum);
+	int (*write)(struct mii_bus *bus, int phy_id, int regnum, u16 val);
+};
+
+/**
+ * struct sw_priv - Switch device private data structure
+ * @hw_dev:		Pointer to hardware access device structure.
+ * @dev:		Pointer to Linux base device of hardware device.
+ * @intr_mode:		Indicate which interrupt mode to use.
+ * @irq:		A copy of the hardware device interrupt.
+ * @sysfs:		Sysfs structure.
+ * @proc_sem:		Semaphore for sysfs accessing.
+ * @hwlock:
+ * @lock:
+ * @link_read:		Work queue for detecting link.
+ * @mib_read:		Work queue for reading MIB counters.
+ * @stp_monitor:	Work queue for STP monitoring.
+ * @mib_timer_info:	Timer information for reading MIB counters.
+ * @monitor_timer_info:	Timer information for monitoring.
+ * @counter:		MIB counter data.
+ * @ports:		Virtual switch ports.
+ * @debug_root:
+ * @debug_file:
+ * @irq_gpio:		GPIO pin used for interrupt.
+ * @gpio_val:		GPIO value during interrupt.
+ * @phy_id:		Point to active PHY.
+ * @intr_working:	Working interrupt indications.
+ * @intr_mask:
+ * @pdev:		Point to platform device.
+ * @bus:		Point to MDIO bus.
+ * @bus_irqs:
+ * @name:
+ * @phydev:		Point to active PHY device.
+ * @sw:			Virtual switch structure.
+ */
+struct sw_priv {
+	void *hw_dev;
+	struct device *dev;
+	struct device *of_dev;
+	int intr_mode;
+	int irq;
+	int spi_mode;
+
+	struct ksz_sw_sysfs sysfs;
+#ifdef CONFIG_1588_PTP
+	struct ksz_ptp_sysfs ptp_sysfs;
+#endif
+	struct semaphore proc_sem;
+
+	struct mutex hwlock;
+	struct mutex lock;
+
+	struct work_struct irq_work;
+	struct delayed_work link_read;
+	struct work_struct mib_read;
+	struct delayed_work stp_monitor;
+	struct ksz_timer_info mib_timer_info;
+	struct ksz_timer_info monitor_timer_info;
+	struct ksz_counter_info counter[TOTAL_PORT_NUM];
+	struct ksz_port ports[TOTAL_PORT_NUM + 1];
+
+	struct dentry *debug_root;
+	struct dentry *debug_file;
+
+	int irq_gpio;
+	int gpio_val;
+	int intr_working;
+	uint intr_mask;
+
+	struct platform_device *pdev;
+	struct mii_bus *bus;
+	int bus_irqs[PHY_MAX_ADDR];
+	char name[40];
+	struct phy_device *phydev;
+
+	/* Switch structure size can be variable. */
+	struct ksz_sw sw;
+};
+
+/**
+ * struct dev_priv - Network device private data structure
+ * @adapter:		Adapter device information.
+ * @dev:
+ * @parent:
+ * @port:
+ * @monitor_timer_info:	Timer information for monitoring.
+ * @stats:		Network statistics.
+ * @phydev:		The PHY device associated with the device.
+ * @phy_pause:		Workqueue to pause the PHY state machine.
+ * @id:			Device ID.
+ * @mii_if:		MII interface information.
+ * @advertising:	Temporary variable to store advertised settings.
+ * @msg_enable:		The message flags controlling driver output.
+ * @media_state:	The connection status of the device.
+ * @multicast:		The all multicast state of the device.
+ * @promiscuous:	The promiscuous state of the device.
+ */
+struct dev_priv {
+	void *adapter;
+	struct net_device *dev;
+	void *parent;
+	struct ksz_port port;
+	struct ksz_timer_info monitor_timer_info;
+	struct net_device_stats stats;
+
+	struct phy_device dummy_phy;
+	struct phy_device *phydev;
+	struct work_struct phy_pause;
+
+	int id;
+
+	struct mii_if_info mii_if;
+	u32 advertising;
+
+	u32 msg_enable;
+	int media_state;
+	int multicast;
+	int promiscuous;
+	u8 phy_addr;
+	u8 state;
+	u8 multi_list_size;
+
+#ifdef MAX_MULTICAST_LIST
+	u8 multi_list[MAX_MULTICAST_LIST][ETH_ALEN];
+#endif
+};
+
+#endif
+
diff -Napur linux-linux4microchip-2022.10-orig/drivers/net/ethernet/micrel/ksz_stp.c linux-linux4microchip-2022.10/drivers/net/ethernet/micrel/ksz_stp.c
--- linux-linux4microchip-2022.10-orig/drivers/net/ethernet/micrel/ksz_stp.c	1969-12-31 16:00:00.000000000 -0800
+++ linux-linux4microchip-2022.10/drivers/net/ethernet/micrel/ksz_stp.c	2023-10-11 12:23:35.000000000 -0700
@@ -0,0 +1,5029 @@
+/**
+ * Microchip RSTP code
+ *
+ * Copyright (c) 2016-2019 Microchip Technology Inc.
+ *	Tristram Ha <Tristram.Ha@microchip.com>
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ */
+
+
+#if 0
+#define DBG_STP_STATE
+#ifdef DBG_STP_STATE
+#if 1
+#endif
+#if 0
+#define DBG_STP_STATE_RX
+#define DBG_STP_STATE_PROTO
+#define DBG_STP_STATE_INFO
+#define DBG_STP_STATE_ROLE_TR
+#define DBG_STP_STATE_TX
+#define DBG_STP_STATE_TR
+#define DBG_STP_STATE_TOPOLOGY
+#endif
+#endif
+#endif
+
+#if 0
+#define DBG_STP_PORT_BLOCK
+#endif
+#if 0
+#define DBG_STP_PORT_FLUSH
+#endif
+#if 0
+#define DBG_STP_ROLE
+#endif
+#if 0
+#define DBG_STP_RX
+#endif
+#if 0
+#define DBG_STP_TX
+#endif
+
+
+#ifndef FALSE
+#define FALSE	0
+#define TRUE	1
+#endif
+
+
+#define BPDU_TYPE_CONFIG	0
+#define BPDU_TYPE_CONFIG_RSTP	2
+#define BPDU_TYPE_TCN		0x80
+
+
+#define TOPOLOGY_CHANGE		(1 << 0)
+#define PROPOSAL		(1 << 1)
+#define PORT_ROLE_S		2
+#define PORT_ROLE_ALTERNATE	1
+#define PORT_ROLE_ROOT		2
+#define PORT_ROLE_DESIGNATED	3
+#define LEARNING		(1 << 4)
+#define FORWARDING		(1 << 5)
+#define AGREEMENT		(1 << 6)
+#define TOPOLOGY_CHANGE_ACK	(1 << 7)
+
+
+static u16 get_bpdu_time(u16 time)
+{
+	int val;
+
+	val = ntohs(time);
+
+	/* Round up to whole second. */
+	val += 255;
+	return (u16)(val / 256);
+}  /* get_bpdu_time */
+
+static void set_bpdu_time(u16 *dst, u16 src)
+{
+	if (src < 256) {
+		src *= 256;
+		*dst = htons(src);
+	} else
+		*dst = 0xffff;
+}  /* set_bpdu_time */
+
+static void set_bpdu_times(struct bpdu *bpdu, struct stp_times *t)
+{
+	set_bpdu_time(&bpdu->message_age, t->message_age);
+	set_bpdu_time(&bpdu->max_age, t->max_age);
+	set_bpdu_time(&bpdu->hello_time, t->hello_time);
+	set_bpdu_time(&bpdu->forward_delay, t->forward_delay);
+}  /* set_bpdu_times */
+
+static void prep_bpdu(struct bpdu *bpdu, struct stp_prio *p,
+	struct stp_times *t)
+{
+	bpdu->protocol = 0;
+	bpdu->flags = 0;
+	memcpy(&bpdu->root, p, sizeof(struct stp_prio));
+	set_bpdu_times(bpdu, t);
+	bpdu->version_1_length = 0;
+}  /* prep_bpdu */
+
+static void prep_stp(struct bpdu *bpdu, struct stp_prio *p,
+	struct stp_times *t)
+{
+	prep_bpdu(bpdu, p, t);
+	bpdu->version = 0;
+	bpdu->type = BPDU_TYPE_CONFIG;
+}  /* prep_stp */
+
+static void prep_rstp(struct bpdu *bpdu, struct stp_prio *p,
+	struct stp_times *t)
+{
+	prep_bpdu(bpdu, p, t);
+	bpdu->version = 2;
+	bpdu->type = BPDU_TYPE_CONFIG_RSTP;
+}  /* prep_rstp */
+
+#if defined(DBG_STP_RX) || defined(DBG_STP_TX)
+static void disp_bpdu(struct bpdu *bpdu)
+{
+	u8 role;
+
+	if (BPDU_TYPE_TCN != bpdu->type) {
+		dbg_msg("%04x=%02x%02x%02x%02x%02x%02x "
+			"%04x=%02x%02x%02x%02x%02x%02x:"
+			"%02x%02x %u\n",
+			ntohs(bpdu->root.prio),
+			bpdu->root.addr[0],
+			bpdu->root.addr[1],
+			bpdu->root.addr[2],
+			bpdu->root.addr[3],
+			bpdu->root.addr[4],
+			bpdu->root.addr[5],
+			ntohs(bpdu->bridge_id.prio),
+			bpdu->bridge_id.addr[0],
+			bpdu->bridge_id.addr[1],
+			bpdu->bridge_id.addr[2],
+			bpdu->bridge_id.addr[3],
+			bpdu->bridge_id.addr[4],
+			bpdu->bridge_id.addr[5],
+			bpdu->port_id.prio, bpdu->port_id.num,
+			ntohl(bpdu->root_path_cost));
+		dbg_msg("%u %u %u %u  ",
+			htons(bpdu->message_age) / 256,
+			htons(bpdu->max_age) / 256,
+			htons(bpdu->hello_time) / 256,
+			htons(bpdu->forward_delay) / 256);
+		dbg_msg("%02X:", bpdu->flags);
+		if (bpdu->flags & TOPOLOGY_CHANGE_ACK)
+			dbg_msg("K");
+		else
+			dbg_msg("-");
+		if (bpdu->flags & AGREEMENT)
+			dbg_msg("A");
+		else
+			dbg_msg("-");
+		if (bpdu->flags & FORWARDING)
+			dbg_msg("F");
+		else
+			dbg_msg("-");
+		if (bpdu->flags & LEARNING)
+			dbg_msg("L");
+		else
+			dbg_msg("-");
+		role = bpdu->flags >> PORT_ROLE_S;
+		role &= PORT_ROLE_DESIGNATED;
+		switch (role) {
+		case PORT_ROLE_ALTERNATE:
+			dbg_msg("N");
+			break;
+		case PORT_ROLE_ROOT:
+			dbg_msg("R");
+			break;
+		case PORT_ROLE_DESIGNATED:
+			dbg_msg("D");
+			break;
+		default:
+			if (BPDU_TYPE_CONFIG_RSTP == bpdu->type)
+				dbg_msg("?");
+			else
+				dbg_msg("-");
+		}
+		if (bpdu->flags & PROPOSAL)
+			dbg_msg("P");
+		else
+			dbg_msg("-");
+		if (bpdu->flags & TOPOLOGY_CHANGE)
+			dbg_msg("T");
+		else
+			dbg_msg("-");
+	}
+	dbg_msg("  %04x %u %02x",
+		htons(bpdu->protocol),
+		bpdu->version, bpdu->type);
+	if (BPDU_TYPE_CONFIG_RSTP == bpdu->type)
+		dbg_msg(" %u", bpdu->version_1_length);
+	dbg_msg("\n");
+}
+#endif
+
+static struct bpdu *chk_bpdu(u8 *data, u16 *size)
+{
+	struct llc *llc = (struct llc *) &data[12];
+	u16 len = ntohs(llc->len);
+
+	if (len < 1500) {
+		if (0x42 == llc->dsap &&
+		    0x42 == llc->ssap &&
+		    0x03 == llc->ctrl)
+		if (size)
+			*size = len - 3;
+		return (struct bpdu *)(llc + 1);
+	}
+	return NULL;
+}  /* chk_bpdu */
+
+
+#define STP_TIMER_TICK		200
+#define STP_TIMER_SCALE		1000
+
+#define to_stp_timer(x)		((x) * STP_TIMER_SCALE)
+#define from_stp_timer(x)	((x) / STP_TIMER_SCALE)
+#define NEQ(x, y)		(abs((y) - (x)) >= STP_TIMER_SCALE)
+
+
+#define BridgeIdentifier	(p->br->vars.br_id_)
+#define BridgePriority		(p->br->vars.bridgePrio_)
+#define BridgeTimes		(p->br->vars.bridgeTimes_)
+#define rootPortId		(p->br->vars.rootPortId_)
+#define rootPriority		(p->br->vars.rootPrio_)
+#define rootTimes		(p->br->vars.rootTimes_)
+
+#define timeSinceTC		(p->br->vars.TC_sec_)
+#define cntTC			(p->br->vars.TC_cnt_)
+#define isTC			(p->br->vars.TC_set_)
+
+/* MigrateTime is only used internally for timer. */
+#define MigrateTime		(p->br->vars.MigrateTime_)
+#define TxHoldCount		(p->br->vars.TxHoldCount_)
+#define ForceProtocolVersion	(p->br->vars.ForceProtocolVersion_)
+
+
+#define edgeDelayWhile		(p->vars.timers[0])
+#define fdWhile			(p->vars.timers[1])
+#define helloWhen		(p->vars.timers[2])
+#define mdelayWhile		(p->vars.timers[3])
+#define rbWhile			(p->vars.timers[4])
+#define rcvdInfoWhile		(p->vars.timers[5])
+#define rrWhile			(p->vars.timers[6])
+#define tcWhile			(p->vars.timers[7])
+
+/* For MRP. */
+#define tcDetected		(p->vars.timers[8])
+
+#define tcPropWhile		(p->vars.timers[9])
+
+#define AdminEdgePort		(p->vars.admin_var.AdminEdgePort_)
+#define AdminPortPathCost	(p->vars.admin_var.adminPortPathCost_)
+#define AutoEdgePort		(p->vars.admin_var.AutoEdgePort_)
+#define adminPointToPointMAC	(p->vars.admin_var.adminPointToPointMAC_)
+#define operPointToPointMAC	(p->vars.admin_var.operPointToPointMAC_)
+
+#define ageingTime		(p->vars.stp_var.ageingTime_)
+#define agree			(p->vars.stp_var.agree_)
+#define agreed			(p->vars.stp_var.agreed_)
+#define designatedPriority	(p->vars.stp_var.desgPrio_)
+#define designatedTimes		(p->vars.stp_var.desgTimes_)
+#define disputed		(p->vars.stp_var.disputed_)
+#define fdbFlush		(p->vars.stp_var.fdbFlush_)
+#define forward			(p->vars.stp_var.forward_)
+#define forwarding		(p->vars.stp_var.forwarding_)
+#define infoIs			(p->vars.stp_var.infoIs_)
+#define learn			(p->vars.stp_var.learn_)
+#define learning		(p->vars.stp_var.learning_)
+#define mcheck			(p->vars.stp_var.mcheck_)
+#define msgPriority		(p->vars.stp_var.msgPrio_)
+#define msgTimes		(p->vars.stp_var.msgTimes_)
+#define newInfo			(p->vars.stp_var.newInfo_)
+#define operEdge		(p->vars.stp_var.operEdge_)
+#define portEnabled		(p->vars.stp_var.portEnabled_)
+#define portId			(p->vars.stp_var.portId_)
+#define PortPathCost		(p->vars.stp_var.PortPathCost_)
+#define portPriority		(p->vars.stp_var.portPrio_)
+#define portTimes		(p->vars.stp_var.portTimes_)
+#define proposed		(p->vars.stp_var.proposed_)
+#define proposing		(p->vars.stp_var.proposing_)
+#define rcvdBPDU		(p->vars.stp_var.rcvdBPDU_)
+#define rcvdInfo		(p->vars.stp_var.rcvdInfo_)
+#define rcvdMsg			(p->vars.stp_var.rcvdMsg_)
+#define rcvdRSTP		(p->vars.stp_var.rcvdRSTP_)
+#define rcvdSTP			(p->vars.stp_var.rcvdSTP_)
+#define rcvdTc			(p->vars.stp_var.rcvdTc_)
+#define rcvdTcAck		(p->vars.stp_var.rcvdTcAck_)
+#define rcvdTcn			(p->vars.stp_var.rcvdTcn_)
+#define reRoot			(p->vars.stp_var.reRoot_)
+#define reselect		(p->vars.stp_var.reselect_)
+#define role			(p->vars.stp_var.role_)
+#define selected		(p->vars.stp_var.selected_)
+#define selectedRole		(p->vars.stp_var.selectedRole_)
+#define sendRSTP		(p->vars.stp_var.sendRSTP_)
+#define sync			(p->vars.stp_var.sync_)
+#define synced			(p->vars.stp_var.synced_)
+#define tcAck			(p->vars.stp_var.tcAck_)
+#define tcProp			(p->vars.stp_var.tcProp_)
+#define tick			(p->vars.stp_var.tick_)
+#define txCount			(p->vars.stp_var.txCount_)
+#define updtInfo		(p->vars.stp_var.updtInfo_)
+
+#define bpduVersion		(p->vars.bpduVersion_)
+#define bpduType		(p->vars.bpduType_)
+#define bpduFlags		(p->vars.bpduFlags_)
+#define bpduRole		(p->vars.bpduRole_)
+#define bpduPriority		(p->vars.bpduPrio_)
+#define bpduTimes		(p->vars.bpduTimes_)
+
+#define DesignatedPort		(ROLE_DESIGNATED == role)
+#define RootPort		(ROLE_ROOT == role)
+#define AlternatePort		(ROLE_ALTERNATE == role)
+#define BackupPort		(ROLE_BACKUP == role)
+#define DisabledPort		(ROLE_DISABLED == role)
+
+#define ForwardPort		\
+	(DesignatedPort || RootPort)
+
+#define BridgeFwdDelay		(BridgeTimes.forward_delay)
+#define BridgeHelloTime		(BridgeTimes.hello_time)
+#define BridgeMaxAge		(BridgeTimes.max_age)
+
+#define AdminEdge		AdminEdgePort
+#define AutoEdge		AutoEdgePort
+
+#define EdgeDelay()		(operPointToPointMAC ? MigrateTime : MaxAge)
+#define forwardDelay()		(sendRSTP ? HelloTime : FwdDelay)
+
+/* These times are in timer unit. */
+#define FwdDelay		to_stp_timer(designatedTimes.forward_delay)
+#define HelloTime		to_stp_timer(designatedTimes.hello_time)
+#define MaxAge			to_stp_timer(designatedTimes.max_age)
+
+#define rstpVersion		(ForceProtocolVersion >= 2)
+#define stpVersion		(ForceProtocolVersion < 2)
+
+
+/* Shortcuts for some common qualifications. */
+#define canChange		(selected && !updtInfo)
+#define canSend			\
+	(newInfo && (txCount < TxHoldCount) && (helloWhen != 0))
+
+/*
+ * agree means the port accepts incoming Designated Port, and AGREEMNT flag is
+ * set.
+ * There is a case that after becoming Designated Port agree is never reset.
+ *
+ * agreed means the proposing Designated Port receives AGREEMENT and so can
+ * stop proposing and can enable learning and forwarding immediately.
+ * Not operPointToPointMAC causes that not to happen and the Designated Port
+ * keeps proposing, but half-duplex connection will cause operPointToPoint to
+ * be FALSE.  In old time half-duplex was associated with hub and what is
+ * called Shared Media.  The sent BPDU may be dropped because of collisions,
+ * but that is not what happens here.
+ * agreed is used to qualify synced after betterorsameInfo call for Designated
+ * Port.
+ *
+ * proposing means the Designated Port is asking approval before opening the
+ * port, and PROPOSAL flag is set.
+ * It is set when the Designated Port is not forwarding.  It is reset when
+ * AGREEMENT flag is received, or when infoIs is set to Mine.
+ * Does it mean in !operPointToPointMAC the PROPOSAL flag will be dropped when
+ * the Designated Port information is changed?!
+ * There is a case that after becoming Root Port proposing is not reset.  It
+ * should not be set in DESIGNATED_PROPOSE state.
+ *
+ * proposed means receiving PROPOSAL from Desginated Port.  It will be reset
+ * either agree is TRUE or not.  The only difference is setSyncTree will be
+ * called when agree is FALSE.
+ *
+ * sync is set in the setSyncTree call.  It is reset when synced is set.
+ * It is reset for Root Port but not Alternate Port.  It is required to be
+ * FALSE for Designated Port to move to learning state.  If it is TRUE then
+ * Designated Port moves to discarding state.
+ *
+ * synced is used in the allSynced call.  That call is only used to get into
+ * _AGREED states in which proposed is reset and agree is set and a BPDU will
+ * be sent.
+ *
+ * tcAck is used by Designated Port to set TOPOLOGY_CHANGE_ACK flag in response
+ * of TOPOLOGY_CHANGE flag.
+ *
+ * rcvdTc is not accepted from inferior designated port.
+ *
+ * rcvdTcAck is used by Root Port to stop sending topology change immediately.
+ *
+ * newInfo is set to send out BPDU.  It is checked periodically so that
+ * Designated Port can always send but Root Port only sends when there is a
+ * topology change.
+ * It is set when agree is set the first time.  However, Alternate Port does
+ * not always send after becoming one.
+ * There is a case a BPDU is sent during initializion when the port role is not
+ * defined yet.  It should be handled in the _DISABLE_PORT state.
+ * HelloTime is zero when Port Transmit state machine starts.  Because of that
+ * PERIODIC is called instead.  Is that the original intention?
+ * However, newInfo is still not cleared because of the OR operation.
+ *
+ * reselect means port information is being changed so selected will not be set
+ * to TRUE.  Typically selected is set to FALSE while reselect is set so that
+ * role selection is done.
+ *
+ * selected means the role selection is completed so states can be changed.  It
+ * is set to FALSE when infoIs is changed.
+ *
+ * updtInfo is set when the port is becoming Designated Port or its parameters
+ * are changed.  It is reset when the Designated Port sets infoIs to Mine.  It
+ * is explicitly set to FALSE for other port roles.
+ * !updtInfo is required in many state changes.  Basically it means the
+ * Designated Port needs to set infoIs to Mine before moving to other states.
+ * There is a case that a Designated Port changing to Root Port will set
+ * proposing, even though it will not be a Designated Port.
+ *
+ * infoIs is initialized in Port Information state machine, so it should be run
+ * before Port Role Selection state machine.
+ *
+ * selectedRole is initialized by updtRoleDisabledTree in INIT_BRIDGE, so
+ * Port Role Selection state machine should be run before Port Role Transitions
+ * state machine.
+ *
+ * There is a problem that selectedRole is changed to DesignatedPort but
+ * DISABLE_PORT changes role like selectedRole is DisabledPort and is stuck in
+ * DISABLED_PORT state.
+ *
+ * MaxAge is initialized in updtRolesTree, so INIT_PORT should be called after
+ * that.
+ */
+
+
+#define CMP(first, second)	memcmp(&first, &second, sizeof(first))
+#define COPY(first, second)	memcpy(&first, &second, sizeof(first))
+#define ZERO(first)		memset(&first, 0, sizeof(first))
+
+
+#define COPY_PRIO(vector, p, id)	\
+	memcpy(&(vector.prio), &p, sizeof(p));	\
+	memcpy(&(vector.port_id), &id, sizeof(id));
+
+
+static int superiorPriority(struct stp_prio *first, struct stp_prio *second)
+{
+	int prio;
+
+	prio = memcmp(first, second, sizeof(struct stp_prio));
+	if (prio > 0 &&
+	    !memcmp(first->bridge_id.addr, second->bridge_id.addr, ETH_ALEN) &&
+	    first->port_id.num == second->port_id.num)
+		prio = -256;
+#ifdef DBG_STP_ROLE
+if (prio < -127)
+dbg_msg("  %s same br/port\n", __func__);
+#endif
+	return (prio < 0);
+}
+
+static void dbgPriority(struct stp_prio *prio, struct _port_id *id)
+{
+	int j;
+	u8 *data;
+
+	data = (u8 *) prio;
+	for (j = 0; j < sizeof(struct stp_prio); j++) {
+		if (8 == j || 12 == j)
+			dbg_msg(" ");
+		dbg_msg("%02x", data[j]);
+	}
+	if (id) {
+		dbg_msg(" ");
+		data = (u8 *) id;
+		for (j = 0; j < sizeof(struct _port_id); j++)
+			dbg_msg("%02x", data[j]);
+	}
+	dbg_msg("\n");
+}
+
+static int betterSamePriority(struct stp_prio *first, struct stp_prio *second)
+{
+	int prio;
+
+	prio = memcmp(first, second, sizeof(struct stp_prio));
+	return (prio <= 0);
+}
+
+static int betterVector(struct stp_vector *first, struct stp_vector *second)
+{
+	int prio;
+
+	prio = memcmp(&first->prio, &second->prio, sizeof(struct stp_prio));
+	if (!prio) {
+#ifdef DBG_STP_ROLE
+dbg_msg(" same prio\n");
+		dbgPriority(&first->prio, &first->port_id);
+		dbgPriority(&second->prio, &second->port_id);
+#endif
+		prio = memcmp(&first->port_id, &second->port_id,
+			sizeof(struct _port_id));
+	}
+	return (prio < 0);
+}
+
+
+#define FOREACH_P_IN_T(statements...)					\
+{									\
+	int c;								\
+	for (c = 0; c < p->br->port_cnt; c++) {				\
+		p = &p->br->ports[c];					\
+		statements						\
+	}								\
+}
+
+
+#if defined(DBG_STP_STATE) || defined(DBG)
+static void dbg_stp(struct ksz_stp_port *p, const char *msg, int rx)
+{
+	if (rx && !p->dbg_rx)
+		return;
+	dbg_msg(" %d=", p->port_index);
+	switch (role) {
+	case ROLE_DISABLED:
+		dbg_msg("Z");
+		break;
+	case ROLE_DESIGNATED:
+		dbg_msg("D");
+		break;
+	case ROLE_ROOT:
+		dbg_msg("R");
+		break;
+	case ROLE_ALTERNATE:
+		dbg_msg("A");
+		break;
+	case ROLE_BACKUP:
+		dbg_msg("B");
+		break;
+	default:
+		dbg_msg(".");
+	}
+	switch (selectedRole) {
+	case ROLE_DISABLED:
+		dbg_msg("z");
+		break;
+	case ROLE_DESIGNATED:
+		dbg_msg("d");
+		break;
+	case ROLE_ROOT:
+		dbg_msg("r");
+		break;
+	case ROLE_ALTERNATE:
+		dbg_msg("a");
+		break;
+	case ROLE_BACKUP:
+		dbg_msg("b");
+		break;
+	default:
+		dbg_msg(".");
+	}
+	dbg_msg(" ");
+	switch (infoIs) {
+	case INFO_TYPE_DISABLED:
+		dbg_msg("Z");
+		break;
+	case INFO_TYPE_MINE:
+		dbg_msg("M");
+		break;
+	case INFO_TYPE_AGED:
+		dbg_msg("A");
+		break;
+	case INFO_TYPE_RECEIVED:
+		dbg_msg("R");
+		break;
+	default:
+		dbg_msg(".");
+	}
+	dbg_msg(" ");
+	dbg_msg("%c", agree ? 'A' : '.');
+	dbg_msg("%c", agreed ? 'a' : '.');
+	dbg_msg("%c", proposing ? 'P' : '.');
+	dbg_msg("%c", proposed ? 'p' : '.');
+	dbg_msg("%c", sync ? 'S' : '.');
+	dbg_msg("%c", synced ? 's' : '.');
+	dbg_msg("%c", learn ? 'L' : '.');
+	dbg_msg("%c", learning ? 'l' : '.');
+	dbg_msg("%c", forward ? 'F' : '.');
+	dbg_msg("%c", forwarding ? 'f' : '.');
+	dbg_msg("%c", disputed ? 'D' : '.');
+	dbg_msg("%c", operEdge ? 'E' : '.');
+	dbg_msg("%c", reRoot ? 'R' : '.');
+	dbg_msg("%c", tcProp ? 'T' : '.');
+	dbg_msg("%c", rcvdTc ? 'C' : '.');
+	dbg_msg("  ");
+	for (rx = 0; rx < NUM_OF_PORT_STATE_MACHINES; rx++)
+		dbg_msg("%2d", p->states[rx]);
+	dbg_msg("  %s\n", msg);
+}  /* dbg_stp */
+
+static void d_stp_states(struct ksz_stp_bridge *br)
+{
+	int i;
+	struct ksz_stp_port *p;
+
+	dbg_msg("\n");
+	for (i = 0; i < br->port_cnt; i++) {
+		p = &br->ports[i];
+		if (p->off)
+			continue;
+		dbg_stp(p, "", false);
+	}
+}
+#endif
+
+
+#define PATH_COST		20000000
+
+static uint computePathCost(int speed)
+{
+	if (speed > 0)
+		return speed < PATH_COST ? PATH_COST / speed : 1;
+	else
+		return PATH_COST * 10;
+}  /* computePathCost */
+
+static int checkP2P(struct ksz_stp_port *p)
+{
+	u8 p2p;
+
+	if (ADMIN_P2P_AUTO == adminPointToPointMAC)
+		p2p = !!p->duplex;
+	else
+		p2p = (ADMIN_P2P_FORCE_TRUE == adminPointToPointMAC);
+	if (p2p != operPointToPointMAC) {
+		operPointToPointMAC = p2p;
+		return TRUE;
+	}
+	return FALSE;
+}  /* checkP2P */
+
+static int checkPathCost(struct ksz_stp_port *p)
+{
+	uint pathCost;
+
+	if (!AdminPortPathCost)
+		pathCost = computePathCost(p->speed);
+	else
+		pathCost = AdminPortPathCost;
+	if (pathCost != PortPathCost) {
+		PortPathCost = pathCost;
+		reselect = TRUE;
+		selected = FALSE;
+		return TRUE;
+	}
+	return FALSE;
+}  /* checkPathCost */
+
+static int checkParameters(int hello_time, int max_age, int fwd_delay)
+{
+	if (max_age < 6 || max_age > 40)
+		return FALSE;
+	if (fwd_delay < 4 || fwd_delay > 30)
+		return FALSE;
+	if (2 * (fwd_delay - 1) < max_age)
+		return FALSE;
+	if (max_age < 2 * (hello_time + 1))
+		return FALSE;
+	return TRUE;
+}  /* checkParameters */
+
+static void sw_cfg_forwarding(struct ksz_sw *sw, uint port, bool open)
+{
+	struct ksz_sw_info *info = sw->info;
+	u8 member = info->member[0];
+	uint m = BIT(port);
+
+	if (open)
+		member |= m;
+	else
+		member &= ~m;
+	if (member != info->member[0]) {
+		info->member[0] = member;
+		bridge_change(sw);
+#ifdef CONFIG_KSZ_MRP
+		if (open && (sw->features & MRP_SUPPORT)) {
+			struct mrp_info *mrp = &sw->mrp;
+
+			mrp_open_port(mrp, port);
+		}
+#endif
+	}
+}  /* sw_cfg_forwarding */
+
+static void doFlush_(struct ksz_stp_port *p)
+{
+	struct ksz_stp_info *stp = p->br->parent;
+	struct ksz_sw *sw = stp->sw_dev;
+	int i = p->port_index;
+#ifdef DBG_STP_PORT_FLUSH
+	struct ksz_stp_port *q;
+	struct ksz_stp_dbg_times *x;
+	struct ksz_stp_dbg_times *y;
+#endif
+
+	if (operEdge) {
+dbg_msg(" no flush\n");
+		fdbFlush = FALSE;
+		return;
+	}
+	sw->ops->acquire(sw);
+	sw_flush_dyn_mac_table(sw, i);
+	sw->ops->release(sw);
+	fdbFlush = FALSE;
+#ifdef DBG_STP_PORT_FLUSH
+	q = p;
+	y = &q->dbg_times[0];
+	for (i = 0; i < p->br->port_cnt; i++) {
+		p = &p->br->ports[i];
+		if (q == p)
+			continue;
+		x = &p->dbg_times[0];
+		if (y->lastPriority.port_id.num) {
+			int cmp = memcmp(&y->lastPriority.bridge_id,
+				&x->downPriority.bridge_id,
+				sizeof(struct _bridge_id));
+			if (!cmp && x->learn_jiffies) {
+dbg_msg(" %ld [%ld] %02x%02x ", jiffies - y->alt_jiffies,
+jiffies - x->learn_jiffies, y->lastPriority.port_id.prio, y->lastPriority.port_id.num);
+				x->learn_jiffies = 0;
+				y->lastPriority.port_id.num = 0;
+				y->learn_jiffies = 0;
+				y->alt_jiffies = 0;
+			}
+		}
+	}
+	p = q;
+	switch (y->role_ & ~0x80) {
+	case PORT_ROLE_ROOT:
+		dbg_msg("  R");
+		break;
+	case PORT_ROLE_ALTERNATE:
+		dbg_msg("  A");
+		break;
+	case PORT_ROLE_DESIGNATED:
+		if (RootPort)
+			dbg_msg("  r");
+		else if (AlternatePort)
+			dbg_msg("  a");
+		else
+			dbg_msg("  d");
+		break;
+	default:
+		if (DisabledPort)
+			dbg_msg("  Z");
+		else if (DesignatedPort)
+			dbg_msg("  D");
+		break;
+	}
+dbg_msg("  F:%d\n", q->port_index);
+#endif
+}  /* doFlush_ */
+
+#define doFlush()			doFlush_(p)
+
+static void stp_chk_flush(struct ksz_stp_port *p)
+{
+#ifdef DBG_STP_PORT_FLUSH
+	struct ksz_stp_port *q;
+	struct ksz_stp_bridge *br = p->br;
+	uint i;
+	struct ksz_stp_dbg_times *x = &p->dbg_times[0];
+	struct ksz_stp_dbg_times *y;
+
+	if (bpduRole == x->role_)
+		return;
+	x->role_ = bpduRole;
+	if (rcvdTc || !learning) {
+		int flush = 0;
+
+		for (i = 0; i < br->port_cnt; i++) {
+			q = &br->ports[i];
+			if (q == p)
+				continue;
+			y = &q->dbg_times[0];
+			if (y->downPriority.port_id.num) {
+				int cmp = memcmp(&y->downPriority.bridge_id,
+					&msgPriority.bridge_id,
+					sizeof(struct _bridge_id));
+				if (!cmp) {
+					if (learning)
+						x->learn_jiffies = jiffies;
+					flush = q->port_index + 1;
+					y->role_ = ROLE_ALT_BACKUP | 0x80;
+					break;
+				}
+			}
+		}
+dbg_msg("  %s %d=%d\n", __func__, p->port_index, flush);
+	}
+	if (bpduRole == ROLE_ALT_BACKUP)
+		x->alt_jiffies = jiffies;
+	if (bpduRole == ROLE_ROOT) {
+		COPY(x->downPriority, msgPriority);
+		COPY(x->lastPriority, msgPriority);
+	}
+#endif
+}
+
+static int allSynced_(struct ksz_stp_port *p)
+{
+	struct ksz_stp_port *q = p;
+	int ret = TRUE;
+	int all_others = FALSE;
+	int skip_root = FALSE;
+
+	if (DesignatedPort)
+		all_others = TRUE;
+	else if (!DisabledPort)
+		skip_root = TRUE;
+
+	/* Check only when role is not in transition. */
+	FOREACH_P_IN_T(
+		if (!(selected && (role == selectedRole) && !updtInfo)) {
+#if 0
+dbg_msg(" allSync: %d= %d %d %d %d\n", i, selected, role, selectedRole, synced);
+#endif
+			ret = FALSE;
+			break;
+		} else {
+			if ((all_others && p == q) ||
+			    (skip_root && RootPort))
+				continue;
+			if (!synced) {
+				ret = FALSE;
+				break;
+			}
+		}
+	)
+	return ret;
+}
+
+#define allSynced()			allSynced_(p)
+
+static int reRooted_(struct ksz_stp_port *p)
+{
+	struct ksz_stp_port *q = p;
+	int ret = TRUE;
+
+	FOREACH_P_IN_T(
+		if (p == q)
+			continue;
+		if (0 != rrWhile) {
+			ret = FALSE;
+			break;
+		}
+	)
+	return ret;
+}
+
+#define reRooted()			reRooted_(p)
+
+static int betterorsameInfo_(struct ksz_stp_port *p, u8 newInfoIs)
+{
+	int ret = FALSE;
+
+	if (INFO_TYPE_RECEIVED == newInfoIs && INFO_TYPE_RECEIVED == infoIs) {
+		ret = betterSamePriority(&msgPriority, &portPriority);
+#ifdef DBG_STP_ROLE
+dbg_msg("  recd: %d\n", ret);
+#endif
+	} else if (INFO_TYPE_MINE == newInfoIs && INFO_TYPE_MINE == infoIs) {
+		ret = betterSamePriority(&designatedPriority, &portPriority);
+#ifdef DBG_STP_ROLE
+dbg_msg("  mine: %d\n", ret);
+#endif
+#if 0
+	} else if (INFO_TYPE_MINE == newInfoIs && INFO_TYPE_AGED == infoIs) {
+		ret = betterSamePriority(&designatedPriority, &portPriority);
+dbgPriority(&designatedPriority, NULL);
+dbgPriority(&portPriority, NULL);
+#endif
+	}
+#ifdef DBG_STP_ROLE
+dbg_msg("%s %d\n", __func__, ret);
+#endif
+	return ret;
+}
+
+#define betterorsameInfo(i)		betterorsameInfo_(p, i)
+
+static void clearReselectTree_(struct ksz_stp_bridge *br)
+{
+	struct ksz_stp_port *p = &br->ports[0];
+
+	FOREACH_P_IN_T(
+		reselect = FALSE;
+	)
+}
+
+#define clearReselectTree()		clearReselectTree_(br)
+
+static void disableForwarding_(struct ksz_stp_port *p)
+{
+	struct ksz_stp_info *stp = p->br->parent;
+	struct ksz_sw *sw = stp->sw_dev;
+	int i = p->port_index;
+
+	sw->ops->acquire(sw);
+	if (!learning) {
+#ifdef DBG_STP_PORT_BLOCK
+		struct ksz_stp_dbg_times *x = &p->dbg_times[0];
+
+		x->block_jiffies = jiffies;
+#endif
+		if (portEnabled)
+			port_set_stp_state(sw, i, STP_STATE_BLOCKED);
+		sw_cfg_forwarding(sw, i, false);
+	} else {
+		port_cfg_rx(sw, i, false);
+		port_cfg_tx(sw, i, false);
+	}
+	sw->ops->release(sw);
+}
+
+#define disableForwarding()		disableForwarding_(p)
+
+static void disableLearning_(struct ksz_stp_port *p)
+{
+	struct ksz_stp_info *stp = p->br->parent;
+	struct ksz_sw *sw = stp->sw_dev;
+	int i = p->port_index;
+
+	sw->ops->acquire(sw);
+	port_cfg_dis_learn(sw, i, true);
+	sw->ops->release(sw);
+}
+
+#define disableLearning()		disableLearning_(p)
+
+static void enableForwarding_(struct ksz_stp_port *p)
+{
+	struct ksz_stp_info *stp = p->br->parent;
+	struct ksz_sw *sw = stp->sw_dev;
+	int i = p->port_index;
+
+	sw->ops->acquire(sw);
+#ifdef DBG_STP_PORT_BLOCK
+	do {
+		struct ksz_stp_dbg_times *x = &p->dbg_times[0];
+
+		if (x->block_jiffies)
+dbg_msg("  B:%d=%ld\n", p->port_index, jiffies - x->block_jiffies);
+		x->block_jiffies = 0;
+	} while (0);
+#endif
+	if (learning) {
+		port_set_stp_state(sw, i, STP_STATE_FORWARDING);
+		sw_cfg_forwarding(sw, i, true);
+	} else {
+		port_cfg_rx(sw, i, true);
+		port_cfg_tx(sw, i, true);
+	}
+	sw->ops->release(sw);
+}
+
+#define enableForwarding()		enableForwarding_(p)
+
+static void enableLearning_(struct ksz_stp_port *p)
+{
+	struct ksz_stp_info *stp = p->br->parent;
+	struct ksz_sw *sw = stp->sw_dev;
+	int i = p->port_index;
+
+	sw->ops->acquire(sw);
+	port_cfg_dis_learn(sw, i, false);
+	sw->ops->release(sw);
+#ifdef DBG_STP_PORT_BLOCK
+	p->dbg_times[0].learn_jiffies = jiffies;
+#endif
+}
+
+#define enableLearning()		enableLearning_(p)
+
+static void newTcDetected_(struct ksz_stp_port *p)
+{
+	if (tcDetected)
+		return;
+	if (sendRSTP) {
+		tcDetected = to_stp_timer(portTimes.hello_time + 1);
+	} else {
+		tcDetected = rootTimes.max_age + rootTimes.forward_delay;
+		tcDetected = to_stp_timer(tcDetected);
+	}
+	do {
+		struct ksz_stp_info *stp = p->br->parent;
+		struct ksz_sw *sw = stp->sw_dev;
+
+		sw->ops->tc_detected(sw, p->port_index);
+	} while (0);
+}
+
+#define newTcDetected()			newTcDetected_(p)
+
+static void newTcWhile_(struct ksz_stp_port *p)
+{
+	if (tcWhile)
+		return;
+	if (sendRSTP) {
+		tcWhile = HelloTime + to_stp_timer(1);
+		newInfo = TRUE;
+	} else {
+		tcWhile = rootTimes.max_age + rootTimes.forward_delay;
+		tcWhile = to_stp_timer(tcWhile);
+	}
+	if (!isTC) {
+		timeSinceTC = 0;
+		cntTC++;
+	}
+	isTC |= (1 << p->port_index);
+}
+
+#define newTcWhile()			newTcWhile_(p)
+
+static u8 rcvInfo_(struct ksz_stp_port *p)
+{
+	int prio;
+	int time;
+
+	if (BPDU_TYPE_TCN == bpduType) {
+		rcvdTcn = TRUE;
+		return INFO_OTHER;
+	}
+
+	if (BPDU_TYPE_CONFIG == bpduType)
+		bpduRole = ROLE_DESIGNATED;
+
+	COPY(msgPriority, bpduPriority);
+	COPY(msgTimes, bpduTimes);
+
+	prio = CMP(msgPriority, portPriority);
+	time = CMP(msgTimes, portTimes);
+	if (bpduRole == ROLE_DESIGNATED) {
+		if (superiorPriority(&msgPriority, &portPriority))
+			return INFO_SUPERIOR_DESIGNATED;
+
+		if (0 == prio && time != 0)
+			return INFO_SUPERIOR_DESIGNATED;
+
+		if (0 == prio && 0 == time)
+			return INFO_REPEATED_DESIGNATED;
+
+		if (prio > 0)
+			return INFO_INFERIOR_DESIGNATED;
+	}
+
+	if ((bpduRole == ROLE_ROOT || bpduRole == ROLE_ALT_BACKUP) &&
+	    prio >= 0)
+		return INFO_INFERIOR_ROOT_ALT;
+
+	return INFO_OTHER;
+}
+
+#define rcvInfo()			rcvInfo_(p)
+
+static void recordAgreement_(struct ksz_stp_port *p)
+{
+	/*
+	 * Not operPointToPointMAC will keep proposing and root port sending
+	 * agreement forever.
+	 */
+	if (rstpVersion && operPointToPointMAC &&
+	    (bpduFlags & AGREEMENT)) {
+		agreed = TRUE;
+		proposing = FALSE;
+	} else {
+		agreed = FALSE;
+	}
+}
+
+#define recordAgreement()		recordAgreement_(p)
+
+static void recordDispute_(struct ksz_stp_port *p)
+{
+	if ((bpduFlags & LEARNING)) {
+		disputed = TRUE;
+		agreed = FALSE;
+	}
+}
+
+#define recordDispute()			recordDispute_(p)
+
+static void recordPriority_(struct ksz_stp_port *p)
+{
+	COPY(portPriority, msgPriority);
+}
+
+#define recordPriority()		recordPriority_(p)
+
+static void recordProposal_(struct ksz_stp_port *p)
+{
+	if (bpduRole == ROLE_DESIGNATED && (bpduFlags & PROPOSAL))
+		proposed = TRUE;
+}
+
+#define recordProposal()		recordProposal_(p)
+
+#define MIN_COMPAT_HELLO_TIME		1
+
+static void recordTimes_(struct ksz_stp_port *p)
+{
+	if (checkParameters(2, msgTimes.max_age, msgTimes.forward_delay)) {
+		COPY(portTimes, msgTimes);
+
+		/* portTimes.hello_time is used to determine rcvdInfoWhile. */
+		if (portTimes.hello_time < MIN_COMPAT_HELLO_TIME)
+			portTimes.hello_time = MIN_COMPAT_HELLO_TIME;
+	} else
+		portTimes.message_age = msgTimes.message_age;
+}
+
+#define recordTimes()			recordTimes_(p)
+
+static void setReRootTree_(struct ksz_stp_port *p)
+{
+	FOREACH_P_IN_T(
+		reRoot = TRUE;
+	)
+}
+
+#define setReRootTree()			setReRootTree_(p)
+
+static void setSelectedTree_(struct ksz_stp_bridge *br)
+{
+	struct ksz_stp_port *p = &br->ports[0];
+
+	FOREACH_P_IN_T(
+		if (reselect)
+			return;
+	)
+	FOREACH_P_IN_T(
+		selected = TRUE;
+	)
+}
+
+#define setSelectedTree()		setSelectedTree_(br)
+
+static void setSyncTree_(struct ksz_stp_port *p)
+{
+	FOREACH_P_IN_T(
+		sync = TRUE;
+	)
+}
+
+#define setSyncTree()			setSyncTree_(p)
+
+static void setTcFlags_(struct ksz_stp_port *p)
+{
+	if (bpduFlags & TOPOLOGY_CHANGE)
+		rcvdTc = TRUE;
+	if (bpduFlags & TOPOLOGY_CHANGE_ACK)
+		rcvdTcAck = TRUE;
+}
+
+#define setTcFlags()			setTcFlags_(p)
+
+static void setTcPropTree_(struct ksz_stp_port *p)
+{
+	struct ksz_stp_port *q = p;
+
+#if 1
+	if (sendRSTP) {
+		if (tcPropWhile)
+			return;
+		tcPropWhile = HelloTime + to_stp_timer(1);
+	}
+#endif
+	FOREACH_P_IN_T(
+		if (p == q)
+			continue;
+		tcProp = TRUE;
+	)
+}
+
+#define setTcPropTree()			setTcPropTree_(p)
+
+static int stp_xmit(struct ksz_stp_info *stp, u8 port)
+{
+	int rc;
+	struct sk_buff *skb;
+	u8 *frame = stp->tx_frame;
+	struct ksz_sw *sw = stp->sw_dev;
+	int len = stp->len;
+	uint ports;
+	const struct net_device_ops *ops = stp->dev->netdev_ops;
+	struct llc *llc = (struct llc *) &frame[12];
+	struct ksz_port_info *info = get_port_info(sw, port);
+
+	/* Do not send if network device is not ready. */
+	if (!netif_running(stp->dev))
+		return 0;
+
+	ports = (1 << port);
+	ports |= TAIL_TAG_SET_OVERRIDE;
+	ports |= TAIL_TAG_SET_QUEUE;
+
+	len += 3;
+	llc->len = htons(len);
+	len += 14;
+	if (len < 60) {
+		memset(&frame[len], 0, 60 - len);
+		len = 60;
+	}
+
+	/* Add extra for tail tagging. */
+	skb = dev_alloc_skb(len + 4 + 8);
+	if (!skb)
+		return -ENOMEM;
+
+	memcpy(skb->data, stp->tx_frame, len);
+	memcpy(&skb->data[6], info->mac_addr, ETH_ALEN);
+
+	skb_put(skb, len);
+	sw->net_ops->add_tail_tag(sw, skb, ports);
+	skb->protocol = htons(STP_TAG_TYPE);
+	skb->dev = stp->dev;
+	do {
+		struct ksz_sw *sw = stp->sw_dev;
+
+		rc = ops->ndo_start_xmit(skb, skb->dev);
+		if (NETDEV_TX_BUSY == rc) {
+			rc = wait_event_interruptible_timeout(sw->queue,
+				!netif_queue_stopped(stp->dev),
+				50 * HZ / 1000);
+
+			rc = NETDEV_TX_BUSY;
+		}
+	} while (NETDEV_TX_BUSY == rc);
+	return rc;
+}  /* stp_xmit */
+
+static void txConfig_(struct ksz_stp_port *p)
+{
+	int rc;
+	struct ksz_stp_info *stp = p->br->parent;
+	struct bpdu *bpdu = stp->bpdu;
+
+	prep_stp(bpdu, &designatedPriority, &designatedTimes);
+	bpdu->flags = 0;
+	if (tcWhile != 0)
+		bpdu->flags |= TOPOLOGY_CHANGE;
+	if (tcAck)
+		bpdu->flags |= TOPOLOGY_CHANGE_ACK;
+
+	stp->len = sizeof(struct bpdu) - 1;
+	rc = stp_xmit(stp, p->port_index);
+}
+
+#define txConfig()			txConfig_(p)
+
+static void txRstp_(struct ksz_stp_port *p)
+{
+	int rc;
+	struct ksz_stp_info *stp = p->br->parent;
+	struct bpdu *bpdu = stp->bpdu;
+	u8 r = role;
+
+if (!portEnabled)
+dbg_msg(" ?! %s %d %d\n", __func__, p->port_index, r);
+	prep_rstp(bpdu, &designatedPriority, &designatedTimes);
+	if (AlternatePort || BackupPort)
+		r = PORT_ROLE_ALTERNATE;
+if (r != PORT_ROLE_ALTERNATE && r != PORT_ROLE_ROOT && r != PORT_ROLE_DESIGNATED)
+dbg_msg("  invalid role %d\n", r);
+	r &= PORT_ROLE_DESIGNATED;
+	r <<= PORT_ROLE_S;
+	bpdu->flags = r;
+	if (tcWhile != 0)
+		bpdu->flags |= TOPOLOGY_CHANGE;
+	if (agree)
+		bpdu->flags |= AGREEMENT;
+	if (proposing)
+		bpdu->flags |= PROPOSAL;
+	if (learning)
+		bpdu->flags |= LEARNING;
+	if (forwarding)
+		bpdu->flags |= FORWARDING;
+	if (tcAck)
+		bpdu->flags |= TOPOLOGY_CHANGE_ACK;
+	stp->len = sizeof(struct bpdu);
+
+#ifdef DBG_STP_TX
+	do {
+		int cmp = memcmp(&p->tx_bpdu0, bpdu, stp->len);
+
+		if (cmp) {
+			p->dbg_tx++;
+			memcpy(&p->tx_bpdu0, bpdu, stp->len);
+		}
+		if (p->dbg_tx) {
+			dbg_msg("<T> %d: ", p->port_index);
+			disp_bpdu(bpdu);
+			p->dbg_tx--;
+		}
+	} while (0);
+#endif
+	rc = stp_xmit(stp, p->port_index);
+}
+
+#define txRstp()			txRstp_(p)
+
+static void txTcn_(struct ksz_stp_port *p)
+{
+	int rc;
+	struct ksz_stp_info *stp = p->br->parent;
+	struct bpdu *bpdu = stp->bpdu;
+
+	bpdu->protocol = 0;
+	bpdu->version = 0;
+	bpdu->type = BPDU_TYPE_TCN;
+
+	stp->len = 4;
+	rc = stp_xmit(stp, p->port_index);
+}
+
+#define txTcn()				txTcn_(p)
+
+static void updtBPDUVersion_(struct ksz_stp_port *p)
+{
+	if ((0 == bpduVersion || 1 == bpduVersion) &&
+	    (BPDU_TYPE_TCN == bpduType || BPDU_TYPE_CONFIG == bpduType))
+		rcvdSTP = TRUE;
+	if (BPDU_TYPE_CONFIG_RSTP == bpduType)
+		rcvdRSTP = TRUE;
+}
+
+#define updtBPDUVersion()		updtBPDUVersion_(p)
+
+static void updtRcvdInfoWhile_(struct ksz_stp_port *p)
+{
+	/*
+	 * Definition is not clear!
+	 * It is mentioned several times that Message Age should be less than
+	 * Max Age in BPDU to be accepted.
+	 */
+	if (portTimes.message_age + 1 <= portTimes.max_age)
+		rcvdInfoWhile = to_stp_timer(3 * portTimes.hello_time);
+	else
+		rcvdInfoWhile = 0;
+}
+
+#define updtRcvdInfoWhile()		updtRcvdInfoWhile_(p)
+
+static void updtRoleDisabledTree_(struct ksz_stp_bridge *br)
+{
+	struct ksz_stp_port *p = &br->ports[0];
+
+	FOREACH_P_IN_T(
+		selectedRole = ROLE_DISABLED;
+	)
+}
+
+#define updtRoleDisabledTree()		updtRoleDisabledTree_(br)
+
+static u32 add_path_cost(u32 x, u32 y)
+{
+	u32 z;
+
+	z = ntohl(x) + y;
+	return htonl(z);
+}
+
+static void updtRolesTree_(struct ksz_stp_bridge *br)
+{
+	int better;
+	uint i;
+	int id;
+	int prio;
+	int time;
+	struct ksz_stp_port *p = &br->ports[0];
+	struct stp_vector root_path;
+	struct stp_vector best_path;
+	struct stp_prio *root_Priority = NULL;
+	struct _port_id root_PortId;
+	struct ksz_stp_port *q = NULL;
+#ifdef DBG_STP_PORT_FLUSH
+	struct ksz_stp_dbg_times *x = &p->dbg_times[0];
+#endif
+
+	COPY(root_PortId, BridgePriority.port_id);
+
+	/* Find out the best path from all ports. */
+	for (i = 0; i < br->port_cnt; i++) {
+		p = &br->ports[i];
+
+		/* Check only after receiving new BPDU. */
+		if (infoIs != INFO_TYPE_RECEIVED)
+			continue;
+
+		/* Not from self. */
+		prio = CMP(portPriority.bridge_id.addr, BridgeIdentifier.addr);
+		if (!prio)
+			continue;
+
+		/* Ports may receive same BPDU when coming through a hub. */
+		COPY_PRIO(root_path, portPriority, portId);
+		root_path.prio.root_path_cost = add_path_cost(
+			root_path.prio.root_path_cost, PortPathCost);
+
+		better = FALSE;
+		if (!root_Priority)
+			better = TRUE;
+		else if (betterVector(&root_path, &best_path))
+			better = TRUE;
+		if (better) {
+			COPY(best_path, root_path);
+			root_Priority = &portPriority;
+			COPY(root_PortId, portId);
+			q = p;
+		}
+	}
+#ifdef DBG_STP_ROLE
+if (root_Priority) {
+dbg_msg(" best \n");
+dbgPriority(&best_path.prio, &best_path.port_id);
+dbg_msg(" root prio\n");
+dbgPriority(root_Priority, &root_PortId);
+}
+#endif
+
+	/* Compare with the bridge. */
+	better = FALSE;
+	if (!root_Priority)
+		better = TRUE;
+	else if (betterVector(&BridgePriority, &best_path))
+		better = TRUE;
+	if (better) {
+		COPY(best_path, BridgePriority);
+		root_Priority = NULL;
+		COPY(root_PortId, BridgePriority.port_id);
+	}
+
+	COPY(rootPriority, best_path.prio);
+	COPY(rootPortId, best_path.port_id);
+
+	if (root_Priority) {
+		struct ksz_stp_info *stp = br->parent;
+		struct ksz_sw *sw = stp->sw_dev;
+		uint num;
+
+		num = get_phy_port(sw, root_PortId.num);
+		p = &br->ports[num];
+		COPY(rootTimes, portTimes);
+		rootTimes.message_age += 1;
+	} else {
+		COPY(rootTimes, BridgeTimes);
+	}
+
+	for (i = 0; i < br->port_cnt; i++) {
+		p = &br->ports[i];
+
+#ifdef DBG_STP_PORT_FLUSH
+		x = &p->dbg_times[0];
+#endif
+
+		COPY(designatedPriority, rootPriority);
+		COPY(designatedPriority.bridge_id, BridgeIdentifier);
+		COPY(designatedPriority.port_id, portId);
+
+		COPY(designatedTimes, rootTimes);
+		designatedTimes.hello_time = BridgeTimes.hello_time;
+
+		switch (infoIs) {
+		case INFO_TYPE_DISABLED:
+			selectedRole = ROLE_DISABLED;
+#ifdef DBG_STP_PORT_FLUSH
+			x->role_ = ROLE_DISABLED;
+			x->downPriority.port_id.num = 0;
+			x->learn_jiffies = 0;
+#endif
+			break;
+		case INFO_TYPE_AGED:
+			updtInfo = TRUE;
+			selectedRole = ROLE_DESIGNATED;
+#ifdef DBG_STP_PORT_FLUSH
+			if (ROLE_DISABLED == x->role_)
+				x->role_ = ROLE_DESIGNATED;
+#endif
+#ifdef DBG_STP_RX
+p->dbg_rx = 3;
+#endif
+#ifdef DBG_STP_TX
+p->dbg_tx = 2;
+#endif
+			break;
+		case INFO_TYPE_MINE:
+			selectedRole = ROLE_DESIGNATED;
+			prio = CMP(designatedPriority, portPriority);
+			time = CMP(designatedTimes, portTimes);
+			if (prio || time)
+				updtInfo = TRUE;
+#ifdef DBG_STP_PORT_FLUSH
+			if (ROLE_DISABLED == x->role_)
+				x->role_ = ROLE_DESIGNATED;
+#endif
+#ifdef DBG_STP_RX
+p->dbg_rx = 3;
+#endif
+#ifdef DBG_STP_TX
+p->dbg_tx = 1;
+#endif
+			break;
+		case INFO_TYPE_RECEIVED:
+			if (root_Priority == &portPriority) {
+				selectedRole = ROLE_ROOT;
+				updtInfo = FALSE;
+#ifdef DBG_STP_PORT_FLUSH
+				x->role_ = ROLE_DESIGNATED;
+#endif
+			} else {
+				prio = CMP(designatedPriority, portPriority);
+				if (prio >= 0) {
+					id = CMP(portPriority.bridge_id.addr,
+						BridgeIdentifier.addr);
+					if (id)
+						selectedRole = ROLE_ALTERNATE;
+					else
+						selectedRole = ROLE_BACKUP;
+					updtInfo = FALSE;
+#ifdef DBG_STP_PORT_FLUSH
+					x->role_ = ROLE_DESIGNATED;
+#endif
+				} else {
+					selectedRole = ROLE_DESIGNATED;
+					updtInfo = TRUE;
+				}
+			}
+#ifdef DBG_STP_RX
+p->dbg_rx = 3;
+#endif
+#ifdef DBG_STP_TX
+p->dbg_tx = 3;
+#endif
+			break;
+		default:
+dbg_msg("unknown\n");
+		}
+#ifdef DBG_STP_ROLE
+if (!p->off)
+dbg_msg("  %s %d %d\n", __func__, p->port_index, selectedRole);
+#endif
+	}
+}
+
+#define updtRolesTree()			updtRolesTree_(br)
+
+
+enum {
+	STP_BEGIN,
+	STP_LAST
+};
+
+enum {
+	STP_PortTimers_ONE_SECOND = STP_LAST,
+	STP_PortTimers_TICK,
+};
+
+enum {
+	STP_PortReceive_DISCARD = STP_LAST,
+	STP_PortReceive_RECEIVE,
+};
+
+enum {
+	STP_PortProtoMigr_CHECKING_RSTP = STP_LAST,
+	STP_PortProtoMigr_SELECTING_STP,
+	STP_PortProtoMigr_SENSING,
+};
+
+enum {
+	STP_BridgeDetection_EDGE = STP_LAST,
+	STP_BridgeDetection_NOT_EDGE,
+};
+
+enum {
+	STP_PortTransmit_INIT = STP_LAST,
+	STP_PortTransmit_PERIODIC,
+	STP_PortTransmit_CONFIG,
+	STP_PortTransmit_TCN,
+	STP_PortTransmit_RSTP,
+	STP_PortTransmit_IDLE,
+};
+
+enum {
+	STP_PortInfo_DISABLED = STP_LAST,
+	STP_PortInfo_AGED,
+	STP_PortInfo_UPDATE,
+	STP_PortInfo_SUPERIOR_DESIGNATED,
+	STP_PortInfo_REPEATED_DESIGNATED,
+	STP_PortInfo_INFERIOR_DESIGNATED,
+	STP_PortInfo_NOT_DESIGNATED,
+	STP_PortInfo_OTHER,
+	STP_PortInfo_CURRENT,
+	STP_PortInfo_RECEIVE,
+};
+
+enum {
+	STP_PortRoleSel_INIT_BRIDGE = STP_LAST,
+	STP_PortRoleSel_ROLE_SELECTION,
+};
+
+enum {
+	STP_PortRoleTrans_INIT_PORT = STP_LAST,
+	STP_PortRoleTrans_DISABLE_PORT,
+	STP_PortRoleTrans_DISABLED_PORT,
+	STP_PortRoleTrans_ROOT_PROPOSED,
+	STP_PortRoleTrans_ROOT_AGREED,
+	STP_PortRoleTrans_ROOT_SYNCED,
+	STP_PortRoleTrans_REROOT,
+	STP_PortRoleTrans_REROOTED,
+	STP_PortRoleTrans_ROOT_LEARN,
+	STP_PortRoleTrans_ROOT_FORWARD,
+	STP_PortRoleTrans_ROOT_PORT,
+	STP_PortRoleTrans_DESIGNATED_PROPOSE,
+	STP_PortRoleTrans_DESIGNATED_SYNCED,
+	STP_PortRoleTrans_DESIGNATED_RETIRED,
+	STP_PortRoleTrans_DESIGNATED_DISCARD,
+	STP_PortRoleTrans_DESIGNATED_LEARN,
+	STP_PortRoleTrans_DESIGNATED_FORWARD,
+	STP_PortRoleTrans_DESIGNATED_PORT,
+	STP_PortRoleTrans_BLOCK_PORT,
+	STP_PortRoleTrans_ALTERNATE_PROPOSED,
+	STP_PortRoleTrans_ALTERNATE_AGREED,
+	STP_PortRoleTrans_BACKUP_PORT,
+	STP_PortRoleTrans_ALTERNATE_PORT,
+};
+
+enum {
+	STP_PortStateTrans_DISCARDING = STP_LAST,
+	STP_PortStateTrans_LEARNING,
+	STP_PortStateTrans_FORWARDING,
+};
+
+enum {
+	STP_TopologyChange_INACTIVE = STP_LAST,
+	STP_TopologyChange_LEARNING,
+	STP_TopologyChange_DETECTED,
+	STP_TopologyChange_ACKNOWLEDGED,
+	STP_TopologyChange_PROPAGATING,
+	STP_TopologyChange_NOTIFIED_TC,
+	STP_TopologyChange_NOTIFIED_TCN,
+	STP_TopologyChange_ACTIVE,
+};
+
+enum {
+	STP_PortTimers,
+	STP_PortReceive,
+	STP_PortProtoMigr,
+	STP_BridgeDetection,
+	STP_PortInfo,
+	STP_PortRoleTrans,
+	STP_PortStateTrans,
+	STP_TopologyChange,
+	STP_PortTransmit,
+
+	STP_PortRoleSel,
+};
+
+#ifdef DBG_STP_STATE
+static char *PortTimers_names[] = {
+	"ONE_SECOND",
+	"TICK",
+};
+
+static char *PortReceive_names[] = {
+	"DISCARD,"
+	"RECEIVE",
+};
+
+static char *PortProtoMigr_names[] = {
+	"CHECKING_RSTP",
+	"SELECTING_STP",
+	"SENSING",
+};
+
+static char *BridgeDetection_names[] = {
+	"EDGE",
+	"NOT_EDGE",
+};
+
+static char *PortTransmit_names[] = {
+	"INIT",
+	"PERIODIC",
+	"CONFIG",
+	"TCN",
+	"RSTP",
+	"IDLE",
+};
+
+static char *PortInfo_names[] = {
+	"DISABLED",
+	"AGED",
+	"UPDATE",
+	"SUPERIOR_DESIGNATED",
+	"REPEATED_DESIGNATED",
+	"INFERIOR_DESIGNATED",
+	"NOT_DESIGNATED",
+	"OTHER",
+	"CURRENT",
+	"RECEIVE",
+};
+
+static char *PortRoleSel_names[] = {
+	"INIT_BRIDGE",
+	"ROLE_SELECTION",
+};
+
+static char *PortRoleTrans_names[] = {
+	"INIT_PORT",
+	"DISABLE_PORT",
+	"DISABLED_PORT",
+	"ROOT_PROPOSED",
+	"ROOT_AGREED",
+	"ROOT_SYNCED",
+	"REROOT",
+	"REROOTED",
+	"ROOT_LEARN",
+	"ROOT_FORWARD",
+	"ROOT_PORT",
+	"DESIGNATED_PROPOSE",
+	"DESIGNATED_SYNCED",
+	"DESIGNATED_RETIRED",
+	"DESIGNATED_DISCARD",
+	"DESIGNATED_LEARN",
+	"DESIGNATED_FORWARD",
+	"DESIGNATED_PORT",
+	"BLOCK_PORT",
+	"ALTERNATE_PROPOSED",
+	"ALTERNATE_AGREED",
+	"BACKUP_PORT",
+	"ALTERNATE_PORT",
+};
+
+static char *PortStateTrans_names[] = {
+	"DISCARDING",
+	"LEARNING",
+	"FORWARDING",
+};
+
+static char *TopologyChange_names[] = {
+	"INACTIVE",
+	"LEARNING",
+	"DETECTED",
+	"ACKNOWLEDGED",
+	"PROPAGATING",
+	"NOTIFIED_TC",
+	"NOTIFIED_TCN",
+	"ACTIVE",
+};
+
+static char **stp_state_names[] = {
+	PortTimers_names,
+	PortReceive_names,
+	PortProtoMigr_names,
+	BridgeDetection_names,
+	PortInfo_names,
+	PortRoleTrans_names,
+	PortStateTrans_names,
+	TopologyChange_names,
+	PortTransmit_names,
+
+	PortRoleSel_names,
+};
+#endif
+
+
+struct ksz_stp_state {
+	int index;
+	int change;
+	int new_state;
+};
+
+static int stp_proc_state(struct ksz_stp_port *p, struct ksz_stp_state *state,
+	void (*state_init)(struct ksz_stp_port *p),
+	void (*state_next)(struct ksz_stp_port *p, struct ksz_stp_state *state))
+{
+	if (state->new_state) {
+		state->new_state = 0;
+		state_init(p);
+	}
+	state_next(p, state);
+	return 0;
+}  /* stp_proc_state */
+
+static void stp_change_state(struct ksz_stp_state *state, int cond, int new)
+{
+	if (!cond)
+		return;
+	if (state->new_state) {
+#ifdef DBG_STP_STATE
+		if (state->change) {
+			char **names = stp_state_names[state->index];
+			char *last;
+			char *next;
+
+			if (state->new_state != STP_BEGIN)
+				last = names[state->new_state - STP_LAST];
+			else
+				last = "BEGIN";
+			if (new != STP_BEGIN)
+				next = names[new - STP_LAST];
+			else
+				next = "BEGIN";
+dbg_msg("  %s %d %s %d %s\n", __func__, state->new_state, last, new, next);
+		}
+#endif
+		return;
+	}
+	state->new_state = new;
+}  /* stp_change_state */
+
+
+static void stp_one_sec_init(struct ksz_stp_port *p)
+{
+	tick = FALSE;
+}  /* stp_one_sec_init */
+
+static void stp_one_sec_next(struct ksz_stp_port *p,
+	struct ksz_stp_state *state)
+{
+	stp_change_state(state, (tick), STP_PortTimers_TICK);
+}  /* stp_one_sec_next */
+
+#define dec_timer(x)	if (x) x -= STP_TIMER_TICK
+#define dec(x)		if (x) x--
+
+static void stp_tick_init(struct ksz_stp_port *p)
+{
+	int i;
+
+	for (i = 0; i < NUM_OF_PORT_TIMERS; i++)
+		dec_timer(p->vars.timers[i]);
+	if (!tcWhile)
+		isTC &= ~(1 << p->port_index);
+	if (!p->port_index && !(helloWhen % STP_TIMER_SCALE))
+		timeSinceTC++;
+	if (!(helloWhen % STP_TIMER_SCALE))
+		dec(txCount);
+}  /* stp_tick_init */
+
+static void stp_tick_next(struct ksz_stp_port *p,
+	struct ksz_stp_state *state)
+{
+	stp_change_state(state, TRUE, STP_PortTimers_ONE_SECOND);
+}  /* stp_tick_next */
+
+static int PortTimers(struct ksz_stp_port *p)
+{
+	struct ksz_stp_state state_info;
+	u8 *state = &p->states[p->state_index];
+	int ret = 0;
+
+	state_info.index = STP_PortTimers;
+	state_info.change = 1;
+	state_info.new_state = 0;
+	if (STP_BEGIN == *state) {
+		state_info.new_state = STP_PortTimers_ONE_SECOND;
+	}
+	do {
+
+		/* There is a new state. */
+		if (state_info.new_state) {
+			*state = state_info.new_state;
+			ret = state_info.change;
+			ret = 1;
+		}
+		switch (*state) {
+		case STP_PortTimers_ONE_SECOND:
+			if (stp_proc_state(p, &state_info,
+			    stp_one_sec_init, stp_one_sec_next))
+				goto done;
+			break;
+		case STP_PortTimers_TICK:
+			if (stp_proc_state(p, &state_info,
+			    stp_tick_init, stp_tick_next))
+				goto done;
+			break;
+		}
+	} while (state_info.new_state);
+
+done:
+	return ret;
+}  /* PortTimers */
+
+static void stp_rx_discard_init(struct ksz_stp_port *p)
+{
+#ifdef DBG_STP_STATE_RX
+	dbg_stp(p, __func__, false);
+#endif
+	rcvdBPDU = rcvdRSTP = rcvdSTP = FALSE;
+	rcvdMsg = FALSE;
+	edgeDelayWhile = MigrateTime;
+}  /* stp_rx_discard_init */
+
+static void stp_rx_discard_next(struct ksz_stp_port *p,
+	struct ksz_stp_state *state)
+{
+	stp_change_state(state,
+		(rcvdBPDU && portEnabled),
+		STP_PortReceive_RECEIVE);
+}  /* stp_rx_discard_next */
+
+static void stp_rx_receive_init(struct ksz_stp_port *p)
+{
+#ifdef DBG_STP_STATE_RX
+	dbg_stp(p, __func__, true);
+#endif
+	updtBPDUVersion();
+	operEdge = rcvdBPDU = FALSE;
+	rcvdMsg = TRUE;
+	edgeDelayWhile = MigrateTime;
+}  /* stp_rx_receive_init */
+
+static void stp_rx_receive_next(struct ksz_stp_port *p,
+	struct ksz_stp_state *state)
+{
+	stp_change_state(state,
+		(rcvdBPDU && portEnabled && !rcvdMsg),
+		STP_PortReceive_RECEIVE);
+}  /* stp_rx_receive_next */
+
+static void stp_rx_next(struct ksz_stp_port *p,
+	struct ksz_stp_state *state)
+{
+	/* Reset rcvdBPDU and edgeDelayWhile if !portEnabled. */
+	stp_change_state(state,
+		((rcvdBPDU || NEQ(edgeDelayWhile, MigrateTime)) &&
+		!portEnabled),
+		STP_PortReceive_DISCARD);
+}  /* stp_rx_next */
+
+static int PortReceive(struct ksz_stp_port *p)
+{
+	struct ksz_stp_state state_info;
+	u8 *state = &p->states[p->state_index];
+	int ret = 0;
+
+	state_info.index = STP_PortReceive;
+	state_info.change = 1;
+	state_info.new_state = 0;
+	if (STP_BEGIN == *state) {
+		state_info.new_state = STP_PortReceive_DISCARD;
+	} else
+		stp_rx_next(p, &state_info);
+	do {
+
+		/* There is a new state. */
+		if (state_info.new_state) {
+			*state = state_info.new_state;
+			ret = state_info.change;
+			ret = 1;
+		}
+		switch (*state) {
+		case STP_PortReceive_DISCARD:
+			if (stp_proc_state(p, &state_info,
+			    stp_rx_discard_init, stp_rx_discard_next))
+				goto done;
+			break;
+		case STP_PortReceive_RECEIVE:
+			if (stp_proc_state(p, &state_info,
+			    stp_rx_receive_init, stp_rx_receive_next))
+				goto done;
+			break;
+		}
+	} while (state_info.new_state);
+
+done:
+	return ret;
+}  /* PortReceive */
+
+static void stp_proto_check_init(struct ksz_stp_port *p)
+{
+#ifdef DBG_STP_STATE_PROTO
+	dbg_stp(p, __func__, false);
+#endif
+	mcheck = FALSE;
+	sendRSTP = rstpVersion;
+	mdelayWhile = MigrateTime;
+}  /* stp_proto_check_init */
+
+static void stp_proto_check_next(struct ksz_stp_port *p,
+	struct ksz_stp_state *state)
+{
+	stp_change_state(state,
+		(0 == mdelayWhile),
+		STP_PortProtoMigr_SENSING);
+
+	/* Reset mdelayWhile if !portEnabled */
+	stp_change_state(state,
+		(NEQ(mdelayWhile, MigrateTime) && !portEnabled),
+		STP_PortProtoMigr_CHECKING_RSTP);
+}  /* stp_proto_check_next */
+
+static void stp_proto_select_init(struct ksz_stp_port *p)
+{
+#ifdef DBG_STP_STATE
+	dbg_stp(p, __func__, false);
+#endif
+	sendRSTP = FALSE;
+	mdelayWhile = MigrateTime;
+}  /* stp_proto_select_init */
+
+static void stp_proto_select_next(struct ksz_stp_port *p,
+	struct ksz_stp_state *state)
+{
+	stp_change_state(state,
+		((0 == mdelayWhile) || !portEnabled || mcheck),
+		STP_PortProtoMigr_SENSING);
+}  /* stp_proto_select_next */
+
+static void stp_proto_sense_init(struct ksz_stp_port *p)
+{
+#ifdef DBG_STP_STATE
+	dbg_stp(p, __func__, false);
+#endif
+	rcvdRSTP = rcvdSTP = FALSE;
+}  /* stp_proto_sense_init */
+
+static void stp_proto_sense_next(struct ksz_stp_port *p,
+	struct ksz_stp_state *state)
+{
+	stp_change_state(state,
+		(!portEnabled || mcheck || (rstpVersion && !sendRSTP &&
+		rcvdRSTP)),
+		STP_PortProtoMigr_CHECKING_RSTP);
+	stp_change_state(state,
+		(sendRSTP && rcvdSTP),
+		STP_PortProtoMigr_SELECTING_STP);
+}  /* stp_proto_sense_next */
+
+static int PortProtocolMigration(struct ksz_stp_port *p)
+{
+	struct ksz_stp_state state_info;
+	u8 *state = &p->states[p->state_index];
+	int ret = 0;
+
+	state_info.index = STP_PortProtoMigr;
+	state_info.change = 1;
+	state_info.new_state = 0;
+	if (STP_BEGIN == *state) {
+		state_info.new_state = STP_PortProtoMigr_CHECKING_RSTP;
+	}
+	do {
+
+		/* There is a new state. */
+		if (state_info.new_state) {
+			*state = state_info.new_state;
+			ret = state_info.change;
+			ret = 1;
+		}
+		switch (*state) {
+		case STP_PortProtoMigr_CHECKING_RSTP:
+			if (stp_proc_state(p, &state_info,
+			    stp_proto_check_init, stp_proto_check_next))
+				goto done;
+			break;
+		case STP_PortProtoMigr_SELECTING_STP:
+			if (stp_proc_state(p, &state_info,
+			    stp_proto_select_init, stp_proto_select_next))
+				goto done;
+			break;
+		case STP_PortProtoMigr_SENSING:
+			if (stp_proc_state(p, &state_info,
+			    stp_proto_sense_init, stp_proto_sense_next))
+				goto done;
+			break;
+		}
+	} while (state_info.new_state);
+
+done:
+	return ret;
+}  /* PortProtocolMigration */
+
+static void stp_br_det_edge_init(struct ksz_stp_port *p)
+{
+#ifdef DBG_STP_STATE
+	dbg_stp(p, __func__, false);
+#endif
+	operEdge = TRUE;
+do {
+	struct ksz_stp_dbg_times *x = &p->dbg_times[0];
+
+if (x->block_jiffies)
+dbg_msg(" b: %ld %s\n", jiffies - x->block_jiffies, __func__);
+} while (0);
+}  /* stp_br_det_edge_init */
+
+static void stp_br_det_edge_next(struct ksz_stp_port *p,
+	struct ksz_stp_state *state)
+{
+	stp_change_state(state,
+		((!portEnabled && !AdminEdge) || !operEdge),
+		STP_BridgeDetection_NOT_EDGE);
+}  /* stp_br_det_edge_next */
+
+static void stp_br_det_not_init(struct ksz_stp_port *p)
+{
+#ifdef DBG_STP_STATE_BR_DET
+	dbg_stp(p, __func__, false);
+#endif
+	operEdge = FALSE;
+}  /* stp_br_det_not_init */
+
+static void stp_br_det_not_next(struct ksz_stp_port *p,
+	struct ksz_stp_state *state)
+{
+	stp_change_state(state,
+		((!portEnabled && AdminEdge) ||
+		((0 == edgeDelayWhile) && AutoEdge && sendRSTP && proposing)),
+		STP_BridgeDetection_EDGE);
+}  /* stp_br_det_not_next */
+
+static int BridgeDetection(struct ksz_stp_port *p)
+{
+	struct ksz_stp_state state_info;
+	u8 *state = &p->states[p->state_index];
+	int ret = 0;
+
+	state_info.index = STP_BridgeDetection;
+	state_info.change = 1;
+	state_info.new_state = 0;
+	if (STP_BEGIN == *state) {
+		state_info.new_state = AdminEdge ?
+			STP_BridgeDetection_EDGE :
+			STP_BridgeDetection_NOT_EDGE;
+	}
+	do {
+
+		/* There is a new state. */
+		if (state_info.new_state) {
+			*state = state_info.new_state;
+			ret = state_info.change;
+			ret = 1;
+		}
+		switch (*state) {
+		case STP_BridgeDetection_EDGE:
+			if (stp_proc_state(p, &state_info,
+			    stp_br_det_edge_init, stp_br_det_edge_next))
+				goto done;
+			break;
+		case STP_BridgeDetection_NOT_EDGE:
+			if (stp_proc_state(p, &state_info,
+			    stp_br_det_not_init, stp_br_det_not_next))
+				goto done;
+			break;
+		}
+	} while (state_info.new_state);
+
+done:
+	return ret;
+}  /* BridgeDetection */
+
+static void stp_tx_init_init(struct ksz_stp_port *p)
+{
+#ifdef DBG_STP_STATE_TX
+	dbg_stp(p, __func__, false);
+#endif
+#if 0
+#if 0
+	/* Send RSTP after initialization when the port is disabled? */
+	if (!sendRSTP)
+#endif
+	newInfo = TRUE;
+#endif
+	txCount = 0;
+}  /* stp_tx_init_init */
+
+static void stp_tx_init_next(struct ksz_stp_port *p,
+	struct ksz_stp_state *state)
+{
+	stp_change_state(state,
+		TRUE, STP_PortTransmit_IDLE);
+}  /* stp_tx_init_next */
+
+static void stp_tx_periodic_init(struct ksz_stp_port *p)
+{
+#ifdef DBG_STP_STATE_TX_0
+	dbg_stp(p, __func__, false);
+#endif
+	newInfo = newInfo || (DesignatedPort || (RootPort && (tcWhile != 0)));
+}  /* stp_tx_periodic_init */
+
+static void stp_tx_periodic_next(struct ksz_stp_port *p,
+	struct ksz_stp_state *state)
+{
+	stp_change_state(state,
+		TRUE, STP_PortTransmit_IDLE);
+}  /* stp_tx_periodic_next */
+
+static void stp_tx_config_init(struct ksz_stp_port *p)
+{
+#ifdef DBG_STP_STATE_TX
+	dbg_stp(p, __func__, false);
+#endif
+	newInfo = FALSE;
+	txConfig();
+	txCount++;
+	tcAck = FALSE;
+}  /* stp_tx_config_init */
+
+static void stp_tx_config_next(struct ksz_stp_port *p,
+	struct ksz_stp_state *state)
+{
+	stp_change_state(state,
+		TRUE, STP_PortTransmit_IDLE);
+}  /* stp_tx_config_next */
+
+static void stp_tx_tcn_init(struct ksz_stp_port *p)
+{
+#ifdef DBG_STP_STATE_TX
+	dbg_stp(p, __func__, false);
+#endif
+	newInfo = FALSE;
+	txTcn();
+	txCount++;
+}  /* stp_tx_tcn_init */
+
+static void stp_tx_tcn_next(struct ksz_stp_port *p,
+	struct ksz_stp_state *state)
+{
+	stp_change_state(state,
+		TRUE, STP_PortTransmit_IDLE);
+}  /* stp_tx_tcn_next */
+
+static void stp_tx_rstp_init(struct ksz_stp_port *p)
+{
+#ifdef DBG_STP_STATE_TX
+	dbg_stp(p, __func__, false);
+#endif
+	newInfo = FALSE;
+	txRstp();
+	txCount++;
+	tcAck = FALSE;
+}  /* stp_tx_rstp_init */
+
+static void stp_tx_rstp_next(struct ksz_stp_port *p,
+	struct ksz_stp_state *state)
+{
+	stp_change_state(state,
+		TRUE, STP_PortTransmit_IDLE);
+}  /* stp_tx_rstp_next */
+
+static void stp_tx_idle_init(struct ksz_stp_port *p)
+{
+#ifdef DBG_STP_STATE_TX_0
+	dbg_stp(p, __func__, false);
+#endif
+	helloWhen = HelloTime;
+}  /* stp_tx_idle_init */
+
+static void stp_tx_idle_next(struct ksz_stp_port *p,
+	struct ksz_stp_state *state)
+{
+	stp_change_state(state,
+		((0 == helloWhen) && canChange),
+		STP_PortTransmit_PERIODIC);
+	stp_change_state(state,
+		(sendRSTP && canChange && canSend),
+		STP_PortTransmit_RSTP);
+	stp_change_state(state,
+		((!sendRSTP && RootPort) && canChange && canSend),
+		STP_PortTransmit_TCN);
+	stp_change_state(state,
+		((!sendRSTP && DesignatedPort) && canChange && canSend),
+		STP_PortTransmit_CONFIG);
+}  /* stp_tx_idle_next */
+
+static int PortTransmit(struct ksz_stp_port *p)
+{
+	struct ksz_stp_state state_info;
+	u8 *state = &p->states[p->state_index];
+	int ret = 0;
+
+	state_info.index = STP_PortTransmit;
+	state_info.change = 1;
+	state_info.new_state = 0;
+	if (STP_BEGIN == *state) {
+		state_info.new_state = STP_PortTransmit_INIT;
+	}
+	do {
+
+		/* There is a new state. */
+		if (state_info.new_state) {
+			*state = state_info.new_state;
+			ret = state_info.change;
+			ret = 1;
+		}
+		switch (*state) {
+		case STP_PortTransmit_INIT:
+			if (stp_proc_state(p, &state_info,
+			    stp_tx_init_init, stp_tx_init_next))
+				goto done;
+			break;
+		case STP_PortTransmit_PERIODIC:
+			if (stp_proc_state(p, &state_info,
+			    stp_tx_periodic_init, stp_tx_periodic_next))
+				goto done;
+			break;
+		case STP_PortTransmit_CONFIG:
+			if (stp_proc_state(p, &state_info,
+			    stp_tx_config_init, stp_tx_config_next))
+				goto done;
+			break;
+		case STP_PortTransmit_TCN:
+			if (stp_proc_state(p, &state_info,
+			    stp_tx_tcn_init, stp_tx_tcn_next))
+				goto done;
+			break;
+		case STP_PortTransmit_RSTP:
+			if (stp_proc_state(p, &state_info,
+			    stp_tx_rstp_init, stp_tx_rstp_next))
+				goto done;
+			break;
+		case STP_PortTransmit_IDLE:
+			if (stp_proc_state(p, &state_info,
+			    stp_tx_idle_init, stp_tx_idle_next))
+				goto done;
+			break;
+		}
+	} while (state_info.new_state);
+
+done:
+	return ret;
+}  /* PortTransmit */
+
+static void stp_info_disable_init(struct ksz_stp_port *p)
+{
+#ifdef DBG_STP_STATE_INFO
+	dbg_stp(p, __func__, false);
+#endif
+	rcvdMsg = FALSE;
+	proposing = proposed = agree = agreed = FALSE;
+	rcvdInfoWhile = 0;
+
+	/* Change role in PortRoleSelection. */
+	infoIs = INFO_TYPE_DISABLED;
+	reselect = TRUE;
+	selected = FALSE;
+}  /* stp_info_disable_init */
+
+static void stp_info_disable_next(struct ksz_stp_port *p,
+	struct ksz_stp_state *state)
+{
+	stp_change_state(state,
+		(portEnabled),
+		STP_PortInfo_AGED);
+	stp_change_state(state,
+		(rcvdMsg),
+		STP_PortInfo_DISABLED);
+}  /* stp_info_disable_next */
+
+static void stp_info_age_init(struct ksz_stp_port *p)
+{
+#ifdef DBG_STP_STATE
+	dbg_stp(p, __func__, false);
+#endif
+	/* Change role in PortRoleSelection. */
+	infoIs = INFO_TYPE_AGED;
+	reselect = TRUE;
+	selected = FALSE;
+}  /* stp_info_age_init */
+
+static void stp_info_age_next(struct ksz_stp_port *p,
+	struct ksz_stp_state *state)
+{
+	stp_change_state(state,
+		(selected && updtInfo),
+		STP_PortInfo_UPDATE);
+}  /* stp_info_age_next */
+
+static void stp_info_update_init(struct ksz_stp_port *p)
+{
+#ifdef DBG_STP_STATE
+	dbg_stp(p, __func__, false);
+#endif
+	proposing = proposed = FALSE;
+	agreed = agreed && betterorsameInfo(INFO_TYPE_MINE);
+
+#if 1
+	/*
+	 * When switching back to Root Bridge because of timeout the port will
+	 * have inferior priority, resetting both agreed and synced.  The port
+	 * is still in forwarding state though.  When the port becomes
+	 * forwarding agreed is always set.  Now that agreed is reset synced
+	 * will not be set, making allSynced to always fail, which prevents
+	 * the Root Port from sending an Agreement, thus failing RSTP.op.5.3.
+	 */
+	if (forward)
+		agreed = sendRSTP;
+#endif
+	synced = synced && agreed;
+
+#if 0
+	/* agree is never turned off if priority is changed. */
+	agree = FALSE;
+#endif
+#ifdef DBG_STP_STATE_INFO
+dbg_msg("  %s %d %d\n", __func__, agreed, synced);
+#endif
+	COPY(portPriority, designatedPriority);
+	COPY(portTimes, designatedTimes);
+	updtInfo = FALSE;
+	infoIs = INFO_TYPE_MINE;
+	newInfo = TRUE;
+}  /* stp_info_update_init */
+
+static void stp_info_update_next(struct ksz_stp_port *p,
+	struct ksz_stp_state *state)
+{
+	stp_change_state(state,
+		TRUE, STP_PortInfo_CURRENT);
+}  /* stp_info_update_next */
+
+static void stp_info_superior_init(struct ksz_stp_port *p)
+{
+#ifdef DBG_STP_STATE
+	dbg_stp(p, __func__, false);
+#endif
+	agreed = proposing = FALSE;
+	recordProposal();
+	setTcFlags();
+	agree = agree && betterorsameInfo(INFO_TYPE_RECEIVED);
+	recordPriority();
+	recordTimes();
+
+	/* Keep from aged out. */
+	updtRcvdInfoWhile();
+
+	/* Change role in PortRoleSelection. */
+	infoIs = INFO_TYPE_RECEIVED;
+	reselect = TRUE;
+	selected = FALSE;
+
+	rcvdMsg = FALSE;
+}  /* stp_info_superior_init */
+
+static void stp_info_superior_next(struct ksz_stp_port *p,
+	struct ksz_stp_state *state)
+{
+	stp_change_state(state,
+		TRUE, STP_PortInfo_CURRENT);
+}  /* stp_info_superior_next */
+
+static void stp_info_repeat_init(struct ksz_stp_port *p)
+{
+#ifdef DBG_STP_STATE
+	dbg_stp(p, __func__, true);
+#endif
+	recordProposal();
+	setTcFlags();
+
+	/* Keep from aged out. */
+	updtRcvdInfoWhile();
+	rcvdMsg = FALSE;
+}  /* stp_info_repeat_init */
+
+static void stp_info_repeat_next(struct ksz_stp_port *p,
+	struct ksz_stp_state *state)
+{
+	stp_change_state(state,
+		TRUE, STP_PortInfo_CURRENT);
+}  /* stp_info_repeat_next */
+
+static void stp_info_inferior_init(struct ksz_stp_port *p)
+{
+#ifdef DBG_STP_STATE
+	dbg_stp(p, __func__, false);
+#endif
+	recordDispute();
+
+#if 1
+	/*
+	 * Priority is not recorded, so in updtRolesTree this port is still
+	 * selected as Root Port!
+	 * This can happen in test tool as the bridge id and port id is changed
+	 * from the port!
+	 */
+	if (p->br->hack_5_2 && INFO_TYPE_RECEIVED == infoIs &&
+	    memcmp(msgPriority.bridge_id.addr, portPriority.bridge_id.addr,
+	    ETH_ALEN)) {
+dbg_msg(" bridge id changed!\n");
+		recordPriority();
+		recordTimes();
+	}
+#endif
+
+	/* Will age out if keep receiving inferior messages. */
+	rcvdMsg = FALSE;
+}  /* stp_info_inferior_init */
+
+static void stp_info_inferior_next(struct ksz_stp_port *p,
+	struct ksz_stp_state *state)
+{
+	stp_change_state(state,
+		TRUE, STP_PortInfo_CURRENT);
+}  /* stp_info_inferior_next */
+
+static void stp_info_not_init(struct ksz_stp_port *p)
+{
+#ifdef DBG_STP_STATE_INFO
+	dbg_stp(p, __func__, false);
+#endif
+	recordAgreement();
+#if 1
+	/*
+	 * This port is not yet designated but receives an Agreement, most
+	 * likely when running the test tool.  Prepare the portPriority so that
+	 * betterorsameInfo can return true.
+	 */
+	if (agreed && role != ROLE_DESIGNATED && role != ROLE_BACKUP) {
+dbgPriority(&rootPriority, &rootPortId);
+		COPY(portPriority, rootPriority);
+		COPY(portPriority.port_id, portId);
+		COPY(portTimes, rootTimes);
+	}
+#endif
+	setTcFlags();
+	rcvdMsg = FALSE;
+
+	if (role != ROLE_BACKUP)
+		stp_chk_flush(p);
+}  /* stp_info_not_init */
+
+static void stp_info_not_next(struct ksz_stp_port *p,
+	struct ksz_stp_state *state)
+{
+	stp_change_state(state,
+		TRUE, STP_PortInfo_CURRENT);
+}  /* stp_info_not_next */
+
+static void stp_info_other_init(struct ksz_stp_port *p)
+{
+#ifdef DBG_STP_STATE
+	dbg_stp(p, __func__, false);
+#endif
+	rcvdMsg = FALSE;
+}  /* stp_info_other_init */
+
+static void stp_info_other_next(struct ksz_stp_port *p,
+	struct ksz_stp_state *state)
+{
+	stp_change_state(state,
+		TRUE, STP_PortInfo_CURRENT);
+}  /* stp_info_other_next */
+
+static void stp_info_cur_init(struct ksz_stp_port *p)
+{
+#ifdef DBG_STP_STATE_INFO
+	dbg_stp(p, __func__, true);
+#endif
+}  /* stp_info_cur_init */
+
+static void stp_info_cur_next(struct ksz_stp_port *p,
+	struct ksz_stp_state *state)
+{
+	stp_change_state(state,
+		(selected && updtInfo),
+		STP_PortInfo_UPDATE);
+	stp_change_state(state,
+		((infoIs == INFO_TYPE_RECEIVED) && (0 == rcvdInfoWhile) &&
+		!updtInfo && !rcvdMsg),
+		STP_PortInfo_AGED);
+	stp_change_state(state,
+		(rcvdMsg && !updtInfo),
+		STP_PortInfo_RECEIVE);
+}  /* stp_info_cur_next */
+
+static void stp_info_receive_init(struct ksz_stp_port *p)
+{
+	rcvdInfo = rcvInfo();
+#ifdef DBG_STP_RX
+if (p->dbg_rx)
+dbg_msg("  %s:%u=%d\n", __func__, p->port_index, rcvdInfo);
+#endif
+}  /* stp_info_receive_init */
+
+static void stp_info_receive_next(struct ksz_stp_port *p,
+	struct ksz_stp_state *state)
+{
+	stp_change_state(state,
+		(rcvdInfo == INFO_SUPERIOR_DESIGNATED),
+		STP_PortInfo_SUPERIOR_DESIGNATED);
+	stp_change_state(state,
+		(rcvdInfo == INFO_REPEATED_DESIGNATED),
+		STP_PortInfo_REPEATED_DESIGNATED);
+	stp_change_state(state,
+		(rcvdInfo == INFO_INFERIOR_DESIGNATED),
+		STP_PortInfo_INFERIOR_DESIGNATED);
+	stp_change_state(state,
+		(rcvdInfo == INFO_INFERIOR_ROOT_ALT),
+		STP_PortInfo_NOT_DESIGNATED);
+	stp_change_state(state,
+		(rcvdInfo == INFO_OTHER),
+		STP_PortInfo_OTHER);
+}  /* stp_info_receive_next */
+
+static void stp_info_next(struct ksz_stp_port *p,
+	struct ksz_stp_state *state)
+{
+	/* Notify port is disabled. */
+	stp_change_state(state,
+		(!portEnabled && (infoIs != INFO_TYPE_DISABLED)),
+		STP_PortInfo_DISABLED);
+}  /* stp_info_next */
+
+static int PortInformation(struct ksz_stp_port *p)
+{
+	struct ksz_stp_state state_info;
+	u8 *state = &p->states[p->state_index];
+	int ret = 0;
+
+	state_info.index = STP_PortInfo;
+	state_info.change = 1;
+	state_info.new_state = 0;
+	if (STP_BEGIN == *state) {
+		state_info.new_state = STP_PortInfo_DISABLED;
+	} else
+		stp_info_next(p, &state_info);
+	do {
+
+		/* There is a new state. */
+		if (state_info.new_state) {
+			*state = state_info.new_state;
+			ret = state_info.change;
+			ret = 1;
+		}
+		switch (*state) {
+		case STP_PortInfo_DISABLED:
+			if (stp_proc_state(p, &state_info,
+			    stp_info_disable_init, stp_info_disable_next))
+				goto done;
+			break;
+		case STP_PortInfo_AGED:
+			if (stp_proc_state(p, &state_info,
+			    stp_info_age_init, stp_info_age_next))
+				goto done;
+			break;
+		case STP_PortInfo_UPDATE:
+			if (stp_proc_state(p, &state_info,
+			    stp_info_update_init, stp_info_update_next))
+				goto done;
+			break;
+		case STP_PortInfo_SUPERIOR_DESIGNATED:
+			if (stp_proc_state(p, &state_info,
+			    stp_info_superior_init, stp_info_superior_next))
+				goto done;
+			break;
+		case STP_PortInfo_REPEATED_DESIGNATED:
+			if (stp_proc_state(p, &state_info,
+			    stp_info_repeat_init, stp_info_repeat_next))
+				goto done;
+			break;
+		case STP_PortInfo_INFERIOR_DESIGNATED:
+			if (stp_proc_state(p, &state_info,
+			    stp_info_inferior_init, stp_info_inferior_next))
+				goto done;
+			break;
+		case STP_PortInfo_NOT_DESIGNATED:
+			if (stp_proc_state(p, &state_info,
+			    stp_info_not_init, stp_info_not_next))
+				goto done;
+			break;
+		case STP_PortInfo_OTHER:
+			if (stp_proc_state(p, &state_info,
+			    stp_info_other_init, stp_info_other_next))
+				goto done;
+			break;
+		case STP_PortInfo_CURRENT:
+			if (stp_proc_state(p, &state_info,
+			    stp_info_cur_init, stp_info_cur_next))
+				goto done;
+			break;
+		case STP_PortInfo_RECEIVE:
+			if (stp_proc_state(p, &state_info,
+			    stp_info_receive_init, stp_info_receive_next))
+				goto done;
+			break;
+		}
+	} while (state_info.new_state);
+
+done:
+	return ret;
+}  /* PortInformation */
+
+static void stp_role_sel_br_init(struct ksz_stp_port *p)
+{
+	struct ksz_stp_bridge *br = p->br;
+
+#ifdef DBG_STP_STATE_ROLE_SEL
+dbg_msg("  %s\n", __func__);
+#endif
+	updtRoleDisabledTree();
+}  /* stp_role_sel_br_init */
+
+static void stp_role_sel_br_next(struct ksz_stp_port *p,
+	struct ksz_stp_state *state)
+{
+	stp_change_state(state,
+		TRUE, STP_PortRoleSel_ROLE_SELECTION);
+}  /* stp_role_sel_br_next */
+
+static void stp_role_sel_role_init(struct ksz_stp_port *p)
+{
+	struct ksz_stp_bridge *br = p->br;
+
+#ifdef DBG_STP_STATE_ROLE_SEL
+dbg_msg("  %s\n", __func__);
+#endif
+	clearReselectTree();
+	updtRolesTree();
+	setSelectedTree();
+}  /* stp_role_sel_role_init */
+
+static void stp_role_sel_role_next(struct ksz_stp_port *p,
+	struct ksz_stp_state *state)
+{
+	FOREACH_P_IN_T(
+		if (reselect) {
+			state->new_state = STP_PortRoleSel_ROLE_SELECTION;
+			break;
+		}
+	)
+}  /* stp_role_sel_role_next */
+
+static int PortRoleSelection(struct ksz_stp_bridge *br)
+{
+	struct ksz_stp_state state_info;
+	struct ksz_stp_port *p = &br->ports[0];
+	u8 *state = &br->states[br->state_index];
+	int ret = 0;
+
+	state_info.index = STP_PortRoleSel;
+	state_info.change = 1;
+	state_info.new_state = 0;
+	if (STP_BEGIN == *state) {
+		state_info.new_state = STP_PortRoleSel_INIT_BRIDGE;
+	}
+	do {
+
+		/* There is a new state. */
+		if (state_info.new_state) {
+			*state = state_info.new_state;
+			ret = state_info.change;
+			ret = 1;
+		}
+		switch (*state) {
+		case STP_PortRoleSel_INIT_BRIDGE:
+			if (stp_proc_state(p, &state_info,
+			    stp_role_sel_br_init, stp_role_sel_br_next))
+				goto done;
+			break;
+		case STP_PortRoleSel_ROLE_SELECTION:
+			if (stp_proc_state(p, &state_info,
+			    stp_role_sel_role_init, stp_role_sel_role_next))
+				goto done;
+			break;
+		}
+	} while (state_info.new_state);
+
+done:
+	return ret;
+}  /* PortRoleSelection */
+
+static void stp_role_tr_init_init(struct ksz_stp_port *p)
+{
+#ifdef DBG_STP_STATE_ROLE_TR
+	dbg_stp(p, __func__, false);
+#endif
+	role = ROLE_DISABLED;
+	learn = forward = FALSE;
+	synced = FALSE;
+	sync = reRoot = TRUE;
+	rrWhile = FwdDelay;
+	fdWhile = MaxAge;
+	rbWhile = 0;
+}  /* stp_role_tr_init_init */
+
+static void stp_role_tr_init_next(struct ksz_stp_port *p,
+	struct ksz_stp_state *state)
+{
+	int cond = canChange;
+
+#if 1
+	cond = TRUE;
+#endif
+	stp_change_state(state,
+		cond, STP_PortRoleTrans_DISABLE_PORT);
+}  /* stp_role_tr_init_next */
+
+static void stp_role_tr_dis_init(struct ksz_stp_port *p)
+{
+#ifdef DBG_STP_STATE
+	dbg_stp(p, __func__, false);
+#endif
+	role = ROLE_DISABLED;
+	learn = forward = FALSE;
+
+#if 1
+	/* Port is disabled.  Do not send. */
+#ifdef DBG_STP_STATE
+if (newInfo)
+dbg_msg("  %s clear newInfo\n", __func__);
+#endif
+	newInfo = FALSE;
+#endif
+}  /* stp_role_tr_dis_init */
+
+static void stp_role_tr_dis_next(struct ksz_stp_port *p,
+	struct ksz_stp_state *state)
+{
+	stp_change_state(state,
+		((!learning && !forwarding) && canChange),
+		STP_PortRoleTrans_DISABLED_PORT);
+}  /* stp_role_tr_dis_next */
+
+static void stp_role_tr_disd_init(struct ksz_stp_port *p)
+{
+#ifdef DBG_STP_STATE_ROLE_TR
+	dbg_stp(p, __func__, false);
+#endif
+	fdWhile = MaxAge;
+#ifdef DBG_STP_STATE
+if (!synced)
+dbg_msg(" %s:%u\n", __func__, p->port_index);
+#endif
+	synced = TRUE;
+	rrWhile = 0;
+	sync = reRoot = FALSE;
+}  /* stp_role_tr_disd_init */
+
+static void stp_role_tr_disd_next(struct ksz_stp_port *p,
+	struct ksz_stp_state *state)
+{
+	/* Reset if !updtInfo. */
+	stp_change_state(state,
+		((NEQ(fdWhile, MaxAge) || sync || reRoot || !synced) &&
+		canChange),
+		STP_PortRoleTrans_DISABLED_PORT);
+}  /* stp_role_tr_disd_next */
+
+static void stp_role_tr_reroot_init(struct ksz_stp_port *p)
+{
+#ifdef DBG_STP_STATE
+	dbg_stp(p, __func__, false);
+#endif
+	setReRootTree();
+}  /* stp_role_tr_reroot_init */
+
+static void stp_role_tr_reroot_next(struct ksz_stp_port *p,
+	struct ksz_stp_state *state)
+{
+	stp_change_state(state,
+		TRUE, STP_PortRoleTrans_ROOT_PORT);
+}  /* stp_role_tr_reroot_next */
+
+static void stp_role_tr_rerooted_init(struct ksz_stp_port *p)
+{
+#ifdef DBG_STP_STATE
+	dbg_stp(p, __func__, false);
+#endif
+	reRoot = FALSE;
+}  /* stp_role_tr_rerooted_init */
+
+static void stp_role_tr_rerooted_next(struct ksz_stp_port *p,
+	struct ksz_stp_state *state)
+{
+	stp_change_state(state,
+		TRUE, STP_PortRoleTrans_ROOT_PORT);
+}  /* stp_role_tr_rerooted_next */
+
+static void stp_role_tr_root_proposed_init(struct ksz_stp_port *p)
+{
+#ifdef DBG_STP_STATE
+	dbg_stp(p, __func__, false);
+#endif
+	setSyncTree();
+	proposed = FALSE;
+}  /* stp_role_tr_root_proposed_init */
+
+static void stp_role_tr_root_proposed_next(struct ksz_stp_port *p,
+	struct ksz_stp_state *state)
+{
+	stp_change_state(state,
+		TRUE, STP_PortRoleTrans_ROOT_PORT);
+}  /* stp_role_tr_root_proposed_next */
+
+static void stp_role_tr_root_agreed_init(struct ksz_stp_port *p)
+{
+#ifdef DBG_STP_STATE
+	dbg_stp(p, __func__, false);
+#endif
+	proposed = sync = FALSE;
+	agree = TRUE;
+	newInfo = TRUE;
+}  /* stp_role_tr_root_agreed_init */
+
+static void stp_role_tr_root_agreed_next(struct ksz_stp_port *p,
+	struct ksz_stp_state *state)
+{
+	stp_change_state(state,
+		TRUE, STP_PortRoleTrans_ROOT_PORT);
+}  /* stp_role_tr_root_agreed_next */
+
+static void stp_role_tr_root_s_init(struct ksz_stp_port *p)
+{
+#ifdef DBG_STP_STATE
+	dbg_stp(p, __func__, false);
+#endif
+	synced = TRUE;
+	sync = FALSE;
+}  /* stp_role_tr_root_s_init */
+
+static void stp_role_tr_root_s_next(struct ksz_stp_port *p,
+	struct ksz_stp_state *state)
+{
+	stp_change_state(state,
+		TRUE, STP_PortRoleTrans_ROOT_PORT);
+}  /* stp_role_tr_root_s_next */
+
+static void stp_role_tr_root_l_init(struct ksz_stp_port *p)
+{
+#ifdef DBG_STP_STATE
+	dbg_stp(p, __func__, false);
+#endif
+	fdWhile = forwardDelay();
+	learn = TRUE;
+}  /* stp_role_tr_root_l_init */
+
+static void stp_role_tr_root_l_next(struct ksz_stp_port *p,
+	struct ksz_stp_state *state)
+{
+	stp_change_state(state,
+		TRUE, STP_PortRoleTrans_ROOT_PORT);
+}  /* stp_role_tr_root_l_next */
+
+static void stp_role_tr_root_f_init(struct ksz_stp_port *p)
+{
+#ifdef DBG_STP_STATE
+	dbg_stp(p, __func__, false);
+#endif
+	fdWhile = 0;
+	forward = TRUE;
+}  /* stp_role_tr_root_f_init */
+
+static void stp_role_tr_root_f_next(struct ksz_stp_port *p,
+	struct ksz_stp_state *state)
+{
+	stp_change_state(state,
+		TRUE, STP_PortRoleTrans_ROOT_PORT);
+}  /* stp_role_tr_root_f_next */
+
+static void stp_role_tr_root_p_init(struct ksz_stp_port *p)
+{
+#ifdef DBG_STP_STATE
+	if (role != ROLE_ROOT)
+		dbg_stp(p, __func__, false);
+#endif
+	do {
+		struct ksz_stp_info *stp = p->br->parent;
+		struct ksz_sw *sw = stp->sw_dev;
+
+		if (DesignatedPort)
+			sw->ops->from_designated(sw, p->port_index, false);
+		else if (!RootPort)
+			sw->ops->from_backup(sw, p->port_index);
+	} while (0);
+	role = ROLE_ROOT;
+	rrWhile = FwdDelay;
+#ifdef CONFIG_KSZ_MSRP
+	if (mrp_10_1_8a_hack)
+		rrWhile = to_stp_timer(4);
+#endif
+}  /* stp_role_tr_root_p_init */
+
+static void stp_role_tr_root_p_next(struct ksz_stp_port *p,
+	struct ksz_stp_state *state)
+{
+	int delay = FwdDelay;
+
+#ifdef CONFIG_KSZ_MSRP
+	if (mrp_10_1_8a_hack)
+		delay = to_stp_timer(4);
+#endif
+
+	stp_change_state(state,
+		((proposed && !agree) && canChange),
+		STP_PortRoleTrans_ROOT_PROPOSED);
+	stp_change_state(state,
+		(((allSynced() && !agree) || (proposed && agree)) && canChange),
+		STP_PortRoleTrans_ROOT_AGREED);
+	stp_change_state(state,
+		(((agreed && !synced) || (sync && synced)) && canChange),
+		STP_PortRoleTrans_ROOT_SYNCED);
+	stp_change_state(state,
+		((!forward && !reRoot) && canChange),
+		STP_PortRoleTrans_REROOT);
+	stp_change_state(state,
+		((reRoot && forward) && canChange),
+		STP_PortRoleTrans_REROOTED);
+	stp_change_state(state,
+		((((0 == fdWhile) || ((reRooted() && (0 == rbWhile)) &&
+		rstpVersion)) && !learn) && canChange),
+		STP_PortRoleTrans_ROOT_LEARN);
+	stp_change_state(state,
+		((((0 == fdWhile) || ((reRooted() && (0 == rbWhile)) &&
+		rstpVersion)) && learn && !forward) && canChange),
+		STP_PortRoleTrans_ROOT_FORWARD);
+	stp_change_state(state,
+		(NEQ(rrWhile, delay) && canChange),
+		STP_PortRoleTrans_ROOT_PORT);
+}  /* stp_role_tr_root_p_next */
+
+static void stp_role_tr_desg_r_init(struct ksz_stp_port *p)
+{
+	int delay = EdgeDelay();
+
+	if (AutoEdge && p->br->hack_4_1)
+		delay = 0;
+
+#ifdef DBG_STP_STATE
+	dbg_stp(p, __func__, false);
+#endif
+	proposing = TRUE;
+	edgeDelayWhile = delay;
+	newInfo = TRUE;
+}  /* stp_role_tr_desg_r_init */
+
+static void stp_role_tr_desg_r_next(struct ksz_stp_port *p,
+	struct ksz_stp_state *state)
+{
+	stp_change_state(state,
+		TRUE, STP_PortRoleTrans_DESIGNATED_PORT);
+}  /* stp_role_tr_desg_r_next */
+
+static void stp_role_tr_desg_s_init(struct ksz_stp_port *p)
+{
+#ifdef DBG_STP_STATE
+	dbg_stp(p, __func__, false);
+#endif
+	rrWhile = 0;
+	synced = TRUE;
+	sync = FALSE;
+}  /* stp_role_tr_desg_s_init */
+
+static void stp_role_tr_desg_s_next(struct ksz_stp_port *p,
+	struct ksz_stp_state *state)
+{
+	stp_change_state(state,
+		TRUE, STP_PortRoleTrans_DESIGNATED_PORT);
+}  /* stp_role_tr_desg_s_next */
+
+static void stp_role_tr_desg_retired_init(struct ksz_stp_port *p)
+{
+#ifdef DBG_STP_STATE
+	dbg_stp(p, __func__, false);
+#endif
+	reRoot = FALSE;
+}  /* stp_role_tr_desg_retired_init */
+
+static void stp_role_tr_desg_retired_next(struct ksz_stp_port *p,
+	struct ksz_stp_state *state)
+{
+	stp_change_state(state,
+		TRUE, STP_PortRoleTrans_DESIGNATED_PORT);
+}  /* stp_role_tr_desg_retired_next */
+
+static void stp_role_tr_desg_discard_init(struct ksz_stp_port *p)
+{
+#ifdef DBG_STP_STATE
+	dbg_stp(p, __func__, false);
+#endif
+	learn = forward = disputed = FALSE;
+	fdWhile = forwardDelay();
+}  /* stp_role_tr_desg_discard_init */
+
+static void stp_role_tr_desg_discard_next(struct ksz_stp_port *p,
+	struct ksz_stp_state *state)
+{
+	stp_change_state(state,
+		TRUE, STP_PortRoleTrans_DESIGNATED_PORT);
+}  /* stp_role_tr_desg_discard_next */
+
+static void stp_role_tr_desg_l_init(struct ksz_stp_port *p)
+{
+#ifdef DBG_STP_STATE
+	dbg_stp(p, __func__, false);
+#endif
+	learn = TRUE;
+	fdWhile = forwardDelay();
+do {
+	struct ksz_stp_dbg_times *x = &p->dbg_times[0];
+
+if (x->block_jiffies)
+dbg_msg(" b: %ld %d %s\n", jiffies - x->block_jiffies, fdWhile, __func__);
+} while (0);
+}  /* stp_role_tr_desg_l_init */
+
+static void stp_role_tr_desg_l_next(struct ksz_stp_port *p,
+	struct ksz_stp_state *state)
+{
+	stp_change_state(state,
+		TRUE, STP_PortRoleTrans_DESIGNATED_PORT);
+}  /* stp_role_tr_desg_l_next */
+
+static void stp_role_tr_desg_f_init(struct ksz_stp_port *p)
+{
+#ifdef DBG_STP_STATE
+	dbg_stp(p, __func__, false);
+#endif
+	forward = TRUE;
+	fdWhile = 0;
+	agreed = sendRSTP;
+
+#if 1
+	/*
+	 * proposing will be reset when infoIs is changed and betterorsameInfo
+	 * is called, even though the forwarding state is not changed.  Why
+	 * not just reset here?
+	 */
+	if (proposing)
+		proposing = FALSE;
+#endif
+#ifdef DBG_STP_STATE_
+	do {
+		char buf[40];
+
+		sprintf(buf, " %s", __func__);
+	dbg_stp(p, buf, false);
+	} while (0);
+#endif
+}  /* stp_role_tr_desg_f_init */
+
+static void stp_role_tr_desg_f_next(struct ksz_stp_port *p,
+	struct ksz_stp_state *state)
+{
+	stp_change_state(state,
+		TRUE, STP_PortRoleTrans_DESIGNATED_PORT);
+}  /* stp_role_tr_desg_f_next */
+
+static void stp_role_tr_desg_p_init(struct ksz_stp_port *p)
+{
+#ifdef DBG_STP_STATE
+	if (role != ROLE_DESIGNATED)
+		dbg_stp(p, __func__, false);
+#endif
+do {
+	struct ksz_stp_dbg_times *x = &p->dbg_times[0];
+
+if (x->block_jiffies && role != ROLE_DESIGNATED)
+dbg_msg(" b: %ld %d %s\n", jiffies - x->block_jiffies, fdWhile, __func__);
+} while (0);
+	if (RootPort || AlternatePort) {
+		struct ksz_stp_info *stp = p->br->parent;
+		struct ksz_sw *sw = stp->sw_dev;
+
+		sw->ops->to_designated(sw, p->port_index);
+	}
+	if (BackupPort) {
+		struct ksz_stp_info *stp = p->br->parent;
+		struct ksz_sw *sw = stp->sw_dev;
+
+		sw->ops->from_backup(sw, p->port_index);
+	}
+	role = ROLE_DESIGNATED;
+}  /* stp_role_tr_desg_p_init */
+
+static void stp_role_tr_desg_p_next(struct ksz_stp_port *p,
+	struct ksz_stp_state *state)
+{
+	stp_change_state(state,
+		((!forward && !agreed && !proposing && !operEdge) &&
+		canChange),
+		STP_PortRoleTrans_DESIGNATED_PROPOSE);
+	stp_change_state(state,
+		(((!learning && !forwarding && !synced) ||
+		(agreed && !synced) || (operEdge && !synced) ||
+		(sync && synced)) && canChange),
+		STP_PortRoleTrans_DESIGNATED_SYNCED);
+	stp_change_state(state,
+		((reRoot && (0 == rrWhile)) && canChange),
+		STP_PortRoleTrans_DESIGNATED_RETIRED);
+	stp_change_state(state,
+		((((sync && !synced) || (reRoot && (rrWhile != 0)) || disputed)
+		&& !operEdge && (learn || forward)) && canChange),
+		STP_PortRoleTrans_DESIGNATED_DISCARD);
+	stp_change_state(state,
+		((((0 == fdWhile) || agreed || operEdge) &&
+		((0 == rrWhile) || !reRoot) && !sync && !learn) && canChange),
+		STP_PortRoleTrans_DESIGNATED_LEARN);
+	stp_change_state(state,
+		((((0 == fdWhile) || agreed || operEdge) &&
+		((0 == rrWhile) || !reRoot) && !sync && (learn && !forward)) &&
+		canChange),
+		STP_PortRoleTrans_DESIGNATED_FORWARD);
+}  /* stp_role_tr_desg_p_next */
+
+static void stp_role_tr_block_init(struct ksz_stp_port *p)
+{
+#ifdef DBG_STP_STATE
+	dbg_stp(p, __func__, false);
+#endif
+	do {
+		struct ksz_stp_info *stp = p->br->parent;
+		struct ksz_sw *sw = stp->sw_dev;
+
+		if (DesignatedPort && selectedRole == ROLE_ALTERNATE)
+			sw->ops->from_designated(sw, p->port_index, true);
+		else
+			sw->ops->to_backup(sw, p->port_index);
+	} while (0);
+	role = selectedRole;
+	learn = forward = FALSE;
+}  /* stp_role_tr_block_init */
+
+static void stp_role_tr_block_next(struct ksz_stp_port *p,
+	struct ksz_stp_state *state)
+{
+	stp_change_state(state,
+		((!learning && !forwarding) && canChange),
+		STP_PortRoleTrans_ALTERNATE_PORT);
+}  /* stp_role_tr_block_next */
+
+static void stp_role_tr_backup_init(struct ksz_stp_port *p)
+{
+#ifdef DBG_STP_STATE
+	dbg_stp(p, __func__, true);
+#endif
+	rbWhile = 2 * HelloTime;
+}  /* stp_role_tr_backup_init */
+
+static void stp_role_tr_backup_next(struct ksz_stp_port *p,
+	struct ksz_stp_state *state)
+{
+	stp_change_state(state,
+		TRUE, STP_PortRoleTrans_ALTERNATE_PORT);
+}  /* stp_role_tr_backup_next */
+
+static void stp_role_tr_alt_proposed_init(struct ksz_stp_port *p)
+{
+#ifdef DBG_STP_STATE
+	dbg_stp(p, __func__, false);
+#endif
+	setSyncTree();
+	proposed = FALSE;
+}  /* stp_role_tr_alt_proposed_init */
+
+static void stp_role_tr_alt_proposed_next(struct ksz_stp_port *p,
+	struct ksz_stp_state *state)
+{
+	stp_change_state(state,
+		TRUE, STP_PortRoleTrans_ALTERNATE_PORT);
+}  /* stp_role_tr_alt_proposed_next */
+
+static void stp_role_tr_alt_a_init(struct ksz_stp_port *p)
+{
+#ifdef DBG_STP_STATE
+	dbg_stp(p, __func__, false);
+#endif
+	proposed = FALSE;
+	agree = TRUE;
+	newInfo = TRUE;
+}  /* stp_role_tr_alt_a_init */
+
+static void stp_role_tr_alt_a_next(struct ksz_stp_port *p,
+	struct ksz_stp_state *state)
+{
+	stp_change_state(state,
+		TRUE, STP_PortRoleTrans_ALTERNATE_PORT);
+}  /* stp_role_tr_alt_a_next */
+
+static void stp_role_tr_alt_p_init(struct ksz_stp_port *p)
+{
+	int delay = forwardDelay();
+
+	if (p->br->hack_4_2)
+		delay = FwdDelay;
+
+#ifdef DBG_STP_STATE_ROLE_TR
+	dbg_stp(p, __func__, true);
+#endif
+	fdWhile = delay;
+	synced = TRUE;
+	rrWhile = 0;
+	sync = reRoot = FALSE;
+}  /* stp_role_tr_alt_p_init */
+
+static void stp_role_tr_alt_p_next(struct ksz_stp_port *p,
+	struct ksz_stp_state *state)
+{
+	int delay = forwardDelay();
+
+	if (p->br->hack_4_2)
+		delay = FwdDelay;
+
+	stp_change_state(state,
+		((proposed && !agree) && canChange),
+		STP_PortRoleTrans_ALTERNATE_PROPOSED);
+	stp_change_state(state,
+		(((allSynced() && !agree) || (proposed && agree)) && canChange),
+		STP_PortRoleTrans_ALTERNATE_AGREED);
+	state->change = 0;
+	stp_change_state(state,
+		((NEQ(rbWhile, 2 * HelloTime) && BackupPort) && canChange),
+		STP_PortRoleTrans_BACKUP_PORT);
+	stp_change_state(state,
+		((NEQ(fdWhile, delay) || sync || reRoot || !synced) &&
+		canChange),
+		STP_PortRoleTrans_ALTERNATE_PORT);
+	state->change = 1;
+}  /* stp_role_tr_alt_p_next */
+
+static void stp_role_tr_next(struct ksz_stp_port *p,
+	struct ksz_stp_state *state)
+{
+	stp_change_state(state,
+		(((selectedRole == ROLE_DISABLED) && (role != selectedRole)) &&
+		canChange),
+		STP_PortRoleTrans_DISABLE_PORT);
+	stp_change_state(state,
+		(((selectedRole == ROLE_ROOT) && (role != selectedRole)) &&
+		canChange),
+		STP_PortRoleTrans_ROOT_PORT);
+	stp_change_state(state,
+		(((selectedRole == ROLE_DESIGNATED) && (role != selectedRole))
+		&& canChange),
+		STP_PortRoleTrans_DESIGNATED_PORT);
+	stp_change_state(state,
+		((((selectedRole == ROLE_ALTERNATE) ||
+		(selectedRole == ROLE_BACKUP)) && (role != selectedRole)) &&
+		canChange),
+		STP_PortRoleTrans_BLOCK_PORT);
+}  /* stp_role_tr_next */
+
+static int PortRoleTransitions(struct ksz_stp_port *p)
+{
+	struct ksz_stp_state state_info;
+	u8 *state = &p->states[p->state_index];
+	int ret = 0;
+
+	state_info.index = STP_PortRoleTrans;
+	state_info.change = 1;
+	state_info.new_state = 0;
+	if (STP_BEGIN == *state) {
+		state_info.new_state = STP_PortRoleTrans_INIT_PORT;
+	} else
+		stp_role_tr_next(p, &state_info);
+	do {
+
+		/* There is a new state. */
+		if (state_info.new_state) {
+			*state = state_info.new_state;
+			ret = state_info.change;
+			ret = 1;
+		}
+		switch (*state) {
+		case STP_PortRoleTrans_INIT_PORT:
+			if (stp_proc_state(p, &state_info,
+			    stp_role_tr_init_init, stp_role_tr_init_next))
+				goto done;
+			break;
+		case STP_PortRoleTrans_DISABLE_PORT:
+			if (stp_proc_state(p, &state_info,
+			    stp_role_tr_dis_init, stp_role_tr_dis_next))
+				goto done;
+			break;
+		case STP_PortRoleTrans_DISABLED_PORT:
+			if (stp_proc_state(p, &state_info,
+			    stp_role_tr_disd_init, stp_role_tr_disd_next))
+				goto done;
+			break;
+		case STP_PortRoleTrans_ROOT_PROPOSED:
+			if (stp_proc_state(p, &state_info,
+			    stp_role_tr_root_proposed_init,
+			    stp_role_tr_root_proposed_next))
+				goto done;
+			break;
+		case STP_PortRoleTrans_ROOT_AGREED:
+			if (stp_proc_state(p, &state_info,
+			    stp_role_tr_root_agreed_init,
+			    stp_role_tr_root_agreed_next))
+				goto done;
+			break;
+		case STP_PortRoleTrans_ROOT_SYNCED:
+			if (stp_proc_state(p, &state_info,
+			    stp_role_tr_root_s_init, stp_role_tr_root_s_next))
+				goto done;
+			break;
+		case STP_PortRoleTrans_REROOT:
+			if (stp_proc_state(p, &state_info,
+			    stp_role_tr_reroot_init, stp_role_tr_reroot_next))
+				goto done;
+			break;
+		case STP_PortRoleTrans_REROOTED:
+			if (stp_proc_state(p, &state_info,
+			    stp_role_tr_rerooted_init,
+			    stp_role_tr_rerooted_next))
+				goto done;
+			break;
+		case STP_PortRoleTrans_ROOT_LEARN:
+			if (stp_proc_state(p, &state_info,
+			    stp_role_tr_root_l_init, stp_role_tr_root_l_next))
+				goto done;
+			break;
+		case STP_PortRoleTrans_ROOT_FORWARD:
+			if (stp_proc_state(p, &state_info,
+			    stp_role_tr_root_f_init, stp_role_tr_root_f_next))
+				goto done;
+			break;
+		case STP_PortRoleTrans_ROOT_PORT:
+			if (stp_proc_state(p, &state_info,
+			    stp_role_tr_root_p_init, stp_role_tr_root_p_next))
+				goto done;
+			break;
+		case STP_PortRoleTrans_DESIGNATED_PROPOSE:
+			if (stp_proc_state(p, &state_info,
+			    stp_role_tr_desg_r_init, stp_role_tr_desg_r_next))
+				goto done;
+			break;
+		case STP_PortRoleTrans_DESIGNATED_SYNCED:
+			if (stp_proc_state(p, &state_info,
+			    stp_role_tr_desg_s_init, stp_role_tr_desg_s_next))
+				goto done;
+			break;
+		case STP_PortRoleTrans_DESIGNATED_RETIRED:
+			if (stp_proc_state(p, &state_info,
+			    stp_role_tr_desg_retired_init,
+			    stp_role_tr_desg_retired_next))
+				goto done;
+			break;
+		case STP_PortRoleTrans_DESIGNATED_DISCARD:
+			if (stp_proc_state(p, &state_info,
+			    stp_role_tr_desg_discard_init,
+			    stp_role_tr_desg_discard_next))
+				goto done;
+			break;
+		case STP_PortRoleTrans_DESIGNATED_LEARN:
+			if (stp_proc_state(p, &state_info,
+			    stp_role_tr_desg_l_init, stp_role_tr_desg_l_next))
+				goto done;
+			break;
+		case STP_PortRoleTrans_DESIGNATED_FORWARD:
+			if (stp_proc_state(p, &state_info,
+			    stp_role_tr_desg_f_init, stp_role_tr_desg_f_next))
+				goto done;
+			break;
+		case STP_PortRoleTrans_DESIGNATED_PORT:
+			if (stp_proc_state(p, &state_info,
+			    stp_role_tr_desg_p_init, stp_role_tr_desg_p_next))
+				goto done;
+			break;
+		case STP_PortRoleTrans_BLOCK_PORT:
+			if (stp_proc_state(p, &state_info,
+			    stp_role_tr_block_init, stp_role_tr_block_next))
+				goto done;
+			break;
+		case STP_PortRoleTrans_ALTERNATE_PROPOSED:
+			if (stp_proc_state(p, &state_info,
+			    stp_role_tr_alt_proposed_init,
+			    stp_role_tr_alt_proposed_next))
+				goto done;
+			break;
+		case STP_PortRoleTrans_ALTERNATE_AGREED:
+			if (stp_proc_state(p, &state_info,
+			    stp_role_tr_alt_a_init, stp_role_tr_alt_a_next))
+				goto done;
+			break;
+		case STP_PortRoleTrans_BACKUP_PORT:
+			if (stp_proc_state(p, &state_info,
+			    stp_role_tr_backup_init, stp_role_tr_backup_next))
+				goto done;
+			break;
+		case STP_PortRoleTrans_ALTERNATE_PORT:
+			if (stp_proc_state(p, &state_info,
+			    stp_role_tr_alt_p_init, stp_role_tr_alt_p_next))
+				goto done;
+			break;
+		}
+	} while (state_info.new_state);
+
+done:
+	return ret;
+}  /* PortRoleTransitions */
+
+static void stp_discarding_init(struct ksz_stp_port *p)
+{
+#ifdef DBG_STP_STATE_TR
+	dbg_stp(p, __func__, false);
+#endif
+	disableLearning();
+	learning = FALSE;
+	disableForwarding();
+	forwarding = FALSE;
+}  /* stp_discarding_init */
+
+static void stp_discarding_next(struct ksz_stp_port *p,
+	struct ksz_stp_state *state)
+{
+	stp_change_state(state,
+		(learn),
+		STP_PortStateTrans_LEARNING);
+}  /* stp_discarding_next */
+
+static void stp_learning_init(struct ksz_stp_port *p)
+{
+#ifdef DBG_STP_STATE_TR
+	dbg_stp(p, __func__, false);
+#endif
+	enableLearning();
+	learning = TRUE;
+}  /* stp_learning_init */
+
+static void stp_learning_next(struct ksz_stp_port *p,
+	struct ksz_stp_state *state)
+{
+	stp_change_state(state,
+		(!learn),
+		STP_PortStateTrans_DISCARDING);
+	stp_change_state(state,
+		(forward),
+		STP_PortStateTrans_FORWARDING);
+}  /* stp_learning_next */
+
+static void stp_forwarding_init(struct ksz_stp_port *p)
+{
+#ifdef DBG_STP_STATE_TR
+	dbg_stp(p, __func__, false);
+#endif
+	enableForwarding();
+	forwarding = TRUE;
+}  /* stp_forwarding_init */
+
+static void stp_forwarding_next(struct ksz_stp_port *p,
+	struct ksz_stp_state *state)
+{
+	stp_change_state(state,
+		(!forward),
+		STP_PortStateTrans_DISCARDING);
+}  /* stp_forwarding_next */
+
+static int PortStateTransition(struct ksz_stp_port *p)
+{
+	struct ksz_stp_state state_info;
+	u8 *state = &p->states[p->state_index];
+	int ret = 0;
+
+	state_info.index = STP_PortStateTrans;
+	state_info.change = 1;
+	state_info.new_state = 0;
+	if (STP_BEGIN == *state) {
+		state_info.new_state = STP_PortStateTrans_DISCARDING;
+	}
+	do {
+
+		/* There is a new state. */
+		if (state_info.new_state) {
+			*state = state_info.new_state;
+			ret = state_info.change;
+			ret = 1;
+		}
+		switch (*state) {
+		case STP_PortStateTrans_DISCARDING:
+			if (stp_proc_state(p, &state_info,
+			    stp_discarding_init, stp_discarding_next))
+				goto done;
+			break;
+		case STP_PortStateTrans_LEARNING:
+			if (stp_proc_state(p, &state_info,
+			    stp_learning_init, stp_learning_next))
+				goto done;
+			break;
+		case STP_PortStateTrans_FORWARDING:
+			if (stp_proc_state(p, &state_info,
+			    stp_forwarding_init, stp_forwarding_next))
+				goto done;
+			break;
+		}
+	} while (state_info.new_state);
+
+done:
+	return ret;
+}  /* PortStateTransition */
+
+static void stp_top_inactive_init(struct ksz_stp_port *p)
+{
+#ifdef DBG_STP_STATE
+	dbg_stp(p, __func__, false);
+#endif
+	fdbFlush = TRUE;
+	doFlush();
+
+	/* Stop sending topology change. */
+	tcDetected = 0;
+	tcWhile = 0;
+	tcPropWhile = 0;
+	tcAck = FALSE;
+}  /* stp_top_inactive_init */
+
+static void stp_top_inactive_next(struct ksz_stp_port *p,
+	struct ksz_stp_state *state)
+{
+	stp_change_state(state,
+		(learn && !fdbFlush),
+		STP_TopologyChange_LEARNING);
+}  /* stp_top_inactive_next */
+
+static void stp_top_learn_init(struct ksz_stp_port *p)
+{
+#ifdef DBG_STP_STATE_TOPOLOGY
+	dbg_stp(p, __func__, false);
+#endif
+	rcvdTc = rcvdTcn = rcvdTcAck = FALSE;
+	tcProp = FALSE;
+}  /* stp_top_learn_init */
+
+static void stp_top_learn_next(struct ksz_stp_port *p,
+	struct ksz_stp_state *state)
+{
+	stp_change_state(state,
+		(ForwardPort && forward && !operEdge),
+		STP_TopologyChange_DETECTED);
+	stp_change_state(state,
+		(!(ForwardPort) && !(learn || learning) &&
+		!(rcvdTc || rcvdTcn || rcvdTcAck || tcProp)),
+		STP_TopologyChange_INACTIVE);
+
+	/*
+	 * LEARNING will be called after DETECTED because the other port is
+	 * in forwarding while this port is going forwarding.
+	 */
+	stp_change_state(state,
+		(rcvdTc || rcvdTcn || rcvdTcAck || tcProp),
+		STP_TopologyChange_LEARNING);
+}  /* stp_top_learn_next */
+
+static void stp_top_det_init(struct ksz_stp_port *p)
+{
+#ifdef DBG_STP_STATE_TOPOLOGY
+	dbg_stp(p, __func__, false);
+#endif
+	newTcWhile();
+	setTcPropTree();
+	newTcDetected();
+	newInfo = TRUE;
+}  /* stp_top_det_init */
+
+static void stp_top_det_next(struct ksz_stp_port *p,
+	struct ksz_stp_state *state)
+{
+	stp_change_state(state,
+		TRUE, STP_TopologyChange_ACTIVE);
+}  /* stp_top_det_next */
+
+static void stp_top_ack_init(struct ksz_stp_port *p)
+{
+#ifdef DBG_STP_STATE
+	dbg_stp(p, __func__, false);
+#endif
+	/* Stop sending topology change. */
+	tcWhile = 0;
+	tcPropWhile = 0;
+	rcvdTcAck = FALSE;
+}  /* stp_top_ack_init */
+
+static void stp_top_ack_next(struct ksz_stp_port *p,
+	struct ksz_stp_state *state)
+{
+	stp_change_state(state,
+		TRUE, STP_TopologyChange_ACTIVE);
+}  /* stp_top_ack_next */
+
+static void stp_top_prop_init(struct ksz_stp_port *p)
+{
+#ifdef DBG_STP_STATE
+	dbg_stp(p, __func__, false);
+#endif
+	newTcWhile();
+	fdbFlush = TRUE;
+	doFlush();
+	tcProp = FALSE;
+}  /* stp_top_prop_init */
+
+static void stp_top_prop_next(struct ksz_stp_port *p,
+	struct ksz_stp_state *state)
+{
+	stp_change_state(state,
+		TRUE, STP_TopologyChange_ACTIVE);
+}  /* stp_top_prop_next */
+
+static void stp_top_tc_init(struct ksz_stp_port *p)
+{
+#ifdef DBG_STP_STATE_TOPOLOGY
+	dbg_stp(p, __func__, false);
+#endif
+	rcvdTc = rcvdTcn = FALSE;
+	if (role == ROLE_DESIGNATED)
+		tcAck = TRUE;
+	setTcPropTree();
+}  /* stp_top_tc_init */
+
+static void stp_top_tc_next(struct ksz_stp_port *p,
+	struct ksz_stp_state *state)
+{
+	stp_change_state(state,
+		TRUE, STP_TopologyChange_ACTIVE);
+}  /* stp_top_tc_next */
+
+static void stp_top_tcn_init(struct ksz_stp_port *p)
+{
+#ifdef DBG_STP_STATE
+	dbg_stp(p, __func__, false);
+#endif
+	newTcWhile();
+}  /* stp_top_tcn_init */
+
+static void stp_top_tcn_next(struct ksz_stp_port *p,
+	struct ksz_stp_state *state)
+{
+	stp_change_state(state,
+		TRUE, STP_TopologyChange_NOTIFIED_TC);
+}  /* stp_top_tcn_next */
+
+static void stp_top_active_init(struct ksz_stp_port *p)
+{
+#ifdef DBG_STP_STATE_TOPOLOGY
+	dbg_stp(p, __func__, false);
+#endif
+}  /* stp_top_active_init */
+
+static void stp_top_active_next(struct ksz_stp_port *p,
+	struct ksz_stp_state *state)
+{
+	/*
+	 * rcvd* variables are reset in LEARNING, so it does have higher
+	 * precedence.
+	 */
+	stp_change_state(state,
+		(!(ForwardPort) || operEdge),
+		STP_TopologyChange_LEARNING);
+	stp_change_state(state,
+		(rcvdTcAck),
+		STP_TopologyChange_ACKNOWLEDGED);
+	stp_change_state(state,
+		(tcProp && !operEdge),
+		STP_TopologyChange_PROPAGATING);
+	stp_change_state(state,
+		(rcvdTc),
+		STP_TopologyChange_NOTIFIED_TC);
+	stp_change_state(state,
+		(rcvdTcn),
+		STP_TopologyChange_NOTIFIED_TCN);
+}  /* stp_top_active_next */
+
+static int TopologyChange(struct ksz_stp_port *p)
+{
+	struct ksz_stp_state state_info;
+	u8 *state = &p->states[p->state_index];
+	int ret = 0;
+
+	state_info.index = STP_TopologyChange;
+	state_info.change = 1;
+	state_info.new_state = 0;
+	if (STP_BEGIN == *state) {
+		state_info.new_state = STP_TopologyChange_INACTIVE;
+	}
+	do {
+
+		/* There is a new state. */
+		if (state_info.new_state) {
+			*state = state_info.new_state;
+			ret = state_info.change;
+			ret = 1;
+		}
+		switch (*state) {
+		case STP_TopologyChange_INACTIVE:
+			if (stp_proc_state(p, &state_info,
+			    stp_top_inactive_init, stp_top_inactive_next))
+				goto done;
+			break;
+		case STP_TopologyChange_LEARNING:
+			if (stp_proc_state(p, &state_info,
+			    stp_top_learn_init, stp_top_learn_next))
+				goto done;
+			break;
+		case STP_TopologyChange_DETECTED:
+			if (stp_proc_state(p, &state_info,
+			    stp_top_det_init, stp_top_det_next))
+				goto done;
+			break;
+		case STP_TopologyChange_ACKNOWLEDGED:
+			if (stp_proc_state(p, &state_info,
+			    stp_top_ack_init, stp_top_ack_next))
+				goto done;
+			break;
+		case STP_TopologyChange_PROPAGATING:
+			if (stp_proc_state(p, &state_info,
+			    stp_top_prop_init, stp_top_prop_next))
+				goto done;
+			break;
+		case STP_TopologyChange_NOTIFIED_TC:
+			if (stp_proc_state(p, &state_info,
+			    stp_top_tc_init, stp_top_tc_next))
+				goto done;
+			break;
+		case STP_TopologyChange_NOTIFIED_TCN:
+			if (stp_proc_state(p, &state_info,
+			    stp_top_tcn_init, stp_top_tcn_next))
+				goto done;
+			break;
+		case STP_TopologyChange_ACTIVE:
+			if (stp_proc_state(p, &state_info,
+			    stp_top_active_init, stp_top_active_next))
+				goto done;
+			break;
+		}
+	} while (state_info.new_state);
+
+done:
+	return ret;
+}  /* TopologyChange */
+
+
+static
+int (*bridge_stms[NUM_OF_BRIDGE_STATE_MACHINES])(struct ksz_stp_bridge *) = {
+	PortRoleSelection,
+};
+
+/* The bridge state machine needs to be run between the port state machines. */
+#define PORT_STATE_MACHINES_BREAK	5
+#define PORT_STATE_MACHINES_TX		8
+
+static
+int (*port_stms[NUM_OF_PORT_STATE_MACHINES])(struct ksz_stp_port *) = {
+	PortTimers,
+	PortReceive,
+	PortProtocolMigration,
+	BridgeDetection,
+	PortInformation,
+
+	PortRoleTransitions,
+	PortStateTransition,
+	TopologyChange,
+	PortTransmit,
+};
+
+
+static void stp_state_machines(struct ksz_stp_bridge *br)
+{
+	int changed;
+	int update;
+	int i;
+	uint p;
+	struct ksz_stp_port *port;
+
+	do {
+		changed = update = 0;
+		for (p = 0; p < br->port_cnt; p++) {
+			port = &br->ports[p];
+			if (port->off)
+				continue;
+			for (i = 0; i < PORT_STATE_MACHINES_BREAK; i++) {
+				port->state_index = i;
+				changed = port_stms[i](port);
+				update |= (changed << (1 + i));
+
+#ifdef DEBUG_MSG
+	dbg_print_work(&db.dbg_print);
+#endif
+			}
+		}
+		for (i = 0; i < NUM_OF_BRIDGE_STATE_MACHINES; i++) {
+			br->state_index = i;
+			changed = bridge_stms[i](br);
+			update |= (changed << 0);
+		}
+		for (p = 0; p < br->port_cnt; p++) {
+			port = &br->ports[p];
+			if (port->off)
+				continue;
+			for (i = PORT_STATE_MACHINES_BREAK;
+			     i < PORT_STATE_MACHINES_TX; i++) {
+				port->state_index = i;
+				changed = port_stms[i](port);
+				update |= (changed << (1 + i));
+
+#ifdef DEBUG_MSG
+	dbg_print_work(&db.dbg_print);
+#endif
+			}
+		}
+
+		/*
+		 * Cannot send if all received BPDU are not processed in case
+		 * they are looped back to the bridge.
+		 */
+		if (br->skip_tx)
+			break;
+		for (p = 0; p < br->port_cnt; p++) {
+			port = &br->ports[p];
+			if (port->off)
+				continue;
+			for (i = PORT_STATE_MACHINES_TX;
+			     i < NUM_OF_PORT_STATE_MACHINES; i++) {
+				port->state_index = i;
+				changed = port_stms[i](port);
+				update |= (changed << (1 + i));
+			}
+		}
+#ifdef DEBUG_MSG
+	dbg_print_work(&db.dbg_print);
+#endif
+	} while (update);
+}  /* stp_state_machines */
+
+static void proc_state_machines(struct work_struct *work)
+{
+	struct ksz_stp_info *stp =
+		container_of(work, struct ksz_stp_info, state_machine);
+
+	stp->machine_running = true;
+	mutex_lock(&stp->br.lock);
+	stp_state_machines(&stp->br);
+	mutex_unlock(&stp->br.lock);
+	stp->machine_running = false;
+	if (stp->br.skip_tx)
+		schedule_work(&stp->rx_proc);
+}  /* proc_state_machines */
+
+static void invoke_state_machines(struct ksz_stp_bridge *br)
+{
+	struct ksz_stp_info *stp = br->parent;
+
+	if (br->bridgeEnabled && stp->timer_tick)
+		schedule_work(&stp->state_machine);
+}  /* invoke_state_machines */
+
+static void stp_br_init(struct ksz_stp_port *p)
+{
+	BridgeIdentifier.prio = htons(0x8000);
+	BridgePriority.prio.root.prio =
+	BridgePriority.prio.bridge_id.prio =
+		BridgeIdentifier.prio;
+
+	/* MigrateTime is only used internally. */
+	MigrateTime = to_stp_timer(3);
+	BridgeHelloTime = 2;
+	BridgeMaxAge = 20;
+	BridgeFwdDelay = 15;
+	TxHoldCount = 6;
+	ForceProtocolVersion = 2;
+
+	p->br->hack_4_1 = 0;
+	p->br->hack_4_2 = 0;
+	p->br->hack_5_2 = 0;
+}  /* stp_br_init */
+
+static void stp_port_init(struct ksz_stp_port *p)
+{
+	portId.prio = 0x80;
+	adminPointToPointMAC = ADMIN_P2P_AUTO;
+	AdminPortPathCost = 0;
+	AdminEdge = FALSE;
+	AutoEdge = FALSE;
+#if 1
+	AutoEdge = TRUE;
+#endif
+}  /* stp_port_init */
+
+static void stp_state_init(struct ksz_stp_bridge *br)
+{
+	int i;
+	uint port;
+	struct ksz_stp_port *p;
+	struct ksz_stp_info *stp = br->parent;
+
+	stp->timer_tick = 1000;
+	for (i = 0; i < NUM_OF_BRIDGE_STATE_MACHINES; i++) {
+		br->state_index = i;
+		br->states[br->state_index] = STP_BEGIN;
+	}
+	for (port = 0; port < br->port_cnt; port++) {
+		p = &br->ports[port];
+		for (i = 0; i < NUM_OF_PORT_STATE_MACHINES; i++) {
+			p->state_index = i;
+			p->states[p->state_index] = STP_BEGIN;
+		}
+
+		memset(p->vars.timers, 0, sizeof(p->vars.timers));
+		selected = FALSE;
+	}
+	invoke_state_machines(br);
+}  /* stp_state_init */
+
+static int stp_cfg_port(struct ksz_stp_port *p, int link, int speed,
+	int duplex)
+{
+	int change = 0;
+
+	if (link) {
+		if (!portEnabled) {
+			struct ksz_stp_dbg_times *x = &p->dbg_times[0];
+
+			portEnabled = TRUE;
+if (x->block_jiffies) {
+dbg_msg("%s %d %lu\n", __func__, p->port_index, jiffies - x->block_jiffies);
+x->block_jiffies = jiffies;
+}
+			change = 1;
+		}
+		if (p->speed != speed) {
+			p->speed = speed;
+			if (checkPathCost(p))
+				change = 1;
+		}
+		if (p->duplex != duplex) {
+			p->duplex = duplex;
+			if (checkP2P(p))
+				change = 1;
+		}
+	} else {
+		if (portEnabled) {
+			portEnabled = FALSE;
+			change = 1;
+		}
+	}
+	return change;
+}  /* stp_cfg_port */
+
+static void stp_disable_port(struct ksz_stp_info *stp, uint port)
+{
+	struct ksz_stp_bridge *br = &stp->br;
+	struct ksz_stp_port *p = &br->ports[port];
+
+	portEnabled = FALSE;
+	p->link = 0;
+}  /* stp_disable_port */
+
+static void stp_enable_port(struct ksz_stp_info *stp, uint port, u8 *state)
+{
+	struct ksz_stp_bridge *br = &stp->br;
+	struct ksz_stp_port *p = &br->ports[port];
+
+	if (br->bridgeEnabled && !p->off)
+		*state = STP_STATE_DISABLED;
+}  /* stp_enable_port */
+
+#if 0
+static u8 wrong_root[] = {
+	0x10, 0x00, 0x11, 0x22, 0x33, 0x44, 0x55, 0x66
+};
+#endif
+
+static void stp_proc_rx(struct ksz_stp_port *p, struct bpdu *bpdu, u16 len)
+{
+	u16 size;
+#ifdef DBG_STP_RX
+	int cmp;
+	int bpdu_len = len;
+
+	if (bpdu_len > sizeof(struct bpdu))
+		bpdu_len = sizeof(struct bpdu);
+	cmp = memcmp(&p->rx_bpdu0, bpdu, bpdu_len);
+	if (cmp || ((bpdu->flags & (PORT_ROLE_DESIGNATED << PORT_ROLE_S)) !=
+	    (PORT_ROLE_DESIGNATED << PORT_ROLE_S) &&
+	    BPDU_TYPE_CONFIG_RSTP == bpdu->type)) {
+		if (p->dbg_rx < 2)
+			p->dbg_rx = 2;
+	}
+	if (p->dbg_rx)
+		p->dbg_rx--;
+	if (p->dbg_rx) {
+dbg_msg("[R] %d: ", p->port_index);
+		disp_bpdu(bpdu);
+	}
+	memcpy(&p->rx_bpdu0, bpdu, bpdu_len);
+#endif
+
+	/* Reject bad BPDU. */
+	if (bpdu->protocol != 0)
+		return;
+	size = sizeof(struct bpdu);
+	if (BPDU_TYPE_CONFIG == bpdu->type)
+		size--;
+	else if (BPDU_TYPE_TCN == bpdu->type)
+		size = 7;
+	else if (BPDU_TYPE_CONFIG_RSTP != bpdu->type)
+		size = 10000;
+	if (len < size)
+		return;
+	bpduVersion = bpdu->version <= 2 ? bpdu->version : 2;
+	bpduType = bpdu->type;
+	bpduFlags = 0;
+	bpduRole = (bpdu->flags >> PORT_ROLE_S) & PORT_ROLE_DESIGNATED;
+	if (BPDU_TYPE_CONFIG_RSTP == bpdu->type)
+		bpduFlags = bpdu->flags;
+	else if (BPDU_TYPE_TCN != bpdu->type)
+		bpduFlags = bpdu->flags &
+			(TOPOLOGY_CHANGE_ACK | TOPOLOGY_CHANGE);
+	if (BPDU_TYPE_TCN != bpdu->type) {
+#if 0
+if (!memcmp(&bpdu->root, wrong_root, 8)) {
+dbg_msg(" wrong root\n");
+bpdu->root.prio = 0;
+}
+#endif
+		COPY(bpduPriority, bpdu->root);
+		bpduTimes.message_age = get_bpdu_time(bpdu->message_age);
+		bpduTimes.max_age = get_bpdu_time(bpdu->max_age);
+		bpduTimes.hello_time = get_bpdu_time(bpdu->hello_time);
+		bpduTimes.forward_delay = get_bpdu_time(bpdu->forward_delay);
+		if ((BPDU_TYPE_CONFIG == bpdu->type &&
+		    bpduTimes.message_age >= bpduTimes.max_age) ||
+		    bpduTimes.message_age > bpduTimes.max_age)
+			return;
+	} else {
+		ZERO(bpduPriority);
+		ZERO(bpduTimes);
+	}
+	rcvdBPDU = TRUE;
+}  /* stp_proc_rx */
+
+static void stp_proc_tick(struct ksz_stp_bridge *br)
+{
+	uint i;
+	struct ksz_stp_port *p;
+
+	for (i = 0; i < br->port_cnt; i++) {
+		p = &br->ports[i];
+		tick = TRUE;
+if (p->dbg_rx)
+	p->dbg_rx--;
+	}
+	invoke_state_machines(br);
+}  /* stp_proc_tick */
+
+static void proc_rx(struct work_struct *work)
+{
+	struct ksz_stp_info *stp =
+		container_of(work, struct ksz_stp_info, rx_proc);
+	struct ksz_stp_bridge *br = &stp->br;
+	struct ksz_stp_port *p;
+	uint i;
+	bool not_empty;
+	bool last;
+	struct sk_buff *skb;
+
+	if (mutex_is_locked(&stp->br.lock)) {
+		schedule_work(&stp->rx_proc);
+		return;
+	}
+	not_empty = false;
+	for (i = 0; i < br->port_cnt; i++) {
+		p = &br->ports[i];
+		if (rcvdBPDU)
+			continue;
+		last = skb_queue_empty(&p->rxq);
+		not_empty |= !last;
+		if (!last) {
+			skb = skb_dequeue(&p->rxq);
+			last = skb_queue_empty(&p->rxq);
+			if (skb) {
+				struct bpdu *bpdu;
+				u16 len = 0;
+
+				bpdu = chk_bpdu(skb->data, &len);
+				if (bpdu)
+					stp_proc_rx(p, bpdu, len);
+				dev_kfree_skb_irq(skb);
+			}
+		} else
+			br->port_rx &= ~(1 << p->port_index);
+	}
+	if (not_empty || br->port_rx) {
+		br->skip_tx = true;
+		invoke_state_machines(br);
+	} else if (!br->port_rx && br->skip_tx) {
+		br->skip_tx = false;
+		invoke_state_machines(br);
+	}
+}  /* proc_rx */
+
+static int stp_rcv(struct ksz_stp_info *stp, struct sk_buff *skb, uint port)
+{
+	struct bpdu *bpdu;
+	u16 len = 0;
+
+	bpdu = chk_bpdu(skb->data, &len);
+	if (bpdu) {
+		struct ksz_stp_bridge *br = &stp->br;
+		struct ksz_stp_port *p = &br->ports[port];
+
+		if (stp->machine_running || rcvdBPDU || br->port_rx) {
+			skb_queue_tail(&p->rxq, skb);
+			br->port_rx |= (1 << port);
+			schedule_work(&stp->rx_proc);
+		} else {
+			stp_proc_rx(p, bpdu, len);
+			dev_kfree_skb_irq(skb);
+
+			invoke_state_machines(br);
+		}
+		return 0;
+	}
+	return 1;
+}  /* stp_rcv */
+
+static void port_timer_monitor(struct timer_list *t)
+{
+	struct ksz_timer_info *info = from_timer(info, t, timer);
+	struct ksz_stp_info *stp = info->dev;
+
+	stp_proc_tick(&stp->br);
+
+	ksz_update_timer(&stp->port_timer_info);
+}  /* port_timer_monitor */
+
+static void reselectAll(struct ksz_stp_bridge *br)
+{
+	struct ksz_stp_port *p = &br->ports[0];
+
+	FOREACH_P_IN_T(
+		reselect = TRUE;
+		selected = FALSE;
+		p->rx_bpdu0.protocol = 0xff;
+		p->tx_bpdu0.protocol = 0xff;
+	)
+}
+
+static const u8 *stp_br_id(struct ksz_stp_info *stp)
+{
+	return (u8 *) &stp->br.vars.br_id_;
+}
+
+static int stp_change_addr(struct ksz_stp_info *stp, u8 *addr)
+{
+	int diff;
+	struct ksz_stp_bridge *br = &stp->br;
+	struct ksz_stp_port *p = &br->ports[0];
+
+	mutex_lock(&br->lock);
+	diff = memcmp(BridgeIdentifier.addr, addr, ETH_ALEN);
+	memcpy(BridgeIdentifier.addr, addr, ETH_ALEN);
+
+	COPY(BridgePriority.prio.root, BridgeIdentifier);
+	COPY(BridgePriority.prio.bridge_id, BridgeIdentifier);
+	mutex_unlock(&br->lock);
+
+	memcpy(&stp->tx_frame[6], addr, ETH_ALEN);
+	if (diff) {
+		reselectAll(br);
+		invoke_state_machines(br);
+	}
+	return diff;
+}  /* stp_change_addr */
+
+static void stp_set_addr(struct ksz_stp_info *stp, u8 *addr)
+{
+	struct ksz_stp_bridge *br = &stp->br;
+	struct ksz_stp_port *p = &br->ports[0];
+
+	memcpy(BridgeIdentifier.addr, addr, ETH_ALEN);
+}  /* stp_set_addr */
+
+static void stp_link_change(struct ksz_stp_info *stp, int update)
+{
+	uint i;
+	int duplex;
+	int speed;
+	u8 state;
+	int change = 0;
+	struct ksz_port_info *info;
+	struct ksz_stp_port *p;
+	struct ksz_sw *sw = stp->sw_dev;
+	struct ksz_stp_bridge *br = &stp->br;
+
+	if (!br->bridgeEnabled)
+		return;
+	mutex_lock(&br->lock);
+	for (i = 0; i < br->port_cnt; i++) {
+		if (!(sw->dev_ports & (1 << i)))
+			continue;
+		p = &br->ports[i];
+		if (p->off)
+			continue;
+		info = get_port_info(sw, i);
+		if (p->link != (media_connected == info->state)) {
+			p->link = (media_connected == info->state);
+			if (p->link)
+				state = STP_STATE_BLOCKED;
+			else
+				state = STP_STATE_DISABLED;
+#ifdef CONFIG_KSZ_MRP
+			if (!p->link && (sw->features & MRP_SUPPORT)) {
+				struct mrp_info *mrp = &sw->mrp;
+
+				mrp_close_port(mrp, i);
+			}
+#endif
+			sw->ops->acquire(sw);
+			port_set_stp_state(sw, i, state);
+			sw->ops->release(sw);
+			duplex = (2 == info->duplex);
+			speed = info->tx_rate / TX_RATE_UNIT;
+			change |= stp_cfg_port(p, p->link, speed, duplex);
+		}
+	}
+	mutex_unlock(&br->lock);
+	if (change && update)
+		invoke_state_machines(br);
+}  /* stp_link_change */
+
+static int stp_get_tcDetected(struct ksz_stp_info *stp, int i)
+{
+	struct ksz_stp_port *p;
+	struct ksz_stp_bridge *br = &stp->br;
+
+	p = &br->ports[i];
+	return tcDetected != 0;
+}  /* stp_get_tcDetected */
+
+static void stp_start(struct ksz_stp_info *stp)
+{
+	uint i;
+	uint n;
+	struct ksz_stp_port *p;
+	struct ksz_sw *sw = stp->sw_dev;
+	struct ksz_stp_bridge *br = &stp->br;
+
+	stp->dev = sw->main_dev;
+	sw->ops->acquire(sw);
+	for (n = 1; n <= sw->mib_port_cnt; n++) {
+		i = get_phy_port(sw, n);
+		p = &br->ports[i];
+		if (p->off)
+			continue;
+		sw->info->port_cfg[i].vid_member = (1 << i);
+		port_set_stp_state(sw, i, STP_STATE_DISABLED);
+	}
+	sw->ops->release(sw);
+	stp->timer_tick = 1000;
+	ksz_start_timer(&stp->port_timer_info, stp->port_timer_info.period);
+	stp_link_change(stp, false);
+	if (!stp_change_addr(stp, sw->info->mac_addr))
+		stp_state_init(&stp->br);
+}  /* stp_start */
+
+static void stp_stop(struct ksz_stp_info *stp, int hw_access)
+{
+	uint i;
+	uint n;
+	struct ksz_stp_port *p;
+	struct ksz_sw *sw = stp->sw_dev;
+	struct ksz_stp_bridge *br = &stp->br;
+
+	if (hw_access) {
+		sw->ops->acquire(sw);
+		for (n = 1; n <= sw->mib_port_cnt; n++) {
+			i = get_phy_port(sw, n);
+			p = &br->ports[i];
+			if (p->off)
+				continue;
+			sw->info->port_cfg[i].vid_member = sw->PORT_MASK;
+			port_set_stp_state(sw, i, STP_STATE_FORWARDING);
+		}
+		sw->ops->release(sw);
+	}
+#ifdef CONFIG_KSZ_IBA_ONLY
+	sw->features &= ~(STP_SUPPORT);
+	stp->br.bridgeEnabled = FALSE;
+#endif
+	ksz_stop_timer(&stp->port_timer_info);
+	stp->timer_tick = 0;
+	flush_work(&stp->rx_proc);
+	flush_work(&stp->state_machine);
+
+	p = &br->ports[0];
+	stp_br_init(p);
+	for (i = 0; i < br->port_cnt; i++) {
+		p = &br->ports[i];
+		stp_port_init(p);
+	}
+}  /* stp_stop */
+
+static void stp_br_test_setup(struct ksz_stp_bridge *br)
+{
+	uint i;
+	struct ksz_stp_port *p;
+
+	p = &br->ports[0];
+	stp_br_init(p);
+	for (i = 0; i < br->port_cnt; i++) {
+		p = &br->ports[i];
+		stp_port_init(p);
+
+		/* Switch back to RSTP if port is in STP. */
+		mcheck = TRUE;
+		AdminPortPathCost = 200000;
+		checkPathCost(p);
+		checkP2P(p);
+		p->rx_bpdu0.protocol = 0xff;
+	}
+}  /* stp_br_test_setup */
+
+#define BR_ID_FMT  "%04x.%02x%02x%02x%02x%02x%02x"
+
+#define BR_ID_ARGS(x)  \
+	ntohs(x.prio), x.addr[0], x.addr[1], x.addr[2], \
+	x.addr[3], x.addr[4], x.addr[5]
+
+#define BOOL_STR(x)  ((x) ? "yes" : "no")
+
+static char *get_admin_p2p_str(int p2p)
+{
+	switch (p2p) {
+	case ADMIN_P2P_FORCE_FALSE:
+		return "no";
+	case ADMIN_P2P_FORCE_TRUE:
+		return "yes";
+	case ADMIN_P2P_AUTO:
+		return "auto";
+	}
+	return "unk";
+}
+
+static char *get_port_state_str(int state)
+{
+	switch (state) {
+	case STP_PortStateTrans_DISCARDING:
+		return "discarding";
+	case STP_PortStateTrans_LEARNING:
+		return "learning";
+	case STP_PortStateTrans_FORWARDING:
+		return "forwarding";
+	}
+	return "unknown";
+}
+
+static char *get_port_role_str(int _role)
+{
+	switch (_role) {
+	case ROLE_ROOT:
+		return "root";
+	case ROLE_DESIGNATED:
+		return "designated";
+	case ROLE_ALTERNATE:
+		return "alternate";
+	case ROLE_BACKUP:
+		return "backup";
+	case ROLE_DISABLED:
+		return "disabled";
+	}
+	return "unknown";
+}
+
+static ssize_t sysfs_stp_read(struct ksz_sw *sw, int proc_num, ssize_t len,
+	char *buf)
+{
+	struct ksz_stp_info *stp = &sw->info->rstp;
+	struct ksz_stp_port *p;
+	int chk = 0;
+	int type = SHOW_HELP_NUM;
+	char note[40];
+
+	note[0] = '\0';
+	if (!(sw->features & STP_SUPPORT))
+		return 0;
+	mutex_lock(&stp->br.lock);
+	p = &stp->br.ports[0];
+	switch (proc_num) {
+	case PROC_GET_STP_BR_INFO:
+		len += sprintf(buf + len,
+			" enabled\t\t%4s\n",
+			BOOL_STR(stp->br.bridgeEnabled));
+		len += sprintf(buf + len,
+			" bridge id\t\t" BR_ID_FMT "\n",
+			BR_ID_ARGS(BridgeIdentifier));
+		len += sprintf(buf + len,
+			" designated root\t" BR_ID_FMT "\n",
+			BR_ID_ARGS(rootPriority.root));
+		len += sprintf(buf + len,
+			" root port\t\t%4u",
+			rootPortId.num);
+		len += sprintf(buf + len,
+			"\t\t\tpath cost\t%12u\n",
+			ntohl(rootPriority.root_path_cost));
+		len += sprintf(buf + len,
+			" max age\t\t%4u",
+			rootTimes.max_age);
+		len += sprintf(buf + len,
+			"\t\t\tbridge max age\t\t%4u\n",
+			BridgeMaxAge);
+		len += sprintf(buf + len,
+			" hello time\t\t%4u",
+			rootTimes.hello_time);
+		len += sprintf(buf + len,
+			"\t\t\tbridge hello time\t%4u\n",
+			BridgeHelloTime);
+		len += sprintf(buf + len,
+			" forward delay\t\t%4u",
+			rootTimes.forward_delay);
+		len += sprintf(buf + len,
+			"\t\t\tbridge forward delay\t%4u\n",
+			BridgeFwdDelay);
+		len += sprintf(buf + len,
+			" tx hold count\t\t%4u",
+			TxHoldCount);
+		len += sprintf(buf + len,
+			"\t\t\tprotocol version\t%4u\n",
+			ForceProtocolVersion);
+		len += sprintf(buf + len,
+			" time since topology change\t%9u\n",
+			timeSinceTC);
+		len += sprintf(buf + len,
+			" topology change count\t\t%9u\n",
+			cntTC);
+		len += sprintf(buf + len,
+			" topology change\t\t%9s\n",
+			BOOL_STR(isTC));
+		type = SHOW_HELP_NONE;
+		break;
+	case PROC_SET_STP_BR_ON:
+		chk = p->br->bridgeEnabled;
+		type = SHOW_HELP_ON_OFF;
+#if defined(DBG_STP_STATE) || defined(DBG)
+		d_stp_states(p->br);
+#endif
+		break;
+	case PROC_SET_STP_BR_PRIO:
+		chk = ntohs(BridgeIdentifier.prio);
+		type = SHOW_HELP_HEX;
+		break;
+	case PROC_SET_STP_BR_FWD_DELAY:
+		chk = BridgeFwdDelay;
+		break;
+	case PROC_SET_STP_BR_MAX_AGE:
+		chk = BridgeMaxAge;
+		break;
+	case PROC_SET_STP_BR_HELLO_TIME:
+		chk = BridgeHelloTime;
+		break;
+	case PROC_SET_STP_BR_TX_HOLD:
+		chk = TxHoldCount;
+		break;
+	case PROC_SET_STP_VERSION:
+		chk = ForceProtocolVersion;
+		if (sw->verbose) {
+			switch (chk) {
+			case 2:
+				strcpy(note, " (rstp)");
+				break;
+			case 0:
+				strcpy(note, " (stp)");
+				break;
+			default:
+				strcpy(note, " (unknown)");
+			}
+		}
+		break;
+	default:
+		type = SHOW_HELP_NONE;
+	}
+	mutex_unlock(&stp->br.lock);
+	return sysfs_show(len, buf, type, chk, note, sw->verbose);
+}  /* sysfs_stp_read */
+
+static int sysfs_stp_write(struct ksz_sw *sw, int proc_num, int num,
+	const char *buf)
+{
+	struct ksz_stp_info *stp = &sw->info->rstp;
+	struct ksz_stp_bridge *br = &stp->br;
+	struct ksz_stp_port *p;
+	uint i;
+	int set;
+	int change = 0;
+	int processed = true;
+
+	if (!(sw->features & STP_SUPPORT))
+		return false;
+	mutex_lock(&stp->br.lock);
+	p = &br->ports[0];
+	switch (proc_num) {
+	case PROC_SET_STP_BR_ON:
+		set = !!num;
+		if (set != br->bridgeEnabled) {
+			br->bridgeEnabled = set;
+			if (br->bridgeEnabled) {
+				mutex_unlock(&stp->br.lock);
+				stp_start(stp);
+				mutex_lock(&stp->br.lock);
+			} else
+				stp_stop(stp, true);
+		}
+		break;
+	case PROC_SET_STP_BR_PRIO:
+		if ('0' != buf[0] || 'x' != buf[1])
+			sscanf(buf, "%x", &num);
+		if ((0 <= num && num <= 0xf000) && !(num & ~0xf000)) {
+			u16 prio = ntohs(BridgeIdentifier.prio);
+
+			if (num != prio) {
+				prio = num;
+				BridgeIdentifier.prio = htons(prio);
+				BridgePriority.prio.root.prio =
+				BridgePriority.prio.bridge_id.prio =
+					BridgeIdentifier.prio;
+				change = 1;
+			}
+		}
+		break;
+	case PROC_SET_STP_BR_FWD_DELAY:
+		if (checkParameters(BridgeHelloTime, BridgeMaxAge, num)) {
+			if (num != BridgeFwdDelay) {
+				BridgeFwdDelay = num;
+				change = 1;
+			}
+		}
+		break;
+	case PROC_SET_STP_BR_MAX_AGE:
+		if (checkParameters(BridgeHelloTime, num, BridgeFwdDelay)) {
+			if (num != BridgeMaxAge) {
+				BridgeMaxAge = num;
+				change = 1;
+			}
+		}
+		break;
+	case PROC_SET_STP_BR_HELLO_TIME:
+		if (num == 2) {
+			if (num != BridgeHelloTime) {
+				BridgeHelloTime = num;
+				change = 1;
+			}
+		}
+		break;
+	case PROC_SET_STP_BR_TX_HOLD:
+		if (1 <= num && num <= 10) {
+			if (num != TxHoldCount) {
+				TxHoldCount = num;
+				for (i = 0; i < br->port_cnt; i++) {
+					p = &br->ports[i];
+					txCount = 0;
+				}
+				change = 1;
+			}
+		}
+		break;
+	case PROC_SET_STP_VERSION:
+		if (0 == num || 2 == num) {
+			ForceProtocolVersion = num;
+			change = 2;
+		}
+		if (1 == num) {
+			stp_br_test_setup(br);
+			change = 1;
+		}
+		if (41 == num)
+			br->hack_4_1 = 1;
+		if (42 == num)
+			br->hack_4_2 = 1;
+		if (52 == num)
+			br->hack_5_2 = 1;
+		break;
+	default:
+		processed = false;
+		break;
+	}
+	mutex_unlock(&stp->br.lock);
+	if (change && br->bridgeEnabled) {
+		reselectAll(br);
+		if (2 == change)
+			stp_state_init(br);
+		else
+			invoke_state_machines(br);
+	}
+	return processed;
+}  /* sysfs_stp_write */
+
+static ssize_t sysfs_stp_port_read(struct ksz_sw *sw, int proc_num, uint port,
+	ssize_t len, char *buf)
+{
+	struct ksz_stp_info *stp = &sw->info->rstp;
+	struct ksz_stp_port *p;
+	char *state_str;
+	int chk = 0;
+	int type = SHOW_HELP_NONE;
+	char note[40];
+
+	note[0] = '\0';
+	if (!(sw->features & STP_SUPPORT))
+		return 0;
+	port = get_sysfs_port(sw, port);
+	if (port == sw->HOST_PORT)
+		return 0;
+	mutex_lock(&stp->br.lock);
+	p = &stp->br.ports[port];
+	switch (proc_num) {
+	case PROC_GET_STP_INFO:
+		if (p->off)
+			break;
+		state_str = get_port_role_str(role);
+		len += sprintf(buf + len,
+			" enabled\t\t%4s",
+			BOOL_STR(portEnabled));
+		len += sprintf(buf + len,
+			"\t\t\trole\t\t%12s\n",
+			state_str);
+		state_str = get_port_state_str(p->states[6]);
+		len += sprintf(buf + len,
+			" port id\t\t%02x%02x\t\t\tstate\t\t%12s\n",
+		       portId.prio, portId.num, state_str);
+		len += sprintf(buf + len,
+			" path cost\t%12u\t\t\tadmin path cost\t%12u\n",
+			PortPathCost, AdminPortPathCost);
+		len += sprintf(buf + len,
+			" designated root\t" BR_ID_FMT,
+			BR_ID_ARGS(designatedPriority.root));
+		len += sprintf(buf + len,
+			"\tdesignated cost\t%12u\n",
+			ntohl(designatedPriority.root_path_cost));
+		len += sprintf(buf + len,
+			" designated bridge\t" BR_ID_FMT,
+			BR_ID_ARGS(designatedPriority.bridge_id));
+		len += sprintf(buf + len,
+			"\tdesignated port\t\t%02x%02x\n",
+			designatedPriority.port_id.prio,
+			designatedPriority.port_id.num);
+		len += sprintf(buf + len,
+			" admin edge port\t%4s",
+			BOOL_STR(AdminEdge));
+		len += sprintf(buf + len,
+			"\t\t\tauto edge port\t\t%4s\n",
+			BOOL_STR(AutoEdge));
+		len += sprintf(buf + len,
+			" oper edge port\t\t%4s",
+			BOOL_STR(operEdge));
+		len += sprintf(buf + len,
+			"\t\t\ttopology change ack\t%4s\n",
+			BOOL_STR(tcAck));
+		len += sprintf(buf + len,
+			" point to point\t\t%4s",
+			BOOL_STR(operPointToPointMAC));
+		len += sprintf(buf + len,
+			"\t\t\tadmin point to point\t%4s\n",
+			get_admin_p2p_str(adminPointToPointMAC));
+		type = SHOW_HELP_NONE;
+		break;
+	case PROC_SET_STP_ON:
+		chk = portEnabled;
+		type = SHOW_HELP_ON_OFF;
+		break;
+	case PROC_SET_STP_PRIO:
+		chk = portId.prio;
+		type = SHOW_HELP_HEX;
+		break;
+	case PROC_SET_STP_ADMIN_PATH_COST:
+		chk = AdminPortPathCost;
+		type = SHOW_HELP_NUM;
+		break;
+	case PROC_SET_STP_PATH_COST:
+		chk = PortPathCost;
+		type = SHOW_HELP_NUM;
+		break;
+	case PROC_SET_STP_ADMIN_EDGE:
+		chk = AdminEdge;
+		type = SHOW_HELP_ON_OFF;
+		break;
+	case PROC_SET_STP_AUTO_EDGE:
+		chk = AutoEdge;
+		type = SHOW_HELP_ON_OFF;
+		break;
+	case PROC_SET_STP_MCHECK:
+		chk = mcheck;
+		type = SHOW_HELP_ON_OFF;
+		break;
+	case PROC_SET_STP_ADMIN_P2P:
+		chk = adminPointToPointMAC;
+		if (sw->verbose) {
+			switch (chk) {
+			case ADMIN_P2P_AUTO:
+				strcpy(note, " (auto)");
+				break;
+			case ADMIN_P2P_FORCE_TRUE:
+				strcpy(note, " (force true)");
+				break;
+			case ADMIN_P2P_FORCE_FALSE:
+				strcpy(note, " (force false)");
+				break;
+			}
+		}
+		type = SHOW_HELP_SPECIAL;
+		break;
+	}
+	mutex_unlock(&stp->br.lock);
+	return sysfs_show(len, buf, type, chk, note, sw->verbose);
+}  /* sysfs_stp_port_read */
+
+static int sysfs_stp_port_write(struct ksz_sw *sw, int proc_num, uint port,
+	int num, const char *buf)
+{
+	struct ksz_stp_info *stp = &sw->info->rstp;
+	struct ksz_stp_port *p;
+	int set;
+	int change = 0;
+	int processed = true;
+
+	if (!(sw->features & STP_SUPPORT))
+		return false;
+	port = get_sysfs_port(sw, port);
+	if (port == sw->HOST_PORT)
+		return false;
+	mutex_lock(&stp->br.lock);
+	p = &stp->br.ports[port];
+	switch (proc_num) {
+	case PROC_SET_STP_ON:
+		set = !!num;
+		if (set != portEnabled) {
+			portEnabled = set;
+			if (portEnabled && p->br->hack_4_2)
+				mdelayWhile = 0;
+			change = 1;
+		}
+		break;
+	case PROC_SET_STP_PRIO:
+		if ('0' != buf[0] || 'x' != buf[1])
+			sscanf(buf, "%x", &num);
+		if ((0 <= num && num <= 0xf0) && !(num & ~0xf0)) {
+			u8 prio = portId.prio;
+
+			if (num != prio) {
+				portId.prio = num;
+				reselect = TRUE;
+				selected = FALSE;
+				change = 1;
+			}
+		}
+		break;
+	case PROC_SET_STP_ADMIN_PATH_COST:
+		if (0 <= num && num <= PATH_COST * 10) {
+			AdminPortPathCost = num;
+			change = checkPathCost(p);
+		}
+		break;
+	case PROC_SET_STP_PATH_COST:
+		if (1 <= num && num <= PATH_COST * 10) {
+			AdminPortPathCost = num;
+			change = checkPathCost(p);
+		}
+		break;
+	case PROC_SET_STP_ADMIN_EDGE:
+		set = !!num;
+		if (set != AdminEdge) {
+			AdminEdge = set;
+			change = 1;
+		}
+		break;
+	case PROC_SET_STP_AUTO_EDGE:
+		set = !!num;
+		if (set != AutoEdge) {
+			AutoEdge = set;
+			change = 1;
+		}
+		break;
+	case PROC_SET_STP_MCHECK:
+		set = !!num;
+		if (set != mcheck) {
+			mcheck = set;
+			change = 1;
+		}
+		break;
+	case PROC_SET_STP_ADMIN_P2P:
+		if (ADMIN_P2P_FORCE_FALSE <= num && num <= ADMIN_P2P_AUTO) {
+			if (num != adminPointToPointMAC) {
+				adminPointToPointMAC = num;
+				change = checkP2P(p);
+			}
+		}
+		break;
+	default:
+		processed = false;
+		break;
+	}
+	mutex_unlock(&stp->br.lock);
+	if (change) {
+		p->rx_bpdu0.protocol = 0xff;
+		invoke_state_machines(p->br);
+	}
+	return processed;
+}  /* sysfs_stp_port_write */
+
+static u8 MAC_ADDR_STP[] = { 0x01, 0x80, 0xC2, 0x00, 0x00, 0x00 };
+
+static void prep_stp_mcast(struct net_device *dev)
+{
+	char addr[MAX_ADDR_LEN];
+
+	memcpy(addr, MAC_ADDR_STP, ETH_ALEN);
+	dev_mc_add(dev, addr);
+}  /* prep_stp_mcast */
+
+static void leave_stp(struct ksz_stp_info *stp)
+{
+	if (stp->dev)
+		dev_mc_del(stp->dev, MAC_ADDR_STP);
+}  /* leave_stp */
+
+static struct stp_ops stp_ops = {
+	.change_addr		= stp_change_addr,
+	.link_change		= stp_link_change,
+
+	.get_tcDetected		= stp_get_tcDetected,
+};
+
+static void ksz_stp_exit(struct ksz_stp_info *stp)
+{
+	/* stp_stop should be called before. */
+}  /* ksz_stp_exit */
+
+static void ksz_stp_init(struct ksz_stp_info *stp, struct ksz_sw *sw)
+{
+	struct ksz_stp_bridge *br;
+	struct ksz_stp_port *p;
+	uint i;
+	int num;
+	struct llc *llc;
+#ifdef DBG_STP_PORT_FLUSH
+	struct ksz_stp_dbg_times *x;
+#endif
+
+	stp->sw_dev = sw;
+	ksz_init_timer(&stp->port_timer_info, STP_TIMER_TICK * HZ / 1000,
+		port_timer_monitor, stp);
+	INIT_WORK(&stp->state_machine, proc_state_machines);
+	INIT_WORK(&stp->rx_proc, proc_rx);
+	mutex_init(&stp->br.lock);
+	stp->ops = &stp_ops;
+
+	memcpy(stp->tx_frame, MAC_ADDR_STP, ETH_ALEN);
+	llc = (struct llc *) &stp->tx_frame[12];
+	llc->dsap = 0x42;
+	llc->ssap = 0x42;
+	llc->ctrl = 0x03;
+	stp->bpdu = (struct bpdu *)(llc + 1);
+
+	br = &stp->br;
+	br->parent = stp;
+	if (sw->stp)
+		br->bridgeEnabled = TRUE;
+
+	br->port_cnt = sw->port_cnt;
+	if (br->port_cnt > SWITCH_PORT_NUM)
+		br->port_cnt = SWITCH_PORT_NUM;
+
+	/* Can turn off ports.  Useful for using one port for telnet. */
+	num = sw->stp;
+	if (1 == num)
+		num = sw->PORT_MASK;
+	num &= ~sw->HOST_MASK;
+	for (i = 0; i < SWITCH_PORT_NUM; i++) {
+		p = &br->ports[i];
+		if (!(num & (1 << i)))
+			p->off = TRUE;
+		skb_queue_head_init(&p->rxq);
+		p->port_index = i;
+		p->br = br;
+		stp_port_init(p);
+	}
+
+	num = 1;
+	for (i = 0; i < br->port_cnt; i++) {
+		p = &br->ports[i];
+		if (p->off) {
+			selectedRole = ROLE_DISABLED;
+			role = ROLE_DISABLED;
+			infoIs = INFO_TYPE_DISABLED;
+			synced = TRUE;
+			continue;
+		}
+		portId.num = get_log_port(sw, i);
+
+#ifdef DBG_STP_PORT_FLUSH
+		x = &p->dbg_times[0];
+
+		/* No Root Port connected to port yet. */
+		x->role_ = ROLE_DISABLED;
+		x->downPriority.port_id.num = 0;
+#endif
+	}
+
+	p = &br->ports[0];
+	ZERO(BridgePriority);
+	stp_br_init(p);
+
+#if defined(HAVE_VID2FID)
+	sw->info->vid2fid[0] = sw->info->vid2fid[NUM_OF_VID + 1] = 0;
+	for (i = 1; i < NUM_OF_VID; i++) {
+		sw->info->vid2fid[i] = ((i - 1) % (FID_ENTRIES - 1)) + 1;
+	}
+#endif
+}  /* ksz_stp_init */
+
+
+#undef forward
+#undef forwarding
+#undef learn
+#undef learning
+#undef proposed
+#undef proposing
+#undef reselect
+#undef selected
+#undef sync
+#undef synced
+
diff -Napur linux-linux4microchip-2022.10-orig/drivers/net/ethernet/micrel/ksz_stp.h linux-linux4microchip-2022.10/drivers/net/ethernet/micrel/ksz_stp.h
--- linux-linux4microchip-2022.10-orig/drivers/net/ethernet/micrel/ksz_stp.h	1969-12-31 16:00:00.000000000 -0800
+++ linux-linux4microchip-2022.10/drivers/net/ethernet/micrel/ksz_stp.h	2023-04-25 16:13:55.364163493 -0700
@@ -0,0 +1,347 @@
+/**
+ * Microchip STP driver header
+ *
+ * Copyright (c) 2016-2017 Microchip Technology Inc.
+ *	Tristram Ha <Tristram.Ha@microchip.com>
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ */
+
+
+#ifndef KSZ_STP_H
+#define KSZ_STP_H
+
+
+#define STP_TAG_TYPE			0x1080
+
+
+struct llc {
+	u16 len;
+	u8 dsap;
+	u8 ssap;
+	u8 ctrl;
+} __packed;
+
+struct _bridge_id {
+	u16 prio;
+	u8 addr[6];
+} __packed;
+
+struct _port_id {
+	u8 prio;
+	u8 num;
+} __packed;
+
+struct bpdu {
+	u16 protocol;
+	u8 version;
+	u8 type;
+	u8 flags;
+	struct _bridge_id root;
+	u32 root_path_cost;
+	struct _bridge_id bridge_id;
+	struct _port_id port_id;
+	u16 message_age;
+	u16 max_age;
+	u16 hello_time;
+	u16 forward_delay;
+	u8 version_1_length;
+} __packed;
+
+
+struct stp_prio {
+	struct _bridge_id root;
+	u32 root_path_cost;
+	struct _bridge_id bridge_id;
+	struct _port_id port_id;
+} __packed;
+
+struct stp_vector {
+	struct stp_prio prio;
+	struct _port_id port_id;
+} __packed;
+
+struct stp_times {
+	u16 message_age;
+	u16 max_age;
+	u16 hello_time;
+	u16 forward_delay;
+} __packed;
+
+
+enum {
+	ADMIN_P2P_FORCE_FALSE,
+	ADMIN_P2P_FORCE_TRUE,
+	ADMIN_P2P_AUTO,
+};
+
+enum {
+	ROLE_UNKNOWN,
+	ROLE_ALT_BACKUP,
+	ROLE_ROOT,
+	ROLE_DESIGNATED,
+	ROLE_ALTERNATE,
+	ROLE_BACKUP,
+	ROLE_DISABLED,
+};
+
+enum {
+	INFO_SUPERIOR_DESIGNATED,
+	INFO_REPEATED_DESIGNATED,
+	INFO_INFERIOR_DESIGNATED,
+	INFO_INFERIOR_ROOT_ALT,
+	INFO_OTHER,
+};
+
+enum {
+	INFO_TYPE_UNKNOWN,
+	INFO_TYPE_MINE,
+	INFO_TYPE_AGED,
+	INFO_TYPE_RECEIVED,
+	INFO_TYPE_DISABLED,
+};
+
+struct stp_br_vars {
+	struct _bridge_id br_id_;
+	struct stp_vector bridgePrio_;
+	struct stp_prio rootPrio_;
+	struct stp_times bridgeTimes_;
+	struct stp_times rootTimes_;
+	struct _port_id rootPortId_;
+	u32 TC_sec_;
+	u32 TC_cnt_;
+	u32 TC_set_;
+
+	uint MigrateTime_;
+	uint TxHoldCount_;
+	u8 ForceProtocolVersion_;
+};
+
+struct ksz_stp_timers {
+	/*
+	 * Edge Delay timer is time remaining before port is identified as an
+	 * operEdgePort.
+	 */
+	uint edgeDelayWhile_;
+	/*
+	 * Forward Delay timer is used to delay Port State transitions until
+	 * other Bridges have received spanning tree information.
+	 */
+	uint fdWhile_;
+	/*
+	 * Hello timer is used to ensure that at least one BPDU is sent by a
+	 * Desginated Port in each HelloTime period.
+	 */
+	uint helloWhen_;
+	/*
+	 * Migration Delay timer is used to allow time for another RSTP Bridge
+	 * to synchronize its migration state before the receipt of a BPDU can
+	 * cause this Port to change the BPDU type it sends.
+	 */
+	uint mdelayWhile_;
+	/*
+	 * Recent Backup timer is maintained at twice HelloTime while the Port
+	 * is a Backup Port.
+	 */
+	uint rbWhile_;
+	/*
+	 * Received Info timer is time remaining before spanning tree
+	 * inforamtion is aged out.
+	 */
+	uint rcvdInfoWhile_;
+	/*
+	 * Recent Root timer.
+	 */
+	uint rrWhile_;
+	/*
+	 * Topology Change timer is used to send TCN Messages.
+	 */
+	uint tcWhile_;
+	uint tcDetected_;
+};
+
+struct stp_admin_vars {
+	u32 AdminEdgePort_:1;
+	u32 AutoEdgePort_:1;
+	u32 operPointToPointMAC_:1;
+
+	u8 adminPointToPointMAC_;
+	uint adminPortPathCost_;
+
+};
+
+struct stp_vars {
+	u32 agree_:1;
+	u32 agreed_:1;
+	u32 disputed_:1;
+	u32 fdbFlush_:1;
+	u32 forward_:1;
+	u32 forwarding_:1;
+	u32 learn_:1;
+	u32 learning_:1;
+	u32 mcheck_:1;
+	u32 newInfo_:1;
+	u32 operEdge_:1;
+	u32 portEnabled_:1;
+	u32 proposed_:1;
+	u32 proposing_:1;
+	u32 rcvdBPDU_:1;
+	u32 rcvdMsg_:1;
+	u32 rcvdRSTP_:1;
+	u32 rcvdSTP_:1;
+	u32 rcvdTc_:1;
+	u32 rcvdTcAck_:1;
+	u32 rcvdTcn_:1;
+	u32 reRoot_:1;
+	u32 reselect_:1;
+	u32 selected_:1;
+	u32 sendRSTP_:1;
+	u32 sync_:1;
+	u32 synced_:1;
+	u32 tcAck_:1;
+	u32 tcProp_:1;
+	u32 tick_:1;
+	u32 updtInfo_:1;
+
+	uint ageingTime_;
+
+	u8 infoIs_;
+	u8 rcvdInfo_;
+	u8 role_;
+	u8 selectedRole_;
+
+	uint txCount_;
+	struct _port_id portId_;
+	uint PortPathCost_;
+
+	struct stp_prio desgPrio_;
+	struct stp_prio msgPrio_;
+	struct stp_prio portPrio_;
+	struct stp_times desgTimes_;
+	struct stp_times msgTimes_;
+	struct stp_times portTimes_;
+};
+
+#define NUM_OF_PORT_TIMERS		(9 + 1)
+
+struct stp_port_vars {
+	uint timers[NUM_OF_PORT_TIMERS];
+	struct stp_admin_vars admin_var;
+	struct stp_vars stp_var;
+
+	u8 bpduVersion_;
+	u8 bpduType_;
+	u8 bpduFlags_;
+	u8 bpduRole_;
+	struct stp_prio bpduPrio_;
+	struct stp_times bpduTimes_;
+};
+
+struct ksz_stp_bridge;
+
+#define NUM_OF_PORT_STATE_MACHINES	9
+
+struct ksz_stp_dbg_times {
+	struct stp_prio downPriority;
+	struct stp_prio lastPriority;
+	unsigned long alt_jiffies;
+	unsigned long learn_jiffies;
+	unsigned long block_jiffies;
+	u8 role_;
+};
+
+struct ksz_stp_port {
+	struct stp_port_vars vars;
+
+	u8 states[NUM_OF_PORT_STATE_MACHINES];
+	u8 state_index;
+	u8 port_index;
+	int off;
+	int link;
+	int duplex;
+	int speed;
+	struct sk_buff_head rxq;
+
+#if 0
+	struct bpdu own_bpdu;
+#endif
+	struct bpdu rx_bpdu0;
+	struct bpdu tx_bpdu0;
+#if 0
+	struct bpdu tx_bpdu1;
+#endif
+	int dbg_rx;
+	int dbg_tx;
+#if 0
+	int dbg_tx_bpdu;
+#endif
+
+	struct ksz_stp_dbg_times dbg_times[1];
+
+	struct ksz_stp_bridge *br;
+};
+
+#define NUM_OF_BRIDGE_STATE_MACHINES	1
+
+struct ksz_stp_bridge {
+	struct stp_br_vars vars;
+
+	u8 bridgeEnabled;
+
+	struct ksz_stp_port ports[SWITCH_PORT_NUM];
+	u8 port_cnt;
+	u8 skip_tx;
+	u16 port_rx;
+
+	u8 states[NUM_OF_BRIDGE_STATE_MACHINES];
+	u8 state_index;
+
+	void *parent;
+	struct mutex lock;
+
+	u32 hack_4_1:1;
+	u32 hack_4_2:1;
+	u32 hack_5_2:1;
+};
+
+
+struct ksz_stp_info;
+
+struct stp_ops {
+	int (*change_addr)(struct ksz_stp_info *stp, u8 *addr);
+	void (*link_change)(struct ksz_stp_info *stp, int update);
+
+	int (*dev_req)(struct ksz_stp_info *stp, char *arg, void *info);
+
+	int (*get_tcDetected)(struct ksz_stp_info *info, int p);
+};
+
+struct ksz_stp_info {
+	struct ksz_stp_bridge br;
+
+	void *sw_dev;
+	struct net_device *dev;
+	struct ksz_timer_info port_timer_info;
+	uint timer_tick;
+	struct work_struct state_machine;
+	bool machine_running;
+	struct work_struct rx_proc;
+
+	u8 tx_frame[60];
+	struct bpdu *bpdu;
+	int len;
+
+	struct sw_dev_info *dev_info;
+	uint notifications;
+
+	const struct stp_ops *ops;
+};
+
+#endif
diff -Napur linux-linux4microchip-2022.10-orig/drivers/net/ethernet/micrel/ksz_sw_8795.c linux-linux4microchip-2022.10/drivers/net/ethernet/micrel/ksz_sw_8795.c
--- linux-linux4microchip-2022.10-orig/drivers/net/ethernet/micrel/ksz_sw_8795.c	1969-12-31 16:00:00.000000000 -0800
+++ linux-linux4microchip-2022.10/drivers/net/ethernet/micrel/ksz_sw_8795.c	2024-04-26 16:30:49.569893539 -0700
@@ -0,0 +1,12230 @@
+/**
+ * Microchip KSZ8795 switch common code
+ *
+ * Copyright (c) 2015-2024 Microchip Technology Inc.
+ *	Tristram Ha <Tristram.Ha@microchip.com>
+ *
+ * Copyright (c) 2010-2015 Micrel, Inc.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ */
+
+
+#ifdef CONFIG_KSZ_DLR
+/* Have ACL to handle beacon timeout. */
+#define CONFIG_HAVE_ACL_HW
+#endif
+
+/* -------------------------------------------------------------------------- */
+
+#define MAX_SYSFS_BUF_SIZE		(4080 - 80)
+
+#if 0
+#define USE_SAME_ADDR
+#endif
+
+enum {
+	PROC_SW_INFO,
+	PROC_SW_VERSION,
+
+	PROC_SET_SW_DUPLEX,
+	PROC_SET_SW_SPEED,
+	PROC_SET_SW_FORCE,
+	PROC_SET_SW_FLOW_CTRL,
+
+	PROC_SET_SW_FEATURES,
+	PROC_SET_SW_OVERRIDES,
+	PROC_SET_SW_MIB,
+
+	PROC_SET_SW_REG,
+	PROC_SET_SW_VID,
+
+	PROC_DYNAMIC,
+	PROC_STATIC,
+	PROC_VLAN,
+
+	PROC_SET_AGING,
+	PROC_SET_FAST_AGING,
+	PROC_SET_LINK_AGING,
+
+	PROC_SET_BROADCAST_STORM,
+	PROC_SET_MULTICAST_STORM,
+	PROC_SET_TX_RATE_QUEUE_BASED,
+	PROC_SET_DIFFSERV,
+	PROC_SET_802_1P,
+
+	PROC_ENABLE_VLAN,
+	PROC_SET_REPLACE_NULL_VID,
+	PROC_SET_MAC_ADDR,
+	PROC_SET_MIRROR_MODE,
+	PROC_SET_TAIL_TAG,
+
+	PROC_SET_IGMP_SNOOP,
+	PROC_SET_IPV6_MLD_SNOOP,
+	PROC_SET_IPV6_MLD_OPTION,
+
+	PROC_SET_AGGR_BACKOFF,
+	PROC_SET_NO_EXC_DROP,
+
+	PROC_SET_HUGE_PACKET,
+	PROC_SET_LEGAL_PACKET,
+	PROC_SET_LENGTH_CHECK,
+
+	PROC_SET_BACK_PRESSURE_MODE,
+	PROC_SET_SWITCH_FLOW_CTRL,
+	PROC_SET_SWITCH_HALF_DUPLEX,
+	PROC_SET_SWITCH_10_MBIT,
+
+	PROC_SET_RX_FLOW_CTRL,
+	PROC_SET_TX_FLOW_CTRL,
+	PROC_SET_FAIR_FLOW_CTRL,
+	PROC_SET_VLAN_BOUNDARY,
+
+	PROC_SET_FORWARD_UNKNOWN_UNICAST,
+	PROC_SET_FORWARD_UNKNOWN_UNICAST_PORTS,
+	PROC_SET_FORWARD_UNKNOWN_MULTICAST,
+	PROC_SET_FORWARD_UNKNOWN_MULTICAST_PORTS,
+	PROC_SET_FORWARD_UNKNOWN_VID,
+	PROC_SET_FORWARD_UNKNOWN_VID_PORTS,
+	PROC_SET_FORWARD_UNKNOWN_IP_MULTICAST,
+	PROC_SET_FORWARD_UNKNOWN_IP_MULTICAST_PORTS,
+	PROC_SET_SELF_ADDR_FILTER,
+	PROC_SET_INS_TAG,
+
+	PROC_SET_PME,
+
+	PROC_SET_PASS_PAUSE,
+	PROC_SET_HI_PRIO_QUEUES,
+
+	PROC_GET_PORTS,
+	PROC_GET_DEV_START,
+	PROC_GET_VLAN_START,
+	PROC_GET_STP,
+
+	PROC_SET_STATIC_FID,
+	PROC_SET_STATIC_USE_FID,
+	PROC_SET_STATIC_OVERRIDE,
+	PROC_SET_STATIC_VALID,
+	PROC_SET_STATIC_PORTS,
+	PROC_SET_STATIC_MAC_ADDR,
+	PROC_SET_STATIC_INDEX,
+	PROC_GET_STATIC_INFO,
+
+	PROC_SET_VLAN_VALID,
+	PROC_SET_VLAN_MEMBER,
+	PROC_SET_VLAN_FID,
+	PROC_SET_VLAN_VID,
+	PROC_GET_VLAN_INFO,
+
+#ifdef CONFIG_KSZ_STP
+	PROC_GET_STP_BR_INFO,
+	PROC_SET_STP_BR_ON,
+	PROC_SET_STP_BR_PRIO,
+	PROC_SET_STP_BR_FWD_DELAY,
+	PROC_SET_STP_BR_MAX_AGE,
+	PROC_SET_STP_BR_HELLO_TIME,
+	PROC_SET_STP_BR_TX_HOLD,
+	PROC_SET_STP_VERSION,
+#endif
+};
+
+enum {
+	PROC_SET_PORT_MIB,
+
+	PROC_SET_DEF_VID,
+	PROC_SET_MEMBER,
+
+	PROC_ENABLE_BROADCAST_STORM,
+	PROC_ENABLE_DIFFSERV,
+	PROC_ENABLE_802_1P,
+
+	PROC_SET_PORT_BASED,
+
+	PROC_SET_DIS_NON_VID,
+	PROC_SET_INGRESS,
+	PROC_SET_INSERT_TAG,
+	PROC_SET_REMOVE_TAG,
+	PROC_SET_DROP_TAG,
+	PROC_SET_REPLACE_PRIO,
+
+	PROC_SET_RX,
+	PROC_SET_TX,
+	PROC_SET_LEARN,
+
+	PROC_SET_INS_TAG_0,
+	PROC_SET_INS_TAG_1,
+	PROC_SET_INS_TAG_2,
+	PROC_SET_INS_TAG_3,
+	PROC_SET_INS_TAG_4,
+
+	PROC_SET_PASS_ALL,
+
+	PROC_ENABLE_PRIO_QUEUE,
+	PROC_SET_TX_Q0_CTRL,
+	PROC_SET_TX_Q1_CTRL,
+	PROC_SET_TX_Q2_CTRL,
+	PROC_SET_TX_Q3_CTRL,
+	PROC_SET_TX_Q0_RATIO,
+	PROC_SET_TX_Q1_RATIO,
+	PROC_SET_TX_Q2_RATIO,
+	PROC_SET_TX_Q3_RATIO,
+
+	PROC_ENABLE_RX_PRIO_RATE,
+	PROC_ENABLE_TX_PRIO_RATE,
+	PROC_SET_RX_LIMIT,
+	PROC_SET_RX_LIMIT_PORT_BASED,
+	PROC_SET_LIMIT_PACKET_BASED,
+	PROC_SET_RX_LIMIT_FLOW_CTRL,
+	PROC_SET_CNT_IFG,
+	PROC_SET_CNT_PRE,
+	PROC_SET_RX_P0_RATE,
+	PROC_SET_RX_P1_RATE,
+	PROC_SET_RX_P2_RATE,
+	PROC_SET_RX_P3_RATE,
+	PROC_SET_TX_Q0_RATE,
+	PROC_SET_TX_Q1_RATE,
+	PROC_SET_TX_Q2_RATE,
+	PROC_SET_TX_Q3_RATE,
+
+	PROC_SET_MIRROR_PORT,
+	PROC_SET_MIRROR_RX,
+	PROC_SET_MIRROR_TX,
+
+	PROC_SET_BACK_PRESSURE,
+	PROC_SET_FORCE_FLOW_CTRL,
+
+	PROC_SET_UNKNOWN_UNICAST_PORT,
+	PROC_SET_UNKNOWN_MULTICAST_PORT,
+	PROC_SET_UNKNOWN_VID_PORT,
+	PROC_SET_UNKNOWN_IP_MULTICAST_PORT,
+
+	PROC_SET_PORT_PME_CTRL,
+	PROC_SET_PORT_PME_STATUS,
+
+	PROC_SET_AUTHEN_MODE,
+	PROC_SET_ACL,
+	PROC_SET_ACL_FIRST_RULE,
+	PROC_SET_ACL_RULESET,
+	PROC_SET_ACL_MODE,
+	PROC_SET_ACL_ENABLE,
+	PROC_SET_ACL_SRC,
+	PROC_SET_ACL_EQUAL,
+	PROC_SET_ACL_MAC_ADDR,
+	PROC_SET_ACL_TYPE,
+	PROC_SET_ACL_CNT,
+	PROC_SET_ACL_MSEC,
+	PROC_SET_ACL_INTR_MODE,
+	PROC_SET_ACL_IP_ADDR,
+	PROC_SET_ACL_IP_MASK,
+	PROC_SET_ACL_PROTOCOL,
+	PROC_SET_ACL_SEQNUM,
+	PROC_SET_ACL_PORT_MODE,
+	PROC_SET_ACL_MAX_PORT,
+	PROC_SET_ACL_MIN_PORT,
+	PROC_SET_ACL_TCP_FLAG_ENABLE,
+	PROC_SET_ACL_TCP_FLAG,
+	PROC_SET_ACL_TCP_FLAG_MASK,
+	PROC_SET_ACL_PRIO_MODE,
+	PROC_SET_ACL_PRIO,
+	PROC_SET_ACL_VLAN_PRIO_REPLACE,
+	PROC_SET_ACL_VLAN_PRIO,
+	PROC_SET_ACL_MAP_MODE,
+	PROC_SET_ACL_PORTS,
+	PROC_SET_ACL_INDEX,
+	PROC_SET_ACL_ACTION_INDEX,
+	PROC_SET_ACL_ACTION,
+	PROC_SET_ACL_RULE_INDEX,
+	PROC_SET_ACL_INFO,
+	PROC_GET_ACL_TABLE,
+
+	PROC_SET_PORT_DUPLEX,
+	PROC_SET_PORT_SPEED,
+	PROC_SET_LINK_MD,
+
+#ifdef CONFIG_KSZ_STP
+	PROC_GET_STP_INFO,
+	PROC_SET_STP_ON,
+	PROC_SET_STP_PRIO,
+	PROC_SET_STP_ADMIN_PATH_COST,
+	PROC_SET_STP_PATH_COST,
+	PROC_SET_STP_ADMIN_EDGE,
+	PROC_SET_STP_AUTO_EDGE,
+	PROC_SET_STP_MCHECK,
+	PROC_SET_STP_ADMIN_P2P,
+#endif
+
+};
+
+/* -------------------------------------------------------------------------- */
+
+static uint get_phy_port(struct ksz_sw *sw, uint n)
+{
+	if (n >= sw->mib_port_cnt + 1)
+		n = 0;
+	return sw->port_info[n].phy_p;
+}
+
+static uint get_log_port(struct ksz_sw *sw, uint p)
+{
+	return sw->port_info[p].log_p;
+}
+
+static uint get_log_port_zero(struct ksz_sw *sw, uint p)
+{
+	uint n;
+
+	n = get_log_port(sw, p);
+	if (n)
+		n--;
+	else
+		n = sw->mib_port_cnt;
+	return n;
+}
+
+static uint get_phy_mask_from_log(struct ksz_sw *sw, uint log_m)
+{
+	struct ksz_port_info *info;
+	uint n;
+	uint p;
+	uint phy_m = 0;
+
+	for (n = 0; n <= sw->mib_port_cnt; n++) {
+		info = &sw->port_info[n];
+		p = info->phy_p;
+		if (log_m & sw->port_info[p].log_m)
+			phy_m |= info->phy_m;
+	}
+	return phy_m;
+}
+
+static uint get_log_mask_from_phy(struct ksz_sw *sw, uint phy_m)
+{
+	struct ksz_port_info *info;
+	uint n;
+	uint p;
+	uint log_m = 0;
+
+	for (n = 0; n <= sw->mib_port_cnt; n++) {
+		info = &sw->port_info[n];
+		p = info->phy_p;
+		if (phy_m & info->phy_m)
+			log_m |= sw->port_info[p].log_m;
+	}
+	return log_m;
+}
+
+static uint get_sysfs_port(struct ksz_sw *sw, uint n)
+{
+	uint p = n;
+
+	if (!(sw->overrides & SYSFS_PHY_PORT)) {
+		n++;
+		if (n > sw->mib_port_cnt)
+			n = 0;
+		p = get_phy_port(sw, n);
+	}
+	return p;
+}
+
+static inline struct ksz_port_cfg *get_port_cfg(struct ksz_sw *sw, uint p)
+{
+	return &sw->info->port_cfg[p];
+}
+
+static inline struct ksz_port_info *get_port_info(struct ksz_sw *sw, uint p)
+{
+	return &sw->port_info[p];
+}
+
+static inline struct ksz_port_mib *get_port_mib(struct ksz_sw *sw, uint p)
+{
+	return &sw->port_mib[p];
+}
+
+static void sw_acquire(struct ksz_sw *sw)
+{
+	mutex_lock(&sw->lock);
+	sw->reg->lock(sw);
+	mutex_unlock(&sw->lock);
+}  /* sw_acquire */
+
+static void sw_release(struct ksz_sw *sw)
+{
+	sw->reg->unlock(sw);
+}  /* sw_release */
+
+/* -------------------------------------------------------------------------- */
+
+/*
+#define STATIC_MAC_TABLE_ADDR		00-0000FFFF-FFFFFFFF
+#define STATIC_MAC_TABLE_FWD_PORTS	00-001F0000-00000000
+#define STATIC_MAC_TABLE_VALID		00-00200000-00000000
+#define STATIC_MAC_TABLE_OVERRIDE	00-00400000-00000000
+#define STATIC_MAC_TABLE_USE_FID	00-00800000-00000000
+#define STATIC_MAC_TABLE_FID		00-7F000000-00000000
+*/
+
+#define STATIC_MAC_TABLE_ADDR		0x0000FFFF
+#define STATIC_MAC_TABLE_FWD_PORTS	0x001F0000
+#define STATIC_MAC_TABLE_VALID		0x00200000
+#define STATIC_MAC_TABLE_OVERRIDE	0x00400000
+#define STATIC_MAC_TABLE_USE_FID	0x00800000
+#define STATIC_MAC_TABLE_FID		0x7F000000
+
+#define STATIC_MAC_FWD_PORTS_S		16
+#define STATIC_MAC_FID_S		24
+
+/*
+#define VLAN_TABLE_FID			00-007F007F-007F007F
+#define VLAN_TABLE_MEMBERSHIP		00-0F800F80-0F800F80
+#define VLAN_TABLE_VALID		00-10001000-10001000
+*/
+
+#define VLAN_TABLE_FID			0x007F
+#define VLAN_TABLE_MEMBERSHIP		0x0F80
+#define VLAN_TABLE_VALID		0x1000
+
+#define VLAN_TABLE_MEMBERSHIP_S		7
+#define VLAN_TABLE_S			16
+
+/*
+#define DYNAMIC_MAC_TABLE_ADDR		00-0000FFFF-FFFFFFFF
+#define DYNAMIC_MAC_TABLE_FID		00-007F0000-00000000
+#define DYNAMIC_MAC_TABLE_NOT_READY	00-00800000-00000000
+#define DYNAMIC_MAC_TABLE_SRC_PORT	00-07000000-00000000
+#define DYNAMIC_MAC_TABLE_TIMESTAMP	00-18000000-00000000
+#define DYNAMIC_MAC_TABLE_ENTRIES	7F-E0000000-00000000
+#define DYNAMIC_MAC_TABLE_MAC_EMPTY	80-00000000-00000000
+*/
+
+#define DYNAMIC_MAC_TABLE_ADDR		0x0000FFFF
+#define DYNAMIC_MAC_TABLE_FID		0x007F0000
+#define DYNAMIC_MAC_TABLE_SRC_PORT	0x07000000
+#define DYNAMIC_MAC_TABLE_TIMESTAMP	0x18000000
+#define DYNAMIC_MAC_TABLE_ENTRIES	0xE0000000
+
+#define DYNAMIC_MAC_TABLE_NOT_READY	0x80
+
+#define DYNAMIC_MAC_TABLE_ENTRIES_H	0x7F
+#define DYNAMIC_MAC_TABLE_MAC_EMPTY	0x80
+
+#define DYNAMIC_MAC_FID_S		16
+#define DYNAMIC_MAC_SRC_PORT_S		24
+#define DYNAMIC_MAC_TIMESTAMP_S		27
+#define DYNAMIC_MAC_ENTRIES_S		29
+#define DYNAMIC_MAC_ENTRIES_H_S		3
+
+/*
+#define MIB_COUNTER_VALUE		00-00000000-3FFFFFFF
+#define MIB_TOTAL_BYTES			00-0000000F-FFFFFFFF
+#define MIB_PACKET_DROPPED		00-00000000-0000FFFF
+#define MIB_COUNTER_VALID		00-00000020-00000000
+#define MIB_COUNTER_OVERFLOW		00-00000040-00000000
+*/
+
+#ifndef MIB_COUNTER_OVERFLOW
+#define MIB_COUNTER_OVERFLOW		(1 << 6)
+#define MIB_COUNTER_VALID		(1 << 5)
+
+#define MIB_COUNTER_VALUE		0x3FFFFFFF
+#endif
+
+#define KS_MIB_TOTAL_RX_0		0x100
+#define KS_MIB_TOTAL_TX_0		0x101
+#define KS_MIB_PACKET_DROPPED_RX_0	0x102
+#define KS_MIB_PACKET_DROPPED_TX_0	0x103
+#define KS_MIB_TOTAL_RX_1		0x104
+#define KS_MIB_TOTAL_TX_1		0x105
+#define KS_MIB_PACKET_DROPPED_TX_1	0x106
+#define KS_MIB_PACKET_DROPPED_RX_1	0x107
+#define KS_MIB_TOTAL_RX_2		0x108
+#define KS_MIB_TOTAL_TX_2		0x109
+#define KS_MIB_PACKET_DROPPED_TX_2	0x10A
+#define KS_MIB_PACKET_DROPPED_RX_2	0x10B
+#define KS_MIB_TOTAL_RX_3		0x10C
+#define KS_MIB_TOTAL_TX_3		0x10D
+#define KS_MIB_PACKET_DROPPED_TX_3	0x10E
+#define KS_MIB_PACKET_DROPPED_RX_3	0x10F
+#define KS_MIB_TOTAL_RX_4		0x110
+#define KS_MIB_TOTAL_TX_4		0x111
+#define KS_MIB_PACKET_DROPPED_TX_4	0x112
+#define KS_MIB_PACKET_DROPPED_RX_4	0x113
+
+#define MIB_PACKET_DROPPED		0x0000FFFF
+
+#define MIB_TOTAL_BYTES_H		0x0000000F
+
+/* -------------------------------------------------------------------------- */
+
+/* Switch functions */
+
+/**
+ * sw_r_table_64 - read 64 bits of data from switch table
+ * @sw:		The switch instance.
+ * @table:	The table selector.
+ * @addr:	The address of the table entry.
+ * @data_hi:	Buffer to store the high part of read data (bit63 ~ bit32).
+ * @data_lo:	Buffer to store the low part of read data (bit31 ~ bit0).
+ *
+ * This routine reads 64 bits of data from the table of the switch.
+ * Hardware is locked to minimize corruption of read data.
+ */
+static void sw_r_table_64(struct ksz_sw *sw, int table, u16 addr, u32 *data_hi,
+	u32 *data_lo)
+{
+	u64 buf;
+	u16 ctrl_addr;
+
+	ctrl_addr = IND_ACC_TABLE(table | TABLE_READ) | addr;
+
+	sw->ops->acquire(sw);
+	sw->reg->w16(sw, REG_IND_CTRL_0, ctrl_addr);
+	sw->reg->r(sw, REG_IND_DATA_HI, &buf, sizeof(buf));
+	sw->ops->release(sw);
+	buf = be64_to_cpu(buf);
+	*data_hi = (u32)(buf >> 32);
+	*data_lo = (u32) buf;
+}  /* sw_r_table_64 */
+
+/**
+ * sw_w_table_64 - write 64 bits of data to switch table
+ * @sw:		The switch instance.
+ * @table:	The table selector.
+ * @addr:	The address of the table entry.
+ * @data_hi:	The high part of data to be written (bit63 ~ bit32).
+ * @data_lo:	The low part of data to be written (bit31 ~ bit0).
+ *
+ * This routine writes 64 bits of data to the table of the switch.
+ * Hardware is locked to minimize corruption of written data.
+ */
+static void sw_w_table_64(struct ksz_sw *sw, int table, u16 addr, u32 data_hi,
+	u32 data_lo)
+{
+	u64 buf;
+	u16 ctrl_addr;
+
+	ctrl_addr = IND_ACC_TABLE(table) | addr;
+	buf = data_hi;
+	buf <<= 32;
+	buf |= data_lo;
+	buf = cpu_to_be64(buf);
+
+	sw->ops->acquire(sw);
+	sw->reg->w(sw, REG_IND_DATA_HI, &buf, sizeof(buf));
+	sw->reg->w16(sw, REG_IND_CTRL_0, ctrl_addr);
+	sw->ops->release(sw);
+}  /* sw_w_table_64 */
+
+/**
+ * sw_r_ext_table - read a byte of data from switch table
+ * @sw:		The switch instance.
+ * @table:	The table selector.
+ * @addr:	The address of the table entry.
+ * @offset:	The offset of the extended table.
+ * @data:	Buffer to store the read data.
+ *
+ * This routine reads a byte of data from the extended table of the switch.
+ * Hardware is locked to minimize corruption of read data.
+ */
+static void sw_r_ext_table(struct ksz_sw *sw, int table, u16 addr, u16 offset,
+	u8 *data)
+{
+	u16 ctrl_addr;
+
+	ctrl_addr = IND_ACC_TABLE(table | TABLE_READ | addr) | offset;
+
+	sw->reg->w16(sw, REG_IND_CTRL_0, ctrl_addr);
+	*data = sw->reg->r8(sw, REG_IND_DATA_PME_EEE_ACL);
+}  /* sw_r_ext_table */
+
+/**
+ * sw_g_ext_table - read bytes of data from switch table
+ * @sw:		The switch instance.
+ * @table:	The table selector.
+ * @addr:	The address of the table entry.
+ * @offset:	The offset of the extended table.
+ * @data:	Buffer to store the read data.
+ * @cnt:	Numbe of bytes to read.
+ *
+ * This routine reads several bytes of data from the extended table of the
+ * switch.
+ * Hardware is locked to minimize corruption of read data.
+ */
+static void sw_g_ext_table(struct ksz_sw *sw, int table, u16 addr, u16 offset,
+	u8 *data, int cnt)
+{
+	u16 ctrl_addr;
+	int i;
+
+	ctrl_addr = IND_ACC_TABLE(table | TABLE_READ | addr) | offset;
+
+	sw->reg->w16(sw, REG_IND_CTRL_0, ctrl_addr);
+	for (i = 0; i < cnt; i++)
+		data[i] = sw->reg->r8(sw, REG_IND_DATA_PME_EEE_ACL);
+}  /* sw_g_ext_table */
+
+/**
+ * sw_w_ext_table - write a byte of data to switch table
+ * @sw:		The switch instance.
+ * @table:	The table selector.
+ * @addr:	The address of the table entry.
+ * @offset:	The offset of the extended table.
+ * @data:	Data to be written.
+ *
+ * This routine writes a byte of data to the extended table of the switch.
+ * Hardware is locked to minimize corruption of written data.
+ */
+static void sw_w_ext_table(struct ksz_sw *sw, int table, u16 addr, u16 offset,
+	u8 data)
+{
+	u16 ctrl_addr;
+
+	ctrl_addr = IND_ACC_TABLE(table | addr) | offset;
+
+	sw->reg->w16(sw, REG_IND_CTRL_0, ctrl_addr);
+	sw->reg->w8(sw, REG_IND_DATA_PME_EEE_ACL, data);
+}  /* sw_w_ext_table */
+
+/**
+ * sw_p_ext_table - write bytes of data to switch table
+ * @sw:		The switch instance.
+ * @table:	The table selector.
+ * @addr:	The address of the table entry.
+ * @offset:	The offset of the extended table.
+ * @data:	Data to be written.
+ * @cnt:	Numbe of bytes to write.
+ *
+ * This routine writes several bytes of data to the extended table of the
+ * switch.
+ * Hardware is locked to minimize corruption of written data.
+ */
+static void sw_p_ext_table(struct ksz_sw *sw, int table, u16 addr, u16 offset,
+	u8 *data, int cnt)
+{
+	u16 ctrl_addr;
+	int i;
+
+	ctrl_addr = IND_ACC_TABLE(table | addr) | offset;
+
+	sw->reg->w16(sw, REG_IND_CTRL_0, ctrl_addr);
+	for (i = 0; i < cnt; i++)
+		sw->reg->w8(sw, REG_IND_DATA_PME_EEE_ACL, data[i]);
+}  /* sw_p_ext_table */
+
+static inline int valid_dyn_entry(struct ksz_sw *sw, u8 *data)
+{
+	int timeout = 100;
+
+	do {
+		*data = sw->reg->r8(sw, REG_IND_DATA_CHECK);
+		timeout--;
+	} while ((*data & DYNAMIC_MAC_TABLE_NOT_READY) && timeout);
+
+	/* Entry is not ready for accessing. */
+	if (*data & DYNAMIC_MAC_TABLE_NOT_READY)
+		return 1;
+
+	/* Entry is ready for accessing. */
+	else {
+		*data = sw->reg->r8(sw, REG_IND_DATA_8);
+
+		/* There is no valid entry in the table. */
+		if (*data & DYNAMIC_MAC_TABLE_MAC_EMPTY)
+			return 2;
+	}
+	return 0;
+}  /* valid_dyn_entry */
+
+/**
+ * sw_r_dyn_mac_table - read from dynamic MAC table
+ * @sw:		The switch instance.
+ * @addr:	The address of the table entry.
+ * @mac_addr:	Buffer to store the MAC address.
+ * @fid:	Buffer to store the FID.
+ * @src_port:	Buffer to store the source port number.
+ * @timestamp:	Buffer to store the timestamp.
+ * @entries:	Buffer to store the number of entries.  If this is zero, the
+ *		table is empty and so this function should not be called again
+ *		until later.
+ *
+ * This function reads an entry of the dynamic MAC table of the switch.
+ * Hardware is locked to minimize corruption of read data.
+ *
+ * Return 0 if the entry is successfully read; otherwise -1.
+ */
+static int sw_r_dyn_mac_table(struct ksz_sw *sw, u16 addr, u8 *mac_addr,
+	u8 *fid, u8 *src_port, u8 *timestamp, u16 *entries)
+{
+	u32 data_hi;
+	u32 data_lo;
+	u16 ctrl_addr;
+	int rc;
+	u8 data;
+
+	ctrl_addr = IND_ACC_TABLE(TABLE_DYNAMIC_MAC | TABLE_READ) | addr;
+
+	sw->ops->acquire(sw);
+	sw->reg->w16(sw, REG_IND_CTRL_0, ctrl_addr);
+
+	rc = valid_dyn_entry(sw, &data);
+	if (1 == rc) {
+		if (0 == addr)
+			*entries = 0;
+	} else if (2 == rc)
+		*entries = 0;
+	/* At least one valid entry in the table. */
+	else {
+		u64 buf;
+
+		sw->reg->r(sw, REG_IND_DATA_HI, &buf, sizeof(buf));
+		buf = be64_to_cpu(buf);
+		data_hi = (u32)(buf >> 32);
+		data_lo = (u32) buf;
+
+		/* Check out how many valid entry in the table. */
+		*entries = (u16)(((((u16)
+			data & DYNAMIC_MAC_TABLE_ENTRIES_H) <<
+			DYNAMIC_MAC_ENTRIES_H_S) |
+			(((data_hi & DYNAMIC_MAC_TABLE_ENTRIES) >>
+			DYNAMIC_MAC_ENTRIES_S))) + 1);
+
+		*fid = (u8)((data_hi & DYNAMIC_MAC_TABLE_FID) >>
+			DYNAMIC_MAC_FID_S);
+		*src_port = (u8)((data_hi & DYNAMIC_MAC_TABLE_SRC_PORT) >>
+			DYNAMIC_MAC_SRC_PORT_S);
+		*timestamp = (u8)((
+			data_hi & DYNAMIC_MAC_TABLE_TIMESTAMP) >>
+			DYNAMIC_MAC_TIMESTAMP_S);
+
+		mac_addr[5] = (u8) data_lo;
+		mac_addr[4] = (u8)(data_lo >> 8);
+		mac_addr[3] = (u8)(data_lo >> 16);
+		mac_addr[2] = (u8)(data_lo >> 24);
+
+		mac_addr[1] = (u8) data_hi;
+		mac_addr[0] = (u8)(data_hi >> 8);
+		rc = 0;
+	}
+	sw->ops->release(sw);
+
+	return rc;
+}  /* sw_r_dyn_mac_table */
+
+/**
+ * sw_d_dyn_mac_table - dump dynamic MAC table
+ * @sw:		The switch instance.
+ *
+ * This routine dumps dynamic MAC table contents.
+ */
+static ssize_t sw_d_dyn_mac_table(struct ksz_sw *sw, char *buf, ssize_t len)
+{
+	u16 entries = 0;
+	u16 i;
+	u8 mac_addr[ETH_ALEN];
+	u8 port = 0;
+	u8 timestamp = 0;
+	u8 fid = 0;
+	int first_break = true;
+
+	memset(mac_addr, 0, ETH_ALEN);
+	i = 0;
+	do {
+		if (!sw_r_dyn_mac_table(sw, i, mac_addr, &fid, &port,
+				&timestamp, &entries)) {
+			port = get_log_port_zero(sw, port);
+			if (len >= MAX_SYSFS_BUF_SIZE && first_break) {
+				first_break = false;
+				len += sprintf(buf + len, "...\n");
+			}
+			if (len < MAX_SYSFS_BUF_SIZE)
+			len += sprintf(buf + len,
+				"%02X:%02X:%02X:%02X:%02X:%02X  "
+				"f:%02x  p:%x  t:%x\n",
+				mac_addr[0], mac_addr[1], mac_addr[2],
+				mac_addr[3], mac_addr[4], mac_addr[5],
+				fid, port, timestamp);
+			else
+			printk(KERN_INFO
+				"%02X:%02X:%02X:%02X:%02X:%02X  "
+				"f:%02x  p:%x  t:%x\n",
+				mac_addr[0], mac_addr[1], mac_addr[2],
+				mac_addr[3], mac_addr[4], mac_addr[5],
+				fid, port, timestamp);
+		}
+		i++;
+	} while (i < entries);
+	if (entries) {
+		if (len < MAX_SYSFS_BUF_SIZE)
+			sprintf(buf + len, "=%03x\n", entries);
+		else
+			printk(KERN_INFO "=%03x\n", entries);
+	}
+	return len;
+}  /* sw_d_dyn_mac_table */
+
+/**
+ * sw_r_sta_mac_table - read from static MAC table
+ * @sw:		The switch instance.
+ * @addr:	The address of the table entry.
+ * @mac:	Buffer to store the MAC table entry.
+ *
+ * This function reads an entry of the static MAC table of the switch.  It
+ * calls sw_r_table_64() to get the data.
+ *
+ * Return 0 if the entry is valid; otherwise -1.
+ */
+static int sw_r_sta_mac_table(struct ksz_sw *sw, u16 addr,
+	struct ksz_mac_table *mac)
+{
+	u32 data_hi;
+	u32 data_lo;
+
+	sw_r_table_64(sw, TABLE_STATIC_MAC, addr, &data_hi, &data_lo);
+	if (data_hi & (STATIC_MAC_TABLE_VALID | STATIC_MAC_TABLE_OVERRIDE)) {
+		mac->addr[5] = (u8) data_lo;
+		mac->addr[4] = (u8)(data_lo >> 8);
+		mac->addr[3] = (u8)(data_lo >> 16);
+		mac->addr[2] = (u8)(data_lo >> 24);
+		mac->addr[1] = (u8) data_hi;
+		mac->addr[0] = (u8)(data_hi >> 8);
+		mac->ports = (u8)((data_hi & STATIC_MAC_TABLE_FWD_PORTS) >>
+			STATIC_MAC_FWD_PORTS_S);
+		mac->override = (data_hi & STATIC_MAC_TABLE_OVERRIDE) ? 1 : 0;
+		data_hi >>= 1;
+		mac->use_fid = (data_hi & STATIC_MAC_TABLE_USE_FID) ? 1 : 0;
+		mac->fid = (u8)((data_hi & STATIC_MAC_TABLE_FID) >>
+			STATIC_MAC_FID_S);
+		mac->dirty = 0;
+		return 0;
+	}
+	return -1;
+}  /* sw_r_sta_mac_table */
+
+/**
+ * sw_w_sta_mac_table - write to static MAC table
+ * @sw:		The switch instance.
+ * @addr:	The address of the table entry.
+ * @mac:	The MAC table entry.
+ *
+ * This routine writes an entry of the static MAC table of the switch.  It
+ * calls sw_w_table_64() to write the data.
+ */
+static void sw_w_sta_mac_table(struct ksz_sw *sw, u16 addr,
+	struct ksz_mac_table *mac)
+{
+	u32 data_hi;
+	u32 data_lo;
+
+	data_lo = ((u32) mac->addr[2] << 24) |
+		((u32) mac->addr[3] << 16) |
+		((u32) mac->addr[4] << 8) | mac->addr[5];
+	data_hi = ((u32) mac->addr[0] << 8) | mac->addr[1];
+	data_hi |= (u32) mac->ports << STATIC_MAC_FWD_PORTS_S;
+
+	if (mac->override)
+		data_hi |= STATIC_MAC_TABLE_OVERRIDE;
+	if (mac->use_fid) {
+		data_hi |= STATIC_MAC_TABLE_USE_FID;
+		data_hi |= (u32) mac->fid << STATIC_MAC_FID_S;
+	}
+	if (mac->valid)
+		data_hi |= STATIC_MAC_TABLE_VALID;
+	else
+		data_hi &= ~STATIC_MAC_TABLE_OVERRIDE;
+
+	sw_w_table_64(sw, TABLE_STATIC_MAC, addr, data_hi, data_lo);
+	mac->dirty = 0;
+}  /* sw_w_sta_mac_table */
+
+/**
+ * sw_d_sta_mac_table - dump static MAC table
+ * @sw:		The switch instance.
+ *
+ * This routine dumps static MAC table contents.
+ */
+static ssize_t sw_d_sta_mac_table(struct ksz_sw *sw, char *buf, ssize_t len)
+{
+	u8 ports;
+	u16 i;
+	struct ksz_mac_table mac;
+
+	i = 0;
+	do {
+		if (!sw_r_sta_mac_table(sw, i, &mac)) {
+			ports = mac.ports;
+			ports = get_log_mask_from_phy(sw, ports);
+			len += sprintf(buf + len,
+				"%2x: %02X:%02X:%02X:%02X:%02X:%02X  "
+				"%02x  %u  %u:%02x\n",
+				i, mac.addr[0], mac.addr[1], mac.addr[2],
+				mac.addr[3], mac.addr[4], mac.addr[5],
+				ports, mac.override, mac.use_fid, mac.fid);
+		}
+		i++;
+	} while (i < STATIC_MAC_TABLE_ENTRIES);
+	return len;
+}  /* sw_d_sta_mac_table */
+
+#ifdef DEBUG
+static ssize_t sw_d_mac_table(struct ksz_sw *sw, char *buf, ssize_t len)
+{
+#if 0
+	struct ksz_mac_table *entry;
+	int i;
+	u8 ports;
+
+	i = STATIC_MAC_TABLE_ENTRIES;
+	do {
+		entry = &sw->info->mac_table[i];
+		if (entry->valid) {
+			ports = entry->ports;
+			ports = get_log_mask_from_phy(sw, ports);
+			len += sprintf(buf + len,
+				"%x: %02X:%02X:%02X:%02X:%02X:%02X  "
+				"%x  %u  %u:%x\n",
+				i, entry->mac_addr[0], entry->mac_addr[1],
+				entry->mac_addr[2], entry->mac_addr[3],
+				entry->mac_addr[4], entry->mac_addr[5],
+				ports, entry->override, entry->use_fid,
+				entry->fid);
+		}
+		i++;
+		if (SWITCH_MAC_TABLE_ENTRIES == i)
+			printk(KERN_INFO "\n");
+	} while (i < MULTI_MAC_TABLE_ENTRIES);
+#endif
+	return len;
+}  /* sw_d_mac_table */
+#endif
+
+/* -------------------------------------------------------------------------- */
+
+/**
+ * sw_r_vlan_entries - read many from VLAN table
+ * @sw:		The switch instance.
+ * @addr:	The address of the table entry.
+ * @valid:	Buffer to store the valid bit.
+ * @fid:	Buffer to store the VID.
+ * @member:	Buffer to store the port membership.
+ *
+ * This function reads several entries of the VLAN table of the switch.  It
+ * calls sw_r_table_64() to get the data.
+ *
+ * Return 0 if the entry is valid; otherwise -1.
+ */
+static int sw_r_vlan_entries(struct ksz_sw *sw, u16 addr, u8 *valid, u8 *fid,
+	u8 *member)
+{
+	u64 buf;
+	u32 *data_lo = (u32 *) &buf;
+	u32 *data_hi = data_lo + 1;
+	u16 data;
+	int i;
+	int ret = -1;
+
+	sw_r_table_64(sw, TABLE_VLAN, addr, data_hi, data_lo);
+	for (i = 0; i < 4; i++) {
+		data = (u16) buf;
+		*valid = 0;
+		if (data & VLAN_TABLE_VALID) {
+			*valid = 1;
+			*fid = (u8)(data & VLAN_TABLE_FID);
+			*member = (u8)((data & VLAN_TABLE_MEMBERSHIP) >>
+				VLAN_TABLE_MEMBERSHIP_S);
+			ret = 0;
+		}
+		++valid;
+		++fid;
+		++member;
+		buf >>= VLAN_TABLE_S;
+	}
+	return ret;
+}  /* sw_r_vlan_entries */
+
+/**
+ * sw_r_vlan_table - read from VLAN table
+ * @sw:		The switch instance.
+ * @vid:	The address of the table entry.
+ * @vlan:	Buffer to store the VLAN table entry.
+ *
+ * This routine reads an entry of the VLAN table of the switch.  It calls
+ * sw_r_table_64() to get the data.
+ */
+static void sw_r_vlan_table(struct ksz_sw *sw, u16 vid,
+	struct ksz_vlan_table *vlan)
+{
+	u64 buf;
+	u32 *data_lo = (u32 *) &buf;
+	u32 *data_hi = data_lo + 1;
+	u16 *data = (u16 *) &buf;
+	u16 addr;
+	int index;
+
+	addr = vid / 4;
+	index = vid & 3;
+	sw_r_table_64(sw, TABLE_VLAN, addr, data_hi, data_lo);
+	vlan->fid = (u8)(data[index] & VLAN_TABLE_FID);
+	vlan->member = (u8)((data[index] & VLAN_TABLE_MEMBERSHIP) >>
+		VLAN_TABLE_MEMBERSHIP_S);
+	vlan->valid = !!(data[index] & VLAN_TABLE_VALID);
+	vlan->dirty = 0;
+}  /* sw_r_vlan_table */
+
+/**
+ * sw_w_vlan_table - write to VLAN table
+ * @sw:		The switch instance.
+ * @vid:	The address of the table entry.
+ * @vlan:	The VLAN table entry.
+ *
+ * This routine writes an entry of the VLAN table of the switch.  It calls
+ * sw_w_table_64() to write the data.
+ */
+static void sw_w_vlan_table(struct ksz_sw *sw, u16 vid,
+	struct ksz_vlan_table *vlan)
+{
+	u64 buf;
+	u32 *data_lo = (u32 *) &buf;
+	u32 *data_hi = data_lo + 1;
+	u16 *data = (u16 *) &buf;
+	u16 addr;
+	int index;
+
+	addr = vid / 4;
+	index = vid & 3;
+	sw_r_table_64(sw, TABLE_VLAN, addr, data_hi, data_lo);
+	data[index] = vlan->fid;
+	data[index] |= (u16) vlan->member << VLAN_TABLE_MEMBERSHIP_S;
+	if (vlan->valid)
+		data[index] |= VLAN_TABLE_VALID;
+	sw_w_table_64(sw, TABLE_VLAN, addr, *data_hi, *data_lo);
+	vlan->dirty = 0;
+}  /* sw_w_vlan_table */
+
+/**
+ * sw_d_vlan_table - dump VLAN table
+ * @sw:		The switch instance.
+ *
+ * This routine dumps the VLAN table.
+ */
+static ssize_t sw_d_vlan_table(struct ksz_sw *sw, char *buf, ssize_t len)
+{
+	u16 i;
+	u16 j;
+	u16 vid;
+	u8 ports;
+	u8 fid[4];
+	u8 member[4];
+	u8 valid[4];
+	int first_break = true;
+
+	i = 0;
+	do {
+		if (!sw_r_vlan_entries(sw, i, valid, fid, member)) {
+			vid = i * 4;
+			for (j = 0; j < 4; j++, vid++) {
+				if (!valid[j])
+					continue;
+				ports = member[j];
+				ports = get_log_mask_from_phy(sw, member[j]);
+				if (len >= MAX_SYSFS_BUF_SIZE && first_break) {
+					first_break = false;
+					len += sprintf(buf + len, "...\n");
+				}
+				if (len < MAX_SYSFS_BUF_SIZE)
+					len += sprintf(buf + len,
+						"0x%03x: %2x  %2x\n", vid,
+						fid[j], ports);
+				else
+					printk(KERN_INFO
+						"0x%03x: %2x  %2x\n", vid,
+						fid[j], ports);
+			}
+		}
+		if (len >= MAX_SYSFS_BUF_SIZE)
+		yield();
+		i++;
+	} while (i < VLAN_TABLE_ENTRIES);
+	return len;
+}  /* sw_d_vlan_table */
+
+/* -------------------------------------------------------------------------- */
+
+/*
+ * Some counters do not need to be read too often because they are less likely
+ * to increase much.
+ */
+static u8 mib_read_max[TOTAL_SWITCH_COUNTER_NUM] = {
+	1,
+	4,
+	4,
+	4,
+	4,
+	4,
+	4,
+	4,
+	4,
+	1,
+	1,
+	1,
+	1,
+	2,
+	2,
+	2,
+	2,
+	2,
+	2,
+	2,
+	2,
+
+	1,
+	4,
+	1,
+	1,
+	1,
+	1,
+	4,
+	4,
+	4,
+	4,
+	4,
+
+	1,
+	1,
+	2,
+	2,
+};
+
+/**
+ * port_r_mib_cnt - read MIB counter
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ * @addr:	The address of the counter.
+ * @cnt:	Buffer to store the counter.
+ *
+ * This routine reads a MIB counter of the port.
+ * Hardware is locked to minimize corruption of read data.
+ */
+static void port_r_mib_cnt(struct ksz_sw *sw, uint port, u16 addr, u64 *cnt)
+{
+	u32 data;
+	u16 ctrl_addr;
+	u8 check;
+	int timeout;
+
+	ctrl_addr = addr + SWITCH_COUNTER_NUM * port;
+
+	sw->ops->acquire(sw);
+
+	ctrl_addr |= IND_ACC_TABLE(TABLE_MIB | TABLE_READ);
+	sw->reg->w16(sw, REG_IND_CTRL_0, ctrl_addr);
+
+	for (timeout = 2; timeout > 0; timeout--) {
+		check = sw->reg->r8(sw, REG_IND_MIB_CHECK);
+
+		if (check & MIB_COUNTER_VALID) {
+			data = sw->reg->r32(sw, REG_IND_DATA_LO);
+			if (check & MIB_COUNTER_OVERFLOW)
+				*cnt += MIB_COUNTER_VALUE + 1;
+			*cnt += data & MIB_COUNTER_VALUE;
+			break;
+		}
+	}
+
+	sw->ops->release(sw);
+}  /* port_r_mib_cnt */
+
+/**
+ * port_r_mib_pkt - read dropped packet counts
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ * @cnt:	Buffer to store the receive and transmit dropped packet counts.
+ *
+ * This routine reads the dropped packet counts of the port.
+ * Hardware is locked to minimize corruption of read data.
+ */
+static void port_r_mib_pkt(struct ksz_sw *sw, uint port, u16 addr, u64 *cnt)
+{
+	u32 data;
+	u16 ctrl_addr;
+	u8 check;
+	int timeout;
+
+	addr -= SWITCH_COUNTER_NUM;
+	ctrl_addr = (KS_MIB_TOTAL_RX_1 - KS_MIB_TOTAL_RX_0) * port;
+	ctrl_addr += addr + KS_MIB_TOTAL_RX_0;
+
+	sw->ops->acquire(sw);
+
+	ctrl_addr |= IND_ACC_TABLE(TABLE_MIB | TABLE_READ);
+	sw->reg->w16(sw, REG_IND_CTRL_0, ctrl_addr);
+
+	for (timeout = 2; timeout > 0; timeout--) {
+		check = sw->reg->r8(sw, REG_IND_MIB_CHECK);
+
+		if (check & MIB_COUNTER_VALID) {
+			data = sw->reg->r32(sw, REG_IND_DATA_LO);
+			if (addr < 2) {
+				u64 total;
+
+				total = check & MIB_TOTAL_BYTES_H;
+				total <<= 32;
+				*cnt += total;
+				*cnt += data;
+				if (check & MIB_COUNTER_OVERFLOW) {
+					total = MIB_TOTAL_BYTES_H + 1;
+					total <<= 32;
+					*cnt += total;
+				}
+			} else {
+				if (check & MIB_COUNTER_OVERFLOW)
+					*cnt += MIB_PACKET_DROPPED + 1;
+				*cnt += data & MIB_PACKET_DROPPED;
+			}
+			break;
+		}
+	}
+
+	sw->ops->release(sw);
+}  /* port_r_mib_pkt */
+
+static int exit_mib_read(struct ksz_sw *sw)
+{
+	if (sw->intr_using)
+		return true;
+	return false;
+}  /* exit_mib_read */
+
+/**
+ * port_r_cnt - read MIB counters periodically
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ *
+ * This routine is used to read the counters of the port periodically to avoid
+ * counter overflow.  The hardware should be acquired first before calling this
+ * routine.
+ *
+ * Return non-zero when not all counters not read.
+ */
+static int port_r_cnt(struct ksz_sw *sw, uint port)
+{
+	struct ksz_port_mib *mib = get_port_mib(sw, port);
+
+	if (mib->mib_start < SWITCH_COUNTER_NUM)
+		while (mib->cnt_ptr < SWITCH_COUNTER_NUM) {
+			if (exit_mib_read(sw))
+				return mib->cnt_ptr;
+			++mib->read_cnt[mib->cnt_ptr];
+			if (mib->read_cnt[mib->cnt_ptr] >=
+					mib->read_max[mib->cnt_ptr]) {
+				mib->read_cnt[mib->cnt_ptr] = 0;
+				port_r_mib_cnt(sw, port, mib->cnt_ptr,
+					&mib->counter[mib->cnt_ptr]);
+			}
+			++mib->cnt_ptr;
+		}
+	if (sw->mib_cnt > SWITCH_COUNTER_NUM)
+		while (mib->cnt_ptr < TOTAL_SWITCH_COUNTER_NUM) {
+			++mib->read_cnt[mib->cnt_ptr];
+			if (mib->read_cnt[mib->cnt_ptr] >=
+					mib->read_max[mib->cnt_ptr]) {
+				mib->read_cnt[mib->cnt_ptr] = 0;
+				port_r_mib_pkt(sw, port, mib->cnt_ptr,
+					&mib->counter[mib->cnt_ptr]);
+			}
+			++mib->cnt_ptr;
+		}
+	mib->cnt_ptr = 0;
+	return 0;
+}  /* port_r_cnt */
+
+/**
+ * sw_cfg_mib_counter_ctrl - configure MIB counter control
+ * @sw:		The switch instance.
+ * @ctrl:	The control.
+ * @port:	The port index.
+ *
+ * This routine configures the MIB counter control for flush or freeze.
+ */
+static void sw_cfg_mib_counter_ctrl(struct ksz_sw *sw, int ctrl, uint port)
+{
+	uint count;
+	uint start;
+	uint stop;
+	u8 data;
+
+	if (port < sw->port_cnt) {
+		start = get_log_port(sw, port);
+		stop = start + 1;
+		data = (1 << port);
+	} else {
+		start = 0;
+		stop = sw->mib_port_cnt + 1;
+		data = sw->PORT_MASK;
+	}
+	if (ctrl & 4)
+		data |= SW_MIB_COUNTER_FLUSH;
+	if (ctrl & 2)
+		data |= SW_MIB_COUNTER_FREEZE;
+	SW_W(sw, REG_SW_CTRL_6, data);
+	if (!(ctrl & 3))
+		for (count = start; count < stop; count++) {
+			struct ksz_port_mib *mib;
+
+			port = get_phy_port(sw, count);
+			mib = get_port_mib(sw, port);
+			memset((void *) mib->counter, 0, sizeof(u64) *
+				TOTAL_SWITCH_COUNTER_NUM);
+			mib->rate[0].last = mib->rate[1].last = 0;
+			mib->rate[0].last_cnt = mib->rate[1].last_cnt = 0;
+			mib->rate[0].peak = mib->rate[1].peak = 0;
+		}
+}  /* sw_cfg_mib_counter_ctrl */
+
+/**
+ * port_init_cnt - initialize MIB counter values
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ *
+ * This routine is used to initialize all counters to zero if the hardware
+ * cannot do it after reset.
+ */
+static inline void port_init_cnt(struct ksz_sw *sw, uint port)
+{
+	struct ksz_port_mib *mib = get_port_mib(sw, port);
+
+	mib->cnt_ptr = 0;
+	if (mib->mib_start < SWITCH_COUNTER_NUM)
+		do {
+			mib->read_cnt[mib->cnt_ptr] = 0;
+			mib->read_max[mib->cnt_ptr] =
+				mib_read_max[mib->cnt_ptr];
+			port_r_mib_cnt(sw, port, mib->cnt_ptr,
+				&mib->counter[mib->cnt_ptr]);
+			++mib->cnt_ptr;
+		} while (mib->cnt_ptr < SWITCH_COUNTER_NUM);
+	if (sw->mib_cnt > SWITCH_COUNTER_NUM)
+		do {
+			mib->read_cnt[mib->cnt_ptr] = 0;
+			mib->read_max[mib->cnt_ptr] =
+				mib_read_max[mib->cnt_ptr];
+			port_r_mib_pkt(sw, port, mib->cnt_ptr,
+				&mib->counter[mib->cnt_ptr]);
+			++mib->cnt_ptr;
+		} while (mib->cnt_ptr < TOTAL_SWITCH_COUNTER_NUM);
+	memset((void *) mib->counter, 0, sizeof(u64) *
+		TOTAL_SWITCH_COUNTER_NUM);
+	mib->cnt_ptr = 0;
+	mib->rate[0].last = mib->rate[1].last = 0;
+	mib->rate[0].last_cnt = mib->rate[1].last_cnt = 0;
+	mib->rate[0].peak = mib->rate[1].peak = 0;
+}  /* port_init_cnt */
+
+/* -------------------------------------------------------------------------- */
+
+/*
+ * Port functions
+ */
+
+/**
+ * port_chk - check port register bits
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ * @offset:	The offset of the port register.
+ * @bits:	The data bits to check.
+ *
+ * This function checks whether the specified bits of the port register are set
+ * or not.
+ *
+ * Return 0 if the bits are not set.
+ */
+static int port_chk(struct ksz_sw *sw, uint port, uint offset, SW_D bits)
+{
+	u32 addr;
+	SW_D data;
+
+	PORT_CTRL_ADDR(port, addr);
+	addr += offset;
+	data = SW_R(sw, addr);
+	return (data & bits) == bits;
+}  /* port_chk */
+
+/**
+ * port_cfg - set port register bits
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ * @offset:	The offset of the port register.
+ * @bits:	The data bits to set.
+ * @set:	The flag indicating whether the bits are to be set or not.
+ *
+ * This routine sets or resets the specified bits of the port register.
+ */
+static void port_cfg(struct ksz_sw *sw, uint port, uint offset, SW_D bits,
+	bool set)
+{
+	u32 addr;
+	SW_D data;
+
+	PORT_CTRL_ADDR(port, addr);
+	addr += offset;
+	data = SW_R(sw, addr);
+	if (set)
+		data |= bits;
+	else
+		data &= ~bits;
+	SW_W(sw, addr, data);
+}  /* port_cfg */
+
+/**
+ * port_r8 - read byte from port register
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ * @offset:	The offset of the port register.
+ * @data:	Buffer to store the data.
+ *
+ * This routine reads a byte from the port register.
+ */
+static void port_r8(struct ksz_sw *sw, uint port, uint offset, u8 *data)
+{
+	u32 addr;
+
+	PORT_CTRL_ADDR(port, addr);
+	addr += offset;
+	*data = sw->reg->r8(sw, addr);
+}  /* port_r8 */
+
+/**
+ * port_w8 - write byte to port register
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ * @offset:	The offset of the port register.
+ * @data:	Data to write.
+ *
+ * This routine writes a byte to the port register.
+ */
+static void port_w8(struct ksz_sw *sw, uint port, uint offset, u8 data)
+{
+	u32 addr;
+
+	PORT_CTRL_ADDR(port, addr);
+	addr += offset;
+	sw->reg->w8(sw, addr, data);
+}  /* port_w8 */
+
+/**
+ * port_r16 - read word from port register.
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ * @offset:	The offset of the port register.
+ * @data:	Buffer to store the data.
+ *
+ * This routine reads a word from the port register.
+ */
+static void port_r16(struct ksz_sw *sw, uint port, uint offset, u16 *data)
+{
+	u32 addr;
+
+	PORT_CTRL_ADDR(port, addr);
+	addr += offset;
+	*data = sw->reg->r16(sw, addr);
+}  /* port_r16 */
+
+/**
+ * port_w16 - write word to port register.
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ * @offset:	The offset of the port register.
+ * @data:	Data to write.
+ *
+ * This routine writes a word to the port register.
+ */
+static void port_w16(struct ksz_sw *sw, uint port, uint offset, u16 data)
+{
+	u32 addr;
+
+	PORT_CTRL_ADDR(port, addr);
+	addr += offset;
+	sw->reg->w16(sw, addr, data);
+}  /* port_w16 */
+
+/**
+ * port_r_s - read bits with shift from port register.
+ * @sw:		The switch instance.
+ * @p:		The port index.
+ * @reg:	The port register.
+ * @mask:	The mask to apply.
+ * @shift:	The shift to use.
+ *
+ * This function reads bits from the port register.
+ */
+static u8 port_r_s(struct ksz_sw *sw, uint p, u32 reg, u8 mask, u8 shift)
+{
+	u8 data;
+
+	port_r8(sw, p, reg, &data);
+	data >>= shift;
+	data &= mask;
+	return data;
+}  /* port_r_s */
+
+/**
+ * port_w_s - write bits with shift to port register.
+ * @sw:		The switch instance.
+ * @p:		The port index.
+ * @reg:	The port register.
+ * @mask:	The mask to apply.
+ * @shift:	The shift to use.
+ *
+ * This routine writes bits to the port register.
+ */
+static void port_w_s(struct ksz_sw *sw, uint p, u32 reg, u8 mask, u8 shift,
+	u8 val)
+{
+	u8 data;
+
+	port_r8(sw, p, reg, &data);
+	data &= ~(mask << shift);
+	data |= (val & mask) << shift;
+	port_w8(sw, p, reg, data);
+}  /* port_w_s */
+
+/**
+ * sw_chk - check switch register bits
+ * @sw:		The switch instance.
+ * @addr:	The address of the switch register.
+ * @bits:	The data bits to check.
+ *
+ * This function checks whether the specified bits of the switch register are
+ * set or not.
+ *
+ * Return 0 if the bits are not set.
+ */
+static int sw_chk(struct ksz_sw *sw, u32 addr, SW_D bits)
+{
+	SW_D data;
+
+	data = SW_R(sw, addr);
+	return (data & bits) == bits;
+}  /* sw_chk */
+
+/**
+ * sw_cfg - set switch register bits
+ * @sw:		The switch instance.
+ * @addr:	The address of the switch register.
+ * @bits:	The data bits to set.
+ * @set:	The flag indicating whether the bits are to be set or not.
+ *
+ * This function sets or resets the specified bits of the switch register.
+ */
+static void sw_cfg(struct ksz_sw *sw, u32 addr, SW_D bits, bool set)
+{
+	SW_D data;
+
+	data = SW_R(sw, addr);
+	if (set)
+		data |= bits;
+	else
+		data &= ~bits;
+	SW_W(sw, addr, data);
+}  /* sw_cfg */
+
+/* -------------------------------------------------------------------------- */
+
+/* ACL */
+
+static inline void port_cfg_acl(struct ksz_sw *sw, uint p, bool set)
+{
+	port_cfg(sw, p,
+		REG_PORT_CTRL_5, PORT_ACL_ENABLE, set);
+}
+
+static inline int port_chk_acl(struct ksz_sw *sw, uint p)
+{
+	return port_chk(sw, p,
+		REG_PORT_CTRL_5, PORT_ACL_ENABLE);
+}
+
+static inline u8 port_get_authen_mode(struct ksz_sw *sw, uint p)
+{
+	return port_r_s(sw, p,
+		REG_PORT_CTRL_5, PORT_AUTHEN_MODE, 0);
+}
+
+static void port_set_authen_mode(struct ksz_sw *sw, uint p, u8 mode)
+{
+	port_w_s(sw, p,
+		REG_PORT_CTRL_5, PORT_AUTHEN_MODE, 0, mode);
+}
+
+/**
+ * get_acl_action_info - Get ACL action field information
+ * @acl:	The ACL entry.
+ * @data:	The ACL data.
+ *
+ * This helper routine gets ACL action field information.
+ */
+static void get_acl_action_info(struct ksz_acl_table *acl, u8 data[])
+{
+	acl->prio_mode = (data[10] >> ACL_PRIO_MODE_S) & ACL_PRIO_MODE_M;
+	acl->prio = (data[10] >> ACL_PRIO_S) & ACL_PRIO_M;
+	acl->vlan_prio_replace = !!(data[10] & ACL_VLAN_PRIO_REPLACE);
+	acl->vlan_prio = data[11] >> ACL_VLAN_PRIO_S;
+	acl->vlan_prio |= (data[10] & ACL_VLAN_PRIO_HI_M) << 1;
+	acl->map_mode = (data[11] >> ACL_MAP_MODE_S) & ACL_MAP_MODE_M;
+	acl->ports = data[11] & ACL_MAP_PORT_M;
+}  /* get_acl_action_info */
+
+/**
+ * get_acl_table_info - Get ACL table information
+ * @acl:	The ACL entry.
+ * @data:	The ACL data.
+ *
+ * This helper routine gets ACL table information.
+ */
+static void get_acl_table_info(struct ksz_acl_table *acl, u8 data[])
+{
+	u16 *ptr_16;
+	u32 *ptr_32;
+	int i;
+	int j;
+	int cnt = 0;
+
+	acl->first_rule = data[0] & ACL_FIRST_RULE_M;
+	acl->mode = (data[1] >> ACL_MODE_S) & ACL_MODE_M;
+	acl->enable = (data[1] >> ACL_ENABLE_S) & ACL_ENABLE_M;
+	acl->src = !!(data[1] & ACL_SRC);
+	acl->equal = !!(data[1] & ACL_EQUAL);
+	switch (acl->mode) {
+	case ACL_MODE_LAYER_2:
+		for (i = 0; i < 6; i++)
+			acl->mac[i] = data[2 + i];
+		ptr_16 = (u16 *) &data[8];
+		acl->eth_type = be16_to_cpu(*ptr_16);
+		if (ACL_ENABLE_2_COUNT == acl->enable) {
+			cnt = 1;
+			ptr_16 = (u16 *) &data[10];
+			acl->cnt = (be16_to_cpu(*ptr_16) >> ACL_CNT_S) &
+				ACL_CNT_M;
+			acl->msec =
+				!!(data[ACL_INTR_CNT_START] & ACL_MSEC_UNIT);
+			acl->intr_mode =
+				!!(data[ACL_INTR_CNT_START] & ACL_INTR_MODE);
+		}
+		break;
+	case ACL_MODE_LAYER_3:
+		j = 2;
+		for (i = 0; i < 4; i++, j++)
+			acl->ip4_addr[i] = data[j];
+		for (i = 0; i < 4; i++, j++)
+			acl->ip4_mask[i] = data[j];
+		break;
+	case ACL_MODE_LAYER_4:
+		switch (acl->enable) {
+		case ACL_ENABLE_4_TCP_SEQN_COMP:
+			ptr_32 = (u32 *) &data[2];
+			acl->seqnum = be32_to_cpu(*ptr_32);
+			break;
+		default:
+			ptr_16 = (u16 *) &data[2];
+			acl->max_port = be16_to_cpu(*ptr_16);
+			++ptr_16;
+			acl->min_port = be16_to_cpu(*ptr_16);
+			acl->port_mode = (data[6] >> ACL_PORT_MODE_S) &
+				ACL_PORT_MODE_M;
+		}
+		acl->protocol = (data[6] & 1) << 7;
+		acl->protocol |= (data[7] >> 1);
+		acl->tcp_flag_enable = !!(data[7] & ACL_TCP_FLAG_ENABLE);
+		acl->tcp_flag_mask = data[8];
+		acl->tcp_flag = data[9];
+		break;
+	default:
+		break;
+	}
+	if (!cnt)
+		get_acl_action_info(acl, data);
+	ptr_16 = (u16 *) &data[ACL_RULESET_START];
+	acl->ruleset = be16_to_cpu(*ptr_16);
+}  /* get_acl_table_info */
+
+/**
+ * set_acl_action_info - Set ACL action field information
+ * @acl:	The ACL entry.
+ * @data:	The ACL data.
+ *
+ * This helper routine sets ACL action field information.
+ */
+static void set_acl_action_info(struct ksz_acl_table *acl, u8 data[])
+{
+	if (acl->data[0] != 0xff)
+		memcpy(data, acl->data, ACL_TABLE_LEN);
+	data[10] = (acl->prio_mode & ACL_PRIO_MODE_M) << ACL_PRIO_MODE_S;
+	data[10] |= (acl->prio & ACL_PRIO_M) << ACL_PRIO_S;
+	if (acl->vlan_prio_replace)
+		data[10] |= ACL_VLAN_PRIO_REPLACE;
+	data[10] |= (acl->vlan_prio >> 1);
+	data[11] = acl->vlan_prio << ACL_VLAN_PRIO_S;
+	data[11] |= (acl->map_mode & ACL_MAP_MODE_M) << ACL_MAP_MODE_S;
+	data[11] |= acl->ports & ACL_MAP_PORT_M;
+}  /* set_acl_action_info */
+
+/**
+ * set_acl_ruleset_info - Set ACL ruleset field information
+ * @acl:	The ACL entry.
+ * @data:	The ACL data.
+ *
+ * This helper routine sets ACL ruleset field information.
+ */
+static void set_acl_ruleset_info(struct ksz_acl_table *acl, u8 data[])
+{
+	u16 *ptr_16;
+
+	if (acl->data[0] != 0xff)
+		memcpy(data, acl->data, ACL_TABLE_LEN);
+	data[0] = acl->first_rule & ACL_FIRST_RULE_M;
+	ptr_16 = (u16 *) &data[ACL_RULESET_START];
+	*ptr_16 = cpu_to_be16(acl->ruleset);
+}  /* set_acl_ruleset_info */
+
+/**
+ * set_acl_table_info - Set ACL table information
+ * @acl:	The ACL entry.
+ * @data:	The ACL data.
+ *
+ * This helper routine sets ACL table information.
+ */
+static void set_acl_table_info(struct ksz_acl_table *acl, u8 data[])
+{
+	u16 *ptr_16;
+	u32 *ptr_32;
+	int i;
+	int j;
+
+	if (acl->data[0] != 0xff)
+		memcpy(data, acl->data, ACL_TABLE_LEN);
+	data[1] = (acl->mode & ACL_MODE_M) << ACL_MODE_S;
+	data[1] |= (acl->enable & ACL_ENABLE_M) << ACL_ENABLE_S;
+	if (acl->src)
+		data[1] |= ACL_SRC;
+	if (acl->equal)
+		data[1] |= ACL_EQUAL;
+	switch (acl->mode) {
+	case ACL_MODE_LAYER_2:
+		for (i = 0; i < 6; i++)
+			data[2 + i] = acl->mac[i];
+		ptr_16 = (u16 *) &data[8];
+		*ptr_16 = cpu_to_be16(acl->eth_type);
+		if (ACL_ENABLE_2_COUNT == acl->enable) {
+			data[ACL_INTR_CNT_START] = 0;
+			ptr_16 = (u16 *) &data[10];
+			*ptr_16 = cpu_to_be16((acl->cnt & ACL_CNT_M) <<
+				ACL_CNT_S);
+			if (acl->msec)
+				data[ACL_INTR_CNT_START] |= ACL_MSEC_UNIT;
+			if (acl->intr_mode)
+				data[ACL_INTR_CNT_START] |= ACL_INTR_MODE;
+		}
+		break;
+	case ACL_MODE_LAYER_3:
+		j = 2;
+		for (i = 0; i < 4; i++, j++)
+			data[j] = acl->ip4_addr[i];
+		for (i = 0; i < 4; i++, j++)
+			data[j] = acl->ip4_mask[i];
+		break;
+	case ACL_MODE_LAYER_4:
+		switch (acl->enable) {
+		case ACL_ENABLE_4_TCP_SEQN_COMP:
+			ptr_32 = (u32 *) &data[2];
+			*ptr_32 = cpu_to_be32(acl->seqnum);
+			data[6] = 0;
+			break;
+		default:
+			ptr_16 = (u16 *) &data[2];
+			*ptr_16 = cpu_to_be16(acl->max_port);
+			++ptr_16;
+			*ptr_16 = cpu_to_be16(acl->min_port);
+			data[6] = (acl->port_mode & ACL_PORT_MODE_M) <<
+				ACL_PORT_MODE_S;
+		}
+		data[6] |= (acl->protocol >> 7);
+		data[7] = (acl->protocol << 1);
+		if (acl->tcp_flag_enable)
+			data[7] |= ACL_TCP_FLAG_ENABLE;
+		data[8] = acl->tcp_flag_mask;
+		data[9] = acl->tcp_flag;
+		break;
+	default:
+		break;
+	}
+}  /* set_acl_table_info */
+
+/**
+ * wait_for_acl_table - Wait for ACL table
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ *
+ * This helper routine waits for ACL table to be ready for access.
+ */
+static void wait_for_acl_table(struct ksz_sw *sw, uint port)
+{
+	u8 ctrl;
+
+	do {
+		sw_r_ext_table(sw, TABLE_ACL, port + 1, REG_PORT_ACL_CTRL_0,
+			&ctrl);
+	} while (!(ctrl & (PORT_ACL_WRITE_DONE | PORT_ACL_READ_DONE)));
+}  /* wait_for_acl_table */
+
+/**
+ * sw_r_acl_hw - read from ACL table
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ * @addr:	The ACL index.
+ * @data:	Buffer to hold the ACL data.
+ *
+ * This routine reads from ACL table of the port.
+ */
+static void sw_r_acl_hw(struct ksz_sw *sw, uint port, u16 addr, u8 data[])
+{
+	u8 ctrl = (addr & PORT_ACL_INDEX_M);
+
+	port++;
+	sw_w_ext_table(sw, TABLE_ACL, port, REG_PORT_ACL_CTRL_0, ctrl);
+	do {
+		sw_r_ext_table(sw, TABLE_ACL, port, REG_PORT_ACL_CTRL_0,
+			&ctrl);
+	} while (!(ctrl & PORT_ACL_READ_DONE));
+	sw_g_ext_table(sw, TABLE_ACL, port, 0, data, ACL_TABLE_LEN);
+}  /* sw_r_acl_hw */
+
+/**
+ * sw_r_acl_table - read from ACL table
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ * @addr:	The address of the table entry.
+ * @acl:	Buffer to store the ACL entry.
+ *
+ * This function reads an entry of the ACL table of the port.
+ *
+ * Return 0 if the entry is valid; otherwise -1.
+ */
+static int sw_r_acl_table(struct ksz_sw *sw, uint port, u16 addr,
+	struct ksz_acl_table *acl)
+{
+	u8 data[20];
+	int rc = -1;
+
+	sw->ops->acquire(sw);
+	wait_for_acl_table(sw, port);
+	sw_r_acl_hw(sw, port, addr, data);
+	get_acl_table_info(acl, data);
+	memcpy(acl->data, data, ACL_TABLE_LEN);
+	sw->ops->release(sw);
+	if (acl->mode)
+		rc = 0;
+	acl->changed = 0;
+	return rc;
+}  /* sw_r_acl_table */
+
+/**
+ * sw_a_acl_hw - write to ACL action field
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ * @addr:	The ACL index.
+ * @data:	The ACL data to write.
+ *
+ * This routine writes to ACL action field of the port.
+ */
+static void sw_a_acl_hw(struct ksz_sw *sw, uint port, u16 addr, u8 data[])
+{
+	u8 ctrl = (addr & PORT_ACL_INDEX_M) | PORT_ACL_WRITE;
+
+	if (sw->info->port_cfg[port].acl_byte_enable != ACL_ACTION_ENABLE)
+		sw->info->port_cfg[port].acl_byte_enable = ACL_ACTION_ENABLE;
+	data[REG_PORT_ACL_BYTE_EN_MSB] = 0;
+	data[REG_PORT_ACL_BYTE_EN_LSB] = ACL_ACTION_ENABLE;
+	data[REG_PORT_ACL_CTRL_0] = ctrl;
+	port++;
+	sw_p_ext_table(sw, TABLE_ACL, port, ACL_ACTION_START,
+		       &data[ACL_ACTION_START], ACL_ACTION_LEN);
+	sw_p_ext_table(sw, TABLE_ACL, port, REG_PORT_ACL_BYTE_EN_MSB,
+		       &data[REG_PORT_ACL_BYTE_EN_MSB], 3);
+	do {
+		sw_r_ext_table(sw, TABLE_ACL, port, REG_PORT_ACL_CTRL_0,
+			&ctrl);
+	} while (!(ctrl & PORT_ACL_WRITE_DONE));
+}  /* sw_a_acl_hw */
+
+/**
+ * sw_w_acl_hw - enable/disable ACL table
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ * @addr:	The ACL index.
+ * @data:	The ACL data to write.
+ *
+ * This routine enables/disables ACL table of the port.
+ */
+static void sw_s_acl_hw(struct ksz_sw *sw, uint port, u16 addr, u8 data)
+{
+	u8 ctrl = (addr & PORT_ACL_INDEX_M) | PORT_ACL_WRITE;
+
+	if (sw->info->port_cfg[port].acl_byte_enable != ACL_MODE_ENABLE) {
+		u8 byte_enable[2] = { ACL_MODE_ENABLE >> 8, 0 };
+
+		sw->info->port_cfg[port].acl_byte_enable = ACL_MODE_ENABLE;
+		sw_p_ext_table(sw, TABLE_ACL, port + 1,
+			       REG_PORT_ACL_BYTE_EN_MSB, byte_enable, 2);
+	}
+	port++;
+	sw_w_ext_table(sw, TABLE_ACL, port, 1, data);
+	sw_w_ext_table(sw, TABLE_ACL, port, REG_PORT_ACL_CTRL_0, ctrl);
+	do {
+		sw_r_ext_table(sw, TABLE_ACL, port, REG_PORT_ACL_CTRL_0,
+			&ctrl);
+	} while (!(ctrl & PORT_ACL_WRITE_DONE));
+}  /* sw_s_acl_hw */
+
+/**
+ * sw_w_acl_hw - write to ACL table
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ * @addr:	The ACL index.
+ * @data:	The ACL data to write.
+ *
+ * This routine writes to ACL table of the port.
+ */
+static void sw_w_acl_hw(struct ksz_sw *sw, uint port, u16 addr, u8 data[],
+	u16 byte_enable)
+{
+	u8 ctrl = (addr & PORT_ACL_INDEX_M) | PORT_ACL_WRITE;
+
+	if (sw->info->port_cfg[port].acl_byte_enable != byte_enable)
+		sw->info->port_cfg[port].acl_byte_enable = byte_enable;
+	data[0xe] = 0;
+	data[0xf] = 0;
+	data[REG_PORT_ACL_BYTE_EN_MSB] = (u8)(byte_enable >> 8);
+	data[REG_PORT_ACL_BYTE_EN_LSB] = (u8)byte_enable;
+	data[REG_PORT_ACL_CTRL_0] = ctrl;
+	port++;
+	sw_p_ext_table(sw, TABLE_ACL, port, 0, data, ACL_TABLE_LEN + 2 + 3);
+	do {
+		sw_r_ext_table(sw, TABLE_ACL, port, REG_PORT_ACL_CTRL_0,
+			&ctrl);
+	} while (!(ctrl & PORT_ACL_WRITE_DONE));
+}  /* sw_w_acl_hw */
+
+/**
+ * sw_w_acl_action - write to ACL action field
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ * @addr:	The address of the table entry.
+ * @acl:	The ACL entry.
+ *
+ * This routine writes to the action field of an entry of the ACL table.
+ */
+static void sw_w_acl_action(struct ksz_sw *sw, uint port, u16 addr,
+	struct ksz_acl_table *acl)
+{
+	u8 data[20];
+
+	sw->ops->acquire(sw);
+	wait_for_acl_table(sw, port);
+	set_acl_action_info(acl, data);
+	sw_a_acl_hw(sw, port, addr, data);
+	memcpy(&acl->data[ACL_ACTION_START], &data[ACL_ACTION_START],
+		ACL_ACTION_LEN);
+	sw->ops->release(sw);
+	acl->action_changed = 0;
+}  /* sw_w_acl_action */
+
+/**
+ * sw_w_acl_ruleset - write to ACL ruleset field
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ * @addr:	The address of the table entry.
+ * @acl:	The ACL entry.
+ *
+ * This routine writes to the ruleset field of an entry of the ACL table.
+ */
+static void sw_w_acl_ruleset(struct ksz_sw *sw, uint port, u16 addr,
+	struct ksz_acl_table *acl)
+{
+	u8 data[20];
+
+	sw->ops->acquire(sw);
+	wait_for_acl_table(sw, port);
+	set_acl_ruleset_info(acl, data);
+	sw_w_acl_hw(sw, port, addr, data, ACL_RULESET_ENABLE);
+
+	/* First rule */
+	acl->data[0] = data[0];
+	memcpy(&acl->data[ACL_RULESET_START], &data[ACL_RULESET_START],
+		ACL_RULESET_LEN);
+	sw->ops->release(sw);
+	acl->ruleset_changed = 0;
+}  /* sw_w_acl_ruleset */
+
+/**
+ * sw_w_acl_rule - write to ACL matching and process fields
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ * @addr:	The address of the table entry.
+ * @acl:	The ACL entry.
+ *
+ * This routine writes to the matching and process fields of an entry of the
+ * ACL table of the port.
+ */
+static void sw_w_acl_rule(struct ksz_sw *sw, uint port, u16 addr,
+	struct ksz_acl_table *acl)
+{
+	u8 data[20];
+	u16 byte_enable = ACL_MATCH_ENABLE;
+
+	if (ACL_MODE_LAYER_2 == acl->mode && ACL_ENABLE_2_COUNT == acl->enable)
+		byte_enable = ACL_BYTE_ENABLE;
+	sw->ops->acquire(sw);
+	wait_for_acl_table(sw, port);
+	set_acl_table_info(acl, data);
+	if (data[0] == acl->data[0] && !memcmp(&data[2], &acl->data[2], 12))
+		sw_s_acl_hw(sw, port, addr, data[1]);
+	else
+		sw_w_acl_hw(sw, port, addr, data, byte_enable);
+	memcpy(acl->data, data, ACL_ACTION_START);
+	sw->ops->release(sw);
+	acl->changed = 0;
+}  /* sw_w_acl_rule */
+
+/**
+ * sw_w_acl_table - write to ACL table
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ * @addr:	The address of the table entry.
+ * @acl:	The ACL entry.
+ *
+ * This routine writes an entry of the ACL table of the port.
+ */
+static void sw_w_acl_table(struct ksz_sw *sw, uint port, u16 addr,
+	struct ksz_acl_table *acl)
+{
+	u8 data[20];
+
+	if (ACL_MODE_LAYER_2 == acl->mode && ACL_ENABLE_2_COUNT == acl->enable)
+		return;
+	acl->data[0] = 0xff;
+	memset(data, 0, sizeof(data));
+	sw->ops->acquire(sw);
+	wait_for_acl_table(sw, port);
+	set_acl_action_info(acl, data);
+	set_acl_table_info(acl, data);
+	set_acl_ruleset_info(acl, data);
+	sw_w_acl_hw(sw, port, addr, data, ACL_BYTE_ENABLE);
+	memcpy(acl->data, data, ACL_TABLE_LEN);
+	sw->ops->release(sw);
+}  /* sw_w_acl_table */
+
+/**
+ * acl_action_info - format ACL action field information
+ * @acl:	The ACL entry.
+ * @index;	The entry index.
+ * @buf:	Buffer to store the strings.
+ * @len:	Lenght of buffer.
+ *
+ * This helper routine formats the ACL action field information.
+ */
+static int acl_action_info(struct ksz_acl_table *acl, u16 index, char *buf,
+	int len)
+{
+	char prio = 'p';
+	char vlan = 'v';
+
+	if (acl->prio_mode != ACL_PRIO_MODE_DISABLE)
+		prio = 'P';
+	if (acl->vlan_prio_replace)
+		vlan = 'V';
+	len += sprintf(buf + len,
+		"%x: %c:%u=%u %c:%u=%u %u=%04x [%u]\n",
+		index,
+		prio, acl->prio_mode, acl->prio,
+		vlan, acl->vlan_prio_replace, acl->vlan_prio,
+		acl->map_mode, acl->ports,
+		acl->action_changed ? 8 : 1);
+	return len;
+}  /* acl_action_info */
+
+/**
+ * acl_ruleset_info - format ACL ruleset field information
+ * @acl:	The ACL entry.
+ * @index;	The entry index.
+ * @buf:	Buffer to store the strings.
+ * @len:	Lenght of buffer.
+ *
+ * This helper routine formats the ACL ruleset field information.
+ */
+static int acl_ruleset_info(struct ksz_acl_table *acl, u16 index, char *buf,
+	int len)
+{
+	len += sprintf(buf + len,
+		"%x: %x:%04x [%u]\n",
+		index,
+		acl->first_rule, acl->ruleset,
+		acl->ruleset_changed ? 8 : 1);
+	return len;
+}  /* acl_ruleset_info */
+
+/**
+ * acl_info - format ACL matching and process field information
+ * @acl:	The ACL entry.
+ * @index;	The entry index.
+ * @buf:	Buffer to store the strings.
+ * @len:	Lenght of buffer.
+ *
+ * This helper routine formats the ACL matching and process field information.
+ */
+static int acl_info(struct ksz_acl_table *acl, u16 index, char *buf, int len)
+{
+	char enable = 'e';
+	char equal = 'q';
+	char src = 's';
+	char cnt = 'c';
+	char protocol = 'x';
+	char flag = 'f';
+	char seqnum = 's';
+	char msec[4];
+
+	switch (acl->mode) {
+	case ACL_MODE_LAYER_2:
+		enable = 'E';
+		*msec = 0;
+		if (ACL_ENABLE_2_COUNT == acl->enable) {
+			equal = 'Q';
+			src = 'S';
+			cnt = 'C';
+			if (acl->intr_mode)
+				*msec = 0;
+			else if (acl->msec)
+				strcpy(msec, "ms ");
+			else
+				strcpy(msec, "us ");
+		} else {
+			equal = 'Q';
+			if (ACL_ENABLE_2_TYPE != acl->enable)
+				src = 'S';
+		}
+		len += sprintf(buf + len,
+			"%x: %02X:%02X:%02X:%02X:%02X:%02X-%04x "
+			"%c:%u.%u %s"
+			"%c:%u %c:%u %c:%u "
+			"[%u]\n",
+			index,
+			acl->mac[0], acl->mac[1], acl->mac[2],
+			acl->mac[3], acl->mac[4], acl->mac[5],
+			acl->eth_type,
+			cnt, acl->intr_mode, acl->cnt, msec,
+			enable, acl->enable,
+			src, acl->src, equal, acl->equal,
+			acl->changed ? 8 : acl->mode);
+		break;
+	case ACL_MODE_LAYER_3:
+		if (ACL_ENABLE_3_IP == acl->enable ||
+		    ACL_ENABLE_3_SRC_DST_COMP == acl->enable)
+			enable = 'E';
+		if (ACL_ENABLE_3_IP == acl->enable) {
+			equal = 'Q';
+			src = 'S';
+		}
+		len += sprintf(buf + len,
+			"%x: %u.%u.%u.%u:%u.%u.%u.%u "
+			"%c:%u %c:%u %c:%u "
+			"[%u]\n",
+			index,
+			acl->ip4_addr[0], acl->ip4_addr[1],
+			acl->ip4_addr[2], acl->ip4_addr[3],
+			acl->ip4_mask[0], acl->ip4_mask[1],
+			acl->ip4_mask[2], acl->ip4_mask[3],
+			enable, acl->enable,
+			src, acl->src, equal, acl->equal,
+			acl->changed ? 8 : acl->mode);
+		break;
+	case ACL_MODE_LAYER_4:
+		enable = 'E';
+		if (ACL_ENABLE_4_PROTOCOL == acl->enable) {
+			protocol = 'X';
+			equal = 'Q';
+		} else if (ACL_ENABLE_4_TCP_SEQN_COMP == acl->enable) {
+			seqnum = 'S';
+			equal = 'Q';
+			if (acl->tcp_flag_enable)
+				flag = 'F';
+		} else if (ACL_ENABLE_4_TCP_PORT_COMP == acl->enable) {
+			src = 'S';
+			if (acl->tcp_flag_enable)
+				flag = 'F';
+		} else
+			src = 'S';
+		len += sprintf(buf + len,
+			"%x: %u=%4x-%4x 0%c%x %c:%08x %c:%u=%x:%x "
+			"%c:%u %c:%u %c:%u "
+			"[%u]\n",
+			index,
+			acl->port_mode, acl->min_port, acl->max_port,
+			protocol, acl->protocol, seqnum, acl->seqnum,
+			flag, acl->tcp_flag_enable,
+			acl->tcp_flag, acl->tcp_flag_mask,
+			enable, acl->enable,
+			src, acl->src, equal, acl->equal,
+			acl->changed ? 8 : acl->mode);
+		break;
+	default:
+		len += sprintf(buf + len,
+			"%x: "
+			"%c:%u %c:%u %c:%u "
+			"[%u]\n",
+			index,
+			enable, acl->enable,
+			src, acl->src, equal, acl->equal,
+			acl->changed ? 8 : acl->mode);
+		break;
+	}
+	return len;
+}  /* acl_info */
+
+/*
+ * THa  2023/03/27
+ * Hardware has a bug in which the ACL index cannot be changed when the port
+ * has link.  Turning ACL on decreases the time needed for link off and
+ * improves reliablity.
+ */
+static void sw_access_acl(struct ksz_sw *sw, uint port, u8 *link, u8 *phy)
+{
+	if (port != sw->HOST_PORT) {
+		sw->ops->acquire(sw);
+		if (!port_chk_acl(sw, port))
+			port_cfg_acl(sw, port, true);
+		port_r8(sw, port, REG_PORT_STATUS_2, link);
+		if (*link & PORT_STAT_LINK_GOOD) {
+			int timeout = 50;
+
+			port_r8(sw, port, REG_PORT_CTRL_10, phy);
+			port_w8(sw, port, REG_PORT_CTRL_10,
+				*phy | PORT_TX_DISABLE);
+			do {
+				delay_milli(1);
+				port_r8(sw, port, REG_PORT_STATUS_2, link);
+				--timeout;
+			} while ((*link & PORT_STAT_LINK_GOOD) && timeout);
+			*link = PORT_STAT_LINK_GOOD;
+		} else {
+			*link = 0;
+		}
+		sw->ops->release(sw);
+	}
+}  /* sw_access_acl */
+
+/**
+ * sw_d_acl_table - dump ACL table
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ *
+ * This routine dumps ACL table of the port.
+ */
+static ssize_t sw_d_acl_table(struct ksz_sw *sw, uint port, char *buf,
+	ssize_t len)
+{
+	struct ksz_port_cfg *cfg = get_port_cfg(sw, port);
+	struct ksz_acl_table *acl;
+	u8 phy, link = 0;
+	int min = 0;
+	int i;
+
+	sw_access_acl(sw, port, &link, &phy);
+	for (i = 0; i < ACL_TABLE_ENTRIES; i++) {
+		acl = &cfg->acl_info[i];
+		acl->action_selected = false;
+		sw_r_acl_table(sw, port, i, acl);
+	}
+	if (link) {
+		sw->ops->acquire(sw);
+		port_w8(sw, port, REG_PORT_CTRL_10, phy);
+
+		/* Port 2 has difficulty getting link again if not reset. */
+		if (port == 1)
+			port_w8(sw, port, REG_PORT_STATUS_3,
+				PORT_PHY_SOFT_RESET);
+		sw->ops->release(sw);
+	}
+	for (i = 0; i < ACL_TABLE_ENTRIES; i++) {
+		acl = &cfg->acl_info[i];
+		if (!acl->mode)
+			continue;
+		if (!min)
+			len += sprintf(buf + len, "rules:\n");
+		len = acl_info(acl, i, buf, len);
+		min = 1;
+	}
+	if (min)
+		len += sprintf(buf + len, "\n");
+	min = 0;
+	for (i = 0; i < ACL_TABLE_ENTRIES; i++) {
+		acl = &cfg->acl_info[i];
+		if (!acl->ruleset)
+			continue;
+		if (!min)
+			len += sprintf(buf + len, "rulesets:\n");
+		cfg->acl_info[acl->first_rule].action_selected = true;
+		len = acl_ruleset_info(acl, i, buf, len);
+		min = 1;
+	}
+	if (min)
+		len += sprintf(buf + len, "\n");
+	min = 0;
+	for (i = 0; i < ACL_TABLE_ENTRIES; i++) {
+		acl = &cfg->acl_info[i];
+		if (ACL_PRIO_MODE_DISABLE == acl->prio_mode &&
+		    ACL_MAP_MODE_DISABLE == acl->map_mode &&
+		    !acl->ports &&
+		    !acl->vlan_prio_replace && !acl->action_selected)
+			continue;
+		if (ACL_MODE_LAYER_2 == acl->mode &&
+		    ACL_ENABLE_2_COUNT == acl->enable)
+			continue;
+		if (!min)
+			len += sprintf(buf + len, "actions:\n");
+		len = acl_action_info(acl, i, buf, len);
+		min = 1;
+	}
+	return len;
+}  /* sw_d_acl_table */
+
+static void sw_reset_acl(struct ksz_sw *sw)
+{
+	struct ksz_port_cfg *cfg;
+	uint n;
+	uint port;
+
+	for (n = 0; n <= sw->mib_port_cnt; n++) {
+		port = get_phy_port(sw, n);
+		cfg = get_port_cfg(sw, port);
+		memset(cfg->acl_info, 0, sizeof(struct ksz_acl_table) *
+			ACL_TABLE_ENTRIES);
+		cfg->acl_index = cfg->acl_act_index = cfg->acl_rule_index = 0;
+	}
+}  /* sw_reset_acl */
+
+static void sw_reset_acl_hw(struct ksz_sw *sw)
+{
+	struct ksz_port_cfg *cfg;
+	struct ksz_acl_table *acl;
+	int i;
+	uint n;
+	uint port;
+
+	sw_reset_acl(sw);
+	for (n = 0; n <= sw->mib_port_cnt; n++) {
+		port = get_phy_port(sw, n);
+		sw->ops->release(sw);
+		cfg = get_port_cfg(sw, port);
+		for (i = 0; i < ACL_TABLE_ENTRIES; i++) {
+			acl = &cfg->acl_info[i];
+			acl->mode = 0;
+			acl->ruleset = 0;
+			sw_w_acl_table(sw, port, i, acl);
+		}
+		sw->ops->acquire(sw);
+	}
+}  /* sw_reset_acl_hw */
+
+static void sw_init_acl(struct ksz_sw *sw)
+{
+	struct ksz_port_cfg *cfg;
+	struct ksz_acl_table *acl;
+	int i;
+	uint n;
+	uint port;
+
+	for (n = 0; n <= sw->mib_port_cnt; n++) {
+		port = get_phy_port(sw, n);
+		cfg = get_port_cfg(sw, port);
+		sw->ops->release(sw);
+		for (i = 0; i < ACL_TABLE_ENTRIES; i++) {
+			acl = &cfg->acl_info[i];
+			sw_r_acl_table(sw, port, i, acl);
+		}
+		sw->ops->acquire(sw);
+	}
+}  /* sw_init_acl */
+
+/* -------------------------------------------------------------------------- */
+
+/* PME */
+
+static u8 sw_get_pme(struct ksz_sw *sw)
+{
+	u8 data;
+
+	sw_r_ext_table(sw, TABLE_PME, 0, 3, &data);
+	return data;
+}
+
+static void sw_set_pme(struct ksz_sw *sw, u8 pme)
+{
+	sw_w_ext_table(sw, TABLE_PME, 0, 3, pme);
+}
+
+static u8 port_get_pme_ctrl(struct ksz_sw *sw, uint p)
+{
+	u8 data;
+
+	sw_r_ext_table(sw, TABLE_PME, p + 1, 7, &data);
+	return data;
+}
+
+static void port_set_pme_ctrl(struct ksz_sw *sw, uint p, u8 pme)
+{
+	sw_w_ext_table(sw, TABLE_PME, p + 1, 7, pme);
+}
+
+static u8 port_get_pme_status(struct ksz_sw *sw, uint p)
+{
+	u8 data;
+
+	sw_r_ext_table(sw, TABLE_PME, p + 1, 3, &data);
+	return data;
+}
+
+static void port_set_pme_status(struct ksz_sw *sw, uint p, u8 pme)
+{
+	sw_w_ext_table(sw, TABLE_PME, p + 1, 3, pme);
+}
+
+/* -------------------------------------------------------------------------- */
+
+/* Bandwidth */
+
+static inline void port_cfg_broad_storm(struct ksz_sw *sw, uint p, bool set)
+{
+	port_cfg(sw, p,
+		P_BCAST_STORM_CTRL, PORT_BROADCAST_STORM, set);
+}
+
+static inline int port_chk_broad_storm(struct ksz_sw *sw, uint p)
+{
+	return port_chk(sw, p,
+		P_BCAST_STORM_CTRL, PORT_BROADCAST_STORM);
+}
+
+/* Driver set switch broadcast storm protection at 10% rate. */
+#define BROADCAST_STORM_PROTECTION_RATE	10
+
+/* 148,800 frames * 50 ms / 100 */
+#define BROADCAST_STORM_VALUE		7440
+
+/**
+ * sw_cfg_broad_storm - configure broadcast storm threshold
+ * @sw:		The switch instance.
+ * @percent:	Broadcast storm threshold in percent of transmit rate.
+ *
+ * This routine configures the broadcast storm threshold of the switch.
+ */
+static void sw_cfg_broad_storm(struct ksz_sw *sw, u8 percent)
+{
+	u16 data;
+	u32 value = ((u32) BROADCAST_STORM_VALUE * (u32) percent / 100);
+
+	if (value > BROADCAST_STORM_RATE)
+		value = BROADCAST_STORM_RATE;
+
+	data = sw->reg->r16(sw, S_REPLACE_VID_CTRL);
+	data &= ~BROADCAST_STORM_RATE;
+	data |= value;
+	sw->reg->w16(sw, S_REPLACE_VID_CTRL, data);
+}  /* sw_cfg_broad_storm */
+
+/**
+ * sw_get_board_storm - get broadcast storm threshold
+ * @sw:		The switch instance.
+ * @percent:	Buffer to store the broadcast storm threshold percentage.
+ *
+ * This routine retrieves the broadcast storm threshold of the switch.
+ */
+static void sw_get_broad_storm(struct ksz_sw *sw, u8 *percent)
+{
+	int num;
+	u16 data;
+
+	data = sw->reg->r16(sw, S_REPLACE_VID_CTRL);
+	num = (data & BROADCAST_STORM_RATE);
+	num = (num * 100 + BROADCAST_STORM_VALUE / 2) / BROADCAST_STORM_VALUE;
+	*percent = (u8) num;
+}  /* sw_get_broad_storm */
+
+/**
+ * sw_dis_broad_storm - disable broadcast storm protection
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ *
+ * This routine disables the broadcast storm limit function of the switch.
+ */
+static void sw_dis_broad_storm(struct ksz_sw *sw, uint port)
+{
+	port_cfg_broad_storm(sw, port, 0);
+}  /* sw_dis_broad_storm */
+
+/**
+ * sw_ena_broad_storm - enable broadcast storm protection
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ *
+ * This routine enables the broadcast storm limit function of the switch.
+ */
+static void sw_ena_broad_storm(struct ksz_sw *sw, uint port)
+{
+	sw_cfg_broad_storm(sw, sw->info->broad_per);
+	port_cfg_broad_storm(sw, port, 1);
+}  /* sw_ena_broad_storm */
+
+/**
+ * sw_init_broad_storm - initialize broadcast storm
+ * @sw:		The switch instance.
+ *
+ * This routine initializes the broadcast storm limit function of the switch.
+ */
+static void sw_init_broad_storm(struct ksz_sw *sw)
+{
+	u8 percent;
+
+	sw_get_broad_storm(sw, &percent);
+	sw->info->broad_per = percent;
+}  /* sw_init_broad_storm */
+
+/**
+ * hw_cfg_broad_storm - configure broadcast storm
+ * @sw:		The switch instance.
+ * @percent:	Broadcast storm threshold in percent of transmit rate.
+ *
+ * This routine configures the broadcast storm threshold of the switch.
+ * It is called by user functions.  The hardware should be acquired first.
+ */
+static void hw_cfg_broad_storm(struct ksz_sw *sw, u8 percent)
+{
+	if (percent > 100)
+		percent = 100;
+
+	sw_cfg_broad_storm(sw, percent);
+	sw_init_broad_storm(sw);
+}  /* hw_cfg_broad_storm */
+
+/**
+ * sw_setup_broad_storm - setup broadcast storm
+ * @sw:		The switch instance.
+ *
+ * This routine setup the broadcast storm limit function of the switch.
+ */
+static void sw_setup_broad_storm(struct ksz_sw *sw)
+{
+	uint n;
+	uint port;
+
+	/* Enable switch broadcast storm protection at 10% percent rate. */
+	hw_cfg_broad_storm(sw, BROADCAST_STORM_PROTECTION_RATE);
+	for (n = 0; n <= sw->mib_port_cnt; n++) {
+		port = get_phy_port(sw, n);
+		sw_ena_broad_storm(sw, port);
+	}
+	sw_cfg(sw, REG_SW_CTRL_2, MULTICAST_STORM_DISABLE, 1);
+}  /* sw_setup_broad_storm */
+
+/* -------------------------------------------------------------------------- */
+
+/* Rate Control */
+
+/**
+ * hw_cfg_rate_ctrl - configure port rate control
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ * @prio:	The priority index to configure.
+ * @ctrl:	The flag indicating whether the rate control bit is set or not.
+ *
+ * This routine configures the priority rate control of the port.
+ */
+static void hw_cfg_rate_ctrl(struct ksz_sw *sw, uint port, int prio, int ctrl)
+{
+	int offset;
+	u8 data;
+	u8 saved;
+
+	offset = REG_PORT_RATE_CTRL_0 - prio;
+	port_r8(sw, port, offset, &data);
+	saved = data;
+	data &= ~RATE_CTRL_ENABLE;
+	if (ctrl)
+		data |= RATE_CTRL_ENABLE;
+	if (data != saved)
+		port_w8(sw, port, offset, data);
+	sw->info->port_cfg[port].rate_ctrl[prio] = data;
+}  /* hw_cfg_rate_ctrl */
+
+/**
+ * hw_cfg_rate_ratio - configure port rate ratio
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ * @prio:	The priority index to configure.
+ * @ratio:	The rate ratio.
+ *
+ * This routine configures the priority rate ratio of the port.
+ */
+static void hw_cfg_rate_ratio(struct ksz_sw *sw, uint port, int prio, u8 ratio)
+{
+	int offset;
+	u8 data;
+	u8 saved;
+
+	if (ratio >= RATE_CTRL_ENABLE)
+		return;
+
+	offset = REG_PORT_RATE_CTRL_0 - prio;
+	port_r8(sw, port, offset, &data);
+	saved = data;
+	data &= RATE_CTRL_ENABLE;
+	data |= ratio;
+	if (data != saved)
+		port_w8(sw, port, offset, data);
+	sw->info->port_cfg[port].rate_ctrl[prio] = data;
+}  /* hw_cfg_rate_ratio */
+
+/**
+ * hw_get_rate_ctrl - get port rate control
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ * @prio:	The priority index to retrieve.
+ *
+ * This routine retrieves the priority rate control of the port.
+ */
+static void hw_get_rate_ctrl(struct ksz_sw *sw, uint port, int prio)
+{
+	int offset;
+	u8 data;
+
+	offset = REG_PORT_RATE_CTRL_0 - prio;
+	port_r8(sw, port, offset, &data);
+	sw->info->port_cfg[port].rate_ctrl[prio] = data;
+}  /* hw_get_rate_ctrl */
+
+/* -------------------------------------------------------------------------- */
+
+/* Rate Limit */
+
+/**
+ * hw_cfg_rate_limit - configure port rate limit modes
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ * @mask:	The mask value.
+ * @shift:	The shift position.
+ * @mode:	The rate limit mode.
+ *
+ * This helper routine configures the rate limit modes of the port.
+ */
+static void hw_cfg_rate_limit(struct ksz_sw *sw, uint port, u8 mask, u8 shift,
+	u8 mode)
+{
+	u8 data;
+	u8 saved;
+
+	port_r8(sw, port, P_RATE_LIMIT_CTRL, &data);
+	saved = data;
+	data &= ~(mask << shift);
+	data |= mode << shift;
+	if (data != saved)
+		port_w8(sw, port, P_RATE_LIMIT_CTRL, data);
+	sw->info->port_cfg[port].rate_limit = data;
+}  /* hw_cfg_rate_limit */
+
+static void hw_cfg_in_port_based(struct ksz_sw *sw, uint port, bool set)
+{
+	hw_cfg_rate_limit(sw, port, 1, PORT_IN_PORT_BASED_S, set);
+}
+
+static void hw_cfg_in_flow_ctrl(struct ksz_sw *sw, uint port, bool set)
+{
+	hw_cfg_rate_limit(sw, port, 1, PORT_IN_FLOW_CTRL_S, set);
+}
+
+/**
+ * hw_cfg_cnt_ifg - configure port rate limit count IFG control
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ * @set:	The flag indicating whether the count control is set or not.
+ *
+ * This routine configures the rate limit count IFG control of the port.
+ */
+static void hw_cfg_cnt_ifg(struct ksz_sw *sw, uint port, bool set)
+{
+	hw_cfg_rate_limit(sw, port, 1, PORT_COUNT_IFG_S, set);
+}  /* hw_cfg_cnt_ifg */
+
+/**
+ * hw_cfg_cnt_pre - configure port rate limit count preamble control
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ * @set:	The flag indicating whether the count control is set or not.
+ *
+ * This routine configures the rate limit count preamble control of the port.
+ */
+static void hw_cfg_cnt_pre(struct ksz_sw *sw, uint port, bool set)
+{
+	hw_cfg_rate_limit(sw, port, 1, PORT_COUNT_PREAMBLE_S, set);
+}  /* hw_cfg_cnt_pre */
+
+/**
+ * hw_cfg_rx_limit - configure port rate limit mode
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ * @mode:	The rate limit mode.
+ *
+ * This routine configures the rate limit mode of the port.
+ */
+static void hw_cfg_rx_limit(struct ksz_sw *sw, uint port, u8 mode)
+{
+	if (mode > PORT_IN_LIMIT_MODE_M)
+		return;
+
+	hw_cfg_rate_limit(sw, port, PORT_IN_LIMIT_MODE_M,
+		PORT_IN_LIMIT_MODE_S, mode);
+}  /* hw_cfg_rx_limit */
+
+/**
+ * hw_get_rate_limit - get port rate limit control
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ *
+ * This routine retrieves the rate limit of the port.
+ */
+static void hw_get_rate_limit(struct ksz_sw *sw, uint port)
+{
+	u8 data;
+	struct ksz_port_cfg *cfg = get_port_cfg(sw, port);
+
+	port_r8(sw, port, P_RATE_LIMIT_CTRL, &data);
+	cfg->rate_limit = data;
+	cfg->packet_based = (data >> PORT_RATE_PACKET_BASED_S) & 1;
+}  /* hw_get_rate_limit */
+
+/* -------------------------------------------------------------------------- */
+
+static uint get_rate_from_val(u8 val)
+{
+	uint i;
+
+	if (0 == val)
+		i = 0;
+	else if (val <= 100)
+		i = 1000 * val;
+	else
+		i = 64 * (val - 100);
+	return i;
+}
+
+static int get_rate_to_val(uint rate)
+{
+	int i;
+
+	if (rate >= 1000) {
+		i = (rate + 500) / 1000;
+		if (i > 100)
+			i = 100;
+	} else if (0 == rate) {
+		i = 0;
+	} else {
+		i = (rate + 32) / 64;
+		if (0 == i)
+			i = 1;
+		else if (i > 15)
+			i = 15;
+		i += 100;
+	}
+	return i;
+}
+
+static uint get_packet_from_val(u8 val)
+{
+	uint i;
+
+	if (0 == val)
+		i = 0;
+	else if (val <= 100)
+		i = 1920 * val;
+	else if (101 == val)
+		i = 64;
+	else
+		i = 128 * (val - 101);
+	return i;
+}
+
+static int get_packet_to_val(uint rate)
+{
+	int i;
+
+	if (rate >= 1920) {
+		i = (rate + 960) / 1920;
+		if (i > 100)
+			i = 100;
+	} else if (0 == rate) {
+		i = 0;
+	} else if (rate <= 64) {
+		i = 101;
+	} else {
+		i = (rate + 64) / 128;
+		if (0 == i)
+			i = 1;
+		else if (i > 14)
+			i = 14;
+		i += 101;
+	}
+	return i;
+}
+
+/**
+ * port_cfg_rate - configure port priority rate
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ * @prio:	The priority index to configure.
+ * @offset:	The receive or transmit rate offset.
+ * @shift:	The shift position to set the value.
+ * @rate:	The rate limit in number of Kbps (or Kpps).
+ * @packet:	Packet indication.
+ *
+ * This helper routine configures the priority rate of the port.
+ */
+static void port_cfg_rate(struct ksz_sw *sw, uint port, uint prio, uint offset,
+	uint rate, bool packet)
+{
+	u8 factor;
+
+	if (packet)
+		factor = (u8) get_packet_to_val(rate);
+	else
+		factor = (u8) get_rate_to_val(rate);
+
+	port_w8(sw, port, offset + prio, factor);
+}  /* port_cfg_rate */
+
+/**
+ * port_get_rate - get port priority rate
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ * @prio:	The priority index to configure.
+ * @offset:	The receive or transmit rate offset.
+ * @shift:	The shift position to get the value.
+ * @rate:	Buffer to store the data rate in number of Kbps (or Kpps).
+ * @packet:	Packet indication.
+ *
+ * This helper routine retrieves the priority rate of the port.
+ */
+static void port_get_rate(struct ksz_sw *sw, uint port, uint prio, uint offset,
+	uint *rate, bool packet)
+{
+	u8 data;
+
+	port_r8(sw, port, offset + prio, &data);
+	if (packet)
+		*rate = get_packet_from_val(data);
+	else
+		*rate = get_rate_from_val(data);
+}  /* port_get_rate */
+
+/**
+ * hw_cfg_prio_rate - configure port priority
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ * @prio:	The priority index to configure.
+ * @rate:	The rate limit in number of Kbps (or Kpps).
+ * @offset:	The receive or transmit rate offset.
+ * @result:	Buffer to store the data rate in number of Kbps (or Kpps).
+ * @packet:	Packet indication.
+ *
+ * This helper routine configures the priority rate of the port and retrieves
+ * the actual rate number.
+ */
+static void hw_cfg_prio_rate(struct ksz_sw *sw, uint port, int prio, uint rate,
+	int offset, uint *result, bool packet)
+{
+	port_cfg_rate(sw, port, prio, offset, rate, packet);
+	port_get_rate(sw, port, prio, offset, result, packet);
+}  /* hw_cfg_prio_rate */
+
+/*
+ * THa  2016/02/24
+ * The receive rate limit does not take effect until the last priority is also
+ * written!  It can be turned off without writing the last priority.  Setting
+ * it turns on rate limiting but the hardware seems to use the last value.
+ */
+static void hw_set_rx_prio(struct ksz_sw *sw, uint port)
+{
+	u8 data;
+
+	port_r8(sw, port, REG_PORT_IN_RATE_0 + PRIO_QUEUES - 1, &data);
+	port_w8(sw, port, REG_PORT_IN_RATE_0 + PRIO_QUEUES - 1, data);
+}  /* hw_set_rx_prio */
+
+/**
+ * hw_cfg_rx_prio_rate - configure port receive priority
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ * @prio:	The priority index to configure.
+ * @rate:	The rate limit in number of Kbps (or Kpps).
+ *
+ * This routine configures the receive priority rate of the port.
+ * It is called by user functions.  The hardware should be acquired first.
+ */
+static void hw_cfg_rx_prio_rate(struct ksz_sw *sw, uint port, int prio,
+	uint rate)
+{
+	uint *result;
+	struct ksz_port_cfg *cfg = get_port_cfg(sw, port);
+
+	if (cfg->packet_based)
+		result = &cfg->rx_packet[prio];
+	else
+		result = &cfg->rx_rate[prio];
+	hw_cfg_prio_rate(sw, port, prio, rate,
+		REG_PORT_IN_RATE_0,
+		result, cfg->packet_based);
+}  /* hw_cfg_rx_prio_rate */
+
+/**
+ * hw_cfg_tx_prio_rate - configure port transmit priority
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ * @prio:	The priority index to configure.
+ * @rate:	The rate limit in number of Kbps (or Kpps).
+ *
+ * This routine configures the transmit priority rate of the port.
+ * It is called by user functions.  The hardware should be acquired first.
+ */
+static void hw_cfg_tx_prio_rate(struct ksz_sw *sw, uint port, int prio,
+	uint rate)
+{
+	uint *result;
+	struct ksz_port_cfg *cfg = get_port_cfg(sw, port);
+
+	if (cfg->packet_based)
+		result = &cfg->tx_packet[prio];
+	else
+		result = &cfg->tx_rate[prio];
+	hw_cfg_prio_rate(sw, port, prio, rate,
+		REG_PORT_OUT_RATE_0,
+		result, cfg->packet_based);
+}  /* hw_cfg_tx_prio_rate */
+
+/**
+ * sw_chk_rx_prio_rate - check switch rx priority rate
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ *
+ * This function checks whether the rx priority rate function of the switch is
+ * enabled.
+ *
+ * Return 0 if not enabled.
+ */
+static int sw_chk_rx_prio_rate(struct ksz_sw *sw, uint port)
+{
+	u32 rate_addr;
+	u32 in_rate;
+
+	PORT_CTRL_ADDR(port, rate_addr);
+	rate_addr += REG_PORT_IN_RATE_0;
+	in_rate = sw->reg->r32(sw, rate_addr);
+	return in_rate != 0;
+}  /* sw_chk_rx_prio_rate */
+
+/**
+ * sw_dis_rx_prio_rate - disable switch rx priority rate
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ *
+ * This routine disables the rx priority rate function of the switch.
+ */
+static void sw_dis_rx_prio_rate(struct ksz_sw *sw, uint port)
+{
+	u32 rate_addr;
+
+	PORT_CTRL_ADDR(port, rate_addr);
+	rate_addr += REG_PORT_IN_RATE_0;
+	sw->reg->w32(sw, rate_addr, 0);
+}  /* sw_dis_rx_prio_rate */
+
+/**
+ * sw_ena_rx_prio_rate - enable switch rx priority rate
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ *
+ * This routine enables the rx priority rate function of the switch.
+ */
+static void sw_ena_rx_prio_rate(struct ksz_sw *sw, uint port)
+{
+	int prio;
+	u32 *rate;
+	struct ksz_port_cfg *cfg = get_port_cfg(sw, port);
+
+	if (cfg->packet_based)
+		rate = cfg->rx_packet;
+	else
+		rate = cfg->rx_rate;
+/*
+ * THa  2016/02/24
+ * The receive rate limit does not take effect until the last priority is also
+ * written!  It can be turned off without writing the last priority.  Setting
+ * it turns on rate limiting but the hardware seems to use the last value.
+ */
+	for (prio = 0; prio < PRIO_QUEUES; prio++, rate++)
+		hw_cfg_rx_prio_rate(sw, port, prio, *rate);
+}  /* sw_ena_rx_prio_rate */
+
+/**
+ * sw_chk_tx_prio_rate - check switch tx priority rate
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ *
+ * This function checks whether the tx priority rate function of the switch is
+ * enabled.
+ *
+ * Return 0 if not enabled.
+ */
+static int sw_chk_tx_prio_rate(struct ksz_sw *sw, uint port)
+{
+	u32 rate_addr;
+	u32 out_rate;
+
+	PORT_CTRL_ADDR(port, rate_addr);
+	rate_addr += REG_PORT_OUT_RATE_0;
+	if (sw_chk(sw, REG_SW_CTRL_19, SW_OUT_RATE_LIMIT_QUEUE_BASED))
+		out_rate = sw->reg->r32(sw, rate_addr);
+
+	/* Only need to check first priority as the others do not matter. */
+	else
+		out_rate = sw->reg->r8(sw, rate_addr);
+	return out_rate != 0;
+}  /* sw_chk_tx_prio_rate */
+
+/**
+ * sw_dis_tx_prio_rate - disable switch tx priority rate
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ *
+ * This routine disables the tx priority rate function of the switch.
+ */
+static void sw_dis_tx_prio_rate(struct ksz_sw *sw, uint port)
+{
+	u32 rate_addr;
+
+	PORT_CTRL_ADDR(port, rate_addr);
+	rate_addr += REG_PORT_OUT_RATE_0;
+	sw->reg->w32(sw, rate_addr, 0);
+}  /* sw_dis_tx_prio_rate */
+
+/**
+ * sw_ena_tx_prio_rate - enable switch tx priority rate
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ *
+ * This routine enables the tx priority rate function of the switch.
+ */
+static void sw_ena_tx_prio_rate(struct ksz_sw *sw, uint port)
+{
+	int prio;
+	u32 *rate;
+	struct ksz_port_cfg *cfg = get_port_cfg(sw, port);
+
+	if (cfg->packet_based)
+		rate = cfg->tx_packet;
+	else
+		rate = cfg->tx_rate;
+	for (prio = 0; prio < PRIO_QUEUES; prio++, rate++)
+		hw_cfg_tx_prio_rate(sw, port, prio, *rate);
+}  /* sw_ena_tx_prio_rate */
+
+static void hw_cfg_rate_packet_based(struct ksz_sw *sw, uint port, bool set)
+{
+	int prio;
+	u32 *rx_rate;
+	u32 *tx_rate;
+	struct ksz_port_cfg *cfg = get_port_cfg(sw, port);
+
+	cfg->packet_based = set;
+	hw_cfg_rate_limit(sw, port, 1, PORT_RATE_PACKET_BASED_S, set);
+	if (cfg->packet_based) {
+		rx_rate = cfg->rx_packet;
+		tx_rate = cfg->tx_packet;
+	} else {
+		rx_rate = cfg->rx_rate;
+		tx_rate = cfg->tx_rate;
+	}
+	for (prio = 0; prio < PRIO_QUEUES; prio++, rx_rate++, tx_rate++) {
+
+/* See issue above about configuring rx priority rates. */
+#if 0
+		/* Rate limiting is not enabled. */
+		if (!cfg->rx_rate[prio] && !cfg->rx_packet[prio])
+			continue;
+#endif
+		hw_cfg_rx_prio_rate(sw, port, prio, *rx_rate);
+
+		/* Rate limiting is not enabled. */
+		if (!cfg->tx_rate[prio] && !cfg->tx_packet[prio])
+			continue;
+		hw_cfg_tx_prio_rate(sw, port, prio, *tx_rate);
+	}
+}  /* hw_cfg_rate_packet_based */
+
+/**
+ * sw_init_prio_rate - initialize switch prioirty rate
+ * @sw:		The switch instance.
+ *
+ * This routine initializes the priority rate function of the switch.
+ */
+static void sw_init_prio_rate(struct ksz_sw *sw)
+{
+	uint n;
+	uint offset;
+	uint port;
+	uint prio;
+	struct ksz_port_cfg *cfg;
+
+	for (n = 0; n <= sw->mib_port_cnt; n++) {
+		port = get_phy_port(sw, n);
+		cfg = get_port_cfg(sw, port);
+		hw_get_rate_limit(sw, port);
+		for (prio = 0; prio < PRIO_QUEUES; prio++) {
+			hw_get_rate_ctrl(sw, port, prio);
+			offset = REG_PORT_IN_RATE_0;
+			port_get_rate(sw, port, prio, offset,
+				&cfg->rx_rate[prio], false);
+			port_get_rate(sw, port, prio, offset,
+				&cfg->rx_packet[prio], true);
+			offset = REG_PORT_OUT_RATE_0;
+			port_get_rate(sw, port, prio, offset,
+				&cfg->tx_rate[prio], false);
+			port_get_rate(sw, port, prio, offset,
+				&cfg->tx_packet[prio], true);
+		}
+	}
+}  /* sw_init_prio_rate */
+
+/* -------------------------------------------------------------------------- */
+
+/* Communication */
+
+static inline void port_cfg_back_pressure(struct ksz_sw *sw, uint p, bool set)
+{
+	port_cfg(sw, p,
+		P_STP_CTRL, PORT_BACK_PRESSURE, set);
+}
+
+static inline void port_cfg_force_flow_ctrl(struct ksz_sw *sw, uint p, bool set)
+{
+	port_cfg(sw, p,
+		P_STP_CTRL, PORT_FORCE_FLOW_CTRL, set);
+}
+
+static inline int port_chk_back_pressure(struct ksz_sw *sw, uint p)
+{
+	return port_chk(sw, p,
+		P_STP_CTRL, PORT_BACK_PRESSURE);
+}
+
+static inline int port_chk_force_flow_ctrl(struct ksz_sw *sw, uint p)
+{
+	return port_chk(sw, p,
+		P_STP_CTRL, PORT_FORCE_FLOW_CTRL);
+}
+
+/* -------------------------------------------------------------------------- */
+
+/* Spanning Tree */
+
+static inline void port_cfg_dis_learn(struct ksz_sw *sw, uint p, bool set)
+{
+	port_cfg(sw, p,
+		P_STP_CTRL, PORT_LEARN_DISABLE, set);
+}
+
+static inline void port_cfg_rx(struct ksz_sw *sw, uint p, bool set)
+{
+	port_cfg(sw, p,
+		P_STP_CTRL, PORT_RX_ENABLE, set);
+	if (set)
+		sw->rx_ports |= (1 << p);
+	else
+		sw->rx_ports &= ~(1 << p);
+}
+
+static inline void port_cfg_tx(struct ksz_sw *sw, uint p, bool set)
+{
+	port_cfg(sw, p,
+		P_STP_CTRL, PORT_TX_ENABLE, set);
+	if (set)
+		sw->tx_ports |= (1 << p);
+	else
+		sw->tx_ports &= ~(1 << p);
+}
+
+static inline int port_chk_dis_learn(struct ksz_sw *sw, uint p)
+{
+	return port_chk(sw, p,
+		P_STP_CTRL, PORT_LEARN_DISABLE);
+}
+
+static inline int port_chk_rx(struct ksz_sw *sw, uint p)
+{
+	return port_chk(sw, p,
+		P_STP_CTRL, PORT_RX_ENABLE);
+}
+
+static inline int port_chk_tx(struct ksz_sw *sw, uint p)
+{
+	return port_chk(sw, p,
+		P_STP_CTRL, PORT_TX_ENABLE);
+}
+
+static inline void sw_cfg_fast_aging(struct ksz_sw *sw, bool set)
+{
+	sw_cfg(sw, REG_SW_CTRL_1, SW_FAST_AGING, set);
+}
+
+static void sw_flush_dyn_mac_table(struct ksz_sw *sw, uint port)
+{
+	uint cnt;
+	uint first;
+	uint index;
+	int learn_disable[TOTAL_PORT_NUM];
+
+	if (port < sw->port_cnt) {
+		first = get_log_port(sw, port);
+		cnt = first + 1;
+	} else {
+		first = 0;
+		cnt = sw->mib_port_cnt + 1;
+	}
+	for (index = first; index < cnt; index++) {
+		port = get_phy_port(sw, index);
+		learn_disable[port] = port_chk_dis_learn(sw, port);
+		if (!learn_disable[port])
+			port_cfg_dis_learn(sw, port, 1);
+	}
+	sw_cfg(sw, S_FLUSH_TABLE_CTRL, SW_FLUSH_DYN_MAC_TABLE, 1);
+	for (index = first; index < cnt; index++) {
+		port = get_phy_port(sw, index);
+		if (!learn_disable[port])
+			port_cfg_dis_learn(sw, port, 0);
+	}
+}  /* sw_flush_dyn_mac_table */
+
+/* -------------------------------------------------------------------------- */
+
+/* VLAN */
+
+static inline void port_cfg_ins_tag(struct ksz_sw *sw, uint p, bool insert)
+{
+	port_cfg(sw, p,
+		P_TAG_CTRL, PORT_INSERT_TAG, insert);
+}
+
+static inline void port_cfg_rmv_tag(struct ksz_sw *sw, uint p, bool remove)
+{
+	port_cfg(sw, p,
+		P_TAG_CTRL, PORT_REMOVE_TAG, remove);
+}
+
+static inline int port_chk_ins_tag(struct ksz_sw *sw, uint p)
+{
+	return port_chk(sw, p,
+		P_TAG_CTRL, PORT_INSERT_TAG);
+}
+
+static inline int port_chk_rmv_tag(struct ksz_sw *sw, uint p)
+{
+	return port_chk(sw, p,
+		P_TAG_CTRL, PORT_REMOVE_TAG);
+}
+
+static inline void port_cfg_dis_non_vid(struct ksz_sw *sw, uint p, bool set)
+{
+	port_cfg(sw, p,
+		P_STP_CTRL, PORT_DISCARD_NON_VID, set);
+}
+
+static inline void port_cfg_drop_tag(struct ksz_sw *sw, uint p, bool set)
+{
+	port_cfg(sw, p,
+		P_DROP_TAG_CTRL, PORT_DROP_TAG, set);
+}
+
+static inline void port_cfg_in_filter(struct ksz_sw *sw, uint p, bool set)
+{
+	port_cfg(sw, p,
+		P_STP_CTRL, PORT_INGRESS_FILTER, set);
+}
+
+static inline int port_chk_dis_non_vid(struct ksz_sw *sw, uint p)
+{
+	return port_chk(sw, p,
+		P_STP_CTRL, PORT_DISCARD_NON_VID);
+}
+
+static inline int port_chk_drop_tag(struct ksz_sw *sw, uint p)
+{
+	return port_chk(sw, p,
+		P_DROP_TAG_CTRL, PORT_DROP_TAG);
+}
+
+static inline int port_chk_in_filter(struct ksz_sw *sw, uint p)
+{
+	return port_chk(sw, p,
+		P_STP_CTRL, PORT_INGRESS_FILTER);
+}
+
+/* -------------------------------------------------------------------------- */
+
+/* Mirroring */
+
+static inline void port_cfg_mirror_sniffer(struct ksz_sw *sw, uint p, bool set)
+{
+	port_cfg(sw, p,
+		P_MIRROR_CTRL, PORT_MIRROR_SNIFFER, set);
+}
+
+static inline void port_cfg_mirror_rx(struct ksz_sw *sw, uint p, bool set)
+{
+	port_cfg(sw, p,
+		P_MIRROR_CTRL, PORT_MIRROR_RX, set);
+}
+
+static inline void port_cfg_mirror_tx(struct ksz_sw *sw, uint p, bool set)
+{
+	port_cfg(sw, p,
+		P_MIRROR_CTRL, PORT_MIRROR_TX, set);
+}
+
+static inline void sw_cfg_mirror_rx_tx(struct ksz_sw *sw, bool set)
+{
+	sw_cfg(sw, S_MIRROR_CTRL, SW_MIRROR_RX_TX, set);
+}
+
+static inline int port_chk_mirror_sniffer(struct ksz_sw *sw, uint p)
+{
+	return port_chk(sw, p,
+		P_MIRROR_CTRL, PORT_MIRROR_SNIFFER);
+}
+
+static inline int port_chk_mirror_rx(struct ksz_sw *sw, uint p)
+{
+	return port_chk(sw, p,
+		P_MIRROR_CTRL, PORT_MIRROR_RX);
+}
+
+static inline int port_chk_mirror_tx(struct ksz_sw *sw, uint p)
+{
+	return port_chk(sw, p,
+		P_MIRROR_CTRL, PORT_MIRROR_TX);
+}
+
+static inline int sw_chk_mirror_rx_tx(struct ksz_sw *sw)
+{
+	return sw_chk(sw, S_MIRROR_CTRL, SW_MIRROR_RX_TX);
+}
+
+static void sw_setup_mirror(struct ksz_sw *sw)
+{
+	uint n;
+	uint port;
+
+	/*
+	 * The mirror sniffer port requires it to be in the port membership
+	 * of the receive and transmit ports.
+	 * For example, port 3 is the mirror port of traffic between ports 1
+	 * and 2.  Port 3 needs only to turn sniffer on; its port membership
+	 * can be 0.  Ordinarily the port membership of ports 1 and 2 is 3 for
+	 * just commnunicating with eath other.  It has to be set to 7 to pass
+	 * the frames to port 3.  Only one of the ports needs to turn on
+	 * receive and transmit mirroring.
+	 * The mirror receive and transmit mode requires at least two ports to
+	 * turn on receive and transmit mirroring.
+	 */
+	for (n = 0; n <= sw->mib_port_cnt; n++) {
+		port = get_phy_port(sw, n);
+		port_cfg_mirror_sniffer(sw, port, 0);
+		port_cfg_mirror_rx(sw, port, 0);
+		port_cfg_mirror_tx(sw, port, 0);
+	}
+	sw_cfg_mirror_rx_tx(sw, 0);
+}
+
+/* -------------------------------------------------------------------------- */
+
+static void sw_cfg_unk_dest(struct ksz_sw *sw, u8 offset, bool set)
+{
+	u8 data;
+
+	data = sw->reg->r8(sw, offset);
+	if (set)
+		data |= SW_UNK_FWD_ENABLE;
+	else
+		data &= ~SW_UNK_FWD_ENABLE;
+	sw->reg->w8(sw, offset, data);
+}
+
+static int sw_chk_unk_dest(struct ksz_sw *sw, u8 offset)
+{
+	u8 data;
+
+	data = sw->reg->r8(sw, offset);
+	return (data & SW_UNK_FWD_ENABLE) == SW_UNK_FWD_ENABLE;
+}
+
+static void sw_cfg_unk_def_port(struct ksz_sw *sw, u8 offset, u8 port, int set)
+{
+	u8 data;
+	u8 bits = 1 << port;
+
+	data = sw->reg->r8(sw, offset);
+	if (2 == set) {
+		data &= ~SW_UNK_FWD_MAP;
+		bits = port & SW_UNK_FWD_MAP;
+	}
+	if (set)
+		data |= bits;
+	else
+		data &= ~bits;
+	sw->reg->w8(sw, offset, data);
+}
+
+static int sw_chk_unk_def_port(struct ksz_sw *sw, u8 offset, uint port)
+{
+	u8 data;
+	u8 bit = 1 << port;
+
+	data = sw->reg->r8(sw, offset);
+	if (port >= sw->port_cnt)
+		return data & SW_UNK_FWD_MAP;
+	return (data & bit) == bit;
+}
+
+static inline int sw_chk_self_filter(struct ksz_sw *sw)
+{
+	return sw_chk(sw, REG_SW_UNK_IP_MCAST_CTRL,
+		SW_SELF_ADDR_FILTER_ENABLE);
+}
+
+static inline void sw_cfg_self_filter(struct ksz_sw *sw, bool set)
+{
+	sw_cfg(sw, REG_SW_UNK_IP_MCAST_CTRL, SW_SELF_ADDR_FILTER_ENABLE, set);
+}
+
+static void sw_cfg_src_filter(struct ksz_sw *sw, bool set)
+{
+}  /* sw_cfg_src_filter */
+
+static void sw_fwd_unk_vid(struct ksz_sw *sw)
+{
+	sw->reg->w8(sw, REG_SW_UNK_VID_CTRL, sw->HOST_MASK | SW_UNK_FWD_ENABLE);
+}  /* sw_fwd_unk_vid */
+
+/* -------------------------------------------------------------------------- */
+
+/* Priority */
+
+static inline void port_cfg_diffserv(struct ksz_sw *sw, uint p, bool set)
+{
+	port_cfg(sw, p,
+		P_PRIO_CTRL, PORT_DIFFSERV_ENABLE, set);
+}
+
+static inline void port_cfg_802_1p(struct ksz_sw *sw, uint p, bool set)
+{
+	port_cfg(sw, p,
+		P_PRIO_CTRL, PORT_802_1P_ENABLE, set);
+}
+
+static inline void port_cfg_replace_prio(struct ksz_sw *sw, uint p, bool set)
+{
+	port_cfg(sw, p,
+		P_802_1P_CTRL, PORT_802_1P_REMAPPING, set);
+}
+
+static inline int port_chk_diffserv(struct ksz_sw *sw, uint p)
+{
+	return port_chk(sw, p,
+		P_PRIO_CTRL, PORT_DIFFSERV_ENABLE);
+}
+
+static inline int port_chk_802_1p(struct ksz_sw *sw, uint p)
+{
+	return port_chk(sw, p,
+		P_PRIO_CTRL, PORT_802_1P_ENABLE);
+}
+
+static inline int port_chk_replace_prio(struct ksz_sw *sw, uint p)
+{
+	return port_chk(sw, p,
+		P_802_1P_CTRL, PORT_802_1P_REMAPPING);
+}
+
+/* -------------------------------------------------------------------------- */
+
+/**
+ * sw_set_tos_prio - program switch TOS priority
+ * @sw:		The switch instance.
+ * @tos:	ToS value from 6-bit (bit7 ~ bit2) of ToS field, ranging from 0
+ *		to 63.
+ * @prio:	Priority to be assigned.
+ *
+ * This routine programs the TOS priority into the switch registers.
+ */
+static void sw_set_tos_prio(struct ksz_sw *sw, u8 tos, SW_D prio)
+{
+	SW_W(sw, S_TOS_PRIO_CTRL + tos * SW_SIZE, prio);
+}  /* sw_set_tos_prio */
+
+/**
+ * sw_dis_diffserv - disable switch DiffServ priority
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ *
+ * This routine disables the DiffServ priority function of the switch.
+ */
+static void sw_dis_diffserv(struct ksz_sw *sw, uint port)
+{
+	port_cfg_diffserv(sw, port, 0);
+}  /* sw_dis_diffserv */
+
+/**
+ * sw_ena_diffserv - enable switch DiffServ priority
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ *
+ * This routine enables the DiffServ priority function of the switch.
+ */
+static void sw_ena_diffserv(struct ksz_sw *sw, uint port)
+{
+	port_cfg_diffserv(sw, port, 1);
+}  /* sw_ena_diffserv */
+
+/**
+ * hw_cfg_tos_prio - configure TOS priority
+ * @sw:		The switch instance.
+ * @tos:	ToS value from 6-bit (bit7 ~ bit2) of ToS field, ranging from 0
+ *		to 63.
+ * @prio:	Priority to be assigned.
+ *
+ * This routine configures the TOS priority in the hardware.
+ * DiffServ Value 0 ~ 63 is mapped to Priority Queue Number 0 ~ 3.
+ * It is called by user functions.  The hardware should be acquired first.
+ */
+static void hw_cfg_tos_prio(struct ksz_sw *sw, u8 tos, SW_D prio)
+{
+	int shift;
+	SW_D data = prio;
+	SW_D mask = KS_PRIO_M;
+
+	if (tos >= DIFFSERV_ENTRIES)
+		return;
+
+	if (prio >= 0x10)
+		mask = 0xff;
+	else if (prio > KS_PRIO_M)
+		mask = 0xf;
+	shift = (tos & (KS_PRIO_IN_REG - 1)) * KS_PRIO_S;
+	prio = prio << shift;
+	if (prio >> shift != data)
+		return;
+	mask <<= shift;
+	tos /= KS_PRIO_IN_REG;
+
+	sw->info->diffserv[tos] &= ~mask;
+	sw->info->diffserv[tos] |= prio;
+
+	sw_set_tos_prio(sw, tos, sw->info->diffserv[tos]);
+}  /* hw_cfg_tos_prio */
+
+/* -------------------------------------------------------------------------- */
+
+/**
+ * sw_set_802_1p_prio - program switch 802.1p priority
+ * @sw:		The switch instance.
+ * @prio:	Priority to be assigned.
+ *
+ * This routine programs the 802.1p priority into the switch register.
+ */
+static void sw_set_802_1p_prio(struct ksz_sw *sw, u8 tag, SW_D prio)
+{
+	SW_W(sw, S_802_1P_PRIO_CTRL + tag / SW_SIZE, prio);
+}  /* sw_set_802_1p_prio */
+
+/**
+ * sw_dis_802_1p - disable switch 802.1p priority
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ *
+ * This routine disables the 802.1p priority function of the switch.
+ */
+static void sw_dis_802_1p(struct ksz_sw *sw, uint port)
+{
+	port_cfg_802_1p(sw, port, 0);
+}  /* sw_dis_802_1p */
+
+/**
+ * sw_ena_802_1p - enable switch 802.1p priority
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ *
+ * This routine enables the 802.1p priority function of the switch.
+ */
+static void sw_ena_802_1p(struct ksz_sw *sw, uint port)
+{
+	port_cfg_802_1p(sw, port, 1);
+}  /* sw_ena_802_1p */
+
+/**
+ * hw_cfg_802_1p_prio - configure 802.1p priority
+ * @sw:		The switch instance.
+ * @tag:	The 802.1p tag priority value, ranging from 0 to 7.
+ * @prio:	Priority to be assigned.
+ *
+ * This routine configures the 802.1p priority in the hardware.
+ * 802.1p Tag priority value 0 ~ 7 is mapped to Priority Queue Number 0 ~ 3.
+ * It is called by user functions.  The hardware should be acquired first.
+ */
+static void hw_cfg_802_1p_prio(struct ksz_sw *sw, u8 tag, SW_D prio)
+{
+	int shift;
+	SW_D data = prio;
+	SW_D mask = KS_PRIO_M;
+
+	if (tag >= PRIO_802_1P_ENTRIES)
+		return;
+
+	if (prio >= 0x10)
+		mask = 0xff;
+	else if (prio > KS_PRIO_M)
+		mask = 0xf;
+	shift = (tag & (KS_PRIO_IN_REG - 1)) * KS_PRIO_S;
+	prio = prio << shift;
+	if (prio >> shift != data)
+		return;
+	mask <<= shift;
+	tag /= KS_PRIO_IN_REG;
+
+	sw->info->p_802_1p[tag] &= ~mask;
+	sw->info->p_802_1p[tag] |= prio;
+
+	sw_set_802_1p_prio(sw, tag, sw->info->p_802_1p[tag]);
+}  /* hw_cfg_802_1p_prio */
+
+/**
+ * sw_cfg_replace_null_vid - enable switch null VID replacement
+ * @sw:		The switch instance.
+ * @set:	The flag to disable or enable.
+ *
+ * This routine enables the VID to be replaced with port default VID if it is
+ * empty.
+ */
+static void sw_cfg_replace_null_vid(struct ksz_sw *sw, bool set)
+{
+	sw_cfg(sw, S_REPLACE_VID_CTRL, SW_REPLACE_VID, set);
+}  /* sw_cfg_replace_null_vid */
+
+/**
+ * sw_cfg_replace_prio - enable switch 802.1p priority re-mapping
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ * @set:	The flag to disable or enable.
+ *
+ * This routine enables the 802.1p priority re-mapping function of the switch.
+ * That allows 802.1p priority field to be replaced with the port's default
+ * tag's priority value if the ingress packet's 802.1p priority has a higher
+ * priority than port's default tag's priority.
+ */
+static void sw_cfg_replace_prio(struct ksz_sw *sw, uint port, bool set)
+{
+	port_cfg_replace_prio(sw, port, set);
+}  /* sw_cfg_replace_prio */
+
+/* -------------------------------------------------------------------------- */
+
+/**
+ * sw_cfg_port_based - configure switch port based priority
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ * @prio:	The priority to set.
+ *
+ * This routine configures the port based priority of the switch.
+ */
+static void sw_cfg_port_based(struct ksz_sw *sw, uint port, u8 prio)
+{
+	SW_D data;
+
+	if (prio > PORT_BASED_PRIO_M)
+		prio = PORT_BASED_PRIO_M;
+
+	port_r(sw, port, P_PRIO_CTRL, &data);
+	data &= ~(PORT_BASED_PRIO_M << PORT_BASED_PRIO_S);
+	data |= prio << PORT_BASED_PRIO_S;
+	port_w(sw, port, P_PRIO_CTRL, data);
+
+	sw->info->port_cfg[port].port_prio = prio;
+}  /* sw_cfg_port_based */
+
+/* -------------------------------------------------------------------------- */
+
+static int sw_get_hi_prio_queues(struct ksz_sw *sw)
+{
+	u8 data;
+	int queue;
+
+	data = sw->reg->r8(sw, REG_SWITCH_CTRL_14);
+	data >>= SW_PRIO_MAPPING_S;
+	data &= SW_PRIO_MAPPING_M;
+	switch (data) {
+	case SW_PRIO_MAP_3_HI:
+		queue = 3;
+		break;
+	case SW_PRIO_MAP_0_LO:
+		queue = 1;
+		break;
+	case SW_PRIO_MAP_2_HI:
+		queue = 2;
+		break;
+	default:
+		queue = 0;
+	}
+	return queue;
+}  /* sw_get_hi_prio_queues */
+
+static void sw_set_hi_prio_queues(struct ksz_sw *sw, int queue)
+{
+	u8 data;
+
+	data = sw->reg->r8(sw, REG_SWITCH_CTRL_14);
+	data &= ~(SW_PRIO_MAPPING_M << SW_PRIO_MAPPING_S);
+	switch (queue) {
+	case 1:
+		queue = SW_PRIO_MAP_0_LO;
+		break;
+	case 3:
+		queue = SW_PRIO_MAP_3_HI;
+		break;
+	default:
+		queue = SW_PRIO_MAP_2_HI;
+	}
+	queue <<= SW_PRIO_MAPPING_S;
+	data |= (u8) queue;
+	sw->reg->w8(sw, REG_SWITCH_CTRL_14, data);
+}  /* sw_set_hi_prio_queues */
+
+/**
+ * port_get_prio_queue - check transmit multiple queues
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ *
+ * This function checks number of transmit queues enabled in the port.
+ */
+static int port_get_prio_queue(struct ksz_sw *sw, uint port)
+{
+	int queue;
+	u8 hi;
+	u8 lo;
+
+	port_r(sw, port, REG_PORT_CTRL_0, &lo);
+	port_r(sw, port, P_DROP_TAG_CTRL, &hi);
+	queue = !!(hi & PORT_QUEUE_SPLIT_H);
+	queue <<= 1;
+	queue |= !!(lo & PORT_QUEUE_SPLIT_L);
+	return 1 << queue;
+}  /* port_get_prio_queue */
+
+/**
+ * port_set_prio_queue - enable transmit multiple queues
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ * @queue:	Number of queues.
+ *
+ * This routine enables the transmit multiple queues selection of the switch
+ * port.  The port transmit queue is split into two or four priority queues.
+ */
+static void port_set_prio_queue(struct ksz_sw *sw, uint port, int queue)
+{
+	u8 hi;
+	u8 lo;
+
+	switch (queue) {
+	case 4:
+	case 3:
+		queue = PORT_QUEUE_SPLIT_4;
+		break;
+	case 2:
+		queue = PORT_QUEUE_SPLIT_2;
+		break;
+	default:
+		queue = PORT_QUEUE_SPLIT_1;
+	}
+	port_r(sw, port, REG_PORT_CTRL_0, &lo);
+	port_r(sw, port, P_DROP_TAG_CTRL, &hi);
+	lo &= ~PORT_QUEUE_SPLIT_L;
+	if (queue & 1)
+		lo |= PORT_QUEUE_SPLIT_L;
+	hi &= ~PORT_QUEUE_SPLIT_H;
+	if (queue & 2)
+		hi |= PORT_QUEUE_SPLIT_H;
+	port_w(sw, port, REG_PORT_CTRL_0, lo);
+	port_w(sw, port, P_DROP_TAG_CTRL, hi);
+
+	/* Default is port based for egress rate limit. */
+	if (queue)
+		sw_cfg(sw, REG_SW_CTRL_19, SW_OUT_RATE_LIMIT_QUEUE_BASED, 1);
+}  /* port_set_prio_queue */
+
+/**
+ * sw_init_prio - initialize switch priority
+ * @sw:		The switch instance.
+ *
+ * This routine initializes the switch QoS priority functions.
+ */
+static void sw_init_prio(struct ksz_sw *sw)
+{
+	uint n;
+	uint port;
+	SW_D data;
+
+	sw->reg->r(sw, S_802_1P_PRIO_CTRL, sw->info->p_802_1p,
+		PRIO_802_1P_ENTRIES / KS_PRIO_IN_REG);
+
+	sw->reg->r(sw, S_TOS_PRIO_CTRL, sw->info->diffserv,
+		DIFFSERV_ENTRIES / KS_PRIO_IN_REG);
+
+	for (n = 0; n <= sw->mib_port_cnt; n++) {
+		port = get_phy_port(sw, n);
+		port_r(sw, port, P_PRIO_CTRL, &data);
+		data &= PORT_BASED_PRIO_M;
+		data >>= PORT_BASED_PRIO_S;
+		sw->info->port_cfg[port].port_prio = data;
+	}
+}  /* sw_init_prio */
+
+/**
+ * sw_setup_prio - setup switch priority
+ * @sw:		The switch instance.
+ *
+ * This routine setup the switch QoS priority functions.
+ */
+static void sw_setup_prio(struct ksz_sw *sw)
+{
+	uint n;
+	uint port;
+
+	/* All QoS functions disabled. */
+	for (n = 0; n <= sw->mib_port_cnt; n++) {
+		port = get_phy_port(sw, n);
+		port_set_prio_queue(sw, port, 4);
+		sw_dis_diffserv(sw, port);
+		sw_cfg_replace_prio(sw, port, 0);
+		sw_cfg_port_based(sw, port, sw->info->port_cfg[port].port_prio);
+
+		sw_ena_802_1p(sw, port);
+	}
+	sw_cfg_replace_null_vid(sw, 0);
+}  /* sw_setup_prio */
+
+/* -------------------------------------------------------------------------- */
+
+/**
+ * port_cfg_def_vid - configure port default VID
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ * @vid:	The VID value.
+ *
+ * This routine configures the default VID of the port.
+ */
+static void port_cfg_def_vid(struct ksz_sw *sw, uint port, u16 vid)
+{
+	port_w16(sw, port, REG_PORT_CTRL_VID, vid);
+}  /* port_cfg_def_vid */
+
+/**
+ * port_get_def_vid - get port default VID.
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ * @vid:	Buffer to store the VID.
+ *
+ * This routine retrieves the default VID of the port.
+ */
+static void port_get_def_vid(struct ksz_sw *sw, uint port, u16 *vid)
+{
+	port_r16(sw, port, REG_PORT_CTRL_VID, vid);
+}  /* port_get_def_vid */
+
+/**
+ * sw_cfg_def_vid - configure switch port default VID
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ * @vid:	The VID value.
+ *
+ * This routine configures the default VID of the port.
+ */
+static void sw_cfg_def_vid(struct ksz_sw *sw, uint port, u16 vid)
+{
+	sw->info->port_cfg[port].vid = vid;
+	port_cfg_def_vid(sw, port, vid);
+}  /* sw_cfg_def_vid */
+
+/**
+ * sw_cfg_port_base_vlan - configure port-based VLAN membership
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ * @member:	The port-based VLAN membership.
+ *
+ * This routine configures the port-based VLAN membership of the port.
+ */
+static void sw_cfg_port_base_vlan(struct ksz_sw *sw, uint port, u8 member)
+{
+	SW_D data;
+
+	port_r(sw, port, P_MIRROR_CTRL, &data);
+	data &= ~PORT_VLAN_MEMBERSHIP;
+	data |= (member & sw->PORT_MASK);
+	port_w(sw, port, P_MIRROR_CTRL, data);
+
+	sw->info->port_cfg[port].member = member;
+}  /* sw_cfg_port_base_vlan */
+
+/**
+ * sw_vlan_cfg_dis_non_vid - configure discard non VID
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ * @set:	The flag to disable or enable.
+ *
+ * This routine configures Discard Non VID packets of the switch port.
+ * If enabled, the device will discard packets whose VLAN id does not match
+ * ingress port-based default VLAN id.
+ */
+static void sw_vlan_cfg_dis_non_vid(struct ksz_sw *sw, uint port, bool set)
+{
+	port_cfg_dis_non_vid(sw, port, set);
+}  /* sw_vlan_cfg_dis_non_vid */
+
+/**
+ * sw_vlan_cfg_drop_tag - configure 802.1q tagged packet drop
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ * @set:	The flag to disable or enable.
+ *
+ */
+static void sw_vlan_cfg_drop_tag(struct ksz_sw *sw, uint port, bool set)
+{
+	port_cfg_drop_tag(sw, port, set);
+}  /* sw_vlan_cfg_drop_tag */
+
+/**
+ * sw_vlan_cfg_in_filter - configure ingress VLAN filtering
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ * @set:	The flag to disable or enable.
+ *
+ * This routine configures Ingress VLAN filtering of the switch port.
+ * If enabled, the device will discard packets whose VLAN id membership	in the
+ * VLAN table bits [18:16] does not include the ingress port that received this
+ * packet.
+ */
+static void sw_vlan_cfg_in_filter(struct ksz_sw *sw, uint port, bool set)
+{
+	port_cfg_in_filter(sw, port, set);
+}  /* sw_vlan_cfg_in_filter */
+
+/**
+ * sw_vlan_cfg_ins_tag - configure 802.1q tag insertion
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ * @set:	The flag to disable or enable.
+ *
+ * This routine configures 802.1q Tag insertion to the switch port.
+ * If enabled, the device will insert 802.1q tag to the transmit packet on this
+ * port if received packet is an untagged packet.  The device will not insert
+ * 802.1q tag if received packet is tagged packet.
+ */
+static void sw_vlan_cfg_ins_tag(struct ksz_sw *sw, uint port, bool set)
+{
+	port_cfg_ins_tag(sw, port, set);
+}  /* sw_vlan_cfg_ins_tag */
+
+/**
+ * sw_vlan_cfg_rmv_tag - configure 802.1q tag removal
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ * @set:	The flag to disable or enable.
+ *
+ * This routine configures 802.1q Tag removal to the switch port.
+ * If enabled, the device will removed 802.1q tag to the transmit packet on
+ * this port if received packet is a tagged packet.  The device will not remove
+ * 802.1q tag if received packet is untagged packet.
+ */
+static void sw_vlan_cfg_rmv_tag(struct ksz_sw *sw, uint port, bool set)
+{
+	port_cfg_rmv_tag(sw, port, set);
+}  /* sw_vlan_cfg_rmv_tag */
+
+static inline int sw_chk_ins_tag(struct ksz_sw *sw)
+{
+	return sw_chk(sw, REG_SW_CTRL_19, SW_INS_TAG_ENABLE);
+}
+
+static inline void sw_cfg_ins_tag(struct ksz_sw *sw, bool set)
+{
+	sw_cfg(sw, REG_SW_CTRL_19, SW_INS_TAG_ENABLE, set);
+}
+
+static int get_ins_shift(int src_port, int dst_port)
+{
+	int shift;
+
+	if (src_port == dst_port)
+		return 0;
+	shift = PORT_INS_TAG_FOR_PORT_5_S + dst_port - 3;
+	if (src_port > shift)
+		shift++;
+	return shift;
+}
+
+static int sw_chk_ins(struct ksz_sw *sw, int src_port, int dst_port)
+{
+	int shift;
+
+	shift = get_ins_shift(src_port, dst_port);
+	if (!shift)
+		return 0;
+	--shift;
+	return port_chk(sw, src_port, P_INS_SRC_PVID_CTRL, 1 << shift);
+}
+
+static void sw_cfg_ins(struct ksz_sw *sw, int src_port, int dst_port, bool set)
+{
+	int shift;
+
+	shift = get_ins_shift(src_port, dst_port);
+	if (!shift)
+		return;
+	--shift;
+	port_cfg(sw, src_port, P_INS_SRC_PVID_CTRL, 1 << shift, set);
+}
+
+/**
+ * sw_dis_vlan - disable switch VLAN
+ * @sw:		The switch instance.
+ *
+ * This routine disables the VLAN function of the switch.
+ */
+static void sw_dis_vlan(struct ksz_sw *sw)
+{
+	sw_cfg(sw, S_MIRROR_CTRL, SW_VLAN_ENABLE, 0);
+}  /* sw_dis_vlan */
+
+/**
+ * sw_ena_vlan - enable switch VLAN
+ * @sw:		The switch instance.
+ *
+ * This routine enables the VLAN function of the switch.
+ */
+static void sw_ena_vlan(struct ksz_sw *sw)
+{
+	/* Enable 802.1q VLAN mode. */
+	sw_cfg(sw, REG_SW_CTRL_2, UNICAST_VLAN_BOUNDARY, 1);
+	sw_cfg(sw, S_MIRROR_CTRL, SW_VLAN_ENABLE, 1);
+}  /* sw_ena_vlan */
+
+/**
+ * sw_init_vlan - initialize switch VLAN
+ * @sw:		The switch instance.
+ *
+ * This routine initializes the VLAN function of the switch.
+ */
+static void sw_init_vlan(struct ksz_sw *sw)
+{
+	uint n;
+	uint port;
+	SW_D data;
+	struct ksz_port_cfg *cfg;
+
+	for (n = 0; n <= sw->mib_port_cnt; n++) {
+		port = get_phy_port(sw, n);
+		cfg = get_port_cfg(sw, port);
+		port_get_def_vid(sw, port, &cfg->vid);
+		port_r(sw, port, P_MIRROR_CTRL, &data);
+		data &= PORT_VLAN_MEMBERSHIP;
+		cfg->member = data;
+		cfg->vid_member = data;
+
+		port_cfg(sw, port, P_INS_SRC_PVID_CTRL,
+			(PORT_INS_TAG_FOR_PORT_5 | PORT_INS_TAG_FOR_PORT_4 |
+			PORT_INS_TAG_FOR_PORT_3 | PORT_INS_TAG_FOR_PORT_2),
+			true);
+	}
+}  /* sw_init_vlan */
+
+static void inc_mac_addr(u8 *dst, u8 *src, u8 inc)
+{
+#ifdef USE_SAME_ADDR
+	inc = 0;
+#endif
+	memcpy(dst, src, ETH_ALEN);
+	dst[5] += inc;
+	if (dst[5] < src[5])
+		dst[4]++;
+	if (dst[4] < src[4])
+		dst[3]++;
+}  /* inc_mac_addr */
+
+/**
+ * sw_get_addr - get the switch MAC address.
+ * @sw:		The switch instance.
+ * @mac_addr:	Buffer to store the MAC address.
+ *
+ * This function retrieves the MAC address of the switch.
+ */
+static inline void sw_get_addr(struct ksz_sw *sw, u8 *mac_addr)
+{
+	sw->reg->r(sw, REG_SW_MAC_ADDR_0, mac_addr, 6);
+	memcpy(sw->info->mac_addr, mac_addr, 6);
+}  /* sw_get_addr */
+
+/**
+ * sw_set_addr - configure switch MAC address
+ * @sw:		The switch instance.
+ * @mac_addr:	The MAC address.
+ *
+ * This function configures the MAC address of the switch.
+ */
+static void sw_set_addr(struct ksz_sw *sw, u8 *mac_addr)
+{
+	uint n;
+	uint p;
+	struct ksz_port_info *info;
+
+	for (n = 0; n <= sw->mib_port_cnt; n++) {
+		p = get_phy_port(sw, n);
+		info = get_port_info(sw, p);
+		inc_mac_addr(info->mac_addr, mac_addr, n);
+	}
+/**
+ * THa  2015/07/17
+ * Switch does not learn unicast address from port if the source address
+ * matches the switch MAC address!  This mostly happens with the host MAC
+ * address.
+ */
+#if 0
+	sw->reg->w(sw, REG_SW_MAC_ADDR_0, mac_addr, 6);
+#endif
+	memcpy(sw->info->mac_addr, mac_addr, 6);
+}  /* sw_set_addr */
+
+#ifdef CONFIG_KSZ_MRP
+#include "ksz_mrp.c"
+#endif
+
+#define STP_ENTRY			0
+#define BROADCAST_ENTRY			1
+#define BRIDGE_ADDR_ENTRY		2
+#define IPV6_ADDR_ENTRY			3
+#define DEV_0_ADDR_ENTRY		4
+#define DEV_1_ADDR_ENTRY		5
+
+/**
+ * sw_set_global_ctrl - set switch global control
+ * @sw:		The switch instance.
+ *
+ * This routine sets the global control of the switch function.
+ */
+static void sw_set_global_ctrl(struct ksz_sw *sw)
+{
+	SW_D data;
+	struct phy_device *phydev = sw->phy[0];
+	struct ksz_port_info *info = &sw->port_info[sw->HOST_PORT];
+
+	data = SW_R(sw, REG_PORT_5_CTRL_6);
+
+	/* Allow slower speed to be used for testing purpose. */
+#ifdef USE_10_MBIT_MODE
+	phydev->speed = SPEED_10;
+	phydev->dev_flags |= 1;
+#endif
+#ifdef USE_HALF_DUPLEX
+	phydev->duplex = DUPLEX_HALF;
+	phydev->dev_flags |= 1;
+#endif
+#ifdef USE_GMII_100_MODE
+	if (phydev->speed > SPEED_100)
+		phydev->speed = SPEED_100;
+	phydev->dev_flags |= 1;
+#endif
+	data &= ~PORT_INTERFACE_TYPE;
+	data &= ~PORT_GMII_MAC_MODE;
+	if (phydev->dev_flags & 2)
+		data |= PORT_GMII_MAC_MODE;
+	switch (phydev->interface) {
+	case PHY_INTERFACE_MODE_MII:
+		data &= ~PORT_GMII_1GPS_MODE;
+		if (phydev->speed > SPEED_100)
+			phydev->speed = SPEED_100;
+		break;
+	case PHY_INTERFACE_MODE_RMII:
+		data &= ~PORT_GMII_1GPS_MODE;
+		data |= PORT_INTERFACE_RMII;
+		if (phydev->speed > SPEED_100)
+			phydev->speed = SPEED_100;
+		break;
+	case PHY_INTERFACE_MODE_GMII:
+		data |= PORT_GMII_1GPS_MODE;
+		if (phydev->dev_flags & 1) {
+			if (SPEED_1000 != phydev->speed)
+				data &= ~PORT_GMII_1GPS_MODE;
+		}
+		if ((data & PORT_GMII_1GPS_MODE) &&
+		    phydev->speed < SPEED_1000)
+			phydev->speed = SPEED_1000;
+		data |= PORT_INTERFACE_GMII;
+		break;
+	default:
+		data &= ~PORT_RGMII_ID_IN_ENABLE;
+		data &= ~PORT_RGMII_ID_OUT_ENABLE;
+		if (PHY_INTERFACE_MODE_RGMII_ID == phydev->interface ||
+		    PHY_INTERFACE_MODE_RGMII_RXID == phydev->interface)
+			data |= PORT_RGMII_ID_IN_ENABLE;
+		if (PHY_INTERFACE_MODE_RGMII_ID == phydev->interface ||
+		    PHY_INTERFACE_MODE_RGMII_TXID == phydev->interface)
+			data |= PORT_RGMII_ID_OUT_ENABLE;
+		data |= PORT_GMII_1GPS_MODE;
+		if (phydev->dev_flags & 1) {
+			if (SPEED_1000 != phydev->speed)
+				data &= ~PORT_GMII_1GPS_MODE;
+		}
+		if ((data & PORT_GMII_1GPS_MODE) &&
+		    phydev->speed < SPEED_1000)
+			phydev->speed = SPEED_1000;
+		data |= PORT_INTERFACE_RGMII;
+		break;
+	}
+	info->tx_rate = phydev->speed * TX_RATE_UNIT;
+	info->duplex = phydev->duplex + 1;
+	SW_W(sw, REG_PORT_5_CTRL_6, data);
+
+/*
+ * THa  2023/03/27
+ * Occassionally the ACL table of the host port gets garbage data again.
+ */
+#if 1
+	do {
+		u8 data[20];
+		uint n;
+
+		memset(data, 0, 20);
+		for (n = 0; n < ACL_TABLE_ENTRIES; n++)
+			sw_w_acl_hw(sw, sw->HOST_PORT, n, data,
+				    ACL_BYTE_ENABLE);
+	} while (0);
+#endif
+
+	/* Enable switch MII flow control. */
+	data = SW_R(sw, S_REPLACE_VID_CTRL);
+	data |= SW_FLOW_CTRL;
+
+	if (phydev->dev_flags & 1) {
+		if (SPEED_10 == phydev->speed)
+			data |= SW_10_MBIT;
+		if (DUPLEX_HALF == phydev->duplex) {
+			data |= SW_HALF_DUPLEX;
+			data |= SW_HALF_DUPLEX_FLOW_CTRL;
+		}
+	}
+	SW_W(sw, S_REPLACE_VID_CTRL, data);
+
+	data = SW_R(sw, S_LINK_AGING_CTRL);
+	data |= SW_LINK_AUTO_AGING;
+	SW_W(sw, S_LINK_AGING_CTRL, data);
+
+	data = SW_R(sw, REG_SW_CTRL_1);
+
+	/* Enable aggressive back off algorithm in half duplex mode. */
+	data |= SW_AGGR_BACKOFF;
+
+	/* Enable automatic fast aging when link changed detected. */
+	data |= SW_AGING_ENABLE;
+
+	if (sw->overrides & FAST_AGING)
+		data |= SW_FAST_AGING;
+	else
+		data &= ~SW_FAST_AGING;
+
+	SW_W(sw, REG_SW_CTRL_1, data);
+
+	data = SW_R(sw, REG_SW_CTRL_2);
+
+	/* Make sure unicast VLAN boundary is set as default. */
+	if (sw->dev_count > 1)
+		data |= UNICAST_VLAN_BOUNDARY;
+
+	/* Enable no excessive collision drop. */
+	data |= NO_EXC_COLLISION_DROP;
+	SW_W(sw, REG_SW_CTRL_2, data);
+}  /* sw_set_global_ctrl */
+
+/* -------------------------------------------------------------------------- */
+
+/**
+ * port_set_stp_state - configure port spanning tree state
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ * @state:	The spanning tree state.
+ *
+ * This routine configures the spanning tree state of the port.
+ */
+static void port_set_stp_state(struct ksz_sw *sw, uint port, int state)
+{
+	SW_D data;
+	struct ksz_port_cfg *port_cfg;
+	int member = -1;
+
+	port_cfg = get_port_cfg(sw, port);
+	port_r(sw, port, P_STP_CTRL, &data);
+	switch (state) {
+	case STP_STATE_DISABLED:
+		data &= ~(PORT_TX_ENABLE | PORT_RX_ENABLE);
+		data |= PORT_LEARN_DISABLE;
+		if (port < SWITCH_PORT_NUM)
+			member = 0;
+		break;
+	case STP_STATE_LISTENING:
+/*
+ * No need to turn on transmit because of port direct mode.
+ * Turning on receive is required if static MAC table is not setup.
+ */
+		data &= ~PORT_TX_ENABLE;
+		data |= PORT_RX_ENABLE;
+		data |= PORT_LEARN_DISABLE;
+		if (port < SWITCH_PORT_NUM &&
+		    STP_STATE_DISABLED == port_cfg->stp_state)
+			member = sw->HOST_MASK | port_cfg->vid_member;
+		break;
+	case STP_STATE_LEARNING:
+		data &= ~PORT_TX_ENABLE;
+		data |= PORT_RX_ENABLE;
+		data &= ~PORT_LEARN_DISABLE;
+		break;
+	case STP_STATE_FORWARDING:
+		data |= (PORT_TX_ENABLE | PORT_RX_ENABLE);
+		data &= ~PORT_LEARN_DISABLE;
+
+#ifdef CONFIG_KSZ_STP
+		/* Actual port membership setting is done in another RSTP
+		 * processing routine.
+		 */
+		if (sw->stp == 1 || (sw->stp && (sw->stp & (1 << port)))) {
+			struct ksz_stp_info *info = &sw->info->rstp;
+
+			if (info->br.bridgeEnabled)
+				break;
+		}
+#endif
+		if (((sw->features & (SW_VLAN_DEV | USE_FEWER_PORTS)) ||
+		    sw->dev_offset) && port != sw->HOST_PORT)
+			/* Set port-base vlan membership with host port. */
+			member = sw->HOST_MASK | port_cfg->vid_member;
+		break;
+	case STP_STATE_BLOCKED:
+/*
+ * Need to setup static MAC table with override to keep receiving BPDU
+ * messages.  See sw_setup_stp routine.
+ */
+		data &= ~(PORT_TX_ENABLE | PORT_RX_ENABLE);
+		data |= PORT_LEARN_DISABLE;
+		if (port < SWITCH_PORT_NUM &&
+		    STP_STATE_DISABLED == port_cfg->stp_state)
+			member = sw->HOST_MASK | port_cfg->vid_member;
+		break;
+	case STP_STATE_SIMPLE:
+		data |= (PORT_TX_ENABLE | PORT_RX_ENABLE);
+		data |= PORT_LEARN_DISABLE;
+		if (port < SWITCH_PORT_NUM)
+			/* Set port-base vlan membership with host port. */
+			member = sw->HOST_MASK | port_cfg->vid_member;
+		break;
+	}
+	port_w(sw, port, P_STP_CTRL, data);
+	port_cfg->stp_state = state;
+	if (data & PORT_RX_ENABLE)
+		sw->rx_ports |= (1 << port);
+	else
+		sw->rx_ports &= ~(1 << port);
+	if (data & PORT_TX_ENABLE)
+		sw->tx_ports |= (1 << port);
+	else
+		sw->tx_ports &= ~(1 << port);
+
+	/* Port membership may share register with STP state. */
+	if (member >= 0)
+		sw_cfg_port_base_vlan(sw, port, (u8) member);
+}  /* port_set_stp_state */
+
+/**
+ * sw_clr_sta_mac_table - clear static MAC table
+ * @sw:		The switch instance.
+ *
+ * This routine clears the static MAC table.
+ */
+static void sw_clr_sta_mac_table(struct ksz_sw *sw)
+{
+	struct ksz_mac_table entry;
+	int i;
+
+	memset(&entry, 0, sizeof(struct ksz_mac_table));
+	for (i = 0; i < STATIC_MAC_TABLE_ENTRIES; i++)
+		sw_w_sta_mac_table(sw, i, &entry);
+}  /* sw_clr_sta_mac_table */
+
+/**
+ * sw_setup_stp - setup switch spanning tree support
+ * @sw:		The switch instance.
+ *
+ * This routine initializes the spanning tree support of the switch.
+ */
+static void sw_setup_stp(struct ksz_sw *sw)
+{
+	struct ksz_mac_table entry;
+
+	entry.addr[0] = 0x01;
+	entry.addr[1] = 0x80;
+	entry.addr[2] = 0xC2;
+	entry.addr[3] = 0x00;
+	entry.addr[4] = 0x00;
+	entry.addr[5] = 0x00;
+	entry.ports = sw->HOST_MASK;
+	entry.use_fid = 0;
+	entry.override = 1;
+	entry.valid = 1;
+	sw_w_sta_mac_table(sw, STP_ENTRY, &entry);
+}  /* sw_setup_stp */
+
+#ifdef CONFIG_KSZ_STP
+static void bridge_change(struct ksz_sw *sw)
+{
+	uint n;
+	uint port;
+	u8 member;
+	struct ksz_port_cfg *cfg;
+	struct ksz_sw_info *info = sw->info;
+
+	for (n = 1; n <= sw->mib_port_cnt; n++) {
+		port = get_phy_port(sw, n);
+		cfg = get_port_cfg(sw, port);
+		if (STP_STATE_FORWARDING == cfg->stp_state)
+			member = sw->HOST_MASK | info->member[0];
+		else if (STP_STATE_DISABLED == cfg->stp_state)
+			member = 0;
+		else
+			member = sw->HOST_MASK | (1 << port);
+		if (member != cfg->member)
+			sw_cfg_port_base_vlan(sw, port, member);
+	}
+}  /* bridge_change */
+#endif
+
+#define MAX_SW_LEN			1500
+
+#ifdef CONFIG_KSZ_STP
+#include "ksz_stp.c"
+#endif
+#ifdef CONFIG_KSZ_DLR
+#include "ksz_dlr.c"
+#endif
+
+/*
+ * Link detection routines
+ */
+
+static inline void dbp_link(struct ksz_port *port, struct ksz_sw *sw,
+	int change)
+{
+	struct ksz_port_info *info;
+	uint i;
+	uint n;
+	uint p;
+
+	for (i = 0, n = port->first_port; i < port->port_cnt; i++, n++) {
+		p = get_phy_port(sw, n);
+		info = get_port_info(sw, p);
+		if (media_connected == info->state) {
+			if (change & (1 << i)) {
+				printk(KERN_INFO "link %d-%d: %d, %d\n",
+					sw->id, i + port->first_port,
+					info->tx_rate / TX_RATE_UNIT,
+					info->duplex);
+			}
+		} else {
+			if (change & (1 << i))
+				printk(KERN_INFO "link %d-%d disconnected\n",
+					sw->id, i + port->first_port);
+		}
+	}
+}
+
+static SW_D port_advertised_flow_ctrl(struct ksz_port *port, SW_D ctrl)
+{
+	ctrl &= ~PORT_AUTO_NEG_SYM_PAUSE;
+	switch (port->flow_ctrl) {
+	case PHY_FLOW_CTRL:
+		ctrl |= PORT_AUTO_NEG_SYM_PAUSE;
+		break;
+	/* Not supported. */
+	case PHY_TX_ONLY:
+	case PHY_RX_ONLY:
+	default:
+		break;
+	}
+	return ctrl;
+}  /* port_advertised_flow_ctrl */
+
+static u8 sw_determine_flow_ctrl(struct ksz_sw *sw, struct ksz_port *port,
+	u8 local, u8 remote)
+{
+	int rx;
+	int tx;
+	u8 flow = 0;
+
+	if (sw->overrides & PAUSE_FLOW_CTRL)
+		return flow;
+
+	rx = tx = 0;
+	if (port->force_link)
+		rx = tx = 1;
+	if (remote & PORT_REMOTE_SYM_PAUSE) {
+		if (local & PORT_AUTO_NEG_SYM_PAUSE)
+			rx = tx = 1;
+	}
+	if (rx)
+		flow |= 0x01;
+	if (tx)
+		flow |= 0x02;
+#ifdef DBG_LINK
+	printk(KERN_INFO "pause: %d, %d; %02x %02x\n",
+		rx, tx, local, remote);
+#endif
+	return flow;
+}  /* sw_determine_flow_ctrl */
+
+static void sw_notify_link_change(struct ksz_sw *sw, uint ports)
+{
+	static u8 link_buf[sizeof(struct ksz_info_opt) +
+		sizeof(struct ksz_info_speed) * TOTAL_PORT_NUM +
+		sizeof(struct ksz_resp_msg)];
+
+	if ((sw->notifications & SW_INFO_LINK_CHANGE)) {
+		struct ksz_resp_msg *msg = (struct ksz_resp_msg *) link_buf;
+		struct ksz_info_opt *opt = (struct ksz_info_opt *)
+			&msg->resp.data;
+		struct ksz_port_info *info;
+		struct ksz_info_speed *speed;
+		struct file_dev_info *dev_info;
+		int c;
+		int n;
+		int p;
+		int q;
+
+		/* Check whether only 1 port has change. */
+		c = 0;
+		q = 0;
+		for (n = 1; n <= sw->mib_port_cnt; n++) {
+			p = get_phy_port(sw, n);
+			if (ports & (1 << p)) {
+				q = n;
+				c++;
+			}
+		}
+		if (c > 1) {
+			c = sw->mib_port_cnt;
+			q = 1;
+		}
+		msg->module = DEV_MOD_BASE;
+		msg->cmd = DEV_INFO_SW_LINK;
+		opt->num = (u8) c;
+		opt->port = (u8) q;
+		speed = &opt->data.speed;
+		for (n = 1; n <= opt->num; n++, q++) {
+			p = get_phy_port(sw, q);
+			info = get_port_info(sw, p);
+			if (info->state == media_connected) {
+				speed->tx_rate = info->tx_rate;
+				speed->duplex = info->duplex;
+				speed->flow_ctrl = info->flow_ctrl;
+			} else {
+				speed->tx_rate = 0;
+				speed->duplex = 0;
+				speed->flow_ctrl = 0;
+			}
+			++speed;
+		}
+		n = opt->num * sizeof(struct ksz_info_speed);
+		n += 2;
+		n += sizeof(struct ksz_resp_msg);
+		n -= 4;
+		dev_info = sw->dev_list[0];
+		while (dev_info) {
+			if ((dev_info->notifications[DEV_MOD_BASE] &
+			    SW_INFO_LINK_CHANGE))
+				file_dev_setup_msg(dev_info, msg, n, NULL,
+						   NULL);
+			dev_info = dev_info->next;
+		}
+	}
+}  /* sw_notify_link_change */
+
+#define KSZ8795_SW_ID		0x8795
+#define PHY_ID_KSZ_SW		((KSZ8795_ID_HI << 16) | KSZ8795_SW_ID)
+
+/**
+ * sw_r_phy - read data from PHY register
+ * @sw:		The switch instance.
+ * @phy:	PHY address to read.
+ * @reg:	PHY register to read.
+ * @val:	Buffer to store the read data.
+ *
+ * This routine reads data from the PHY register.
+ */
+static void sw_r_phy(struct ksz_sw *sw, u16 phy, u16 reg, u16 *val)
+{
+	u8 ctrl;
+	u8 restart;
+	u8 link;
+	u8 speed;
+	u8 force;
+	u8 p;
+	u16 data = 0;
+
+	if (phy) {
+		p = phy - 1;
+	} else {
+		switch (reg) {
+		case PHY_REG_CTRL:
+			data = 0x1140;
+			break;
+		case PHY_REG_STATUS:
+			data = 0x7808;
+			break;
+		case PHY_REG_ID_1:
+			data = KSZ8795_ID_HI;
+			break;
+		case PHY_REG_ID_2:
+			data = KSZ8795_SW_ID;
+			break;
+		case PHY_REG_AUTO_NEGOTIATION:
+			data = 0x05e1;
+			break;
+		case PHY_REG_REMOTE_CAPABILITY:
+			data = 0xc5e1;
+			break;
+		}
+		*val = data;
+		return;
+	}
+	switch (reg) {
+	case PHY_REG_CTRL:
+		port_r(sw, p, P_LOCAL_CTRL, &ctrl);
+		port_r(sw, p, P_NEG_RESTART_CTRL, &restart);
+		port_r(sw, p, P_SPEED_STATUS, &speed);
+		port_r(sw, p, P_FORCE_CTRL, &force);
+		if (restart & PORT_PHY_LOOPBACK)
+			data |= PHY_LOOPBACK;
+		if (force & PORT_FORCE_100_MBIT)
+			data |= PHY_SPEED_100MBIT;
+		if (!(force & PORT_AUTO_NEG_DISABLE))
+			data |= PHY_AUTO_NEG_ENABLE;
+		if (restart & PORT_POWER_DOWN)
+			data |= PHY_POWER_DOWN;
+		if (restart & PORT_AUTO_NEG_RESTART)
+			data |= PHY_AUTO_NEG_RESTART;
+		if (force & PORT_FORCE_FULL_DUPLEX)
+			data |= PHY_FULL_DUPLEX;
+		if (speed & PORT_HP_MDIX)
+			data |= PHY_HP_MDIX;
+		if (restart & PORT_FORCE_MDIX)
+			data |= PHY_FORCE_MDIX;
+		if (restart & PORT_AUTO_MDIX_DISABLE)
+			data |= PHY_AUTO_MDIX_DISABLE;
+		if (restart & PORT_TX_DISABLE)
+			data |= PHY_TRANSMIT_DISABLE;
+		if (restart & PORT_LED_OFF)
+			data |= PHY_LED_DISABLE;
+		break;
+	case PHY_REG_STATUS:
+		port_r(sw, p, P_LINK_STATUS, &link);
+		port_r(sw, p, P_SPEED_STATUS, &speed);
+		data = PHY_100BTX_FD_CAPABLE |
+			PHY_100BTX_CAPABLE |
+			PHY_10BT_FD_CAPABLE |
+			PHY_10BT_CAPABLE |
+			PHY_AUTO_NEG_CAPABLE;
+		if (link & PORT_AUTO_NEG_COMPLETE)
+			data |= PHY_AUTO_NEG_ACKNOWLEDGE;
+		if (link & PORT_STAT_LINK_GOOD)
+			data |= PHY_LINK_STATUS;
+		break;
+	case PHY_REG_ID_1:
+		data = KSZ8795_ID_HI;
+		break;
+	case PHY_REG_ID_2:
+		/* Use unique switch id to differentiate from regular PHY. */
+		data = KSZ8795_SW_ID;
+		break;
+	case PHY_REG_AUTO_NEGOTIATION:
+		port_r(sw, p, P_LOCAL_CTRL, &ctrl);
+		data = PHY_AUTO_NEG_802_3;
+		if (ctrl & PORT_AUTO_NEG_SYM_PAUSE)
+			data |= PHY_AUTO_NEG_SYM_PAUSE;
+		if (ctrl & PORT_AUTO_NEG_100BTX_FD)
+			data |= PHY_AUTO_NEG_100BTX_FD;
+		if (ctrl & PORT_AUTO_NEG_100BTX)
+			data |= PHY_AUTO_NEG_100BTX;
+		if (ctrl & PORT_AUTO_NEG_10BT_FD)
+			data |= PHY_AUTO_NEG_10BT_FD;
+		if (ctrl & PORT_AUTO_NEG_10BT)
+			data |= PHY_AUTO_NEG_10BT;
+		break;
+	case PHY_REG_REMOTE_CAPABILITY:
+		port_r(sw, p, P_REMOTE_STATUS, &link);
+		data = PHY_AUTO_NEG_802_3;
+		if (link & PORT_REMOTE_SYM_PAUSE)
+			data |= PHY_AUTO_NEG_SYM_PAUSE;
+		if (link & PORT_REMOTE_100BTX_FD)
+			data |= PHY_AUTO_NEG_100BTX_FD;
+		if (link & PORT_REMOTE_100BTX)
+			data |= PHY_AUTO_NEG_100BTX;
+		if (link & PORT_REMOTE_10BT_FD)
+			data |= PHY_AUTO_NEG_10BT_FD;
+		if (link & PORT_REMOTE_10BT)
+			data |= PHY_AUTO_NEG_10BT;
+		break;
+	default:
+		break;
+	}
+	*val = data;
+}  /* sw_r_phy */
+
+/**
+ * sw_w_phy - write data to PHY register
+ * @hw:		The switch instance.
+ * @phy:	PHY address to write.
+ * @reg:	PHY register to write.
+ * @val:	Word data to write.
+ *
+ * This routine writes data to the PHY register.
+ */
+static void sw_w_phy(struct ksz_sw *sw, u16 phy, u16 reg, u16 val)
+{
+	u8 ctrl;
+	u8 restart;
+	u8 speed;
+	u8 data;
+	u8 p;
+
+	if (phy)
+		p = phy - 1;
+	else
+		return;
+	switch (reg) {
+	case PHY_REG_CTRL:
+		port_r(sw, p, P_SPEED_STATUS, &speed);
+		data = speed;
+		if (val & PHY_HP_MDIX)
+			data |= PORT_HP_MDIX;
+		else
+			data &= ~PORT_HP_MDIX;
+		if (data != speed)
+			port_w(sw, p, P_SPEED_STATUS, data);
+		port_r(sw, p, P_FORCE_CTRL, &ctrl);
+		data = ctrl;
+		if (!(val & PHY_AUTO_NEG_ENABLE))
+			data |= PORT_AUTO_NEG_DISABLE;
+		else
+			data &= ~PORT_AUTO_NEG_DISABLE;
+		if (val & PHY_SPEED_100MBIT)
+			data |= PORT_FORCE_100_MBIT;
+		else
+			data &= ~PORT_FORCE_100_MBIT;
+		if (val & PHY_FULL_DUPLEX)
+			data |= PORT_FORCE_FULL_DUPLEX;
+		else
+			data &= ~PORT_FORCE_FULL_DUPLEX;
+		if (data != ctrl)
+			port_w(sw, p, P_FORCE_CTRL, data);
+		port_r(sw, p, P_NEG_RESTART_CTRL, &restart);
+		data = restart;
+		if (val & PHY_LED_DISABLE)
+			data |= PORT_LED_OFF;
+		else
+			data &= ~PORT_LED_OFF;
+		if (val & PHY_TRANSMIT_DISABLE)
+			data |= PORT_TX_DISABLE;
+		else
+			data &= ~PORT_TX_DISABLE;
+		if (val & PHY_AUTO_NEG_RESTART)
+			data |= PORT_AUTO_NEG_RESTART;
+		else
+			data &= ~(PORT_AUTO_NEG_RESTART);
+		if (val & PHY_POWER_DOWN)
+			data |= PORT_POWER_DOWN;
+		else
+			data &= ~PORT_POWER_DOWN;
+		if (val & PHY_AUTO_MDIX_DISABLE)
+			data |= PORT_AUTO_MDIX_DISABLE;
+		else
+			data &= ~PORT_AUTO_MDIX_DISABLE;
+		if (val & PHY_FORCE_MDIX)
+			data |= PORT_FORCE_MDIX;
+		else
+			data &= ~PORT_FORCE_MDIX;
+		if (val & PHY_LOOPBACK)
+			data |= PORT_PHY_LOOPBACK;
+		else
+			data &= ~PORT_PHY_LOOPBACK;
+		if (data != restart)
+			port_w(sw, p, P_NEG_RESTART_CTRL, data);
+		break;
+	case PHY_REG_AUTO_NEGOTIATION:
+		port_r(sw, p, P_LOCAL_CTRL, &ctrl);
+		data = ctrl;
+		data &= ~(PORT_AUTO_NEG_SYM_PAUSE |
+			PORT_AUTO_NEG_100BTX_FD |
+			PORT_AUTO_NEG_100BTX |
+			PORT_AUTO_NEG_10BT_FD |
+			PORT_AUTO_NEG_10BT);
+		if (val & PHY_AUTO_NEG_SYM_PAUSE)
+			data |= PORT_AUTO_NEG_SYM_PAUSE;
+		if (val & PHY_AUTO_NEG_100BTX_FD)
+			data |= PORT_AUTO_NEG_100BTX_FD;
+		if (val & PHY_AUTO_NEG_100BTX)
+			data |= PORT_AUTO_NEG_100BTX;
+		if (val & PHY_AUTO_NEG_10BT_FD)
+			data |= PORT_AUTO_NEG_10BT_FD;
+		if (val & PHY_AUTO_NEG_10BT)
+			data |= PORT_AUTO_NEG_10BT;
+		if (data != ctrl)
+			port_w(sw, p, P_LOCAL_CTRL, data);
+		break;
+	default:
+		break;
+	}
+}  /* sw_w_phy */
+
+static int port_chk_force_link(struct ksz_sw *sw, uint p, SW_D local,
+	SW_D remote, SW_D status)
+{
+#define PORT_REMOTE_STATUS				\
+	(PORT_REMOTE_100BTX_FD | PORT_REMOTE_100BTX |	\
+	PORT_REMOTE_10BT_FD | PORT_REMOTE_10BT)
+
+	SW_D data;
+	static SW_D saved_ctrl;
+	static SW_D saved_status;
+	static int test_stage;
+
+	port_r(sw, p, P_FORCE_CTRL, &data);
+	if (data & PORT_AUTO_NEG_DISABLE)
+		return 0;
+	if ((local & (PORT_REMOTE_100BTX_FD | PORT_REMOTE_100BTX)) !=
+	    (PORT_REMOTE_100BTX_FD | PORT_REMOTE_100BTX) &&
+	    (local & (PORT_REMOTE_10BT_FD | PORT_REMOTE_10BT)) !=
+	    (PORT_REMOTE_10BT_FD | PORT_REMOTE_10BT))
+		return 0;
+	if (!(remote & PORT_REMOTE_SYM_PAUSE) &&
+	    (remote & PORT_REMOTE_STATUS) != PORT_REMOTE_STATUS) {
+		if (saved_ctrl) {
+			if ((status & PORT_STAT_FULL_DUPLEX) !=
+			    (saved_status & PORT_STAT_FULL_DUPLEX)) {
+				printk(KERN_INFO
+					"%d-%d: duplex is defaulted to %s\n",
+					sw->id, p,
+					(saved_ctrl & PORT_FORCE_FULL_DUPLEX) ?
+					"full" : "half");
+			}
+			if (data != saved_ctrl) {
+				port_w(sw, p, P_FORCE_CTRL, saved_ctrl);
+				port_r(sw, p, P_NEG_RESTART_CTRL, &data);
+				data |= PORT_AUTO_NEG_RESTART;
+				port_w(sw, p, P_NEG_RESTART_CTRL, data);
+				test_stage = 2;
+			} else
+				test_stage = 0;
+			saved_ctrl = 0;
+			if (test_stage)
+				return 1;
+		} else if (!test_stage) {
+			saved_ctrl = data;
+			saved_status = status;
+			if (status & PORT_STAT_FULL_DUPLEX)
+				data &= ~PORT_FORCE_FULL_DUPLEX;
+			else
+				data |= PORT_FORCE_FULL_DUPLEX;
+			port_w(sw, p, P_FORCE_CTRL, data);
+			port_r(sw, p, P_NEG_RESTART_CTRL, &data);
+			data |= PORT_AUTO_NEG_RESTART;
+			port_w(sw, p, P_NEG_RESTART_CTRL, data);
+			test_stage = 1;
+			return 1;
+		}
+		test_stage = 0;
+	}
+	return 0;
+}  /* port_chk_force_link */
+
+/**
+ * port_get_link_speed - get current link status
+ * @port:	The port instance.
+ *
+ * This routine reads PHY registers to determine the current link status of the
+ * switch ports.
+ */
+static int port_get_link_speed(struct ksz_port *port)
+{
+	struct ksz_port_info *info;
+	struct ksz_port_info *linked = NULL;
+	struct ksz_port_state *state;
+	struct ksz_sw *sw = port->sw;
+	SW_D data;
+	SW_D status;
+	SW_D link;
+	SW_D local;
+	SW_D remote;
+	uint i;
+	uint n;
+	uint p;
+	int change = 0;
+
+	/*
+	 * Only check port which has interrupts triggered.
+	 * If no interrupt poll all the ports with PHY.
+	 */
+	if (!sw->phy_intr)
+		sw->phy_intr = sw->PORT_MASK;
+	for (i = 0, n = port->first_port; i < port->port_cnt; i++, n++) {
+		p = get_phy_port(sw, n);
+		info = get_port_info(sw, p);
+		if (!(sw->phy_intr & (1 << p))) {
+			if (!linked && p != sw->HOST_PORT &&
+			    info->state == media_connected)
+				linked = info;
+			continue;
+		}
+		sw->phy_intr &= ~(1 << p);
+
+		state = &sw->port_state[p];
+		port_r(sw, p, P_LOCAL_CTRL, &local);
+		port_r(sw, p, P_REMOTE_STATUS, &remote);
+		port_r(sw, p, P_SPEED_STATUS, &status);
+		port_r(sw, p, P_LINK_STATUS, &data);
+
+		/*
+		 * The partner capability register is updated but the
+		 * auto-negotiation is not completed yet.
+		 */
+		link = data & (PORT_AUTO_NEG_COMPLETE | PORT_STAT_LINK_GOOD);
+		link |= status &
+			(PORT_STAT_SPEED_100MBIT | PORT_STAT_FULL_DUPLEX);
+
+		if (data & PORT_STAT_LINK_GOOD) {
+
+			/* Remember the first linked port. */
+			if (!linked)
+				linked = info;
+		}
+
+		/* No change to status. */
+		if (local == info->advertised && link == info->link)
+			continue;
+
+#ifdef DBG_LINK
+		printk(KERN_INFO
+			"%d=advertised: %02X-%02X; partner: %02X-%02X"
+			"; link: %02X-%02X\n", p,
+			local, info->advertised, remote, info->partner,
+			link, info->link);
+#endif
+		if (data & PORT_STAT_LINK_GOOD) {
+			if (port_chk_force_link(sw, p, local, remote, status)) {
+				if (linked == info)
+					linked = NULL;
+				continue;
+			}
+			info->tx_rate = 10 * TX_RATE_UNIT;
+			if (status & PORT_STAT_SPEED_100MBIT)
+				info->tx_rate = 100 * TX_RATE_UNIT;
+
+			info->duplex = 1;
+			if (status & PORT_STAT_FULL_DUPLEX)
+				info->duplex = 2;
+
+#ifdef DBG_LINK
+			printk(KERN_INFO "flow_ctrl: "SW_SIZE_STR"\n", status &
+				(PORT_RX_FLOW_CTRL | PORT_TX_FLOW_CTRL));
+#endif
+			if (media_connected != info->state) {
+				info->flow_ctrl = sw_determine_flow_ctrl(sw,
+					port, local, remote);
+				if (status & PORT_RX_FLOW_CTRL)
+					info->flow_ctrl |= 0x10;
+				if (status & PORT_TX_FLOW_CTRL)
+					info->flow_ctrl |= 0x20;
+				if (sw->info)
+					port_cfg_back_pressure(sw, p,
+						(1 == info->duplex));
+				change |= 1 << i;
+			} else if (link != info->link)
+				change |= 1 << i;
+			info->state = media_connected;
+			sw_r_phy(sw, p + 1, PHY_REG_REMOTE_CAPABILITY,
+				 &info->lpa);
+		} else {
+			if (media_disconnected != info->state) {
+				change |= 1 << i;
+
+				/* Indicate the link just goes down. */
+				state->link_down = 1;
+			}
+			info->state = media_disconnected;
+		}
+		info->advertised = local;
+		info->partner = remote;
+		info->link = link;
+		state->state = info->state;
+	}
+
+	if (linked && media_disconnected == port->linked->state)
+		port->linked = linked;
+
+#ifdef DBG_LINK
+	if (change)
+		dbp_link(port, sw, change);
+#endif
+	if (change) {
+		port->report = true;
+		port->link_ports |= change;
+	}
+	return change;
+}  /* port_get_link_speed */
+
+/**
+ * port_set_link_speed - set port speed
+ * @port:	The port instance.
+ *
+ * This routine sets the link speed of the switch ports.
+ */
+static void port_set_link_speed(struct ksz_port *port)
+{
+	struct ksz_sw *sw = port->sw;
+	struct ksz_port_info *info;
+	SW_D adv;
+	SW_D cfg;
+	SW_D data;
+	SW_D local;
+	SW_D status;
+	uint i;
+	uint n;
+	uint p;
+
+	for (i = 0, n = port->first_port; i < port->port_cnt; i++, n++) {
+		p = get_phy_port(sw, n);
+		info = get_port_info(sw, p);
+		if (info->fiber)
+			continue;
+
+		info->own_flow_ctrl = port->flow_ctrl;
+		info->own_duplex = port->duplex;
+		info->own_speed = port->speed;
+
+		port_r(sw, p, P_FORCE_CTRL, &data);
+		port_r(sw, p, P_LOCAL_CTRL, &local);
+		port_r(sw, p, P_LINK_STATUS, &status);
+
+		adv = 0;
+		cfg = 0;
+		if (status & PORT_STAT_LINK_GOOD) {
+			cfg = data;
+			adv = local;
+		}
+
+		data &= ~PORT_AUTO_NEG_DISABLE;
+		local = port_advertised_flow_ctrl(port, local);
+
+		local |= PORT_AUTO_NEG_100BTX_FD | PORT_AUTO_NEG_100BTX |
+			PORT_AUTO_NEG_10BT_FD | PORT_AUTO_NEG_10BT;
+
+		/* Check if manual configuration is specified by the user. */
+		if (port->speed || port->duplex) {
+			if (10 == port->speed)
+				local &= ~(PORT_AUTO_NEG_100BTX_FD |
+					PORT_AUTO_NEG_100BTX);
+			else if (100 == port->speed)
+				local &= ~(PORT_AUTO_NEG_10BT_FD |
+					PORT_AUTO_NEG_10BT);
+			if (1 == port->duplex)
+				local &= ~(PORT_AUTO_NEG_100BTX_FD |
+					PORT_AUTO_NEG_10BT_FD);
+			else if (2 == port->duplex)
+				local &= ~(PORT_AUTO_NEG_100BTX |
+					PORT_AUTO_NEG_10BT);
+		}
+		if (data != cfg || local != adv) {
+			port_w(sw, p, P_FORCE_CTRL, data);
+			port_w(sw, p, P_LOCAL_CTRL, local);
+			port_r(sw, p, P_NEG_RESTART_CTRL, &data);
+			data |= PORT_AUTO_NEG_RESTART;
+			port_w(sw, p, P_NEG_RESTART_CTRL, data);
+
+			/* Link is going down. */
+			sw->port_state[p].state = media_disconnected;
+		}
+	}
+}  /* port_set_link_speed */
+
+/**
+ * port_force_link_speed - force port speed
+ * @port:	The port instance.
+ *
+ * This routine forces the link speed of the switch ports.
+ */
+static void port_force_link_speed(struct ksz_port *port)
+{
+	struct ksz_sw *sw = port->sw;
+	struct ksz_port_info *info;
+	SW_D data;
+	uint i;
+	uint n;
+	uint p;
+
+	for (i = 0, n = port->first_port; i < port->port_cnt; i++, n++) {
+		p = get_phy_port(sw, n);
+		info = get_port_info(sw, p);
+		info->own_flow_ctrl = port->flow_ctrl;
+		info->own_duplex = port->duplex;
+		info->own_speed = port->speed;
+
+		port_r(sw, p, P_FORCE_CTRL, &data);
+		data |= PORT_AUTO_NEG_DISABLE;
+		if (10 == port->speed)
+			data &= ~PORT_FORCE_100_MBIT;
+		else if (100 == port->speed)
+			data |= PORT_FORCE_100_MBIT;
+		if (1 == port->duplex)
+			data &= ~PORT_FORCE_FULL_DUPLEX;
+		else if (2 == port->duplex)
+			data |= PORT_FORCE_FULL_DUPLEX;
+		port_w(sw, p, P_FORCE_CTRL, data);
+	}
+}  /* port_force_link_speed */
+
+/**
+ * sw_enable - enable the switch
+ * @sw:		The switch instance.
+ *
+ */
+static void sw_enable(struct ksz_sw *sw)
+{
+	uint n;
+	uint port;
+	bool fewer;
+	struct ksz_port_cfg *cfg;
+	struct ksz_port_info *info;
+	int state = STP_STATE_FORWARDING;
+
+	/* Manually change default membership when not all ports are used. */
+	fewer = false;
+	for (port = 0; port < sw->port_cnt; port++) {
+		info = get_port_info(sw, port);
+		if (info->log_m) {
+			cfg = get_port_cfg(sw, port);
+			cfg->vid_member = sw->PORT_MASK;
+		} else {
+			fewer = true;
+			port_set_stp_state(sw, port, STP_STATE_DISABLED);
+		}
+	}
+	if (fewer)
+dbg_msg(" fewer: %d %d\n", fewer, sw->eth_cnt);
+	if (fewer)
+		sw_cfg_port_base_vlan(sw, sw->HOST_PORT, sw->PORT_MASK);
+	if ((sw->dev_count > 1 && !sw->dev_offset) ||
+	    (sw->features & STP_SUPPORT)) {
+		u8 member;
+
+		for (n = 1; n <= sw->mib_port_cnt; n++) {
+			port = get_phy_port(sw, n);
+			member = (1 << port);
+			if (sw->features & SW_VLAN_DEV) {
+				struct ksz_dev_map *map;
+				int q;
+
+				for (q = 0; q < sw->eth_cnt; q++) {
+					map = &sw->eth_maps[q];
+					if (map->first <= n &&
+					    n <= map->first + map->cnt - 1) {
+						member = map->mask;
+						break;
+					}
+				}
+			}
+			cfg = get_port_cfg(sw, port);
+			cfg->vid_member = member;
+		}
+	} else if (1 == sw->eth_cnt) {
+		u8 member;
+
+		for (n = 1; n <= sw->mib_port_cnt; n++) {
+			port = get_phy_port(sw, n);
+			member = 0;
+			if (sw->eth_maps[0].mask & (1 << port))
+				member = sw->eth_maps[0].mask;
+			cfg = get_port_cfg(sw, port);
+			cfg->vid_member = member;
+		}
+	}
+	for (n = 1; n <= sw->mib_port_cnt; n++) {
+		port = get_phy_port(sw, n);
+		if (sw->dev_count > 1 ||
+		    (sw->eth_maps[0].mask &&
+		    !(sw->eth_maps[0].mask & (1 << port))))
+			port_set_stp_state(sw, port, STP_STATE_DISABLED);
+		else if ((sw->eth_maps[0].mask & (1 << port)) &&
+			 (sw->eth_maps[0].proto & HSR_HW))
+			port_set_stp_state(sw, port, STP_STATE_SIMPLE);
+		else
+			port_set_stp_state(sw, port, state);
+	}
+	if (sw->dev_count > 1 && !sw->dev_offset && sw->eth_cnt < 2)
+		port_set_stp_state(sw, sw->HOST_PORT, STP_STATE_SIMPLE);
+	else
+		port_set_stp_state(sw, sw->HOST_PORT, state);
+
+	/*
+	 * There may be some entries in the dynamic MAC table before the
+	 * the learning is turned off.  Once the entries are in the table the
+	 * switch may keep updating them even learning is off.
+	 */
+	if (sw->dev_count > 1)
+		sw_flush_dyn_mac_table(sw, TOTAL_PORT_NUM);
+}  /* sw_enable */
+
+/**
+ * sw_init - initialize the switch
+ * @sw:		The switch instance.
+ *
+ * This routine initializes the hardware switch engine for default operation.
+ */
+static void sw_init(struct ksz_sw *sw)
+{
+	memset(sw->tx_pad, 0, 60);
+	sw->tx_start = 0;
+	sw_init_broad_storm(sw);
+
+	sw_init_prio(sw);
+
+	sw_init_prio_rate(sw);
+
+	sw_init_vlan(sw);
+
+	sw_init_acl(sw);
+
+	if (sw_chk(sw, REG_SW_CTRL_1,
+			SW_TX_FLOW_CTRL_DISABLE | SW_RX_FLOW_CTRL_DISABLE))
+		sw->overrides |= PAUSE_FLOW_CTRL;
+}  /* sw_init */
+
+/**
+ * sw_setup - setup the switch
+ * @sw:		The switch instance.
+ *
+ * This routine setup the hardware switch engine for default operation.
+ */
+static void sw_setup(struct ksz_sw *sw)
+{
+	uint n;
+	uint port;
+
+	sw_set_global_ctrl(sw);
+	for (n = 1; n <= sw->mib_port_cnt; n++) {
+		SW_D data;
+
+		port = get_phy_port(sw, n);
+		port_cfg_back_pressure(sw, port, 1);
+
+		/*
+		 * Switch actually cannot do auto-negotiation with old 10Mbit
+		 * hub.
+		 */
+		port_r(sw, port, P_FORCE_CTRL, &data);
+		if (sw->port_info[port].fiber) {
+			port_cfg_force_flow_ctrl(sw, port, 1);
+			data |= PORT_AUTO_NEG_DISABLE;
+			data |= PORT_FORCE_FULL_DUPLEX;
+		} else {
+			port_cfg_force_flow_ctrl(sw, port, 0);
+			data &= ~PORT_FORCE_FULL_DUPLEX;
+		}
+		port_w(sw, port, P_FORCE_CTRL, data);
+	}
+
+	sw_setup_broad_storm(sw);
+
+	sw_setup_prio(sw);
+
+	sw_setup_mirror(sw);
+
+	sw->info->multi_sys = MULTI_MAC_TABLE_ENTRIES;
+	sw->info->multi_net = SWITCH_MAC_TABLE_ENTRIES;
+	if (sw->features & STP_SUPPORT) {
+		sw->ops->release(sw);
+		sw_setup_stp(sw);
+		sw->ops->acquire(sw);
+	}
+#ifdef CONFIG_KSZ_DLR
+	if (sw->features & DLR_HW)
+		sw_setup_dlr(sw);
+#endif
+}  /* sw_setup */
+
+static void sw_reset_acl_all(struct ksz_sw *sw)
+{
+	uint i;
+	uint n;
+
+	for (n = 0; n <= sw->mib_port_cnt; n++) {
+		i = get_phy_port(sw, n);
+		sw->info->port_cfg[i].acl_byte_enable = ACL_BYTE_ENABLE;
+		sw_w_ext_table(sw, TABLE_ACL, i + 1,
+			REG_PORT_ACL_BYTE_EN_MSB, ACL_BYTE_EN_MSB_M);
+		sw_w_ext_table(sw, TABLE_ACL, i + 1,
+			REG_PORT_ACL_BYTE_EN_LSB, 0xFF);
+	}
+	sw_reset_acl_hw(sw);
+}  /* sw_reset_acl_all */
+
+static inline void sw_reset(struct ksz_sw *sw)
+{
+	uint i;
+
+	sw->reg->w8(sw, REG_POWER_MANAGEMENT_1,
+		SW_SOFTWARE_POWER_DOWN << SW_POWER_MANAGEMENT_MODE_S);
+	sw->reg->w8(sw, REG_POWER_MANAGEMENT_1, 0);
+
+	/* Need to enable ACL for better reliability. */
+	for (i = 0; i < sw->port_cnt; i++)
+		port_w8(sw, i, REG_PORT_CTRL_5, PORT_ACL_ENABLE);
+
+	sw_reset_acl_all(sw);
+}  /* sw_reset */
+
+/* -------------------------------------------------------------------------- */
+
+#define KSZSW_REGS_SIZE			0x100
+
+static struct sw_regs {
+	int start;
+	int end;
+} sw_regs_range[] = {
+	{ 0x000, 0x100 },
+	{ 0, 0 }
+};
+
+static int check_sw_reg_range(unsigned addr)
+{
+	struct sw_regs *range = sw_regs_range;
+
+	while (range->end > range->start) {
+		if (range->start <= addr && addr < range->end)
+			return true;
+		range++;
+	}
+	return false;
+}
+
+/* -------------------------------------------------------------------------- */
+
+#ifdef KSZSW_REGS_SIZE
+static struct ksz_sw *get_sw_data(struct device *d)
+{
+	struct sw_priv *hw_priv = dev_get_drvdata(d);
+
+	return &hw_priv->sw;
+}
+
+static ssize_t kszsw_registers_read(struct file *filp, struct kobject *kobj,
+	struct bin_attribute *bin_attr, char *buf, loff_t off, size_t count)
+{
+	size_t i;
+	SW_D reg;
+	struct device *dev;
+	struct ksz_sw *sw;
+
+	if (unlikely(off > KSZSW_REGS_SIZE))
+		return 0;
+
+	if ((off + count) > KSZSW_REGS_SIZE)
+		count = KSZSW_REGS_SIZE - off;
+
+	if (unlikely(!count))
+		return count;
+
+	dev = container_of(kobj, struct device, kobj);
+	sw = get_sw_data(dev);
+
+	reg = off;
+	sw->ops->acquire(sw);
+	i = sw->reg->get(sw, reg, count, buf);
+	sw->ops->release(sw);
+
+	/* Linux 5.10 return -1 to application if sizes do not match. */
+	if (i > count)
+		i = count;
+	return i;
+}
+
+static ssize_t kszsw_registers_write(struct file *filp, struct kobject *kobj,
+	struct bin_attribute *bin_attr, char *buf, loff_t off, size_t count)
+{
+	size_t i;
+	SW_D reg;
+	struct device *dev;
+	struct ksz_sw *sw;
+
+	if (unlikely(off >= KSZSW_REGS_SIZE))
+		return -EFBIG;
+
+	if ((off + count) > KSZSW_REGS_SIZE)
+		count = KSZSW_REGS_SIZE - off;
+
+	if (unlikely(!count))
+		return count;
+
+	dev = container_of(kobj, struct device, kobj);
+	sw = get_sw_data(dev);
+
+	reg = off;
+	sw->ops->acquire(sw);
+	i = sw->reg->set(sw, reg, count, buf);
+	sw->ops->release(sw);
+	return i;
+}
+
+static struct bin_attribute kszsw_registers_attr = {
+	.attr = {
+		.name	= "registers",
+		.mode	= S_IRUSR | S_IWUSR,
+	},
+	.size	= KSZSW_REGS_SIZE,
+	.read	= kszsw_registers_read,
+	.write	= kszsw_registers_write,
+};
+#endif
+
+static int sw_reg_get(struct ksz_sw *sw, u32 reg, size_t count, char *buf)
+{
+	size_t i;
+	SW_D *addr;
+
+	addr = (SW_D *) buf;
+	for (i = 0; i < count; i += SW_SIZE, reg += SW_SIZE, addr++) {
+		*addr = 0;
+		if (check_sw_reg_range(reg))
+			*addr = SW_R(sw, reg);
+	}
+	return i;
+}
+
+static int sw_reg_set(struct ksz_sw *sw, u32 reg, size_t count, char *buf)
+{
+	size_t i;
+	SW_D *addr;
+
+	addr = (SW_D *) buf;
+	for (i = 0; i < count; i += SW_SIZE, reg += SW_SIZE, addr++) {
+		if (check_sw_reg_range(reg))
+			SW_W(sw, reg, *addr);
+	}
+	return i;
+}
+
+#ifdef CONFIG_KSZ_MRP
+static void sw_set_mrp(struct work_struct *work)
+{
+	struct delayed_work *dwork = to_delayed_work(work);
+	struct ksz_sw *sw = container_of(dwork, struct ksz_sw, set_mrp);
+
+	if (!netif_carrier_ok(sw->main_dev)) {
+		schedule_delayed_work(&sw->set_mrp, msecs_to_jiffies(500));
+		return;
+	}
+
+	mrp_start(&sw->mrp);
+}  /* sw_set_mrp */
+#endif
+
+/* -------------------------------------------------------------------------- */
+
+/*
+ * Microchip LinkMD routines
+ */
+
+enum {
+	CABLE_UNKNOWN,
+	CABLE_GOOD,
+	CABLE_CROSSED,
+	CABLE_REVERSED,
+	CABLE_CROSSED_REVERSED,
+	CABLE_OPEN,
+	CABLE_SHORT
+};
+
+#define STATUS_FULL_DUPLEX		0x01
+#define STATUS_CROSSOVER		0x02
+#define STATUS_REVERSED			0x04
+
+#define LINK_10MBPS_FULL		0x00000001
+#define LINK_10MBPS_HALF		0x00000002
+#define LINK_100MBPS_FULL		0x00000004
+#define LINK_100MBPS_HALF		0x00000008
+#define LINK_1GBPS_FULL			0x00000010
+#define LINK_1GBPS_HALF			0x00000020
+#define LINK_10GBPS_FULL		0x00000040
+#define LINK_10GBPS_HALF		0x00000080
+#define LINK_SYM_PAUSE			0x00000100
+#define LINK_ASYM_PAUSE			0x00000200
+
+#define LINK_AUTO_MDIX			0x00010000
+#define LINK_MDIX			0x00020000
+#define LINK_AUTO_POLARITY		0x00040000
+
+#define CABLE_LEN_MAXIMUM		15000
+#define CABLE_LEN_MULTIPLIER		40
+
+#define PHY_RESET_TIMEOUT		10
+
+/**
+ * sw_get_link_md - get LinkMD status
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ *
+ * This routine is used to get the LinkMD status.
+ */
+static void sw_get_link_md(struct ksz_sw *sw, uint port)
+{
+	SW_D crossover;
+	SW_D data;
+	SW_D link;
+	u16 len;
+	int i;
+	int timeout;
+	struct ksz_port_info *port_info = get_port_info(sw, port);
+
+	if (port_info->fiber)
+		return;
+	port_r(sw, port, P_SPEED_STATUS, &data);
+	port_r(sw, port, P_LINK_STATUS, &link);
+	port_info->status[0] = CABLE_UNKNOWN;
+	if (link & PORT_STAT_LINK_GOOD) {
+		int stat = 0;
+
+		port_info->status[0] = CABLE_GOOD;
+		port_info->length[0] = 1;
+		port_info->status[1] = CABLE_GOOD;
+		port_info->length[1] = 1;
+		port_info->status[2] = CABLE_GOOD;
+		port_info->length[2] = 1;
+
+		if (link & PORT_MDIX_STATUS)
+			stat |= STATUS_CROSSOVER;
+		if (data & PORT_REVERSED_POLARITY)
+			stat |= STATUS_REVERSED;
+		if ((stat & (STATUS_CROSSOVER | STATUS_REVERSED)) ==
+				(STATUS_CROSSOVER | STATUS_REVERSED))
+			port_info->status[0] = CABLE_CROSSED_REVERSED;
+		else if ((stat & STATUS_CROSSOVER) == STATUS_CROSSOVER)
+			port_info->status[0] = CABLE_CROSSED;
+		else if ((stat & STATUS_REVERSED) == STATUS_REVERSED)
+			port_info->status[0] = CABLE_REVERSED;
+		return;
+	}
+
+#if 0
+	sw_w_ext_table(sw, TABLE_LINK_MD, 0, 0x4D, 0x80);
+#endif
+
+	port_r(sw, port, P_NEG_RESTART_CTRL, &data);
+	crossover = data;
+
+	for (i = 1; i <= 2; i++) {
+		data = crossover;
+
+		/* Disable auto MDIX. */
+		data |= PORT_AUTO_MDIX_DISABLE;
+		if (0 == i)
+			data &= ~PORT_FORCE_MDIX;
+		else
+			data |= PORT_FORCE_MDIX;
+
+		port_w(sw, port, P_NEG_RESTART_CTRL, data);
+
+		/* Start cable diagnostic test. */
+		port_r(sw, port, REG_PORT_LINK_MD_CTRL, &data);
+		data |= PORT_START_CABLE_DIAG;
+		port_w(sw, port, REG_PORT_LINK_MD_CTRL, data);
+		timeout = PHY_RESET_TIMEOUT;
+		do {
+			if (!--timeout)
+				break;
+			delay_milli(10);
+			port_r(sw, port, REG_PORT_LINK_MD_CTRL, &data);
+		} while ((data & PORT_START_CABLE_DIAG));
+
+		port_info->length[i] = 0;
+		port_info->status[i] = CABLE_UNKNOWN;
+
+		if (!(data & PORT_START_CABLE_DIAG)) {
+			port_r8(sw, port, REG_PORT_LINK_MD_RESULT, &link);
+			len = data & PORT_CABLE_FAULT_COUNTER_H;
+			len <<= 16;
+			len |= link;
+			len *= CABLE_LEN_MULTIPLIER;
+			len /= 100;
+			port_info->length[i] = len;
+			if (data & PORT_CABLE_10M_SHORT)
+				port_info->length[i] = 1;
+			data >>= PORT_CABLE_DIAG_RESULT_S;
+			data &= PORT_CABLE_DIAG_RESULT_M;
+			switch (data) {
+			case PORT_CABLE_STAT_NORMAL:
+				port_info->status[i] = CABLE_GOOD;
+				port_info->length[i] = 1;
+				break;
+			case PORT_CABLE_STAT_OPEN:
+				port_info->status[i] = CABLE_OPEN;
+				break;
+			case PORT_CABLE_STAT_SHORT:
+				port_info->status[i] = CABLE_SHORT;
+				break;
+			}
+		}
+	}
+
+	crossover |= PORT_AUTO_NEG_RESTART;
+	port_w(sw, port, P_NEG_RESTART_CTRL, crossover);
+
+	port_info->length[0] = port_info->length[1];
+	port_info->status[0] = port_info->status[1];
+	for (i = 2; i < 3; i++) {
+		if (CABLE_GOOD == port_info->status[0]) {
+			if (port_info->status[i] != CABLE_GOOD) {
+				port_info->status[0] = port_info->status[i];
+				port_info->length[0] = port_info->length[i];
+				break;
+			}
+		}
+	}
+}  /* sw_get_link_md */
+
+/* -------------------------------------------------------------------------- */
+
+static void get_sw_mib_counters(struct ksz_sw *sw, int first, int cnt,
+	u64 *counter)
+{
+	int i;
+	int mib;
+	uint n;
+	uint p;
+	struct ksz_port_mib *port_mib;
+
+	memset(counter, 0, sizeof(u64) * TOTAL_SWITCH_COUNTER_NUM);
+	for (i = 0, n = first; i < cnt; i++, n++) {
+		p = get_phy_port(sw, n);
+		port_mib = get_port_mib(sw, p);
+		for (mib = port_mib->mib_start; mib < sw->mib_cnt; mib++)
+			counter[mib] += port_mib->counter[mib];
+	}
+}
+
+#define MIB_RX_HI_PRIO			0x00
+#define MIB_RX_UNDERSIZE		0x01
+#define MIB_RX_FRAGMENT			0x02
+#define MIB_RX_OVERSIZE			0x03
+#define MIB_RX_JABBER			0x04
+#define MIB_RX_SYMBOL_ERR		0x05
+#define MIB_RX_CRC_ERR			0x06
+#define MIB_RX_ALIGNMENT_ERR		0x07
+#define MIB_RX_CTRL_8808		0x08
+#define MIB_RX_PAUSE			0x09
+#define MIB_RX_BROADCAST		0x0A
+#define MIB_RX_MULTICAST		0x0B
+#define MIB_RX_UNICAST			0x0C
+#define MIB_RX_OCTET_64			0x0D
+#define MIB_RX_OCTET_65_127		0x0E
+#define MIB_RX_OCTET_128_255		0x0F
+#define MIB_RX_OCTET_256_511		0x10
+#define MIB_RX_OCTET_512_1023		0x11
+#define MIB_RX_OCTET_1024_1522		0x12
+#define MIB_RX_OCTET_1523_2000		0x13
+#define MIB_RX_OCTET_2001		0x14
+#define MIB_TX_HI_PRIO			0x15
+#define MIB_TX_LATE_COLLISION		0x16
+#define MIB_TX_PAUSE			0x17
+#define MIB_TX_BROADCAST		0x18
+#define MIB_TX_MULTICAST		0x19
+#define MIB_TX_UNICAST			0x1A
+#define MIB_TX_DEFERRED			0x1B
+#define MIB_TX_TOTAL_COLLISION		0x1C
+#define MIB_TX_EXCESS_COLLISION		0x1D
+#define MIB_TX_SINGLE_COLLISION		0x1E
+#define MIB_TX_MULTI_COLLISION		0x1F
+
+#define MIB_RX_TOTAL			0x20
+#define MIB_TX_TOTAL			0x21
+#define MIB_RX_DROPS			0x22
+#define MIB_TX_DROPS			0x23
+
+static struct {
+	char string[20];
+} mib_names[TOTAL_SWITCH_COUNTER_NUM] = {
+	{ "rx_hi        " },
+	{ "rx_undersize" },
+	{ "rx_fragments" },
+	{ "rx_oversize" },
+	{ "rx_jabbers" },
+	{ "rx_symbol_err" },
+	{ "rx_crc_err" },
+	{ "rx_align_err" },
+	{ "rx_mac_ctrl" },
+	{ "rx_pause" },
+	{ "rx_bcast" },
+	{ "rx_mcast" },
+	{ "rx_ucast" },
+	{ "rx_64_or_less" },
+	{ "rx_65_127" },
+	{ "rx_128_255" },
+	{ "rx_256_511" },
+	{ "rx_512_1023" },
+	{ "rx_1024_1522" },
+	{ "rx_1523_2000" },
+	{ "rx_2001     " },
+
+	{ "tx_hi        " },
+	{ "tx_late_col" },
+	{ "tx_pause" },
+	{ "tx_bcast" },
+	{ "tx_mcast" },
+	{ "tx_ucast" },
+	{ "tx_deferred" },
+	{ "tx_total_col" },
+	{ "tx_exc_col" },
+	{ "tx_single_col" },
+	{ "tx_mult_col" },
+
+	{ "rx_total" },
+	{ "tx_total" },
+	{ "rx_discards" },
+	{ "tx_discards" },
+};
+
+static struct {
+	int rx;
+	int tx;
+} mib_display[TOTAL_SWITCH_COUNTER_NUM / 2] = {
+	{ MIB_RX_TOTAL, MIB_TX_TOTAL },
+	{ MIB_RX_HI_PRIO, MIB_TX_HI_PRIO },
+	{ MIB_RX_PAUSE, MIB_TX_PAUSE },
+	{ MIB_RX_BROADCAST, MIB_TX_BROADCAST },
+	{ MIB_RX_MULTICAST, MIB_TX_MULTICAST },
+	{ MIB_RX_UNICAST, MIB_TX_UNICAST },
+	{ MIB_RX_DROPS, MIB_TX_DROPS },
+	{ MIB_RX_OCTET_64, MIB_RX_OCTET_65_127 },
+	{ MIB_RX_OCTET_128_255, MIB_RX_OCTET_256_511 },
+	{ MIB_RX_OCTET_512_1023, MIB_RX_OCTET_1024_1522 },
+	{ MIB_RX_OCTET_1523_2000, MIB_RX_OCTET_2001 },
+	{ MIB_RX_UNDERSIZE, MIB_RX_OVERSIZE },
+	{ MIB_RX_FRAGMENT, MIB_RX_JABBER },
+	{ MIB_RX_SYMBOL_ERR, MIB_RX_CRC_ERR },
+	{ MIB_RX_ALIGNMENT_ERR, MIB_RX_CTRL_8808 },
+	{ MIB_TX_LATE_COLLISION, MIB_TX_DEFERRED },
+	{ MIB_TX_TOTAL_COLLISION, MIB_TX_EXCESS_COLLISION },
+	{ MIB_TX_SINGLE_COLLISION, MIB_TX_MULTI_COLLISION },
+};
+
+static int display_sw_mib_counters(struct ksz_sw *sw, int first, int cnt,
+	char *buf)
+{
+	int mib;
+	int n;
+	int len = 0;
+	u64 counter[TOTAL_SWITCH_COUNTER_NUM];
+
+	get_sw_mib_counters(sw, first, cnt, counter);
+	for (mib = 0; mib < TOTAL_SWITCH_COUNTER_NUM / 2; mib++) {
+		int rx = mib_display[mib].rx;
+		int tx = mib_display[mib].tx;
+		if (buf)
+			len += sprintf(buf + len,
+				"%s\t= %-20llu\t%s\t= %-20llu\n",
+				mib_names[rx].string,
+				counter[rx],
+				mib_names[tx].string,
+				counter[tx]);
+		else
+			printk(KERN_INFO "%s\t= %-20llu\t%s\t= %-20llu\n",
+				mib_names[rx].string,
+				counter[rx],
+				mib_names[tx].string,
+				counter[tx]);
+	}
+	for (n = 0, mib = first; n < cnt; n++, mib++) {
+		int j;
+		uint p = get_phy_port(sw, mib);
+		struct ksz_port_mib *port_mib = get_port_mib(sw, p);
+
+		for (j = 0; j < 2; j++) {
+			if (port_mib->rate[j].peak) {
+				u32 num;
+				u32 frac;
+
+				num = port_mib->rate[j].peak / 10;
+				frac = port_mib->rate[j].peak % 10;
+				if (buf)
+					len += sprintf(buf + len,
+						"%d:%d=%u.%u\n", mib, j,
+						num, frac);
+				else
+					printk(KERN_INFO
+						"%d:%d=%u.%u\n", mib, j,
+						num, frac);
+				port_mib->rate[j].peak = 0;
+			}
+		}
+	}
+	return len;
+}  /* display_sw_mib_counters */
+
+/* -------------------------------------------------------------------------- */
+
+static ssize_t display_sw_info(int cnt, char *buf, ssize_t len)
+{
+	len += sprintf(buf + len, "duplex:\t\t");
+	len += sprintf(buf + len, "0 for auto; ");
+	len += sprintf(buf + len,
+		"set to 1 for half-duplex; 2, full-duplex\n");
+	len += sprintf(buf + len, "speed:\t\t");
+	len += sprintf(buf + len,
+		"0 for auto; set to 10 or 100\n");
+	len += sprintf(buf + len, "force:\t\t");
+	len += sprintf(buf + len,
+		"set to 1 to force link to specific speed setting\n");
+	len += sprintf(buf + len, "flow_ctrl:\t");
+	len += sprintf(buf + len,
+		"set to 0 to disable flow control\n");
+	len += sprintf(buf + len, "mib:\t\t");
+	len += sprintf(buf + len,
+		"display the MIB table\n");
+	printk(KERN_INFO "%s", buf);
+	len = 0;
+
+	if (TOTAL_PORT_NUM != cnt)
+		return len;
+
+	len += sprintf(buf + len, "\ndynamic_table:\t");
+	len += sprintf(buf + len,
+		"display the switch's dynamic MAC table\n");
+	len += sprintf(buf + len, "static_table:\t");
+	len += sprintf(buf + len,
+		"display the switch's static MAC table\n");
+	len += sprintf(buf + len, "vlan_table:\t");
+	len += sprintf(buf + len,
+		"display the switch's VLAN table\n");
+
+	len += sprintf(buf + len, "\naging:\t\t");
+	len += sprintf(buf + len,
+		"disable/enable aging\n");
+	len += sprintf(buf + len, "fast_aging:\t");
+	len += sprintf(buf + len,
+		"disable/enable fast aging\n");
+	len += sprintf(buf + len, "link_aging:\t");
+	len += sprintf(buf + len,
+		"disable/enable link change auto aging\n");
+
+	len += sprintf(buf + len, "\nbcast_per:\t");
+	len += sprintf(buf + len,
+		"set broadcast storm percentage\n");
+	len += sprintf(buf + len, "mcast_storm:\t");
+	len += sprintf(buf + len,
+		"disable multicast storm protection\n");
+	len += sprintf(buf + len, "diffserv_map:\t");
+	len += sprintf(buf + len,
+		"set DiffServ value.  Use \"decimal=hexadecimal\" format\n");
+	len += sprintf(buf + len, "p_802_1p_map:\t");
+	len += sprintf(buf + len,
+		"set 802.1p value.  Use \"decimal=hexadecimal\" format\n");
+	printk(KERN_INFO "%s", buf);
+	len = 0;
+
+	len += sprintf(buf + len, "\nvlan:\t\t");
+	len += sprintf(buf + len,
+		"disable/enable 802.1Q VLAN\n");
+	len += sprintf(buf + len, "null_vid:\t");
+	len += sprintf(buf + len,
+		"set to 1 to replace null vid\n");
+	len += sprintf(buf + len, "macaddr:\t");
+	len += sprintf(buf + len,
+		"set switch MAC address\n");
+	len += sprintf(buf + len, "mirror_mode:\t");
+	len += sprintf(buf + len,
+		"set to 1 to use mirror rx AND tx mode\n");
+	len += sprintf(buf + len, "tail_tag:\t");
+	len += sprintf(buf + len,
+		"disable/enable tail tagging\n");
+
+	len += sprintf(buf + len, "\nigmp_snoop:\t");
+	len += sprintf(buf + len,
+		"disable/enable IGMP snooping\n");
+	len += sprintf(buf + len, "ipv6_mld_snoop:\t");
+	len += sprintf(buf + len,
+		"disable/enable IPv6 MLD snooping\n");
+	len += sprintf(buf + len, "ipv6_mld_option:");
+	len += sprintf(buf + len,
+		"disable/enable IPv6 MLD option snooping\n");
+
+	len += sprintf(buf + len, "\naggr_backoff:\t");
+	len += sprintf(buf + len,
+		"disable/enable aggressive backoff in half-duplex mode\n");
+	len += sprintf(buf + len, "no_exc_drop:\t");
+	len += sprintf(buf + len,
+		"disable/enable no excessive collision drop\n");
+	len += sprintf(buf + len, "buf_reserve:\t");
+	len += sprintf(buf + len,
+		"disable/enable buffer reserve\n");
+	printk(KERN_INFO "%s", buf);
+	len = 0;
+
+	len += sprintf(buf + len, "\nhuge_packet:\t");
+	len += sprintf(buf + len,
+		"disable/enable huge packet support\n");
+	len += sprintf(buf + len, "legal_packet:\t");
+	len += sprintf(buf + len,
+		"disable/enable legal packet\n");
+	len += sprintf(buf + len, "length_check:\t");
+	len += sprintf(buf + len,
+		"disable/enable legal packet length check\n");
+
+	len += sprintf(buf + len, "\nback_pressure:\t");
+	len += sprintf(buf + len,
+		"set back pressure mode\n");
+	len += sprintf(buf + len, "sw_flow_ctrl:\t");
+	len += sprintf(buf + len,
+		"disable/enable switch host port flow control\n");
+	len += sprintf(buf + len, "sw_half_duplex:\t");
+	len += sprintf(buf + len,
+		"disable/enable switch host port half-duplex mode\n");
+#ifdef SW_10_MBIT
+	len += sprintf(buf + len, "sw_10_mbit:\t");
+	len += sprintf(buf + len,
+		"disable/enable switch host port 10Mbit mode\n");
+#endif
+	len += sprintf(buf + len, "rx_flow_ctrl:\t");
+	len += sprintf(buf + len,
+		"disable/enable receive flow control\n");
+	len += sprintf(buf + len, "tx_flow_ctrl:\t");
+	len += sprintf(buf + len,
+		"disable/enable transmit flow control\n");
+	len += sprintf(buf + len, "fair_flow_ctrl:\t");
+	len += sprintf(buf + len,
+		"disable/enable fair flow control mode\n");
+	printk(KERN_INFO "%s", buf);
+	len = 0;
+
+	len += sprintf(buf + len, "vlan_bound:\t");
+	len += sprintf(buf + len,
+		"disable/enable unicast VLAN boundary\n");
+	len += sprintf(buf + len, "fw_unk_dest:\t");
+	len += sprintf(buf + len,
+		"disable/enable unknown destination address forwarding\n");
+
+	len += sprintf(buf + len, "\nins_tag_0_1:\t");
+	len += sprintf(buf + len,
+		"set to 1 to insert tag from port 1 to 2\n");
+	len += sprintf(buf + len, "ins_tag_0_2:\t");
+	len += sprintf(buf + len,
+		"set to 1 to insert tag from port 1 to 3\n");
+	len += sprintf(buf + len, "ins_tag_1_0:\t");
+	len += sprintf(buf + len,
+		"set to 1 to insert tag from port 2 to 1\n");
+	len += sprintf(buf + len, "ins_tag_1_2:\t");
+	len += sprintf(buf + len,
+		"set to 1 to insert tag from port 2 to 3\n");
+	len += sprintf(buf + len, "ins_tag_2_0:\t");
+	len += sprintf(buf + len,
+		"set to 1 to insert tag from port 3 to 1\n");
+	len += sprintf(buf + len, "ins_tag_2_1:\t");
+	len += sprintf(buf + len,
+		"set to 1 to insert tag from port 3 to 2\n");
+
+	len += sprintf(buf + len, "\npass_all:\t");
+	len += sprintf(buf + len,
+		"set to 1 to pass all frames for debugging\n");
+	len += sprintf(buf + len, "pass_pause:\t");
+	len += sprintf(buf + len,
+		"set to 1 to pass PAUSE frames for debugging\n");
+	printk(KERN_INFO "%s", buf);
+	len = 0;
+
+	len += sprintf(buf + len, "\nswitch port settings:\n");
+	len += sprintf(buf + len, "duplex:\t\t");
+	len += sprintf(buf + len,
+		"display the port's duplex setting\n");
+	len += sprintf(buf + len, "speed:\t\t");
+	len += sprintf(buf + len,
+		"display the port's link speed\n");
+	len += sprintf(buf + len, "linkmd:\t\t");
+	len += sprintf(buf + len,
+		"write to start LinkMD test.  read for result\n");
+	len += sprintf(buf + len, "mib:\t\t");
+	len += sprintf(buf + len,
+		"display the port's MIB table\n");
+	printk(KERN_INFO "%s", buf);
+	len = 0;
+
+	len += sprintf(buf + len, "vid:\t\t");
+	len += sprintf(buf + len,
+		"set default VID value\n");
+	len += sprintf(buf + len, "member:\t\t");
+	len += sprintf(buf + len,
+		"set VLAN membership\n");
+
+	len += sprintf(buf + len, "bcast_storm:\t");
+	len += sprintf(buf + len,
+		"disable/enable broadcast storm protection\n");
+	len += sprintf(buf + len, "diffserv:\t");
+	len += sprintf(buf + len,
+		"disable/enable DiffServ priority\n");
+	len += sprintf(buf + len, "p_802_1p:\t");
+	len += sprintf(buf + len,
+		"disable/enable 802.1p priority\n");
+
+	len += sprintf(buf + len, "port_based:\t");
+	len += sprintf(buf + len,
+		"disable/enable port-based priority\n");
+	printk(KERN_INFO "%s", buf);
+	len = 0;
+
+	len += sprintf(buf + len, "prio_queue:\t");
+	len += sprintf(buf + len,
+		"disable/enable priority queue\n");
+	len += sprintf(buf + len, "tx_p0_ctrl:\t");
+	len += sprintf(buf + len,
+		"set priority queue 0 control\n");
+	len += sprintf(buf + len, "tx_p1_ctrl:\t");
+	len += sprintf(buf + len,
+		"set priority queue 1 control\n");
+	len += sprintf(buf + len, "tx_p2_ctrl:\t");
+	len += sprintf(buf + len,
+		"set priority queue 2 control\n");
+	len += sprintf(buf + len, "tx_p3_ctrl:\t");
+	len += sprintf(buf + len,
+		"set priority queue 3 control\n");
+	len += sprintf(buf + len, "tx_p0_ratio:\t");
+	len += sprintf(buf + len,
+		"set priority queue 0 ratio\n");
+	len += sprintf(buf + len, "tx_p1_ratio:\t");
+	len += sprintf(buf + len,
+		"set priority queue 1 ratio\n");
+	len += sprintf(buf + len, "tx_p2_ratio:\t");
+	len += sprintf(buf + len,
+		"set priority queue 2 ratio\n");
+	len += sprintf(buf + len, "tx_p3_ratio:\t");
+	len += sprintf(buf + len,
+		"set priority queue 3 ratio\n");
+	len += sprintf(buf + len, "prio_rate:\t");
+	len += sprintf(buf + len,
+		"disable/enable priority queue rate limiting\n");
+	len += sprintf(buf + len, "rx_limit:\t");
+	len += sprintf(buf + len,
+		"set rx rate limiting mode\n");
+	len += sprintf(buf + len, "cnt_ifg:\t");
+	len += sprintf(buf + len,
+		"set to 1 to count IPG\n");
+	len += sprintf(buf + len, "cnt_pre:\t");
+	len += sprintf(buf + len,
+		"set to 1 to count preamble\n");
+	len += sprintf(buf + len, "rx_p0_rate:\t");
+	len += sprintf(buf + len,
+		"set rx priority queue 0 rate in 64Kbps unit\n");
+	len += sprintf(buf + len, "rx_p1_rate:\t");
+	len += sprintf(buf + len,
+		"set rx priority queue 1 rate in 64Kbps unit\n");
+	len += sprintf(buf + len, "rx_p2_rate:\t");
+	len += sprintf(buf + len,
+		"set rx priority queue 2 rate in 64Kbps unit\n");
+	len += sprintf(buf + len, "rx_p3_rate:\t");
+	len += sprintf(buf + len,
+		"set rx priority queue 3 rate in 64Kbps unit\n");
+	len += sprintf(buf + len, "tx_p0_rate:\t");
+	len += sprintf(buf + len,
+		"set tx priority queue 0 rate in 64Kbps unit\n");
+	len += sprintf(buf + len, "tx_p1_rate:\t");
+	len += sprintf(buf + len,
+		"set tx priority queue 1 rate in 64Kbps unit\n");
+	len += sprintf(buf + len, "tx_p2_rate:\t");
+	len += sprintf(buf + len,
+		"set tx priority queue 2 rate in 64Kbps unit\n");
+	len += sprintf(buf + len, "tx_p3_rate:\t");
+	len += sprintf(buf + len,
+		"set tx priority queue 3 rate in 64Kbps unit\n");
+	printk(KERN_INFO "%s", buf);
+	len = 0;
+
+	len += sprintf(buf + len, "rx:\t\t");
+	len += sprintf(buf + len,
+		"disable/enable rx\n");
+	len += sprintf(buf + len, "tx:\t\t");
+	len += sprintf(buf + len,
+		"disable/enable tx\n");
+	len += sprintf(buf + len, "learn:\t\t");
+	len += sprintf(buf + len,
+		"disable/enable learning\n");
+
+	len += sprintf(buf + len, "mirror_port:\t");
+	len += sprintf(buf + len,
+		"disable/enable mirror port\n");
+	len += sprintf(buf + len, "mirror_rx:\t");
+	len += sprintf(buf + len,
+		"disable/enable mirror receive\n");
+	len += sprintf(buf + len, "mirror_tx:\t");
+	len += sprintf(buf + len,
+		"disable/enable mirror transmit\n");
+	printk(KERN_INFO "%s", buf);
+	len = 0;
+
+	len += sprintf(buf + len, "\nnon_vid:\t");
+	len += sprintf(buf + len,
+		"set to 1 to discard non-VID packets\n");
+	len += sprintf(buf + len, "ingress:\t");
+	len += sprintf(buf + len,
+		"disable/enable ingress VLAN filtering\n");
+	len += sprintf(buf + len, "ins_tag:\t");
+	len += sprintf(buf + len,
+		"disable/enable insert VLAN tag feature\n");
+	len += sprintf(buf + len, "rmv_tag:\t");
+	len += sprintf(buf + len,
+		"disable/enable remove VLAN tag feature\n");
+	len += sprintf(buf + len, "drop_tagged:\t");
+	len += sprintf(buf + len,
+		"disable/enable drop tagged packet feature\n");
+	len += sprintf(buf + len, "replace_prio:\t");
+	len += sprintf(buf + len,
+		"disable/enable replace 802.1p priority feature\n");
+	len += sprintf(buf + len, "back_pressure:\t");
+	len += sprintf(buf + len,
+		"disable/enable back pressure in half-duplex mode\n");
+	len += sprintf(buf + len, "force_flow_ctrl:");
+	len += sprintf(buf + len,
+		"set to 1 to force flow control\n");
+	len += sprintf(buf + len, "fw_unk_dest:\t");
+	len += sprintf(buf + len,
+		"set to 1 to forward unknown destination address packets\n");
+	len += sprintf(buf + len, "fw_inv_vid:\t");
+	len += sprintf(buf + len,
+		"set to 1 to forward invalid VID packets\n");
+	printk(KERN_INFO "%s", buf);
+	len = 0;
+
+	len += sprintf(buf + len, "\nstatic MAC table:\n");
+	len += sprintf(buf + len, "addr:\t\t");
+	len += sprintf(buf + len,
+		"set MAC address\n");
+	len += sprintf(buf + len, "ports:\t\t");
+	len += sprintf(buf + len,
+		"set destination ports\n");
+	len += sprintf(buf + len, "override:\t");
+	len += sprintf(buf + len,
+		"set override bit\n");
+	len += sprintf(buf + len, "use_fid:\t");
+	len += sprintf(buf + len,
+		"set use FID bit\n");
+	len += sprintf(buf + len, "fid:\t\t");
+	len += sprintf(buf + len,
+		"set FID\n");
+	len += sprintf(buf + len, "valid:\t\t");
+	len += sprintf(buf + len,
+		"set valid bit and write to table\n");
+	len += sprintf(buf + len, "\nVLAN table:\n");
+	len += sprintf(buf + len, "vid:\t\t");
+	len += sprintf(buf + len,
+		"set VID\n");
+	len += sprintf(buf + len, "fid:\t\t");
+	len += sprintf(buf + len,
+		"set FID\n");
+	len += sprintf(buf + len, "member:\t\t");
+	len += sprintf(buf + len,
+		"set membership\n");
+	len += sprintf(buf + len, "valid:\t\t");
+	len += sprintf(buf + len,
+		"set valid bit and write to table\n");
+	printk(KERN_INFO "%s", buf);
+	len = 0;
+
+	return len;
+}  /* display_sw_info */
+
+static ssize_t sysfs_sw_read(struct ksz_sw *sw, int proc_num,
+	struct ksz_port *port, ssize_t len, char *buf)
+{
+	int i;
+	int j;
+	u16 map;
+	struct ksz_sw_info *info = sw->info;
+
+	switch (proc_num) {
+	case PROC_SW_INFO:
+		len = display_sw_info(TOTAL_PORT_NUM, buf, len);
+		break;
+	case PROC_SW_VERSION:
+		len += sprintf(buf + len, "%s  %s\n",
+			SW_DRV_VERSION, SW_DRV_RELDATE);
+		break;
+	case PROC_SET_SW_DUPLEX:
+		if (!port)
+			break;
+		len += sprintf(buf + len, "%u; ", port->duplex);
+		if (media_connected == port->linked->state) {
+			if (1 == port->linked->duplex)
+				len += sprintf(buf + len, "half-duplex\n");
+			else if (2 == port->linked->duplex)
+				len += sprintf(buf + len, "full-duplex\n");
+		} else
+			len += sprintf(buf + len, "unlinked\n");
+		break;
+	case PROC_SET_SW_SPEED:
+		if (!port)
+			break;
+		len += sprintf(buf + len, "%u; ", port->speed);
+		if (media_connected == port->linked->state)
+			len += sprintf(buf + len, "%u\n",
+				port->linked->tx_rate / TX_RATE_UNIT);
+		else
+			len += sprintf(buf + len, "unlinked\n");
+		break;
+	case PROC_SET_SW_FORCE:
+		if (!port)
+			break;
+		len += sprintf(buf + len, "%u\n", port->force_link);
+		break;
+	case PROC_SET_SW_FLOW_CTRL:
+		if (!port)
+			break;
+		len += sprintf(buf + len, "%u; ", port->flow_ctrl);
+		switch (port->flow_ctrl) {
+		case PHY_FLOW_CTRL:
+			len += sprintf(buf + len, "flow control\n");
+			break;
+		case PHY_TX_ONLY:
+			len += sprintf(buf + len, "tx only\n");
+			break;
+		case PHY_RX_ONLY:
+			len += sprintf(buf + len, "rx only\n");
+			break;
+		default:
+			len += sprintf(buf + len, "no flow control\n");
+			break;
+		}
+		break;
+	case PROC_SET_SW_MIB:
+		if (!port)
+			break;
+		len += display_sw_mib_counters(sw, port->first_port,
+			port->mib_port_cnt, buf + len);
+		break;
+	case PROC_SET_BROADCAST_STORM:
+		len += sprintf(buf + len, "%u%%\n", info->broad_per);
+		break;
+	case PROC_SET_DIFFSERV:
+		for (i = 0; i < DIFFSERV_ENTRIES / KS_PRIO_IN_REG; i++) {
+			len += sprintf(buf + len, "%2u=",
+				i * KS_PRIO_IN_REG);
+			map = info->diffserv[i];
+			for (j = 0; j < KS_PRIO_IN_REG; j++) {
+				len += sprintf(buf + len, "%u ",
+					map & KS_PRIO_M);
+				map >>= KS_PRIO_S;
+			}
+			len += sprintf(buf + len, "\t"SW_SIZE_STR"\n",
+				info->diffserv[i]);
+		}
+		break;
+	case PROC_SET_802_1P:
+		for (i = 0; i < PRIO_802_1P_ENTRIES / KS_PRIO_IN_REG; i++) {
+			len += sprintf(buf + len, "%u=",
+				i * KS_PRIO_IN_REG);
+			map = info->p_802_1p[i];
+			for (j = 0; j < KS_PRIO_IN_REG; j++) {
+				len += sprintf(buf + len, "%u ",
+					map & KS_PRIO_M);
+				map >>= KS_PRIO_S;
+			}
+			len += sprintf(buf + len, "\t"SW_SIZE_STR"\n",
+				info->p_802_1p[i]);
+		}
+		break;
+	case PROC_SET_SW_VID:
+		len += sprintf(buf + len, "0x%04x\n", sw->vid);
+		break;
+	case PROC_GET_PORTS:
+	{
+		uint ports = sw->mib_port_cnt;
+
+		if (sw->eth_cnt)
+			ports = sw->eth_maps[0].cnt;
+		len += sprintf(buf + len, "%u\n", ports);
+		break;
+	}
+	case PROC_GET_DEV_START:
+	{
+		int start = 0;
+
+		if (sw->dev_offset)
+			start = 100;
+		len += sprintf(buf + len, "%u\n", start);
+		break;
+	}
+	case PROC_GET_VLAN_START:
+	{
+		int start = 0;
+
+		if (sw->features & VLAN_PORT)
+			start = VLAN_PORT_START;
+		len += sprintf(buf + len, "%u\n", start);
+		break;
+	}
+	case PROC_GET_STP:
+		len += sprintf(buf + len, "0\n");
+		break;
+	case PROC_SET_SW_FEATURES:
+		len += sprintf(buf + len, "%08x:\n", sw->features);
+		len += sprintf(buf + len, "\t%08x = STP support\n",
+			STP_SUPPORT);
+		len += sprintf(buf + len, "\t%08x = VLAN port forwarding\n",
+			VLAN_PORT);
+		len += sprintf(buf + len, "\t%08x = VLAN port remove tag\n",
+			VLAN_PORT_REMOVE_TAG);
+		len += sprintf(buf + len, "\t%08x = VLAN port tag tailing\n",
+			VLAN_PORT_TAGGING);
+#ifdef CONFIG_KSZ_DLR
+		len += sprintf(buf + len, "\t%08x = DLR support\n",
+			DLR_HW);
+#endif
+		len += sprintf(buf + len, "\t%08x = different MAC addresses\n",
+			DIFF_MAC_ADDR);
+		break;
+	case PROC_SET_SW_OVERRIDES:
+		len += sprintf(buf + len, "%08x:\n", sw->overrides);
+		len += sprintf(buf + len, "\t%08x = flow control\n",
+			PAUSE_FLOW_CTRL);
+		len += sprintf(buf + len, "\t%08x = fast aging\n",
+			FAST_AGING);
+		len += sprintf(buf + len, "\t%08x = ACL intr monitor\n",
+			ACL_INTR_MONITOR);
+		len += sprintf(buf + len, "\t%08x = tag is removed\n",
+			TAG_REMOVE);
+		len += sprintf(buf + len, "\t%08x = tail tagging\n",
+			TAIL_TAGGING);
+		break;
+	case PROC_DYNAMIC:
+		len = sw_d_dyn_mac_table(sw, buf, len);
+		break;
+	case PROC_STATIC:
+		len = sw_d_sta_mac_table(sw, buf, len);
+#ifdef DEBUG
+		len = sw_d_mac_table(sw, buf, len);
+#endif
+		break;
+	case PROC_VLAN:
+		len = sw_d_vlan_table(sw, buf, len);
+		break;
+	}
+	return len;
+}  /* sysfs_sw_read */
+
+static ssize_t sysfs_sw_read_hw(struct ksz_sw *sw, int proc_num, ssize_t len,
+	char *buf)
+{
+	u8 data[8];
+	int chk = 0;
+	int type = SHOW_HELP_ON_OFF;
+	char note[40];
+
+	note[0] = '\0';
+	switch (proc_num) {
+	case PROC_SET_AGING:
+		chk = sw_chk(sw, REG_SW_CTRL_1, SW_AGING_ENABLE);
+		break;
+	case PROC_SET_FAST_AGING:
+		chk = sw_chk(sw, REG_SW_CTRL_1, SW_FAST_AGING);
+		break;
+	case PROC_SET_LINK_AGING:
+		chk = sw_chk(sw, S_LINK_AGING_CTRL, SW_LINK_AUTO_AGING);
+		break;
+	case PROC_SET_MULTICAST_STORM:
+		chk = !sw_chk(sw, REG_SW_CTRL_2, MULTICAST_STORM_DISABLE);
+		break;
+	case PROC_SET_TX_RATE_QUEUE_BASED:
+		chk = sw_chk(sw, REG_SW_CTRL_19,
+			SW_OUT_RATE_LIMIT_QUEUE_BASED);
+		break;
+	case PROC_ENABLE_VLAN:
+		chk = sw_chk(sw, S_MIRROR_CTRL, SW_VLAN_ENABLE);
+		break;
+	case PROC_SET_REPLACE_NULL_VID:
+		chk = sw_chk(sw, S_REPLACE_VID_CTRL, SW_REPLACE_VID);
+		break;
+	case PROC_SET_MAC_ADDR:
+		sw_get_addr(sw, data);
+		len += sprintf(buf + len, "%02X:%02X:%02X:%02X:%02X:%02X\n",
+			data[0], data[1], data[2], data[3], data[4], data[5]);
+		type = SHOW_HELP_NONE;
+		break;
+	case PROC_SET_MIRROR_MODE:
+		chk = sw_chk_mirror_rx_tx(sw);
+		if (sw->verbose) {
+			if (chk)
+				strcpy(note, " (rx and tx)");
+			else
+				strcpy(note, " (rx or tx)");
+		}
+		type = SHOW_HELP_SPECIAL;
+		break;
+	case PROC_SET_IGMP_SNOOP:
+		chk = sw_chk(sw, S_MIRROR_CTRL, SW_IGMP_SNOOP);
+		break;
+	case PROC_SET_IPV6_MLD_SNOOP:
+		chk = sw_chk(sw, S_IPV6_MLD_CTRL, SW_IPV6_MLD_SNOOP);
+		break;
+	case PROC_SET_IPV6_MLD_OPTION:
+		chk = sw_chk(sw, S_IPV6_MLD_CTRL, SW_IPV6_MLD_OPTION);
+		break;
+	case PROC_SET_TAIL_TAG:
+		chk = sw_chk(sw, S_TAIL_TAG_CTRL, SW_TAIL_TAG_ENABLE);
+		break;
+	case PROC_SET_AGGR_BACKOFF:
+		chk = sw_chk(sw, REG_SW_CTRL_1, SW_AGGR_BACKOFF);
+		break;
+	case PROC_SET_NO_EXC_DROP:
+		chk = sw_chk(sw, REG_SW_CTRL_2, NO_EXC_COLLISION_DROP);
+		break;
+	case PROC_SET_VLAN_BOUNDARY:
+		chk = sw_chk(sw, REG_SW_CTRL_2, UNICAST_VLAN_BOUNDARY);
+		break;
+	case PROC_SET_HUGE_PACKET:
+		chk = sw_chk(sw, S_HUGE_PACKET_CTRL, SW_HUGE_PACKET);
+		break;
+	case PROC_SET_LEGAL_PACKET:
+		chk = !sw_chk(sw, REG_SW_CTRL_2, SW_LEGAL_PACKET_DISABLE);
+		break;
+	case PROC_SET_LENGTH_CHECK:
+		chk = sw_chk(sw, REG_SW_CTRL_1, SW_CHECK_LENGTH);
+		break;
+	case PROC_SET_BACK_PRESSURE_MODE:
+		chk = sw_chk(sw, REG_SW_CTRL_2, SW_BACK_PRESSURE);
+		break;
+	case PROC_SET_SWITCH_FLOW_CTRL:
+		chk = sw_chk(sw, S_REPLACE_VID_CTRL, SW_FLOW_CTRL);
+		break;
+	case PROC_SET_SWITCH_HALF_DUPLEX:
+		chk = sw_chk(sw, S_REPLACE_VID_CTRL, SW_HALF_DUPLEX);
+		break;
+#ifdef SW_10_MBIT
+	case PROC_SET_SWITCH_10_MBIT:
+		chk = sw_chk(sw, S_REPLACE_VID_CTRL, SW_10_MBIT);
+		break;
+#endif
+	case PROC_SET_RX_FLOW_CTRL:
+		chk = !sw_chk(sw, REG_SW_CTRL_1, SW_RX_FLOW_CTRL_DISABLE);
+		break;
+	case PROC_SET_TX_FLOW_CTRL:
+		chk = !sw_chk(sw, REG_SW_CTRL_1, SW_TX_FLOW_CTRL_DISABLE);
+		break;
+	case PROC_SET_FAIR_FLOW_CTRL:
+		chk = sw_chk(sw, REG_SW_CTRL_2, FAIR_FLOW_CTRL);
+		break;
+	case PROC_SET_FORWARD_UNKNOWN_UNICAST:
+		chk = sw_chk_unk_dest(sw, REG_SW_UNK_UCAST_CTRL);
+		break;
+	case PROC_SET_FORWARD_UNKNOWN_UNICAST_PORTS:
+		chk = sw_chk_unk_def_port(sw, REG_SW_UNK_UCAST_CTRL,
+			sw->port_cnt);
+		chk = get_log_mask_from_phy(sw, chk);
+		type = SHOW_HELP_HEX;
+		break;
+	case PROC_SET_FORWARD_UNKNOWN_MULTICAST:
+		chk = sw_chk_unk_dest(sw, REG_SW_UNK_MCAST_CTRL);
+		break;
+	case PROC_SET_FORWARD_UNKNOWN_MULTICAST_PORTS:
+		chk = sw_chk_unk_def_port(sw, REG_SW_UNK_MCAST_CTRL,
+			sw->port_cnt);
+		chk = get_log_mask_from_phy(sw, chk);
+		type = SHOW_HELP_HEX;
+		break;
+	case PROC_SET_FORWARD_UNKNOWN_VID:
+		chk = sw_chk_unk_dest(sw, REG_SW_UNK_VID_CTRL);
+		break;
+	case PROC_SET_FORWARD_UNKNOWN_VID_PORTS:
+		chk = sw_chk_unk_def_port(sw, REG_SW_UNK_VID_CTRL,
+			sw->port_cnt);
+		chk = get_log_mask_from_phy(sw, chk);
+		type = SHOW_HELP_HEX;
+		break;
+	case PROC_SET_FORWARD_UNKNOWN_IP_MULTICAST:
+		chk = sw_chk_unk_dest(sw, REG_SW_UNK_IP_MCAST_CTRL);
+		break;
+	case PROC_SET_FORWARD_UNKNOWN_IP_MULTICAST_PORTS:
+		chk = sw_chk_unk_def_port(sw, REG_SW_UNK_IP_MCAST_CTRL,
+			sw->port_cnt);
+		chk = get_log_mask_from_phy(sw, chk);
+		type = SHOW_HELP_HEX;
+		break;
+	case PROC_SET_SELF_ADDR_FILTER:
+		chk = sw_chk_self_filter(sw);
+		break;
+	case PROC_SET_INS_TAG:
+		chk = sw_chk_ins_tag(sw);
+		break;
+	case PROC_SET_PME:
+		data[0] = sw_get_pme(sw);
+		len += sprintf(buf + len, "%02x:\n", data[0]);
+		len += sprintf(buf + len, "\t%02x = PME enable\n",
+			SW_PME_OUTPUT_ENABLE);
+		len += sprintf(buf + len, "\t%02x = PME active high\n",
+			SW_PME_ACTIVE_HIGH);
+		type = SHOW_HELP_NONE;
+		break;
+	case PROC_SET_PASS_PAUSE:
+		chk = sw_chk(sw, S_PASS_PAUSE_CTRL, SW_PASS_PAUSE);
+		break;
+	case PROC_SET_HI_PRIO_QUEUES:
+		chk = sw_get_hi_prio_queues(sw);
+		if (sw->verbose) {
+			switch (chk) {
+			case 3:
+				strcpy(note, " (3 = H; 2, 1, 0 = L)");
+				break;
+			case 1:
+				strcpy(note, " (3, 2, 1 = H; 0 = L)");
+				break;
+			default:
+				strcpy(note, " (3, 2 = H; 1, 0 = L)");
+			}
+		}
+		type = SHOW_HELP_SPECIAL;
+		break;
+	default:
+		type = SHOW_HELP_NONE;
+		break;
+	}
+	return sysfs_show(len, buf, type, chk, note, sw->verbose);
+}  /* sysfs_sw_read_hw */
+
+static int sysfs_sw_write(struct ksz_sw *sw, int proc_num,
+	struct ksz_port *port, int num, const char *buf)
+{
+	int changes;
+	int count;
+	unsigned int val;
+	u8 data[8];
+	int processed = true;
+
+	switch (proc_num) {
+	case PROC_SW_INFO:
+		sw_init(sw);
+		sw->verbose = !!num;
+		break;
+	case PROC_SET_SW_DUPLEX:
+		if (!port)
+			break;
+		if (num <= 2)
+			port->duplex = (u8) num;
+		break;
+	case PROC_SET_SW_SPEED:
+		if (!port)
+			break;
+		if (0 == num || 10 == num || 100 == num)
+			port->speed = (u8) num;
+		break;
+	case PROC_SET_SW_FORCE:
+		if (!port)
+			break;
+		port->force_link = (u8) num;
+		if (port->force_link) {
+			port_force_link_speed(port);
+			sw->phy_intr = sw->PORT_MASK;
+			port_get_link_speed(port);
+			if (port->link_ports)
+				schedule_delayed_work(&port->link_update, 0);
+			sw->phy_intr = 0;
+		} else
+			port_set_link_speed(port);
+		break;
+	case PROC_SET_SW_FLOW_CTRL:
+		if (!port)
+			break;
+		if (num <= PHY_FLOW_CTRL)
+			port->flow_ctrl = (u8) num;
+		break;
+	case PROC_SET_SW_MIB:
+		sw_cfg_mib_counter_ctrl(sw, num, sw->port_cnt);
+		break;
+	case PROC_SET_SW_REG:
+		count = sscanf(buf, "%x=%x", (unsigned int *) &num, &val);
+		if (1 == count)
+			printk(KERN_INFO SW_SIZE_STR"\n",
+				SW_R(sw, num));
+		else if (2 == count)
+			SW_W(sw, num, val);
+		break;
+	case PROC_SET_SW_VID:
+		sw->vid = num;
+		break;
+	case PROC_SET_SW_FEATURES:
+		if ('0' != buf[0] || 'x' != buf[1])
+			sscanf(buf, "%x", &num);
+		changes = sw->features ^ num;
+		sw->features = num;
+		if (changes & VLAN_PORT_REMOVE_TAG) {
+			uint n;
+			uint p;
+			bool enabled;
+
+			if (num & VLAN_PORT_REMOVE_TAG) {
+				enabled = true;
+				sw->overrides |= TAG_REMOVE;
+			} else {
+				enabled = false;
+				sw->overrides &= ~TAG_REMOVE;
+			}
+			for (n = 1; n <= sw->mib_port_cnt; n++) {
+				p = get_phy_port(sw, n);
+				port_cfg_rmv_tag(sw, p, enabled);
+			}
+		}
+		break;
+	case PROC_SET_SW_OVERRIDES:
+		if ('0' != buf[0] || 'x' != buf[1])
+			sscanf(buf, "%x", &num);
+		sw->overrides = num;
+		break;
+	case PROC_DYNAMIC:
+		sw_flush_dyn_mac_table(sw, TOTAL_PORT_NUM);
+		break;
+	case PROC_STATIC:
+		sw->ops->release(sw);
+		sw_clr_sta_mac_table(sw);
+		sw->ops->acquire(sw);
+		break;
+	case PROC_SET_AGING:
+		sw_cfg(sw, REG_SW_CTRL_1, SW_AGING_ENABLE, num);
+		break;
+	case PROC_SET_FAST_AGING:
+		sw_cfg(sw, REG_SW_CTRL_1, SW_FAST_AGING, num);
+		break;
+	case PROC_SET_LINK_AGING:
+		sw_cfg(sw, S_LINK_AGING_CTRL, SW_LINK_AUTO_AGING, num);
+		break;
+	case PROC_SET_BROADCAST_STORM:
+		hw_cfg_broad_storm(sw, num);
+		break;
+	case PROC_SET_MULTICAST_STORM:
+		sw_cfg(sw, REG_SW_CTRL_2, MULTICAST_STORM_DISABLE, !num);
+		break;
+	case PROC_SET_TX_RATE_QUEUE_BASED:
+		sw_cfg(sw, REG_SW_CTRL_19, SW_OUT_RATE_LIMIT_QUEUE_BASED,
+			num);
+		break;
+	case PROC_SET_DIFFSERV:
+		count = sscanf(buf, "%d=%x", (unsigned int *) &num, &val);
+		if (2 == count)
+			hw_cfg_tos_prio(sw, (u8) num, (u16) val);
+		break;
+	case PROC_SET_802_1P:
+		count = sscanf(buf, "%d=%x", (unsigned int *) &num, &val);
+		if (2 == count)
+			hw_cfg_802_1p_prio(sw, (u8) num, (u16) val);
+		break;
+	case PROC_ENABLE_VLAN:
+		if (!num)
+			sw_dis_vlan(sw);
+		else
+			sw_ena_vlan(sw);
+		break;
+	case PROC_SET_REPLACE_NULL_VID:
+		sw_cfg_replace_null_vid(sw, num);
+		break;
+	case PROC_SET_MAC_ADDR:
+	{
+		int i;
+		int n[6];
+
+		i = sscanf(buf, "%x:%x:%x:%x:%x:%x",
+			&n[0], &n[1], &n[2], &n[3], &n[4], &n[5]);
+		if (6 == i) {
+			for (i = 0; i < 6; i++)
+				data[i] = (u8) n[i];
+			sw_set_addr(sw, data);
+		}
+		break;
+	}
+	case PROC_SET_MIRROR_MODE:
+		sw_cfg_mirror_rx_tx(sw, num);
+		break;
+	case PROC_SET_IGMP_SNOOP:
+		sw_cfg(sw, S_MIRROR_CTRL, SW_IGMP_SNOOP, num);
+		break;
+	case PROC_SET_IPV6_MLD_SNOOP:
+		sw_cfg(sw, S_IPV6_MLD_CTRL, SW_IPV6_MLD_SNOOP, num);
+		break;
+	case PROC_SET_IPV6_MLD_OPTION:
+		sw_cfg(sw, S_IPV6_MLD_CTRL, SW_IPV6_MLD_OPTION, num);
+		break;
+	case PROC_SET_TAIL_TAG:
+		sw_cfg(sw, S_TAIL_TAG_CTRL, SW_TAIL_TAG_ENABLE, num);
+		if (num)
+			sw->overrides |= TAIL_TAGGING;
+		else
+			sw->overrides &= ~TAIL_TAGGING;
+		break;
+	case PROC_SET_AGGR_BACKOFF:
+		sw_cfg(sw, REG_SW_CTRL_1, SW_AGGR_BACKOFF, num);
+		break;
+	case PROC_SET_NO_EXC_DROP:
+		sw_cfg(sw, REG_SW_CTRL_2, NO_EXC_COLLISION_DROP, num);
+		break;
+	case PROC_SET_VLAN_BOUNDARY:
+		sw_cfg(sw, REG_SW_CTRL_2, UNICAST_VLAN_BOUNDARY, num);
+		break;
+	case PROC_SET_HUGE_PACKET:
+		sw_cfg(sw, S_HUGE_PACKET_CTRL, SW_HUGE_PACKET, num);
+		break;
+	case PROC_SET_LEGAL_PACKET:
+		sw_cfg(sw, REG_SW_CTRL_2, SW_LEGAL_PACKET_DISABLE, !num);
+		break;
+	case PROC_SET_LENGTH_CHECK:
+		sw_cfg(sw, REG_SW_CTRL_1, SW_CHECK_LENGTH, num);
+		break;
+	case PROC_SET_BACK_PRESSURE_MODE:
+		sw_cfg(sw, REG_SW_CTRL_2, SW_BACK_PRESSURE, num);
+		break;
+	case PROC_SET_SWITCH_FLOW_CTRL:
+		sw_cfg(sw, S_REPLACE_VID_CTRL, SW_FLOW_CTRL, num);
+		break;
+	case PROC_SET_SWITCH_HALF_DUPLEX:
+		sw_cfg(sw, S_REPLACE_VID_CTRL, SW_HALF_DUPLEX, num);
+		break;
+#ifdef SW_10_MBIT
+	case PROC_SET_SWITCH_10_MBIT:
+		sw_cfg(sw, S_REPLACE_VID_CTRL, SW_10_MBIT, num);
+		break;
+#endif
+	case PROC_SET_RX_FLOW_CTRL:
+		sw_cfg(sw, REG_SW_CTRL_1, SW_RX_FLOW_CTRL_DISABLE, !num);
+		break;
+	case PROC_SET_TX_FLOW_CTRL:
+		sw_cfg(sw, REG_SW_CTRL_1, SW_TX_FLOW_CTRL_DISABLE, !num);
+		break;
+	case PROC_SET_FAIR_FLOW_CTRL:
+		sw_cfg(sw, REG_SW_CTRL_2, FAIR_FLOW_CTRL, num);
+		break;
+	case PROC_SET_FORWARD_UNKNOWN_UNICAST:
+		sw_cfg_unk_dest(sw, REG_SW_UNK_UCAST_CTRL, num);
+		break;
+	case PROC_SET_FORWARD_UNKNOWN_UNICAST_PORTS:
+		num = get_phy_mask_from_log(sw, num);
+		sw_cfg_unk_def_port(sw, REG_SW_UNK_UCAST_CTRL, num, 2);
+		break;
+	case PROC_SET_FORWARD_UNKNOWN_MULTICAST:
+		sw_cfg_unk_dest(sw, REG_SW_UNK_MCAST_CTRL, num);
+		break;
+	case PROC_SET_FORWARD_UNKNOWN_MULTICAST_PORTS:
+		num = get_phy_mask_from_log(sw, num);
+		sw_cfg_unk_def_port(sw, REG_SW_UNK_MCAST_CTRL, num, 2);
+		break;
+	case PROC_SET_FORWARD_UNKNOWN_VID:
+		sw_cfg_unk_dest(sw, REG_SW_UNK_VID_CTRL, num);
+		break;
+	case PROC_SET_FORWARD_UNKNOWN_VID_PORTS:
+		num = get_phy_mask_from_log(sw, num);
+		sw_cfg_unk_def_port(sw, REG_SW_UNK_VID_CTRL, num, 2);
+		break;
+	case PROC_SET_FORWARD_UNKNOWN_IP_MULTICAST:
+		sw_cfg_unk_dest(sw, REG_SW_UNK_IP_MCAST_CTRL, num);
+		break;
+	case PROC_SET_FORWARD_UNKNOWN_IP_MULTICAST_PORTS:
+		num = get_phy_mask_from_log(sw, num);
+		sw_cfg_unk_def_port(sw, REG_SW_UNK_IP_MCAST_CTRL, num, 2);
+		break;
+	case PROC_SET_SELF_ADDR_FILTER:
+		sw_cfg_self_filter(sw, num);
+		break;
+	case PROC_SET_INS_TAG:
+		sw_cfg_ins_tag(sw, num);
+		break;
+	case PROC_SET_PME:
+		sw_set_pme(sw, num);
+		break;
+	case PROC_SET_PASS_PAUSE:
+		sw_cfg(sw, S_PASS_PAUSE_CTRL, SW_PASS_PAUSE, num);
+		break;
+	case PROC_SET_HI_PRIO_QUEUES:
+		sw_set_hi_prio_queues(sw, num);
+		break;
+	default:
+		processed = false;
+		break;
+	}
+	return processed;
+}  /* sysfs_sw_write */
+
+static ssize_t sysfs_port_read(struct ksz_sw *sw, int proc_num, uint n,
+	ssize_t len, char *buf)
+{
+	struct ksz_port_cfg *port_cfg;
+	struct ksz_port_info *port_info;
+	uint port;
+	int chk = 0;
+	int type = SHOW_HELP_NONE;
+	char note[40];
+
+	note[0] = '\0';
+	port = get_sysfs_port(sw, n);
+	port_cfg = get_port_cfg(sw, port);
+	port_info = get_port_info(sw, port);
+	switch (proc_num) {
+	case PROC_SET_PORT_DUPLEX:
+		if (media_connected == port_info->state) {
+			if (1 == port_info->duplex)
+				len += sprintf(buf + len, "half-duplex\n");
+			else if (2 == port_info->duplex)
+				len += sprintf(buf + len, "full-duplex\n");
+		} else
+			len += sprintf(buf + len, "unlinked\n");
+		break;
+	case PROC_SET_PORT_SPEED:
+		if (media_connected == port_info->state)
+			len += sprintf(buf + len, "%u\n",
+				port_info->tx_rate / TX_RATE_UNIT);
+		else
+			len += sprintf(buf + len, "unlinked\n");
+		break;
+	case PROC_SET_PORT_MIB:
+		port = get_log_port(sw, port);
+		len += display_sw_mib_counters(sw, port, 1, buf + len);
+		break;
+	case PROC_SET_LINK_MD:
+		len += sprintf(buf + len, "%u:%u %u:%u %u:%u\n",
+			port_info->length[0], port_info->status[0],
+			port_info->length[1], port_info->status[1],
+			port_info->length[2], port_info->status[2]);
+		if (sw->verbose)
+			len += sprintf(buf + len,
+				"(%d=unknown; %d=normal; %d=open; %d=short)\n",
+				CABLE_UNKNOWN, CABLE_GOOD, CABLE_OPEN,
+				CABLE_SHORT);
+		break;
+	case PROC_SET_PORT_BASED:
+		chk = port_cfg->port_prio;
+		type = SHOW_HELP_NUM;
+		break;
+	case PROC_SET_DEF_VID:
+		chk = port_cfg->vid;
+		type = SHOW_HELP_HEX_4;
+		break;
+	case PROC_SET_MEMBER:
+		chk = port_cfg->member;
+		chk = get_log_mask_from_phy(sw, chk);
+		type = SHOW_HELP_HEX_2;
+		break;
+	case PROC_SET_TX_Q0_CTRL:
+		chk = (port_cfg->rate_ctrl[0] & RATE_CTRL_ENABLE) != 0;
+		type = SHOW_HELP_ON_OFF;
+		break;
+	case PROC_SET_TX_Q1_CTRL:
+		chk = (port_cfg->rate_ctrl[1] & RATE_CTRL_ENABLE) != 0;
+		type = SHOW_HELP_ON_OFF;
+		break;
+	case PROC_SET_TX_Q2_CTRL:
+		chk = (port_cfg->rate_ctrl[2] & RATE_CTRL_ENABLE) != 0;
+		type = SHOW_HELP_ON_OFF;
+		break;
+	case PROC_SET_TX_Q3_CTRL:
+		chk = (port_cfg->rate_ctrl[3] & RATE_CTRL_ENABLE) != 0;
+		type = SHOW_HELP_ON_OFF;
+		break;
+	case PROC_SET_TX_Q0_RATIO:
+		chk = port_cfg->rate_ctrl[0] & RATE_RATIO_M;
+		type = SHOW_HELP_NUM;
+		break;
+	case PROC_SET_TX_Q1_RATIO:
+		chk = port_cfg->rate_ctrl[1] & RATE_RATIO_M;
+		type = SHOW_HELP_NUM;
+		break;
+	case PROC_SET_TX_Q2_RATIO:
+		chk = port_cfg->rate_ctrl[2] & RATE_RATIO_M;
+		type = SHOW_HELP_NUM;
+		break;
+	case PROC_SET_TX_Q3_RATIO:
+		chk = port_cfg->rate_ctrl[3] & RATE_RATIO_M;
+		type = SHOW_HELP_NUM;
+		break;
+	case PROC_SET_RX_LIMIT:
+		chk = ((port_cfg->rate_limit >> PORT_IN_LIMIT_MODE_S) &
+			PORT_IN_LIMIT_MODE_M);
+		if (sw->verbose) {
+			switch (chk) {
+			case 1:
+				strcpy(note, " (flooded unicast)");
+				break;
+			case 2:
+				strcpy(note, " (multicast)");
+				break;
+			case 3:
+				strcpy(note, " (broadcast)");
+				break;
+			default:
+				strcpy(note, " (all)");
+			}
+		}
+		type = SHOW_HELP_SPECIAL;
+		break;
+	case PROC_SET_RX_LIMIT_PORT_BASED:
+		chk = ((port_cfg->rate_limit >> PORT_IN_PORT_BASED_S) & 1);
+		type = SHOW_HELP_ON_OFF;
+		break;
+	case PROC_SET_LIMIT_PACKET_BASED:
+		chk = ((port_cfg->rate_limit >> PORT_RATE_PACKET_BASED_S) & 1);
+		type = SHOW_HELP_ON_OFF;
+		break;
+	case PROC_SET_RX_LIMIT_FLOW_CTRL:
+		chk = ((port_cfg->rate_limit >> PORT_IN_FLOW_CTRL_S) & 1);
+		type = SHOW_HELP_ON_OFF;
+		break;
+	case PROC_SET_CNT_IFG:
+		chk = ((port_cfg->rate_limit >> PORT_COUNT_IFG_S) & 1);
+		type = SHOW_HELP_ON_OFF;
+		break;
+	case PROC_SET_CNT_PRE:
+		chk = ((port_cfg->rate_limit >> PORT_COUNT_PREAMBLE_S) & 1);
+		type = SHOW_HELP_ON_OFF;
+		break;
+	case PROC_SET_RX_P0_RATE:
+		len += sprintf(buf + len, "%u %s\n", (int)
+			port_cfg->packet_based ?
+			port_cfg->rx_packet[0] : port_cfg->rx_rate[0],
+			port_cfg->packet_based ? "pps" : "bps");
+		break;
+	case PROC_SET_RX_P1_RATE:
+		len += sprintf(buf + len, "%u %s\n", (int)
+			port_cfg->packet_based ?
+			port_cfg->rx_packet[1] : port_cfg->rx_rate[1],
+			port_cfg->packet_based ? "pps" : "bps");
+		break;
+	case PROC_SET_RX_P2_RATE:
+		len += sprintf(buf + len, "%u %s\n", (int)
+			port_cfg->packet_based ?
+			port_cfg->rx_packet[2] : port_cfg->rx_rate[2],
+			port_cfg->packet_based ? "pps" : "bps");
+		break;
+	case PROC_SET_RX_P3_RATE:
+		len += sprintf(buf + len, "%u %s\n", (int)
+			port_cfg->packet_based ?
+			port_cfg->rx_packet[3] : port_cfg->rx_rate[3],
+			port_cfg->packet_based ? "pps" : "bps");
+		break;
+	case PROC_SET_TX_Q0_RATE:
+		len += sprintf(buf + len, "%u %s\n", (int)
+			port_cfg->packet_based ?
+			port_cfg->tx_packet[0] : port_cfg->tx_rate[0],
+			port_cfg->packet_based ? "pps" : "bps");
+		break;
+	case PROC_SET_TX_Q1_RATE:
+		len += sprintf(buf + len, "%u %s\n", (int)
+			port_cfg->packet_based ?
+			port_cfg->tx_packet[1] : port_cfg->tx_rate[1],
+			port_cfg->packet_based ? "pps" : "bps");
+		break;
+	case PROC_SET_TX_Q2_RATE:
+		len += sprintf(buf + len, "%u %s\n", (int)
+			port_cfg->packet_based ?
+			port_cfg->tx_packet[2] : port_cfg->tx_rate[2],
+			port_cfg->packet_based ? "pps" : "bps");
+		break;
+	case PROC_SET_TX_Q3_RATE:
+		len += sprintf(buf + len, "%u %s\n", (int)
+			port_cfg->packet_based ?
+			port_cfg->tx_packet[3] : port_cfg->tx_rate[3],
+			port_cfg->packet_based ? "pps" : "bps");
+		break;
+	}
+	return sysfs_show(len, buf, type, chk, note, sw->verbose);
+}  /* sysfs_port_read */
+
+static ssize_t sysfs_port_read_hw(struct ksz_sw *sw, int proc_num, uint n,
+	ssize_t len, char *buf)
+{
+	u8 data[8];
+	uint port;
+	int chk = 0;
+	int type = SHOW_HELP_ON_OFF;
+	char note[40];
+
+	note[0] = '\0';
+	port = get_sysfs_port(sw, n);
+	switch (proc_num) {
+	case PROC_ENABLE_BROADCAST_STORM:
+		chk = port_chk_broad_storm(sw, port);
+		break;
+	case PROC_ENABLE_DIFFSERV:
+		chk = port_chk_diffserv(sw, port);
+		break;
+	case PROC_ENABLE_802_1P:
+		chk = port_chk_802_1p(sw, port);
+		break;
+	case PROC_ENABLE_PRIO_QUEUE:
+		chk = port_get_prio_queue(sw, port);
+		type = SHOW_HELP_NUM;
+		break;
+	case PROC_SET_MIRROR_PORT:
+		chk = port_chk_mirror_sniffer(sw, port);
+		break;
+	case PROC_SET_MIRROR_RX:
+		chk = port_chk_mirror_rx(sw, port);
+		break;
+	case PROC_SET_MIRROR_TX:
+		chk = port_chk_mirror_tx(sw, port);
+		break;
+	case PROC_SET_RX:
+		chk = port_chk_rx(sw, port);
+		break;
+	case PROC_SET_TX:
+		chk = port_chk_tx(sw, port);
+		break;
+	case PROC_SET_LEARN:
+		chk = !port_chk_dis_learn(sw, port);
+		break;
+	case PROC_SET_INSERT_TAG:
+		chk = port_chk_ins_tag(sw, port);
+		break;
+	case PROC_SET_REMOVE_TAG:
+		chk = port_chk_rmv_tag(sw, port);
+		break;
+	case PROC_SET_DROP_TAG:
+		chk = port_chk_drop_tag(sw, port);
+		break;
+	case PROC_SET_REPLACE_PRIO:
+		chk = port_chk_replace_prio(sw, port);
+		break;
+	case PROC_ENABLE_RX_PRIO_RATE:
+		chk = sw_chk_rx_prio_rate(sw, port);
+		break;
+	case PROC_ENABLE_TX_PRIO_RATE:
+		chk = sw_chk_tx_prio_rate(sw, port);
+		break;
+	case PROC_SET_DIS_NON_VID:
+		chk = port_chk_dis_non_vid(sw, port);
+		break;
+	case PROC_SET_INGRESS:
+		chk = port_chk_in_filter(sw, port);
+		break;
+	case PROC_SET_BACK_PRESSURE:
+		chk = port_chk_back_pressure(sw, port);
+		break;
+	case PROC_SET_FORCE_FLOW_CTRL:
+		chk = port_chk_force_flow_ctrl(sw, port);
+		break;
+	case PROC_SET_INS_TAG_0:
+		chk = sw_chk_ins(sw, port, 0);
+		break;
+	case PROC_SET_INS_TAG_1:
+		chk = sw_chk_ins(sw, port, 1);
+		break;
+	case PROC_SET_INS_TAG_2:
+		chk = sw_chk_ins(sw, port, 2);
+		break;
+	case PROC_SET_INS_TAG_3:
+		chk = sw_chk_ins(sw, port, 3);
+		break;
+	case PROC_SET_INS_TAG_4:
+		chk = sw_chk_ins(sw, port, 4);
+		break;
+	case PROC_SET_PASS_ALL:
+		chk = port_chk(sw, port, P_PASS_ALL_CTRL, PORT_PASS_ALL);
+		break;
+	case PROC_SET_UNKNOWN_UNICAST_PORT:
+		chk = sw_chk_unk_def_port(sw, REG_SW_UNK_UCAST_CTRL, port);
+		break;
+	case PROC_SET_UNKNOWN_MULTICAST_PORT:
+		chk = sw_chk_unk_def_port(sw, REG_SW_UNK_MCAST_CTRL, port);
+		break;
+	case PROC_SET_UNKNOWN_VID_PORT:
+		chk = sw_chk_unk_def_port(sw, REG_SW_UNK_VID_CTRL, port);
+		break;
+	case PROC_SET_UNKNOWN_IP_MULTICAST_PORT:
+		chk = sw_chk_unk_def_port(sw, REG_SW_UNK_IP_MCAST_CTRL, port);
+		break;
+	case PROC_SET_PORT_PME_CTRL:
+		data[0] = port_get_pme_ctrl(sw, port);
+		len += sprintf(buf + len, "%02x:\n", data[0]);
+		len += sprintf(buf + len, "\t%02x = Magic Packet detect\n",
+			PORT_MAGIC_PACKET_DETECT);
+		len += sprintf(buf + len, "\t%02x = link up detect\n",
+			PORT_LINK_UP_DETECT);
+		len += sprintf(buf + len, "\t%02x = energy detect\n",
+			PORT_ENERGY_DETECT);
+		type = SHOW_HELP_NONE;
+		break;
+	case PROC_SET_PORT_PME_STATUS:
+		data[0] = port_get_pme_status(sw, port);
+		len += sprintf(buf + len, "%02x:\n", data[0]);
+		len += sprintf(buf + len, "\t%02x = Magic Packet detect\n",
+			PORT_MAGIC_PACKET_DETECT);
+		len += sprintf(buf + len, "\t%02x = link up detect\n",
+			PORT_LINK_UP_DETECT);
+		len += sprintf(buf + len, "\t%02x = energy detect\n",
+			PORT_ENERGY_DETECT);
+		type = SHOW_HELP_NONE;
+		break;
+	case PROC_SET_AUTHEN_MODE:
+		len += sprintf(buf + len, "%u\n",
+			port_get_authen_mode(sw, port));
+		type = SHOW_HELP_NONE;
+		break;
+	case PROC_SET_ACL:
+		chk = port_chk_acl(sw, port);
+		break;
+	default:
+		type = SHOW_HELP_NONE;
+		break;
+	}
+	return sysfs_show(len, buf, type, chk, note, sw->verbose);
+}  /* sysfs_port_read_hw */
+
+static int sysfs_port_write(struct ksz_sw *sw, int proc_num, uint n,
+	int num, const char *buf)
+{
+	uint port;
+	int processed = true;
+
+	port = get_sysfs_port(sw, n);
+	switch (proc_num) {
+	case PROC_SET_PORT_DUPLEX:
+	case PROC_SET_PORT_SPEED:
+	{
+		struct ksz_port phy_port;
+		struct ksz_port_info *port_info = get_port_info(sw, port);
+
+		if ((PROC_SET_PORT_DUPLEX == proc_num && num > 2) ||
+		    (PROC_SET_PORT_SPEED == proc_num &&
+		    num != 0 && num != 10 && num != 100))
+			break;
+
+		if (port == sw->HOST_PORT)
+			break;
+		phy_port.sw = sw;
+		phy_port.port_cnt = 1;
+		phy_port.first_port = get_log_port(sw, port);
+		phy_port.flow_ctrl = port_info->own_flow_ctrl;
+		phy_port.duplex = port_info->own_duplex;
+		phy_port.speed = port_info->own_speed;
+		if (PROC_SET_PORT_DUPLEX == proc_num)
+			phy_port.duplex = (u8) num;
+		else
+			phy_port.speed = (u16) num;
+		port_set_link_speed(&phy_port);
+		break;
+	}
+	case PROC_SET_PORT_MIB:
+		sw_cfg_mib_counter_ctrl(sw, num, port);
+		break;
+	case PROC_ENABLE_BROADCAST_STORM:
+		if (!num)
+			sw_dis_broad_storm(sw, port);
+		else
+			sw_ena_broad_storm(sw, port);
+		break;
+	case PROC_ENABLE_DIFFSERV:
+		if (!num)
+			sw_dis_diffserv(sw, port);
+		else
+			sw_ena_diffserv(sw, port);
+		break;
+	case PROC_ENABLE_802_1P:
+		if (!num)
+			sw_dis_802_1p(sw, port);
+		else
+			sw_ena_802_1p(sw, port);
+		break;
+	case PROC_SET_PORT_BASED:
+		sw_cfg_port_based(sw, port, num);
+		break;
+	case PROC_SET_DEF_VID:
+		sw_cfg_def_vid(sw, port, num);
+		break;
+	case PROC_SET_MEMBER:
+		num = get_phy_mask_from_log(sw, num);
+		sw_cfg_port_base_vlan(sw, port, (u8) num);
+		break;
+	case PROC_ENABLE_PRIO_QUEUE:
+		if (0 <= num && num <= 4)
+			port_set_prio_queue(sw, port, num);
+		break;
+	case PROC_SET_INS_TAG_0:
+		sw_cfg_ins(sw, port, 0, num);
+		break;
+	case PROC_SET_INS_TAG_1:
+		sw_cfg_ins(sw, port, 1, num);
+		break;
+	case PROC_SET_INS_TAG_2:
+		sw_cfg_ins(sw, port, 2, num);
+		break;
+	case PROC_SET_INS_TAG_3:
+		sw_cfg_ins(sw, port, 3, num);
+		break;
+	case PROC_SET_INS_TAG_4:
+		sw_cfg_ins(sw, port, 4, num);
+		break;
+	case PROC_SET_PASS_ALL:
+		port_cfg(sw, port, P_PASS_ALL_CTRL, PORT_PASS_ALL, num);
+		break;
+	case PROC_SET_TX_Q0_CTRL:
+		hw_cfg_rate_ctrl(sw, port, 0, num);
+		break;
+	case PROC_SET_TX_Q1_CTRL:
+		hw_cfg_rate_ctrl(sw, port, 1, num);
+		break;
+	case PROC_SET_TX_Q2_CTRL:
+		hw_cfg_rate_ctrl(sw, port, 2, num);
+		break;
+	case PROC_SET_TX_Q3_CTRL:
+		hw_cfg_rate_ctrl(sw, port, 3, num);
+		break;
+	case PROC_SET_TX_Q0_RATIO:
+		hw_cfg_rate_ratio(sw, port, 0, (u8) num);
+		break;
+	case PROC_SET_TX_Q1_RATIO:
+		hw_cfg_rate_ratio(sw, port, 1, (u8) num);
+		break;
+	case PROC_SET_TX_Q2_RATIO:
+		hw_cfg_rate_ratio(sw, port, 2, (u8) num);
+		break;
+	case PROC_SET_TX_Q3_RATIO:
+		hw_cfg_rate_ratio(sw, port, 3, (u8) num);
+		break;
+	case PROC_SET_RX_LIMIT:
+		hw_cfg_rx_limit(sw, port, (u8) num);
+		break;
+	case PROC_SET_RX_LIMIT_PORT_BASED:
+		hw_cfg_in_port_based(sw, port, num);
+		break;
+	case PROC_SET_LIMIT_PACKET_BASED:
+		hw_cfg_rate_packet_based(sw, port, num);
+		break;
+	case PROC_SET_RX_LIMIT_FLOW_CTRL:
+		hw_cfg_in_flow_ctrl(sw, port, num);
+		break;
+	case PROC_SET_CNT_IFG:
+		hw_cfg_cnt_ifg(sw, port, num);
+		break;
+	case PROC_SET_CNT_PRE:
+		hw_cfg_cnt_pre(sw, port, num);
+		break;
+	case PROC_SET_RX_P0_RATE:
+		hw_cfg_rx_prio_rate(sw, port, 0, num);
+		hw_set_rx_prio(sw, port);
+		break;
+	case PROC_SET_RX_P1_RATE:
+		hw_cfg_rx_prio_rate(sw, port, 1, num);
+		hw_set_rx_prio(sw, port);
+		break;
+	case PROC_SET_RX_P2_RATE:
+		hw_cfg_rx_prio_rate(sw, port, 2, num);
+		hw_set_rx_prio(sw, port);
+		break;
+	case PROC_SET_RX_P3_RATE:
+		hw_cfg_rx_prio_rate(sw, port, 3, num);
+		break;
+	case PROC_SET_TX_Q0_RATE:
+		hw_cfg_tx_prio_rate(sw, port, 0, num);
+		break;
+	case PROC_SET_TX_Q1_RATE:
+		hw_cfg_tx_prio_rate(sw, port, 1, num);
+		break;
+	case PROC_SET_TX_Q2_RATE:
+		hw_cfg_tx_prio_rate(sw, port, 2, num);
+		break;
+	case PROC_SET_TX_Q3_RATE:
+		hw_cfg_tx_prio_rate(sw, port, 3, num);
+		break;
+	case PROC_SET_MIRROR_PORT:
+		port_cfg_mirror_sniffer(sw, port, num);
+		break;
+	case PROC_SET_MIRROR_RX:
+		port_cfg_mirror_rx(sw, port, num);
+		break;
+	case PROC_SET_MIRROR_TX:
+		port_cfg_mirror_tx(sw, port, num);
+		break;
+	case PROC_SET_RX:
+		port_cfg_rx(sw, port, num);
+		break;
+	case PROC_SET_TX:
+		port_cfg_tx(sw, port, num);
+		break;
+	case PROC_SET_LEARN:
+		port_cfg_dis_learn(sw, port, !num);
+		if (!num)
+			sw_cfg(sw, S_FLUSH_TABLE_CTRL,
+				SW_FLUSH_DYN_MAC_TABLE, 1);
+		break;
+	case PROC_SET_INSERT_TAG:
+		sw_vlan_cfg_ins_tag(sw, port, num);
+		break;
+	case PROC_SET_REMOVE_TAG:
+		sw_vlan_cfg_rmv_tag(sw, port, num);
+		break;
+	case PROC_SET_DROP_TAG:
+		sw_vlan_cfg_drop_tag(sw, port, num);
+		break;
+	case PROC_SET_REPLACE_PRIO:
+		sw_cfg_replace_prio(sw, port, num);
+		break;
+	case PROC_ENABLE_RX_PRIO_RATE:
+		if (!num)
+			sw_dis_rx_prio_rate(sw, port);
+		else
+			sw_ena_rx_prio_rate(sw, port);
+		break;
+	case PROC_ENABLE_TX_PRIO_RATE:
+		if (!num)
+			sw_dis_tx_prio_rate(sw, port);
+		else
+			sw_ena_tx_prio_rate(sw, port);
+		break;
+	case PROC_SET_DIS_NON_VID:
+		sw_vlan_cfg_dis_non_vid(sw, port, num);
+		break;
+	case PROC_SET_INGRESS:
+		sw_vlan_cfg_in_filter(sw, port, num);
+		break;
+	case PROC_SET_BACK_PRESSURE:
+		port_cfg_back_pressure(sw, port, num);
+		break;
+	case PROC_SET_FORCE_FLOW_CTRL:
+		port_cfg_force_flow_ctrl(sw, port, num);
+		break;
+	case PROC_SET_UNKNOWN_UNICAST_PORT:
+		sw_cfg_unk_def_port(sw, REG_SW_UNK_UCAST_CTRL, port, num);
+		break;
+	case PROC_SET_UNKNOWN_MULTICAST_PORT:
+		sw_cfg_unk_def_port(sw, REG_SW_UNK_MCAST_CTRL, port, num);
+		break;
+	case PROC_SET_UNKNOWN_VID_PORT:
+		sw_cfg_unk_def_port(sw, REG_SW_UNK_VID_CTRL, port, num);
+		break;
+	case PROC_SET_UNKNOWN_IP_MULTICAST_PORT:
+		sw_cfg_unk_def_port(sw, REG_SW_UNK_IP_MCAST_CTRL, port, num);
+		break;
+	case PROC_SET_PORT_PME_CTRL:
+		port_set_pme_ctrl(sw, port, num);
+		break;
+	case PROC_SET_PORT_PME_STATUS:
+		port_set_pme_status(sw, port, num);
+		break;
+	case PROC_SET_AUTHEN_MODE:
+		port_set_authen_mode(sw, port, num);
+		break;
+	case PROC_SET_ACL:
+		port_cfg_acl(sw, port, num);
+		break;
+	case PROC_SET_LINK_MD:
+		sw_get_link_md(sw, port);
+		break;
+	default:
+		processed = false;
+		break;
+	}
+	return processed;
+}  /* sysfs_port_write */
+
+static ssize_t sysfs_mac_read(struct ksz_sw *sw, int proc_num, ssize_t len,
+	char *buf)
+{
+	struct ksz_mac_table *entry;
+	u8 ports;
+
+	entry = &sw->info->mac_entry;
+	switch (proc_num) {
+	case PROC_SET_STATIC_FID:
+		len += sprintf(buf + len, "0x%02x\n", entry->fid);
+		break;
+	case PROC_SET_STATIC_USE_FID:
+		len += sprintf(buf + len, "%u\n", entry->use_fid);
+		break;
+	case PROC_SET_STATIC_OVERRIDE:
+		len += sprintf(buf + len, "%u\n", entry->override);
+		break;
+	case PROC_SET_STATIC_VALID:
+		len += sprintf(buf + len, "%u\n", entry->valid);
+		break;
+	case PROC_SET_STATIC_PORTS:
+		ports = entry->ports;
+		ports = get_log_mask_from_phy(sw, entry->ports);
+		len += sprintf(buf + len, "0x%02x\n", ports);
+		break;
+	case PROC_SET_STATIC_MAC_ADDR:
+		len += sprintf(buf + len, "%02x:%02x:%02x:%02x:%02x:%02x\n",
+			entry->addr[0], entry->addr[1],
+			entry->addr[2], entry->addr[3],
+			entry->addr[4], entry->addr[5]);
+		break;
+	case PROC_SET_STATIC_INDEX:
+		len += sprintf(buf + len, "0x%02x\n", sw->mac_index);
+		break;
+	case PROC_GET_STATIC_INFO:
+		if (sw->mac_dirty) {
+			if (!entry->dirty)
+				sw_r_sta_mac_table(sw, sw->mac_index, entry);
+			sw->mac_dirty = 0;
+		}
+		ports = entry->ports;
+		ports = get_log_mask_from_phy(sw, ports);
+		len += sprintf(buf + len,
+			"%2x: %02X:%02X:%02X:%02X:%02X:%02X "
+			"%02x o:%u %u:%02x [%u]\n",
+			sw->mac_index,
+			entry->addr[0], entry->addr[1], entry->addr[2],
+			entry->addr[3], entry->addr[4], entry->addr[5],
+			ports, entry->override,
+			entry->use_fid, entry->fid,
+			entry->dirty ? 2 : entry->valid);
+		break;
+	}
+	return len;
+}  /* sysfs_mac_read */
+
+static int sysfs_mac_write(struct ksz_sw *sw, int proc_num, int num,
+	const char *buf)
+{
+	struct ksz_mac_table *entry;
+	u8 ports;
+	int processed = true;
+
+	entry = &sw->info->mac_entry;
+	switch (proc_num) {
+	case PROC_SET_STATIC_FID:
+		if (0 <= num && num < 128) {
+			entry->fid = num;
+			entry->dirty = 1;
+		}
+		break;
+	case PROC_SET_STATIC_USE_FID:
+		if (num)
+			entry->use_fid = 1;
+		else
+			entry->use_fid = 0;
+		entry->dirty = 1;
+		break;
+	case PROC_SET_STATIC_OVERRIDE:
+		if (num)
+			entry->override = 1;
+		else
+			entry->override = 0;
+		entry->dirty = 1;
+		break;
+	case PROC_SET_STATIC_VALID:
+		if (num)
+			entry->valid = 1;
+		else
+			entry->valid = 0;
+		sw_w_sta_mac_table(sw, sw->mac_index, entry);
+		break;
+	case PROC_SET_STATIC_PORTS:
+		if (0 <= num && num <= sw->PORT_MASK) {
+			ports = num;
+			ports = get_phy_mask_from_log(sw, ports);
+			entry->ports = ports;
+			entry->dirty = 1;
+		}
+		break;
+	case PROC_SET_STATIC_MAC_ADDR:
+	{
+		int i;
+		int n[6];
+
+		i = sscanf(buf, "%x:%x:%x:%x:%x:%x",
+			&n[0], &n[1], &n[2], &n[3], &n[4], &n[5]);
+		if (6 == i) {
+			for (i = 0; i < 6; i++)
+				entry->addr[i] = (u8) n[i];
+			entry->dirty = 1;
+		}
+		break;
+	}
+	case PROC_SET_STATIC_INDEX:
+		if (0 <= num && num < STATIC_MAC_TABLE_ENTRIES) {
+			sw->mac_index = num;
+			sw->mac_dirty = 1;
+		}
+		break;
+	default:
+		processed = false;
+		break;
+	}
+	return processed;
+}  /* sysfs_mac_write */
+
+static ssize_t sysfs_vlan_read(struct ksz_sw *sw, int proc_num,	ssize_t len,
+	char *buf)
+{
+	struct ksz_vlan_table *entry;
+
+	entry = &sw->info->vlan_entry;
+	switch (proc_num) {
+	case PROC_SET_VLAN_VALID:
+		len += sprintf(buf + len, "%u\n", entry->valid);
+		break;
+	case PROC_SET_VLAN_MEMBER:
+		len += sprintf(buf + len, "0x%02x\n", entry->member);
+		break;
+	case PROC_SET_VLAN_FID:
+		len += sprintf(buf + len, "0x%02x\n", entry->fid);
+		break;
+	case PROC_SET_VLAN_VID:
+		len += sprintf(buf + len, "0x%03x\n", sw->vlan_index);
+		break;
+	case PROC_GET_VLAN_INFO:
+		if (sw->vlan_dirty) {
+			if (!entry->dirty) {
+				sw_r_vlan_table(sw, sw->vlan_index, entry);
+				entry->member =
+					get_log_mask_from_phy(sw, entry->member);
+			}
+			sw->vlan_dirty = 0;
+		}
+		len += sprintf(buf + len,
+			"%3x: 0x%02x %02x [%u]\n",
+			sw->vlan_index, entry->fid, entry->member,
+			entry->dirty ? 2 : entry->valid);
+		break;
+	}
+	return len;
+}  /* sysfs_vlan_read */
+
+static int sysfs_vlan_write(struct ksz_sw *sw, int proc_num, int num)
+{
+	struct ksz_vlan_table *entry;
+	u8 ports;
+	int processed = true;
+
+	entry = &sw->info->vlan_entry;
+	switch (proc_num) {
+	case PROC_SET_VLAN_VALID:
+		if (num)
+			entry->valid = 1;
+		else
+			entry->valid = 0;
+		ports = entry->member;
+		entry->member = get_phy_mask_from_log(sw, entry->member);
+		sw_w_vlan_table(sw, sw->vlan_index, entry);
+		entry->member = ports;
+		sw->vlan_dirty = 0;
+		break;
+	case PROC_SET_VLAN_MEMBER:
+		if (0 <= num && num <= sw->PORT_MASK) {
+			entry->member = num;
+			entry->dirty = 1;
+		}
+		break;
+	case PROC_SET_VLAN_FID:
+		if (0 <= num && num < 128) {
+			entry->fid = num;
+			entry->dirty = 1;
+		}
+		break;
+	case PROC_SET_VLAN_VID:
+		if (0 <= num && num < 0x1000) {
+			sw->vlan_index = num;
+			sw->vlan_dirty = 1;
+		}
+		break;
+	default:
+		processed = false;
+		break;
+	}
+	return processed;
+}  /* sysfs_vlan_write */
+
+static ssize_t sysfs_acl_read(struct ksz_sw *sw, int proc_num, uint n,
+	ssize_t len, char *buf)
+{
+	struct ksz_port_cfg *cfg;
+	struct ksz_acl_table *acl;
+	struct ksz_acl_table *action;
+	struct ksz_acl_table *ruleset;
+	uint port;
+	int chk = 0;
+	int type = SHOW_HELP_NONE;
+	char note[40];
+
+	note[0] = '\0';
+	port = get_sysfs_port(sw, n);
+	cfg = get_port_cfg(sw, port);
+	acl = &cfg->acl_info[cfg->acl_index];
+	action = &cfg->acl_info[cfg->acl_act_index];
+	ruleset = &cfg->acl_info[cfg->acl_rule_index];
+	switch (proc_num) {
+	case PROC_SET_ACL_FIRST_RULE:
+		chk = ruleset->first_rule;
+		type = SHOW_HELP_HEX;
+		break;
+	case PROC_SET_ACL_RULESET:
+		len += acl_ruleset_info(ruleset, cfg->acl_rule_index, buf, len);
+		break;
+	case PROC_SET_ACL_MODE:
+		chk = acl->mode;
+		if (sw->verbose) {
+			switch (chk) {
+			case 1:
+				strcpy(note, " (layer 2)");
+				break;
+			case 2:
+				strcpy(note, " (layer 3)");
+				break;
+			case 3:
+				strcpy(note, " (layer 4)");
+				break;
+			default:
+				strcpy(note, " (off)");
+			}
+		}
+		type = SHOW_HELP_SPECIAL;
+		break;
+	case PROC_SET_ACL_ENABLE:
+		chk = acl->enable;
+		if (sw->verbose) {
+			switch (chk) {
+			case 1:
+				strcpy(note, " (type; ip; tcp port)");
+				break;
+			case 2:
+				strcpy(note, " (mac; src/dst; udp port)");
+				break;
+			case 3:
+				strcpy(note, " (both; -; tcp seq)");
+				break;
+			default:
+				strcpy(note, " (count; -; protocol)");
+			}
+		}
+		type = SHOW_HELP_SPECIAL;
+		break;
+	case PROC_SET_ACL_SRC:
+		chk = acl->src;
+		type = SHOW_HELP_ON_OFF;
+		break;
+	case PROC_SET_ACL_EQUAL:
+		chk = acl->equal;
+		type = SHOW_HELP_ON_OFF;
+		break;
+	case PROC_SET_ACL_PRIO_MODE:
+		chk = action->prio_mode;
+		if (sw->verbose) {
+			switch (chk) {
+			case 1:
+				strcpy(note, " (higher)");
+				break;
+			case 2:
+				strcpy(note, " (lower)");
+				break;
+			case 3:
+				strcpy(note, " (replace)");
+				break;
+			default:
+				strcpy(note, " (disabled)");
+			}
+		}
+		type = SHOW_HELP_SPECIAL;
+		break;
+	case PROC_SET_ACL_PRIO:
+		chk = action->prio;
+		type = SHOW_HELP_NUM;
+		break;
+	case PROC_SET_ACL_VLAN_PRIO_REPLACE:
+		chk = action->vlan_prio_replace;
+		type = SHOW_HELP_ON_OFF;
+		break;
+	case PROC_SET_ACL_VLAN_PRIO:
+		chk = action->vlan_prio;
+		type = SHOW_HELP_NUM;
+		break;
+	case PROC_SET_ACL_MAP_MODE:
+		chk = action->map_mode;
+		if (sw->verbose) {
+			switch (chk) {
+			case 1:
+				strcpy(note, " (or)");
+				break;
+			case 2:
+				strcpy(note, " (and)");
+				break;
+			case 3:
+				strcpy(note, " (replace)");
+				break;
+			default:
+				strcpy(note, " (disabled)");
+			}
+		}
+		type = SHOW_HELP_SPECIAL;
+		break;
+	case PROC_SET_ACL_PORTS:
+		chk = action->ports;
+		type = SHOW_HELP_HEX_4;
+		break;
+	case PROC_SET_ACL_MAC_ADDR:
+		len += sprintf(buf + len, "%02x:%02x:%02x:%02x:%02x:%02x\n",
+			acl->mac[0], acl->mac[1], acl->mac[2],
+			acl->mac[3], acl->mac[4], acl->mac[5]);
+		break;
+	case PROC_SET_ACL_TYPE:
+		chk = acl->eth_type;
+		type = SHOW_HELP_HEX_4;
+		break;
+	case PROC_SET_ACL_CNT:
+		chk = acl->cnt;
+		type = SHOW_HELP_NUM;
+		break;
+	case PROC_SET_ACL_MSEC:
+		chk = acl->msec;
+		type = SHOW_HELP_ON_OFF;
+		break;
+	case PROC_SET_ACL_INTR_MODE:
+		chk = acl->intr_mode;
+		type = SHOW_HELP_ON_OFF;
+		break;
+	case PROC_SET_ACL_IP_ADDR:
+		len += sprintf(buf + len, "%u.%u.%u.%u\n",
+			acl->ip4_addr[0], acl->ip4_addr[1],
+			acl->ip4_addr[2], acl->ip4_addr[3]);
+		break;
+	case PROC_SET_ACL_IP_MASK:
+		len += sprintf(buf + len, "%u.%u.%u.%u\n",
+			acl->ip4_mask[0], acl->ip4_mask[1],
+			acl->ip4_mask[2], acl->ip4_mask[3]);
+		break;
+	case PROC_SET_ACL_PROTOCOL:
+		chk = acl->protocol;
+		type = SHOW_HELP_NUM;
+		break;
+	case PROC_SET_ACL_PORT_MODE:
+		chk = acl->port_mode;
+		if (sw->verbose) {
+			switch (chk) {
+			case 1:
+				strcpy(note, " (either)");
+				break;
+			case 2:
+				strcpy(note, " (in range)");
+				break;
+			case 3:
+				strcpy(note, " (out of range)");
+				break;
+			default:
+				strcpy(note, " (disabled)");
+			}
+		}
+		type = SHOW_HELP_SPECIAL;
+		break;
+	case PROC_SET_ACL_MAX_PORT:
+		chk = acl->max_port;
+		type = SHOW_HELP_NUM;
+		break;
+	case PROC_SET_ACL_MIN_PORT:
+		chk = acl->min_port;
+		type = SHOW_HELP_NUM;
+		break;
+	case PROC_SET_ACL_SEQNUM:
+		chk = acl->seqnum;
+		type = SHOW_HELP_NUM;
+		break;
+	case PROC_SET_ACL_TCP_FLAG_ENABLE:
+		chk = acl->tcp_flag_enable;
+		type = SHOW_HELP_ON_OFF;
+		break;
+	case PROC_SET_ACL_TCP_FLAG:
+		chk = acl->tcp_flag;
+		type = SHOW_HELP_HEX;
+		break;
+	case PROC_SET_ACL_TCP_FLAG_MASK:
+		chk = acl->tcp_flag_mask;
+		type = SHOW_HELP_HEX;
+		break;
+	case PROC_SET_ACL_INDEX:
+		chk = cfg->acl_index;
+		type = SHOW_HELP_HEX;
+		break;
+	case PROC_SET_ACL_ACTION_INDEX:
+		chk = cfg->acl_act_index;
+		type = SHOW_HELP_HEX;
+		break;
+	case PROC_SET_ACL_ACTION:
+		len += acl_action_info(action, cfg->acl_act_index, buf, len);
+		break;
+	case PROC_SET_ACL_RULE_INDEX:
+		chk = cfg->acl_rule_index;
+		type = SHOW_HELP_HEX;
+		break;
+	case PROC_SET_ACL_INFO:
+		len += acl_info(acl, cfg->acl_index, buf, len);
+		break;
+	case PROC_GET_ACL_TABLE:
+		len = sw_d_acl_table(sw, port, buf, len);
+		break;
+	}
+	return sysfs_show(len, buf, type, chk, note, sw->verbose);
+}  /* sysfs_acl_read */
+
+static int sysfs_acl_write(struct ksz_sw *sw, int proc_num, uint n, int num,
+	const char *buf)
+{
+	struct ksz_port_cfg *cfg;
+	struct ksz_acl_table *acl;
+	struct ksz_acl_table *action;
+	struct ksz_acl_table *ruleset;
+	int processed = true;
+	u8 phy, link = 0;
+	uint port;
+
+	port = get_sysfs_port(sw, n);
+	cfg = get_port_cfg(sw, port);
+	acl = &cfg->acl_info[cfg->acl_index];
+	action = &cfg->acl_info[cfg->acl_act_index];
+	ruleset = &cfg->acl_info[cfg->acl_rule_index];
+	switch (proc_num) {
+	case PROC_SET_ACL_RULESET:
+	case PROC_SET_ACL_MODE:
+	case PROC_SET_ACL_ACTION:
+	case PROC_SET_ACL_INFO:
+		sw_access_acl(sw, port, &link, &phy);
+		break;
+	}
+	switch (proc_num) {
+	case PROC_SET_ACL_FIRST_RULE:
+		ruleset->first_rule = (u16) num;
+		ruleset->ruleset_changed = 1;
+		break;
+	case PROC_SET_ACL_RULESET:
+		sscanf(buf, "%x", &num);
+		ruleset->ruleset = (u16) num;
+		sw_w_acl_ruleset(sw, port, cfg->acl_rule_index, ruleset);
+		break;
+	case PROC_SET_ACL_MODE:
+		if (0 <= num && num < 4) {
+			acl->mode = num;
+			sw_w_acl_rule(sw, port, cfg->acl_index, acl);
+		}
+		break;
+	case PROC_SET_ACL_ENABLE:
+		if (0 <= num && num < 4) {
+			acl->enable = num;
+			acl->changed = 1;
+		}
+		break;
+	case PROC_SET_ACL_SRC:
+		if (num)
+			acl->src = 1;
+		else
+			acl->src = 0;
+		acl->changed = 1;
+		break;
+	case PROC_SET_ACL_EQUAL:
+		if (num)
+			acl->equal = 1;
+		else
+			acl->equal = 0;
+		acl->changed = 1;
+		break;
+	case PROC_SET_ACL_PRIO_MODE:
+		if (0 <= num && num < 4) {
+			action->prio_mode = num;
+			action->action_changed = 1;
+		}
+		break;
+	case PROC_SET_ACL_PRIO:
+		if (0 <= num && num < 4) {
+			action->prio = num;
+			action->action_changed = 1;
+		}
+		break;
+	case PROC_SET_ACL_VLAN_PRIO_REPLACE:
+		if (num)
+			action->vlan_prio_replace = 1;
+		else
+			action->vlan_prio_replace = 0;
+		action->action_changed = 1;
+		break;
+	case PROC_SET_ACL_VLAN_PRIO:
+		if (0 <= num && num < 4) {
+			action->vlan_prio = num;
+			action->action_changed = 1;
+		}
+		break;
+	case PROC_SET_ACL_MAP_MODE:
+		if (0 <= num && num < 4) {
+			action->map_mode = num;
+			action->action_changed = 1;
+		}
+		break;
+	case PROC_SET_ACL_PORTS:
+		sscanf(buf, "%x", &num);
+		if (0 <= num && num <= sw->PORT_MASK) {
+			action->ports = (u16) num;
+			action->action_changed = 1;
+		}
+		break;
+	case PROC_SET_ACL_MAC_ADDR:
+	{
+		int i;
+		int n[6];
+
+		i = sscanf(buf, "%x:%x:%x:%x:%x:%x",
+			&n[0], &n[1], &n[2], &n[3], &n[4], &n[5]);
+		if (6 == i) {
+			for (i = 0; i < 6; i++)
+				acl->mac[i] = (u8) n[i];
+			acl->changed = 1;
+		}
+		break;
+	}
+	case PROC_SET_ACL_TYPE:
+		sscanf(buf, "%x", &num);
+		acl->eth_type = (u16) num;
+		acl->changed = 1;
+		break;
+	case PROC_SET_ACL_CNT:
+		if (0 <= num && num <= ACL_CNT_M) {
+			acl->cnt = (u16) num;
+			acl->changed = 1;
+		}
+		break;
+	case PROC_SET_ACL_MSEC:
+		if (num)
+			acl->msec = 1;
+		else
+			acl->msec = 0;
+		acl->changed = 1;
+		break;
+	case PROC_SET_ACL_INTR_MODE:
+		if (num)
+			acl->intr_mode = 1;
+		else
+			acl->intr_mode = 0;
+		acl->changed = 1;
+		break;
+	case PROC_SET_ACL_IP_ADDR:
+	{
+		int i;
+		int n[4];
+
+		i = sscanf(buf, "%u.%u.%u.%u",
+			&n[0], &n[1], &n[2], &n[3]);
+		if (4 == i) {
+			for (i = 0; i < 4; i++)
+				acl->ip4_addr[i] = (u8) n[i];
+			acl->changed = 1;
+		}
+		break;
+	}
+	case PROC_SET_ACL_IP_MASK:
+	{
+		int i;
+		int n[4];
+
+		i = sscanf(buf, "%u.%u.%u.%u",
+			&n[0], &n[1], &n[2], &n[3]);
+		if (4 == i) {
+			for (i = 0; i < 4; i++)
+				acl->ip4_mask[i] = (u8) n[i];
+			acl->changed = 1;
+		}
+		break;
+	}
+	case PROC_SET_ACL_PROTOCOL:
+		acl->protocol = (u8) num;
+		acl->changed = 1;
+		break;
+	case PROC_SET_ACL_PORT_MODE:
+		if (0 <= num && num < 4) {
+			acl->port_mode = num;
+			acl->changed = 1;
+		}
+		break;
+	case PROC_SET_ACL_MAX_PORT:
+		acl->max_port = (u16) num;
+		acl->changed = 1;
+		break;
+	case PROC_SET_ACL_MIN_PORT:
+		acl->min_port = (u16) num;
+		acl->changed = 1;
+		break;
+	case PROC_SET_ACL_SEQNUM:
+		acl->seqnum = num;
+		acl->changed = 1;
+		break;
+	case PROC_SET_ACL_TCP_FLAG_ENABLE:
+		if (num)
+			acl->tcp_flag_enable = 1;
+		else
+			acl->tcp_flag_enable = 0;
+		acl->changed = 1;
+		break;
+	case PROC_SET_ACL_TCP_FLAG:
+		acl->tcp_flag = (u8) num;
+		acl->changed = 1;
+		break;
+	case PROC_SET_ACL_TCP_FLAG_MASK:
+		acl->tcp_flag_mask = (u8) num;
+		acl->changed = 1;
+		break;
+	case PROC_SET_ACL_INDEX:
+		if (0 <= num && num < ACL_TABLE_ENTRIES) {
+			cfg->acl_index = num;
+		}
+		break;
+	case PROC_SET_ACL_ACTION_INDEX:
+		if (0 <= num && num < ACL_TABLE_ENTRIES) {
+			cfg->acl_act_index = num;
+		}
+		break;
+	case PROC_SET_ACL_ACTION:
+		if (num)
+			sw_w_acl_action(sw, port, cfg->acl_act_index, action);
+		else
+			sw_r_acl_table(sw, port, cfg->acl_act_index, action);
+		break;
+	case PROC_SET_ACL_RULE_INDEX:
+		if (0 <= num && num < ACL_TABLE_ENTRIES) {
+			cfg->acl_rule_index = num;
+		}
+		break;
+	case PROC_SET_ACL_INFO:
+		sw_r_acl_table(sw, port, cfg->acl_index, acl);
+		break;
+	default:
+		processed = false;
+		break;
+	}
+	switch (proc_num) {
+	case PROC_SET_ACL_RULESET:
+	case PROC_SET_ACL_MODE:
+	case PROC_SET_ACL_ACTION:
+	case PROC_SET_ACL_INFO:
+		if (link) {
+			sw->ops->acquire(sw);
+			port_w8(sw, port, REG_PORT_CTRL_10, phy);
+
+			/* Port 2 has difficulty getting link again if not
+			 * reset.
+			 */
+			if (port == 1)
+				port_w8(sw, port, REG_PORT_STATUS_3,
+					PORT_PHY_SOFT_RESET);
+			sw->ops->release(sw);
+		}
+		break;
+	}
+	return processed;
+}  /* sysfs_acl_write */
+
+static void sw_cfg_mac(struct ksz_sw *sw, u8 index, u8 *dest, u32 ports,
+	int override, int use_fid, u16 fid)
+{
+	struct ksz_mac_table mac;
+
+	if (index >= STATIC_MAC_TABLE_ENTRIES)
+		return;
+	memset(&mac, 0, sizeof(struct ksz_mac_table));
+	memcpy(mac.addr, dest, ETH_ALEN);
+	mac.ports = (u8) ports;
+	mac.override = override;
+	mac.use_fid = use_fid;
+	mac.fid = (u8) fid;
+	mac.valid = mac.ports != 0;
+	if (!mac.valid && mac.override) {
+		mac.override = 0;
+		mac.valid = 1;
+	}
+	sw_w_sta_mac_table(sw, index, &mac);
+}  /* sw_cfg_mac */
+
+static void sw_cfg_vlan(struct ksz_sw *sw, u8 index, u16 vid, u16 fid,
+	u32 ports)
+{
+	struct ksz_vlan_table vlan;
+
+	if (0xffff == ports)
+		ports = sw->PORT_MASK;
+	memset(&vlan, 0, sizeof(struct ksz_vlan_table));
+	vlan.vid = vid;
+	vlan.fid = (u8) fid;
+	vlan.member = (u8)(ports & sw->PORT_MASK);
+	vlan.valid = ports != 0;
+	sw_w_vlan_table(sw, vlan.vid, &vlan);
+}  /* sw_cfg_vlan */
+
+static u8 sw_alloc_mac(struct ksz_sw *sw)
+{
+	int i;
+
+	for (i = 1; i < STATIC_MAC_TABLE_ENTRIES; i++) {
+		if (!(sw->info->mac_table_used & (1 << i))) {
+			sw->info->mac_table_used |= (1 << i);
+			return i;
+		}
+	}
+
+	/* Reject request. */
+	return 0;
+}  /* sw_alloc_mac */
+
+static void sw_free_mac(struct ksz_sw *sw, u8 index)
+{
+	sw->info->mac_table_used &= ~(1 << index);
+}  /* sw_free_mac */
+
+static u8 sw_alloc_vlan(struct ksz_sw *sw)
+{
+	return 1;
+}  /* sw_alloc_vlan */
+
+static void sw_free_vlan(struct ksz_sw *sw, u8 index)
+{
+}  /* sw_free_vlan */
+
+static u16 sw_alloc_fid(struct ksz_sw *sw, u16 vid)
+{
+	int x;
+	int y;
+	u16 fid;
+
+	if (sw->info->fid_cnt + 2 == FID_ENTRIES)
+		return 0;
+	fid = vid & (FID_ENTRIES - 1);
+	if (vid < 2)
+		fid = 100;
+	x = fid / FID_IN_DATA;
+	y = fid % FID_IN_DATA;
+	while (sw->info->fid[x] & (1 << y)) {
+		++fid;
+		++y;
+		if (y >= FID_IN_DATA) {
+			y = 0;
+			++x;
+		}
+	}
+	sw->info->fid[x] |= (1 << y);
+	++sw->info->fid_cnt;
+	return fid;
+}  /* sw_alloc_fid */
+
+static void sw_free_fid(struct ksz_sw *sw, u16 fid)
+{
+	int x;
+	int y;
+
+	x = fid / FID_IN_DATA;
+	y = fid % FID_IN_DATA;
+	if (sw->info->fid[x] & (1 << y)) {
+		sw->info->fid[x] &= ~(1 << y);
+		--sw->info->fid_cnt;
+	}
+}  /* sw_free_fid */
+
+static const u8 *sw_get_br_id(struct ksz_sw *sw)
+{
+	static u8 id[8];
+	const u8* ret = id;
+
+	memcpy(&id[2], sw->info->mac_addr, ETH_ALEN);
+	id[0] = 0x80;
+	id[1] = 0x00;
+
+#ifdef CONFIG_KSZ_STP
+	ret = stp_br_id(&sw->info->rstp);
+#endif
+	return ret;
+}  /* sw_get_br_id */
+
+static void sw_from_backup(struct ksz_sw *sw, uint p)
+{
+#ifdef CONFIG_KSZ_MRP
+	if (sw->features & MRP_SUPPORT) {
+		struct mrp_info *mrp = &sw->mrp;
+
+		mrp->ops->from_backup(mrp, p);
+	}
+#endif
+}  /* sw_from_backup */
+
+static void sw_to_backup(struct ksz_sw *sw, uint p)
+{
+#ifdef CONFIG_KSZ_MRP
+	if (sw->features & MRP_SUPPORT) {
+		struct mrp_info *mrp = &sw->mrp;
+
+		mrp->ops->to_backup(mrp, p);
+	}
+#endif
+}  /* sw_to_backup */
+
+static void sw_from_designated(struct ksz_sw *sw, uint p, bool alt)
+{
+#ifdef CONFIG_KSZ_MRP
+	if (sw->features & MRP_SUPPORT) {
+		struct mrp_info *mrp = &sw->mrp;
+
+		mrp->ops->from_designated(mrp, p, alt);
+	}
+#endif
+}  /* sw_from_designated */
+
+static void sw_to_designated(struct ksz_sw *sw, uint p)
+{
+#ifdef CONFIG_KSZ_MRP
+	if (sw->features & MRP_SUPPORT) {
+		struct mrp_info *mrp = &sw->mrp;
+
+		mrp->ops->to_designated(mrp, p);
+	}
+#endif
+}  /* sw_to_designated */
+
+static void sw_tc_detected(struct ksz_sw *sw, uint p)
+{
+#ifdef CONFIG_KSZ_MRP
+	if (sw->features & MRP_SUPPORT) {
+		struct mrp_info *mrp = &sw->mrp;
+
+		mrp->ops->tc_detected(mrp, p);
+	}
+#endif
+}  /* sw_tc_detected */
+
+static int sw_get_tcDetected(struct ksz_sw *sw, uint p)
+{
+	int ret = false;
+
+#ifdef CONFIG_KSZ_STP
+	if (sw->features & STP_SUPPORT) {
+		struct ksz_stp_info *info = &sw->info->rstp;
+
+		ret = info->ops->get_tcDetected(info, p);
+	}
+#endif
+	return ret;
+}  /* sw_get_tcDetected */
+
+static void sw_ena_intr(struct ksz_sw *sw)
+{
+	struct sw_priv *ks = container_of(sw, struct sw_priv, sw);
+
+	sw->reg->w8(sw, REG_INT_ENABLE, ks->intr_mask);
+	sw->reg->w8(sw, REG_ACL_INT_ENABLE, INT_PORT_ALL);
+}  /* sw_ena_intr */
+
+enum {
+	KSZ8795_SW_CHIP,
+	KSZ8794_SW_CHIP,
+	KSZ8765_SW_CHIP,
+};
+
+static void sw_cfg_tail_tag(struct ksz_sw *sw, bool enable)
+{
+	/* Switch marks the maximum frame with extra byte as oversize. */
+	sw_cfg(sw, REG_SW_CTRL_2, SW_LEGAL_PACKET_DISABLE, enable);
+	sw_cfg(sw, S_TAIL_TAG_CTRL, SW_TAIL_TAG_ENABLE, enable);
+}
+
+static void sw_set_multi(struct ksz_sw *sw, struct net_device *dev,
+	struct ksz_port *priv)
+{
+	struct ksz_mac_table *entry;
+	struct ksz_alu_table *alu;
+	struct netdev_hw_addr *ha;
+	int i;
+	int found;
+	int owner;
+	int port = 0;
+	struct ksz_sw_info *info = sw->info;
+
+	/* This device is associated with a switch port. */
+	if (1 == priv->port_cnt)
+		port = priv->first_port;
+	owner = 1 << port;
+
+	/* Remove old multicast entries. */
+	for (i = SWITCH_MAC_TABLE_ENTRIES; i < info->multi_net; i++) {
+		entry = &info->mac_table[i];
+		alu = &info->alu_table[i];
+		if (alu->valid && (alu->owner & owner)) {
+			/* Remove device ownership. */
+			alu->owner &= ~owner;
+			if (!port)
+				alu->forward &= ~FWD_MAIN_DEV;
+			else if (alu->owner <= 1)
+				alu->forward &= ~FWD_STP_DEV;
+			if (!alu->owner) {
+				alu->valid = 0;
+				entry->ports = 0;
+				entry->valid = 0;
+			}
+		}
+	}
+	netdev_for_each_mc_addr(ha, dev) {
+		if (!(*ha->addr & 1))
+			continue;
+		if (info->multi_net == info->multi_sys)
+			break;
+		found = 0;
+		for (i = 0; i < MULTI_MAC_TABLE_ENTRIES; i++) {
+			entry = &info->mac_table[i];
+			alu = &info->alu_table[i];
+			if (alu->valid &&
+			    !memcmp(entry->addr, ha->addr, ETH_ALEN)) {
+				found = i + 1;
+				break;
+			}
+			if (!alu->valid && !found &&
+			    i >= SWITCH_MAC_TABLE_ENTRIES &&
+			    i < info->multi_net)
+				found = i + 1;
+		}
+		if (!found) {
+			info->multi_net++;
+			found = info->multi_net;
+		}
+		found--;
+		if (found >= SWITCH_MAC_TABLE_ENTRIES &&
+		    found < info->multi_net) {
+			entry = &info->mac_table[found];
+			alu = &info->alu_table[found];
+			if (port)
+				alu->forward |= FWD_STP_DEV;
+			else
+				alu->forward |= FWD_MAIN_DEV;
+			alu->owner |= owner;
+			alu->valid = 1;
+			memcpy(entry->addr, ha->addr, ETH_ALEN);
+			entry->ports = sw->PORT_MASK;
+			entry->valid = 1;
+		}
+	}
+}  /* sw_set_multi */
+
+static struct net_device *sw_rx_dev(struct ksz_sw *sw, u8 *data, u32 *len,
+	int *tag, int *port)
+{
+	struct vlan_ethhdr *vlan = (struct vlan_ethhdr *) data;
+	struct net_device *dev;
+	u16 *proto_loc;
+	u16 proto;
+	int index = -1;
+	int vid = 0;
+
+	proto_loc = &vlan->h_vlan_proto;
+	proto = htons(*proto_loc);
+
+	/* Ignore PAUSE frame sent by switch. */
+	if (!memcmp(vlan->h_source, sw->info->mac_addr, ETH_ALEN) &&
+	    proto == ETH_P_PAUSE)
+		return NULL;
+
+	/* Get received port number. */
+	if (sw->overrides & TAIL_TAGGING) {
+		(*len)--;
+		*tag = data[*len];
+
+		/* In case tagging is not working right. */
+		if (*tag >= SWITCH_PORT_NUM)
+			*tag = 0;
+
+		/* Save receiving port. */
+		*port = *tag;
+		index = sw->info->port_cfg[*tag].index;
+	}
+
+	/* Determine network device from VLAN id. */
+	if (index < 0) {
+		index = 0;
+		if (vlan->h_vlan_proto == htons(ETH_P_8021Q)) {
+			u16 vlan_tci = ntohs(vlan->h_vlan_TCI);
+
+			vid = vlan_tci & VLAN_VID_MASK;
+			proto_loc = &vlan->h_vlan_encapsulated_proto;
+			proto = htons(*proto_loc);
+		}
+		if (vid && (sw->features & SW_VLAN_DEV)) {
+			struct ksz_dev_map *map;
+			int p;
+
+			for (p = 0; p < sw->eth_cnt; p++) {
+				map = &sw->eth_maps[p];
+				if (vid == map->vlan) {
+					*port = map->first;
+					p = get_phy_port(sw, *port);
+					*port = p;
+					index = sw->info->port_cfg[p].index;
+					break;
+				}
+			}
+		}
+	}
+	if (index >= sw->dev_count + sw->dev_offset) {
+		printk(KERN_INFO "  [%s] netdev not correct\n", __func__);
+		BUG();
+	}
+	dev = sw->netdev[index];
+#ifdef CONFIG_KSZ_DLR
+	if (proto == DLR_TAG_TYPE)
+		return dev;
+#endif
+	if (!(sw->features & VLAN_PORT_TAGGING) ||
+	    !(sw->vlan_id & (1 << *tag))) {
+		*tag = 0;
+	}
+	return dev;
+}  /* sw_rx_dev */
+
+static int pkt_matched(struct sk_buff *skb, struct net_device *dev, void *ptr,
+	int (*match_multi)(void *ptr, u8 *addr), u8 h_promiscuous)
+{
+	int drop = false;
+	u8 bcast_addr[] = { 0xff, 0xff, 0xff, 0xff, 0xff, 0xff };
+
+	if (skb->data[0] & 0x01) {
+		if (memcmp(skb->data, bcast_addr, ETH_ALEN))
+			drop = match_multi(ptr, skb->data);
+	} else if (h_promiscuous && memcmp(skb->data, dev->dev_addr, ETH_ALEN))
+		drop = true;
+	if (drop)
+		return 0;
+	return skb->len;
+}  /* pkt_matched */
+
+static int sw_match_pkt(struct ksz_sw *sw, struct net_device **dev,
+	void **priv, int (*get_promiscuous)(void *ptr),
+	int (*match_multi)(void *ptr, u8 *data), struct sk_buff *skb,
+	u8 h_promiscuous)
+{
+	int s_promiscuous;
+
+	if (sw->dev_count <= 1)
+		return true;
+	s_promiscuous = get_promiscuous(*priv);
+	if (!s_promiscuous && !pkt_matched(skb, *dev, *priv, match_multi,
+			h_promiscuous)) {
+		int matched = false;
+
+		/* There is a parent network device. */
+		if (sw->dev_offset) {
+			matched = true;
+			*dev = sw->netdev[0];
+			*priv = netdev_priv(*dev);
+			s_promiscuous = get_promiscuous(*priv);
+			if (!s_promiscuous && !pkt_matched(skb, *dev, *priv,
+					match_multi, h_promiscuous))
+				matched = false;
+		}
+		return matched;
+	}
+	return true;
+}  /* sw_match_pkt */
+
+static struct net_device *sw_parent_rx(struct ksz_sw *sw,
+				       struct net_device *dev, int *forward)
+{
+	if (sw->dev_offset && dev != sw->netdev[0]) {
+		if (!*forward)
+			*forward = FWD_MAIN_DEV;
+		if (!(*forward & FWD_STP_DEV))
+			dev = sw->netdev[0];
+		else
+			*forward &= ~FWD_VLAN_DEV;
+	}
+	return dev;
+}  /* sw_parent_rx */
+
+static int sw_port_vlan_rx(struct sk_buff *skb, int forward, int tag)
+{
+	/* Add VLAN tag manually. */
+	if (!(forward & FWD_VLAN_DEV) || !tag)
+		return false;
+
+	tag += VLAN_PORT_START;
+
+	/* Only forward to one network device. */
+	__vlan_hwaccel_put_tag(skb, htons(ETH_P_8021Q), tag);
+	return true;
+}  /* sw_port_vlan_rx */
+
+static int sw_drv_rx(struct ksz_sw *sw, struct sk_buff *skb, uint port)
+{
+	int ret = 1;
+
+#ifdef CONFIG_KSZ_STP
+	if (sw->features & STP_SUPPORT) {
+		ret = stp_rcv(&sw->info->rstp, skb, port);
+		if (!ret)
+			return ret;
+	}
+#endif
+#ifdef PROC_MRP
+	if (sw->features & MRP_SUPPORT) {
+		ret = mrp_rcv(&sw->mrp, skb, port);
+		if (!ret)
+			return ret;
+	}
+#endif
+#ifdef CONFIG_KSZ_DLR
+	if (sw->features & DLR_HW) {
+		ret = dlr_rcv(&sw->info->dlr, skb, port);
+		if (!ret)
+			return ret;
+	}
+#endif
+
+	/* Need to remove VLAN tag if not using tail tag. */
+	if (sw->dev_count > 1 && (sw->features & SW_VLAN_DEV) &&
+	    !(sw->overrides & TAIL_TAGGING)) {
+		struct vlan_ethhdr *vlan = (struct vlan_ethhdr *) skb->data;
+
+		if (vlan->h_vlan_proto == htons(ETH_P_8021Q)) {
+			int p;
+			int vid;
+			struct ethhdr *eth;
+			u16 vlan_tci = ntohs(vlan->h_vlan_TCI);
+
+			vid = vlan_tci & VLAN_VID_MASK;
+			for (p = 0; p < sw->eth_cnt; p++) {
+				if (vid == sw->eth_maps[p].vlan) {
+					eth = (struct ethhdr *)
+						skb_pull(skb, VLAN_HLEN);
+					memmove(eth, vlan, 12);
+					break;
+				}
+			}
+		}
+	}
+	return ret;
+}  /* sw_drv_rx */
+
+static int sw_get_mtu(struct ksz_sw *sw)
+{
+	int need_tail_tag = false;
+	int header = 0;
+	int mtu = 0;
+
+#ifdef CONFIG_KSZ_DLR
+	if (sw->features & DLR_HW)
+		need_tail_tag = true;
+#endif
+	if (sw->dev_count > 1 && !(sw->features & SW_VLAN_DEV))
+		need_tail_tag = true;
+	if (sw->features & VLAN_PORT_TAGGING)
+		need_tail_tag = true;
+	if (sw->features & STP_SUPPORT)
+		need_tail_tag = true;
+	if (need_tail_tag)
+		mtu += 1;
+	if (sw->features & SW_VLAN_DEV)
+		if (header < VLAN_HLEN)
+			header = VLAN_HLEN;
+	mtu += header;
+	return mtu;
+}  /* sw_get_mtu */
+
+static int sw_get_tx_len(struct ksz_sw *sw, struct sk_buff *skb, uint port,
+	int *header)
+{
+	int len = skb->len;
+	int hlen = 0;
+
+	if (sw->features & SW_VLAN_DEV)
+		hlen = VLAN_HLEN;
+	*header += hlen;
+	if (!(sw->overrides & TAIL_TAGGING))
+		return len;
+	if (len < 60)
+		len = 60;
+	len += 1;
+	return len;
+}  /* sw_get_tx_len */
+
+static void sw_add_tail_tag(struct ksz_sw *sw, struct sk_buff *skb, uint ports)
+{
+	u8 *trailer;
+	int len = 1;
+
+	trailer = skb_put(skb, len);
+	if (!ports)
+		ports = TAIL_TAG_LOOKUP;
+	else if (ports & TAIL_TAG_SET_OVERRIDE) {
+		ports &= ~TAIL_TAG_SET_OVERRIDE;
+		ports |= TAIL_TAG_OVERRIDE;
+	}
+	trailer[0] = (u8) ports;
+}  /* sw_add_tail_tag */
+
+static int sw_get_tail_tag(u8 *trailer, int *port)
+{
+	int len = 1;
+
+	*port = *trailer;
+	return len;
+}  /* sw_get_tail_tag */
+
+static void sw_add_vid(struct ksz_sw *sw, u16 vid)
+{
+	if ((sw->features & VLAN_PORT) && vid >= VLAN_PORT_START) {
+		vid -= VLAN_PORT_START;
+		if (vid <= SWITCH_PORT_NUM)
+			sw->vlan_id |= (1 << vid);
+	}
+}  /* sw_add_vid */
+
+static void sw_kill_vid(struct ksz_sw *sw, u16 vid)
+{
+	if ((sw->features & VLAN_PORT) && vid >= VLAN_PORT_START) {
+		vid -= VLAN_PORT_START;
+		if (vid <= SWITCH_PORT_NUM)
+			sw->vlan_id &= ~(1 << vid);
+	}
+}  /* sw_kill_vid */
+
+static struct sk_buff *sw_ins_vlan(struct ksz_sw *sw, uint port,
+	struct sk_buff *skb)
+{
+	port = get_phy_port(sw, port);
+
+	/* Need to insert VLAN tag. */
+	if (sw->dev_count > 1 && (sw->features & SW_VLAN_DEV)) {
+		u16 vid;
+		struct vlan_ethhdr *vlan;
+		struct ethhdr *eth;
+		struct sk_buff *nskb = NULL;
+
+		/*
+		 * Bridge uses clones of socket buffer to send to both
+		 * devices!
+		 */
+		if (!skb_cloned(skb) && skb_headroom(skb) >= VLAN_HLEN)
+			nskb = skb;
+		if (!nskb) {
+			nskb = skb_copy_expand(skb, VLAN_HLEN, skb->len,
+					       GFP_ATOMIC);
+			if (!nskb)
+				return skb;
+			dev_kfree_skb_irq(skb);
+			skb = nskb;
+		}
+		eth = (struct ethhdr *) skb->data;
+
+		vid = sw->info->port_cfg[port].vid;
+		vlan = (struct vlan_ethhdr *) skb_push(skb, VLAN_HLEN);
+		memmove(vlan, eth, 12);
+		vlan->h_vlan_TCI = htons(vid);
+		vlan->h_vlan_proto = htons(ETH_P_8021Q);
+	}
+	return skb;
+}  /* sw_ins_vlan */
+
+static struct sk_buff *sw_check_skb(struct ksz_sw *sw, struct sk_buff *skb,
+	struct ksz_port *priv, void *ptr,
+	int (*update_msg)(u8 *data, u32 port, u32 overrides))
+{
+	bool need_new_copy = false;
+	int len;
+	int padlen = 0;
+	uint port;
+	u8 dest;
+	struct sk_buff *org_skb;
+	int update_dst = (sw->overrides & TAIL_TAGGING);
+	int headlen = 0;
+
+	if (!update_dst)
+		return sw_ins_vlan(sw, priv->first_port, skb);
+
+#ifdef CONFIG_KSZ_STP
+	if (skb->protocol == htons(STP_TAG_TYPE))
+		return skb;
+#endif
+#ifdef CONFIG_KSZ_DLR
+	if (skb->protocol == htons(DLR_TAG_TYPE))
+		return skb;
+#endif
+
+	if (sw->features & SW_VLAN_DEV)
+		headlen = VLAN_HLEN;
+	org_skb = skb;
+	port = 0;
+
+	/* This device is associated with a switch port. */
+	if (1 == priv->port_cnt)
+		port = priv->first_port;
+
+#if 0
+	do {
+		u16 prio;
+		u16 vid;
+		uint i;
+		uint p;
+		u32 features = sw->features;
+
+		p = get_phy_port(sw, priv->first_port);
+		i = sw->info->port_cfg[p].index;
+		if (sw->features & SW_VLAN_DEV)
+			features = sw->eth_maps[i].proto;
+		if (!(features & VLAN_PORT) || port || vlan_get_tag(skb, &vid))
+			break;
+		prio = vid & VLAN_PRIO_MASK;
+		vid &= VLAN_VID_MASK;
+		if (vid < VLAN_PORT_START)
+			break;
+		vid -= VLAN_PORT_START;
+		if (!vid || vid > SWITCH_PORT_NUM)
+			break;
+		port = vid;
+
+		if (sw->vid || prio) {
+			struct vlan_ethhdr *vlan =
+				(struct vlan_ethhdr *) skb->data;
+			u16 vlan_tci = ntohs(vlan->h_vlan_TCI);
+
+			vlan_tci &= ~VLAN_VID_MASK;
+			vlan_tci |= sw->vid;
+			vlan->h_vlan_TCI = htons(vlan_tci);
+
+		/* Need to remove VLAN tag manually. */
+		} else if (!(sw->overrides & TAG_REMOVE)) {
+			u8 *data;
+
+			len = VLAN_ETH_HLEN - 2;
+			data = &skb->data[len];
+			memmove(data - VLAN_HLEN, data, skb->len - len);
+			skb->len -= VLAN_HLEN;
+		}
+	} while (0);
+#endif
+
+	dest = 0;
+	if (port) {
+		port = get_phy_port(sw, port);
+		dest = 1 << port;
+	}
+
+	/* Check the socket buffer length is enough to hold the tail tag. */
+	if (skb->len < ETH_ZLEN)
+		padlen = ETH_ZLEN - skb->len;
+	len = skb_tailroom(skb);
+	if (len < 1 + padlen) {
+		need_new_copy = true;
+		len = (skb->len + padlen + 4) & ~3;
+	}
+	if (skb_headroom(skb) < headlen) {
+		need_new_copy = true;
+	}
+	if (need_new_copy) {
+		int headerlen = skb_headroom(skb);
+
+		if (headerlen < headlen)
+			headerlen = headlen;
+		skb = skb_copy_expand(org_skb, headerlen, len, GFP_ATOMIC);
+		if (!skb)
+			return NULL;
+		consume_skb(org_skb);
+	}
+
+	/* skb_put requires tail pointer set first. */
+	skb_set_tail_pointer(skb, skb->len);
+	if (padlen) {
+		if (__skb_put_padto(skb, skb->len + padlen, false))
+			return NULL;
+	}
+	len = skb->len;
+
+	if (!dest) {
+		dest = TAIL_TAG_LOOKUP;
+	}
+	if (dest == TAIL_TAG_LOOKUP) {
+		/* Use VLAN for port forwarding if not specified directly. */
+		skb = sw_ins_vlan(sw, priv->first_port, skb);
+		if (len != skb->len)
+			len = skb->len;
+	}
+	skb->data[len] = dest;
+	skb_put(skb, 1);
+
+	/* Need to compensate checksum for some devices. */
+	if (skb->ip_summed != CHECKSUM_PARTIAL)
+		dest = 0;
+	if (dest && (sw->overrides & UPDATE_CSUM)) {
+		__sum16 *csum_loc = (__sum16 *)
+			(skb->head + skb->csum_start + skb->csum_offset);
+
+		/* Checksum is cleared by driver to be filled by hardware. */
+		if (!*csum_loc) {
+			__sum16 new_csum;
+
+			new_csum = dest << 8;
+			*csum_loc = ~htons(new_csum);
+		}
+	}
+	return skb;
+}  /* sw_check_skb */
+
+static struct sk_buff *sw_check_tx(struct ksz_sw *sw, struct net_device *dev,
+	struct sk_buff *skb, struct ksz_port *priv)
+{
+	void *ptr = NULL;
+	int (*update_msg)(u8 *data, u32 port, u32 overrides) = NULL;
+
+	return sw_check_skb(sw, skb, priv, ptr, update_msg);
+}  /* sw_check_tx */
+
+static struct sk_buff *sw_final_skb(struct ksz_sw *sw, struct sk_buff *skb,
+	struct net_device *dev, struct ksz_port *port)
+{
+	skb = sw->net_ops->check_tx(sw, dev, skb, port);
+	if (!skb)
+		return NULL;
+	return skb;
+}  /* sw_final_skb */
+
+static void sw_start(struct ksz_sw *sw, u8 *addr)
+{
+	int need_tail_tag = false;
+	int need_vlan = false;
+
+	sw->ops->acquire(sw);
+	sw_setup(sw);
+	sw_enable(sw);
+	sw_set_addr(sw, addr);
+#if 0
+	if (1 == sw->dev_count)
+		sw_cfg_self_filter(sw, true);
+#endif
+#ifdef CONFIG_KSZ_DLR
+	if (sw->features & DLR_HW)
+		need_tail_tag = true;
+#endif
+	if (sw->dev_count > 1 && !(sw->features & SW_VLAN_DEV))
+		need_tail_tag = true;
+	if (sw->features & VLAN_PORT) {
+		if (sw->features & VLAN_PORT_REMOVE_TAG) {
+			uint n;
+			uint p;
+
+			for (n = 1; n <= sw->mib_port_cnt; n++) {
+				p = get_phy_port(sw, n);
+				port_cfg_rmv_tag(sw, p, true);
+			}
+			sw->overrides |= TAG_REMOVE;
+		}
+		if (sw->features & VLAN_PORT_TAGGING)
+			need_tail_tag = true;
+	}
+	if (sw->features & SW_VLAN_DEV) {
+		struct ksz_vlan_table entry;
+		struct ksz_dev_map *map;
+		int i;
+		int p;
+		uint port;
+		uint q;
+
+		memset(&entry, 0, sizeof(struct ksz_vlan_table));
+		for (p = 0; p < sw->eth_cnt; p++) {
+
+			/* Not really using VLAN. */
+			if (1 == sw->eth_maps[p].vlan)
+				continue;
+
+			map = &sw->eth_maps[p];
+
+			/*
+			 * Setting FID allows same MAC address in different
+			 * VLANs.
+			 */
+			entry.fid = map->vlan & (FID_ENTRIES - 1);
+			entry.member = sw->HOST_MASK | map->mask;
+			entry.valid = 1;
+			sw->ops->release(sw);
+			sw_w_vlan_table(sw, map->vlan, &entry);
+			sw->ops->acquire(sw);
+			for (i = 0, q = map->first;
+			     i < map->cnt; i++, q++) {
+				port = get_phy_port(sw, q);
+				sw_cfg_def_vid(sw, port, map->vlan);
+#ifdef CONFIG_KSZ_DLR
+				if ((sw->eth_maps[p].proto & DLR_HW))
+					continue;
+#endif
+				port_cfg_rmv_tag(sw, port, true);
+			}
+		}
+
+		/* Use VLAN tag to determine the network device. */
+		if (!need_tail_tag)
+			port_cfg_ins_tag(sw, sw->HOST_PORT, true);
+		need_vlan = true;
+	}
+	if (sw->features & STP_SUPPORT)
+		need_tail_tag = true;
+	if (sw->features & (DLR_HW))
+		need_vlan = true;
+	if (need_tail_tag) {
+		sw_cfg_tail_tag(sw, true);
+		sw->overrides |= TAIL_TAGGING;
+	}
+	if (need_vlan) {
+		struct ksz_vlan_table entry;
+
+		/* Default VID 1 is not in VLAN table. */
+		sw->ops->release(sw);
+		entry.fid = 0;
+		entry.member = sw->PORT_MASK;
+		entry.valid = 1;
+		sw_w_vlan_table(sw, 1, &entry);
+		entry.fid = 0;
+		entry.member = sw->PORT_MASK;
+		entry.valid = 1;
+		sw_w_vlan_table(sw, 0, &entry);
+		sw->ops->acquire(sw);
+		sw_ena_vlan(sw);
+	}
+	sw_ena_intr(sw);
+	sw->ops->release(sw);
+
+#ifdef CONFIG_KSZ_STP
+	if (sw->features & STP_SUPPORT) {
+		struct ksz_stp_info *stp = &sw->info->rstp;
+
+		if (stp->br.bridgeEnabled)
+			stp_start(stp);
+	} else
+		stp_set_addr(&sw->info->rstp, sw->info->mac_addr);
+#endif
+}  /* sw_start */
+
+static int sw_stop(struct ksz_sw *sw, int complete)
+{
+	int reset = false;
+
+#ifdef CONFIG_KSZ_STP
+	if (sw->features & STP_SUPPORT) {
+		struct ksz_stp_info *stp = &sw->info->rstp;
+
+		if (stp->br.bridgeEnabled)
+			stp_stop(stp, true);
+	}
+#endif
+
+#ifdef CONFIG_KSZ_MRP
+	if (sw->features & MRP_SUPPORT) {
+		struct mrp_info *mrp = &sw->mrp;
+
+		mrp_stop(mrp);
+	}
+#endif
+
+	sw->ops->acquire(sw);
+	if (!reset)
+		sw_reset(sw);
+	reset = true;
+	sw_init(sw);
+	sw->ops->release(sw);
+
+	/* Clean out static MAC table when the switch shutdown. */
+	if (complete)
+		sw_clr_sta_mac_table(sw);
+	return reset;
+}  /* sw_stop */
+
+static void sw_init_mib(struct ksz_sw *sw)
+{
+	unsigned long interval;
+	uint n;
+	uint p;
+
+	interval = MIB_READ_INTERVAL * 2 / (sw->mib_port_cnt + 1);
+	for (n = 0; n <= sw->mib_port_cnt; n++) {
+		p = get_phy_port(sw, n);
+		sw->port_mib[p].mib_start = 0;
+		if (sw->next_jiffies < jiffies)
+			sw->next_jiffies = jiffies + HZ * 2;
+		else
+			sw->next_jiffies += interval;
+		sw->counter[p].time = sw->next_jiffies;
+		sw->port_state[p].state = media_disconnected;
+		port_init_cnt(sw, p);
+	}
+	sw->port_state[sw->HOST_PORT].state = media_connected;
+}  /* sw_init_mib */
+
+#ifdef CONFIG_PHYLINK
+static void sw_set_phylink_support(struct ksz_sw *sw, struct ksz_port *port,
+				   unsigned long *supported,
+				   struct phylink_link_state *state)
+{
+	__ETHTOOL_DECLARE_LINK_MODE_MASK(mask) = { 0, };
+
+	phylink_set(mask, TP);
+	phylink_set(mask, MII);
+
+	phylink_set(mask, Autoneg);
+
+	phylink_set(mask, 10baseT_Half);
+	phylink_set(mask, 10baseT_Full);
+	phylink_set(mask, 100baseT_Half);
+	phylink_set(mask, 100baseT_Full);
+
+	switch (port->flow_ctrl) {
+	case PHY_NO_FLOW_CTRL:
+		phylink_clear(mask, Pause);
+		phylink_clear(mask, Asym_Pause);
+		break;
+	case PHY_RX_ONLY:
+		/* Depend on link partner to only advertise Asym_Pause. */
+		phylink_set(mask, Pause);
+		phylink_set(mask, Asym_Pause);
+		break;
+	case PHY_TX_ONLY:
+		phylink_set(mask, Asym_Pause);
+		phylink_clear(mask, Pause);
+		break;
+	default:
+		phylink_set(mask, Pause);
+		phylink_clear(mask, Asym_Pause);
+	}
+
+	bitmap_and(supported, supported, mask, __ETHTOOL_LINK_MODE_MASK_NBITS);
+	bitmap_and(state->advertising, state->advertising, mask,
+		   __ETHTOOL_LINK_MODE_MASK_NBITS);
+	linkmode_copy(port->phydev->supported, supported);
+	linkmode_copy(port->phydev->advertising, state->advertising);
+}  /* sw_set_phylink_support */
+
+static void sw_port_phylink_get_fixed_state(struct phylink_config *config,
+					    struct phylink_link_state *s)
+{
+	struct ksz_port *p = container_of(config, struct ksz_port, pl_config);
+	struct ksz_sw *sw = p->sw;
+	struct ksz_port_info *info = get_port_info(sw, sw->HOST_PORT);
+
+dbg_msg(" fixed state: %d %d\n", sw->interface, info->interface);
+	s->interface = sw->interface;
+	s->speed = info->tx_rate / TX_RATE_UNIT;
+	s->duplex = 1;
+	s->pause = 3;
+	s->link = 1;
+	s->an_enabled = 0;
+	s->an_complete = 0;
+}  /* sw_port_phylink_get_fixed_state */
+
+static void sw_port_phylink_validate(struct phylink_config *config,
+				     unsigned long *supported,
+				     struct phylink_link_state *state)
+{
+	struct ksz_port *p = container_of(config, struct ksz_port, pl_config);
+	struct ksz_sw *sw = p->sw;
+
+dbg_msg(" validate: %d\n", state->interface);
+	if ((sw->dev_offset && p->port_cnt > 1) ||
+	    (!sw->dev_offset && !sw->phy_offset)) {
+		if (sw->phylink_ops && sw->phylink_ops->validate)
+			sw->phylink_ops->validate(config, supported, state);
+	} else {
+		sw_set_phylink_support(sw, p, supported, state);
+	}
+}
+
+static int sw_port_phylink_mac_prepare(struct phylink_config *config,
+				       unsigned int mode,
+				       phy_interface_t interface)
+{
+	struct ksz_port *p = container_of(config, struct ksz_port, pl_config);
+	struct ksz_sw *sw = p->sw;
+
+	if (sw->phylink_ops && sw->phylink_ops->mac_prepare)
+		return sw->phylink_ops->mac_prepare(config, mode, interface);
+	return 0;
+}
+
+static void sw_port_phylink_mac_config(struct phylink_config *config,
+				       unsigned int mode,
+				       const struct phylink_link_state *state)
+{
+	/* The switch is always connected to the MAC. */
+}
+
+static void sw_port_phylink_mac_link_down(struct phylink_config *config,
+					  unsigned int mode,
+					  phy_interface_t interface)
+{
+	struct ksz_port *p = container_of(config, struct ksz_port, pl_config);
+	struct ksz_sw *sw = p->sw;
+
+	/* Tell MAC driver to turn off transmit queues. */
+	interface = PHY_INTERFACE_MODE_INTERNAL;
+	if (sw->phylink_ops && sw->phylink_ops->mac_link_down)
+		sw->phylink_ops->mac_link_down(config, mode, interface);
+}
+
+static void sw_port_phylink_mac_link_up(struct phylink_config *config,
+					struct phy_device *phydev,
+					unsigned int mode,
+					phy_interface_t interface,
+					int speed, int duplex,
+					bool tx_pause, bool rx_pause)
+{
+	struct ksz_port *p = container_of(config, struct ksz_port, pl_config);
+	struct ksz_sw *sw = p->sw;
+
+	/* Tell MAC driver to turn on transmit queues. */
+	interface = PHY_INTERFACE_MODE_INTERNAL;
+	if (sw->phylink_ops && sw->phylink_ops->mac_link_up)
+		sw->phylink_ops->mac_link_up(config, phydev, mode,
+					     interface, speed, duplex,
+					     tx_pause, rx_pause);
+}
+
+static const struct phylink_mac_ops sw_port_phylink_mac_ops = {
+	.validate = sw_port_phylink_validate,
+	.mac_prepare = sw_port_phylink_mac_prepare,
+	.mac_config = sw_port_phylink_mac_config,
+	.mac_link_down = sw_port_phylink_mac_link_down,
+	.mac_link_up = sw_port_phylink_mac_link_up,
+};
+
+static int setup_phylink(struct ksz_port *port)
+{
+	struct device_node *dn = port->dn;
+	phy_interface_t mode;
+	int ret;
+
+	ret = of_get_phy_mode(dn, &mode);
+	if (ret)
+		mode = PHY_INTERFACE_MODE_NA;
+
+	port->pl_config.dev = &port->netdev->dev;
+
+	/* netif_carrier_on is called automatically for netdevice. */
+	port->pl_config.type = PHYLINK_NETDEV;
+
+#if 0
+	/* netif_carrier_on is not called for base device. */
+	port->pl_config.type = PHYLINK_DEV;
+#endif
+	port->pl_config.get_fixed_state = sw_port_phylink_get_fixed_state;
+
+	port->pl = phylink_create(&port->pl_config, of_fwnode_handle(dn), mode,
+				  &sw_port_phylink_mac_ops);
+	if (IS_ERR(port->pl)) {
+		netdev_err(port->netdev,
+			   "error creating PHYLINK: %ld\n", PTR_ERR(port->pl));
+		return PTR_ERR(port->pl);
+	}
+
+	return ret;
+}
+
+static void sw_exit_phylink(struct ksz_sw *sw, struct ksz_port *port)
+{
+	const struct phylink_mac_ops *ops = sw->phylink_ops;
+
+	if (ops && port) {
+		unsigned int mode = MLO_AN_PHY;
+		struct phylink_config *config;
+
+		config = &port->pl_config;
+		ops->mac_link_down(config, mode, sw->interface);
+	}
+}  /* sw_exit_phylink */
+
+static void sw_init_phylink(struct ksz_sw *sw, struct ksz_port *port)
+{
+	const struct phylink_mac_ops *ops = sw->phylink_ops;
+
+	if (ops && port) {
+		struct phylink_link_state *state;
+		unsigned int mode = MLO_AN_PHY;
+		struct phylink_config *config;
+		bool rx_pause = true;
+		bool tx_pause = true;
+
+		config = &port->pl_config;
+		state = &port->pl_state;
+		sw_port_phylink_get_fixed_state(config, state);
+		ops->mac_config(config, mode, state);
+		ops->mac_link_up(config, port->phydev, mode,
+				 sw->interface,
+				 state->speed, state->duplex,
+				 tx_pause, rx_pause);
+	}
+}  /* sw_init_phylink */
+#endif
+
+static void setup_device_node(struct ksz_sw *sw)
+{
+	struct sw_priv *ks = sw->dev;
+	struct device_node *np;
+	struct device_node *ports, *port;
+	struct device_node *ethernet;
+	const char *name;
+	int err;
+	u32 reg;
+
+	if (!ks->of_dev)
+		return;
+	np = ks->of_dev->of_node;
+	if (np) {
+		ports = of_get_child_by_name(np, "ports");
+		if (ports) {
+			for_each_available_child_of_node(ports, port) {
+				err = of_property_read_u32(port, "reg", &reg);
+				if (err)
+					break;
+dbg_msg(" reg: %d\n", reg);
+				ethernet = of_parse_phandle(port, "ethernet", 0);
+				if (ethernet)
+dbg_msg(" found eth\n");
+				if (ethernet) {
+					name = of_get_property(port,
+							       "phy-mode",
+							       NULL);
+					if (name && !strcmp(name, "rmii"))
+						sw->interface =
+							PHY_INTERFACE_MODE_RMII;
+				}
+				name = of_get_property(port, "label", NULL);
+				if (name)
+dbg_msg(" name: %s\n", name);
+				/* Save the device node. */
+				sw->devnode[reg] = port;
+			}
+		}
+	}
+}
+
+static int sw_open_dev(struct ksz_sw *sw, struct net_device *dev,
+	struct ksz_port *port, u8 *addr)
+{
+	int mode = 0;
+
+	sw_init_mib(sw);
+
+	sw->main_dev = dev;
+	sw->main_port = port;
+	sw->net_ops->start(sw, addr);
+	if (sw->dev_count > 1)
+		mode |= 1;
+	if (sw->features & DIFF_MAC_ADDR)
+		mode |= 2;
+	return mode;
+}  /* sw_open_dev */
+
+static void sw_open_port(struct ksz_sw *sw, struct net_device *dev,
+	struct ksz_port *port)
+{
+	uint i;
+	uint n;
+	uint p;
+	struct ksz_port_info *info;
+	struct ksz_port_info *host;
+
+	host = get_port_info(sw, sw->HOST_PORT);
+	for (i = 0, n = port->first_port; i < port->port_cnt; i++, n++) {
+		p = get_phy_port(sw, n);
+		info = get_port_info(sw, p);
+		/*
+		 * Initialize to invalid value so that link detection
+		 * is done.
+		 */
+		info->link = 0xFF;
+		info->state = media_unknown;
+		info->tx_rate = host->tx_rate;
+		info->duplex = host->duplex;
+		if (port->port_cnt == 1) {
+			if (sw->netdev[0]) {
+				struct ksz_port *sw_port = sw->netport[0];
+
+				port->speed = sw_port->speed;
+				port->duplex = sw_port->duplex;
+				port->flow_ctrl = sw_port->flow_ctrl;
+			}
+			if (info->own_speed != port->speed ||
+			    info->own_duplex != port->duplex) {
+				if (info->own_speed)
+					port->speed = info->own_speed;
+				if (info->own_duplex)
+					port->duplex = info->own_duplex;
+			}
+		}
+	}
+	port->opened = true;
+	port->report = true;
+
+	sw->ops->acquire(sw);
+
+	/* Need to open the port in multiple device interfaces mode. */
+	if (sw->dev_count > 1 && (!sw->dev_offset || dev != sw->netdev[0])) {
+		port->state = STP_STATE_SIMPLE;
+		if (sw->dev_offset && !(sw->features & STP_SUPPORT)) {
+			port->state = STP_STATE_FORWARDING;
+		}
+		if (sw->features & SW_VLAN_DEV) {
+			p = get_phy_port(sw, port->first_port);
+			i = sw->info->port_cfg[p].index;
+			if (!(sw->eth_maps[i].proto & HSR_HW))
+				port->state = STP_STATE_FORWARDING;
+		}
+		for (i = 0, n = port->first_port; i < port->port_cnt;
+		     i++, n++) {
+			p = get_phy_port(sw, n);
+			sw->dev_ports |= (1 << p);
+#ifdef CONFIG_KSZ_STP
+			if (sw->features & STP_SUPPORT) {
+				stp_enable_port(&sw->info->rstp, p,
+						&port->state);
+			}
+#endif
+			port_set_stp_state(sw, p, port->state);
+		}
+	} else if (sw->dev_count == 1) {
+		sw->dev_ports = sw->PORT_MASK;
+	}
+
+	sw->phy_intr = sw->PORT_MASK;
+	if (port->force_link)
+		port_force_link_speed(port);
+	else
+		port_set_link_speed(port);
+	port_get_link_speed(port);
+	if (port->link_ports)
+		schedule_delayed_work(&port->link_update, 0);
+	sw->phy_intr = 0;
+	sw->ops->release(sw);
+	for (i = 0; i < sw->eth_cnt; i++) {
+		if (dev != sw->netdev[i])
+			continue;
+#ifdef CONFIG_KSZ_DLR
+		if (sw->eth_maps[i].proto & DLR_HW) {
+			struct ksz_dlr_info *info = &sw->info->dlr;
+
+			p = get_phy_port(sw, port->first_port);
+			if (info->ports[0] == p)
+				prep_dlr(info, dev, dev->dev_addr);
+		}
+#endif
+	}
+}  /* sw_open_port */
+
+static void sw_close_port(struct ksz_sw *sw, struct net_device *dev,
+	struct ksz_port *port)
+{
+	int i;
+	uint p;
+#ifdef CONFIG_KSZ_MRP
+	struct mrp_info *mrp = &sw->mrp;
+#endif
+
+	port->opened = false;
+
+	/* Need to shut the port manually in multiple device interfaces mode. */
+	if (sw->dev_count > 1 && (!sw->dev_offset || dev != sw->netdev[0])) {
+		uint n;
+
+		sw->ops->acquire(sw);
+		for (i = 0, n = port->first_port; i < port->port_cnt;
+		     i++, n++) {
+			p = get_phy_port(sw, n);
+			if (p == sw->HOST_PORT)
+				continue;
+#ifdef CONFIG_KSZ_STP
+			if (sw->features & STP_SUPPORT)
+				stp_disable_port(&sw->info->rstp, p);
+#endif
+#ifdef CONFIG_KSZ_MRP
+			if (sw->features & MRP_SUPPORT) {
+				mrp_close_port(mrp, p);
+			}
+#endif
+			sw->dev_ports &= ~(1 << p);
+			port_set_stp_state(sw, p, STP_STATE_DISABLED);
+		}
+		sw->ops->release(sw);
+	} else if (sw->dev_count == 1) {
+#ifdef CONFIG_KSZ_MRP
+		uint n;
+
+		if (sw->features & MRP_SUPPORT) {
+			for (n = 0; n <= sw->mib_port_cnt; n++) {
+				p = get_phy_port(sw, n);
+				mrp_close_port(mrp, p);
+			}
+		}
+#endif
+		sw->dev_ports = 0;
+	}
+	for (i = 0; i < sw->eth_cnt; i++) {
+		if (dev != sw->netdev[i])
+			continue;
+	}
+}  /* sw_close_port */
+
+static void sw_open(struct ksz_sw *sw)
+{
+#ifdef CONFIG_PHYLINK
+	sw_init_phylink(sw, sw->main_port);
+#endif
+#ifdef CONFIG_KSZ_MRP
+	if (sw->features & MRP_SUPPORT) {
+		struct ksz_port_info *info = get_port_info(sw, sw->HOST_PORT);
+		u32 speed = info->tx_rate / TX_RATE_UNIT;
+
+		mrp_set_speed(&sw->mrp, sw->HOST_PORT, speed, true);
+		mrp_open(&sw->mrp);
+		schedule_delayed_work(&sw->set_mrp, msecs_to_jiffies(1000));
+	}
+#endif
+	/* Timer may already be started by the SPI device. */
+	if (!sw->monitor_timer_info->max)
+		ksz_start_timer(sw->monitor_timer_info,
+			sw->monitor_timer_info->period);
+}  /* sw_open */
+
+static void sw_close(struct ksz_sw *sw)
+{
+#ifdef CONFIG_PHYLINK
+	sw_exit_phylink(sw, sw->main_port);
+#endif
+#ifdef CONFIG_KSZ_MRP
+	if (sw->features & MRP_SUPPORT)
+		mrp_close(&sw->mrp, true);
+#endif
+	ksz_stop_timer(sw->monitor_timer_info);
+	cancel_delayed_work_sync(sw->link_read);
+}  /* sw_close */
+
+static u8 sw_set_mac_addr(struct ksz_sw *sw, struct net_device *dev,
+	u8 promiscuous, uint port)
+{
+	int n;
+
+	/* See if different MAC addresses are used. */
+	if (sw->dev_count > 1) {
+		int i;
+		int dev_count = sw->dev_count + sw->dev_offset;
+
+		for (i = 0; i < dev_count; i++) {
+			if (dev == sw->netdev[i])
+				continue;
+			if (memcmp(sw->netdev[i]->dev_addr,
+			    dev->dev_addr, ETH_ALEN))
+				break;
+		}
+		if (sw->features & DIFF_MAC_ADDR) {
+			struct ksz_port *priv;
+			struct ksz_port_info *info;
+			uint p;
+
+			/* All addresses are the same. */
+			if (i == dev_count) {
+				sw->features &= ~DIFF_MAC_ADDR;
+				--promiscuous;
+			} else if (sw->dev_offset && dev == sw->netdev[0]) {
+				for (n = 1; n < dev_count; n++) {
+					priv = sw->netport[n];
+					p = get_phy_port(sw, priv->first_port);
+					info = get_port_info(sw, p);
+					inc_mac_addr(info->mac_addr,
+						dev->dev_addr, n);
+					inc_mac_addr(sw->netdev[n]->dev_addr,
+						dev->dev_addr, n);
+				}
+			}
+		} else {
+			if (dev == sw->netdev[0] && i < dev_count) {
+
+				/* Make MAC address the same in all devices. */
+				for (i = 1; i < dev_count; i++) {
+					memcpy(sw->netdev[i]->dev_addr,
+						dev->dev_addr, ETH_ALEN);
+				}
+			} else {
+				if (i < dev_count) {
+					sw->features |= DIFF_MAC_ADDR;
+					++promiscuous;
+				}
+			}
+		}
+	}
+	if (dev == sw->netdev[0]) {
+		sw->ops->acquire(sw);
+		sw_set_addr(sw, dev->dev_addr);
+		sw->ops->release(sw);
+	}
+	for (n = 0; n < sw->eth_cnt; n++) {
+		if (sw->netdev[n] != dev)
+			continue;
+#ifdef CONFIG_KSZ_STP
+		if (sw->features & STP_SUPPORT) {
+			struct ksz_stp_info *stp = &sw->info->rstp;
+
+			stp->ops->change_addr(stp, dev->dev_addr);
+		}
+#endif
+#ifdef CONFIG_KSZ_DLR
+		if (sw->eth_maps[n].proto & DLR_HW) {
+			dlr_change_addr(&sw->info->dlr, dev->dev_addr);
+		}
+#endif
+	}
+	return promiscuous;
+}  /* sw_set_mac_addr */
+
+static struct ksz_dev_major sw_majors[MAX_SW_DEVICES];
+
+static struct file_dev_info *alloc_sw_dev_info(struct ksz_sw *sw, uint minor)
+{
+	struct file_dev_info *info;
+
+	info = kzalloc(sizeof(struct file_dev_info), GFP_KERNEL);
+	if (info) {
+		info->dev = sw;
+		sema_init(&info->sem, 1);
+		mutex_init(&info->lock);
+		init_waitqueue_head(&info->wait_msg);
+		info->read_max = 60000;
+		info->read_tmp = MAX_SW_LEN;
+		info->read_buf = kzalloc(info->read_max + info->read_tmp,
+			GFP_KERNEL);
+		info->read_in = &info->read_buf[info->read_max];
+		info->write_len = 1000;
+		info->write_buf = kzalloc(info->write_len, GFP_KERNEL);
+
+		info->minor = minor;
+		info->next = sw->dev_list[minor];
+		sw->dev_list[minor] = info;
+	}
+	return info;
+}  /* alloc_sw_dev_info */
+
+static void free_sw_dev_info(struct file_dev_info *info)
+{
+	if (info) {
+		struct ksz_sw *sw = info->dev;
+		uint minor = info->minor;
+
+		file_dev_clear_notify(sw->dev_list[minor], info, DEV_MOD_BASE,
+				      &sw->notifications);
+		file_gen_dev_release(info, &sw->dev_list[minor]);
+	}
+}  /* free_sw_dev_info */
+
+static int sw_dev_open(struct inode *inode, struct file *filp)
+{
+	struct file_dev_info *info = (struct file_dev_info *)
+		filp->private_data;
+	uint minor = MINOR(inode->i_rdev);
+	uint major = MAJOR(inode->i_rdev);
+	struct ksz_sw *sw = NULL;
+	int i;
+
+	if (minor > 1)
+		return -ENODEV;
+	for (i = 0; i < MAX_SW_DEVICES; i++) {
+		if (sw_majors[i].major == major) {
+			sw = sw_majors[i].dev;
+			break;
+		}
+	}
+	if (!sw)
+		return -ENODEV;
+	if (!info) {
+		info = alloc_sw_dev_info(sw, minor);
+		if (info)
+			filp->private_data = info;
+		else
+			return -ENOMEM;
+	}
+	return 0;
+}  /* sw_dev_open */
+
+static int sw_dev_release(struct inode *inode, struct file *filp)
+{
+	struct file_dev_info *info = (struct file_dev_info *)
+		filp->private_data;
+
+	free_sw_dev_info(info);
+	filp->private_data = NULL;
+	return 0;
+}  /* sw_dev_release */
+
+static int sw_get_attrib(struct ksz_sw *sw, int subcmd, int size,
+	int *req_size, size_t *len, u8 *data, int *output)
+{
+	struct ksz_info_opt *opt = (struct ksz_info_opt *) data;
+	struct ksz_info_cfg *cfg = &opt->data.cfg;
+	uint i;
+	uint n;
+	uint p;
+
+	*len = 0;
+	*output = 0;
+	switch (subcmd) {
+	case DEV_SW_CFG:
+		n = opt->num;
+		p = opt->port;
+		if (!n)
+			n = 1;
+		*len = 2 + n * sizeof(struct ksz_info_cfg);
+		if (n > sw->mib_port_cnt ||
+		    p > sw->mib_port_cnt)
+			return DEV_IOC_INVALID_CMD;
+		break;
+	}
+	if (!*len)
+		return DEV_IOC_INVALID_CMD;
+	if (size < *len) {
+		*req_size = *len + SIZEOF_ksz_request;
+		return DEV_IOC_INVALID_LEN;
+	}
+	switch (subcmd) {
+	case DEV_SW_CFG:
+		n = opt->port;
+		sw->ops->acquire(sw);
+		for (i = 0; i < opt->num; i++, n++) {
+			p = get_phy_port(sw, n);
+			cfg->on_off = 0;
+			if (cfg->set & SP_LEARN) {
+				if (!port_chk_dis_learn(sw, p))
+					cfg->on_off |= SP_LEARN;
+			}
+			if (cfg->set & SP_RX) {
+				if (port_chk_rx(sw, p))
+					cfg->on_off |= SP_RX;
+			}
+			if (cfg->set & SP_TX) {
+				if (port_chk_tx(sw, p))
+					cfg->on_off |= SP_TX;
+			}
+			if (p == sw->HOST_PORT)
+				continue;
+#if 0
+			if (cfg->set & SP_PHY_POWER) {
+				if (port_chk_power(sw, p))
+					cfg->on_off |= SP_PHY_POWER;
+			}
+#endif
+			cfg++;
+		}
+		sw->ops->release(sw);
+		break;
+	}
+	return DEV_IOC_OK;
+}  /* sw_get_attrib */
+
+static int sw_set_attrib(struct ksz_sw *sw, int subcmd, int size,
+	int *req_size, u8 *data, int *output)
+{
+	struct ksz_info_opt *opt = (struct ksz_info_opt *) data;
+	struct ksz_info_cfg *cfg = &opt->data.cfg;
+	int len;
+	uint i;
+	uint n;
+	uint p;
+
+	*output = 0;
+	switch (subcmd) {
+	case DEV_SW_CFG:
+		n = opt->num;
+		p = opt->port;
+		if (!n)
+			n = 1;
+		len = 2 + n * sizeof(struct ksz_info_cfg);
+		if (size < len)
+			goto not_enough;
+		if (n > sw->mib_port_cnt ||
+		    p > sw->mib_port_cnt)
+			return DEV_IOC_INVALID_CMD;
+		n = opt->port;
+		sw->ops->acquire(sw);
+		for (i = 0; i < opt->num; i++, n++) {
+			p = get_phy_port(sw, n);
+			if (cfg->set & SP_LEARN)
+				port_cfg_dis_learn(sw, p,
+					!(cfg->on_off & SP_LEARN));
+			if (cfg->set & SP_RX)
+				port_cfg_rx(sw, p,
+					!!(cfg->on_off & SP_RX));
+			if (cfg->set & SP_TX)
+				port_cfg_tx(sw, p,
+					!!(cfg->on_off & SP_TX));
+			if (p == sw->HOST_PORT)
+				continue;
+#if 0
+			if (cfg->set & SP_PHY_POWER)
+				port_cfg_power(sw, p,
+					!!(cfg->on_off & SP_PHY_POWER));
+#endif
+			cfg++;
+		}
+		sw->ops->release(sw);
+		break;
+	default:
+		return DEV_IOC_INVALID_CMD;
+	}
+	return DEV_IOC_OK;
+
+not_enough:
+	*req_size = len + SIZEOF_ksz_request;
+	return DEV_IOC_INVALID_LEN;
+}  /* sw_set_attrib */
+
+static int sw_get_info(struct ksz_sw *sw, int subcmd, int size,
+	int *req_size, size_t *len, u8 *data)
+{
+	struct ksz_info_opt *opt = (struct ksz_info_opt *) data;
+	struct ksz_info_speed *speed = &opt->data.speed;
+	struct ksz_port_info *info;
+	uint i;
+	uint n;
+	uint p;
+
+	*len = 0;
+	switch (subcmd) {
+	case DEV_INFO_SW_LINK:
+		n = opt->num;
+		p = opt->port;
+		if (!n)
+			n = 1;
+		*len = 2 + n * sizeof(struct ksz_info_speed);
+		if (n > sw->mib_port_cnt ||
+		    p > sw->mib_port_cnt)
+			return DEV_IOC_INVALID_CMD;
+		break;
+	}
+	if (!*len)
+		return DEV_IOC_INVALID_CMD;
+	if (size < *len) {
+		*req_size = *len + SIZEOF_ksz_request;
+		return DEV_IOC_INVALID_LEN;
+	}
+	switch (subcmd) {
+	case DEV_INFO_SW_LINK:
+		n = p;
+		for (i = 0; i < opt->num; i++, n++) {
+			p = get_phy_port(sw, n);
+			info = get_port_info(sw, p);
+			if (info->state == media_connected) {
+				speed->tx_rate = info->tx_rate;
+				speed->duplex = info->duplex;
+				speed->flow_ctrl = info->flow_ctrl;
+			} else {
+				memset(speed, 0, sizeof(struct ksz_info_speed));
+			}
+			++speed;
+		}
+		break;
+	}
+	return DEV_IOC_OK;
+}  /* sw_get_info */
+
+static int base_dev_req(struct ksz_sw *sw, char *arg, void *info)
+{
+	struct ksz_request *req = (struct ksz_request *) arg;
+	int len;
+	int maincmd;
+	int req_size;
+	int subcmd;
+	int output;
+	size_t param_size;
+	u8 data[PARAM_DATA_SIZE];
+	struct ksz_resp_msg *msg = (struct ksz_resp_msg *) data;
+	int err = 0;
+	int result = 0;
+
+	get_user_data(&req_size, &req->size, info);
+	get_user_data(&maincmd, &req->cmd, info);
+	get_user_data(&subcmd, &req->subcmd, info);
+	get_user_data(&output, &req->output, info);
+	len = req_size - SIZEOF_ksz_request;
+
+	maincmd &= 0xffff;
+	switch (maincmd) {
+	case DEV_CMD_INFO:
+		switch (subcmd) {
+		case DEV_INFO_INIT:
+			req_size = SIZEOF_ksz_request + 4;
+			if (len >= 4) {
+				data[0] = 'M';
+				data[1] = 'i';
+				data[2] = 'c';
+				data[3] = 'r';
+				data[4] = 1;
+				data[5] = sw->mib_port_cnt;
+				err = write_user_data(data, req->param.data,
+					6, info);
+				if (err)
+					goto dev_ioctl_done;
+			} else
+				result = DEV_IOC_INVALID_LEN;
+			break;
+		case DEV_INFO_EXIT:
+			fallthrough;
+
+		case DEV_INFO_QUIT:
+
+			/* Not called through char device. */
+			if (!info)
+				break;
+			file_dev_clear_notify(sw->dev_list[0], info,
+					      DEV_MOD_BASE,
+					      &sw->notifications);
+			msg->module = DEV_MOD_BASE;
+			msg->cmd = DEV_INFO_QUIT;
+			msg->resp.data[0] = 0;
+			file_dev_setup_msg(info, msg, 8, NULL, NULL);
+			break;
+		case DEV_INFO_NOTIFY:
+			if (len >= 4) {
+				struct file_dev_info *dev_info = info;
+				uint *notify = (uint *) data;
+
+				_chk_ioctl_size(len, 4, 0, &req_size, &result,
+					&req->param, data, info);
+				dev_info->notifications[DEV_MOD_BASE] =
+					*notify;
+				sw->notifications |= *notify;
+			}
+			break;
+		case DEV_INFO_SW_LINK:
+			if (_chk_ioctl_size(len, len, 0, &req_size, &result,
+			    &req->param, data, info))
+				goto dev_ioctl_resp;
+			result = sw_get_info(sw, subcmd, len, &req_size,
+					     &param_size, data);
+			if (result)
+				goto dev_ioctl_resp;
+			err = write_user_data(data, req->param.data,
+					      param_size, info);
+			if (err)
+				goto dev_ioctl_done;
+			req_size = param_size + SIZEOF_ksz_request;
+			break;
+		default:
+			result = DEV_IOC_INVALID_CMD;
+			break;
+		}
+		break;
+	case DEV_CMD_PUT:
+		if (_chk_ioctl_size(len, len, 0, &req_size, &result,
+		    &req->param, data, info))
+			goto dev_ioctl_resp;
+		result = sw_set_attrib(sw, subcmd, len, &req_size, data,
+			&output);
+		if (result)
+			goto dev_ioctl_resp;
+		put_user_data(&output, &req->output, info);
+		break;
+	case DEV_CMD_GET:
+		if (_chk_ioctl_size(len, len, 0, &req_size, &result,
+		    &req->param, data, info))
+			goto dev_ioctl_resp;
+		result = sw_get_attrib(sw, subcmd, len, &req_size,
+			&param_size, data, &output);
+		if (result)
+			goto dev_ioctl_resp;
+		err = write_user_data(data, req->param.data, param_size, info);
+		if (err)
+			goto dev_ioctl_done;
+		req_size = param_size + SIZEOF_ksz_request;
+		put_user_data(&output, &req->output, info);
+		break;
+	default:
+		result = DEV_IOC_INVALID_CMD;
+		break;
+	}
+
+dev_ioctl_resp:
+	put_user_data(&req_size, &req->size, info);
+	put_user_data(&result, &req->result, info);
+
+	/* Return ERESTARTSYS so that the system call is called again. */
+	if (result < 0)
+		err = result;
+
+dev_ioctl_done:
+	return err;
+}  /* base_dev_req */
+
+static int sw_dev_req(struct ksz_sw *sw, char *arg,
+	struct file_dev_info *info)
+{
+	struct ksz_request *req = (struct ksz_request *) arg;
+	int maincmd;
+	int req_size;
+	int err = 0;
+	int result = DEV_IOC_OK;
+
+	/* Check request size. */
+	get_user_data(&req_size, &req->size, info);
+	if (chk_ioctl_size(req_size, SIZEOF_ksz_request, 0, &req_size,
+	    &result, NULL, NULL))
+		goto dev_ioctl_resp;
+
+	result = -EOPNOTSUPP;
+	get_user_data(&maincmd, &req->cmd, info);
+	maincmd >>= 16;
+	switch (maincmd) {
+	case DEV_MOD_BASE:
+		err = base_dev_req(sw, arg, info);
+		result = 0;
+		break;
+#ifdef CONFIG_KSZ_DLR
+	case DEV_MOD_DLR:
+		if (sw->features & DLR_HW) {
+			struct ksz_dlr_info *dlr = &sw->info->dlr;
+
+			err = dlr->ops->dev_req(dlr, arg, info);
+			result = 0;
+		}
+		break;
+#endif
+	default:
+		break;
+	}
+
+	/* Processed by specific module. */
+	if (!result)
+		return err;
+	if (result < 0)
+		goto dev_ioctl_done;
+
+dev_ioctl_resp:
+	put_user_data(&req_size, &req->size, info);
+	put_user_data(&result, &req->result, info);
+
+dev_ioctl_done:
+
+	/* Return ERESTARTSYS so that the system call is called again. */
+	if (result < 0)
+		err = result;
+
+	return err;
+}  /* sw_dev_req */
+
+static ssize_t sw_dev_read(struct file *filp, char *buf, size_t count,
+	loff_t *offp)
+{
+	struct file_dev_info *info = (struct file_dev_info *)
+		filp->private_data;
+	ssize_t result = 0;
+	int rc;
+
+	if (!info->read_len) {
+		*offp = 0;
+		rc = wait_event_interruptible(info->wait_msg,
+			0 != info->read_len);
+
+		/* Cannot continue if ERESTARTSYS. */
+		if (rc < 0)
+			return 0;
+	}
+
+	if (down_interruptible(&info->sem))
+		return -ERESTARTSYS;
+
+	mutex_lock(&info->lock);
+	if (*offp >= info->read_len) {
+		info->read_len = 0;
+		count = 0;
+		*offp = 0;
+		goto dev_read_done;
+	}
+
+	if (*offp + count > info->read_len) {
+		count = info->read_len - *offp;
+		info->read_len = 0;
+	}
+
+	if (copy_to_user(buf, &info->read_buf[*offp], count)) {
+		result = -EFAULT;
+		goto dev_read_done;
+	}
+	if (info->read_len)
+		*offp += count;
+	else
+		*offp = 0;
+	result = count;
+
+dev_read_done:
+	mutex_unlock(&info->lock);
+	up(&info->sem);
+	return result;
+}  /* sw_dev_read */
+
+static long sw_dev_ioctl(struct file *filp, unsigned int cmd,
+	unsigned long arg)
+{
+	struct file_dev_info *info = (struct file_dev_info *)
+		filp->private_data;
+	struct ksz_sw *sw = info->dev;
+	int err = 0;
+
+	if (_IOC_TYPE(cmd) != DEV_IOC_MAGIC)
+		return -ENOTTY;
+	if (_IOC_NR(cmd) > DEV_IOC_MAX)
+		return -ENOTTY;
+	if (_IOC_DIR(cmd) & _IOC_READ)
+		err = !access_ok((void *) arg, _IOC_SIZE(cmd));
+	else if (_IOC_DIR(cmd) & _IOC_WRITE)
+		err = !access_ok((void *) arg, _IOC_SIZE(cmd));
+	if (err) {
+		printk(KERN_ALERT "err fault\n");
+		return -EFAULT;
+	}
+	if (down_interruptible(&info->sem))
+		return -ERESTARTSYS;
+
+	err = sw_dev_req(sw, (char *) arg, info);
+	up(&info->sem);
+	return err;
+}  /* sw_dev_ioctl */
+
+static ssize_t sw_dev_write(struct file *filp, const char *buf, size_t count,
+	loff_t *offp)
+{
+	struct file_dev_info *info = (struct file_dev_info *)
+		filp->private_data;
+	ssize_t result = 0;
+	size_t size;
+	int rc;
+
+	if (!count)
+		return result;
+
+	if (down_interruptible(&info->sem))
+		return -ERESTARTSYS;
+
+	if (*offp >= info->write_len) {
+		result = -ENOSPC;
+		goto dev_write_done;
+	}
+	if (*offp + count > info->write_len)
+		count = info->write_len - *offp;
+	if (copy_from_user(info->write_buf, buf, count)) {
+		result = -EFAULT;
+		goto dev_write_done;
+	}
+	size = 0;
+	result = size;
+	rc = 0;
+	if (rc)
+		result = rc;
+
+dev_write_done:
+	up(&info->sem);
+	return result;
+}  /* sw_dev_write */
+
+static const struct file_operations sw_dev_fops = {
+	.read		= sw_dev_read,
+	.write		= sw_dev_write,
+	.unlocked_ioctl	= sw_dev_ioctl,
+	.open		= sw_dev_open,
+	.release	= sw_dev_release,
+};
+
+static struct class *sw_class[MAX_SW_DEVICES];
+
+static int init_sw_dev(int id, int dev_major, char *dev_name)
+{
+	int result;
+
+	result = register_chrdev(dev_major, dev_name, &sw_dev_fops);
+	if (result < 0) {
+		printk(KERN_WARNING "%s: can't get major %d\n", dev_name,
+			dev_major);
+		return result;
+	}
+	if (0 == dev_major)
+		dev_major = result;
+	sw_class[id] = class_create(THIS_MODULE, dev_name);
+	if (IS_ERR(sw_class[id])) {
+		unregister_chrdev(dev_major, dev_name);
+		return -ENODEV;
+	}
+	device_create(sw_class[id], NULL, MKDEV(dev_major, 0), NULL, dev_name);
+	return dev_major;
+}  /* init_sw_dev */
+
+static void exit_sw_dev(int id, int dev_major, char *dev_name)
+{
+	device_destroy(sw_class[id], MKDEV(dev_major, 0));
+	class_destroy(sw_class[id]);
+	unregister_chrdev(dev_major, dev_name);
+}  /* exit_sw_dev */
+
+static void sw_init_dev(struct ksz_sw *sw)
+{
+	sprintf(sw->dev_name, "sw_dev");
+	if (sw->id)
+		sprintf(sw->dev_name, "sw_dev_%u", sw->id);
+	sw->dev_major = init_sw_dev(sw->id, 0, sw->dev_name);
+	sw->msg_buf = kzalloc(MAX_SW_LEN, GFP_KERNEL);
+	sw_majors[sw->id].dev = sw;
+	sw_majors[sw->id].major = sw->dev_major;
+}  /* sw_init_dev */
+
+static void sw_exit_dev(struct ksz_sw *sw)
+{
+	kfree(sw->msg_buf);
+	if (sw->dev_major >= 0)
+		exit_sw_dev(sw->id, sw->dev_major, sw->dev_name);
+}  /* sw_exit_dev */
+
+static void sw_report_link(struct ksz_sw *sw, struct ksz_port *port,
+			   struct ksz_port_info *info)
+{
+	struct ksz_port_info *linked = port->linked;
+	struct phy_device *phydev = port->phydev;
+	struct net_device *dev = port->netdev;
+	int lpa = info->lpa;
+	int phy_link = 0;
+	int link;
+
+	phydev->link = (info->state == media_connected);
+	phydev->speed = info->tx_rate / TX_RATE_UNIT;
+	if (!phydev->speed)
+		phydev->speed = 10;
+	phydev->duplex = (info->duplex == 2);
+
+#ifdef CONFIG_PHYLINK
+	/* Not started yet. */
+	if (sw->phylink_ops && port != sw->main_port &&
+	    phydev->state == PHY_READY)
+		return;
+#endif
+	if (phydev->link)
+		phy_link = (linked->state == media_connected);
+	link = netif_carrier_ok(dev);
+	if (port->report) {
+		port->report = false;
+		link = !phy_link;
+	}
+	if (phy_link == link)
+		return;
+
+	/* Update link partner capabilities. */
+	if (lpa) {
+		mii_lpa_mod_linkmode_lpa_t(phydev->lp_advertising, lpa);
+#if 0
+dbg_msg(" %*pb\n", __ETHTOOL_LINK_MODE_MASK_NBITS, phydev->advertising);
+dbg_msg(" %*pb\n", __ETHTOOL_LINK_MODE_MASK_NBITS, phydev->lp_advertising);
+#endif
+	}
+	if (netif_msg_link(sw))
+		pr_info("%s link %s"NL,
+			dev->name,
+			phy_link ? "on" : "off");
+	if (phydev->phy_link_change) {
+		phydev->phy_link_change(phydev, phy_link);
+	} else if (phy_link != link) {
+		if (phy_link)
+			netif_carrier_on(dev);
+		else
+			netif_carrier_off(dev);
+	}
+}  /* sw_report_link */
+
+static void link_update_work(struct work_struct *work)
+{
+	struct delayed_work *dwork = to_delayed_work(work);
+	struct ksz_port *port =
+		container_of(dwork, struct ksz_port, link_update);
+	struct ksz_sw *sw = port->sw;
+	struct ksz_port_info *info;
+
+	/* Netdevice associated with port was closed. */
+	if (!port->opened)
+		goto do_main;
+
+	if (sw->dev_offset && sw->netport[0]) {
+		int dev_cnt = sw->dev_count + sw->dev_offset;
+		struct ksz_port *sw_port = sw->netport[0];
+		struct ksz_port *dev_port;
+		int i;
+
+		for (i = sw->dev_offset; i < dev_cnt; i++) {
+			struct phy_priv *phydata;
+
+			dev_port = sw->netport[i];
+			if (!dev_port) {
+				phydata = &sw->phydata[i];
+				dev_port = phydata->port;
+			}
+			if (media_connected == dev_port->linked->state) {
+				sw_port->linked = dev_port->linked;
+				break;
+			}
+		}
+	}
+
+#ifdef CONFIG_KSZ_DLR
+	if (sw->features & DLR_HW) {
+		struct ksz_dlr_info *dlr = &sw->info->dlr;
+
+		dlr->ops->link_change(dlr,
+			sw->port_info[dlr->ports[0]].state == media_connected,
+			sw->port_info[dlr->ports[1]].state == media_connected);
+	}
+#endif
+
+	sw_notify_link_change(sw, port->link_ports);
+
+	if ((!sw->dev_offset || port != sw->netport[0]) && port->netdev)
+		sw_report_link(sw, port, port->linked);
+
+#ifdef CONFIG_KSZ_STP
+	if (sw->features & STP_SUPPORT) {
+		struct ksz_stp_info *stp = &sw->info->rstp;
+
+		stp->ops->link_change(stp, true);
+	}
+#endif
+
+do_main:
+	port->link_ports = 0;
+
+	/* There is an extra network device for the main device. */
+	/* The switch is always linked; speed and duplex are also fixed. */
+	if (sw->dev_offset) {
+		port = sw->netport[0];
+		if (port && port->opened && netif_running(port->netdev)) {
+			info = get_port_info(sw, sw->HOST_PORT);
+			sw_report_link(sw, port, info);
+		}
+	}
+}  /* link_update_work */
+
+static void set_phy_support(struct ksz_port *port, struct phy_device *phydev)
+{
+	switch (port->flow_ctrl) {
+	case PHY_NO_FLOW_CTRL:
+		linkmode_clear_bit(ETHTOOL_LINK_MODE_Pause_BIT,
+				   phydev->supported);
+		linkmode_clear_bit(ETHTOOL_LINK_MODE_Asym_Pause_BIT,
+				   phydev->supported);
+		break;
+	case PHY_RX_ONLY:
+		/* Depend on link partner to only advertise Asym_Pause. */
+		linkmode_set_bit(ETHTOOL_LINK_MODE_Pause_BIT,
+				 phydev->supported);
+		linkmode_set_bit(ETHTOOL_LINK_MODE_Asym_Pause_BIT,
+				 phydev->supported);
+		break;
+	case PHY_TX_ONLY:
+		linkmode_clear_bit(ETHTOOL_LINK_MODE_Pause_BIT,
+				   phydev->supported);
+		linkmode_set_bit(ETHTOOL_LINK_MODE_Asym_Pause_BIT,
+				 phydev->supported);
+		break;
+	default:
+		linkmode_set_bit(ETHTOOL_LINK_MODE_Pause_BIT,
+				 phydev->supported);
+		linkmode_clear_bit(ETHTOOL_LINK_MODE_Asym_Pause_BIT,
+				   phydev->supported);
+		break;
+	}
+}  /* set_phy_support */
+
+#if 0
+#define USE_DLR
+#endif
+
+/*
+ * This enables multiple network device mode for the switch, which contains at
+ * least two physical ports.  Some users like to take control of the ports for
+ * running Spanning Tree Protocol.  The driver will create an additional eth?
+ * device for each port depending on the mode.
+ *
+ * Some limitations are the network devices cannot have different MTU and
+ * multicast hash tables.
+ */
+static int multi_dev = -1;
+
+static int stp = -1;
+
+/*
+ * This enables fast aging in the switch.  Not sure what situation requires
+ * that.  However, fast aging is used to flush the dynamic MAC table when STP
+ * support is enabled.
+ */
+static int fast_aging;
+
+#if defined(USE_DLR)
+static int eth1_ports = 0x3;
+#else
+static int eth1_ports;
+#endif
+static int eth2_ports;
+static int eth3_ports;
+static int eth4_ports;
+
+static int eth1_vlan;
+static int eth2_vlan;
+static int eth3_vlan;
+static int eth4_vlan;
+
+#if defined(USE_DLR)
+static char *eth1_proto = "dlr";
+#endif
+#if !defined(USE_DLR)
+static char *eth1_proto = " ";
+#endif
+static char *eth2_proto = " ";
+static char *eth3_proto = " ";
+static char *eth4_proto = " ";
+
+static int *eth_ports[] = {
+	&eth1_ports,
+	&eth2_ports,
+	&eth3_ports,
+	&eth4_ports,
+	NULL
+};
+
+static int *eth_vlans[] = {
+	&eth1_vlan,
+	&eth2_vlan,
+	&eth3_vlan,
+	&eth4_vlan,
+	NULL
+};
+
+static char **eth_proto[] = {
+	&eth1_proto,
+	&eth2_proto,
+	&eth3_proto,
+	&eth4_proto,
+	NULL
+};
+
+struct ksz_port_mapping {
+	u8 id;
+	u8 cnt;
+	u8 fiber;
+	u8 map[8];
+};
+
+enum {
+	KSZ8795_SKU,
+	KSZ8794_SKU,
+	KSZ8765_SKU,
+};
+
+static struct ksz_port_mapping port_mappings[] = {
+	{ KSZ8795_SKU,   5, 0, { 1, 2, 3, 4, 5, 0, 0, 0 }},
+	{ KSZ8794_SKU,   5, 0, { 1, 2, 3, 5, 0, 0, 0, 0 }},
+	{ KSZ8765_SKU,   5, 1, { 1, 2, 3, 4, 5, 0, 0, 0 }},
+};
+
+static u8 port_map[8];
+
+static void ksz_setup_logical_ports(struct ksz_sw *sw, u8 id, uint ports)
+{
+	struct ksz_port_mapping *map;
+	struct ksz_port_info *info;
+	struct ksz_port_info *pinfo;
+	uint i;
+	uint l;
+	uint n;
+	uint p;
+	uint cnt = 0;
+
+	for (i = 0; i < ARRAY_SIZE(port_mappings); i++) {
+		map = &port_mappings[i];
+		if (id == map->id) {
+			cnt = map->cnt;
+			memset(port_map, 0, 8 * sizeof(u8));
+			memcpy(port_map, map->map, cnt * sizeof(u8));
+			for (n = 0; n < sw->port_cnt; n++) {
+				if (n == sw->HOST_PORT)
+					break;
+				info = &sw->port_info[n];
+				info->phy = 1;
+				info->fiber = map->fiber;
+			}
+			break;
+		}
+	}
+	if (!cnt)
+		return;
+
+	for (n = 0; n < cnt; n++) {
+		p = map->map[n];
+		if (!p)
+			break;
+		--p;
+		info = &sw->port_info[p];
+		if (p == sw->HOST_PORT) {
+			l = 0;
+		} else {
+			l = p + 1;
+			if (p > sw->HOST_PORT)
+				--l;
+		}
+		info->log_p = l;
+		info->log_m = 0;
+	}
+	n = (1 << cnt) - 1;
+	ports &= n;
+	for (i = 0, n = 0; n <= sw->port_cnt; n++) {
+		if (n > 0) {
+			p = n - 1;
+			if (!(ports & BIT(p)))
+				continue;
+			p = map->map[p] - 1;
+			if (p == sw->HOST_PORT)
+				continue;
+			l = i;
+			++i;
+		} else {
+			p = sw->HOST_PORT;
+			l = 0;
+		}
+		info = &sw->port_info[i];
+		info->phy_p = p;
+		info->phy_m = BIT(p);
+		info = &sw->port_info[p];
+		info->log_p = i;
+		info->log_m = BIT(l);
+		info->phy_id = p + 1;
+	}
+	info = &sw->port_info[sw->HOST_PORT];
+	info->log_m = BIT(i);
+
+	ports = 0;
+	for (n = 0; n <= i; n++) {
+		info = &sw->port_info[n];
+		ports |= info->phy_m;
+	}
+dbg_msg("ports: %d %x\n", i, ports);
+
+	for (n = 0; n <= i; n++) {
+		info = &sw->port_info[n];
+		pinfo = &sw->port_info[info->phy_p];
+dbg_msg("%d= %d:%02x %d:%02x %d:%02x %d\n", n,
+info->phy_p, info->phy_m, info->log_p, info->log_m,
+pinfo->log_p, pinfo->log_m, pinfo->fiber);
+	}
+#if 1
+	if (i + 1 < sw->port_cnt) {
+		for (n = 0; n < sw->port_cnt; n++) {
+			info = &sw->port_info[n];
+			if (!info->log_m)
+dbg_msg(" %d= %d %d\n", n, info->log_p, info->fiber);
+			if (!info->log_m)
+				info->log_p = sw->port_cnt;
+		}
+	}
+#endif
+	sw->PORT_MASK = ports;
+dbg_msg("mask: %x %x\n", sw->HOST_MASK, sw->PORT_MASK);
+	sw->mib_port_cnt = i;
+	if (sw->mib_port_cnt + 1 < sw->port_cnt)
+		sw->features |= USE_FEWER_PORTS;
+	for (i = 0; i < sw->eth_cnt; i++) {
+dbg_msg(" eth_maps: %d=%x\n", i, sw->eth_maps[i].mask);
+		sw->eth_maps[i].mask = 0;
+		for (l = 0, n = sw->eth_maps[i].first;
+		     l < sw->eth_maps[i].cnt; l++, n++) {
+			p = get_phy_port(sw, n);
+			sw->eth_maps[i].mask |= BIT(p);
+		}
+dbg_msg("   eth_maps: %d=%x\n", i, sw->eth_maps[i].mask);
+	}
+#ifdef CONFIG_KSZ_DLR
+	if (sw->features & DLR_HW) {
+		struct ksz_dlr_info *dlr = &sw->info->dlr;
+
+		dlr->ports[0] = port_map[dlr->ports[0]] - 1;
+		dlr->ports[1] = port_map[dlr->ports[1]] - 1;
+		dlr->member = BIT(dlr->ports[0]) | BIT(dlr->ports[1]);
+dbg_msg(" dlr member: %d:%d %x\n", dlr->ports[0], dlr->ports[1], dlr->member);
+	}
+#endif
+}  /* ksz_setup_logical_ports */
+
+static uint sw_setup_zone(struct ksz_sw *sw, uint in_ports)
+{
+	int c;
+	int f;
+	int limit;
+	int m;
+	uint p;
+	uint q;
+	int w;
+	int *v;
+	char **s;
+	uint features;
+	struct ksz_dev_map *map;
+	uint ports;
+	uint used = 0;
+	int last_log_port = 0;
+	int last_phy_port = 0;
+	int last_vlan = 0;
+	uint left = (1 << sw->port_cnt) - 1;
+
+	if (multi_dev > 2) {
+		ports = in_ports;
+		goto setup_next;
+	}
+	q = 0;
+	ports = 0;
+	for (p = 0; p < sw->port_cnt - 1; p++) {
+		v = eth_ports[p];
+
+		/* No more port setting. */
+		if (!v || !*v)
+			break;
+		m = *v;
+		if (!(m & left)) {
+			left = 0;
+			break;
+		}
+
+		/* Find out how the ports are to be used. */
+		limit = 0;
+		w = last_vlan;
+		features = 0;
+		s = eth_proto[p];
+#ifdef CONFIG_KSZ_DLR
+		if (!strcmp(*s, "dlr")) {
+			features = DLR_HW;
+			limit = 2;
+			if (!w)
+				w = 1;
+		}
+#endif
+#ifdef CONFIG_KSZ_STP
+		if (!strcmp(*s, "stp")) {
+			features = STP_SUPPORT;
+		}
+#endif
+
+		m &= ~((1 << last_phy_port) - 1);
+		m &= left;
+
+		/* No more legimate port. */
+		if (!m)
+			break;
+
+		v = eth_vlans[p];
+		if (!w && (!v || !*v))
+			break;
+		if (*v)
+			w = *v;
+
+		/* Check VLAN id is unused. */
+		for (q = 0; q < p; q++) {
+			if (w > 1 && w == sw->eth_maps[q].vlan)
+				w = last_vlan + 1;
+		}
+		c = 0;
+		f = -1;
+		for (q = p; q < sw->port_cnt - 1; q++) {
+			if (m & (1 << q)) {
+				if (f < 0)
+					f = last_log_port;
+				++c;
+				++last_log_port;
+
+				/* Limit to certain ports. */
+				if (limit && c >= limit) {
+					if (!(used & features)) {
+						used |= features;
+						++q;
+						last_phy_port = q;
+						break;
+					}
+					features = 0;
+				}
+				last_phy_port = q + 1;
+			} else if (f >= 0) {
+				if (limit && c < limit)
+					features = 0;
+				break;
+			}
+		}
+		if (!c)
+			continue;
+		m &= (1 << q) - 1;
+		if (!p && c > 1)
+			used |= (features & VLAN_PORT);
+#ifdef CONFIG_KSZ_STP
+		if ((features & STP_SUPPORT) && c > 1) {
+			used |= (features & STP_SUPPORT);
+			stp = m;
+		}
+#endif
+		++f;
+		map = &sw->eth_maps[p];
+		map->cnt = c;
+		map->mask = m;
+		map->first = f;
+		map->phy_id = f;
+		map->vlan = w & (4096 - 1);
+		map->proto = features;
+		if (last_vlan < w)
+			last_vlan = w;
+		ports |= m;
+#ifdef CONFIG_KSZ_DLR
+		if (features & DLR_HW) {
+			struct ksz_dlr_info *dlr = &sw->info->dlr;
+
+			c = 0;
+			f = 0;
+			do {
+				if (m & 1) {
+					dlr->ports[c++] = f;
+				}
+				m >>= 1;
+				++f;
+			} while (m && c < map->cnt);
+		}
+#endif
+	}
+
+	/* No VLAN devices specified. */
+	if (!p) {
+		ports = in_ports;
+		goto setup_next;
+	}
+
+	/* Not all ports are used. */
+	left &= ~((1 << last_phy_port) - 1);
+	if (multi_dev != 1)
+		left = 0;
+	features = 0;
+	s = eth_proto[p];
+#ifdef CONFIG_KSZ_STP
+	if (s && !strcmp(*s, "stp"))
+		features = STP_SUPPORT;
+#endif
+	if (left) {
+		m = left;
+		c = 0;
+		f = -1;
+		for (q = 0; q < sw->mib_port_cnt - 1; q++) {
+			if (m & (1 << q)) {
+				if (f < 0)
+					f = last_log_port;
+				++c;
+			}
+		}
+		m &= (1 << q) - 1;
+		if ((features & STP_SUPPORT) && c > 1) {
+			used |= (features & STP_SUPPORT);
+			stp = m;
+		}
+		++f;
+		map = &sw->eth_maps[p];
+		map->cnt = c;
+		map->mask = m;
+		map->first = f;
+		map->phy_id = f;
+		map->vlan = ++last_vlan & (4096 - 1);
+		map->proto = features;
+		ports |= m;
+		p++;
+	}
+	if (p > 1)
+		sw->features |= SW_VLAN_DEV;
+	sw->eth_cnt = p;
+	for (p = 0; p < sw->eth_cnt; p++) {
+		map = &sw->eth_maps[p];
+		dbg_msg("%d: %d:%d %04x %03x %08x\n",
+			p, map->first, map->cnt,
+			map->mask, map->vlan, map->proto);
+	}
+
+setup_next:
+#ifdef CONFIG_KSZ_DLR
+	if (!(used & DLR_HW))
+		sw->features &= ~DLR_HW;
+#endif
+	if ((sw->features & (DLR_HW | HSR_HW)) || sw->eth_cnt > 1) {
+		if (stp <= 1)
+			stp = 0;
+#if 0
+		sw->overrides &= ~USE_802_1X_AUTH;
+#endif
+	}
+#ifdef CONFIG_KSZ_STP
+	if (stp > 0) {
+		sw->features |= STP_SUPPORT;
+	}
+#endif
+
+dbg_msg("features: %x m:%d s:%x\n", sw->features, multi_dev, stp);
+	return ports;
+}  /* sw_setup_zone */
+
+static int phy_offset;
+
+static void sw_setup_special(struct ksz_sw *sw, int *port_cnt,
+	int *mib_port_cnt, int *dev_cnt,
+	const void *phylink_ops)
+{
+#ifdef CONFIG_PHYLINK
+	sw->phylink_ops = phylink_ops;
+#endif
+	phy_offset = 0;
+	sw->dev_offset = 0;
+	sw->phy_offset = 0;
+
+	/* Multiple network device interfaces are required. */
+	if (1 == sw->multi_dev) {
+		sw->dev_count = sw->mib_port_cnt;
+		sw->phy_offset = 1;
+	} else if (2 == sw->multi_dev)
+		sw->features |= VLAN_PORT | VLAN_PORT_TAGGING;
+	else if (3 == sw->multi_dev) {
+		sw->dev_count = sw->mib_port_cnt;
+		sw->dev_offset = 1;
+	} else if (4 == sw->multi_dev)
+		sw->features |= VLAN_PORT;
+	else if (5 == sw->multi_dev) {
+		sw->dev_count = sw->mib_port_cnt;
+		sw->dev_offset = 1;
+		sw->features |= VLAN_PORT | VLAN_PORT_TAGGING;
+	}
+
+	/* Single network device has multiple ports. */
+	if (1 == sw->dev_count) {
+		*port_cnt = sw->mib_port_cnt;
+		*mib_port_cnt = sw->mib_port_cnt;
+	}
+	if (1 == sw->multi_dev && sw->eth_cnt)
+		sw->dev_count = sw->eth_cnt;
+	*dev_cnt = sw->dev_count;
+	if (3 == sw->multi_dev || 5 == sw->multi_dev)
+		(*dev_cnt)++;
+}  /* sw_setup_special */
+
+static void sw_leave_dev(struct ksz_sw *sw)
+{
+	int dev_count = sw->dev_count + sw->dev_offset;
+	struct sw_priv *ks = sw->dev;
+	struct phy_priv *phydata;
+#ifdef CONFIG_PHYLINK
+	struct ksz_port *port;
+#endif
+	int i;
+
+#ifdef CONFIG_KSZ_STP
+	if (sw->features & STP_SUPPORT)
+		leave_stp(&sw->info->rstp);
+#endif
+#ifdef CONFIG_KSZ_MRP
+	if (sw->features & MRP_SUPPORT)
+		leave_mrp(&sw->mrp);
+#endif
+	for (i = 0; i < dev_count; i++) {
+#ifdef CONFIG_PHYLINK
+		port = sw->netport[i];
+		if (port && port->pl) {
+		       phylink_destroy(port->pl);
+		       port->pl = NULL;
+		}
+#endif
+		sw->netdev[i] = NULL;
+		sw->netport[i] = NULL;
+	}
+
+	/* Reset port pointer as it is pointed to one from device. */
+	for (i = 0; i <= sw->port_cnt; i++) {
+		phydata = &sw->phydata[i];
+		phydata->port = &ks->ports[i];
+	}
+	sw->eth_cnt = 0;
+	sw->dev_count = 1;
+	sw->dev_offset = 0;
+	sw->phy_offset = 0;
+}  /* sw_leave_dev */
+
+static int sw_setup_dev(struct ksz_sw *sw, struct net_device *dev,
+	char *dev_name, struct ksz_port *port, int i, uint port_cnt,
+	uint mib_port_cnt)
+{
+	struct ksz_port_info *info;
+	uint cnt;
+	uint n;
+	uint p;
+	uint pi;
+	int phy_id;
+	u32 features;
+	struct ksz_dev_map *map;
+
+	if (!phy_offset)
+		phy_offset = sw->phy_offset;
+
+	/* dev_offset is ether 0 or 1. */
+	p = i;
+	if (p)
+		p -= sw->dev_offset;
+
+	if (sw->dev_offset) {
+		/*
+		 * First device associated with switch has been
+		 * created.
+		 */
+		if (i) {
+			snprintf(dev->name, IFNAMSIZ, "%s.10%%d", dev_name);
+			pi = get_phy_port(sw, p + 1);
+			memcpy(dev->dev_addr, sw->port_info[pi].mac_addr,
+				ETH_ALEN);
+		} else {
+			port_cnt = sw->mib_port_cnt;
+			mib_port_cnt = sw->mib_port_cnt;
+			sw->ops->acquire(sw);
+			sw_set_addr(sw, dev->dev_addr);
+			sw->ops->release(sw);
+		}
+	}
+
+	map = &sw->eth_maps[i];
+	if (1 == sw->multi_dev && sw->eth_cnt) {
+		port_cnt = map->cnt;
+		p = map->first - 1;
+		mib_port_cnt = port_cnt;
+	}
+
+	port->port_cnt = port_cnt;
+	port->mib_port_cnt = mib_port_cnt;
+	port->first_port = p + 1;
+	port->flow_ctrl = PHY_FLOW_CTRL;
+
+#ifdef CONFIG_KSZ_STP
+	if (!i && (sw->features & STP_SUPPORT))
+		prep_stp_mcast(dev);
+#endif
+
+#ifdef CONFIG_KSZ_DLR
+	/* Cannot flow control because of beacon timeout. */
+	if (sw->eth_cnt && (map->proto & DLR_HW)) {
+		port->flow_ctrl = PHY_NO_FLOW_CTRL;
+		prep_dlr_mcast(dev);
+	}
+#endif
+#ifdef CONFIG_KSZ_MRP
+	if (!i && (sw->features & MRP_SUPPORT))
+		setup_mrp(&sw->mrp, dev);
+#endif
+
+	p = get_phy_port(sw, port->first_port);
+	port->sw = sw;
+	port->linked = get_port_info(sw, p);
+
+	/* Point to port under netdev. */
+	if (phy_offset)
+		phy_id = port->linked->phy_id;
+	else
+		phy_id = 0;
+
+	/* Replace virtual port with one from network device. */
+	do {
+		struct phy_device *phydev;
+		struct phy_priv *priv;
+		struct sw_priv *hw_priv = container_of(sw, struct sw_priv, sw);
+
+		phydev = mdiobus_get_phy(hw_priv->bus, phy_id);
+		priv = phydev->priv;
+		priv->port = port;
+		set_phy_support(port, phydev);
+	} while (0);
+
+	if (!phy_offset)
+		phy_offset = 1;
+
+	for (cnt = 0, n = port->first_port; cnt < port_cnt; cnt++, n++) {
+		pi = get_phy_port(sw, n);
+		info = get_port_info(sw, pi);
+		info->state = media_disconnected;
+		sw->info->port_cfg[pi].index = i;
+	}
+	sw->netdev[i] = dev;
+	sw->netport[i] = port;
+	port->netdev = dev;
+	port->phydev = sw->phy[phy_id];
+	if (phy_id)
+		port->dn = sw->devnode[phy_id - 1];
+#ifdef CONFIG_PHYLINK
+	setup_phylink(port);
+#endif
+	if (sw->dev_count > 1 && i && !(sw->features & DIFF_MAC_ADDR)) {
+		if (memcmp(dev->dev_addr, sw->netdev[0]->dev_addr, ETH_ALEN))
+			sw->features |= DIFF_MAC_ADDR;
+	}
+
+	INIT_DELAYED_WORK(&port->link_update, link_update_work);
+	features = sw->features;
+	if (sw->features & SW_VLAN_DEV)
+		features = map->proto;
+
+	if (features & VLAN_PORT)
+		dev->features |= NETIF_F_HW_VLAN_CTAG_FILTER;
+
+	/* Needed for inserting VLAN tag. */
+	if (sw->features & SW_VLAN_DEV)
+		dev->hard_header_len += VLAN_HLEN;
+dbg_msg("%s %d:%d phy:%d\n", __func__, port->first_port, port->port_cnt, phy_id);
+
+	return phy_id;
+}  /* sw_setup_dev */
+
+static int netdev_chk_running(struct net_device *dev)
+{
+	return netif_running(dev);
+}
+
+static int netdev_chk_stopped(struct net_device *dev)
+{
+	return netif_running(dev) && netif_queue_stopped(dev);
+}
+
+static void netdev_start_queue(struct net_device *dev)
+{
+	netif_start_queue(dev);
+}
+
+static void netdev_stop_queue(struct net_device *dev)
+{
+	netif_stop_queue(dev);
+}
+
+static void netdev_wake_queue(struct net_device *dev)
+{
+	netif_wake_queue(dev);
+}
+
+static void sw_netdev_oper(struct ksz_sw *sw, struct net_device *dev,
+	int (*netdev_chk)(struct net_device *dev),
+	void (*netdev_oper)(struct net_device *dev))
+{
+	uint port;
+	int dev_count = 1;
+
+	dev_count = sw->dev_count + sw->dev_offset;
+	if (dev_count <= 1) {
+		netdev_oper(dev);
+		return;
+	}
+	for (port = 0; port < dev_count; port++) {
+		dev = sw->netdev[port];
+		if (!dev)
+			continue;
+		if (!netdev_chk || netdev_chk(dev))
+			netdev_oper(dev);
+	}
+}  /* sw_netdev_oper */
+
+static void sw_netdev_open_port(struct ksz_sw *sw, struct net_device *dev)
+{
+	struct ksz_port *port;
+	int p;
+	int dev_count = 1;
+
+	dev_count = sw->dev_count + sw->dev_offset;
+	if (dev_count <= 1) {
+		port = sw->netport[0];
+		sw->net_ops->open_port(sw, dev, port);
+		return;
+	}
+	for (p = 0; p < dev_count; p++) {
+		dev = sw->netdev[p];
+		if (!dev)
+			continue;
+		port = sw->netport[p];
+		sw->net_ops->open_port(sw, dev, port);
+	}
+}  /* sw_netdev_open_port */
+
+static void sw_netdev_start_queue(struct ksz_sw *sw, struct net_device *dev)
+{
+	sw_netdev_oper(sw, dev, netdev_chk_running, netdev_start_queue);
+}
+
+static void sw_netdev_stop_queue(struct ksz_sw *sw, struct net_device *dev)
+{
+	sw_netdev_oper(sw, dev, netdev_chk_running, netdev_stop_queue);
+}
+
+static void sw_netdev_wake_queue(struct ksz_sw *sw, struct net_device *dev)
+{
+	sw_netdev_oper(sw, dev, netdev_chk_stopped, netdev_wake_queue);
+}
+
+static struct ksz_sw_net_ops sw_net_ops = {
+	.setup_special		= sw_setup_special,
+	.setup_dev		= sw_setup_dev,
+	.leave_dev		= sw_leave_dev,
+
+	.start			= sw_start,
+	.stop			= sw_stop,
+	.open_dev		= sw_open_dev,
+	.open_port		= sw_open_port,
+	.close_port		= sw_close_port,
+	.open			= sw_open,
+	.close			= sw_close,
+
+	.netdev_start_queue	= sw_netdev_start_queue,
+	.netdev_stop_queue	= sw_netdev_stop_queue,
+	.netdev_wake_queue	= sw_netdev_wake_queue,
+	.netdev_open_port	= sw_netdev_open_port,
+
+	.set_mac_addr		= sw_set_mac_addr,
+
+	.get_mtu		= sw_get_mtu,
+	.get_tx_len		= sw_get_tx_len,
+	.add_tail_tag		= sw_add_tail_tag,
+	.get_tail_tag		= sw_get_tail_tag,
+	.add_vid		= sw_add_vid,
+	.kill_vid		= sw_kill_vid,
+	.check_tx		= sw_check_tx,
+	.rx_dev			= sw_rx_dev,
+	.match_pkt		= sw_match_pkt,
+	.parent_rx		= sw_parent_rx,
+	.port_vlan_rx		= sw_port_vlan_rx,
+	.final_skb		= sw_final_skb,
+	.drv_rx			= sw_drv_rx,
+	.set_multi		= sw_set_multi,
+
+};
+
+static struct ksz_sw_ops sw_ops = {
+	.init			= sw_init_dev,
+	.exit			= sw_exit_dev,
+	.dev_req		= sw_dev_req,
+
+	.get_phy_port		= get_phy_port,
+	.get_log_port		= get_log_port,
+
+	.acquire		= sw_acquire,
+	.release		= sw_release,
+
+	.chk			= sw_chk,
+	.cfg			= sw_cfg,
+
+	.port_get_link_speed	= port_get_link_speed,
+	.port_set_link_speed	= port_set_link_speed,
+	.port_force_link_speed	= port_force_link_speed,
+
+	.port_r_cnt		= port_r_cnt,
+	.get_mib_counters	= get_sw_mib_counters,
+
+	.sysfs_read		= sysfs_sw_read,
+	.sysfs_read_hw		= sysfs_sw_read_hw,
+	.sysfs_write		= sysfs_sw_write,
+	.sysfs_port_read	= sysfs_port_read,
+	.sysfs_port_read_hw	= sysfs_port_read_hw,
+	.sysfs_port_write	= sysfs_port_write,
+	.sysfs_mac_read		= sysfs_mac_read,
+	.sysfs_mac_write	= sysfs_mac_write,
+	.sysfs_vlan_read	= sysfs_vlan_read,
+	.sysfs_vlan_write	= sysfs_vlan_write,
+	.sysfs_acl_read		= sysfs_acl_read,
+	.sysfs_acl_write	= sysfs_acl_write,
+
+#ifdef CONFIG_KSZ_STP
+	.sysfs_stp_read		= sysfs_stp_read,
+	.sysfs_stp_write	= sysfs_stp_write,
+	.sysfs_stp_port_read	= sysfs_stp_port_read,
+	.sysfs_stp_port_write	= sysfs_stp_port_write,
+#endif
+
+#ifdef CONFIG_KSZ_MRP
+	.sysfs_mrp_read		= sysfs_mrp_read,
+	.sysfs_mrp_write	= sysfs_mrp_write,
+	.sysfs_mrp_port_read	= sysfs_mrp_port_read,
+	.sysfs_mrp_port_write	= sysfs_mrp_port_write,
+#endif
+
+	.cfg_mac		= sw_cfg_mac,
+	.cfg_vlan		= sw_cfg_vlan,
+	.alloc_mac		= sw_alloc_mac,
+	.free_mac		= sw_free_mac,
+	.alloc_vlan		= sw_alloc_vlan,
+	.free_vlan		= sw_free_vlan,
+	.alloc_fid		= sw_alloc_fid,
+	.free_fid		= sw_free_fid,
+
+	.get_br_id		= sw_get_br_id,
+	.from_backup		= sw_from_backup,
+	.to_backup		= sw_to_backup,
+	.from_designated	= sw_from_designated,
+	.to_designated		= sw_to_designated,
+	.tc_detected		= sw_tc_detected,
+	.get_tcDetected		= sw_get_tcDetected,
+
+	.cfg_src_filter		= sw_cfg_src_filter,
+	.flush_table		= sw_flush_dyn_mac_table,
+	.fwd_unk_vid		= sw_fwd_unk_vid,
+
+};
+
+/* -------------------------------------------------------------------------- */
+
+/* debugfs code */
+static int state_show(struct seq_file *seq, void *v)
+{
+	int i;
+	int j;
+	SW_D data[16 / SW_SIZE];
+	struct sw_priv *priv = seq->private;
+	struct ksz_sw *sw = &priv->sw;
+
+	for (i = 0; i < 0x100; i += 16) {
+		seq_printf(seq, SW_SIZE_STR":\t", i);
+		mutex_lock(&priv->lock);
+		for (j = 0; j < 16 / SW_SIZE; j++)
+			data[j] = sw->reg->r8(sw, i + j * SW_SIZE);
+		mutex_unlock(&priv->lock);
+		for (j = 0; j < 16 / SW_SIZE; j++)
+			seq_printf(seq, SW_SIZE_STR" ", data[j]);
+		seq_printf(seq, "\n");
+	}
+	return 0;
+}
+
+static int state_open(struct inode *inode, struct file *file)
+{
+	return single_open(file, state_show, inode->i_private);
+}
+
+static const struct file_operations state_fops = {
+	.owner	= THIS_MODULE,
+	.open	= state_open,
+	.read	= seq_read,
+	.llseek	= seq_lseek,
+	.release = single_release,
+};
+
+/**
+ * create_debugfs - create debugfs directory and files
+ * @priv:	The switch device structure.
+ *
+ * Create the debugfs entries for the specific device.
+ */
+static void create_debugfs(struct sw_priv *priv)
+{
+	struct dentry *root;
+	char root_name[32];
+
+	snprintf(root_name, sizeof(root_name), "%s",
+		 dev_name(priv->dev));
+
+	root = debugfs_create_dir(root_name, NULL);
+	if (IS_ERR(root)) {
+		pr_err("cannot create debugfs root\n");
+		return;
+	}
+
+	priv->debug_root = root;
+	priv->debug_file = debugfs_create_file("state", 0444, root,
+		priv, &state_fops);
+	if (IS_ERR(priv->debug_file))
+		pr_err("cannot create debugfs state file\n");
+}
+
+static void delete_debugfs(struct sw_priv *priv)
+{
+	debugfs_remove(priv->debug_file);
+	debugfs_remove(priv->debug_root);
+}
+
+/* -------------------------------------------------------------------------- */
+
+#define USE_SPEED_LINK
+#define USE_MIB
+#include "ksz_sw_sysfs_8795.c"
+#ifdef CONFIG_KSZ_DLR
+#include "ksz_dlr_sysfs.c"
+#endif
+
+static irqreturn_t sw_interrupt(int irq, void *phy_dat)
+{
+	struct sw_priv *ks = phy_dat;
+
+	ks->sw.intr_using += 1;
+	ks->irq_work.func(&ks->irq_work);
+
+	return IRQ_HANDLED;
+}  /* sw_interrupt */
+
+static void sw_change(struct work_struct *work)
+{
+	struct sw_priv *ks =
+		container_of(work, struct sw_priv, irq_work);
+	struct ksz_sw *sw = &ks->sw;
+	SW_D status;
+
+	mutex_lock(&ks->hwlock);
+	mutex_lock(&ks->lock);
+	sw->intr_using++;
+	status = sw->reg->r8(sw, REG_ACL_INT_STATUS);
+	if (status) {
+		int p;
+
+		sw->reg->w8(sw, REG_ACL_INT_ENABLE, INT_PORT_ALL & ~status);
+		sw->reg->w8(sw, REG_ACL_INT_STATUS, status);
+		sw->reg->w8(sw, REG_ACL_INT_ENABLE, INT_PORT_ALL);
+		for (p = 0; p < sw->mib_port_cnt; p++) {
+			if (!(status & (1 << p)))
+				continue;
+#ifdef CONFIG_KSZ_DLR
+			if (sw->features & DLR_HW)
+				dlr_timeout(&sw->info->dlr, p);
+#endif
+		}
+		ks->intr_working |= 0x40000000;
+	} else
+		ks->intr_working &= ~0x40000000;
+	status = sw->reg->r8(sw, REG_INT_STATUS);
+	status &= ks->intr_mask;
+	if (status & ks->intr_mask) {
+		sw->phy_intr = status;
+		sw->reg->w8(sw, REG_INT_STATUS, status);
+		status &= ~ks->intr_mask;
+		schedule_delayed_work(&ks->link_read, 0);
+		if (ks->intr_working & 0x80000000)
+			ks->intr_working |= 1;
+		ks->intr_working |= 0x80000000;
+	} else
+		ks->intr_working &= ~0x80000000;
+	if (!(ks->intr_working & 0xc0000000))
+		ks->intr_working = 0;
+	sw->intr_using--;
+	mutex_unlock(&ks->lock);
+	if (status) {
+		mutex_lock(&ks->lock);
+		sw->reg->w8(sw, REG_INT_STATUS, status);
+		mutex_unlock(&ks->lock);
+	}
+	mutex_unlock(&ks->hwlock);
+	sw->intr_using = 0;
+}  /* sw_change */
+
+static int sw_start_interrupt(struct sw_priv *ks, const char *name)
+{
+	int err = 0;
+
+	INIT_WORK(&ks->irq_work, sw_change);
+
+	err = request_threaded_irq(ks->irq, NULL, sw_interrupt,
+		ks->intr_mode, name, ks);
+	if (err < 0) {
+		printk(KERN_WARNING "%s: Can't get IRQ %d (PHY)\n",
+			name,
+			ks->irq);
+		ks->irq = 0;
+	}
+
+	return err;
+}  /* sw_start_interrupt */
+
+static void sw_stop_interrupt(struct sw_priv *ks)
+{
+	free_irq(ks->irq, ks);
+	cancel_work_sync(&ks->irq_work);
+}  /* sw_stop_interrupt */
+
+/* -------------------------------------------------------------------------- */
+
+static char *kszsw_phy_driver_names[] = {
+	"Microchip KSZ8795 Switch",
+	"Microchip KSZ8794 Switch",
+	"Microchip KSZ8765 Switch",
+};
+
+static int kszphy_probe(struct phy_device *phydev)
+{
+	struct mii_bus *bus = phydev->mdio.bus;
+	struct sw_priv *sw_priv = bus->priv;
+	struct ksz_sw *sw = &sw_priv->sw;
+	struct ksz_port_info *info;
+	uint p;
+
+	p = phydev->mdio.addr;
+	phydev->priv = &sw->phydata[p];
+	if (p)
+		--p;
+	else
+		p = sw->HOST_PORT;
+	info = get_port_info(sw, p);
+	phydev->interface = info->interface;
+	return 0;
+}
+
+static int kszphy_get_features(struct phy_device *phydev)
+{
+	struct phy_priv *priv = phydev->priv;
+	struct ksz_port *port = priv->port;
+	struct ksz_sw *sw = port->sw;
+	int ret;
+
+	ret = genphy_read_abilities(phydev);
+	if (ret < 0)
+		return ret;
+
+	set_phy_support(port, phydev);
+
+	/* Special for first PHY connected to MAC. */
+	if (phydev->mdio.addr == 0) {
+		struct ksz_port_info *info = get_port_info(sw, sw->HOST_PORT);
+		int speed = info->tx_rate / TX_RATE_UNIT;
+
+		if (speed == 1000)
+			linkmode_set_bit(ETHTOOL_LINK_MODE_1000baseT_Full_BIT,
+					 phydev->supported);
+		linkmode_clear_bit(ETHTOOL_LINK_MODE_Autoneg_BIT,
+				   phydev->supported);
+	}
+	return 0;
+}
+
+static int kszphy_config_init(struct phy_device *phydev)
+{
+	return 0;
+}
+
+static struct phy_driver kszsw_phy_driver = {
+	.phy_id		= PHY_ID_KSZ_SW,
+	.phy_id_mask	= 0x00ffffff,
+	.name		= "Microchip KSZ8795 Switch",
+	.probe		= kszphy_probe,
+	.get_features	= kszphy_get_features,
+	.config_init	= kszphy_config_init,
+	.config_aneg	= genphy_config_aneg,
+	.read_status	= genphy_read_status,
+};
+
+static int ksz_mii_read(struct mii_bus *bus, int phy_id, int regnum)
+{
+	struct sw_priv *ks = bus->priv;
+	struct ksz_sw *sw = &ks->sw;
+	int ret = 0xffff;
+
+	if (phy_id > sw->port_cnt)
+		return 0xffff;
+	if (phy_id && get_log_port(sw, phy_id - 1) > sw->mib_port_cnt)
+		return 0xffff;
+
+	mutex_lock(&ks->lock);
+	ret = 0;
+	if (regnum < 6) {
+		u16 data;
+
+		sw_r_phy(sw, phy_id, regnum, &data);
+		ret = data;
+	}
+	mutex_unlock(&ks->lock);
+	return ret;
+}  /* ksz_mii_read */
+
+static int ksz_mii_write(struct mii_bus *bus, int phy_id, int regnum, u16 val)
+{
+	struct sw_priv *ks = bus->priv;
+	struct ksz_sw *sw = &ks->sw;
+
+	if (phy_id > sw->port_cnt)
+		return -EINVAL;
+
+	/* Zero is used for the whole switch. */
+	if ((sw->multi_dev & 1) && phy_id == 0)
+		return 0;
+
+	mutex_lock(&ks->lock);
+	if (regnum < 6) {
+		uint i;
+		uint p;
+		int first;
+		int last;
+
+		if (0 == phy_id) {
+			first = 1;
+			last = sw->mib_port_cnt;
+		} else {
+			bool found;
+			int n;
+			int f;
+			int l;
+			struct ksz_dev_map *map;
+
+			first = phy_id;
+			last = phy_id;
+			found = false;
+			for (n = 0; n < sw->eth_cnt; n++) {
+				map = &sw->eth_maps[n];
+				f = map->first;
+				l = f + map->cnt - 1;
+				for (i = f; i <= l; i++) {
+					p = get_phy_port(sw, i);
+					if (phy_id == p + 1) {
+						found = true;
+						break;
+					}
+				}
+				if (found) {
+					first = map->first;
+					last = first + map->cnt - 1;
+					break;
+				}
+			}
+dbg_msg(" %d f:%d l:%d\n", phy_id, first, last);
+		}
+
+		/* PHY device driver resets or powers down the PHY. */
+		if (0 == regnum &&
+		    (val & (PHY_RESET | PHY_POWER_DOWN)))
+			goto done;
+		for (i = first; i <= last; i++) {
+			p = get_phy_port(sw, i) + 1;
+			sw_w_phy(sw, p, regnum, val);
+		}
+		if (PHY_REG_CTRL == regnum &&
+		    !(val & PHY_AUTO_NEG_ENABLE))
+			schedule_delayed_work(&ks->link_read, 1);
+	}
+done:
+	mutex_unlock(&ks->lock);
+	return 0;
+}  /* ksz_mii_write */
+
+static void sw_init_phy_priv(struct sw_priv *ks)
+{
+	struct phy_priv *phydata;
+	struct ksz_port *port;
+	struct ksz_sw *sw = &ks->sw;
+	uint n;
+	uint p;
+
+	for (n = 0; n <= sw->port_cnt; n++) {
+		phydata = &sw->phydata[n];
+		port = &ks->ports[n];
+		phydata->port = port;
+		port->sw = sw;
+		port->phydev = &sw->phy_map[n];
+		port->flow_ctrl = PHY_FLOW_CTRL;
+		port->port_cnt = 1;
+		port->mib_port_cnt = 1;
+		p = n;
+		if (!n) {
+			port->port_cnt = sw->mib_port_cnt;
+			port->mib_port_cnt = sw->mib_port_cnt;
+			p = 1;
+		}
+		port->first_port = p;
+		p = get_phy_port(sw, p);
+		port->linked = get_port_info(sw, p);
+dbg_msg(" %s %d=p:%d; f:%d c:%d i:%d\n", __func__, n, p,
+port->first_port, port->port_cnt, port->linked->phy_id);
+		INIT_DELAYED_WORK(&port->link_update, link_update_work);
+		sw->phy_map[n].priv = phydata;
+	}
+}  /* sw_init_phy_priv */
+
+static void sw_init_phydev(struct ksz_sw *sw, struct phy_device *phydev)
+{
+	struct ksz_port_info *info = get_port_info(sw, sw->HOST_PORT);
+
+	phydev->interface = sw->interface;
+	phydev->link = 1;
+	phydev->speed = info->tx_rate / TX_RATE_UNIT;
+	phydev->duplex = (info->duplex == 2);
+	phydev->pause = 1;
+}  /* sw_init_phydev */
+
+static int driver_installed;
+
+static int ksz_mii_init(struct sw_priv *ks)
+{
+	struct platform_device *pdev;
+	struct mii_bus *bus;
+	struct phy_device *phydev;
+	int err;
+	int i;
+
+	pdev = platform_device_register_simple("Switch MII bus", ks->sw.id,
+		NULL, 0);
+	if (!pdev)
+		return -ENOMEM;
+
+	bus = mdiobus_alloc();
+	if (bus == NULL) {
+		err = -ENOMEM;
+		goto mii_init_reg;
+	}
+
+	if (!driver_installed) {
+		kszsw_phy_driver.name =
+			kszsw_phy_driver_names[ks->sw.chip_id];
+		err = phy_driver_register(&kszsw_phy_driver, THIS_MODULE);
+		if (err)
+			goto mii_init_free_mii_bus;
+		driver_installed = true;
+	}
+
+	bus->name = "Switch MII bus",
+	bus->read = ksz_mii_read;
+	bus->write = ksz_mii_write;
+	snprintf(bus->id, MII_BUS_ID_SIZE, "sw.%d", ks->sw.id);
+	bus->parent = &pdev->dev;
+	bus->phy_mask = ~((1 << (ks->sw.port_cnt + 1)) - 1);
+	bus->priv = ks;
+
+	for (i = 0; i < PHY_MAX_ADDR; i++)
+		bus->irq[i] = -1;
+
+	err = mdiobus_register(bus);
+	if (err < 0)
+		goto mii_init_free_mii_bus;
+
+	for (i = 0; i < PHY_MAX_ADDR; i++) {
+		phydev = mdiobus_get_phy(bus, i);
+		if (phydev) {
+			struct phy_priv *priv = phydev->priv;
+
+			priv->state = phydev->state;
+		}
+	}
+
+	ks->bus = bus;
+	ks->pdev = pdev;
+	phydev = mdiobus_get_phy(bus, 0);
+	ks->phydev = phydev;
+	sw_init_phydev(&ks->sw, phydev);
+
+	return 0;
+
+mii_init_free_mii_bus:
+	if (driver_installed) {
+		phy_driver_unregister(&kszsw_phy_driver);
+		driver_installed = false;
+	}
+	mdiobus_free(bus);
+
+mii_init_reg:
+	platform_device_unregister(pdev);
+
+	return err;
+}  /* ksz_mii_init */
+
+static void ksz_mii_exit(struct sw_priv *ks)
+{
+	int i;
+	struct phy_device *phydev;
+	struct platform_device *pdev = ks->pdev;
+	struct mii_bus *bus = ks->bus;
+
+	if (ks->irq > 0) {
+		struct ksz_sw *sw = &ks->sw;
+
+		mutex_lock(&ks->lock);
+		sw->reg->w8(sw, REG_INT_ENABLE, 0);
+		sw->reg->w8(sw, REG_ACL_INT_ENABLE, 0);
+		mutex_unlock(&ks->lock);
+		sw_stop_interrupt(ks);
+	}
+	for (i = 0; i < PHY_MAX_ADDR; i++) {
+		phydev = mdiobus_get_phy(bus, i);
+		if (phydev) {
+			struct ksz_port *port;
+
+			port = &ks->ports[i];
+			cancel_delayed_work_sync(&port->link_update);
+		}
+	}
+	mdiobus_unregister(bus);
+	if (driver_installed) {
+		phy_driver_unregister(&kszsw_phy_driver);
+		driver_installed = false;
+	}
+	mdiobus_free(bus);
+	platform_device_unregister(pdev);
+}  /* ksz_mii_exit */
+
+/* driver bus management functions */
+
+static void determine_rate(struct ksz_sw *sw, struct ksz_port_mib *mib)
+{
+	int j;
+
+	for (j = 0; j < 2; j++) {
+		if (mib->rate[j].last) {
+			unsigned long diff = jiffies - mib->rate[j].last;
+			u64 cnt = mib->counter[MIB_RX_TOTAL + j] -
+				mib->rate[j].last_cnt;
+
+			if (cnt > 1000000 && diff >= HZ) {
+				u64 rate = cnt;
+
+				rate *= 8;
+				diff *= 1000 * 100 / HZ;
+				rate = div_u64_u32(rate, diff);
+				mib->rate[j].last = jiffies;
+				mib->rate[j].last_cnt =
+					mib->counter[MIB_RX_TOTAL + j];
+				if (mib->rate[j].peak < (u32) rate)
+					mib->rate[j].peak = (u32) rate;
+			}
+		} else
+			mib->rate[j].last = jiffies;
+	}
+}  /* determine_rate */
+
+static void ksz8795_mib_read_work(struct work_struct *work)
+{
+	struct sw_priv *hw_priv =
+		container_of(work, struct sw_priv, mib_read);
+	struct ksz_sw *sw = &hw_priv->sw;
+	struct ksz_port_mib *mib;
+	unsigned long interval;
+	uint n;
+	uint p;
+	int cnt = 0;
+
+	/* Find out how many ports are connected. */
+	for (n = 1; n <= sw->mib_port_cnt; n++) {
+		p = get_phy_port(sw, n);
+		if (media_connected == sw->port_state[p].state)
+			++cnt;
+	}
+	cnt++;
+	interval = MIB_READ_INTERVAL * 2 / cnt;
+	if (time_before(sw->next_jiffies, jiffies)) {
+		sw->next_jiffies = jiffies;
+		sw->next_jiffies += interval;
+	}
+	for (n = 0; n <= sw->mib_port_cnt; n++) {
+		p = get_phy_port(sw, n);
+		mib = get_port_mib(sw, p);
+
+		if (mib->cnt_ptr || 1 == hw_priv->counter[p].read) {
+		} else if (time_after_eq(jiffies, hw_priv->counter[p].time)) {
+			hw_priv->counter[p].time = sw->next_jiffies;
+			/* Only read MIB counters when the port is connected. */
+			if (media_connected == sw->port_state[p].state) {
+				hw_priv->counter[p].read = 1;
+				sw->next_jiffies += interval;
+			}
+
+		/* Port is just disconnected. */
+		} else if (sw->port_state[p].link_down) {
+			int j;
+
+			for (j = 0; j < TOTAL_SWITCH_COUNTER_NUM; j++)
+				mib->read_cnt[j] += mib->read_max[j];
+			sw->port_state[p].link_down = 0;
+
+			/* Read counters one last time after link is lost. */
+			hw_priv->counter[p].read = 1;
+		}
+
+		/* Reading MIB counters or requested to read. */
+		if (mib->cnt_ptr || 1 == hw_priv->counter[p].read) {
+
+			/* Need to process interrupt. */
+			if (port_r_cnt(sw, p))
+				return;
+			hw_priv->counter[p].read = 0;
+
+			/* Finish reading counters. */
+			if (0 == mib->cnt_ptr) {
+				hw_priv->counter[p].read = 2;
+				wake_up_interruptible(
+					&hw_priv->counter[p].counter);
+				if (p != sw->HOST_PORT)
+					determine_rate(sw, mib);
+			}
+		}
+	}
+}  /* ksz8795_mib_read_work */
+
+static void copy_port_status(struct ksz_port *src, struct ksz_port *dst)
+{
+	dst->duplex = src->duplex;
+	dst->speed = src->speed;
+	dst->force_link = src->force_link;
+	dst->linked = src->linked;
+}
+
+static void link_read_work(struct work_struct *work)
+{
+	struct delayed_work *dwork = to_delayed_work(work);
+	struct sw_priv *hw_priv =
+		container_of(dwork, struct sw_priv, link_read);
+	struct ksz_sw *sw = &hw_priv->sw;
+	struct ksz_port *port = NULL;
+	struct ksz_port *sw_port = NULL;
+	int i;
+	int s = 1;
+	int dev_cnt = sw->dev_count + sw->dev_offset;
+
+	if (1 == sw->dev_count || 1 == sw->dev_offset)
+		s = 0;
+
+	/* Check main device when child devices are used. */
+	if (sw->dev_offset)
+		sw_port = sw->netport[0];
+	sw->ops->acquire(sw);
+	if (!sw->phy_intr) {
+		sw->phy_intr = sw->reg->r8(sw, REG_INT_STATUS);
+		sw->phy_intr &= ~INT_PME;
+		if (sw->phy_intr)
+			sw->reg->w8(sw, REG_INT_STATUS, sw->phy_intr);
+	}
+	for (i = sw->dev_offset; i < dev_cnt; i++) {
+		struct phy_priv *phydata;
+		int n = i + s;
+
+		port = sw->netport[i];
+		phydata = &sw->phydata[n];
+		if (!port)
+			port = phydata->port;
+		port_get_link_speed(port);
+
+		/* Copy all port information for user access. */
+		if (port != phydata->port)
+			copy_port_status(port, phydata->port);
+	}
+
+	sw->phy_intr = 0;
+	sw->ops->release(sw);
+
+	/* Need to invoke link_update_work before sw_port->linked is updated
+	 * as link_update_work can be called before link_read_work is
+	 * finished if the delay is not long enough.
+	 */
+	for (i = sw->dev_offset; i < dev_cnt; i++) {
+		struct phy_priv *phydata;
+
+		port = sw->netport[i];
+		if (!port) {
+			phydata = &sw->phydata[i];
+			port = phydata->port;
+		}
+		if (port->link_ports)
+			schedule_delayed_work(&port->link_update, 0);
+	}
+}  /* link_read_work */
+
+/*
+ * Hardware monitoring
+ */
+
+static void ksz8795_mib_monitor(struct timer_list *t)
+{
+	struct ksz_timer_info *info = from_timer(info, t, timer);
+	struct sw_priv *hw_priv = info->dev;
+
+	schedule_work(&hw_priv->mib_read);
+
+	ksz_update_timer(&hw_priv->mib_timer_info);
+}  /* ksz8795_mib_monitor */
+
+static void ksz8795_dev_monitor(struct timer_list *t)
+{
+	struct ksz_timer_info *info = from_timer(info, t, timer);
+	struct sw_priv *hw_priv = info->dev;
+	struct phy_device *phydev;
+	struct phy_priv *priv;
+	int i;
+
+	/* For MAC driver that does not add code to device open function. */
+	for (i = 0; i <= TOTAL_PORT_NUM; i++) {
+		phydev = mdiobus_get_phy(hw_priv->bus, i);
+		if (!phydev)
+			continue;
+		priv = phydev->priv;
+		if (priv->state != phydev->state) {
+			priv->state = phydev->state;
+			if (PHY_UP == phydev->state)
+				schedule_delayed_work(&priv->port->link_update,
+					              0);
+		}
+	}
+	if (!(hw_priv->intr_working & 1))
+		schedule_delayed_work(&hw_priv->link_read, 0);
+
+	ksz_update_timer(&hw_priv->monitor_timer_info);
+}  /* ksz8795_dev_monitor */
+
+static int intr_mode;
+static int ports;
+
+static int sw_device_present;
+
+static int ksz_probe(struct sw_priv *ks)
+{
+	struct ksz_sw *sw = &ks->sw;
+	struct ksz_port_info *info;
+	u16 id;
+	u8 id1;
+	u8 id2;
+	u8 sku;
+	int i;
+	uint p;
+	uint mib_port_count;
+	uint pi;
+	uint port_count;
+	int ret;
+
+	if (sw_device_present >= MAX_SW_DEVICES)
+		return -ENODEV;
+
+	ks->intr_mode = intr_mode ? IRQF_TRIGGER_FALLING :
+		IRQF_TRIGGER_LOW;
+	ks->intr_mode |= IRQF_ONESHOT;
+
+	dev_set_drvdata(ks->dev, ks);
+
+	mutex_init(&ks->hwlock);
+	mutex_init(&ks->lock);
+
+	sw = &ks->sw;
+	mutex_init(&sw->lock);
+	mutex_init(&sw->acllock);
+	sw->hwlock = &ks->hwlock;
+	sw->reglock = &ks->lock;
+	sw->dev = ks;
+
+	sw->net_ops = &sw_net_ops;
+	sw->ops = &sw_ops;
+	init_waitqueue_head(&sw->queue);
+
+	/* simple check for a valid chip being connected to the bus */
+	mutex_lock(&ks->lock);
+#ifdef CONFIG_SPI_ATMEL
+	/* Switch may not be accessible the very first time when SPI mode is
+	 * not 0 in newer kernels where Atmel SPI was changed to use standard
+	 * SPI transfer function.
+	 */
+	if (ks->spi_mode & 2)
+		sw->reg->w8(sw, REG_CHIP_ID0, 0);
+#endif
+	id = sw->reg->r16(sw, REG_CHIP_ID0);
+	mutex_unlock(&ks->lock);
+	id1 = id >> 8;
+	id2 = id & SW_CHIP_ID_M;
+	if (id1 != FAMILY_ID ||
+	    (id2 != CHIP_ID_94 && id2 != CHIP_ID_95)) {
+		dev_err(ks->dev, "failed to read device ID(0x%x)\n", id);
+		ret = -ENODEV;
+		goto err_sw;
+	}
+	dev_info(ks->dev, "chip id 0x%04x\n", id);
+
+	sw->info = kzalloc(sizeof(struct ksz_sw_info), GFP_KERNEL);
+	if (!sw->info) {
+		ret = -ENOMEM;
+		goto err_sw;
+	}
+
+	port_count = TOTAL_PORT_NUM;
+	mib_port_count = TOTAL_PORT_NUM;
+
+	sku = KSZ8795_SKU;
+	sw->chip_id = KSZ8795_SW_CHIP;
+	if (CHIP_ID_94 == id2) {
+		sku = KSZ8794_SKU;
+		sw->chip_id = KSZ8794_SW_CHIP;
+		mib_port_count--;
+	}
+	mutex_lock(&ks->lock);
+	for (p = 0; p < SWITCH_PORT_NUM; p++) {
+		SW_D remote;
+
+		port_r(sw, p, P_REMOTE_STATUS, &remote);
+		if (remote & PORT_FIBER_MODE) {
+			sku = KSZ8765_SKU;
+			sw->chip_id = KSZ8765_SW_CHIP;
+			break;
+		}
+	}
+	mutex_unlock(&ks->lock);
+	sw->PORT_INTR_MASK = (1 << port_count) - 1;
+	sw->PORT_MASK = (1 << mib_port_count) - 1;
+
+	sw->id = sw_device_present;
+
+	sw->HOST_PORT = port_count - 1;
+	sw->HOST_MASK = (1 << sw->HOST_PORT);
+
+	sw->dev_count = 1;
+
+	sw->mib_cnt = TOTAL_SWITCH_COUNTER_NUM;
+	sw->mib_port_cnt = mib_port_count;
+	sw->port_cnt = port_count;
+
+#ifdef DEBUG
+	sw->verbose = 1;
+#endif
+#if 0
+	if (authen)
+		sw->overrides |= USE_802_1X_AUTH;
+#endif
+	if (multi_dev < 0)
+		multi_dev = 0;
+	if (stp < 0)
+		stp = 0;
+
+	/* No specific ports are specified. */
+	if (!ports)
+		ports = sw->PORT_MASK;
+dbg_msg("ports: %x\n", ports);
+
+	ports = sw_setup_zone(sw, ports);
+
+	ksz_setup_logical_ports(sw, sku, ports);
+
+	sw->PORT_MASK |= sw->HOST_MASK;
+
+	dbg_msg("%s\n", kszsw_phy_driver_names[ks->sw.chip_id]);
+
+	INIT_DELAYED_WORK(&ks->link_read, link_read_work);
+
+	mutex_lock(&ks->lock);
+	for (pi = 0; pi < SWITCH_PORT_NUM; pi++) {
+		SW_D remote;
+
+		/*
+		 * Initialize to invalid value so that link detection
+		 * is done.
+		 */
+		info = get_port_info(sw, pi);
+		info->link = 0xFF;
+		info->state = media_disconnected;
+		port_r(sw, pi, P_REMOTE_STATUS, &remote);
+		if (remote & PORT_FIBER_MODE)
+			info->fiber = true;
+	}
+	mutex_unlock(&ks->lock);
+	sw->interface = PHY_INTERFACE_MODE_MII;
+	mutex_lock(&ks->lock);
+	for (; pi < sw->port_cnt; pi++) {
+		u8 ctrl;
+		u8 data;
+		int speed;
+		phy_interface_t phy;
+
+		info = get_port_info(sw, pi);
+		ctrl = SW_R(sw, REG_SW_CTRL_4);
+#ifdef USE_10_MBIT_MODE
+		ctrl |= SW_10_MBIT;
+#endif
+#ifdef USE_HALF_DUPLEX
+		ctrl |= SW_HALF_DUPLEX;
+#endif
+
+		data = sw_r8(sw, REG_PORT_5_CTRL_6);
+#ifdef USE_GMII_100_MODE
+		data &= ~PORT_GMII_1GPS_MODE;
+#endif
+#ifdef USE_MII_MODE
+		data &= ~PORT_INTERFACE_TYPE;
+#endif
+#ifdef USE_GMII_MODE
+		data &= ~PORT_INTERFACE_TYPE;
+		data |= PORT_INTERFACE_GMII;
+#endif
+#ifdef USE_RMII_MODE
+		data &= ~PORT_INTERFACE_TYPE;
+		data |= PORT_INTERFACE_RMII;
+#endif
+		switch (data & PORT_INTERFACE_TYPE) {
+		case PORT_INTERFACE_GMII:
+			phy = PHY_INTERFACE_MODE_GMII;
+			speed = 1000;
+			if (data & PORT_GMII_1GPS_MODE)
+				break;
+			fallthrough;
+
+		case PORT_INTERFACE_MII:
+			phy = PHY_INTERFACE_MODE_MII;
+			speed = 100;
+			break;
+		case PORT_INTERFACE_RMII:
+			phy = PHY_INTERFACE_MODE_RMII;
+			speed = 100;
+			break;
+		default:
+			phy = PHY_INTERFACE_MODE_RGMII;
+			if (data & PORT_RGMII_ID_IN_ENABLE)
+				phy = PHY_INTERFACE_MODE_RGMII_RXID;
+			if (data & PORT_RGMII_ID_IN_ENABLE) {
+				if (PHY_INTERFACE_MODE_RGMII_RXID == phy)
+					phy = PHY_INTERFACE_MODE_RGMII_ID;
+				else
+					phy = PHY_INTERFACE_MODE_RGMII_TXID;
+			}
+			speed = 100;
+			if (data & PORT_GMII_1GPS_MODE)
+				speed = 1000;
+			break;
+		}
+		info->interface = phy;
+		if (sw->HOST_PORT == pi)
+			sw->interface = phy;
+		if (info->phy)
+			info->state = media_disconnected;
+		else
+			info->state = media_connected;
+		if (ctrl & SW_10_MBIT)
+			info->tx_rate = 10 * TX_RATE_UNIT;
+		else
+			info->tx_rate = speed * TX_RATE_UNIT;
+		if (ctrl & SW_HALF_DUPLEX)
+			info->duplex = 1;
+		else
+			info->duplex = 2;
+		info->flow_ctrl = 0x33;
+		info->lpa = 0x05e1;
+	}
+	mutex_unlock(&ks->lock);
+
+	sw_init_phy_priv(ks);
+	setup_device_node(sw);
+
+	ret = ksz_mii_init(ks);
+	if (ret)
+		goto err_mii;
+
+	if (ks->bus) {
+		for (i = 0; i <= sw->port_cnt; i++)
+			sw->phy[i] = mdiobus_get_phy(ks->bus, i);
+	} else {
+		for (i = 0; i <= sw->port_cnt; i++)
+			sw->phy[i] = &sw->phy_map[i];
+	}
+
+	sw->multi_dev |= multi_dev;
+	sw->stp |= stp;
+	sw->fast_aging |= fast_aging;
+	if (sw->stp)
+		sw->features |= STP_SUPPORT;
+	if (sw->fast_aging)
+		sw->overrides |= FAST_AGING;
+
+	sw->counter = ks->counter;
+	sw->monitor_timer_info = &ks->monitor_timer_info;
+	sw->link_read = &ks->link_read;
+
+	sw_init_mib(sw);
+
+	for (i = 0; i < TOTAL_PORT_NUM; i++)
+		init_waitqueue_head(&ks->counter[i].counter);
+
+	create_debugfs(ks);
+
+#ifdef CONFIG_KSZ_STP
+	ksz_stp_init(&sw->info->rstp, sw);
+#endif
+#ifdef CONFIG_KSZ_DLR
+	if (sw->features & DLR_HW)
+		ksz_dlr_init(&sw->info->dlr, sw);
+#endif
+	sw->ops->acquire(sw);
+	sw_reset(sw);
+	sw_init(sw);
+	sw_setup(sw);
+	sw_enable(sw);
+	sw->ops->release(sw);
+	sw->ops->init(sw);
+
+#ifndef CONFIG_KSZ_SWITCH_EMBEDDED
+	init_sw_sysfs(sw, &ks->sysfs, ks->dev);
+#ifdef CONFIG_KSZ_DLR
+	if (sw->features & DLR_HW)
+		init_dlr_sysfs(ks->dev);
+#endif
+#endif
+#ifdef KSZSW_REGS_SIZE
+	ret = sysfs_create_bin_file(&ks->dev->kobj,
+		&kszsw_registers_attr);
+#endif
+	sema_init(&ks->proc_sem, 1);
+
+	INIT_WORK(&ks->mib_read, ksz8795_mib_read_work);
+
+	/* 500 ms timeout */
+	ksz_init_timer(&ks->mib_timer_info, 500 * HZ / 1000,
+		ksz8795_mib_monitor, ks);
+	ksz_init_timer(&ks->monitor_timer_info, 100 * HZ / 1000,
+		ksz8795_dev_monitor, ks);
+
+	ksz_start_timer(&ks->mib_timer_info, ks->mib_timer_info.period);
+	if (!(sw->multi_dev & 1) && !sw->stp)
+		ksz_start_timer(&ks->monitor_timer_info,
+			ks->monitor_timer_info.period * 10);
+
+	sw_device_present++;
+
+#ifdef CONFIG_KSZ_MRP
+	if (sw->features & MRP_SUPPORT) {
+		struct mrp_info *mrp = &sw->mrp;
+
+		INIT_DELAYED_WORK(&sw->set_mrp, sw_set_mrp);
+		mrp->ops = &mrp_ops;
+		mrp->ops->init(mrp);
+	}
+#endif
+
+	if (ks->irq <= 0)
+		return 0;
+	ks->intr_mask = INT_PORT_1 | INT_PORT_2 |
+		INT_PORT_3 | INT_PORT_4 | INT_PME;
+	mutex_lock(&ks->lock);
+	sw->reg->w8(sw, REG_INT_ENABLE, 0);
+	sw->reg->w8(sw, REG_ACL_INT_ENABLE, 0);
+	sw->reg->w8(sw, REG_INT_STATUS, ks->intr_mask);
+	sw->reg->w8(sw, REG_ACL_INT_STATUS, INT_PORT_ALL);
+	mutex_unlock(&ks->lock);
+	ret = sw_start_interrupt(ks, dev_name(ks->dev));
+	if (ret < 0)
+		printk(KERN_WARNING "No switch interrupt\n");
+	else {
+		mutex_lock(&ks->lock);
+		sw->reg->w8(sw, REG_INT_ENABLE, ks->intr_mask);
+		sw->reg->w8(sw, REG_ACL_INT_ENABLE, INT_PORT_ALL);
+		mutex_unlock(&ks->lock);
+	}
+
+	return 0;
+
+err_mii:
+	kfree(sw->info);
+
+err_sw:
+	kfree(ks->hw_dev);
+	kfree(ks);
+
+	return ret;
+}
+
+static int ksz_remove(struct sw_priv *ks)
+{
+	struct ksz_sw *sw = &ks->sw;
+
+	ksz_stop_timer(&ks->monitor_timer_info);
+	ksz_stop_timer(&ks->mib_timer_info);
+	flush_work(&ks->mib_read);
+	cancel_delayed_work_sync(&ks->link_read);
+	ksz_mii_exit(ks);
+
+#ifdef KSZSW_REGS_SIZE
+	sysfs_remove_bin_file(&ks->dev->kobj, &kszsw_registers_attr);
+#endif
+
+#ifndef CONFIG_KSZ_SWITCH_EMBEDDED
+#ifdef CONFIG_KSZ_DLR
+	if (sw->features & DLR_HW)
+		exit_dlr_sysfs(ks->dev);
+#endif
+	exit_sw_sysfs(sw, &ks->sysfs, ks->dev);
+#endif
+	sw->ops->exit(sw);
+
+#ifdef CONFIG_KSZ_STP
+	ksz_stp_exit(&sw->info->rstp);
+#endif
+#ifdef CONFIG_KSZ_DLR
+	if (sw->features & DLR_HW)
+		ksz_dlr_exit(&sw->info->dlr);
+#endif
+	delete_debugfs(ks);
+
+	kfree(sw->info);
+	kfree(ks->hw_dev);
+	kfree(ks);
+
+	return 0;
+}
+
+module_param(intr_mode, int, 0);
+MODULE_PARM_DESC(intr_mode,
+	"Configure which interrupt mode to use(0=level low, 1=falling)");
+
+module_param(fast_aging, int, 0);
+module_param(multi_dev, int, 0);
+module_param(stp, int, 0);
+MODULE_PARM_DESC(fast_aging, "Fast aging");
+MODULE_PARM_DESC(multi_dev, "Multiple device interfaces");
+MODULE_PARM_DESC(stp, "STP support");
+
+module_param(ports, int, 0);
+MODULE_PARM_DESC(ports,
+	"Configure number of ports");
+
+module_param(eth1_ports, int, 0);
+module_param(eth2_ports, int, 0);
+module_param(eth3_ports, int, 0);
+module_param(eth4_ports, int, 0);
+MODULE_PARM_DESC(eth1_ports, "Ports to use on device 1.");
+MODULE_PARM_DESC(eth2_ports, "Ports to use on device 2.");
+MODULE_PARM_DESC(eth3_ports, "Ports to use on device 3.");
+MODULE_PARM_DESC(eth4_ports, "Ports to use on device 4.");
+
+module_param(eth1_vlan, int, 0);
+module_param(eth2_vlan, int, 0);
+module_param(eth3_vlan, int, 0);
+module_param(eth4_vlan, int, 0);
+MODULE_PARM_DESC(eth1_vlan, "VLAN to use on device 1.");
+MODULE_PARM_DESC(eth2_vlan, "VLAN to use on device 2.");
+MODULE_PARM_DESC(eth3_vlan, "VLAN to use on device 3.");
+MODULE_PARM_DESC(eth4_vlan, "VLAN to use on device 4.");
+
+module_param(eth1_proto, charp, 0);
+module_param(eth2_proto, charp, 0);
+module_param(eth3_proto, charp, 0);
+module_param(eth4_proto, charp, 0);
+MODULE_PARM_DESC(eth1_proto, "Protocol to use on device 1.");
+MODULE_PARM_DESC(eth2_proto, "Protocol to use on device 2.");
+MODULE_PARM_DESC(eth3_proto, "Protocol to use on device 3.");
+MODULE_PARM_DESC(eth4_proto, "Protocol to use on device 4.");
+
diff -Napur linux-linux4microchip-2022.10-orig/drivers/net/ethernet/micrel/ksz_sw_8795.h linux-linux4microchip-2022.10/drivers/net/ethernet/micrel/ksz_sw_8795.h
--- linux-linux4microchip-2022.10-orig/drivers/net/ethernet/micrel/ksz_sw_8795.h	1969-12-31 16:00:00.000000000 -0800
+++ linux-linux4microchip-2022.10/drivers/net/ethernet/micrel/ksz_sw_8795.h	2023-12-06 23:02:03.514286614 -0800
@@ -0,0 +1,992 @@
+/**
+ * Microchip KSZ8795 switch common header
+ *
+ * Copyright (c) 2015-2023 Microchip Technology Inc.
+ *	Tristram Ha <Tristram.Ha@microchip.com>
+ *
+ * Copyright (c) 2010-2015 Micrel, Inc.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ */
+
+#ifndef KSZ_SW_8795_H
+#define KSZ_SW_8795_H
+
+
+#ifdef CONFIG_PHYLINK
+#include <linux/phylink.h>
+#endif
+
+/* These definitions should be defined before this header file. */
+#ifndef PRIO_QUEUES
+#define PRIO_QUEUES			4
+#endif
+
+#ifndef KS_PRIO_IN_REG
+#define KS_PRIO_IN_REG			4
+#endif
+
+#ifndef TOTAL_PORT_NUM
+#define TOTAL_PORT_NUM			5
+#endif
+
+#ifndef SWITCH_COUNTER_NUM
+#define SWITCH_COUNTER_NUM		0x20
+#endif
+#ifndef TOTAL_SWITCH_COUNTER_NUM
+#define TOTAL_SWITCH_COUNTER_NUM	(SWITCH_COUNTER_NUM + 2)
+#endif
+
+#ifndef SW_D
+#error "SW_D and other data bus parameters need to be defined."
+#endif
+
+/* Host port can only be last of them. */
+#define SWITCH_PORT_NUM			(TOTAL_PORT_NUM - 1)
+
+#define MAX_SW_DEVICES			2
+
+
+#include "ksz_sw_api.h"
+#ifdef CONFIG_KSZ_STP
+#include "ksz_stp.h"
+#endif
+#ifdef CONFIG_KSZ_MRP
+#include "ksz_mrp.h"
+#endif
+#ifdef CONFIG_KSZ_DLR
+#include "ksz_dlr.h"
+#endif
+
+
+#define LEARNED_MAC_TABLE_ENTRIES	1024
+#define STATIC_MAC_TABLE_ENTRIES	32
+#define SWITCH_MAC_TABLE_ENTRIES	32
+#define MULTI_MAC_TABLE_ENTRIES		56
+
+#define RX_TABLE_ENTRIES		128
+#define TX_TABLE_ENTRIES		8
+
+
+/**
+ * struct ksz_mac_table - Static MAC table data structure
+ * @mac_addr:	MAC address to filter.
+ * @vid:	VID value.
+ * @fid:	FID value.
+ * @ports:	Port membership.
+ * @override:	Override setting.
+ * @use_fid:	FID use setting.
+ * @valid:	Valid setting indicating the entry is being used.
+ */
+struct ksz_mac_table {
+	u8 addr[ETH_ALEN];
+	u8 fid;
+	u8 ports;
+	u8 override:1;
+	u8 use_fid:1;
+	u8 valid:1;
+	u8 dirty:1;
+};
+
+#define FWD_HOST_OVERRIDE		(1 << 0)
+#define FWD_HOST			(1 << 1)
+#define FWD_STP_DEV			(1 << 2)
+#define FWD_MAIN_DEV			(1 << 3)
+#define FWD_VLAN_DEV			(1 << 4)
+
+struct ksz_alu_table {
+	u8 owner;
+	u8 forward;
+	u8 valid:1;
+};
+
+#define VLAN_TABLE_ENTRIES		(4096 / 4)
+#define FID_ENTRIES			128
+#define FID_IN_DATA			32
+
+/**
+ * struct ksz_vlan_table - VLAN table data structure
+ * @vid:	VID value.
+ * @fid:	FID value.
+ * @member:	Port membership.
+ * @valid:	Valid setting indicating the entry is being used.
+ */
+struct ksz_vlan_table {
+	u16 vid;
+	u8 fid;
+	u8 member;
+	u8 valid:1;
+	u8 dirty:1;
+};
+
+#define PRIO_802_1P_ENTRIES		8
+
+#define DIFFSERV_ENTRIES		64
+
+#define ACL_TABLE_ENTRIES		16
+
+struct ksz_acl_table {
+	u16 first_rule;
+	u16 ruleset;
+	u8 mac[ETH_ALEN];
+	u16 eth_type;
+	u8 protocol;
+	u8 ip4_addr[4];
+	u8 ip4_mask[4];
+	u32 seqnum;
+	u16 max_port;
+	u16 min_port;
+	u8 prio;
+	u8 vlan_prio;
+	u16 ports;
+	u16 cnt;
+	u8 tcp_flag_mask;
+	u8 tcp_flag;
+	u32 mode:2;
+	u32 enable:2;
+	u32 src:1;
+	u32 equal:1;
+	u32 port_mode:2;
+	u32 tcp_flag_enable:1;
+	u32 msec:1;
+	u32 intr_mode:1;
+	u32 prio_mode:2;
+	u32 vlan_prio_replace:1;
+	u32 map_mode:2;
+	u32 changed:1;
+	u32 action_changed:1;
+	u32 ruleset_changed:1;
+	u32 action_selected:1;
+
+	u8 data[ACL_TABLE_LEN];
+};
+
+/**
+ * struct ksz_port_mib - Port MIB data structure
+ * @cnt_ptr:	Current pointer to MIB counter index.
+ * @mib_start:	The starting counter index.  Some ports do not start at 0.
+ * @counter:	64-bit MIB counter value.
+ * @dropped:	Temporary buffer to remember last read packet dropped values.
+ * @read_cnt:	Used to signal when to read the MIB counter.
+ * @read_max:	Used to indicate how often to read the MIB counter.
+ *
+ * MIB counters needs to be read periodically so that counters do not get
+ * overflowed and give incorrect values.  A right balance is needed to
+ * satisfy this condition and not waste too much CPU time.
+ */
+struct ksz_port_mib {
+	u8 cnt_ptr;
+	u8 mib_start;
+	u8 reserved[2];
+
+	u64 counter[TOTAL_SWITCH_COUNTER_NUM];
+	u32 dropped[2];
+	u8 read_cnt[TOTAL_SWITCH_COUNTER_NUM];
+	u8 read_max[TOTAL_SWITCH_COUNTER_NUM];
+	struct {
+		unsigned long last;
+		u64 last_cnt;
+		u32 peak;
+	} rate[2];
+};
+
+enum {
+	STP_STATE_DISABLED = 0,
+	STP_STATE_LISTENING,
+	STP_STATE_LEARNING,
+	STP_STATE_FORWARDING,
+	STP_STATE_BLOCKED,
+	STP_STATE_SIMPLE
+};
+
+/**
+ * struct ksz_port_cfg - Port configuration data structure
+ * @vid:	VID value.
+ * @member:	Port membership.
+ * @port_prio:	Port priority.
+ * @rate_ctrl:	Priority rate control.
+ * @rx_rate:	Receive priority rate.
+ * @tx_rate:	Transmit priority rate.
+ * @rate_limit: Priority rate limit value.
+ * @vid_member:	VLAN membership.
+ * @index:	Net device pointer.
+ * @packet_based: Packet based indication.
+ * @stp_state:	Current Spanning Tree Protocol state.
+ */
+struct ksz_port_cfg {
+	u16 vid;
+	u8 member;
+	u8 port_prio;
+	u8 rate_ctrl[PRIO_QUEUES];
+	u32 rx_packet[PRIO_QUEUES];
+	u32 rx_rate[PRIO_QUEUES];
+	u32 tx_packet[PRIO_QUEUES];
+	u32 tx_rate[PRIO_QUEUES];
+	u8 rate_limit;
+	u8 vid_member;
+	int index;
+	int packet_based;
+	int stp_state;
+
+	struct ksz_acl_table acl_info[ACL_TABLE_ENTRIES];
+	u16 acl_index;
+	u16 acl_act_index;
+	u16 acl_rule_index;
+	u16 acl_byte_enable;
+};
+
+/**
+ * struct ksz_sw_info - KSZ8795 switch information data structure
+ * @mac_table:	MAC table entries information.
+ * @multi_net:	Network multicast addresses used.
+ * @multi_sys:	System multicast addresses used.
+ * @blocked_rx:	Blocked receive addresses.
+ * @blocked_rx_cnt: Blocked receive addresses count.
+ * @vlan_table:	VLAN table entries information.
+ * @port_cfg:	Port configuration information.
+ * @rx_table:	Receive frame information.
+ * @tx_table:	Transmit frame information.
+ * @diffserv:	DiffServ priority settings.  Possible values from 6-bit of ToS
+ *		(bit7 ~ bit2) field.
+ * @p_802_1p:	802.1P priority settings.  Possible values from 3-bit of 802.1p
+ *		Tag priority field.
+ * @br_addr:	Bridge address.  Used for STP.
+ * @mac_addr:	Switch MAC address.
+ * @broad_per:	Broadcast storm percentage.
+ * @member:	Current port membership.  Used for STP.
+ * @phy_addr:	PHY address used by first port.
+ */
+struct ksz_sw_info {
+	struct ksz_mac_table mac_table[MULTI_MAC_TABLE_ENTRIES];
+	struct ksz_alu_table alu_table[MULTI_MAC_TABLE_ENTRIES];
+	u32 mac_table_used;
+	int multi_net;
+	int multi_sys;
+	struct ksz_port_cfg port_cfg[TOTAL_PORT_NUM];
+#ifdef CONFIG_KSZ_STP
+	struct ksz_stp_info rstp;
+#endif
+#ifdef CONFIG_KSZ_DLR
+	struct ksz_dlr_info dlr;
+#endif
+	struct ksz_mac_table mac_entry;
+	struct ksz_vlan_table vlan_entry;
+
+	SW_D diffserv[DIFFSERV_ENTRIES / KS_PRIO_IN_REG];
+	SW_D p_802_1p[PRIO_802_1P_ENTRIES / KS_PRIO_IN_REG];
+
+	u8 br_addr[ETH_ALEN];
+	u8 mac_addr[ETH_ALEN];
+
+	u32 fid[FID_ENTRIES / FID_IN_DATA];
+	u16 fid_cnt;
+
+	u8 broad_per;
+	u8 member[1];
+	u8 phy_addr;
+};
+
+/**
+ * struct ksz_port_state - Port state information data structure
+ * @state:	Connection status of the port.
+ * @link_down:	Indication the link has just gone down.
+ *
+ * It is pointless to read MIB counters when the port is disconnected.  The
+ * @state provides the connection status so that MIB counters are read only
+ * when the port is connected.  The @link_down indicates the port is just
+ * disconnected so that all MIB counters are read one last time to update the
+ * information.
+ */
+struct ksz_port_state {
+	uint state;
+	u8 link_down;
+};
+
+#define TX_RATE_UNIT			10000
+
+/**
+ * struct ksz_port_info - Port information data structure
+ * @interface:	PHY interface.
+ * @state:	Connection status of the port.
+ * @tx_rate:	Transmit rate divided by 10000 to get Mbit.
+ * @duplex:	Duplex mode.
+ * @flow_ctrl:	Flow control.
+ * @advertised:	Advertised auto-negotiation setting.  Used to determine link.
+ * @partner:	Auto-negotiation partner setting.  Used to determine link.
+ * @link:	Link status.  Used to determine link.
+ * @status:	LinkMD status values.
+ * @length:	LinkMD length values.
+ * @mac_addr:	MAC address of the port.
+ * @phy_id:	PHY id used by the port.
+ */
+struct ksz_port_info {
+	phy_interface_t interface;
+	uint state;
+	uint tx_rate;
+	u8 duplex;
+	u8 flow_ctrl;
+	u8 advertised;
+	u8 partner;
+	u8 link;
+	u16 lpa;
+	u32 status[3];
+	u32 length[3];
+	u8 mac_addr[ETH_ALEN];
+	u8 own_flow_ctrl;
+	u8 own_duplex;
+	u16 own_speed;
+	u8 phy_id;
+	u32 phy:1;
+	u32 fiber:1;
+
+	u8 phy_p;
+	u8 log_p;
+	u16 phy_m;
+	u16 log_m;
+};
+
+struct ksz_sw;
+struct ksz_port;
+
+struct ksz_sw_reg_ops {
+	void (*lock)(struct ksz_sw *sw);
+	void (*unlock)(struct ksz_sw *sw);
+
+	u8 (*r8)(struct ksz_sw *sw, unsigned reg);
+	u16 (*r16)(struct ksz_sw *sw, unsigned reg);
+	u32 (*r32)(struct ksz_sw *sw, unsigned reg);
+	void (*w8)(struct ksz_sw *sw, unsigned reg, unsigned val);
+	void (*w16)(struct ksz_sw *sw, unsigned reg, unsigned val);
+	void (*w32)(struct ksz_sw *sw, unsigned reg, unsigned val);
+
+	void (*r)(struct ksz_sw *sw, unsigned reg, void *buf, size_t cnt);
+	void (*w)(struct ksz_sw *sw, unsigned reg, void *buf, size_t cnt);
+
+	int (*get)(struct ksz_sw *sw, u32 reg, size_t count, char *buf);
+	int (*set)(struct ksz_sw *sw, u32 reg, size_t count, char *buf);
+};
+
+struct ksz_sw_net_ops {
+	void (*setup_special)(struct ksz_sw *sw, int *port_cnt,
+		int *mib_port_cnt, int *dev_cnt,
+		const void *ops);
+	void (*setup_mdiobus)(struct ksz_sw *sw, void *bus);
+	int (*setup_dev)(struct ksz_sw *sw, struct net_device *dev,
+		char *dev_name, struct ksz_port *port, int i, uint port_cnt,
+		uint mib_port_cnt);
+	void (*leave_dev)(struct ksz_sw *sw);
+
+	void (*start)(struct ksz_sw *sw, u8 *addr);
+	int (*stop)(struct ksz_sw *sw, int complete);
+	int (*open_dev)(struct ksz_sw *sw, struct net_device *dev,
+		struct ksz_port *port, u8 *addr);
+	void (*open_port)(struct ksz_sw *sw, struct net_device *dev,
+		struct ksz_port *port);
+	void (*close_port)(struct ksz_sw *sw, struct net_device *dev,
+		struct ksz_port *port);
+	void (*open)(struct ksz_sw *sw);
+	void (*close)(struct ksz_sw *sw);
+
+	void (*netdev_start_queue)(struct ksz_sw *sw, struct net_device *dev);
+	void (*netdev_stop_queue)(struct ksz_sw *sw, struct net_device *dev);
+	void (*netdev_wake_queue)(struct ksz_sw *sw, struct net_device *dev);
+	void (*netdev_open_port)(struct ksz_sw *sw, struct net_device *dev);
+
+	u8 (*set_mac_addr)(struct ksz_sw *sw, struct net_device *dev,
+		u8 promiscuous, uint port);
+
+	int (*get_mtu)(struct ksz_sw *sw);
+	int (*get_tx_len)(struct ksz_sw *sw, struct sk_buff *skb, uint port,
+		int *header);
+	void (*add_tail_tag)(struct ksz_sw *sw, struct sk_buff *skb, uint dst);
+	int (*get_tail_tag)(u8 *trailer, int *port);
+	void (*add_vid)(struct ksz_sw *sw, u16 vid);
+	void (*kill_vid)(struct ksz_sw *sw, u16 vid);
+	struct sk_buff *(*check_tx)(struct ksz_sw *sw, struct net_device *dev,
+		struct sk_buff *skb, struct ksz_port *priv);
+	struct net_device *(*rx_dev)(struct ksz_sw *sw, u8 *data, u32 *len,
+		int *tag, int *port);
+	int (*match_pkt)(struct ksz_sw *sw, struct net_device **dev,
+		void **priv, int (*get_promiscuous)(void *ptr),
+		int (*match_multi)(void *ptr, u8 *data),
+		struct sk_buff *skb, u8 h_promiscuous);
+	struct net_device *(*parent_rx)(struct ksz_sw *sw,
+		struct net_device *dev, int *forward);
+	int (*port_vlan_rx)(struct sk_buff *skb, int forward, int tag);
+	struct sk_buff *(*final_skb)(struct ksz_sw *sw, struct sk_buff *skb,
+		struct net_device *dev, struct ksz_port *port);
+	int (*drv_rx)(struct ksz_sw *sw, struct sk_buff *skb, uint port);
+	void (*set_multi)(struct ksz_sw *sw, struct net_device *dev,
+		struct ksz_port *priv);
+};
+
+struct ksz_sw_ops {
+	void (*init)(struct ksz_sw *sw);
+	void (*exit)(struct ksz_sw *sw);
+	int (*dev_req)(struct ksz_sw *sw, char *arg,
+		struct file_dev_info *info);
+
+	uint (*get_phy_port)(struct ksz_sw *sw, uint n);
+	uint (*get_log_port)(struct ksz_sw *sw, uint p);
+
+	void (*acquire)(struct ksz_sw *sw);
+	void (*release)(struct ksz_sw *sw);
+
+	int (*chk)(struct ksz_sw *sw, u32 addr, SW_D bits);
+	void (*cfg)(struct ksz_sw *sw, u32 addr, SW_D bits, bool set);
+
+	int (*port_get_link_speed)(struct ksz_port *port);
+	void (*port_set_link_speed)(struct ksz_port *port);
+	void (*port_force_link_speed)(struct ksz_port *port);
+
+	int (*port_r_cnt)(struct ksz_sw *sw, uint port);
+	void (*get_mib_counters)(struct ksz_sw *sw, int first, int cnt,
+		u64 *counter);
+
+	ssize_t (*sysfs_read)(struct ksz_sw *sw, int proc_num,
+		struct ksz_port *port, ssize_t len, char *buf);
+	ssize_t (*sysfs_read_hw)(struct ksz_sw *sw, int proc_num, ssize_t len,
+		char *buf);
+	int (*sysfs_write)(struct ksz_sw *sw, int proc_num,
+		struct ksz_port *port, int num, const char *buf);
+	ssize_t (*sysfs_port_read)(struct ksz_sw *sw, int proc_num, uint port,
+		ssize_t len, char *buf);
+	ssize_t (*sysfs_port_read_hw)(struct ksz_sw *sw, int proc_num,
+		uint port, ssize_t len, char *buf);
+	int (*sysfs_port_write)(struct ksz_sw *sw, int proc_num, uint port,
+		int num, const char *buf);
+	ssize_t (*sysfs_mac_read)(struct ksz_sw *sw, int proc_num, ssize_t len,
+		char *buf);
+	int (*sysfs_mac_write)(struct ksz_sw *sw, int proc_num, int num,
+		const char *buf);
+	ssize_t (*sysfs_vlan_read)(struct ksz_sw *sw, int proc_num,
+		ssize_t len, char *buf);
+	int (*sysfs_vlan_write)(struct ksz_sw *sw, int proc_num, int num);
+	ssize_t (*sysfs_acl_read)(struct ksz_sw *sw, int proc_num, uint port,
+		ssize_t len, char *buf);
+	int (*sysfs_acl_write)(struct ksz_sw *sw, int proc_num, uint port,
+		int num, const char *buf);
+
+#ifdef CONFIG_KSZ_STP
+	ssize_t (*sysfs_stp_read)(struct ksz_sw *sw, int proc_num, ssize_t len,
+		char *buf);
+	int (*sysfs_stp_write)(struct ksz_sw *sw, int proc_num, int num,
+		const char *buf);
+	ssize_t (*sysfs_stp_port_read)(struct ksz_sw *sw, int proc_num,
+		uint port, ssize_t len, char *buf);
+	int (*sysfs_stp_port_write)(struct ksz_sw *sw, int proc_num, uint port,
+		int num, const char *buf);
+#endif
+
+#ifdef CONFIG_KSZ_MRP
+	ssize_t (*sysfs_mrp_read)(struct ksz_sw *sw, int proc_num, ssize_t len,
+		char *buf);
+	int (*sysfs_mrp_write)(struct ksz_sw *sw, int proc_num, int num,
+		const char *buf);
+	ssize_t (*sysfs_mrp_port_read)(struct ksz_sw *sw, int proc_num,
+		uint port, ssize_t len, char *buf);
+	int (*sysfs_mrp_port_write)(struct ksz_sw *sw, int proc_num, uint port,
+		int num, const char *buf);
+#endif
+
+	void (*cfg_mac)(struct ksz_sw *sw, u8 index, u8 *dest, u32 ports,
+		int override, int use_fid, u16 fid);
+	void (*cfg_vlan)(struct ksz_sw *sw, u8 index, u16 vid, u16 fid,
+		u32 ports);
+	u8 (*alloc_mac)(struct ksz_sw *sw);
+	void (*free_mac)(struct ksz_sw *sw, u8 index);
+	u8 (*alloc_vlan)(struct ksz_sw *sw);
+	void (*free_vlan)(struct ksz_sw *sw, u8 index);
+	u16 (*alloc_fid)(struct ksz_sw *sw, u16 vid);
+	void (*free_fid)(struct ksz_sw *sw, u16 fid);
+
+	const u8 *(*get_br_id)(struct ksz_sw *sw);
+	void (*from_backup)(struct ksz_sw *sw, uint p);
+	void (*to_backup)(struct ksz_sw *sw, uint p);
+	void (*from_designated)(struct ksz_sw *sw, uint p, bool alt);
+	void (*to_designated)(struct ksz_sw *sw, uint p);
+	void (*tc_detected)(struct ksz_sw *sw, uint p);
+	int (*get_tcDetected)(struct ksz_sw *sw, uint p);
+
+	void (*cfg_src_filter)(struct ksz_sw *sw, bool set);
+	void (*flush_table)(struct ksz_sw *sw, uint port);
+	void (*fwd_unk_vid)(struct ksz_sw *sw);
+};
+
+struct ksz_dev_map {
+	u8 cnt;
+	u8 mask;
+	u8 first;
+	u8 phy_id;
+	u16 vlan;
+	uint proto;
+};
+
+struct phy_priv {
+	struct ksz_port *port;
+	enum phy_state state;
+};
+
+/* Switch features and bug fixes. */
+#define STP_SUPPORT			(1 << 0)
+#define VLAN_PORT			(1 << 1)
+#define VLAN_PORT_REMOVE_TAG		(1 << 2)
+#define VLAN_PORT_TAGGING		(1 << 3)
+#define VLAN_PORT_START			200
+#define SW_VLAN_DEV			(1 << 4)
+#define MRP_SUPPORT			(1 << 5)
+
+#define USE_FEWER_PORTS			(1 << 18)
+#define DLR_HW				(1 << 24)
+#define HSR_HW				(1 << 25)
+
+#define DSA_SUPPORT			(1 << 28)
+#define DIFF_MAC_ADDR			(1 << 30)
+
+/* Software overrides. */
+#define PAUSE_FLOW_CTRL			(1 << 0)
+#define FAST_AGING			(1 << 1)
+#define UPDATE_CSUM			(1 << 2)
+#define HAVE_MORE_THAN_2_PORTS		(1 << 3)
+
+#define ACL_INTR_MONITOR		(1 << 17)
+#define SYSFS_PHY_PORT			(1 << 18)
+
+#define TAIL_PRP_0			(1 << 24)
+#define TAIL_PRP_1			(1 << 25)
+
+#define TAG_REMOVE			(1 << 30)
+#define TAIL_TAGGING			(1 << 31)
+
+#define TAIL_TAG_SET_OVERRIDE		BIT(31)
+#define TAIL_TAG_SET_QUEUE		BIT(30)
+
+/**
+ * struct ksz_sw - Virtual switch data structure
+ * @dev:		Pointer to hardware device.
+ * @phydev:		Pointer to PHY device interface.
+ * @interface:		The R/G/MII interface used.
+ * @msg_enable:		The message flags controlling driver output.
+ * @hwlock:		Pointer to hardware lock.
+ * @reglock:		Pointer to register lock.
+ * @acllock:		ACL table lock.
+ * @lock		Software lock to switch structure.
+ * @locked:		locked status.
+ * @info:		Pointer to switch information structure.
+ * @port_info:		Port information.
+ * @netdev:		Pointer to OS dependent network devices.
+ * @phy:		Pointer to OS dependent PHY devices.
+ * @dev_offset:		Indication of a switch associated network device.
+ * @phy_offset:		Indication of a port associated PHY device.
+ * @port_state:		Port state information.
+ * @port_mib:		Port MIB information.
+ * @mib_cnt:		Number of MIB counters this switch has.
+ * @mib_port_cnt:	Number of ports with MIB counters.
+ * @port_cnt:		Number of ports to support.
+ * @monitor_timer_info:	Timer information for monitoring ports.
+ * @counter:		Pointer to OS dependent MIB counter information.
+ * @link_read:		Workqueue for link monitoring.
+ * @ops:		Switch function access.
+ * @reg:		Switch register access.
+ * @net_ops:		Network related switch function access.
+ * @HOST_PORT:		A predefined value indicating the host port.
+ * @HOST_MASK:		A predefined value indicating the host port mask.
+ * @PORT_MASK:		A predefined value indicating the port mask.
+ * @rx_ports:		Bitmap of ports with receive enabled.
+ * @tx_ports:		Bitmap of ports with transmit enabled.
+ * @dev_count:		Number of network devices this switch supports.
+ * @id:			Hardware ID.  Used for display only.
+ * @vlan_id		Used for the VLAN port forwarding feature.
+ * @vid:		Used for the VLAN port forwarding feature.
+ * @features:		Switch features to enable.
+ * @overrides:		Switch features to override.
+ * @multi_dev:		Used to specify multiple devices mode.
+ * @stp:		Used to enable STP.
+ * @fast_aging:		Used to enable fast aging.
+ */
+struct ksz_sw {
+	void *dev;
+	phy_interface_t interface;
+	u32 msg_enable;
+	wait_queue_head_t queue;
+	struct mutex *hwlock;
+	struct mutex *reglock;
+	struct mutex acllock;
+	struct mutex lock;
+	int intr_using;
+
+	struct ksz_sw_info *info;
+	struct ksz_port_info port_info[TOTAL_PORT_NUM];
+	struct net_device *main_dev;
+	struct ksz_port *main_port;
+	struct net_device *netdev[TOTAL_PORT_NUM];
+	struct ksz_port *netport[TOTAL_PORT_NUM];
+	struct device_node *devnode[TOTAL_PORT_NUM];
+	struct phy_device phy_map[TOTAL_PORT_NUM + 1];
+	struct phy_device *phy[TOTAL_PORT_NUM + 1];
+	struct phy_priv phydata[TOTAL_PORT_NUM + 1];
+	int dev_offset;
+	int phy_offset;
+	struct ksz_port_state port_state[TOTAL_PORT_NUM];
+	struct ksz_port_mib port_mib[TOTAL_PORT_NUM];
+	unsigned long next_jiffies;
+	int mib_cnt;
+	int mib_port_cnt;
+	int dsa_port_cnt;
+	int port_cnt;
+	struct ksz_timer_info *monitor_timer_info;
+	struct ksz_counter_info *counter;
+	struct delayed_work *link_read;
+
+#ifdef CONFIG_PHYLINK
+	const struct phylink_mac_ops *phylink_ops;
+#endif
+
+	const struct ksz_sw_ops *ops;
+	const struct ksz_sw_reg_ops *reg;
+	struct ksz_sw_net_ops *net_ops;
+
+	int HOST_PORT;
+	u16 HOST_MASK;
+	u16 PORT_MASK;
+	u16 PORT_INTR_MASK;
+	u8 phy_intr;
+	u16 dev_ports;
+	u16 rx_ports;
+	u16 tx_ports;
+	u8 tx_pad[60];
+	int tx_start;
+
+	int dev_major;
+	u8 *msg_buf;
+	struct file_dev_info *dev_list[2];
+	uint notifications;
+	char dev_name[20];
+
+	int chip_id;
+	int dev_count;
+	int id;
+	u32 vlan_id;
+	u16 vid;
+	u16 mac_index;
+	u16 vlan_index;
+	u8 mac_dirty;
+	u8 vlan_dirty;
+	u8 verbose;
+
+	uint features;
+	uint overrides;
+
+	int multi_dev;
+	int stp;
+	int fast_aging;
+	struct ksz_dev_map eth_maps[SWITCH_PORT_NUM];
+	int eth_cnt;
+
+#ifdef CONFIG_KSZ_MRP
+	struct delayed_work set_mrp;
+	struct mrp_info mrp;
+#endif
+};
+
+struct ksz_sw_sysfs {
+	struct ksz_dev_attr *ksz_port_attrs[TOTAL_PORT_NUM];
+	struct attribute **port_attrs[TOTAL_PORT_NUM];
+};
+
+/**
+ * struct ksz_port - Virtual port data structure
+ * @first_port:		Index of first port this port supports.
+ * @mib_port_cnt:	Number of ports with MIB counters.
+ * @port_cnt:		Number of ports this port supports.
+ * @flow_ctrl:		Flow control setting.  PHY_NO_FLOW_CTRL for no flow
+ *			control, and PHY_FLOW_CTRL for flow control.
+ *			PHY_TX_ONLY and PHY_RX_ONLY are not supported for 100
+ *			Mbit PHY.
+ * @duplex:		Duplex mode setting.  1 for half duplex, 2 for full
+ *			duplex, and 0 for auto, which normally results in full
+ *			duplex.
+ * @speed:		Speed setting.  10 for 10 Mbit, 100 for 100 Mbit, and
+ *			0 for auto, which normally results in 100 Mbit.
+ * @force_link:		Force link setting.  0 for auto-negotiation, and 1 for
+ *			force.
+ * @linked:		Pointer to port information linked to this port.
+ * @sw:			Pointer to virtual switch structure.
+ */
+struct ksz_port {
+	int first_port;
+	int mib_port_cnt;
+	int port_cnt;
+
+	u8 flow_ctrl;
+	u8 duplex;
+	u8 speed;
+	u8 force_link;
+	u8 state;
+	uint opened:1;
+	uint ready:1;
+	uint report:1;
+	u16 link_ports;
+
+	struct ksz_port_info *linked;
+
+	struct ksz_sw *sw;
+
+	struct delayed_work link_update;
+	struct net_device *netdev;
+	struct phy_device *phydev;
+	struct device_node *dn;
+#ifdef CONFIG_PHYLINK
+	struct phylink *pl;
+	struct phylink_config pl_config;
+	struct phylink_link_state pl_state;
+#endif
+};
+
+static inline void sw_update_csum(struct ksz_sw *sw)
+{
+	sw->overrides |= UPDATE_CSUM;
+}
+
+#ifdef CONFIG_KSZ_HSR
+static inline bool using_hsr(struct ksz_sw *sw)
+{
+	return (sw->features & HSR_HW);
+}
+#endif
+
+static inline bool using_tail_tag(struct ksz_sw *sw)
+{
+	return (sw->overrides & TAIL_TAGGING);
+}
+
+struct lan_attributes {
+	int info;
+	int version;
+	int duplex;
+	int speed;
+	int force;
+	int flow_ctrl;
+	int features;
+	int overrides;
+	int mib;
+	int reg;
+	int vid;
+	int dynamic_table;
+	int static_table;
+	int vlan_table;
+	int aging;
+	int fast_aging;
+	int link_aging;
+	int bcast_per;
+	int mcast_storm;
+	int tx_queue_based;
+	int diffserv_map;
+	int p_802_1p_map;
+	int vlan;
+	int null_vid;
+	int macaddr;
+	int mirror_mode;
+	int tail_tag;
+	int igmp_snoop;
+	int ipv6_mld_snoop;
+	int ipv6_mld_option;
+	int aggr_backoff;
+	int no_exc_drop;
+	int huge_packet;
+	int legal_packet;
+	int length_check;
+	int back_pressure;
+	int sw_flow_ctrl;
+	int sw_half_duplex;
+	int sw_10_mbit;
+	int rx_flow_ctrl;
+	int tx_flow_ctrl;
+	int fair_flow_ctrl;
+	int vlan_bound;
+
+	int fw_unk_ucast_dest;
+	int fw_unk_ucast_ports;
+	int fw_unk_mcast_dest;
+	int fw_unk_mcast_ports;
+	int fw_inv_vid;
+	int fw_inv_vid_ports;
+	int fw_unk_ip_mcast_dest;
+	int fw_unk_ip_mcast_ports;
+	int self_filter;
+	int ins_tag;
+
+	int pme;
+	int pass_pause;
+	int hi_prio_queues;
+
+	int ports;
+	int dev_start;
+	int vlan_start;
+	int stp;
+
+	int mac_fid;
+	int mac_use_fid;
+	int mac_override;
+	int mac_valid;
+	int mac_ports;
+	int mac_addr;
+	int mac_index;
+	int mac_info;
+
+	int vlan_valid;
+	int vlan_ports;
+	int vlan_fid;
+	int vlan_index;
+	int vlan_info;
+
+#ifdef CONFIG_KSZ_STP
+	int stp_br_info;
+	int stp_br_on;
+	int stp_br_prio;
+	int stp_br_fwd_delay;
+	int stp_br_max_age;
+	int stp_br_hello_time;
+	int stp_br_tx_hold;
+	int stp_version;
+#endif
+};
+
+struct sw_attributes {
+	int mib;
+	int vid;
+	int member;
+	int bcast_storm;
+	int diffserv;
+	int p_802_1p;
+	int port_based;
+	int non_vid;
+	int ingress;
+	int ins_tag;
+	int rmv_tag;
+	int drop_tagged;
+	int replace_prio;
+	int rx;
+	int tx;
+	int learn;
+	int ins_tag_0;
+	int ins_tag_1;
+	int ins_tag_2;
+	int ins_tag_3;
+	int ins_tag_4;
+	int pass_all;
+	int prio_queue;
+	int tx_q0_ctrl;
+	int tx_q1_ctrl;
+	int tx_q2_ctrl;
+	int tx_q3_ctrl;
+	int tx_q0_ratio;
+	int tx_q1_ratio;
+	int tx_q2_ratio;
+	int tx_q3_ratio;
+	int rx_prio_rate;
+	int tx_prio_rate;
+	int rx_limit;
+	int rx_limit_port_based;
+	int limit_packet_based;
+	int rx_limit_flow_ctrl;
+	int cnt_ifg;
+	int cnt_pre;
+	int rx_p0_rate;
+	int rx_p1_rate;
+	int rx_p2_rate;
+	int rx_p3_rate;
+	int tx_q0_rate;
+	int tx_q1_rate;
+	int tx_q2_rate;
+	int tx_q3_rate;
+	int mirror_port;
+	int mirror_rx;
+	int mirror_tx;
+	int back_pressure;
+	int force_flow_ctrl;
+	int fw_unk_ucast_dest;
+	int fw_unk_mcast_dest;
+	int fw_inv_vid;
+	int fw_unk_ip_mcast_dest;
+
+	int pme_ctrl;
+	int pme_status;
+
+	int authen_mode;
+	int acl;
+	int acl_first_rule;
+	int acl_ruleset;
+	int acl_mode;
+	int acl_enable;
+	int acl_src;
+	int acl_equal;
+	int acl_addr;
+	int acl_type;
+	int acl_cnt;
+	int acl_msec;
+	int acl_intr_mode;
+	int acl_ip_addr;
+	int acl_ip_mask;
+	int acl_protocol;
+	int acl_seqnum;
+	int acl_port_mode;
+	int acl_max_port;
+	int acl_min_port;
+	int acl_tcp_flag_enable;
+	int acl_tcp_flag;
+	int acl_tcp_flag_mask;
+	int acl_prio_mode;
+	int acl_prio;
+	int acl_vlan_prio_replace;
+	int acl_vlan_prio;
+	int acl_map_mode;
+	int acl_ports;
+	int acl_index;
+	int acl_act_index;
+	int acl_act;
+	int acl_rule_index;
+	int acl_info;
+	int acl_table;
+
+	int duplex;
+	int speed;
+	int linkmd;
+
+#ifdef CONFIG_KSZ_STP
+	int stp_info;
+	int stp_on;
+	int stp_prio;
+	int stp_admin_path_cost;
+	int stp_path_cost;
+	int stp_admin_edge;
+	int stp_auto_edge;
+	int stp_mcheck;
+	int stp_admin_p2p;
+#endif
+};
+
+struct static_mac_attributes {
+	int fid;
+	int use_fid;
+	int override;
+	int valid;
+	int ports;
+	int addr;
+};
+
+struct vlan_attributes {
+	int valid;
+	int member;
+	int fid;
+	int vid;
+};
+
+#endif
diff -Napur linux-linux4microchip-2022.10-orig/drivers/net/ethernet/micrel/ksz_sw_8895.c linux-linux4microchip-2022.10/drivers/net/ethernet/micrel/ksz_sw_8895.c
--- linux-linux4microchip-2022.10-orig/drivers/net/ethernet/micrel/ksz_sw_8895.c	1969-12-31 16:00:00.000000000 -0800
+++ linux-linux4microchip-2022.10/drivers/net/ethernet/micrel/ksz_sw_8895.c	2024-04-26 16:31:09.093099348 -0700
@@ -0,0 +1,10023 @@
+/**
+ * Microchip KSZ8895 switch common code
+ *
+ * Copyright (c) 2015-2024 Microchip Technology Inc.
+ *	Tristram Ha <Tristram.Ha@microchip.com>
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ */
+
+
+/* -------------------------------------------------------------------------- */
+
+#define MAX_SYSFS_BUF_SIZE		(4080 - 80)
+
+#if 0
+#define USE_SAME_ADDR
+#endif
+
+enum {
+	PROC_SW_INFO,
+	PROC_SW_VERSION,
+
+	PROC_SET_SW_DUPLEX,
+	PROC_SET_SW_SPEED,
+	PROC_SET_SW_FORCE,
+	PROC_SET_SW_FLOW_CTRL,
+
+	PROC_SET_SW_FEATURES,
+	PROC_SET_SW_OVERRIDES,
+	PROC_SET_SW_MIB,
+
+	PROC_SET_SW_REG,
+	PROC_SET_SW_VID,
+
+	PROC_DYNAMIC,
+	PROC_STATIC,
+	PROC_VLAN,
+
+	PROC_SET_AGING,
+	PROC_SET_FAST_AGING,
+	PROC_SET_LINK_AGING,
+
+	PROC_SET_BROADCAST_STORM,
+	PROC_SET_MULTICAST_STORM,
+	PROC_SET_TX_RATE_QUEUE_BASED,
+	PROC_SET_DIFFSERV,
+	PROC_SET_802_1P,
+
+	PROC_ENABLE_VLAN,
+	PROC_SET_REPLACE_NULL_VID,
+	PROC_SET_MAC_ADDR,
+	PROC_SET_MIRROR_MODE,
+	PROC_SET_TAIL_TAG,
+
+	PROC_SET_IGMP_SNOOP,
+
+	PROC_SET_AGGR_BACKOFF,
+	PROC_SET_NO_EXC_DROP,
+
+	PROC_SET_HUGE_PACKET,
+	PROC_SET_LEGAL_PACKET,
+	PROC_SET_LENGTH_CHECK,
+
+	PROC_SET_BACK_PRESSURE_MODE,
+	PROC_SET_SWITCH_FLOW_CTRL,
+	PROC_SET_SWITCH_HALF_DUPLEX,
+	PROC_SET_SWITCH_10_MBIT,
+
+	PROC_SET_RX_FLOW_CTRL,
+	PROC_SET_TX_FLOW_CTRL,
+	PROC_SET_FAIR_FLOW_CTRL,
+	PROC_SET_VLAN_BOUNDARY,
+
+	PROC_SET_FORWARD_UNKNOWN_UNICAST,
+	PROC_SET_FORWARD_UNKNOWN_UNICAST_PORTS,
+	PROC_SET_FORWARD_UNKNOWN_MULTICAST,
+	PROC_SET_FORWARD_UNKNOWN_MULTICAST_PORTS,
+	PROC_SET_FORWARD_UNKNOWN_VID,
+	PROC_SET_FORWARD_UNKNOWN_VID_PORTS,
+	PROC_SET_FORWARD_UNKNOWN_IP_MULTICAST,
+	PROC_SET_FORWARD_UNKNOWN_IP_MULTICAST_PORTS,
+	PROC_SET_SELF_ADDR_FILTER,
+	PROC_SET_INS_TAG,
+
+	PROC_SET_PASS_ALL,
+
+	PROC_SET_PASS_PAUSE,
+	PROC_SET_HI_PRIO_QUEUES,
+
+	PROC_GET_PORTS,
+	PROC_GET_DEV_START,
+	PROC_GET_VLAN_START,
+	PROC_GET_STP,
+
+	PROC_SET_STATIC_FID,
+	PROC_SET_STATIC_USE_FID,
+	PROC_SET_STATIC_OVERRIDE,
+	PROC_SET_STATIC_VALID,
+	PROC_SET_STATIC_PORTS,
+	PROC_SET_STATIC_MAC_ADDR,
+	PROC_SET_STATIC_INDEX,
+	PROC_GET_STATIC_INFO,
+
+	PROC_SET_VLAN_VALID,
+	PROC_SET_VLAN_MEMBER,
+	PROC_SET_VLAN_FID,
+	PROC_SET_VLAN_VID,
+	PROC_GET_VLAN_INFO,
+
+#ifdef CONFIG_KSZ_STP
+	PROC_GET_STP_BR_INFO,
+	PROC_SET_STP_BR_ON,
+	PROC_SET_STP_BR_PRIO,
+	PROC_SET_STP_BR_FWD_DELAY,
+	PROC_SET_STP_BR_MAX_AGE,
+	PROC_SET_STP_BR_HELLO_TIME,
+	PROC_SET_STP_BR_TX_HOLD,
+	PROC_SET_STP_VERSION,
+#endif
+};
+
+enum {
+	PROC_SET_PORT_MIB,
+
+	PROC_SET_DEF_VID,
+	PROC_SET_MEMBER,
+
+	PROC_ENABLE_BROADCAST_STORM,
+	PROC_ENABLE_DIFFSERV,
+	PROC_ENABLE_802_1P,
+
+	PROC_SET_PORT_BASED,
+
+	PROC_SET_DIS_NON_VID,
+	PROC_SET_INGRESS,
+	PROC_SET_INSERT_TAG,
+	PROC_SET_REMOVE_TAG,
+	PROC_SET_DROP_TAG,
+	PROC_SET_REPLACE_PRIO,
+
+	PROC_SET_RX,
+	PROC_SET_TX,
+	PROC_SET_LEARN,
+
+	PROC_SET_INS_TAG_0,
+	PROC_SET_INS_TAG_1,
+	PROC_SET_INS_TAG_2,
+	PROC_SET_INS_TAG_3,
+	PROC_SET_INS_TAG_4,
+
+	PROC_ENABLE_PRIO_QUEUE,
+	PROC_SET_TX_Q0_CTRL,
+	PROC_SET_TX_Q1_CTRL,
+	PROC_SET_TX_Q2_CTRL,
+	PROC_SET_TX_Q3_CTRL,
+	PROC_SET_TX_Q0_RATIO,
+	PROC_SET_TX_Q1_RATIO,
+	PROC_SET_TX_Q2_RATIO,
+	PROC_SET_TX_Q3_RATIO,
+
+	PROC_ENABLE_RX_PRIO_RATE,
+	PROC_ENABLE_TX_PRIO_RATE,
+	PROC_SET_RX_LIMIT,
+	PROC_SET_RX_LIMIT_FLOW_CTRL,
+	PROC_SET_CNT_IFG,
+	PROC_SET_CNT_PRE,
+	PROC_SET_RX_P0_RATE,
+	PROC_SET_RX_P1_RATE,
+	PROC_SET_RX_P2_RATE,
+	PROC_SET_RX_P3_RATE,
+	PROC_SET_TX_Q0_RATE,
+	PROC_SET_TX_Q1_RATE,
+	PROC_SET_TX_Q2_RATE,
+	PROC_SET_TX_Q3_RATE,
+
+	PROC_SET_MIRROR_PORT,
+	PROC_SET_MIRROR_RX,
+	PROC_SET_MIRROR_TX,
+
+	PROC_SET_BACK_PRESSURE,
+	PROC_SET_FORCE_FLOW_CTRL,
+
+	PROC_SET_UNKNOWN_UNICAST_PORT,
+	PROC_SET_UNKNOWN_MULTICAST_PORT,
+	PROC_SET_UNKNOWN_VID_PORT,
+	PROC_SET_UNKNOWN_IP_MULTICAST_PORT,
+
+	PROC_SET_PORT_DUPLEX,
+	PROC_SET_PORT_SPEED,
+	PROC_SET_LINK_MD,
+
+#ifdef CONFIG_KSZ_STP
+	PROC_GET_STP_INFO,
+	PROC_SET_STP_ON,
+	PROC_SET_STP_PRIO,
+	PROC_SET_STP_ADMIN_PATH_COST,
+	PROC_SET_STP_PATH_COST,
+	PROC_SET_STP_ADMIN_EDGE,
+	PROC_SET_STP_AUTO_EDGE,
+	PROC_SET_STP_MCHECK,
+	PROC_SET_STP_ADMIN_P2P,
+#endif
+
+};
+
+/* -------------------------------------------------------------------------- */
+
+static uint get_phy_port(struct ksz_sw *sw, uint n)
+{
+	if (n >= sw->mib_port_cnt + 1)
+		n = 0;
+	return sw->port_info[n].phy_p;
+}
+
+static uint get_log_port(struct ksz_sw *sw, uint p)
+{
+	return sw->port_info[p].log_p;
+}
+
+static uint get_log_port_zero(struct ksz_sw *sw, uint p)
+{
+	uint n;
+
+	n = get_log_port(sw, p);
+	if (n)
+		n--;
+	else
+		n = sw->mib_port_cnt;
+	return n;
+}
+
+static uint get_phy_mask_from_log(struct ksz_sw *sw, uint log_m)
+{
+	struct ksz_port_info *info;
+	uint n;
+	uint p;
+	uint phy_m = 0;
+
+	for (n = 0; n <= sw->mib_port_cnt; n++) {
+		info = &sw->port_info[n];
+		p = info->phy_p;
+		if (log_m & sw->port_info[p].log_m)
+			phy_m |= info->phy_m;
+	}
+	return phy_m;
+}
+
+static uint get_log_mask_from_phy(struct ksz_sw *sw, uint phy_m)
+{
+	struct ksz_port_info *info;
+	uint n;
+	uint p;
+	uint log_m = 0;
+
+	for (n = 0; n <= sw->mib_port_cnt; n++) {
+		info = &sw->port_info[n];
+		p = info->phy_p;
+		if (phy_m & info->phy_m)
+			log_m |= sw->port_info[p].log_m;
+	}
+	return log_m;
+}
+
+static uint get_sysfs_port(struct ksz_sw *sw, uint n)
+{
+	uint p = n;
+
+	if (!(sw->overrides & SYSFS_PHY_PORT)) {
+		n++;
+		if (n > sw->mib_port_cnt)
+			n = 0;
+		p = get_phy_port(sw, n);
+	}
+	return p;
+}
+
+static inline struct ksz_port_cfg *get_port_cfg(struct ksz_sw *sw, uint p)
+{
+	return &sw->info->port_cfg[p];
+}
+
+static inline struct ksz_port_info *get_port_info(struct ksz_sw *sw, uint p)
+{
+	return &sw->port_info[p];
+}
+
+static inline struct ksz_port_mib *get_port_mib(struct ksz_sw *sw, uint p)
+{
+	return &sw->port_mib[p];
+}
+
+static void sw_acquire(struct ksz_sw *sw)
+{
+	mutex_lock(&sw->lock);
+	mutex_lock(sw->reglock);
+	mutex_unlock(&sw->lock);
+}  /* sw_acquire */
+
+static void sw_release(struct ksz_sw *sw)
+{
+	mutex_unlock(sw->reglock);
+}  /* sw_release */
+
+/* -------------------------------------------------------------------------- */
+
+/*
+#define STATIC_MAC_TABLE_ADDR		00-0000FFFF-FFFFFFFF
+#define STATIC_MAC_TABLE_FWD_PORTS	00-001F0000-00000000
+#define STATIC_MAC_TABLE_VALID		00-00200000-00000000
+#define STATIC_MAC_TABLE_OVERRIDE	00-00400000-00000000
+#define STATIC_MAC_TABLE_USE_FID	00-00800000-00000000
+#define STATIC_MAC_TABLE_FID		00-7F000000-00000000
+*/
+
+#define STATIC_MAC_TABLE_ADDR		0x0000FFFF
+#define STATIC_MAC_TABLE_FWD_PORTS	0x001F0000
+#define STATIC_MAC_TABLE_VALID		0x00200000
+#define STATIC_MAC_TABLE_OVERRIDE	0x00400000
+#define STATIC_MAC_TABLE_USE_FID	0x00800000
+#define STATIC_MAC_TABLE_FID		0x7F000000
+
+#define STATIC_MAC_FWD_PORTS_S		16
+#define STATIC_MAC_FID_S		24
+
+/*
+#define VLAN_TABLE_FID			00-00003F81-FC0FE07F
+#define VLAN_TABLE_MEMBERSHIP		00-0007C03E-01F00F80
+#define VLAN_TABLE_VALID		00-00080040-02001000
+*/
+
+#define VLAN_TABLE_FID			0x007F
+#define VLAN_TABLE_MEMBERSHIP		0x0F80
+#define VLAN_TABLE_VALID		0x1000
+
+#define VLAN_TABLE_MEMBERSHIP_S		7
+#define VLAN_TABLE_S			13
+
+/*
+#define DYNAMIC_MAC_TABLE_ADDR		00-0000FFFF-FFFFFFFF
+#define DYNAMIC_MAC_TABLE_FID		00-007F0000-00000000
+#define DYNAMIC_MAC_TABLE_NOT_READY	00-00800000-00000000
+#define DYNAMIC_MAC_TABLE_SRC_PORT	00-07000000-00000000
+#define DYNAMIC_MAC_TABLE_TIMESTAMP	00-18000000-00000000
+#define DYNAMIC_MAC_TABLE_ENTRIES	7F-E0000000-00000000
+#define DYNAMIC_MAC_TABLE_MAC_EMPTY	80-00000000-00000000
+*/
+
+#define DYNAMIC_MAC_TABLE_ADDR		0x0000FFFF
+#define DYNAMIC_MAC_TABLE_FID		0x007F0000
+#define DYNAMIC_MAC_TABLE_SRC_PORT	0x07000000
+#define DYNAMIC_MAC_TABLE_TIMESTAMP	0x18000000
+#define DYNAMIC_MAC_TABLE_ENTRIES	0xE0000000
+
+#define DYNAMIC_MAC_TABLE_NOT_READY	0x80
+
+#define DYNAMIC_MAC_TABLE_ENTRIES_H	0x7F
+#define DYNAMIC_MAC_TABLE_MAC_EMPTY	0x80
+
+#define DYNAMIC_MAC_FID_S		16
+#define DYNAMIC_MAC_SRC_PORT_S		24
+#define DYNAMIC_MAC_TIMESTAMP_S		27
+#define DYNAMIC_MAC_ENTRIES_S		29
+#define DYNAMIC_MAC_ENTRIES_H_S		3
+
+/*
+#define MIB_COUNTER_VALUE		00-00000000-3FFFFFFF
+#define MIB_PACKET_DROPPED		00-00000000-0000FFFF
+#define MIB_COUNTER_VALID		00-00000000-40000000
+#define MIB_COUNTER_OVERFLOW		00-00000000-80000000
+*/
+
+#ifndef MIB_COUNTER_OVERFLOW
+#define MIB_COUNTER_OVERFLOW		(1 << 7)
+#define MIB_COUNTER_VALID		(1 << 6)
+
+#define MIB_COUNTER_VALUE		0x3FFFFFFF
+#endif
+
+#define KS_MIB_PACKET_DROPPED_TX_0	0x100
+#define KS_MIB_PACKET_DROPPED_TX_1	0x101
+#define KS_MIB_PACKET_DROPPED_TX_2	0x102
+#define KS_MIB_PACKET_DROPPED_TX_3	0x103
+#define KS_MIB_PACKET_DROPPED_TX_4	0x104
+#define KS_MIB_PACKET_DROPPED_RX_0	0x105
+#define KS_MIB_PACKET_DROPPED_RX_1	0x106
+#define KS_MIB_PACKET_DROPPED_RX_2	0x107
+#define KS_MIB_PACKET_DROPPED_RX_3	0x108
+#define KS_MIB_PACKET_DROPPED_RX_4	0x109
+
+#define MIB_PACKET_DROPPED		0x0000FFFF
+
+/* -------------------------------------------------------------------------- */
+
+/* Switch functions */
+
+/**
+ * sw_r_table_64 - read 64 bits of data from switch table
+ * @sw:		The switch instance.
+ * @table:	The table selector.
+ * @addr:	The address of the table entry.
+ * @data_hi:	Buffer to store the high part of read data (bit63 ~ bit32).
+ * @data_lo:	Buffer to store the low part of read data (bit31 ~ bit0).
+ *
+ * This routine reads 64 bits of data from the table of the switch.
+ * Hardware is locked to minimize corruption of read data.
+ */
+static void sw_r_table_64(struct ksz_sw *sw, int table, u16 addr, u32 *data_hi,
+	u32 *data_lo)
+{
+	u64 buf;
+	u16 ctrl_addr;
+
+	ctrl_addr = IND_ACC_TABLE(table | TABLE_READ) | addr;
+
+	sw->ops->acquire(sw);
+	sw->reg->w16(sw, REG_IND_CTRL_0, ctrl_addr);
+	sw->reg->r(sw, REG_IND_DATA_HI, &buf, sizeof(buf));
+	sw->ops->release(sw);
+	buf = be64_to_cpu(buf);
+	*data_hi = (u32)(buf >> 32);
+	*data_lo = (u32) buf;
+}  /* sw_r_table_64 */
+
+/**
+ * sw_w_table_64 - write 64 bits of data to switch table
+ * @sw:		The switch instance.
+ * @table:	The table selector.
+ * @addr:	The address of the table entry.
+ * @data_hi:	The high part of data to be written (bit63 ~ bit32).
+ * @data_lo:	The low part of data to be written (bit31 ~ bit0).
+ *
+ * This routine writes 64 bits of data to the table of the switch.
+ * Hardware is locked to minimize corruption of written data.
+ */
+static void sw_w_table_64(struct ksz_sw *sw, int table, u16 addr, u32 data_hi,
+	u32 data_lo)
+{
+	u64 buf;
+	u16 ctrl_addr;
+
+	ctrl_addr = IND_ACC_TABLE(table) | addr;
+	buf = data_hi;
+	buf <<= 32;
+	buf |= data_lo;
+	buf = cpu_to_be64(buf);
+
+	sw->ops->acquire(sw);
+	sw->reg->w(sw, REG_IND_DATA_HI, &buf, sizeof(buf));
+	sw->reg->w16(sw, REG_IND_CTRL_0, ctrl_addr);
+	sw->ops->release(sw);
+}  /* sw_w_table_64 */
+
+static inline int valid_dyn_entry(struct ksz_sw *sw, u8 *data)
+{
+	int timeout = 100;
+
+	do {
+		*data = sw->reg->r8(sw, REG_IND_DATA_CHECK);
+		timeout--;
+	} while ((*data & DYNAMIC_MAC_TABLE_NOT_READY) && timeout);
+
+	/* Entry is not ready for accessing. */
+	if (*data & DYNAMIC_MAC_TABLE_NOT_READY)
+		return 1;
+
+	/* Entry is ready for accessing. */
+	else {
+		*data = sw->reg->r8(sw, REG_IND_DATA_8);
+
+		/* There is no valid entry in the table. */
+		if (*data & DYNAMIC_MAC_TABLE_MAC_EMPTY)
+			return 2;
+	}
+	return 0;
+}  /* valid_dyn_entry */
+
+/**
+ * sw_r_dyn_mac_table - read from dynamic MAC table
+ * @sw:		The switch instance.
+ * @addr:	The address of the table entry.
+ * @mac_addr:	Buffer to store the MAC address.
+ * @fid:	Buffer to store the FID.
+ * @src_port:	Buffer to store the source port number.
+ * @timestamp:	Buffer to store the timestamp.
+ * @entries:	Buffer to store the number of entries.  If this is zero, the
+ *		table is empty and so this function should not be called again
+ *		until later.
+ *
+ * This function reads an entry of the dynamic MAC table of the switch.
+ * Hardware is locked to minimize corruption of read data.
+ *
+ * Return 0 if the entry is successfully read; otherwise -1.
+ */
+static int sw_r_dyn_mac_table(struct ksz_sw *sw, u16 addr, u8 *mac_addr,
+	u8 *fid, u8 *src_port, u8 *timestamp, u16 *entries)
+{
+	u32 data_hi;
+	u32 data_lo;
+	u16 ctrl_addr;
+	int rc;
+	u8 data;
+
+	ctrl_addr = IND_ACC_TABLE(TABLE_DYNAMIC_MAC | TABLE_READ) | addr;
+
+	sw->ops->acquire(sw);
+	sw->reg->w16(sw, REG_IND_CTRL_0, ctrl_addr);
+
+	rc = valid_dyn_entry(sw, &data);
+	if (1 == rc) {
+		if (0 == addr)
+			*entries = 0;
+	} else if (2 == rc)
+		*entries = 0;
+	/* At least one valid entry in the table. */
+	else {
+		u64 buf;
+
+		sw->reg->r(sw, REG_IND_DATA_HI, &buf, sizeof(buf));
+		buf = be64_to_cpu(buf);
+		data_hi = (u32)(buf >> 32);
+		data_lo = (u32) buf;
+
+		/* Check out how many valid entry in the table. */
+		*entries = (u16)(((((u16)
+			data & DYNAMIC_MAC_TABLE_ENTRIES_H) <<
+			DYNAMIC_MAC_ENTRIES_H_S) |
+			(((data_hi & DYNAMIC_MAC_TABLE_ENTRIES) >>
+			DYNAMIC_MAC_ENTRIES_S))) + 1);
+
+		*fid = (u8)((data_hi & DYNAMIC_MAC_TABLE_FID) >>
+			DYNAMIC_MAC_FID_S);
+		*src_port = (u8)((data_hi & DYNAMIC_MAC_TABLE_SRC_PORT) >>
+			DYNAMIC_MAC_SRC_PORT_S);
+		*timestamp = (u8)((
+			data_hi & DYNAMIC_MAC_TABLE_TIMESTAMP) >>
+			DYNAMIC_MAC_TIMESTAMP_S);
+
+		mac_addr[5] = (u8) data_lo;
+		mac_addr[4] = (u8)(data_lo >> 8);
+		mac_addr[3] = (u8)(data_lo >> 16);
+		mac_addr[2] = (u8)(data_lo >> 24);
+
+		mac_addr[1] = (u8) data_hi;
+		mac_addr[0] = (u8)(data_hi >> 8);
+		rc = 0;
+	}
+	sw->ops->release(sw);
+
+	return rc;
+}  /* sw_r_dyn_mac_table */
+
+/**
+ * sw_d_dyn_mac_table - dump dynamic MAC table
+ * @sw:		The switch instance.
+ *
+ * This routine dumps dynamic MAC table contents.
+ */
+static ssize_t sw_d_dyn_mac_table(struct ksz_sw *sw, char *buf, ssize_t len)
+{
+	u16 entries = 0;
+	u16 i;
+	u8 mac_addr[ETH_ALEN];
+	u8 port = 0;
+	u8 timestamp = 0;
+	u8 fid = 0;
+	int first_break = true;
+
+	memset(mac_addr, 0, ETH_ALEN);
+	i = 0;
+	do {
+		if (!sw_r_dyn_mac_table(sw, i, mac_addr, &fid, &port,
+				&timestamp, &entries)) {
+			port = get_log_port_zero(sw, port);
+			if (len >= MAX_SYSFS_BUF_SIZE && first_break) {
+				first_break = false;
+				len += sprintf(buf + len, "...\n");
+			}
+			if (len < MAX_SYSFS_BUF_SIZE)
+			len += sprintf(buf + len,
+				"%02X:%02X:%02X:%02X:%02X:%02X  "
+				"f:%02x  p:%x  t:%x\n",
+				mac_addr[0], mac_addr[1], mac_addr[2],
+				mac_addr[3], mac_addr[4], mac_addr[5],
+				fid, port, timestamp);
+			else
+			printk(KERN_INFO
+				"%02X:%02X:%02X:%02X:%02X:%02X  "
+				"f:%02x  p:%x  t:%x\n",
+				mac_addr[0], mac_addr[1], mac_addr[2],
+				mac_addr[3], mac_addr[4], mac_addr[5],
+				fid, port, timestamp);
+		}
+		i++;
+	} while (i < entries);
+	if (entries) {
+		if (len < MAX_SYSFS_BUF_SIZE)
+			sprintf(buf + len, "=%03x\n", entries);
+		else
+			printk(KERN_INFO "=%03x\n", entries);
+	}
+	return len;
+}  /* sw_d_dyn_mac_table */
+
+/**
+ * sw_r_sta_mac_table - read from static MAC table
+ * @sw:		The switch instance.
+ * @addr:	The address of the table entry.
+ * @mac:	Buffer to store the MAC table entry.
+ *
+ * This function reads an entry of the static MAC table of the switch.  It
+ * calls sw_r_table_64() to get the data.
+ *
+ * Return 0 if the entry is valid; otherwise -1.
+ */
+static int sw_r_sta_mac_table(struct ksz_sw *sw, u16 addr,
+	struct ksz_mac_table *mac)
+{
+	u32 data_hi;
+	u32 data_lo;
+
+	sw_r_table_64(sw, TABLE_STATIC_MAC, addr, &data_hi, &data_lo);
+	if (data_hi & (STATIC_MAC_TABLE_VALID | STATIC_MAC_TABLE_OVERRIDE)) {
+		mac->addr[5] = (u8) data_lo;
+		mac->addr[4] = (u8)(data_lo >> 8);
+		mac->addr[3] = (u8)(data_lo >> 16);
+		mac->addr[2] = (u8)(data_lo >> 24);
+		mac->addr[1] = (u8) data_hi;
+		mac->addr[0] = (u8)(data_hi >> 8);
+		mac->ports = (u8)((data_hi & STATIC_MAC_TABLE_FWD_PORTS) >>
+			STATIC_MAC_FWD_PORTS_S);
+		mac->override = (data_hi & STATIC_MAC_TABLE_OVERRIDE) ? 1 : 0;
+		data_hi >>= 1;
+		mac->use_fid = (data_hi & STATIC_MAC_TABLE_USE_FID) ? 1 : 0;
+		mac->fid = (u8)((data_hi & STATIC_MAC_TABLE_FID) >>
+			STATIC_MAC_FID_S);
+		mac->dirty = 0;
+		return 0;
+	}
+	return -1;
+}  /* sw_r_sta_mac_table */
+
+/**
+ * sw_w_sta_mac_table - write to static MAC table
+ * @sw:		The switch instance.
+ * @addr:	The address of the table entry.
+ * @mac:	The MAC table entry.
+ *
+ * This routine writes an entry of the static MAC table of the switch.  It
+ * calls sw_w_table_64() to write the data.
+ */
+static void sw_w_sta_mac_table(struct ksz_sw *sw, u16 addr,
+	struct ksz_mac_table *mac)
+{
+	u32 data_hi;
+	u32 data_lo;
+
+	data_lo = ((u32) mac->addr[2] << 24) |
+		((u32) mac->addr[3] << 16) |
+		((u32) mac->addr[4] << 8) | mac->addr[5];
+	data_hi = ((u32) mac->addr[0] << 8) | mac->addr[1];
+	data_hi |= (u32) mac->ports << STATIC_MAC_FWD_PORTS_S;
+
+	if (mac->override)
+		data_hi |= STATIC_MAC_TABLE_OVERRIDE;
+	if (mac->use_fid) {
+		data_hi |= STATIC_MAC_TABLE_USE_FID;
+		data_hi |= (u32) mac->fid << STATIC_MAC_FID_S;
+	}
+	if (mac->valid)
+		data_hi |= STATIC_MAC_TABLE_VALID;
+	else
+		data_hi &= ~STATIC_MAC_TABLE_OVERRIDE;
+
+	sw_w_table_64(sw, TABLE_STATIC_MAC, addr, data_hi, data_lo);
+	mac->dirty = 0;
+}  /* sw_w_sta_mac_table */
+
+/**
+ * sw_d_sta_mac_table - dump static MAC table
+ * @sw:		The switch instance.
+ *
+ * This routine dumps static MAC table contents.
+ */
+static ssize_t sw_d_sta_mac_table(struct ksz_sw *sw, char *buf, ssize_t len)
+{
+	u8 ports;
+	u16 i;
+	struct ksz_mac_table mac;
+
+	i = 0;
+	do {
+		if (!sw_r_sta_mac_table(sw, i, &mac)) {
+			ports = mac.ports;
+			ports = get_log_mask_from_phy(sw, ports);
+			len += sprintf(buf + len,
+				"%2x: %02X:%02X:%02X:%02X:%02X:%02X  "
+				"%02x  %u  %u:%02x\n",
+				i, mac.addr[0], mac.addr[1], mac.addr[2],
+				mac.addr[3], mac.addr[4], mac.addr[5],
+				ports, mac.override, mac.use_fid, mac.fid);
+		}
+		i++;
+	} while (i < STATIC_MAC_TABLE_ENTRIES);
+	return len;
+}  /* sw_d_sta_mac_table */
+
+#ifdef DEBUG
+static ssize_t sw_d_mac_table(struct ksz_sw *sw, char *buf, ssize_t len)
+{
+#if 0
+	struct ksz_mac_table *entry;
+	int i;
+	u8 ports;
+
+	i = STATIC_MAC_TABLE_ENTRIES;
+	do {
+		entry = &sw->info->mac_table[i];
+		if (entry->valid) {
+			ports = entry->ports;
+			ports = get_log_mask_from_phy(sw, ports);
+			len += sprintf(buf + len,
+				"%x: %02X:%02X:%02X:%02X:%02X:%02X  "
+				"%x  %u  %u:%x\n",
+				i, entry->mac_addr[0], entry->mac_addr[1],
+				entry->mac_addr[2], entry->mac_addr[3],
+				entry->mac_addr[4], entry->mac_addr[5],
+				ports, entry->override, entry->use_fid,
+				entry->fid);
+		}
+		i++;
+		if (SWITCH_MAC_TABLE_ENTRIES == i)
+			printk(KERN_INFO "\n");
+	} while (i < MULTI_MAC_TABLE_ENTRIES);
+#endif
+	return len;
+}  /* sw_d_mac_table */
+#endif
+
+/* -------------------------------------------------------------------------- */
+
+/**
+ * sw_r_vlan_entries - read many from VLAN table
+ * @sw:		The switch instance.
+ * @addr:	The address of the table entry.
+ * @valid:	Buffer to store the valid bit.
+ * @fid:	Buffer to store the VID.
+ * @member:	Buffer to store the port membership.
+ *
+ * This function reads several entries of the VLAN table of the switch.  It
+ * calls sw_r_table_64() to get the data.
+ *
+ * Return 0 if the entry is valid; otherwise -1.
+ */
+static int sw_r_vlan_entries(struct ksz_sw *sw, u16 addr, u8 *valid, u8 *fid,
+	u8 *member)
+{
+	u64 buf;
+	u32 *data_lo = (u32 *) &buf;
+	u32 *data_hi = data_lo + 1;
+	u16 data;
+	int i;
+	int ret = -1;
+
+	sw_r_table_64(sw, TABLE_VLAN, addr, data_hi, data_lo);
+	for (i = 0; i < 4; i++) {
+		data = (u16) buf;
+		*valid = 0;
+		if (data & VLAN_TABLE_VALID) {
+			*valid = 1;
+			*fid = (u8)(data & VLAN_TABLE_FID);
+			*member = (u8)((data & VLAN_TABLE_MEMBERSHIP) >>
+				VLAN_TABLE_MEMBERSHIP_S);
+			ret = 0;
+		}
+		++valid;
+		++fid;
+		++member;
+		buf >>= VLAN_TABLE_S;
+	}
+	return ret;
+}  /* sw_r_vlan_entries */
+
+/**
+ * sw_r_vlan_table - read from VLAN table
+ * @sw:		The switch instance.
+ * @vid:	The address of the table entry.
+ * @vlan:	Buffer to store the VLAN table entry.
+ *
+ * This routine reads an entry of the VLAN table of the switch.  It calls
+ * sw_r_table_64() to get the data.
+ */
+static void sw_r_vlan_table(struct ksz_sw *sw, u16 vid,
+	struct ksz_vlan_table *vlan)
+{
+	u64 buf;
+	u32 *data_lo = (u32 *) &buf;
+	u32 *data_hi = data_lo + 1;
+	u16 addr;
+	int index;
+
+	addr = vid / 4;
+	index = vid & 3;
+	sw_r_table_64(sw, TABLE_VLAN, addr, data_hi, data_lo);
+	buf >>= VLAN_TABLE_S * index;
+	buf &= (1 << VLAN_TABLE_S) - 1;
+	vlan->fid = (u8)(buf & VLAN_TABLE_FID);
+	vlan->member = (u8)((buf & VLAN_TABLE_MEMBERSHIP) >>
+		VLAN_TABLE_MEMBERSHIP_S);
+	vlan->valid = !!(buf & VLAN_TABLE_VALID);
+	vlan->dirty = 0;
+}  /* sw_r_vlan_table */
+
+/**
+ * sw_w_vlan_table - write to VLAN table
+ * @sw:		The switch instance.
+ * @vid:	The address of the table entry.
+ * @vlan:	The VLAN table entry.
+ *
+ * This routine writes an entry of the VLAN table of the switch.  It calls
+ * sw_w_table_64() to write the data.
+ */
+static void sw_w_vlan_table(struct ksz_sw *sw, u16 vid,
+	struct ksz_vlan_table *vlan)
+{
+	u64 buf;
+	u64 mask;
+	u64 val;
+	u32 *data_lo = (u32 *) &buf;
+	u32 *data_hi = data_lo + 1;
+	u16 addr;
+	int index;
+
+	addr = vid / 4;
+	index = vid & 3;
+	sw_r_table_64(sw, TABLE_VLAN, addr, data_hi, data_lo);
+	mask = (1 << VLAN_TABLE_S) - 1;
+	mask <<= VLAN_TABLE_S * index;
+	val = vlan->fid;
+	val |= (u16) vlan->member << VLAN_TABLE_MEMBERSHIP_S;
+	if (vlan->valid)
+		val |= VLAN_TABLE_VALID;
+	val <<= VLAN_TABLE_S * index;
+	buf &= ~mask;
+	buf |= val;
+	sw_w_table_64(sw, TABLE_VLAN, addr, *data_hi, *data_lo);
+	vlan->dirty = 0;
+}  /* sw_w_vlan_table */
+
+/**
+ * sw_d_vlan_table - dump VLAN table
+ * @sw:		The switch instance.
+ *
+ * This routine dumps the VLAN table.
+ */
+static ssize_t sw_d_vlan_table(struct ksz_sw *sw, char *buf, ssize_t len)
+{
+	u16 i;
+	u16 j;
+	u16 vid;
+	u8 ports;
+	u8 fid[4];
+	u8 member[4];
+	u8 valid[4];
+	int first_break = true;
+
+	i = 0;
+	do {
+		if (!sw_r_vlan_entries(sw, i, valid, fid, member)) {
+			vid = i * 4;
+			for (j = 0; j < 4; j++, vid++) {
+				if (!valid[j])
+					continue;
+				ports = member[j];
+				ports = get_log_mask_from_phy(sw, member[j]);
+				if (len >= MAX_SYSFS_BUF_SIZE && first_break) {
+					first_break = false;
+					len += sprintf(buf + len, "...\n");
+				}
+				if (len < MAX_SYSFS_BUF_SIZE)
+					len += sprintf(buf + len,
+						"0x%03x: %2x  %2x\n", vid,
+						fid[j], ports);
+				else
+					printk(KERN_INFO
+						"0x%03x: %2x  %2x\n", vid,
+						fid[j], ports);
+			}
+		}
+		if (len >= MAX_SYSFS_BUF_SIZE)
+		yield();
+		i++;
+	} while (i < VLAN_TABLE_ENTRIES);
+	return len;
+}  /* sw_d_vlan_table */
+
+/* -------------------------------------------------------------------------- */
+
+/*
+ * Some counters do not need to be read too often because they are less likely
+ * to increase much.
+ */
+static u8 mib_read_max[TOTAL_SWITCH_COUNTER_NUM] = {
+	1,
+	1,
+	4,
+	4,
+	4,
+	4,
+	4,
+	4,
+	4,
+	4,
+	1,
+	1,
+	1,
+	1,
+	2,
+	2,
+	2,
+	2,
+	2,
+	2,
+
+	1,
+	1,
+	4,
+	1,
+	1,
+	1,
+	1,
+	4,
+	4,
+	4,
+	4,
+	4,
+
+	2,
+	2,
+};
+
+/**
+ * port_r_mib_cnt - read MIB counter
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ * @addr:	The address of the counter.
+ * @cnt:	Buffer to store the counter.
+ *
+ * This routine reads a MIB counter of the port.
+ * Hardware is locked to minimize corruption of read data.
+ */
+static void port_r_mib_cnt(struct ksz_sw *sw, uint port, u16 addr, u64 *cnt)
+{
+	u32 data;
+	u16 ctrl_addr;
+	u8 check;
+	int timeout;
+
+	ctrl_addr = addr + SWITCH_COUNTER_NUM * port;
+
+	sw->ops->acquire(sw);
+
+	ctrl_addr |= IND_ACC_TABLE(TABLE_MIB | TABLE_READ);
+	sw->reg->w16(sw, REG_IND_CTRL_0, ctrl_addr);
+
+	for (timeout = 2; timeout > 0; timeout--) {
+		check = sw->reg->r8(sw, REG_IND_MIB_CHECK);
+
+		if (check & MIB_COUNTER_VALID) {
+			data = sw->reg->r32(sw, REG_IND_DATA_LO);
+			if (check & MIB_COUNTER_OVERFLOW)
+				*cnt += MIB_COUNTER_VALUE + 1;
+			*cnt += data & MIB_COUNTER_VALUE;
+			break;
+		}
+	}
+
+	sw->ops->release(sw);
+}  /* port_r_mib_cnt */
+
+/**
+ * port_r_mib_pkt - read dropped packet counts
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ * @cnt:	Buffer to store the receive and transmit dropped packet counts.
+ *
+ * This routine reads the dropped packet counts of the port.
+ * Hardware is locked to minimize corruption of read data.
+ */
+static void port_r_mib_pkt(struct ksz_sw *sw, uint port, u16 addr, u32 *last,
+	u64 *cnt)
+{
+	u32 cur;
+	u32 data;
+	u16 ctrl_addr;
+
+	addr -= SWITCH_COUNTER_NUM;
+	ctrl_addr = addr ? KS_MIB_PACKET_DROPPED_TX_0 :
+		KS_MIB_PACKET_DROPPED_RX_0;
+	ctrl_addr += port;
+
+	sw->ops->acquire(sw);
+
+	ctrl_addr |= IND_ACC_TABLE(TABLE_MIB | TABLE_READ);
+	sw->reg->w16(sw, REG_IND_CTRL_0, ctrl_addr);
+
+	data = sw->reg->r32(sw, REG_IND_DATA_LO);
+	sw->ops->release(sw);
+	data &= MIB_PACKET_DROPPED;
+	cur = last[addr];
+	if (data != cur) {
+		last[addr] = data;
+		if (data < cur)
+			data += MIB_PACKET_DROPPED + 1;
+		data -= cur;
+		*cnt += data;
+	}
+}  /* port_r_mib_pkt */
+
+static int exit_mib_read(struct ksz_sw *sw)
+{
+	if (sw->intr_using)
+		return true;
+	return false;
+}  /* exit_mib_read */
+
+/**
+ * port_r_cnt - read MIB counters periodically
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ *
+ * This routine is used to read the counters of the port periodically to avoid
+ * counter overflow.  The hardware should be acquired first before calling this
+ * routine.
+ *
+ * Return non-zero when not all counters not read.
+ */
+static int port_r_cnt(struct ksz_sw *sw, uint port)
+{
+	struct ksz_port_mib *mib = get_port_mib(sw, port);
+
+	if (mib->mib_start < SWITCH_COUNTER_NUM)
+		while (mib->cnt_ptr < SWITCH_COUNTER_NUM) {
+			if (exit_mib_read(sw))
+				return mib->cnt_ptr;
+			++mib->read_cnt[mib->cnt_ptr];
+			if (mib->read_cnt[mib->cnt_ptr] >=
+					mib->read_max[mib->cnt_ptr]) {
+				mib->read_cnt[mib->cnt_ptr] = 0;
+				port_r_mib_cnt(sw, port, mib->cnt_ptr,
+					&mib->counter[mib->cnt_ptr]);
+			}
+			++mib->cnt_ptr;
+		}
+	if (sw->mib_cnt > SWITCH_COUNTER_NUM)
+		while (mib->cnt_ptr < TOTAL_SWITCH_COUNTER_NUM) {
+			++mib->read_cnt[mib->cnt_ptr];
+			if (mib->read_cnt[mib->cnt_ptr] >=
+					mib->read_max[mib->cnt_ptr]) {
+				mib->read_cnt[mib->cnt_ptr] = 0;
+				port_r_mib_pkt(sw, port, mib->cnt_ptr,
+					mib->dropped,
+					&mib->counter[mib->cnt_ptr]);
+			}
+			++mib->cnt_ptr;
+		}
+	mib->cnt_ptr = 0;
+	return 0;
+}  /* port_r_cnt */
+
+/**
+ * sw_cfg_mib_counter_ctrl - configure MIB counter control
+ * @sw:		The switch instance.
+ * @ctrl:	The control.
+ * @port:	The port index.
+ *
+ * This routine configures the MIB counter control for flush or freeze.
+ */
+static void sw_cfg_mib_counter_ctrl(struct ksz_sw *sw, int ctrl, uint port)
+{
+	uint count;
+	uint start;
+	uint stop;
+
+	if (port < sw->port_cnt) {
+		start = get_log_port(sw, port);
+		stop = start + 1;
+	} else {
+		start = 0;
+		stop = sw->mib_port_cnt + 1;
+	}
+	if (ctrl != 2)
+		for (count = start; count < stop; count++) {
+			struct ksz_port_mib *mib;
+
+			port = get_phy_port(sw, count);
+			mib = get_port_mib(sw, port);
+			memset((void *) mib->counter, 0, sizeof(u64) *
+				TOTAL_SWITCH_COUNTER_NUM);
+			mib->rate[0].last = mib->rate[1].last = 0;
+			mib->rate[0].last_cnt = mib->rate[1].last_cnt = 0;
+			mib->rate[0].peak = mib->rate[1].peak = 0;
+		}
+}  /* sw_cfg_mib_counter_ctrl */
+
+/**
+ * port_init_cnt - initialize MIB counter values
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ *
+ * This routine is used to initialize all counters to zero if the hardware
+ * cannot do it after reset.
+ */
+static inline void port_init_cnt(struct ksz_sw *sw, uint port)
+{
+	struct ksz_port_mib *mib = get_port_mib(sw, port);
+
+	mib->cnt_ptr = 0;
+	if (mib->mib_start < SWITCH_COUNTER_NUM)
+		do {
+			mib->read_cnt[mib->cnt_ptr] = 0;
+			mib->read_max[mib->cnt_ptr] =
+				mib_read_max[mib->cnt_ptr];
+			port_r_mib_cnt(sw, port, mib->cnt_ptr,
+				&mib->counter[mib->cnt_ptr]);
+			++mib->cnt_ptr;
+		} while (mib->cnt_ptr < SWITCH_COUNTER_NUM);
+	if (sw->mib_cnt > SWITCH_COUNTER_NUM)
+		do {
+			mib->read_cnt[mib->cnt_ptr] = 0;
+			mib->read_max[mib->cnt_ptr] =
+				mib_read_max[mib->cnt_ptr];
+			port_r_mib_pkt(sw, port, mib->cnt_ptr, mib->dropped,
+				&mib->counter[mib->cnt_ptr]);
+			++mib->cnt_ptr;
+		} while (mib->cnt_ptr < TOTAL_SWITCH_COUNTER_NUM);
+	memset((void *) mib->counter, 0, sizeof(u64) *
+		TOTAL_SWITCH_COUNTER_NUM);
+	mib->cnt_ptr = 0;
+	mib->rate[0].last = mib->rate[1].last = 0;
+	mib->rate[0].last_cnt = mib->rate[1].last_cnt = 0;
+	mib->rate[0].peak = mib->rate[1].peak = 0;
+}  /* port_init_cnt */
+
+/* -------------------------------------------------------------------------- */
+
+/*
+ * Port functions
+ */
+
+/**
+ * port_chk - check port register bits
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ * @offset:	The offset of the port register.
+ * @bits:	The data bits to check.
+ *
+ * This function checks whether the specified bits of the port register are set
+ * or not.
+ *
+ * Return 0 if the bits are not set.
+ */
+static int port_chk(struct ksz_sw *sw, uint port, uint offset, SW_D bits)
+{
+	u32 addr;
+	SW_D data;
+
+	PORT_CTRL_ADDR(port, addr);
+	addr += offset;
+	data = SW_R(sw, addr);
+	return (data & bits) == bits;
+}  /* port_chk */
+
+/**
+ * port_cfg - set port register bits
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ * @offset:	The offset of the port register.
+ * @bits:	The data bits to set.
+ * @set:	The flag indicating whether the bits are to be set or not.
+ *
+ * This routine sets or resets the specified bits of the port register.
+ */
+static void port_cfg(struct ksz_sw *sw, uint port, uint offset, SW_D bits,
+	bool set)
+{
+	u32 addr;
+	SW_D data;
+
+	PORT_CTRL_ADDR(port, addr);
+	addr += offset;
+	data = SW_R(sw, addr);
+	if (set)
+		data |= bits;
+	else
+		data &= ~bits;
+	SW_W(sw, addr, data);
+}  /* port_cfg */
+
+/**
+ * port_r8 - read byte from port register
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ * @offset:	The offset of the port register.
+ * @data:	Buffer to store the data.
+ *
+ * This routine reads a byte from the port register.
+ */
+static void port_r8(struct ksz_sw *sw, uint port, uint offset, u8 *data)
+{
+	u32 addr;
+
+	PORT_CTRL_ADDR(port, addr);
+	addr += offset;
+	*data = sw->reg->r8(sw, addr);
+}  /* port_r8 */
+
+/**
+ * port_w8 - write byte to port register
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ * @offset:	The offset of the port register.
+ * @data:	Data to write.
+ *
+ * This routine writes a byte to the port register.
+ */
+static void port_w8(struct ksz_sw *sw, uint port, uint offset, u8 data)
+{
+	u32 addr;
+
+	PORT_CTRL_ADDR(port, addr);
+	addr += offset;
+	sw->reg->w8(sw, addr, data);
+}  /* port_w8 */
+
+/**
+ * port_r16 - read word from port register.
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ * @offset:	The offset of the port register.
+ * @data:	Buffer to store the data.
+ *
+ * This routine reads a word from the port register.
+ */
+static void port_r16(struct ksz_sw *sw, uint port, uint offset, u16 *data)
+{
+	u32 addr;
+
+	PORT_CTRL_ADDR(port, addr);
+	addr += offset;
+	*data = sw->reg->r16(sw, addr);
+}  /* port_r16 */
+
+/**
+ * port_w16 - write word to port register.
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ * @offset:	The offset of the port register.
+ * @data:	Data to write.
+ *
+ * This routine writes a word to the port register.
+ */
+static void port_w16(struct ksz_sw *sw, uint port, uint offset, u16 data)
+{
+	u32 addr;
+
+	PORT_CTRL_ADDR(port, addr);
+	addr += offset;
+	sw->reg->w16(sw, addr, data);
+}  /* port_w16 */
+
+/**
+ * sw_chk - check switch register bits
+ * @sw:		The switch instance.
+ * @addr:	The address of the switch register.
+ * @bits:	The data bits to check.
+ *
+ * This function checks whether the specified bits of the switch register are
+ * set or not.
+ *
+ * Return 0 if the bits are not set.
+ */
+static int sw_chk(struct ksz_sw *sw, u32 addr, SW_D bits)
+{
+	SW_D data;
+
+	data = SW_R(sw, addr);
+	return (data & bits) == bits;
+}  /* sw_chk */
+
+/**
+ * sw_cfg - set switch register bits
+ * @sw:		The switch instance.
+ * @addr:	The address of the switch register.
+ * @bits:	The data bits to set.
+ * @set:	The flag indicating whether the bits are to be set or not.
+ *
+ * This function sets or resets the specified bits of the switch register.
+ */
+static void sw_cfg(struct ksz_sw *sw, u32 addr, SW_D bits, bool set)
+{
+	SW_D data;
+
+	data = SW_R(sw, addr);
+	if (set)
+		data |= bits;
+	else
+		data &= ~bits;
+	SW_W(sw, addr, data);
+}  /* sw_cfg */
+
+/* -------------------------------------------------------------------------- */
+
+/* Bandwidth */
+
+static inline void port_cfg_broad_storm(struct ksz_sw *sw, uint p, bool set)
+{
+	port_cfg(sw, p,
+		P_BCAST_STORM_CTRL, PORT_BROADCAST_STORM, set);
+}
+
+static inline int port_chk_broad_storm(struct ksz_sw *sw, uint p)
+{
+	return port_chk(sw, p,
+		P_BCAST_STORM_CTRL, PORT_BROADCAST_STORM);
+}
+
+/* Driver set switch broadcast storm protection at 10% rate. */
+#define BROADCAST_STORM_PROTECTION_RATE	10
+
+/* 148,800 frames * 50 ms / 100 */
+#define BROADCAST_STORM_VALUE		7440
+
+/**
+ * sw_cfg_broad_storm - configure broadcast storm threshold
+ * @sw:		The switch instance.
+ * @percent:	Broadcast storm threshold in percent of transmit rate.
+ *
+ * This routine configures the broadcast storm threshold of the switch.
+ */
+static void sw_cfg_broad_storm(struct ksz_sw *sw, u8 percent)
+{
+	u16 data;
+	u32 value = ((u32) BROADCAST_STORM_VALUE * (u32) percent / 100);
+
+	if (value > BROADCAST_STORM_RATE)
+		value = BROADCAST_STORM_RATE;
+
+	data = sw->reg->r16(sw, S_REPLACE_VID_CTRL);
+	data &= ~BROADCAST_STORM_RATE;
+	data |= value;
+	sw->reg->w16(sw, S_REPLACE_VID_CTRL, data);
+}  /* sw_cfg_broad_storm */
+
+/**
+ * sw_get_board_storm - get broadcast storm threshold
+ * @sw:		The switch instance.
+ * @percent:	Buffer to store the broadcast storm threshold percentage.
+ *
+ * This routine retrieves the broadcast storm threshold of the switch.
+ */
+static void sw_get_broad_storm(struct ksz_sw *sw, u8 *percent)
+{
+	int num;
+	u16 data;
+
+	data = sw->reg->r16(sw, S_REPLACE_VID_CTRL);
+	num = (data & BROADCAST_STORM_RATE);
+	num = (num * 100 + BROADCAST_STORM_VALUE / 2) / BROADCAST_STORM_VALUE;
+	*percent = (u8) num;
+}  /* sw_get_broad_storm */
+
+/**
+ * sw_dis_broad_storm - disable broadcast storm protection
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ *
+ * This routine disables the broadcast storm limit function of the switch.
+ */
+static void sw_dis_broad_storm(struct ksz_sw *sw, uint port)
+{
+	port_cfg_broad_storm(sw, port, 0);
+}  /* sw_dis_broad_storm */
+
+/**
+ * sw_ena_broad_storm - enable broadcast storm protection
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ *
+ * This routine enables the broadcast storm limit function of the switch.
+ */
+static void sw_ena_broad_storm(struct ksz_sw *sw, uint port)
+{
+	sw_cfg_broad_storm(sw, sw->info->broad_per);
+	port_cfg_broad_storm(sw, port, 1);
+}  /* sw_ena_broad_storm */
+
+/**
+ * sw_init_broad_storm - initialize broadcast storm
+ * @sw:		The switch instance.
+ *
+ * This routine initializes the broadcast storm limit function of the switch.
+ */
+static void sw_init_broad_storm(struct ksz_sw *sw)
+{
+	u8 percent;
+
+	sw_get_broad_storm(sw, &percent);
+	sw->info->broad_per = percent;
+}  /* sw_init_broad_storm */
+
+/**
+ * hw_cfg_broad_storm - configure broadcast storm
+ * @sw:		The switch instance.
+ * @percent:	Broadcast storm threshold in percent of transmit rate.
+ *
+ * This routine configures the broadcast storm threshold of the switch.
+ * It is called by user functions.  The hardware should be acquired first.
+ */
+static void hw_cfg_broad_storm(struct ksz_sw *sw, u8 percent)
+{
+	if (percent > 100)
+		percent = 100;
+
+	sw_cfg_broad_storm(sw, percent);
+	sw_init_broad_storm(sw);
+}  /* hw_cfg_broad_storm */
+
+/**
+ * sw_setup_broad_storm - setup broadcast storm
+ * @sw:		The switch instance.
+ *
+ * This routine setup the broadcast storm limit function of the switch.
+ */
+static void sw_setup_broad_storm(struct ksz_sw *sw)
+{
+	uint n;
+	uint port;
+
+	/* Enable switch broadcast storm protection at 10% percent rate. */
+	hw_cfg_broad_storm(sw, BROADCAST_STORM_PROTECTION_RATE);
+	for (n = 0; n <= sw->mib_port_cnt; n++) {
+		port = get_phy_port(sw, n);
+		sw_ena_broad_storm(sw, port);
+	}
+	sw_cfg(sw, REG_SW_CTRL_2, MULTICAST_STORM_DISABLE, 1);
+}  /* sw_setup_broad_storm */
+
+/* -------------------------------------------------------------------------- */
+
+/* Rate Control */
+
+/**
+ * hw_cfg_rate_ctrl - configure port rate control
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ * @prio:	The priority index to configure.
+ * @ctrl:	The flag indicating whether the rate control bit is set or not.
+ *
+ * This routine configures the priority rate control of the port.
+ */
+static void hw_cfg_rate_ctrl(struct ksz_sw *sw, uint port, int prio, int ctrl)
+{
+	int offset;
+	u8 data;
+	u8 saved;
+
+	offset = REG_PORT_RATE_CTRL_0 - prio;
+	port_r8(sw, port, offset, &data);
+	saved = data;
+	data &= ~RATE_CTRL_ENABLE;
+	if (ctrl)
+		data |= RATE_CTRL_ENABLE;
+	if (data != saved)
+		port_w8(sw, port, offset, data);
+	sw->info->port_cfg[port].rate_ctrl[prio] = data;
+}  /* hw_cfg_rate_ctrl */
+
+/**
+ * hw_cfg_rate_ratio - configure port rate ratio
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ * @prio:	The priority index to configure.
+ * @ratio:	The rate ratio.
+ *
+ * This routine configures the priority rate ratio of the port.
+ */
+static void hw_cfg_rate_ratio(struct ksz_sw *sw, uint port, int prio, u8 ratio)
+{
+	int offset;
+	u8 data;
+	u8 saved;
+
+	if (ratio >= RATE_CTRL_ENABLE)
+		return;
+
+	offset = REG_PORT_RATE_CTRL_0 - prio;
+	port_r8(sw, port, offset, &data);
+	saved = data;
+	data &= RATE_CTRL_ENABLE;
+	data |= ratio;
+	if (data != saved)
+		port_w8(sw, port, offset, data);
+	sw->info->port_cfg[port].rate_ctrl[prio] = data;
+}  /* hw_cfg_rate_ratio */
+
+/**
+ * hw_get_rate_ctrl - get port rate control
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ * @prio:	The priority index to retrieve.
+ *
+ * This routine retrieves the priority rate control of the port.
+ */
+static void hw_get_rate_ctrl(struct ksz_sw *sw, uint port, int prio)
+{
+	int offset;
+	u8 data;
+
+	offset = REG_PORT_RATE_CTRL_0 - prio;
+	port_r8(sw, port, offset, &data);
+	sw->info->port_cfg[port].rate_ctrl[prio] = data;
+}  /* hw_get_rate_ctrl */
+
+/* -------------------------------------------------------------------------- */
+
+/* Rate Limit */
+
+/**
+ * hw_cfg_rate_limit - configure port rate limit modes
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ * @mask:	The mask value.
+ * @shift:	The shift position.
+ * @mode:	The rate limit mode.
+ *
+ * This helper routine configures the rate limit modes of the port.
+ */
+static void hw_cfg_rate_limit(struct ksz_sw *sw, uint port, u8 mask, u8 shift,
+	u8 mode)
+{
+	u8 data;
+	u8 saved;
+
+	port_r8(sw, port, P_RATE_LIMIT_CTRL, &data);
+	saved = data;
+	data &= ~(mask << shift);
+	data |= mode << shift;
+	if (data != saved)
+		port_w8(sw, port, P_RATE_LIMIT_CTRL, data);
+	sw->info->port_cfg[port].rate_limit = data;
+}  /* hw_cfg_rate_limit */
+
+static void hw_cfg_in_flow_ctrl(struct ksz_sw *sw, uint port, bool set)
+{
+	hw_cfg_rate_limit(sw, port, 1, PORT_IN_FLOW_CTRL_S, set);
+}
+
+/**
+ * hw_cfg_cnt_ifg - configure port rate limit count IFG control
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ * @set:	The flag indicating whether the count control is set or not.
+ *
+ * This routine configures the rate limit count IFG control of the port.
+ */
+static void hw_cfg_cnt_ifg(struct ksz_sw *sw, uint port, bool set)
+{
+	hw_cfg_rate_limit(sw, port, 1, PORT_COUNT_IFG_S, set);
+}  /* hw_cfg_cnt_ifg */
+
+/**
+ * hw_cfg_cnt_pre - configure port rate limit count preamble control
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ * @set:	The flag indicating whether the count control is set or not.
+ *
+ * This routine configures the rate limit count preamble control of the port.
+ */
+static void hw_cfg_cnt_pre(struct ksz_sw *sw, uint port, bool set)
+{
+	hw_cfg_rate_limit(sw, port, 1, PORT_COUNT_PREAMBLE_S, set);
+}  /* hw_cfg_cnt_pre */
+
+/**
+ * hw_cfg_rx_limit - configure port rate limit mode
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ * @mode:	The rate limit mode.
+ *
+ * This routine configures the rate limit mode of the port.
+ */
+static void hw_cfg_rx_limit(struct ksz_sw *sw, uint port, u8 mode)
+{
+	if (mode > PORT_IN_LIMIT_MODE_M)
+		return;
+
+	hw_cfg_rate_limit(sw, port, PORT_IN_LIMIT_MODE_M,
+		PORT_IN_LIMIT_MODE_S, mode);
+}  /* hw_cfg_rx_limit */
+
+/**
+ * hw_get_rate_limit - get port rate limit control
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ *
+ * This routine retrieves the rate limit of the port.
+ */
+static void hw_get_rate_limit(struct ksz_sw *sw, uint port)
+{
+	u8 data;
+
+	port_r8(sw, port, P_RATE_LIMIT_CTRL, &data);
+	sw->info->port_cfg[port].rate_limit = data;
+}  /* hw_get_rate_limit */
+
+/* -------------------------------------------------------------------------- */
+
+static uint get_rate_from_val(u8 val)
+{
+	uint i;
+
+	if (0 == val)
+		i = 0;
+	else if (val <= 100)
+		i = 1000 * val;
+	else
+		i = 64 * (val - 100);
+	return i;
+}
+
+static int get_rate_to_val(uint rate)
+{
+	int i;
+
+	if (rate >= 1000) {
+		i = (rate + 500) / 1000;
+		if (i > 100)
+			i = 100;
+	} else if (0 == rate) {
+		i = 0;
+	} else {
+		i = (rate + 32) / 64;
+		if (0 == i)
+			i = 1;
+		else if (i > 15)
+			i = 15;
+		i += 100;
+	}
+	return i;
+}
+
+/**
+ * port_cfg_rate - configure port priority rate
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ * @prio:	The priority index to configure.
+ * @offset:	The receive or transmit rate offset.
+ * @shift:	The shift position to set the value.
+ * @rate:	The rate limit in number of Kbps.
+ *
+ * This helper routine configures the priority rate of the port.
+ */
+static void port_cfg_rate(struct ksz_sw *sw, uint port, uint prio, uint offset,
+	uint rate)
+{
+	u8 factor;
+
+	factor = (u8) get_rate_to_val(rate);
+
+	port_w8(sw, port, offset + prio, factor);
+}  /* port_cfg_rate */
+
+/**
+ * port_get_rate - get port priority rate
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ * @prio:	The priority index to configure.
+ * @offset:	The receive or transmit rate offset.
+ * @shift:	The shift position to get the value.
+ * @rate:	Buffer to store the data rate in number of Kbps.
+ *
+ * This helper routine retrieves the priority rate of the port.
+ */
+static void port_get_rate(struct ksz_sw *sw, uint port, uint prio, uint offset,
+	uint *rate)
+{
+	u8 data;
+
+	port_r8(sw, port, offset + prio, &data);
+	*rate = get_rate_from_val(data);
+}  /* port_get_rate */
+
+/**
+ * hw_cfg_prio_rate - configure port priority
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ * @prio:	The priority index to configure.
+ * @rate:	The rate limit in number of Kbps.
+ * @offset:	The receive or transmit rate offset.
+ * @result:	Buffer to store the data rate in number of Kbps.
+ *
+ * This helper routine configures the priority rate of the port and retrieves
+ * the actual rate number.
+ */
+static void hw_cfg_prio_rate(struct ksz_sw *sw, uint port, int prio, uint rate,
+	int offset, uint *result)
+{
+	port_cfg_rate(sw, port, prio, offset, rate);
+	port_get_rate(sw, port, prio, offset, result);
+}  /* hw_cfg_prio_rate */
+
+/**
+ * hw_cfg_rx_prio_rate - configure port receive priority
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ * @prio:	The priority index to configure.
+ * @rate:	The rate limit in number of Kbps.
+ *
+ * This routine configures the receive priority rate of the port.
+ * It is called by user functions.  The hardware should be acquired first.
+ */
+static void hw_cfg_rx_prio_rate(struct ksz_sw *sw, uint port, int prio,
+	uint rate)
+{
+	hw_cfg_prio_rate(sw, port, prio, rate,
+		REG_PORT_IN_RATE_0,
+		&sw->info->port_cfg[port].rx_rate[prio]);
+}  /* hw_cfg_rx_prio_rate */
+
+/**
+ * hw_cfg_tx_prio_rate - configure port transmit priority
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ * @prio:	The priority index to configure.
+ * @rate:	The rate limit in number of Kbps.
+ *
+ * This routine configures the transmit priority rate of the port.
+ * It is called by user functions.  The hardware should be acquired first.
+ */
+static void hw_cfg_tx_prio_rate(struct ksz_sw *sw, uint port, int prio,
+	uint rate)
+{
+	hw_cfg_prio_rate(sw, port, prio, rate,
+		REG_PORT_OUT_RATE_0,
+		&sw->info->port_cfg[port].tx_rate[prio]);
+}  /* hw_cfg_tx_prio_rate */
+
+/**
+ * sw_chk_rx_prio_rate - check switch rx priority rate
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ *
+ * This function checks whether the rx priority rate function of the switch is
+ * enabled.
+ *
+ * Return 0 if not enabled.
+ */
+static int sw_chk_rx_prio_rate(struct ksz_sw *sw, uint port)
+{
+	u32 rate_addr;
+	u32 in_rate;
+
+	PORT_CTRL_ADDR(port, rate_addr);
+	rate_addr += REG_PORT_IN_RATE_0;
+	in_rate = sw->reg->r32(sw, rate_addr);
+	return in_rate != 0;
+}  /* sw_chk_rx_prio_rate */
+
+/**
+ * sw_dis_rx_prio_rate - disable switch rx priority rate
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ *
+ * This routine disables the rx priority rate function of the switch.
+ */
+static void sw_dis_rx_prio_rate(struct ksz_sw *sw, uint port)
+{
+	u32 rate_addr;
+
+	PORT_CTRL_ADDR(port, rate_addr);
+	rate_addr += REG_PORT_IN_RATE_0;
+	sw->reg->w32(sw, rate_addr, 0);
+}  /* sw_dis_rx_prio_rate */
+
+/**
+ * sw_ena_rx_prio_rate - enable switch rx priority rate
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ *
+ * This routine enables the rx priority rate function of the switch.
+ */
+static void sw_ena_rx_prio_rate(struct ksz_sw *sw, uint port)
+{
+	int prio;
+
+	for (prio = 0; prio < PRIO_QUEUES; prio++) {
+		hw_cfg_rx_prio_rate(sw, port, prio,
+			sw->info->port_cfg[port].rx_rate[prio]);
+	}
+}  /* sw_ena_rx_prio_rate */
+
+/**
+ * sw_chk_tx_prio_rate - check switch tx priority rate
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ *
+ * This function checks whether the tx priority rate function of the switch is
+ * enabled.
+ *
+ * Return 0 if not enabled.
+ */
+static int sw_chk_tx_prio_rate(struct ksz_sw *sw, uint port)
+{
+	u32 rate_addr;
+	u32 out_rate;
+
+	PORT_CTRL_ADDR(port, rate_addr);
+	rate_addr += REG_PORT_OUT_RATE_0;
+	if (sw_chk(sw, REG_SW_CTRL_19, SW_OUT_RATE_LIMIT_QUEUE_BASED))
+		out_rate = sw->reg->r32(sw, rate_addr);
+
+	/* Only need to check first priority as the others do not matter. */
+	else
+		out_rate = sw->reg->r8(sw, rate_addr);
+	return out_rate != 0;
+}  /* sw_chk_tx_prio_rate */
+
+/**
+ * sw_dis_tx_prio_rate - disable switch tx priority rate
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ *
+ * This routine disables the tx priority rate function of the switch.
+ */
+static void sw_dis_tx_prio_rate(struct ksz_sw *sw, uint port)
+{
+	u32 rate_addr;
+
+	PORT_CTRL_ADDR(port, rate_addr);
+	rate_addr += REG_PORT_OUT_RATE_0;
+	sw->reg->w32(sw, rate_addr, 0);
+}  /* sw_dis_tx_prio_rate */
+
+/**
+ * sw_ena_tx_prio_rate - enable switch tx priority rate
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ *
+ * This routine enables the tx priority rate function of the switch.
+ */
+static void sw_ena_tx_prio_rate(struct ksz_sw *sw, uint port)
+{
+	int prio;
+
+	for (prio = 0; prio < PRIO_QUEUES; prio++) {
+		hw_cfg_tx_prio_rate(sw, port, prio,
+			sw->info->port_cfg[port].tx_rate[prio]);
+	}
+}  /* sw_ena_tx_prio_rate */
+
+/**
+ * sw_init_prio_rate - initialize switch prioirty rate
+ * @sw:		The switch instance.
+ *
+ * This routine initializes the priority rate function of the switch.
+ */
+static void sw_init_prio_rate(struct ksz_sw *sw)
+{
+	uint n;
+	uint offset;
+	uint port;
+	uint prio;
+	struct ksz_port_cfg *cfg;
+
+	for (n = 0; n <= sw->mib_port_cnt; n++) {
+		port = get_phy_port(sw, n);
+		cfg = get_port_cfg(sw, port);
+		hw_get_rate_limit(sw, port);
+		for (prio = 0; prio < PRIO_QUEUES; prio++) {
+			hw_get_rate_ctrl(sw, port, prio);
+			offset = REG_PORT_IN_RATE_0;
+			port_get_rate(sw, port, prio, offset,
+				&cfg->rx_rate[prio]);
+			offset = REG_PORT_OUT_RATE_0;
+			port_get_rate(sw, port, prio, offset,
+				&cfg->tx_rate[prio]);
+		}
+	}
+}  /* sw_init_prio_rate */
+
+/* -------------------------------------------------------------------------- */
+
+/* Communication */
+
+static inline void port_cfg_back_pressure(struct ksz_sw *sw, uint p, bool set)
+{
+	port_cfg(sw, p,
+		P_STP_CTRL, PORT_BACK_PRESSURE, set);
+}
+
+static inline void port_cfg_force_flow_ctrl(struct ksz_sw *sw, uint p, bool set)
+{
+	port_cfg(sw, p,
+		P_STP_CTRL, PORT_FORCE_FLOW_CTRL, set);
+}
+
+static inline int port_chk_back_pressure(struct ksz_sw *sw, uint p)
+{
+	return port_chk(sw, p,
+		P_STP_CTRL, PORT_BACK_PRESSURE);
+}
+
+static inline int port_chk_force_flow_ctrl(struct ksz_sw *sw, uint p)
+{
+	return port_chk(sw, p,
+		P_STP_CTRL, PORT_FORCE_FLOW_CTRL);
+}
+
+/* -------------------------------------------------------------------------- */
+
+/* Spanning Tree */
+
+static inline void port_cfg_dis_learn(struct ksz_sw *sw, uint p, bool set)
+{
+	port_cfg(sw, p,
+		P_STP_CTRL, PORT_LEARN_DISABLE, set);
+}
+
+static inline void port_cfg_rx(struct ksz_sw *sw, uint p, bool set)
+{
+	port_cfg(sw, p,
+		P_STP_CTRL, PORT_RX_ENABLE, set);
+	if (set)
+		sw->rx_ports |= (1 << p);
+	else
+		sw->rx_ports &= ~(1 << p);
+}
+
+static inline void port_cfg_tx(struct ksz_sw *sw, uint p, bool set)
+{
+	port_cfg(sw, p,
+		P_STP_CTRL, PORT_TX_ENABLE, set);
+	if (set)
+		sw->tx_ports |= (1 << p);
+	else
+		sw->tx_ports &= ~(1 << p);
+}
+
+static inline int port_chk_dis_learn(struct ksz_sw *sw, uint p)
+{
+	return port_chk(sw, p,
+		P_STP_CTRL, PORT_LEARN_DISABLE);
+}
+
+static inline int port_chk_rx(struct ksz_sw *sw, uint p)
+{
+	return port_chk(sw, p,
+		P_STP_CTRL, PORT_RX_ENABLE);
+}
+
+static inline int port_chk_tx(struct ksz_sw *sw, uint p)
+{
+	return port_chk(sw, p,
+		P_STP_CTRL, PORT_TX_ENABLE);
+}
+
+static inline void sw_cfg_fast_aging(struct ksz_sw *sw, bool set)
+{
+	sw_cfg(sw, REG_SW_CTRL_1, SW_FAST_AGING, set);
+}
+
+static void sw_flush_dyn_mac_table(struct ksz_sw *sw, uint port)
+{
+	uint cnt;
+	uint first;
+	uint index;
+	int learn_disable[TOTAL_PORT_NUM];
+
+	if (port < sw->port_cnt) {
+		first = get_log_port(sw, port);
+		cnt = first + 1;
+	} else {
+		first = 0;
+		cnt = sw->mib_port_cnt + 1;
+	}
+	for (index = first; index < cnt; index++) {
+		port = get_phy_port(sw, index);
+		learn_disable[port] = port_chk_dis_learn(sw, port);
+		if (!learn_disable[port])
+			port_cfg_dis_learn(sw, port, 1);
+	}
+	sw_cfg(sw, S_FLUSH_TABLE_CTRL, SW_FLUSH_DYN_MAC_TABLE, 1);
+	for (index = first; index < cnt; index++) {
+		port = get_phy_port(sw, index);
+		if (!learn_disable[port])
+			port_cfg_dis_learn(sw, port, 0);
+	}
+}  /* sw_flush_dyn_mac_table */
+
+/* -------------------------------------------------------------------------- */
+
+/* VLAN */
+
+static inline void port_cfg_ins_tag(struct ksz_sw *sw, uint p, bool insert)
+{
+	port_cfg(sw, p,
+		P_TAG_CTRL, PORT_INSERT_TAG, insert);
+}
+
+static inline void port_cfg_rmv_tag(struct ksz_sw *sw, uint p, bool remove)
+{
+	port_cfg(sw, p,
+		P_TAG_CTRL, PORT_REMOVE_TAG, remove);
+}
+
+static inline int port_chk_ins_tag(struct ksz_sw *sw, uint p)
+{
+	return port_chk(sw, p,
+		P_TAG_CTRL, PORT_INSERT_TAG);
+}
+
+static inline int port_chk_rmv_tag(struct ksz_sw *sw, uint p)
+{
+	return port_chk(sw, p,
+		P_TAG_CTRL, PORT_REMOVE_TAG);
+}
+
+static inline void port_cfg_dis_non_vid(struct ksz_sw *sw, uint p, bool set)
+{
+	port_cfg(sw, p,
+		P_STP_CTRL, PORT_DISCARD_NON_VID, set);
+}
+
+static inline void port_cfg_drop_tag(struct ksz_sw *sw, uint p, bool set)
+{
+	port_cfg(sw, p,
+		P_DROP_TAG_CTRL, PORT_DROP_TAG, set);
+}
+
+static inline void port_cfg_in_filter(struct ksz_sw *sw, uint p, bool set)
+{
+	port_cfg(sw, p,
+		P_STP_CTRL, PORT_INGRESS_FILTER, set);
+}
+
+static inline int port_chk_dis_non_vid(struct ksz_sw *sw, uint p)
+{
+	return port_chk(sw, p,
+		P_STP_CTRL, PORT_DISCARD_NON_VID);
+}
+
+static inline int port_chk_drop_tag(struct ksz_sw *sw, uint p)
+{
+	return port_chk(sw, p,
+		P_DROP_TAG_CTRL, PORT_DROP_TAG);
+}
+
+static inline int port_chk_in_filter(struct ksz_sw *sw, uint p)
+{
+	return port_chk(sw, p,
+		P_STP_CTRL, PORT_INGRESS_FILTER);
+}
+
+/* -------------------------------------------------------------------------- */
+
+/* Mirroring */
+
+static inline void port_cfg_mirror_sniffer(struct ksz_sw *sw, uint p, bool set)
+{
+	port_cfg(sw, p,
+		P_MIRROR_CTRL, PORT_MIRROR_SNIFFER, set);
+}
+
+static inline void port_cfg_mirror_rx(struct ksz_sw *sw, uint p, bool set)
+{
+	port_cfg(sw, p,
+		P_MIRROR_CTRL, PORT_MIRROR_RX, set);
+}
+
+static inline void port_cfg_mirror_tx(struct ksz_sw *sw, uint p, bool set)
+{
+	port_cfg(sw, p,
+		P_MIRROR_CTRL, PORT_MIRROR_TX, set);
+}
+
+static inline void sw_cfg_mirror_rx_tx(struct ksz_sw *sw, bool set)
+{
+	sw_cfg(sw, S_MIRROR_CTRL, SW_MIRROR_RX_TX, set);
+}
+
+static inline int port_chk_mirror_sniffer(struct ksz_sw *sw, uint p)
+{
+	return port_chk(sw, p,
+		P_MIRROR_CTRL, PORT_MIRROR_SNIFFER);
+}
+
+static inline int port_chk_mirror_rx(struct ksz_sw *sw, uint p)
+{
+	return port_chk(sw, p,
+		P_MIRROR_CTRL, PORT_MIRROR_RX);
+}
+
+static inline int port_chk_mirror_tx(struct ksz_sw *sw, uint p)
+{
+	return port_chk(sw, p,
+		P_MIRROR_CTRL, PORT_MIRROR_TX);
+}
+
+static inline int sw_chk_mirror_rx_tx(struct ksz_sw *sw)
+{
+	return sw_chk(sw, S_MIRROR_CTRL, SW_MIRROR_RX_TX);
+}
+
+static void sw_setup_mirror(struct ksz_sw *sw)
+{
+	uint n;
+	uint port;
+
+	/*
+	 * The mirror sniffer port requires it to be in the port membership
+	 * of the receive and transmit ports.
+	 * For example, port 3 is the mirror port of traffic between ports 1
+	 * and 2.  Port 3 needs only to turn sniffer on; its port membership
+	 * can be 0.  Ordinarily the port membership of ports 1 and 2 is 3 for
+	 * just commnunicating with eath other.  It has to be set to 7 to pass
+	 * the frames to port 3.  Only one of the ports needs to turn on
+	 * receive and transmit mirroring.
+	 * The mirror receive and transmit mode requires at least two ports to
+	 * turn on receive and transmit mirroring.
+	 */
+	for (n = 0; n <= sw->mib_port_cnt; n++) {
+		port = get_phy_port(sw, n);
+		port_cfg_mirror_sniffer(sw, port, 0);
+		port_cfg_mirror_rx(sw, port, 0);
+		port_cfg_mirror_tx(sw, port, 0);
+	}
+	sw_cfg_mirror_rx_tx(sw, 0);
+}
+
+/* -------------------------------------------------------------------------- */
+
+static void sw_cfg_unk_dest(struct ksz_sw *sw, u8 offset, bool set)
+{
+	u8 data;
+
+	data = sw->reg->r8(sw, offset);
+	if (set)
+		data |= SW_UNK_FWD_ENABLE;
+	else
+		data &= ~SW_UNK_FWD_ENABLE;
+	sw->reg->w8(sw, offset, data);
+}
+
+static int sw_chk_unk_dest(struct ksz_sw *sw, u8 offset)
+{
+	u8 data;
+
+	data = sw->reg->r8(sw, offset);
+	return (data & SW_UNK_FWD_ENABLE) == SW_UNK_FWD_ENABLE;
+}
+
+static void sw_cfg_unk_def_port(struct ksz_sw *sw, u8 offset, u8 port, int set)
+{
+	u8 data;
+	u8 bits = 1 << port;
+
+	data = sw->reg->r8(sw, offset);
+	if (2 == set) {
+		data &= ~SW_UNK_FWD_MAP;
+		bits = port & SW_UNK_FWD_MAP;
+	}
+	if (set)
+		data |= bits;
+	else
+		data &= ~bits;
+	sw->reg->w8(sw, offset, data);
+}
+
+static int sw_chk_unk_def_port(struct ksz_sw *sw, u8 offset, uint port)
+{
+	u8 data;
+	u8 bit = 1 << port;
+
+	data = sw->reg->r8(sw, offset);
+	if (port >= sw->port_cnt)
+		return data & SW_UNK_FWD_MAP;
+	return (data & bit) == bit;
+}
+
+static inline int sw_chk_self_filter(struct ksz_sw *sw)
+{
+	return sw_chk(sw, REG_SW_CTRL_18,
+		SW_SELF_ADDR_FILTER_ENABLE);
+}
+
+static inline void sw_cfg_self_filter(struct ksz_sw *sw, bool set)
+{
+	sw_cfg(sw, REG_SW_CTRL_18, SW_SELF_ADDR_FILTER_ENABLE, set);
+}
+
+static void sw_cfg_src_filter(struct ksz_sw *sw, bool set)
+{
+}  /* sw_cfg_src_filter */
+
+static void sw_fwd_unk_vid(struct ksz_sw *sw)
+{
+	sw->reg->w8(sw, REG_SW_UNK_VID_CTRL, sw->HOST_MASK | SW_UNK_FWD_ENABLE);
+}  /* sw_fwd_unk_vid */
+
+/* -------------------------------------------------------------------------- */
+
+/* Priority */
+
+static inline void port_cfg_diffserv(struct ksz_sw *sw, uint p, bool set)
+{
+	port_cfg(sw, p,
+		P_PRIO_CTRL, PORT_DIFFSERV_ENABLE, set);
+}
+
+static inline void port_cfg_802_1p(struct ksz_sw *sw, uint p, bool set)
+{
+	port_cfg(sw, p,
+		P_PRIO_CTRL, PORT_802_1P_ENABLE, set);
+}
+
+static inline void port_cfg_replace_prio(struct ksz_sw *sw, uint p, bool set)
+{
+	port_cfg(sw, p,
+		P_802_1P_CTRL, PORT_802_1P_REMAPPING, set);
+}
+
+static inline int port_chk_diffserv(struct ksz_sw *sw, uint p)
+{
+	return port_chk(sw, p,
+		P_PRIO_CTRL, PORT_DIFFSERV_ENABLE);
+}
+
+static inline int port_chk_802_1p(struct ksz_sw *sw, uint p)
+{
+	return port_chk(sw, p,
+		P_PRIO_CTRL, PORT_802_1P_ENABLE);
+}
+
+static inline int port_chk_replace_prio(struct ksz_sw *sw, uint p)
+{
+	return port_chk(sw, p,
+		P_802_1P_CTRL, PORT_802_1P_REMAPPING);
+}
+
+/* -------------------------------------------------------------------------- */
+
+/**
+ * sw_set_tos_prio - program switch TOS priority
+ * @sw:		The switch instance.
+ * @tos:	ToS value from 6-bit (bit7 ~ bit2) of ToS field, ranging from 0
+ *		to 63.
+ * @prio:	Priority to be assigned.
+ *
+ * This routine programs the TOS priority into the switch registers.
+ */
+static void sw_set_tos_prio(struct ksz_sw *sw, u8 tos, SW_D prio)
+{
+	SW_W(sw, S_TOS_PRIO_CTRL + tos * SW_SIZE, prio);
+}  /* sw_set_tos_prio */
+
+/**
+ * sw_dis_diffserv - disable switch DiffServ priority
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ *
+ * This routine disables the DiffServ priority function of the switch.
+ */
+static void sw_dis_diffserv(struct ksz_sw *sw, uint port)
+{
+	port_cfg_diffserv(sw, port, 0);
+}  /* sw_dis_diffserv */
+
+/**
+ * sw_ena_diffserv - enable switch DiffServ priority
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ *
+ * This routine enables the DiffServ priority function of the switch.
+ */
+static void sw_ena_diffserv(struct ksz_sw *sw, uint port)
+{
+	port_cfg_diffserv(sw, port, 1);
+}  /* sw_ena_diffserv */
+
+/**
+ * hw_cfg_tos_prio - configure TOS priority
+ * @sw:		The switch instance.
+ * @tos:	ToS value from 6-bit (bit7 ~ bit2) of ToS field, ranging from 0
+ *		to 63.
+ * @prio:	Priority to be assigned.
+ *
+ * This routine configures the TOS priority in the hardware.
+ * DiffServ Value 0 ~ 63 is mapped to Priority Queue Number 0 ~ 3.
+ * It is called by user functions.  The hardware should be acquired first.
+ */
+static void hw_cfg_tos_prio(struct ksz_sw *sw, u8 tos, SW_D prio)
+{
+	int shift;
+	SW_D data = prio;
+	SW_D mask = KS_PRIO_M;
+
+	if (tos >= DIFFSERV_ENTRIES)
+		return;
+
+	if (prio >= 0x10)
+		mask = 0xff;
+	else if (prio > KS_PRIO_M)
+		mask = 0xf;
+	shift = (tos & (KS_PRIO_IN_REG - 1)) * KS_PRIO_S;
+	prio = prio << shift;
+	if (prio >> shift != data)
+		return;
+	mask <<= shift;
+	tos /= KS_PRIO_IN_REG;
+
+	sw->info->diffserv[tos] &= ~mask;
+	sw->info->diffserv[tos] |= prio;
+
+	sw_set_tos_prio(sw, tos, sw->info->diffserv[tos]);
+}  /* hw_cfg_tos_prio */
+
+/* -------------------------------------------------------------------------- */
+
+/**
+ * sw_set_802_1p_prio - program switch 802.1p priority
+ * @sw:		The switch instance.
+ * @prio:	Priority to be assigned.
+ *
+ * This routine programs the 802.1p priority into the switch register.
+ */
+static void sw_set_802_1p_prio(struct ksz_sw *sw, u8 tag, SW_D prio)
+{
+	SW_W(sw, S_802_1P_PRIO_CTRL + tag / SW_SIZE, prio);
+}  /* sw_set_802_1p_prio */
+
+/**
+ * sw_dis_802_1p - disable switch 802.1p priority
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ *
+ * This routine disables the 802.1p priority function of the switch.
+ */
+static void sw_dis_802_1p(struct ksz_sw *sw, uint port)
+{
+	port_cfg_802_1p(sw, port, 0);
+}  /* sw_dis_802_1p */
+
+/**
+ * sw_ena_802_1p - enable switch 802.1p priority
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ *
+ * This routine enables the 802.1p priority function of the switch.
+ */
+static void sw_ena_802_1p(struct ksz_sw *sw, uint port)
+{
+	port_cfg_802_1p(sw, port, 1);
+}  /* sw_ena_802_1p */
+
+/**
+ * hw_cfg_802_1p_prio - configure 802.1p priority
+ * @sw:		The switch instance.
+ * @tag:	The 802.1p tag priority value, ranging from 0 to 7.
+ * @prio:	Priority to be assigned.
+ *
+ * This routine configures the 802.1p priority in the hardware.
+ * 802.1p Tag priority value 0 ~ 7 is mapped to Priority Queue Number 0 ~ 3.
+ * It is called by user functions.  The hardware should be acquired first.
+ */
+static void hw_cfg_802_1p_prio(struct ksz_sw *sw, u8 tag, SW_D prio)
+{
+	int shift;
+	SW_D data = prio;
+	SW_D mask = KS_PRIO_M;
+
+	if (tag >= PRIO_802_1P_ENTRIES)
+		return;
+
+	if (prio >= 0x10)
+		mask = 0xff;
+	else if (prio > KS_PRIO_M)
+		mask = 0xf;
+	shift = (tag & (KS_PRIO_IN_REG - 1)) * KS_PRIO_S;
+	prio = prio << shift;
+	if (prio >> shift != data)
+		return;
+	mask <<= shift;
+	tag /= KS_PRIO_IN_REG;
+
+	sw->info->p_802_1p[tag] &= ~mask;
+	sw->info->p_802_1p[tag] |= prio;
+
+	sw_set_802_1p_prio(sw, tag, sw->info->p_802_1p[tag]);
+}  /* hw_cfg_802_1p_prio */
+
+/**
+ * sw_cfg_replace_null_vid - enable switch null VID replacement
+ * @sw:		The switch instance.
+ * @set:	The flag to disable or enable.
+ *
+ * This routine enables the VID to be replaced with port default VID if it is
+ * empty.
+ */
+static void sw_cfg_replace_null_vid(struct ksz_sw *sw, bool set)
+{
+	sw_cfg(sw, S_REPLACE_VID_CTRL, SW_REPLACE_VID, set);
+}  /* sw_cfg_replace_null_vid */
+
+/**
+ * sw_cfg_replace_prio - enable switch 802.1p priority re-mapping
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ * @set:	The flag to disable or enable.
+ *
+ * This routine enables the 802.1p priority re-mapping function of the switch.
+ * That allows 802.1p priority field to be replaced with the port's default
+ * tag's priority value if the ingress packet's 802.1p priority has a higher
+ * priority than port's default tag's priority.
+ */
+static void sw_cfg_replace_prio(struct ksz_sw *sw, uint port, bool set)
+{
+	port_cfg_replace_prio(sw, port, set);
+}  /* sw_cfg_replace_prio */
+
+/* -------------------------------------------------------------------------- */
+
+/**
+ * sw_cfg_port_based - configure switch port based priority
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ * @prio:	The priority to set.
+ *
+ * This routine configures the port based priority of the switch.
+ */
+static void sw_cfg_port_based(struct ksz_sw *sw, uint port, u8 prio)
+{
+	SW_D data;
+
+	if (prio > PORT_BASED_PRIO_M)
+		prio = PORT_BASED_PRIO_M;
+
+	port_r(sw, port, P_PRIO_CTRL, &data);
+	data &= ~(PORT_BASED_PRIO_M << PORT_BASED_PRIO_S);
+	data |= prio << PORT_BASED_PRIO_S;
+	port_w(sw, port, P_PRIO_CTRL, data);
+
+	sw->info->port_cfg[port].port_prio = prio;
+}  /* sw_cfg_port_based */
+
+/* -------------------------------------------------------------------------- */
+
+static int sw_get_hi_prio_queues(struct ksz_sw *sw)
+{
+	u8 data;
+	int queue;
+
+	data = sw->reg->r8(sw, REG_SWITCH_CTRL_14);
+	data >>= SW_PRIO_MAPPING_S;
+	data &= SW_PRIO_MAPPING_M;
+	switch (data) {
+	case SW_PRIO_MAP_3_HI:
+		queue = 3;
+		break;
+	case SW_PRIO_MAP_0_LO:
+		queue = 1;
+		break;
+	case SW_PRIO_MAP_2_HI:
+		queue = 2;
+		break;
+	default:
+		queue = 0;
+	}
+	return queue;
+}  /* sw_get_hi_prio_queues */
+
+static void sw_set_hi_prio_queues(struct ksz_sw *sw, int queue)
+{
+	u8 data;
+
+	data = sw->reg->r8(sw, REG_SWITCH_CTRL_14);
+	data &= ~(SW_PRIO_MAPPING_M << SW_PRIO_MAPPING_S);
+	switch (queue) {
+	case 1:
+		queue = SW_PRIO_MAP_0_LO;
+		break;
+	case 3:
+		queue = SW_PRIO_MAP_3_HI;
+		break;
+	default:
+		queue = SW_PRIO_MAP_2_HI;
+	}
+	queue <<= SW_PRIO_MAPPING_S;
+	data |= (u8) queue;
+	sw->reg->w8(sw, REG_SWITCH_CTRL_14, data);
+}  /* sw_set_hi_prio_queues */
+
+/**
+ * port_get_prio_queue - check transmit multiple queues
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ *
+ * This function checks number of transmit queues enabled in the port.
+ */
+static int port_get_prio_queue(struct ksz_sw *sw, uint port)
+{
+	int queue;
+	u8 hi;
+	u8 lo;
+
+	port_r(sw, port, REG_PORT_CTRL_0, &lo);
+	port_r(sw, port, P_DROP_TAG_CTRL, &hi);
+	queue = !!(hi & PORT_QUEUE_SPLIT_H);
+	queue <<= 1;
+	queue |= !!(lo & PORT_QUEUE_SPLIT_L);
+	return 1 << queue;
+}  /* port_get_prio_queue */
+
+/**
+ * port_set_prio_queue - enable transmit multiple queues
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ * @queue:	Number of queues.
+ *
+ * This routine enables the transmit multiple queues selection of the switch
+ * port.  The port transmit queue is split into two or four priority queues.
+ */
+static void port_set_prio_queue(struct ksz_sw *sw, uint port, int queue)
+{
+	u8 hi;
+	u8 lo;
+
+	switch (queue) {
+	case 4:
+	case 3:
+		queue = PORT_QUEUE_SPLIT_4;
+		break;
+	case 2:
+		queue = PORT_QUEUE_SPLIT_2;
+		break;
+	default:
+		queue = PORT_QUEUE_SPLIT_1;
+	}
+	port_r(sw, port, REG_PORT_CTRL_0, &lo);
+	port_r(sw, port, P_DROP_TAG_CTRL, &hi);
+	lo &= ~PORT_QUEUE_SPLIT_L;
+	if (queue & 1)
+		lo |= PORT_QUEUE_SPLIT_L;
+	hi &= ~PORT_QUEUE_SPLIT_H;
+	if (queue & 2)
+		hi |= PORT_QUEUE_SPLIT_H;
+	port_w(sw, port, REG_PORT_CTRL_0, lo);
+	port_w(sw, port, P_DROP_TAG_CTRL, hi);
+
+	/* Default is port based for egress rate limit. */
+	if (queue)
+		sw_cfg(sw, REG_SW_CTRL_19, SW_OUT_RATE_LIMIT_QUEUE_BASED, 1);
+}  /* port_set_prio_queue */
+
+/**
+ * sw_init_prio - initialize switch priority
+ * @sw:		The switch instance.
+ *
+ * This routine initializes the switch QoS priority functions.
+ */
+static void sw_init_prio(struct ksz_sw *sw)
+{
+	uint n;
+	uint port;
+	SW_D data;
+
+	sw->reg->r(sw, S_802_1P_PRIO_CTRL, sw->info->p_802_1p,
+		PRIO_802_1P_ENTRIES / KS_PRIO_IN_REG);
+
+	sw->reg->r(sw, S_TOS_PRIO_CTRL, sw->info->diffserv,
+		DIFFSERV_ENTRIES / KS_PRIO_IN_REG);
+
+	for (n = 0; n <= sw->mib_port_cnt; n++) {
+		port = get_phy_port(sw, n);
+		port_r(sw, port, P_PRIO_CTRL, &data);
+		data &= PORT_BASED_PRIO_M;
+		data >>= PORT_BASED_PRIO_S;
+		sw->info->port_cfg[port].port_prio = data;
+	}
+}  /* sw_init_prio */
+
+/**
+ * sw_setup_prio - setup switch priority
+ * @sw:		The switch instance.
+ *
+ * This routine setup the switch QoS priority functions.
+ */
+static void sw_setup_prio(struct ksz_sw *sw)
+{
+	uint n;
+	uint port;
+
+	/* All QoS functions disabled. */
+	for (n = 0; n <= sw->mib_port_cnt; n++) {
+		port = get_phy_port(sw, n);
+		port_set_prio_queue(sw, port, 4);
+		sw_dis_diffserv(sw, port);
+		sw_cfg_replace_prio(sw, port, 0);
+		sw_cfg_port_based(sw, port, sw->info->port_cfg[port].port_prio);
+
+		sw_ena_802_1p(sw, port);
+	}
+	sw_cfg_replace_null_vid(sw, 0);
+}  /* sw_setup_prio */
+
+/* -------------------------------------------------------------------------- */
+
+/**
+ * port_cfg_def_vid - configure port default VID
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ * @vid:	The VID value.
+ *
+ * This routine configures the default VID of the port.
+ */
+static void port_cfg_def_vid(struct ksz_sw *sw, uint port, u16 vid)
+{
+	port_w16(sw, port, REG_PORT_CTRL_VID, vid);
+}  /* port_cfg_def_vid */
+
+/**
+ * port_get_def_vid - get port default VID.
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ * @vid:	Buffer to store the VID.
+ *
+ * This routine retrieves the default VID of the port.
+ */
+static void port_get_def_vid(struct ksz_sw *sw, uint port, u16 *vid)
+{
+	port_r16(sw, port, REG_PORT_CTRL_VID, vid);
+}  /* port_get_def_vid */
+
+/**
+ * sw_cfg_def_vid - configure switch port default VID
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ * @vid:	The VID value.
+ *
+ * This routine configures the default VID of the port.
+ */
+static void sw_cfg_def_vid(struct ksz_sw *sw, uint port, u16 vid)
+{
+	sw->info->port_cfg[port].vid = vid;
+	port_cfg_def_vid(sw, port, vid);
+}  /* sw_cfg_def_vid */
+
+/**
+ * sw_cfg_port_base_vlan - configure port-based VLAN membership
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ * @member:	The port-based VLAN membership.
+ *
+ * This routine configures the port-based VLAN membership of the port.
+ */
+static void sw_cfg_port_base_vlan(struct ksz_sw *sw, uint port, u8 member)
+{
+	SW_D data;
+
+	port_r(sw, port, P_MIRROR_CTRL, &data);
+	data &= ~PORT_VLAN_MEMBERSHIP;
+	data |= (member & sw->PORT_MASK);
+	port_w(sw, port, P_MIRROR_CTRL, data);
+
+	sw->info->port_cfg[port].member = member;
+}  /* sw_cfg_port_base_vlan */
+
+/**
+ * sw_vlan_cfg_dis_non_vid - configure discard non VID
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ * @set:	The flag to disable or enable.
+ *
+ * This routine configures Discard Non VID packets of the switch port.
+ * If enabled, the device will discard packets whose VLAN id does not match
+ * ingress port-based default VLAN id.
+ */
+static void sw_vlan_cfg_dis_non_vid(struct ksz_sw *sw, uint port, bool set)
+{
+	port_cfg_dis_non_vid(sw, port, set);
+}  /* sw_vlan_cfg_dis_non_vid */
+
+/**
+ * sw_vlan_cfg_drop_tag - configure 802.1q tagged packet drop
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ * @set:	The flag to disable or enable.
+ *
+ */
+static void sw_vlan_cfg_drop_tag(struct ksz_sw *sw, uint port, bool set)
+{
+	port_cfg_drop_tag(sw, port, set);
+}  /* sw_vlan_cfg_drop_tag */
+
+/**
+ * sw_vlan_cfg_in_filter - configure ingress VLAN filtering
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ * @set:	The flag to disable or enable.
+ *
+ * This routine configures Ingress VLAN filtering of the switch port.
+ * If enabled, the device will discard packets whose VLAN id membership	in the
+ * VLAN table bits [18:16] does not include the ingress port that received this
+ * packet.
+ */
+static void sw_vlan_cfg_in_filter(struct ksz_sw *sw, uint port, bool set)
+{
+	port_cfg_in_filter(sw, port, set);
+}  /* sw_vlan_cfg_in_filter */
+
+/**
+ * sw_vlan_cfg_ins_tag - configure 802.1q tag insertion
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ * @set:	The flag to disable or enable.
+ *
+ * This routine configures 802.1q Tag insertion to the switch port.
+ * If enabled, the device will insert 802.1q tag to the transmit packet on this
+ * port if received packet is an untagged packet.  The device will not insert
+ * 802.1q tag if received packet is tagged packet.
+ */
+static void sw_vlan_cfg_ins_tag(struct ksz_sw *sw, uint port, bool set)
+{
+	port_cfg_ins_tag(sw, port, set);
+}  /* sw_vlan_cfg_ins_tag */
+
+/**
+ * sw_vlan_cfg_rmv_tag - configure 802.1q tag removal
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ * @set:	The flag to disable or enable.
+ *
+ * This routine configures 802.1q Tag removal to the switch port.
+ * If enabled, the device will removed 802.1q tag to the transmit packet on
+ * this port if received packet is a tagged packet.  The device will not remove
+ * 802.1q tag if received packet is untagged packet.
+ */
+static void sw_vlan_cfg_rmv_tag(struct ksz_sw *sw, uint port, bool set)
+{
+	port_cfg_rmv_tag(sw, port, set);
+}  /* sw_vlan_cfg_rmv_tag */
+
+static inline int sw_chk_ins_tag(struct ksz_sw *sw)
+{
+	return sw_chk(sw, REG_SW_CTRL_19, SW_INS_TAG_ENABLE);
+}
+
+static inline void sw_cfg_ins_tag(struct ksz_sw *sw, bool set)
+{
+	sw_cfg(sw, REG_SW_CTRL_19, SW_INS_TAG_ENABLE, set);
+}
+
+static int get_ins_shift(int src_port, int dst_port)
+{
+	int shift;
+
+	if (src_port == dst_port)
+		return 0;
+	shift = PORT_INS_TAG_FOR_PORT_5_S + dst_port - 3;
+	if (src_port > shift)
+		shift++;
+	return shift;
+}
+
+static int sw_chk_ins(struct ksz_sw *sw, int src_port, int dst_port)
+{
+	int shift;
+
+	shift = get_ins_shift(src_port, dst_port);
+	if (!shift)
+		return 0;
+	--shift;
+	return port_chk(sw, src_port, P_INS_SRC_PVID_CTRL, 1 << shift);
+}
+
+static void sw_cfg_ins(struct ksz_sw *sw, int src_port, int dst_port, bool set)
+{
+	int shift;
+
+	shift = get_ins_shift(src_port, dst_port);
+	if (!shift)
+		return;
+	--shift;
+	port_cfg(sw, src_port, P_INS_SRC_PVID_CTRL, 1 << shift, set);
+}
+
+/**
+ * sw_dis_vlan - disable switch VLAN
+ * @sw:		The switch instance.
+ *
+ * This routine disables the VLAN function of the switch.
+ */
+static void sw_dis_vlan(struct ksz_sw *sw)
+{
+	sw_cfg(sw, S_MIRROR_CTRL, SW_VLAN_ENABLE, 0);
+}  /* sw_dis_vlan */
+
+/**
+ * sw_ena_vlan - enable switch VLAN
+ * @sw:		The switch instance.
+ *
+ * This routine enables the VLAN function of the switch.
+ */
+static void sw_ena_vlan(struct ksz_sw *sw)
+{
+	/* Enable 802.1q VLAN mode. */
+	sw_cfg(sw, REG_SW_CTRL_2, UNICAST_VLAN_BOUNDARY, 1);
+	sw_cfg(sw, S_MIRROR_CTRL, SW_VLAN_ENABLE, 1);
+}  /* sw_ena_vlan */
+
+/**
+ * sw_init_vlan - initialize switch VLAN
+ * @sw:		The switch instance.
+ *
+ * This routine initializes the VLAN function of the switch.
+ */
+static void sw_init_vlan(struct ksz_sw *sw)
+{
+	uint n;
+	uint port;
+	SW_D data;
+	struct ksz_port_cfg *cfg;
+
+	for (n = 0; n <= sw->mib_port_cnt; n++) {
+		port = get_phy_port(sw, n);
+		cfg = get_port_cfg(sw, port);
+		port_get_def_vid(sw, port, &cfg->vid);
+		port_r(sw, port, P_MIRROR_CTRL, &data);
+		data &= PORT_VLAN_MEMBERSHIP;
+		cfg->member = data;
+		cfg->vid_member = data;
+
+		port_cfg(sw, port, P_INS_SRC_PVID_CTRL,
+			(PORT_INS_TAG_FOR_PORT_5 | PORT_INS_TAG_FOR_PORT_4 |
+			PORT_INS_TAG_FOR_PORT_3 | PORT_INS_TAG_FOR_PORT_2),
+			true);
+	}
+}  /* sw_init_vlan */
+
+static void inc_mac_addr(u8 *dst, u8 *src, u8 inc)
+{
+#ifdef USE_SAME_ADDR
+	inc = 0;
+#endif
+	memcpy(dst, src, ETH_ALEN);
+	dst[5] += inc;
+	if (dst[5] < src[5])
+		dst[4]++;
+	if (dst[4] < src[4])
+		dst[3]++;
+}  /* inc_mac_addr */
+
+/**
+ * sw_get_addr - get the switch MAC address.
+ * @sw:		The switch instance.
+ * @mac_addr:	Buffer to store the MAC address.
+ *
+ * This function retrieves the MAC address of the switch.
+ */
+static inline void sw_get_addr(struct ksz_sw *sw, u8 *mac_addr)
+{
+	sw->reg->r(sw, REG_SW_MAC_ADDR_0, mac_addr, 6);
+	memcpy(sw->info->mac_addr, mac_addr, 6);
+}  /* sw_get_addr */
+
+/**
+ * sw_set_addr - configure switch MAC address
+ * @sw:		The switch instance.
+ * @mac_addr:	The MAC address.
+ *
+ * This function configures the MAC address of the switch.
+ */
+static void sw_set_addr(struct ksz_sw *sw, u8 *mac_addr)
+{
+	uint n;
+	uint p;
+	struct ksz_port_info *info;
+
+	for (n = 0; n <= sw->mib_port_cnt; n++) {
+		p = get_phy_port(sw, n);
+		info = get_port_info(sw, p);
+		inc_mac_addr(info->mac_addr, mac_addr, n);
+	}
+/**
+ * THa  2015/07/17
+ * Switch does not learn unicast address from port if the source address
+ * matches the switch MAC address!  This mostly happens with the host MAC
+ * address.
+ */
+#if 0
+	sw->reg->w(sw, REG_SW_MAC_ADDR_0, mac_addr, 6);
+#endif
+	memcpy(sw->info->mac_addr, mac_addr, 6);
+}  /* sw_set_addr */
+
+#ifdef CONFIG_KSZ_MRP
+#include "ksz_mrp.c"
+#endif
+
+#define STP_ENTRY			0
+#define BROADCAST_ENTRY			1
+#define BRIDGE_ADDR_ENTRY		2
+#define IPV6_ADDR_ENTRY			3
+
+/**
+ * sw_set_global_ctrl - set switch global control
+ * @sw:		The switch instance.
+ *
+ * This routine sets the global control of the switch function.
+ */
+static void sw_set_global_ctrl(struct ksz_sw *sw)
+{
+	SW_D data;
+
+	/* Enable switch MII flow control. */
+	data = SW_R(sw, S_REPLACE_VID_CTRL);
+	data |= SW_FLOW_CTRL;
+
+	SW_W(sw, S_REPLACE_VID_CTRL, data);
+
+	data = SW_R(sw, S_LINK_AGING_CTRL);
+	data |= SW_LINK_AUTO_AGING;
+	SW_W(sw, S_LINK_AGING_CTRL, data);
+
+	data = SW_R(sw, REG_SW_CTRL_1);
+
+	/* Enable aggressive back off algorithm in half duplex mode. */
+	data |= SW_AGGR_BACKOFF;
+
+	/* Enable automatic fast aging when link changed detected. */
+	data |= SW_AGING_ENABLE;
+
+	if (sw->overrides & FAST_AGING)
+		data |= SW_FAST_AGING;
+	else
+		data &= ~SW_FAST_AGING;
+
+	SW_W(sw, REG_SW_CTRL_1, data);
+
+	data = SW_R(sw, REG_SW_CTRL_2);
+
+	/* Make sure unicast VLAN boundary is set as default. */
+	if (sw->dev_count > 1)
+		data |= UNICAST_VLAN_BOUNDARY;
+
+	/* Enable no excessive collision drop. */
+	data |= NO_EXC_COLLISION_DROP;
+	SW_W(sw, REG_SW_CTRL_2, data);
+}  /* sw_set_global_ctrl */
+
+/* -------------------------------------------------------------------------- */
+
+/**
+ * port_set_stp_state - configure port spanning tree state
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ * @state:	The spanning tree state.
+ *
+ * This routine configures the spanning tree state of the port.
+ */
+static void port_set_stp_state(struct ksz_sw *sw, uint port, int state)
+{
+	SW_D data;
+	struct ksz_port_cfg *port_cfg;
+	int member = -1;
+
+	port_cfg = get_port_cfg(sw, port);
+	port_r(sw, port, P_STP_CTRL, &data);
+	switch (state) {
+	case STP_STATE_DISABLED:
+		data &= ~(PORT_TX_ENABLE | PORT_RX_ENABLE);
+		data |= PORT_LEARN_DISABLE;
+		if (port < SWITCH_PORT_NUM)
+			member = 0;
+		break;
+	case STP_STATE_LISTENING:
+/*
+ * No need to turn on transmit because of port direct mode.
+ * Turning on receive is required if static MAC table is not setup.
+ */
+		data &= ~PORT_TX_ENABLE;
+		data |= PORT_RX_ENABLE;
+		data |= PORT_LEARN_DISABLE;
+		if (port < SWITCH_PORT_NUM &&
+		    STP_STATE_DISABLED == port_cfg->stp_state)
+			member = sw->HOST_MASK | port_cfg->vid_member;
+		break;
+	case STP_STATE_LEARNING:
+		data &= ~PORT_TX_ENABLE;
+		data |= PORT_RX_ENABLE;
+		data &= ~PORT_LEARN_DISABLE;
+		break;
+	case STP_STATE_FORWARDING:
+		data |= (PORT_TX_ENABLE | PORT_RX_ENABLE);
+		data &= ~PORT_LEARN_DISABLE;
+
+#ifdef CONFIG_KSZ_STP
+		/* Actual port membership setting is done in another RSTP
+		 * processing routine.
+		 */
+		if (sw->stp == 1 || (sw->stp && (sw->stp & (1 << port)))) {
+			struct ksz_stp_info *info = &sw->info->rstp;
+
+			if (info->br.bridgeEnabled)
+				break;
+		}
+#endif
+		if (((sw->features & (SW_VLAN_DEV | USE_FEWER_PORTS)) ||
+		    sw->dev_offset) && port != sw->HOST_PORT)
+			/* Set port-base vlan membership with host port. */
+			member = sw->HOST_MASK | port_cfg->vid_member;
+		break;
+	case STP_STATE_BLOCKED:
+/*
+ * Need to setup static MAC table with override to keep receiving BPDU
+ * messages.  See sw_setup_stp routine.
+ */
+		data &= ~(PORT_TX_ENABLE | PORT_RX_ENABLE);
+		data |= PORT_LEARN_DISABLE;
+		if (port < SWITCH_PORT_NUM &&
+		    STP_STATE_DISABLED == port_cfg->stp_state)
+			member = sw->HOST_MASK | port_cfg->vid_member;
+		break;
+	case STP_STATE_SIMPLE:
+		data |= (PORT_TX_ENABLE | PORT_RX_ENABLE);
+		data |= PORT_LEARN_DISABLE;
+		if (port < SWITCH_PORT_NUM)
+			/* Set port-base vlan membership with host port. */
+			member = sw->HOST_MASK | port_cfg->vid_member;
+		break;
+	}
+	port_w(sw, port, P_STP_CTRL, data);
+	port_cfg->stp_state = state;
+	if (data & PORT_RX_ENABLE)
+		sw->rx_ports |= (1 << port);
+	else
+		sw->rx_ports &= ~(1 << port);
+	if (data & PORT_TX_ENABLE)
+		sw->tx_ports |= (1 << port);
+	else
+		sw->tx_ports &= ~(1 << port);
+
+	/* Port membership may share register with STP state. */
+	if (member >= 0)
+		sw_cfg_port_base_vlan(sw, port, (u8) member);
+}  /* port_set_stp_state */
+
+/**
+ * sw_clr_sta_mac_table - clear static MAC table
+ * @sw:		The switch instance.
+ *
+ * This routine clears the static MAC table.
+ */
+static void sw_clr_sta_mac_table(struct ksz_sw *sw)
+{
+	struct ksz_mac_table entry;
+	int i;
+
+	memset(&entry, 0, sizeof(struct ksz_mac_table));
+	for (i = 0; i < STATIC_MAC_TABLE_ENTRIES; i++)
+		sw_w_sta_mac_table(sw, i, &entry);
+}  /* sw_clr_sta_mac_table */
+
+/**
+ * sw_setup_stp - setup switch spanning tree support
+ * @sw:		The switch instance.
+ *
+ * This routine initializes the spanning tree support of the switch.
+ */
+static void sw_setup_stp(struct ksz_sw *sw)
+{
+	struct ksz_mac_table entry;
+
+	entry.addr[0] = 0x01;
+	entry.addr[1] = 0x80;
+	entry.addr[2] = 0xC2;
+	entry.addr[3] = 0x00;
+	entry.addr[4] = 0x00;
+	entry.addr[5] = 0x00;
+	entry.ports = sw->HOST_MASK;
+	entry.use_fid = 0;
+	entry.override = 1;
+	entry.valid = 1;
+	sw_w_sta_mac_table(sw, STP_ENTRY, &entry);
+}  /* sw_setup_stp */
+
+#ifdef CONFIG_KSZ_STP
+static void bridge_change(struct ksz_sw *sw)
+{
+	uint n;
+	uint port;
+	u8 member;
+	struct ksz_port_cfg *cfg;
+	struct ksz_sw_info *info = sw->info;
+
+	for (n = 1; n <= sw->mib_port_cnt; n++) {
+		port = get_phy_port(sw, n);
+		cfg = get_port_cfg(sw, port);
+		if (STP_STATE_FORWARDING == cfg->stp_state)
+			member = sw->HOST_MASK | info->member[0];
+		else if (STP_STATE_DISABLED == cfg->stp_state)
+			member = 0;
+		else
+			member = sw->HOST_MASK | (1 << port);
+		if (member != cfg->member)
+			sw_cfg_port_base_vlan(sw, port, member);
+	}
+}  /* bridge_change */
+#endif
+
+#define MAX_SW_LEN			1500
+
+#ifdef CONFIG_KSZ_STP
+#include "ksz_stp.c"
+#endif
+
+/*
+ * Link detection routines
+ */
+
+static inline void dbp_link(struct ksz_port *port, struct ksz_sw *sw,
+	int change)
+{
+	struct ksz_port_info *info;
+	uint i;
+	uint n;
+	uint p;
+
+	for (i = 0, n = port->first_port; i < port->port_cnt; i++, n++) {
+		p = get_phy_port(sw, n);
+		info = get_port_info(sw, p);
+		if (media_connected == info->state) {
+			if (change & (1 << i)) {
+				printk(KERN_INFO "link %d-%d: %d, %d\n",
+					sw->id, i + port->first_port,
+					info->tx_rate / TX_RATE_UNIT,
+					info->duplex);
+			}
+		} else {
+			if (change & (1 << i))
+				printk(KERN_INFO "link %d-%d disconnected\n",
+					sw->id, i + port->first_port);
+		}
+	}
+}
+
+static SW_D port_advertised_flow_ctrl(struct ksz_port *port, SW_D ctrl)
+{
+	ctrl &= ~PORT_AUTO_NEG_SYM_PAUSE;
+	switch (port->flow_ctrl) {
+	case PHY_FLOW_CTRL:
+		ctrl |= PORT_AUTO_NEG_SYM_PAUSE;
+		break;
+	/* Not supported. */
+	case PHY_TX_ONLY:
+	case PHY_RX_ONLY:
+	default:
+		break;
+	}
+	return ctrl;
+}  /* port_advertised_flow_ctrl */
+
+static u8 sw_determine_flow_ctrl(struct ksz_sw *sw, struct ksz_port *port,
+	u8 local, u8 remote)
+{
+	int rx;
+	int tx;
+	u8 flow = 0;
+
+	if (sw->overrides & PAUSE_FLOW_CTRL)
+		return flow;
+
+	rx = tx = 0;
+	if (port->force_link)
+		rx = tx = 1;
+	if (remote & PORT_REMOTE_SYM_PAUSE) {
+		if (local & PORT_AUTO_NEG_SYM_PAUSE)
+			rx = tx = 1;
+	}
+	if (rx)
+		flow |= 0x01;
+	if (tx)
+		flow |= 0x02;
+#ifdef DBG_LINK
+	printk(KERN_INFO "pause: %d, %d; %02x %02x\n",
+		rx, tx, local, remote);
+#endif
+	return flow;
+}  /* sw_determine_flow_ctrl */
+
+static void sw_notify_link_change(struct ksz_sw *sw, uint ports)
+{
+	static u8 link_buf[sizeof(struct ksz_info_opt) +
+		sizeof(struct ksz_info_speed) * TOTAL_PORT_NUM +
+		sizeof(struct ksz_resp_msg)];
+
+	if ((sw->notifications & SW_INFO_LINK_CHANGE)) {
+		struct ksz_resp_msg *msg = (struct ksz_resp_msg *) link_buf;
+		struct ksz_info_opt *opt = (struct ksz_info_opt *)
+			&msg->resp.data;
+		struct ksz_port_info *info;
+		struct ksz_info_speed *speed;
+		struct file_dev_info *dev_info;
+		int c;
+		int n;
+		int p;
+		int q;
+
+		/* Check whether only 1 port has change. */
+		c = 0;
+		q = 0;
+		for (n = 1; n <= sw->mib_port_cnt; n++) {
+			p = get_phy_port(sw, n);
+			if (ports & (1 << p)) {
+				q = n;
+				c++;
+			}
+		}
+		if (c > 1) {
+			c = sw->mib_port_cnt;
+			q = 1;
+		}
+		msg->module = DEV_MOD_BASE;
+		msg->cmd = DEV_INFO_SW_LINK;
+		opt->num = (u8) c;
+		opt->port = (u8) q;
+		speed = &opt->data.speed;
+		for (n = 1; n <= opt->num; n++, q++) {
+			p = get_phy_port(sw, q);
+			info = get_port_info(sw, p);
+			if (info->state == media_connected) {
+				speed->tx_rate = info->tx_rate;
+				speed->duplex = info->duplex;
+				speed->flow_ctrl = info->flow_ctrl;
+			} else {
+				speed->tx_rate = 0;
+				speed->duplex = 0;
+				speed->flow_ctrl = 0;
+			}
+			++speed;
+		}
+		n = opt->num * sizeof(struct ksz_info_speed);
+		n += 2;
+		n += sizeof(struct ksz_resp_msg);
+		n -= 4;
+		dev_info = sw->dev_list[0];
+		while (dev_info) {
+			if ((dev_info->notifications[DEV_MOD_BASE] &
+			    SW_INFO_LINK_CHANGE))
+				file_dev_setup_msg(dev_info, msg, n, NULL,
+						   NULL);
+			dev_info = dev_info->next;
+		}
+	}
+}  /* sw_notify_link_change */
+
+#define KSZ8895_SW_ID		0x8895
+#define PHY_ID_KSZ_SW		((KSZ8895_ID_HI << 16) | KSZ8895_SW_ID)
+
+/**
+ * sw_r_phy - read data from PHY register
+ * @sw:		The switch instance.
+ * @phy:	PHY address to read.
+ * @reg:	PHY register to read.
+ * @val:	Buffer to store the read data.
+ *
+ * This routine reads data from the PHY register.
+ */
+static void sw_r_phy(struct ksz_sw *sw, u16 phy, u16 reg, u16 *val)
+{
+	u8 ctrl;
+	u8 restart;
+	u8 link;
+	u8 speed;
+	u8 force;
+	u8 p;
+	u16 data = 0;
+
+	if (phy) {
+		p = phy - 1;
+	} else {
+		switch (reg) {
+		case PHY_REG_CTRL:
+			data = 0x1140;
+			break;
+		case PHY_REG_STATUS:
+			data = 0x7808;
+			break;
+		case PHY_REG_ID_1:
+			data = KSZ8895_ID_HI;
+			break;
+		case PHY_REG_ID_2:
+			data = KSZ8895_SW_ID;
+			break;
+		case PHY_REG_AUTO_NEGOTIATION:
+			data = 0x05e1;
+			break;
+		case PHY_REG_REMOTE_CAPABILITY:
+			data = 0xc5e1;
+			break;
+		}
+		*val = data;
+		return;
+	}
+	switch (reg) {
+	case PHY_REG_CTRL:
+		port_r(sw, p, P_LOCAL_CTRL, &ctrl);
+		port_r(sw, p, P_NEG_RESTART_CTRL, &restart);
+		port_r(sw, p, P_SPEED_STATUS, &speed);
+		port_r(sw, p, P_FORCE_CTRL, &force);
+		if (restart & PORT_PHY_LOOPBACK)
+			data |= PHY_LOOPBACK;
+		if (force & PORT_FORCE_100_MBIT)
+			data |= PHY_SPEED_100MBIT;
+		if (!(force & PORT_AUTO_NEG_DISABLE))
+			data |= PHY_AUTO_NEG_ENABLE;
+		if (restart & PORT_POWER_DOWN)
+			data |= PHY_POWER_DOWN;
+		if (restart & PORT_AUTO_NEG_RESTART)
+			data |= PHY_AUTO_NEG_RESTART;
+		if (force & PORT_FORCE_FULL_DUPLEX)
+			data |= PHY_FULL_DUPLEX;
+		if (speed & PORT_HP_MDIX)
+			data |= PHY_HP_MDIX;
+		if (restart & PORT_FORCE_MDIX)
+			data |= PHY_FORCE_MDIX;
+		if (restart & PORT_AUTO_MDIX_DISABLE)
+			data |= PHY_AUTO_MDIX_DISABLE;
+		if (restart & PORT_TX_DISABLE)
+			data |= PHY_TRANSMIT_DISABLE;
+		if (restart & PORT_LED_OFF)
+			data |= PHY_LED_DISABLE;
+		break;
+	case PHY_REG_STATUS:
+		port_r(sw, p, P_LINK_STATUS, &link);
+		port_r(sw, p, P_SPEED_STATUS, &speed);
+		data = PHY_100BTX_FD_CAPABLE |
+			PHY_100BTX_CAPABLE |
+			PHY_10BT_FD_CAPABLE |
+			PHY_10BT_CAPABLE |
+			PHY_AUTO_NEG_CAPABLE;
+		if (link & PORT_AUTO_NEG_COMPLETE)
+			data |= PHY_AUTO_NEG_ACKNOWLEDGE;
+		if (link & PORT_STAT_LINK_GOOD)
+			data |= PHY_LINK_STATUS;
+		break;
+	case PHY_REG_ID_1:
+		data = KSZ8895_ID_HI;
+		break;
+	case PHY_REG_ID_2:
+		/* Use unique switch id to differentiate from regular PHY. */
+		data = KSZ8895_SW_ID;
+		break;
+	case PHY_REG_AUTO_NEGOTIATION:
+		port_r(sw, p, P_LOCAL_CTRL, &ctrl);
+		data = PHY_AUTO_NEG_802_3;
+		if (ctrl & PORT_AUTO_NEG_SYM_PAUSE)
+			data |= PHY_AUTO_NEG_SYM_PAUSE;
+		if (ctrl & PORT_AUTO_NEG_100BTX_FD)
+			data |= PHY_AUTO_NEG_100BTX_FD;
+		if (ctrl & PORT_AUTO_NEG_100BTX)
+			data |= PHY_AUTO_NEG_100BTX;
+		if (ctrl & PORT_AUTO_NEG_10BT_FD)
+			data |= PHY_AUTO_NEG_10BT_FD;
+		if (ctrl & PORT_AUTO_NEG_10BT)
+			data |= PHY_AUTO_NEG_10BT;
+		break;
+	case PHY_REG_REMOTE_CAPABILITY:
+		port_r(sw, p, P_REMOTE_STATUS, &link);
+		data = PHY_AUTO_NEG_802_3;
+		if (link & PORT_REMOTE_SYM_PAUSE)
+			data |= PHY_AUTO_NEG_SYM_PAUSE;
+		if (link & PORT_REMOTE_100BTX_FD)
+			data |= PHY_AUTO_NEG_100BTX_FD;
+		if (link & PORT_REMOTE_100BTX)
+			data |= PHY_AUTO_NEG_100BTX;
+		if (link & PORT_REMOTE_10BT_FD)
+			data |= PHY_AUTO_NEG_10BT_FD;
+		if (link & PORT_REMOTE_10BT)
+			data |= PHY_AUTO_NEG_10BT;
+		break;
+	default:
+		break;
+	}
+	*val = data;
+}  /* sw_r_phy */
+
+/**
+ * sw_w_phy - write data to PHY register
+ * @hw:		The switch instance.
+ * @phy:	PHY address to write.
+ * @reg:	PHY register to write.
+ * @val:	Word data to write.
+ *
+ * This routine writes data to the PHY register.
+ */
+static void sw_w_phy(struct ksz_sw *sw, u16 phy, u16 reg, u16 val)
+{
+	u8 ctrl;
+	u8 restart;
+	u8 speed;
+	u8 data;
+	u8 p;
+
+	if (phy)
+		p = phy - 1;
+	else
+		return;
+	switch (reg) {
+	case PHY_REG_CTRL:
+		port_r(sw, p, P_SPEED_STATUS, &speed);
+		data = speed;
+		if (val & PHY_HP_MDIX)
+			data |= PORT_HP_MDIX;
+		else
+			data &= ~PORT_HP_MDIX;
+		if (data != speed)
+			port_w(sw, p, P_SPEED_STATUS, data);
+		port_r(sw, p, P_FORCE_CTRL, &ctrl);
+		data = ctrl;
+		if (!(val & PHY_AUTO_NEG_ENABLE))
+			data |= PORT_AUTO_NEG_DISABLE;
+		else
+			data &= ~PORT_AUTO_NEG_DISABLE;
+		if (val & PHY_SPEED_100MBIT)
+			data |= PORT_FORCE_100_MBIT;
+		else
+			data &= ~PORT_FORCE_100_MBIT;
+		if (val & PHY_FULL_DUPLEX)
+			data |= PORT_FORCE_FULL_DUPLEX;
+		else
+			data &= ~PORT_FORCE_FULL_DUPLEX;
+		if (data != ctrl)
+			port_w(sw, p, P_FORCE_CTRL, data);
+		port_r(sw, p, P_NEG_RESTART_CTRL, &restart);
+		data = restart;
+		if (val & PHY_LED_DISABLE)
+			data |= PORT_LED_OFF;
+		else
+			data &= ~PORT_LED_OFF;
+		if (val & PHY_TRANSMIT_DISABLE)
+			data |= PORT_TX_DISABLE;
+		else
+			data &= ~PORT_TX_DISABLE;
+		if (val & PHY_AUTO_NEG_RESTART)
+			data |= PORT_AUTO_NEG_RESTART;
+		else
+			data &= ~(PORT_AUTO_NEG_RESTART);
+		if (val & PHY_POWER_DOWN)
+			data |= PORT_POWER_DOWN;
+		else
+			data &= ~PORT_POWER_DOWN;
+		if (val & PHY_AUTO_MDIX_DISABLE)
+			data |= PORT_AUTO_MDIX_DISABLE;
+		else
+			data &= ~PORT_AUTO_MDIX_DISABLE;
+		if (val & PHY_FORCE_MDIX)
+			data |= PORT_FORCE_MDIX;
+		else
+			data &= ~PORT_FORCE_MDIX;
+		if (val & PHY_LOOPBACK)
+			data |= PORT_PHY_LOOPBACK;
+		else
+			data &= ~PORT_PHY_LOOPBACK;
+		if (data != restart)
+			port_w(sw, p, P_NEG_RESTART_CTRL, data);
+		break;
+	case PHY_REG_AUTO_NEGOTIATION:
+		port_r(sw, p, P_LOCAL_CTRL, &ctrl);
+		data = ctrl;
+		data &= ~(PORT_AUTO_NEG_SYM_PAUSE |
+			PORT_AUTO_NEG_100BTX_FD |
+			PORT_AUTO_NEG_100BTX |
+			PORT_AUTO_NEG_10BT_FD |
+			PORT_AUTO_NEG_10BT);
+		if (val & PHY_AUTO_NEG_SYM_PAUSE)
+			data |= PORT_AUTO_NEG_SYM_PAUSE;
+		if (val & PHY_AUTO_NEG_100BTX_FD)
+			data |= PORT_AUTO_NEG_100BTX_FD;
+		if (val & PHY_AUTO_NEG_100BTX)
+			data |= PORT_AUTO_NEG_100BTX;
+		if (val & PHY_AUTO_NEG_10BT_FD)
+			data |= PORT_AUTO_NEG_10BT_FD;
+		if (val & PHY_AUTO_NEG_10BT)
+			data |= PORT_AUTO_NEG_10BT;
+		if (data != ctrl)
+			port_w(sw, p, P_LOCAL_CTRL, data);
+		break;
+	default:
+		break;
+	}
+}  /* sw_w_phy */
+
+static int port_chk_force_link(struct ksz_sw *sw, uint p, SW_D local,
+	SW_D remote, SW_D status)
+{
+#define PORT_REMOTE_STATUS				\
+	(PORT_REMOTE_100BTX_FD | PORT_REMOTE_100BTX |	\
+	PORT_REMOTE_10BT_FD | PORT_REMOTE_10BT)
+
+	SW_D data;
+	static SW_D saved_ctrl;
+	static SW_D saved_status;
+	static int test_stage;
+
+	port_r(sw, p, P_FORCE_CTRL, &data);
+	if (data & PORT_AUTO_NEG_DISABLE)
+		return 0;
+	if ((local & (PORT_REMOTE_100BTX_FD | PORT_REMOTE_100BTX)) !=
+	    (PORT_REMOTE_100BTX_FD | PORT_REMOTE_100BTX) &&
+	    (local & (PORT_REMOTE_10BT_FD | PORT_REMOTE_10BT)) !=
+	    (PORT_REMOTE_10BT_FD | PORT_REMOTE_10BT))
+		return 0;
+	if (!(remote & PORT_REMOTE_SYM_PAUSE) &&
+	    (remote & PORT_REMOTE_STATUS) != PORT_REMOTE_STATUS) {
+		if (saved_ctrl) {
+			if ((status & PORT_STAT_FULL_DUPLEX) !=
+			    (saved_status & PORT_STAT_FULL_DUPLEX)) {
+				printk(KERN_INFO
+					"%d-%d: duplex is defaulted to %s\n",
+					sw->id, p,
+					(saved_ctrl & PORT_FORCE_FULL_DUPLEX) ?
+					"full" : "half");
+			}
+			if (data != saved_ctrl) {
+				port_w(sw, p, P_FORCE_CTRL, saved_ctrl);
+				port_r(sw, p, P_NEG_RESTART_CTRL, &data);
+				data |= PORT_AUTO_NEG_RESTART;
+				port_w(sw, p, P_NEG_RESTART_CTRL, data);
+				test_stage = 2;
+			} else
+				test_stage = 0;
+			saved_ctrl = 0;
+			if (test_stage)
+				return 1;
+		} else if (!test_stage) {
+			saved_ctrl = data;
+			saved_status = status;
+			if (status & PORT_STAT_FULL_DUPLEX)
+				data &= ~PORT_FORCE_FULL_DUPLEX;
+			else
+				data |= PORT_FORCE_FULL_DUPLEX;
+			port_w(sw, p, P_FORCE_CTRL, data);
+			port_r(sw, p, P_NEG_RESTART_CTRL, &data);
+			data |= PORT_AUTO_NEG_RESTART;
+			port_w(sw, p, P_NEG_RESTART_CTRL, data);
+			test_stage = 1;
+			return 1;
+		}
+		test_stage = 0;
+	}
+	return 0;
+}  /* port_chk_force_link */
+
+/**
+ * port_get_link_speed - get current link status
+ * @port:	The port instance.
+ *
+ * This routine reads PHY registers to determine the current link status of the
+ * switch ports.
+ */
+static int port_get_link_speed(struct ksz_port *port)
+{
+	struct ksz_port_info *info;
+	struct ksz_port_info *linked = NULL;
+	struct ksz_port_state *state;
+	struct ksz_sw *sw = port->sw;
+	SW_D data;
+	SW_D status;
+	SW_D link;
+	SW_D local;
+	SW_D remote;
+	uint i;
+	uint n;
+	uint p;
+	int change = 0;
+
+	/*
+	 * Only check port which has interrupts triggered.
+	 * If no interrupt poll all the ports with PHY.
+	 */
+	if (!sw->phy_intr)
+		sw->phy_intr = sw->PORT_MASK;
+	for (i = 0, n = port->first_port; i < port->port_cnt; i++, n++) {
+		p = get_phy_port(sw, n);
+		info = get_port_info(sw, p);
+		if (!(sw->phy_intr & (1 << p))) {
+			if (!linked && p != sw->HOST_PORT &&
+			    info->state == media_connected)
+				linked = info;
+			continue;
+		}
+		sw->phy_intr &= ~(1 << p);
+
+		state = &sw->port_state[p];
+		port_r(sw, p, P_LOCAL_CTRL, &local);
+		port_r(sw, p, P_REMOTE_STATUS, &remote);
+		port_r(sw, p, P_SPEED_STATUS, &status);
+		data = remote;
+
+		/*
+		 * The partner capability register is updated but the
+		 * auto-negotiation is not completed yet.
+		 */
+		link = data & (PORT_AUTO_NEG_COMPLETE | PORT_STAT_LINK_GOOD);
+		link |= status &
+			(PORT_STAT_SPEED_100MBIT | PORT_STAT_FULL_DUPLEX);
+
+		if (data & PORT_STAT_LINK_GOOD) {
+
+			/* Remember the first linked port. */
+			if (!linked)
+				linked = info;
+		}
+
+#if 1
+		/* Bit 7 fluctuates in port 0. */
+		if (0 == p)
+			local &= ~PORT_AUTO_NEG_DISABLE;
+#endif
+
+		/* No change to status. */
+		if (local == info->advertised && link == info->link)
+			continue;
+
+#ifdef DBG_LINK
+		printk(KERN_INFO
+			"%d=advertised: %02X-%02X; partner: %02X-%02X"
+			"; link: %02X-%02X\n", p,
+			local, info->advertised, remote, info->partner,
+			link, info->link);
+#endif
+		if (data & PORT_STAT_LINK_GOOD) {
+			if (port_chk_force_link(sw, p, local, remote, status)) {
+				if (linked == info)
+					linked = NULL;
+				continue;
+			}
+			info->tx_rate = 10 * TX_RATE_UNIT;
+			if (status & PORT_STAT_SPEED_100MBIT)
+				info->tx_rate = 100 * TX_RATE_UNIT;
+
+			info->duplex = 1;
+			if (status & PORT_STAT_FULL_DUPLEX)
+				info->duplex = 2;
+
+#ifdef DBG_LINK
+			printk(KERN_INFO "flow_ctrl: "SW_SIZE_STR"\n", status &
+				(PORT_RX_FLOW_CTRL | PORT_TX_FLOW_CTRL));
+#endif
+			if (media_connected != info->state) {
+				info->flow_ctrl = sw_determine_flow_ctrl(sw,
+					port, local, remote);
+				if (status & PORT_RX_FLOW_CTRL)
+					info->flow_ctrl |= 0x10;
+				if (status & PORT_TX_FLOW_CTRL)
+					info->flow_ctrl |= 0x20;
+				if (sw->info)
+					port_cfg_back_pressure(sw, p,
+						(1 == info->duplex));
+				change |= 1 << i;
+			} else if (link != info->link)
+				change |= 1 << i;
+			info->state = media_connected;
+			sw_r_phy(sw, p + 1, PHY_REG_REMOTE_CAPABILITY,
+				 &info->lpa);
+		} else {
+			if (media_disconnected != info->state) {
+				change |= 1 << i;
+
+				/* Indicate the link just goes down. */
+				state->link_down = 1;
+			}
+			info->state = media_disconnected;
+		}
+		info->advertised = local;
+		info->partner = remote;
+		info->link = link;
+		state->state = info->state;
+	}
+
+	if (linked && media_disconnected == port->linked->state)
+		port->linked = linked;
+
+#ifdef DBG_LINK
+	if (change)
+		dbp_link(port, sw, change);
+#endif
+	/* Only update for regular port. */
+	if (change && port->first_port <= sw->mib_port_cnt) {
+		port->report = true;
+		port->link_ports |= change;
+	}
+	return change;
+}  /* port_get_link_speed */
+
+/**
+ * port_set_link_speed - set port speed
+ * @port:	The port instance.
+ *
+ * This routine sets the link speed of the switch ports.
+ */
+static void port_set_link_speed(struct ksz_port *port)
+{
+	struct ksz_sw *sw = port->sw;
+	struct ksz_port_info *info;
+	SW_D adv;
+	SW_D data;
+	SW_D local;
+	SW_D status;
+	uint i;
+	uint n;
+	uint p;
+
+	for (i = 0, n = port->first_port; i < port->port_cnt; i++, n++) {
+		p = get_phy_port(sw, n);
+		info = get_port_info(sw, p);
+		info->own_flow_ctrl = port->flow_ctrl;
+		info->own_duplex = port->duplex;
+		info->own_speed = port->speed;
+
+		port_r(sw, p, P_LOCAL_CTRL, &local);
+		port_r(sw, p, P_LINK_STATUS, &status);
+
+		adv = 0;
+		if (status & PORT_STAT_LINK_GOOD) {
+			adv = local;
+		}
+
+		local &= ~PORT_AUTO_NEG_DISABLE;
+		local = port_advertised_flow_ctrl(port, local);
+
+		local |= PORT_AUTO_NEG_100BTX_FD | PORT_AUTO_NEG_100BTX |
+			PORT_AUTO_NEG_10BT_FD | PORT_AUTO_NEG_10BT;
+
+		/* Check if manual configuration is specified by the user. */
+		if (port->speed || port->duplex) {
+			if (10 == port->speed)
+				local &= ~(PORT_AUTO_NEG_100BTX_FD |
+					PORT_AUTO_NEG_100BTX);
+			else if (100 == port->speed)
+				local &= ~(PORT_AUTO_NEG_10BT_FD |
+					PORT_AUTO_NEG_10BT);
+			if (1 == port->duplex)
+				local &= ~(PORT_AUTO_NEG_100BTX_FD |
+					PORT_AUTO_NEG_10BT_FD);
+			else if (2 == port->duplex)
+				local &= ~(PORT_AUTO_NEG_100BTX |
+					PORT_AUTO_NEG_10BT);
+		}
+		if (local != adv) {
+			port_w(sw, p, P_LOCAL_CTRL, local);
+			port_r(sw, p, P_NEG_RESTART_CTRL, &data);
+			data |= PORT_AUTO_NEG_RESTART;
+			port_w(sw, p, P_NEG_RESTART_CTRL, data);
+
+			/* Link is going down. */
+			sw->port_state[p].state = media_disconnected;
+		}
+	}
+}  /* port_set_link_speed */
+
+/**
+ * port_force_link_speed - force port speed
+ * @port:	The port instance.
+ *
+ * This routine forces the link speed of the switch ports.
+ */
+static void port_force_link_speed(struct ksz_port *port)
+{
+	struct ksz_sw *sw = port->sw;
+	struct ksz_port_info *info;
+	SW_D data;
+	uint i;
+	uint n;
+	uint p;
+
+	for (i = 0, n = port->first_port; i < port->port_cnt; i++, n++) {
+		p = get_phy_port(sw, n);
+		info = get_port_info(sw, p);
+		info->own_flow_ctrl = port->flow_ctrl;
+		info->own_duplex = port->duplex;
+		info->own_speed = port->speed;
+
+		port_r(sw, p, P_FORCE_CTRL, &data);
+		data |= PORT_AUTO_NEG_DISABLE;
+		if (10 == port->speed)
+			data &= ~PORT_FORCE_100_MBIT;
+		else if (100 == port->speed)
+			data |= PORT_FORCE_100_MBIT;
+		if (1 == port->duplex)
+			data &= ~PORT_FORCE_FULL_DUPLEX;
+		else if (2 == port->duplex)
+			data |= PORT_FORCE_FULL_DUPLEX;
+		port_w(sw, p, P_FORCE_CTRL, data);
+	}
+}  /* port_force_link_speed */
+
+/**
+ * sw_enable - enable the switch
+ * @sw:		The switch instance.
+ *
+ */
+static void sw_enable(struct ksz_sw *sw)
+{
+	uint n;
+	uint port;
+	bool fewer;
+	struct ksz_port_cfg *cfg;
+	struct ksz_port_info *info;
+	int state = STP_STATE_FORWARDING;
+
+	/* Manually change default membership when not all ports are used. */
+	fewer = false;
+	for (port = 0; port < sw->port_cnt; port++) {
+		info = get_port_info(sw, port);
+		if (info->log_m) {
+			cfg = get_port_cfg(sw, port);
+			cfg->vid_member = sw->PORT_MASK;
+		} else {
+			fewer = true;
+			port_set_stp_state(sw, port, STP_STATE_DISABLED);
+		}
+	}
+	if (fewer)
+dbg_msg(" fewer: %d %d\n", fewer, sw->eth_cnt);
+	if (fewer)
+		sw_cfg_port_base_vlan(sw, sw->HOST_PORT, sw->PORT_MASK);
+	if ((sw->dev_count > 1 && !sw->dev_offset) ||
+	    (sw->features & STP_SUPPORT)) {
+		u8 member;
+
+		for (n = 1; n <= sw->mib_port_cnt; n++) {
+			port = get_phy_port(sw, n);
+			member = (1 << port);
+			if (sw->features & SW_VLAN_DEV) {
+				struct ksz_dev_map *map;
+				int q;
+
+				for (q = 0; q < sw->eth_cnt; q++) {
+					map = &sw->eth_maps[q];
+					if (map->first <= n &&
+					    n <= map->first + map->cnt - 1) {
+						member = map->mask;
+						break;
+					}
+				}
+			}
+			cfg = get_port_cfg(sw, port);
+			cfg->vid_member = member;
+		}
+	} else if (1 == sw->eth_cnt) {
+		u8 member;
+
+		for (n = 1; n <= sw->mib_port_cnt; n++) {
+			port = get_phy_port(sw, n);
+			member = 0;
+			if (sw->eth_maps[0].mask & (1 << port))
+				member = sw->eth_maps[0].mask;
+			cfg = get_port_cfg(sw, port);
+			cfg->vid_member = member;
+		}
+	}
+	for (n = 1; n <= sw->mib_port_cnt; n++) {
+		port = get_phy_port(sw, n);
+		if (sw->dev_count > 1 ||
+		    (sw->eth_maps[0].mask &&
+		    !(sw->eth_maps[0].mask & (1 << port))))
+			port_set_stp_state(sw, port, STP_STATE_DISABLED);
+		else
+			port_set_stp_state(sw, port, state);
+	}
+	if (sw->dev_count > 1 && !sw->dev_offset && sw->eth_cnt < 2)
+		port_set_stp_state(sw, sw->HOST_PORT, STP_STATE_SIMPLE);
+	else
+		port_set_stp_state(sw, sw->HOST_PORT, state);
+
+	/*
+	 * There may be some entries in the dynamic MAC table before the
+	 * the learning is turned off.  Once the entries are in the table the
+	 * switch may keep updating them even learning is off.
+	 */
+	if (sw->dev_count > 1)
+		sw_flush_dyn_mac_table(sw, TOTAL_PORT_NUM);
+	sw->reg->w8(sw, REG_CHIP_ID1, SW_START);
+}  /* sw_enable */
+
+/**
+ * sw_init - initialize the switch
+ * @sw:		The switch instance.
+ *
+ * This routine initializes the hardware switch engine for default operation.
+ */
+static void sw_init(struct ksz_sw *sw)
+{
+	memset(sw->tx_pad, 0, 60);
+	sw->tx_start = 0;
+	sw_init_broad_storm(sw);
+
+	sw_init_prio(sw);
+
+	sw_init_prio_rate(sw);
+
+	sw_init_vlan(sw);
+
+	if (sw_chk(sw, REG_SW_CTRL_1,
+			SW_TX_FLOW_CTRL_DISABLE | SW_RX_FLOW_CTRL_DISABLE))
+		sw->overrides |= PAUSE_FLOW_CTRL;
+}  /* sw_init */
+
+/**
+ * sw_setup - setup the switch
+ * @sw:		The switch instance.
+ *
+ * This routine setup the hardware switch engine for default operation.
+ */
+static void sw_setup(struct ksz_sw *sw)
+{
+	uint n;
+	uint port;
+
+	sw_set_global_ctrl(sw);
+	for (n = 1; n <= sw->mib_port_cnt; n++) {
+		SW_D data;
+
+		port = get_phy_port(sw, n);
+		port_cfg_back_pressure(sw, port, 1);
+		port_cfg_force_flow_ctrl(sw, port, 0);
+
+		/*
+		 * Switch actually cannot do auto-negotiation with old 10Mbit
+		 * hub.
+		 */
+		port_r(sw, port, P_FORCE_CTRL, &data);
+		data &= ~PORT_FORCE_FULL_DUPLEX;
+		port_w(sw, port, P_FORCE_CTRL, data);
+	}
+
+	sw_setup_broad_storm(sw);
+
+	sw_setup_prio(sw);
+
+	sw_setup_mirror(sw);
+
+	sw->info->multi_sys = MULTI_MAC_TABLE_ENTRIES;
+	sw->info->multi_net = SWITCH_MAC_TABLE_ENTRIES;
+	if (sw->features & STP_SUPPORT) {
+		sw->ops->release(sw);
+		sw_setup_stp(sw);
+		sw->ops->acquire(sw);
+	}
+}  /* sw_setup */
+
+static inline void sw_reset(struct ksz_sw *sw)
+{
+	sw->reg->w8(sw, REG_POWER_MANAGEMENT_1,
+		SW_SOFTWARE_POWER_DOWN << SW_POWER_MANAGEMENT_MODE_S);
+	sw->reg->w8(sw, REG_POWER_MANAGEMENT_1, 0);
+}  /* sw_reset */
+
+/* -------------------------------------------------------------------------- */
+
+#define KSZSW_REGS_SIZE			0x100
+
+static struct sw_regs {
+	int start;
+	int end;
+} sw_regs_range[] = {
+	{ 0x000, 0x100 },
+	{ 0, 0 }
+};
+
+static int check_sw_reg_range(unsigned addr)
+{
+	struct sw_regs *range = sw_regs_range;
+
+	while (range->end > range->start) {
+		if (range->start <= addr && addr < range->end)
+			return true;
+		range++;
+	}
+	return false;
+}
+
+/* -------------------------------------------------------------------------- */
+
+#ifdef KSZSW_REGS_SIZE
+static struct ksz_sw *get_sw_data(struct device *d)
+{
+	struct sw_priv *hw_priv = dev_get_drvdata(d);
+
+	return &hw_priv->sw;
+}
+
+static ssize_t kszsw_registers_read(struct file *filp, struct kobject *kobj,
+	struct bin_attribute *bin_attr, char *buf, loff_t off, size_t count)
+{
+	size_t i;
+	SW_D reg;
+	struct device *dev;
+	struct ksz_sw *sw;
+
+	if (unlikely(off > KSZSW_REGS_SIZE))
+		return 0;
+
+	if ((off + count) > KSZSW_REGS_SIZE)
+		count = KSZSW_REGS_SIZE - off;
+
+	if (unlikely(!count))
+		return count;
+
+	dev = container_of(kobj, struct device, kobj);
+	sw = get_sw_data(dev);
+
+	reg = off;
+	sw->ops->acquire(sw);
+	i = sw->reg->get(sw, reg, count, buf);
+	sw->ops->release(sw);
+
+	/* Linux 5.10 return -1 to application if sizes do not match. */
+	if (i > count)
+		i = count;
+	return i;
+}
+
+static ssize_t kszsw_registers_write(struct file *filp, struct kobject *kobj,
+	struct bin_attribute *bin_attr, char *buf, loff_t off, size_t count)
+{
+	size_t i;
+	SW_D reg;
+	struct device *dev;
+	struct ksz_sw *sw;
+
+	if (unlikely(off >= KSZSW_REGS_SIZE))
+		return -EFBIG;
+
+	if ((off + count) > KSZSW_REGS_SIZE)
+		count = KSZSW_REGS_SIZE - off;
+
+	if (unlikely(!count))
+		return count;
+
+	dev = container_of(kobj, struct device, kobj);
+	sw = get_sw_data(dev);
+
+	reg = off;
+	sw->ops->acquire(sw);
+	i = sw->reg->set(sw, reg, count, buf);
+	sw->ops->release(sw);
+	return i;
+}
+
+static struct bin_attribute kszsw_registers_attr = {
+	.attr = {
+		.name	= "registers",
+		.mode	= S_IRUSR | S_IWUSR,
+	},
+	.size	= KSZSW_REGS_SIZE,
+	.read	= kszsw_registers_read,
+	.write	= kszsw_registers_write,
+};
+#endif
+
+static int sw_reg_get(struct ksz_sw *sw, u32 reg, size_t count, char *buf)
+{
+	size_t i;
+	SW_D *addr;
+
+	addr = (SW_D *) buf;
+	for (i = 0; i < count; i += SW_SIZE, reg += SW_SIZE, addr++) {
+		*addr = 0;
+		if (check_sw_reg_range(reg))
+			*addr = SW_R(sw, reg);
+	}
+	return i;
+}
+
+static int sw_reg_set(struct ksz_sw *sw, u32 reg, size_t count, char *buf)
+{
+	size_t i;
+	SW_D *addr;
+
+	addr = (SW_D *) buf;
+	for (i = 0; i < count; i += SW_SIZE, reg += SW_SIZE, addr++) {
+		if (check_sw_reg_range(reg))
+			SW_W(sw, reg, *addr);
+	}
+	return i;
+}
+
+#ifdef CONFIG_KSZ_MRP
+static void sw_set_mrp(struct work_struct *work)
+{
+	struct delayed_work *dwork = to_delayed_work(work);
+	struct ksz_sw *sw = container_of(dwork, struct ksz_sw, set_mrp);
+
+	if (!netif_carrier_ok(sw->main_dev)) {
+		schedule_delayed_work(&sw->set_mrp, msecs_to_jiffies(500));
+		return;
+	}
+
+	mrp_start(&sw->mrp);
+}  /* sw_set_mrp */
+#endif
+
+/* -------------------------------------------------------------------------- */
+
+/*
+ * Microchip LinkMD routines
+ */
+
+enum {
+	CABLE_UNKNOWN,
+	CABLE_GOOD,
+	CABLE_CROSSED,
+	CABLE_REVERSED,
+	CABLE_CROSSED_REVERSED,
+	CABLE_OPEN,
+	CABLE_SHORT
+};
+
+#define STATUS_FULL_DUPLEX		0x01
+#define STATUS_CROSSOVER		0x02
+#define STATUS_REVERSED			0x04
+
+#define LINK_10MBPS_FULL		0x00000001
+#define LINK_10MBPS_HALF		0x00000002
+#define LINK_100MBPS_FULL		0x00000004
+#define LINK_100MBPS_HALF		0x00000008
+#define LINK_1GBPS_FULL			0x00000010
+#define LINK_1GBPS_HALF			0x00000020
+#define LINK_10GBPS_FULL		0x00000040
+#define LINK_10GBPS_HALF		0x00000080
+#define LINK_SYM_PAUSE			0x00000100
+#define LINK_ASYM_PAUSE			0x00000200
+
+#define LINK_AUTO_MDIX			0x00010000
+#define LINK_MDIX			0x00020000
+#define LINK_AUTO_POLARITY		0x00040000
+
+#define CABLE_LEN_MAXIMUM		15000
+#define CABLE_LEN_MULTIPLIER		40
+
+#define PHY_RESET_TIMEOUT		10
+
+/**
+ * sw_get_link_md -
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ *
+ * This routine is used to get the LinkMD status.
+ */
+static void sw_get_link_md(struct ksz_sw *sw, uint port)
+{
+	SW_D crossover;
+	SW_D ctrl;
+	SW_D data;
+	SW_D link;
+	u16 len;
+	int i;
+	int timeout;
+	struct ksz_port_info *port_info = get_port_info(sw, port);
+
+	port_r(sw, port, P_SPEED_STATUS, &data);
+	port_r(sw, port, P_LINK_STATUS, &link);
+	port_info->status[0] = CABLE_UNKNOWN;
+	if (link & PORT_STAT_LINK_GOOD) {
+		int stat = 0;
+
+		port_info->status[0] = CABLE_GOOD;
+		port_info->length[0] = 1;
+		port_info->status[1] = CABLE_GOOD;
+		port_info->length[1] = 1;
+		port_info->status[2] = CABLE_GOOD;
+		port_info->length[2] = 1;
+
+		if (link & PORT_MDIX_STATUS)
+			stat |= STATUS_CROSSOVER;
+		if (data & PORT_REVERSED_POLARITY)
+			stat |= STATUS_REVERSED;
+		if ((stat & (STATUS_CROSSOVER | STATUS_REVERSED)) ==
+				(STATUS_CROSSOVER | STATUS_REVERSED))
+			port_info->status[0] = CABLE_CROSSED_REVERSED;
+		else if ((stat & STATUS_CROSSOVER) == STATUS_CROSSOVER)
+			port_info->status[0] = CABLE_CROSSED;
+		else if ((stat & STATUS_REVERSED) == STATUS_REVERSED)
+			port_info->status[0] = CABLE_REVERSED;
+		return;
+	}
+
+	/* Put in 10 Mbps mode. */
+	port_r(sw, port, P_FORCE_CTRL, &ctrl);
+	data = ctrl;
+	data &= ~(PORT_FORCE_100_MBIT |	PORT_FORCE_FULL_DUPLEX);
+	data |= PORT_AUTO_NEG_DISABLE;
+	port_w(sw, port, P_FORCE_CTRL, data);
+
+	port_r(sw, port, P_NEG_RESTART_CTRL, &data);
+	crossover = data;
+
+	for (i = 1; i <= 2; i++) {
+		data = crossover;
+
+		/* Disable auto MDIX. */
+		data |= PORT_AUTO_MDIX_DISABLE;
+		if (0 == i)
+			data &= ~PORT_FORCE_MDIX;
+		else
+			data |= PORT_FORCE_MDIX;
+
+		/* Disable transmitter. */
+		data |= PORT_TX_DISABLE;
+		port_w(sw, port, P_NEG_RESTART_CTRL, data);
+
+		/* Wait at most 1 second. */
+		delay_milli(100);
+
+		/* Enable transmitter. */
+		data &= ~PORT_TX_DISABLE;
+		port_w(sw, port, P_NEG_RESTART_CTRL, data);
+
+		/* Start cable diagnostic test. */
+		port_r(sw, port, REG_PORT_LINK_MD_CTRL, &data);
+		data |= PORT_START_CABLE_DIAG;
+		port_w(sw, port, REG_PORT_LINK_MD_CTRL, data);
+		timeout = PHY_RESET_TIMEOUT;
+		do {
+			if (!--timeout)
+				break;
+			delay_milli(10);
+			port_r(sw, port, REG_PORT_LINK_MD_CTRL, &data);
+		} while ((data & PORT_START_CABLE_DIAG));
+
+		port_info->length[i] = 0;
+		port_info->status[i] = CABLE_UNKNOWN;
+
+		if (!(data & PORT_START_CABLE_DIAG)) {
+			port_r8(sw, port, REG_PORT_LINK_MD_RESULT, &link);
+			len = data & PORT_CABLE_FAULT_COUNTER_H;
+			len <<= 16;
+			len |= link;
+			len *= CABLE_LEN_MULTIPLIER;
+			len /= 100;
+			port_info->length[i] = len;
+			if (data & PORT_CABLE_10M_SHORT)
+				port_info->length[i] = 1;
+			data >>= PORT_CABLE_DIAG_RESULT_S;
+			data &= PORT_CABLE_DIAG_RESULT_M;
+			switch (data) {
+			case PORT_CABLE_STAT_NORMAL:
+				port_info->status[i] = CABLE_GOOD;
+				port_info->length[i] = 1;
+				break;
+			case PORT_CABLE_STAT_OPEN:
+				port_info->status[i] = CABLE_OPEN;
+				break;
+			case PORT_CABLE_STAT_SHORT:
+				port_info->status[i] = CABLE_SHORT;
+				break;
+			}
+		}
+	}
+
+	port_w(sw, port, P_FORCE_CTRL, ctrl);
+	if (!(ctrl & PORT_AUTO_NEG_DISABLE)) {
+		crossover |= PORT_AUTO_NEG_RESTART;
+		port_w(sw, port, P_NEG_RESTART_CTRL, crossover);
+	}
+
+	port_info->length[0] = port_info->length[1];
+	port_info->status[0] = port_info->status[1];
+	for (i = 2; i < 3; i++) {
+		if (CABLE_GOOD == port_info->status[0]) {
+			if (port_info->status[i] != CABLE_GOOD) {
+				port_info->status[0] = port_info->status[i];
+				port_info->length[0] = port_info->length[i];
+				break;
+			}
+		}
+	}
+}  /* sw_get_link_md */
+
+/* -------------------------------------------------------------------------- */
+
+static void get_sw_mib_counters(struct ksz_sw *sw, int first, int cnt,
+	u64 *counter)
+{
+	int i;
+	int mib;
+	uint n;
+	uint p;
+	struct ksz_port_mib *port_mib;
+
+	memset(counter, 0, sizeof(u64) * TOTAL_SWITCH_COUNTER_NUM);
+	for (i = 0, n = first; i < cnt; i++, n++) {
+		p = get_phy_port(sw, n);
+		port_mib = get_port_mib(sw, p);
+		for (mib = port_mib->mib_start; mib < sw->mib_cnt; mib++)
+			counter[mib] += port_mib->counter[mib];
+	}
+}
+
+#define MIB_RX_LO_PRIO			0x00
+#define MIB_RX_HI_PRIO			0x01
+#define MIB_RX_UNDERSIZE		0x02
+#define MIB_RX_FRAGMENT			0x03
+#define MIB_RX_OVERSIZE			0x04
+#define MIB_RX_JABBER			0x05
+#define MIB_RX_SYMBOL_ERR		0x06
+#define MIB_RX_CRC_ERR			0x07
+#define MIB_RX_ALIGNMENT_ERR		0x08
+#define MIB_RX_CTRL_8808		0x09
+#define MIB_RX_PAUSE			0x0A
+#define MIB_RX_BROADCAST		0x0B
+#define MIB_RX_MULTICAST		0x0C
+#define MIB_RX_UNICAST			0x0D
+#define MIB_RX_OCTET_64			0x0E
+#define MIB_RX_OCTET_65_127		0x0F
+#define MIB_RX_OCTET_128_255		0x10
+#define MIB_RX_OCTET_256_511		0x11
+#define MIB_RX_OCTET_512_1023		0x12
+#define MIB_RX_OCTET_1024_1522		0x13
+#define MIB_TX_LO_PRIO			0x14
+#define MIB_TX_HI_PRIO			0x15
+#define MIB_TX_LATE_COLLISION		0x16
+#define MIB_TX_PAUSE			0x17
+#define MIB_TX_BROADCAST		0x18
+#define MIB_TX_MULTICAST		0x19
+#define MIB_TX_UNICAST			0x1A
+#define MIB_TX_DEFERRED			0x1B
+#define MIB_TX_TOTAL_COLLISION		0x1C
+#define MIB_TX_EXCESS_COLLISION		0x1D
+#define MIB_TX_SINGLE_COLLISION		0x1E
+#define MIB_TX_MULTI_COLLISION		0x1F
+
+#define MIB_RX_DROPS			0x20
+#define MIB_TX_DROPS			0x21
+
+static struct {
+	char string[20];
+} mib_names[TOTAL_SWITCH_COUNTER_NUM] = {
+	{ "rx           " },
+	{ "rx_hi        " },
+	{ "rx_undersize" },
+	{ "rx_fragments" },
+	{ "rx_oversize" },
+	{ "rx_jabbers" },
+	{ "rx_symbol_err" },
+	{ "rx_crc_err" },
+	{ "rx_align_err" },
+	{ "rx_mac_ctrl" },
+	{ "rx_pause" },
+	{ "rx_bcast" },
+	{ "rx_mcast" },
+	{ "rx_ucast" },
+	{ "rx_64_or_less" },
+	{ "rx_65_127" },
+	{ "rx_128_255" },
+	{ "rx_256_511" },
+	{ "rx_512_1023" },
+	{ "rx_1024_1522" },
+
+	{ "tx           " },
+	{ "tx_hi        " },
+	{ "tx_late_col" },
+	{ "tx_pause" },
+	{ "tx_bcast" },
+	{ "tx_mcast" },
+	{ "tx_ucast" },
+	{ "tx_deferred" },
+	{ "tx_total_col" },
+	{ "tx_exc_col" },
+	{ "tx_single_col" },
+	{ "tx_mult_col" },
+
+	{ "rx_discards" },
+	{ "tx_discards" },
+};
+
+static struct {
+	int rx;
+	int tx;
+} mib_display[TOTAL_SWITCH_COUNTER_NUM / 2] = {
+	{ MIB_RX_LO_PRIO, MIB_TX_LO_PRIO },
+	{ MIB_RX_HI_PRIO, MIB_TX_HI_PRIO },
+	{ MIB_RX_PAUSE, MIB_TX_PAUSE },
+	{ MIB_RX_BROADCAST, MIB_TX_BROADCAST },
+	{ MIB_RX_MULTICAST, MIB_TX_MULTICAST },
+	{ MIB_RX_UNICAST, MIB_TX_UNICAST },
+	{ MIB_RX_OCTET_64, MIB_RX_OCTET_65_127 },
+	{ MIB_RX_OCTET_128_255, MIB_RX_OCTET_256_511 },
+	{ MIB_RX_OCTET_512_1023, MIB_RX_OCTET_1024_1522 },
+	{ MIB_RX_UNDERSIZE, MIB_RX_OVERSIZE },
+	{ MIB_RX_FRAGMENT, MIB_RX_JABBER },
+	{ MIB_RX_SYMBOL_ERR, MIB_RX_CRC_ERR },
+	{ MIB_RX_ALIGNMENT_ERR, MIB_RX_CTRL_8808 },
+	{ MIB_TX_LATE_COLLISION, MIB_TX_DEFERRED },
+	{ MIB_TX_TOTAL_COLLISION, MIB_TX_EXCESS_COLLISION },
+	{ MIB_TX_SINGLE_COLLISION, MIB_TX_MULTI_COLLISION },
+	{ MIB_RX_DROPS, MIB_TX_DROPS },
+};
+
+static int display_sw_mib_counters(struct ksz_sw *sw, int first, int cnt,
+	char *buf)
+{
+	int mib;
+	int n;
+	int len = 0;
+	u64 counter[TOTAL_SWITCH_COUNTER_NUM];
+
+	get_sw_mib_counters(sw, first, cnt, counter);
+	for (mib = 0; mib < TOTAL_SWITCH_COUNTER_NUM / 2; mib++) {
+		int rx = mib_display[mib].rx;
+		int tx = mib_display[mib].tx;
+		if (buf)
+			len += sprintf(buf + len,
+				"%s\t= %-20llu\t%s\t= %-20llu\n",
+				mib_names[rx].string,
+				counter[rx],
+				mib_names[tx].string,
+				counter[tx]);
+		else
+			printk(KERN_INFO "%s\t= %-20llu\t%s\t= %-20llu\n",
+				mib_names[rx].string,
+				counter[rx],
+				mib_names[tx].string,
+				counter[tx]);
+	}
+	for (n = 0, mib = first; n < cnt; n++, mib++) {
+		int j;
+		uint p = get_phy_port(sw, mib);
+		struct ksz_port_mib *port_mib = get_port_mib(sw, p);
+
+		for (j = 0; j < 2; j++) {
+			if (port_mib->rate[j].peak) {
+				u32 num;
+				u32 frac;
+
+				num = port_mib->rate[j].peak / 10;
+				frac = port_mib->rate[j].peak % 10;
+				if (buf)
+					len += sprintf(buf + len,
+						"%d:%d=%u.%u\n", mib, j,
+						num, frac);
+				else
+					printk(KERN_INFO
+						"%d:%d=%u.%u\n", mib, j,
+						num, frac);
+				port_mib->rate[j].peak = 0;
+			}
+		}
+	}
+	return len;
+}  /* display_sw_mib_counters */
+
+/* -------------------------------------------------------------------------- */
+
+static ssize_t display_sw_info(int cnt, char *buf, ssize_t len)
+{
+	len += sprintf(buf + len, "duplex:\t\t");
+	len += sprintf(buf + len, "0 for auto; ");
+	len += sprintf(buf + len,
+		"set to 1 for half-duplex; 2, full-duplex\n");
+	len += sprintf(buf + len, "speed:\t\t");
+	len += sprintf(buf + len,
+		"0 for auto; set to 10 or 100\n");
+	len += sprintf(buf + len, "force:\t\t");
+	len += sprintf(buf + len,
+		"set to 1 to force link to specific speed setting\n");
+	len += sprintf(buf + len, "flow_ctrl:\t");
+	len += sprintf(buf + len,
+		"set to 0 to disable flow control\n");
+	len += sprintf(buf + len, "mib:\t\t");
+	len += sprintf(buf + len,
+		"display the MIB table\n");
+	printk(KERN_INFO "%s", buf);
+	len = 0;
+
+	if (TOTAL_PORT_NUM != cnt)
+		return len;
+
+	len += sprintf(buf + len, "\ndynamic_table:\t");
+	len += sprintf(buf + len,
+		"display the switch's dynamic MAC table\n");
+	len += sprintf(buf + len, "static_table:\t");
+	len += sprintf(buf + len,
+		"display the switch's static MAC table\n");
+	len += sprintf(buf + len, "vlan_table:\t");
+	len += sprintf(buf + len,
+		"display the switch's VLAN table\n");
+
+	len += sprintf(buf + len, "\naging:\t\t");
+	len += sprintf(buf + len,
+		"disable/enable aging\n");
+	len += sprintf(buf + len, "fast_aging:\t");
+	len += sprintf(buf + len,
+		"disable/enable fast aging\n");
+	len += sprintf(buf + len, "link_aging:\t");
+	len += sprintf(buf + len,
+		"disable/enable link change auto aging\n");
+
+	len += sprintf(buf + len, "\nbcast_per:\t");
+	len += sprintf(buf + len,
+		"set broadcast storm percentage\n");
+	len += sprintf(buf + len, "mcast_storm:\t");
+	len += sprintf(buf + len,
+		"disable multicast storm protection\n");
+	len += sprintf(buf + len, "diffserv_map:\t");
+	len += sprintf(buf + len,
+		"set DiffServ value.  Use \"decimal=hexadecimal\" format\n");
+	len += sprintf(buf + len, "p_802_1p_map:\t");
+	len += sprintf(buf + len,
+		"set 802.1p value.  Use \"decimal=hexadecimal\" format\n");
+	printk(KERN_INFO "%s", buf);
+	len = 0;
+
+	len += sprintf(buf + len, "\nvlan:\t\t");
+	len += sprintf(buf + len,
+		"disable/enable 802.1Q VLAN\n");
+	len += sprintf(buf + len, "null_vid:\t");
+	len += sprintf(buf + len,
+		"set to 1 to replace null vid\n");
+	len += sprintf(buf + len, "macaddr:\t");
+	len += sprintf(buf + len,
+		"set switch MAC address\n");
+	len += sprintf(buf + len, "mirror_mode:\t");
+	len += sprintf(buf + len,
+		"set to 1 to use mirror rx AND tx mode\n");
+	len += sprintf(buf + len, "tail_tag:\t");
+	len += sprintf(buf + len,
+		"disable/enable tail tagging\n");
+
+	len += sprintf(buf + len, "\nigmp_snoop:\t");
+	len += sprintf(buf + len,
+		"disable/enable IGMP snooping\n");
+	len += sprintf(buf + len, "ipv6_mld_snoop:\t");
+	len += sprintf(buf + len,
+		"disable/enable IPv6 MLD snooping\n");
+	len += sprintf(buf + len, "ipv6_mld_option:");
+	len += sprintf(buf + len,
+		"disable/enable IPv6 MLD option snooping\n");
+
+	len += sprintf(buf + len, "\naggr_backoff:\t");
+	len += sprintf(buf + len,
+		"disable/enable aggressive backoff in half-duplex mode\n");
+	len += sprintf(buf + len, "no_exc_drop:\t");
+	len += sprintf(buf + len,
+		"disable/enable no excessive collision drop\n");
+	len += sprintf(buf + len, "buf_reserve:\t");
+	len += sprintf(buf + len,
+		"disable/enable buffer reserve\n");
+	printk(KERN_INFO "%s", buf);
+	len = 0;
+
+	len += sprintf(buf + len, "\nhuge_packet:\t");
+	len += sprintf(buf + len,
+		"disable/enable huge packet support\n");
+	len += sprintf(buf + len, "legal_packet:\t");
+	len += sprintf(buf + len,
+		"disable/enable legal packet\n");
+	len += sprintf(buf + len, "length_check:\t");
+	len += sprintf(buf + len,
+		"disable/enable legal packet length check\n");
+
+	len += sprintf(buf + len, "\nback_pressure:\t");
+	len += sprintf(buf + len,
+		"set back pressure mode\n");
+	len += sprintf(buf + len, "sw_flow_ctrl:\t");
+	len += sprintf(buf + len,
+		"disable/enable switch host port flow control\n");
+	len += sprintf(buf + len, "sw_half_duplex:\t");
+	len += sprintf(buf + len,
+		"disable/enable switch host port half-duplex mode\n");
+#ifdef SW_10_MBIT
+	len += sprintf(buf + len, "sw_10_mbit:\t");
+	len += sprintf(buf + len,
+		"disable/enable switch host port 10Mbit mode\n");
+#endif
+	len += sprintf(buf + len, "rx_flow_ctrl:\t");
+	len += sprintf(buf + len,
+		"disable/enable receive flow control\n");
+	len += sprintf(buf + len, "tx_flow_ctrl:\t");
+	len += sprintf(buf + len,
+		"disable/enable transmit flow control\n");
+	len += sprintf(buf + len, "fair_flow_ctrl:\t");
+	len += sprintf(buf + len,
+		"disable/enable fair flow control mode\n");
+	printk(KERN_INFO "%s", buf);
+	len = 0;
+
+	len += sprintf(buf + len, "vlan_bound:\t");
+	len += sprintf(buf + len,
+		"disable/enable unicast VLAN boundary\n");
+	len += sprintf(buf + len, "fw_unk_dest:\t");
+	len += sprintf(buf + len,
+		"disable/enable unknown destination address forwarding\n");
+
+	len += sprintf(buf + len, "\nins_tag_0_1:\t");
+	len += sprintf(buf + len,
+		"set to 1 to insert tag from port 1 to 2\n");
+	len += sprintf(buf + len, "ins_tag_0_2:\t");
+	len += sprintf(buf + len,
+		"set to 1 to insert tag from port 1 to 3\n");
+	len += sprintf(buf + len, "ins_tag_1_0:\t");
+	len += sprintf(buf + len,
+		"set to 1 to insert tag from port 2 to 1\n");
+	len += sprintf(buf + len, "ins_tag_1_2:\t");
+	len += sprintf(buf + len,
+		"set to 1 to insert tag from port 2 to 3\n");
+	len += sprintf(buf + len, "ins_tag_2_0:\t");
+	len += sprintf(buf + len,
+		"set to 1 to insert tag from port 3 to 1\n");
+	len += sprintf(buf + len, "ins_tag_2_1:\t");
+	len += sprintf(buf + len,
+		"set to 1 to insert tag from port 3 to 2\n");
+
+	len += sprintf(buf + len, "\npass_all:\t");
+	len += sprintf(buf + len,
+		"set to 1 to pass all frames for debugging\n");
+	len += sprintf(buf + len, "pass_pause:\t");
+	len += sprintf(buf + len,
+		"set to 1 to pass PAUSE frames for debugging\n");
+	printk(KERN_INFO "%s", buf);
+	len = 0;
+
+	len += sprintf(buf + len, "\nswitch port settings:\n");
+	len += sprintf(buf + len, "duplex:\t\t");
+	len += sprintf(buf + len,
+		"display the port's duplex setting\n");
+	len += sprintf(buf + len, "speed:\t\t");
+	len += sprintf(buf + len,
+		"display the port's link speed\n");
+	len += sprintf(buf + len, "linkmd:\t\t");
+	len += sprintf(buf + len,
+		"write to start LinkMD test.  read for result\n");
+	len += sprintf(buf + len, "mib:\t\t");
+	len += sprintf(buf + len,
+		"display the port's MIB table\n");
+	printk(KERN_INFO "%s", buf);
+	len = 0;
+
+	len += sprintf(buf + len, "vid:\t\t");
+	len += sprintf(buf + len,
+		"set default VID value\n");
+	len += sprintf(buf + len, "member:\t\t");
+	len += sprintf(buf + len,
+		"set VLAN membership\n");
+
+	len += sprintf(buf + len, "bcast_storm:\t");
+	len += sprintf(buf + len,
+		"disable/enable broadcast storm protection\n");
+	len += sprintf(buf + len, "diffserv:\t");
+	len += sprintf(buf + len,
+		"disable/enable DiffServ priority\n");
+	len += sprintf(buf + len, "p_802_1p:\t");
+	len += sprintf(buf + len,
+		"disable/enable 802.1p priority\n");
+
+	len += sprintf(buf + len, "port_based:\t");
+	len += sprintf(buf + len,
+		"disable/enable port-based priority\n");
+	printk(KERN_INFO "%s", buf);
+	len = 0;
+
+	len += sprintf(buf + len, "prio_queue:\t");
+	len += sprintf(buf + len,
+		"disable/enable priority queue\n");
+	len += sprintf(buf + len, "tx_p0_ctrl:\t");
+	len += sprintf(buf + len,
+		"set priority queue 0 control\n");
+	len += sprintf(buf + len, "tx_p1_ctrl:\t");
+	len += sprintf(buf + len,
+		"set priority queue 1 control\n");
+	len += sprintf(buf + len, "tx_p2_ctrl:\t");
+	len += sprintf(buf + len,
+		"set priority queue 2 control\n");
+	len += sprintf(buf + len, "tx_p3_ctrl:\t");
+	len += sprintf(buf + len,
+		"set priority queue 3 control\n");
+	len += sprintf(buf + len, "tx_p0_ratio:\t");
+	len += sprintf(buf + len,
+		"set priority queue 0 ratio\n");
+	len += sprintf(buf + len, "tx_p1_ratio:\t");
+	len += sprintf(buf + len,
+		"set priority queue 1 ratio\n");
+	len += sprintf(buf + len, "tx_p2_ratio:\t");
+	len += sprintf(buf + len,
+		"set priority queue 2 ratio\n");
+	len += sprintf(buf + len, "tx_p3_ratio:\t");
+	len += sprintf(buf + len,
+		"set priority queue 3 ratio\n");
+	len += sprintf(buf + len, "prio_rate:\t");
+	len += sprintf(buf + len,
+		"disable/enable priority queue rate limiting\n");
+	len += sprintf(buf + len, "rx_limit:\t");
+	len += sprintf(buf + len,
+		"set rx rate limiting mode\n");
+	len += sprintf(buf + len, "cnt_ifg:\t");
+	len += sprintf(buf + len,
+		"set to 1 to count IPG\n");
+	len += sprintf(buf + len, "cnt_pre:\t");
+	len += sprintf(buf + len,
+		"set to 1 to count preamble\n");
+	len += sprintf(buf + len, "rx_p0_rate:\t");
+	len += sprintf(buf + len,
+		"set rx priority queue 0 rate in 64Kbps unit\n");
+	len += sprintf(buf + len, "rx_p1_rate:\t");
+	len += sprintf(buf + len,
+		"set rx priority queue 1 rate in 64Kbps unit\n");
+	len += sprintf(buf + len, "rx_p2_rate:\t");
+	len += sprintf(buf + len,
+		"set rx priority queue 2 rate in 64Kbps unit\n");
+	len += sprintf(buf + len, "rx_p3_rate:\t");
+	len += sprintf(buf + len,
+		"set rx priority queue 3 rate in 64Kbps unit\n");
+	len += sprintf(buf + len, "tx_p0_rate:\t");
+	len += sprintf(buf + len,
+		"set tx priority queue 0 rate in 64Kbps unit\n");
+	len += sprintf(buf + len, "tx_p1_rate:\t");
+	len += sprintf(buf + len,
+		"set tx priority queue 1 rate in 64Kbps unit\n");
+	len += sprintf(buf + len, "tx_p2_rate:\t");
+	len += sprintf(buf + len,
+		"set tx priority queue 2 rate in 64Kbps unit\n");
+	len += sprintf(buf + len, "tx_p3_rate:\t");
+	len += sprintf(buf + len,
+		"set tx priority queue 3 rate in 64Kbps unit\n");
+	printk(KERN_INFO "%s", buf);
+	len = 0;
+
+	len += sprintf(buf + len, "rx:\t\t");
+	len += sprintf(buf + len,
+		"disable/enable rx\n");
+	len += sprintf(buf + len, "tx:\t\t");
+	len += sprintf(buf + len,
+		"disable/enable tx\n");
+	len += sprintf(buf + len, "learn:\t\t");
+	len += sprintf(buf + len,
+		"disable/enable learning\n");
+
+	len += sprintf(buf + len, "mirror_port:\t");
+	len += sprintf(buf + len,
+		"disable/enable mirror port\n");
+	len += sprintf(buf + len, "mirror_rx:\t");
+	len += sprintf(buf + len,
+		"disable/enable mirror receive\n");
+	len += sprintf(buf + len, "mirror_tx:\t");
+	len += sprintf(buf + len,
+		"disable/enable mirror transmit\n");
+	printk(KERN_INFO "%s", buf);
+	len = 0;
+
+	len += sprintf(buf + len, "\nnon_vid:\t");
+	len += sprintf(buf + len,
+		"set to 1 to discard non-VID packets\n");
+	len += sprintf(buf + len, "ingress:\t");
+	len += sprintf(buf + len,
+		"disable/enable ingress VLAN filtering\n");
+	len += sprintf(buf + len, "ins_tag:\t");
+	len += sprintf(buf + len,
+		"disable/enable insert VLAN tag feature\n");
+	len += sprintf(buf + len, "rmv_tag:\t");
+	len += sprintf(buf + len,
+		"disable/enable remove VLAN tag feature\n");
+	len += sprintf(buf + len, "drop_tagged:\t");
+	len += sprintf(buf + len,
+		"disable/enable drop tagged packet feature\n");
+	len += sprintf(buf + len, "replace_prio:\t");
+	len += sprintf(buf + len,
+		"disable/enable replace 802.1p priority feature\n");
+	len += sprintf(buf + len, "back_pressure:\t");
+	len += sprintf(buf + len,
+		"disable/enable back pressure in half-duplex mode\n");
+	len += sprintf(buf + len, "force_flow_ctrl:");
+	len += sprintf(buf + len,
+		"set to 1 to force flow control\n");
+	len += sprintf(buf + len, "fw_unk_dest:\t");
+	len += sprintf(buf + len,
+		"set to 1 to forward unknown destination address packets\n");
+	len += sprintf(buf + len, "fw_inv_vid:\t");
+	len += sprintf(buf + len,
+		"set to 1 to forward invalid VID packets\n");
+	printk(KERN_INFO "%s", buf);
+	len = 0;
+
+	len += sprintf(buf + len, "\nstatic MAC table:\n");
+	len += sprintf(buf + len, "addr:\t\t");
+	len += sprintf(buf + len,
+		"set MAC address\n");
+	len += sprintf(buf + len, "ports:\t\t");
+	len += sprintf(buf + len,
+		"set destination ports\n");
+	len += sprintf(buf + len, "override:\t");
+	len += sprintf(buf + len,
+		"set override bit\n");
+	len += sprintf(buf + len, "use_fid:\t");
+	len += sprintf(buf + len,
+		"set use FID bit\n");
+	len += sprintf(buf + len, "fid:\t\t");
+	len += sprintf(buf + len,
+		"set FID\n");
+	len += sprintf(buf + len, "valid:\t\t");
+	len += sprintf(buf + len,
+		"set valid bit and write to table\n");
+	len += sprintf(buf + len, "\nVLAN table:\n");
+	len += sprintf(buf + len, "vid:\t\t");
+	len += sprintf(buf + len,
+		"set VID\n");
+	len += sprintf(buf + len, "fid:\t\t");
+	len += sprintf(buf + len,
+		"set FID\n");
+	len += sprintf(buf + len, "member:\t\t");
+	len += sprintf(buf + len,
+		"set membership\n");
+	len += sprintf(buf + len, "valid:\t\t");
+	len += sprintf(buf + len,
+		"set valid bit and write to table\n");
+	printk(KERN_INFO "%s", buf);
+	len = 0;
+
+	return len;
+}  /* display_sw_info */
+
+static ssize_t sysfs_sw_read(struct ksz_sw *sw, int proc_num,
+	struct ksz_port *port, ssize_t len, char *buf)
+{
+	int i;
+	int j;
+	u16 map;
+	struct ksz_sw_info *info = sw->info;
+
+	switch (proc_num) {
+	case PROC_SW_INFO:
+		len = display_sw_info(TOTAL_PORT_NUM, buf, len);
+		break;
+	case PROC_SW_VERSION:
+		len += sprintf(buf + len, "%s  %s\n",
+			SW_DRV_VERSION, SW_DRV_RELDATE);
+		break;
+	case PROC_SET_SW_DUPLEX:
+		if (!port)
+			break;
+		len += sprintf(buf + len, "%u; ", port->duplex);
+		if (media_connected == port->linked->state) {
+			if (1 == port->linked->duplex)
+				len += sprintf(buf + len, "half-duplex\n");
+			else if (2 == port->linked->duplex)
+				len += sprintf(buf + len, "full-duplex\n");
+		} else
+			len += sprintf(buf + len, "unlinked\n");
+		break;
+	case PROC_SET_SW_SPEED:
+		if (!port)
+			break;
+		len += sprintf(buf + len, "%u; ", port->speed);
+		if (media_connected == port->linked->state)
+			len += sprintf(buf + len, "%u\n",
+				port->linked->tx_rate / TX_RATE_UNIT);
+		else
+			len += sprintf(buf + len, "unlinked\n");
+		break;
+	case PROC_SET_SW_FORCE:
+		if (!port)
+			break;
+		len += sprintf(buf + len, "%u\n", port->force_link);
+		break;
+	case PROC_SET_SW_FLOW_CTRL:
+		if (!port)
+			break;
+		len += sprintf(buf + len, "%u; ", port->flow_ctrl);
+		switch (port->flow_ctrl) {
+		case PHY_FLOW_CTRL:
+			len += sprintf(buf + len, "flow control\n");
+			break;
+		case PHY_TX_ONLY:
+			len += sprintf(buf + len, "tx only\n");
+			break;
+		case PHY_RX_ONLY:
+			len += sprintf(buf + len, "rx only\n");
+			break;
+		default:
+			len += sprintf(buf + len, "no flow control\n");
+			break;
+		}
+		break;
+	case PROC_SET_SW_MIB:
+		if (!port)
+			break;
+		len += display_sw_mib_counters(sw, port->first_port,
+			port->mib_port_cnt, buf + len);
+		break;
+	case PROC_SET_BROADCAST_STORM:
+		len += sprintf(buf + len, "%u%%\n", info->broad_per);
+		break;
+	case PROC_SET_DIFFSERV:
+		for (i = 0; i < DIFFSERV_ENTRIES / KS_PRIO_IN_REG; i++) {
+			len += sprintf(buf + len, "%2u=",
+				i * KS_PRIO_IN_REG);
+			map = info->diffserv[i];
+			for (j = 0; j < KS_PRIO_IN_REG; j++) {
+				len += sprintf(buf + len, "%u ",
+					map & KS_PRIO_M);
+				map >>= KS_PRIO_S;
+			}
+			len += sprintf(buf + len, "\t"SW_SIZE_STR"\n",
+				info->diffserv[i]);
+		}
+		break;
+	case PROC_SET_802_1P:
+		for (i = 0; i < PRIO_802_1P_ENTRIES / KS_PRIO_IN_REG; i++) {
+			len += sprintf(buf + len, "%u=",
+				i * KS_PRIO_IN_REG);
+			map = info->p_802_1p[i];
+			for (j = 0; j < KS_PRIO_IN_REG; j++) {
+				len += sprintf(buf + len, "%u ",
+					map & KS_PRIO_M);
+				map >>= KS_PRIO_S;
+			}
+			len += sprintf(buf + len, "\t"SW_SIZE_STR"\n",
+				info->p_802_1p[i]);
+		}
+		break;
+	case PROC_SET_SW_VID:
+		len += sprintf(buf + len, "0x%04x\n", sw->vid);
+		break;
+	case PROC_GET_PORTS:
+	{
+		uint ports = sw->mib_port_cnt;
+
+		if (sw->eth_cnt)
+			ports = sw->eth_maps[0].cnt;
+		len += sprintf(buf + len, "%u\n", ports);
+		break;
+	}
+	case PROC_GET_DEV_START:
+	{
+		int start = 0;
+
+		if (sw->dev_offset)
+			start = 100;
+		len += sprintf(buf + len, "%u\n", start);
+		break;
+	}
+	case PROC_GET_VLAN_START:
+	{
+		int start = 0;
+
+		if (sw->features & VLAN_PORT)
+			start = VLAN_PORT_START;
+		len += sprintf(buf + len, "%u\n", start);
+		break;
+	}
+	case PROC_GET_STP:
+		len += sprintf(buf + len, "0\n");
+		break;
+	case PROC_SET_SW_FEATURES:
+		len += sprintf(buf + len, "%08x:\n", sw->features);
+		len += sprintf(buf + len, "\t%08x = STP support\n",
+			STP_SUPPORT);
+		len += sprintf(buf + len, "\t%08x = VLAN port forwarding\n",
+			VLAN_PORT);
+		len += sprintf(buf + len, "\t%08x = VLAN port remove tag\n",
+			VLAN_PORT_REMOVE_TAG);
+		len += sprintf(buf + len, "\t%08x = VLAN port tag tailing\n",
+			VLAN_PORT_TAGGING);
+		len += sprintf(buf + len, "\t%08x = different MAC addresses\n",
+			DIFF_MAC_ADDR);
+		break;
+	case PROC_SET_SW_OVERRIDES:
+		len += sprintf(buf + len, "%08x:\n", sw->overrides);
+		len += sprintf(buf + len, "\t%08x = flow control\n",
+			PAUSE_FLOW_CTRL);
+		len += sprintf(buf + len, "\t%08x = fast aging\n",
+			FAST_AGING);
+		len += sprintf(buf + len, "\t%08x = tag is removed\n",
+			TAG_REMOVE);
+		len += sprintf(buf + len, "\t%08x = tail tagging\n",
+			TAIL_TAGGING);
+		break;
+	case PROC_DYNAMIC:
+		len = sw_d_dyn_mac_table(sw, buf, len);
+		break;
+	case PROC_STATIC:
+		len = sw_d_sta_mac_table(sw, buf, len);
+#ifdef DEBUG
+		len = sw_d_mac_table(sw, buf, len);
+#endif
+		break;
+	case PROC_VLAN:
+		len = sw_d_vlan_table(sw, buf, len);
+		break;
+	}
+	return len;
+}  /* sysfs_sw_read */
+
+static ssize_t sysfs_sw_read_hw(struct ksz_sw *sw, int proc_num, ssize_t len,
+	char *buf)
+{
+	u8 data[8];
+	int chk = 0;
+	int type = SHOW_HELP_ON_OFF;
+	char note[40];
+
+	note[0] = '\0';
+	switch (proc_num) {
+	case PROC_SET_AGING:
+		chk = sw_chk(sw, REG_SW_CTRL_1, SW_AGING_ENABLE);
+		break;
+	case PROC_SET_FAST_AGING:
+		chk = sw_chk(sw, REG_SW_CTRL_1, SW_FAST_AGING);
+		break;
+	case PROC_SET_LINK_AGING:
+		chk = sw_chk(sw, S_LINK_AGING_CTRL, SW_LINK_AUTO_AGING);
+		break;
+	case PROC_SET_MULTICAST_STORM:
+		chk = !sw_chk(sw, REG_SW_CTRL_2, MULTICAST_STORM_DISABLE);
+		break;
+	case PROC_SET_TX_RATE_QUEUE_BASED:
+		chk = sw_chk(sw, REG_SW_CTRL_19,
+			SW_OUT_RATE_LIMIT_QUEUE_BASED);
+		break;
+	case PROC_ENABLE_VLAN:
+		chk = sw_chk(sw, S_MIRROR_CTRL, SW_VLAN_ENABLE);
+		break;
+	case PROC_SET_REPLACE_NULL_VID:
+		chk = sw_chk(sw, S_REPLACE_VID_CTRL, SW_REPLACE_VID);
+		break;
+	case PROC_SET_MAC_ADDR:
+		sw_get_addr(sw, data);
+		len += sprintf(buf + len, "%02X:%02X:%02X:%02X:%02X:%02X\n",
+			data[0], data[1], data[2], data[3], data[4], data[5]);
+		type = SHOW_HELP_NONE;
+		break;
+	case PROC_SET_MIRROR_MODE:
+		chk = sw_chk_mirror_rx_tx(sw);
+		if (sw->verbose) {
+			if (chk)
+				strcpy(note, " (rx and tx)");
+			else
+				strcpy(note, " (rx or tx)");
+		}
+		type = SHOW_HELP_SPECIAL;
+		break;
+	case PROC_SET_IGMP_SNOOP:
+		chk = sw_chk(sw, S_MIRROR_CTRL, SW_IGMP_SNOOP);
+		break;
+	case PROC_SET_TAIL_TAG:
+		chk = sw_chk(sw, S_TAIL_TAG_CTRL, SW_TAIL_TAG_ENABLE);
+		break;
+	case PROC_SET_AGGR_BACKOFF:
+		chk = sw_chk(sw, REG_SW_CTRL_1, SW_AGGR_BACKOFF);
+		break;
+	case PROC_SET_NO_EXC_DROP:
+		chk = sw_chk(sw, REG_SW_CTRL_2, NO_EXC_COLLISION_DROP);
+		break;
+	case PROC_SET_VLAN_BOUNDARY:
+		chk = sw_chk(sw, REG_SW_CTRL_2, UNICAST_VLAN_BOUNDARY);
+		break;
+	case PROC_SET_HUGE_PACKET:
+		chk = sw_chk(sw, S_HUGE_PACKET_CTRL, SW_HUGE_PACKET);
+		break;
+	case PROC_SET_LEGAL_PACKET:
+		chk = sw_chk(sw, REG_SW_CTRL_2, SW_LEGAL_PACKET);
+		break;
+	case PROC_SET_LENGTH_CHECK:
+		chk = sw_chk(sw, REG_SW_CTRL_1, SW_CHECK_LENGTH);
+		break;
+	case PROC_SET_BACK_PRESSURE_MODE:
+		chk = sw_chk(sw, REG_SW_CTRL_2, SW_BACK_PRESSURE);
+		break;
+	case PROC_SET_SWITCH_FLOW_CTRL:
+		chk = sw_chk(sw, S_REPLACE_VID_CTRL, SW_FLOW_CTRL);
+		break;
+	case PROC_SET_SWITCH_HALF_DUPLEX:
+		chk = sw_chk(sw, S_REPLACE_VID_CTRL, SW_HALF_DUPLEX);
+		break;
+#ifdef SW_10_MBIT
+	case PROC_SET_SWITCH_10_MBIT:
+		chk = sw_chk(sw, S_REPLACE_VID_CTRL, SW_10_MBIT);
+		break;
+#endif
+	case PROC_SET_RX_FLOW_CTRL:
+		chk = !sw_chk(sw, REG_SW_CTRL_1, SW_RX_FLOW_CTRL_DISABLE);
+		break;
+	case PROC_SET_TX_FLOW_CTRL:
+		chk = !sw_chk(sw, REG_SW_CTRL_1, SW_TX_FLOW_CTRL_DISABLE);
+		break;
+	case PROC_SET_FAIR_FLOW_CTRL:
+		chk = sw_chk(sw, REG_SW_CTRL_2, FAIR_FLOW_CTRL);
+		break;
+	case PROC_SET_FORWARD_UNKNOWN_UNICAST:
+		chk = sw_chk_unk_dest(sw, REG_SW_UNK_UCAST_CTRL);
+		break;
+	case PROC_SET_FORWARD_UNKNOWN_UNICAST_PORTS:
+		chk = sw_chk_unk_def_port(sw, REG_SW_UNK_UCAST_CTRL,
+			sw->port_cnt);
+		chk = get_log_mask_from_phy(sw, chk);
+		type = SHOW_HELP_HEX;
+		break;
+	case PROC_SET_FORWARD_UNKNOWN_MULTICAST:
+		chk = sw_chk_unk_dest(sw, REG_SW_UNK_MCAST_CTRL);
+		break;
+	case PROC_SET_FORWARD_UNKNOWN_MULTICAST_PORTS:
+		chk = sw_chk_unk_def_port(sw, REG_SW_UNK_MCAST_CTRL,
+			sw->port_cnt);
+		chk = get_log_mask_from_phy(sw, chk);
+		type = SHOW_HELP_HEX;
+		break;
+	case PROC_SET_FORWARD_UNKNOWN_VID:
+		chk = sw_chk_unk_dest(sw, REG_SW_UNK_VID_CTRL);
+		break;
+	case PROC_SET_FORWARD_UNKNOWN_VID_PORTS:
+		chk = sw_chk_unk_def_port(sw, REG_SW_UNK_VID_CTRL,
+			sw->port_cnt);
+		chk = get_log_mask_from_phy(sw, chk);
+		type = SHOW_HELP_HEX;
+		break;
+	case PROC_SET_FORWARD_UNKNOWN_IP_MULTICAST:
+		chk = sw_chk_unk_dest(sw, REG_SW_UNK_IP_MCAST_CTRL);
+		break;
+	case PROC_SET_FORWARD_UNKNOWN_IP_MULTICAST_PORTS:
+		chk = sw_chk_unk_def_port(sw, REG_SW_UNK_IP_MCAST_CTRL,
+			sw->port_cnt);
+		chk = get_log_mask_from_phy(sw, chk);
+		type = SHOW_HELP_HEX;
+		break;
+	case PROC_SET_SELF_ADDR_FILTER:
+		chk = sw_chk_self_filter(sw);
+		break;
+	case PROC_SET_INS_TAG:
+		chk = sw_chk_ins_tag(sw);
+		break;
+	case PROC_SET_PASS_ALL:
+		chk = sw_chk(sw, REG_SW_CTRL_1, SW_PASS_ALL);
+		break;
+	case PROC_SET_PASS_PAUSE:
+		chk = sw_chk(sw, S_PASS_PAUSE_CTRL, SW_PASS_PAUSE);
+		break;
+	case PROC_SET_HI_PRIO_QUEUES:
+		chk = sw_get_hi_prio_queues(sw);
+		if (sw->verbose) {
+			switch (chk) {
+			case 3:
+				strcpy(note, " (3 = H; 2, 1, 0 = L)");
+				break;
+			case 1:
+				strcpy(note, " (3, 2, 1 = H; 0 = L)");
+				break;
+			default:
+				strcpy(note, " (3, 2 = H; 1, 0 = L)");
+			}
+		}
+		type = SHOW_HELP_SPECIAL;
+		break;
+	default:
+		type = SHOW_HELP_NONE;
+		break;
+	}
+	return sysfs_show(len, buf, type, chk, note, sw->verbose);
+}  /* sysfs_sw_read_hw */
+
+static int sysfs_sw_write(struct ksz_sw *sw, int proc_num,
+	struct ksz_port *port, int num, const char *buf)
+{
+	int changes;
+	int count;
+	unsigned int val;
+	u8 data[8];
+	int processed = true;
+
+	switch (proc_num) {
+	case PROC_SW_INFO:
+		sw_init(sw);
+		sw->verbose = !!num;
+		break;
+	case PROC_SET_SW_DUPLEX:
+		if (!port)
+			break;
+		if (num <= 2)
+			port->duplex = (u8) num;
+		break;
+	case PROC_SET_SW_SPEED:
+		if (!port)
+			break;
+		if (0 == num || 10 == num || 100 == num)
+			port->speed = (u8) num;
+		break;
+	case PROC_SET_SW_FORCE:
+		if (!port)
+			break;
+		port->force_link = (u8) num;
+		if (port->force_link) {
+			port_force_link_speed(port);
+			sw->phy_intr = sw->PORT_MASK;
+			port_get_link_speed(port);
+			if (port->link_ports)
+				schedule_delayed_work(&port->link_update, 0);
+			sw->phy_intr = 0;
+		} else
+			port_set_link_speed(port);
+		break;
+	case PROC_SET_SW_FLOW_CTRL:
+		if (!port)
+			break;
+		if (num <= PHY_FLOW_CTRL)
+			port->flow_ctrl = (u8) num;
+		break;
+	case PROC_SET_SW_MIB:
+		sw_cfg_mib_counter_ctrl(sw, num, sw->port_cnt);
+		break;
+	case PROC_SET_SW_REG:
+		count = sscanf(buf, "%x=%x", (unsigned int *) &num, &val);
+		if (1 == count)
+			printk(KERN_INFO SW_SIZE_STR"\n",
+				SW_R(sw, num));
+		else if (2 == count)
+			SW_W(sw, num, val);
+		break;
+	case PROC_SET_SW_VID:
+		sw->vid = num;
+		break;
+	case PROC_SET_SW_FEATURES:
+		if ('0' != buf[0] || 'x' != buf[1])
+			sscanf(buf, "%x", &num);
+		changes = sw->features ^ num;
+		sw->features = num;
+		if (changes & VLAN_PORT_REMOVE_TAG) {
+			uint n;
+			uint p;
+			bool enabled;
+
+			if (num & VLAN_PORT_REMOVE_TAG) {
+				enabled = true;
+				sw->overrides |= TAG_REMOVE;
+			} else {
+				enabled = false;
+				sw->overrides &= ~TAG_REMOVE;
+			}
+			for (n = 1; n <= sw->mib_port_cnt; n++) {
+				p = get_phy_port(sw, n);
+				port_cfg_rmv_tag(sw, p, enabled);
+			}
+		}
+		break;
+	case PROC_SET_SW_OVERRIDES:
+		if ('0' != buf[0] || 'x' != buf[1])
+			sscanf(buf, "%x", &num);
+		sw->overrides = num;
+		break;
+	case PROC_DYNAMIC:
+		sw_flush_dyn_mac_table(sw, TOTAL_PORT_NUM);
+		break;
+	case PROC_STATIC:
+		sw->ops->release(sw);
+		sw_clr_sta_mac_table(sw);
+		sw->ops->acquire(sw);
+		break;
+	case PROC_SET_AGING:
+		sw_cfg(sw, REG_SW_CTRL_1, SW_AGING_ENABLE, num);
+		break;
+	case PROC_SET_FAST_AGING:
+		sw_cfg(sw, REG_SW_CTRL_1, SW_FAST_AGING, num);
+		break;
+	case PROC_SET_LINK_AGING:
+		sw_cfg(sw, S_LINK_AGING_CTRL, SW_LINK_AUTO_AGING, num);
+		break;
+	case PROC_SET_BROADCAST_STORM:
+		hw_cfg_broad_storm(sw, num);
+		break;
+	case PROC_SET_MULTICAST_STORM:
+		sw_cfg(sw, REG_SW_CTRL_2, MULTICAST_STORM_DISABLE, !num);
+		break;
+	case PROC_SET_TX_RATE_QUEUE_BASED:
+		sw_cfg(sw, REG_SW_CTRL_19, SW_OUT_RATE_LIMIT_QUEUE_BASED,
+			num);
+		break;
+	case PROC_SET_DIFFSERV:
+		count = sscanf(buf, "%d=%x", (unsigned int *) &num, &val);
+		if (2 == count)
+			hw_cfg_tos_prio(sw, (u8) num, (u16) val);
+		break;
+	case PROC_SET_802_1P:
+		count = sscanf(buf, "%d=%x", (unsigned int *) &num, &val);
+		if (2 == count)
+			hw_cfg_802_1p_prio(sw, (u8) num, (u16) val);
+		break;
+	case PROC_ENABLE_VLAN:
+		if (!num)
+			sw_dis_vlan(sw);
+		else
+			sw_ena_vlan(sw);
+		break;
+	case PROC_SET_REPLACE_NULL_VID:
+		sw_cfg_replace_null_vid(sw, num);
+		break;
+	case PROC_SET_MAC_ADDR:
+	{
+		int i;
+		int n[6];
+
+		i = sscanf(buf, "%x:%x:%x:%x:%x:%x",
+			&n[0], &n[1], &n[2], &n[3], &n[4], &n[5]);
+		if (6 == i) {
+			for (i = 0; i < 6; i++)
+				data[i] = (u8) n[i];
+			sw_set_addr(sw, data);
+		}
+		break;
+	}
+	case PROC_SET_MIRROR_MODE:
+		sw_cfg_mirror_rx_tx(sw, num);
+		break;
+	case PROC_SET_IGMP_SNOOP:
+		sw_cfg(sw, S_MIRROR_CTRL, SW_IGMP_SNOOP, num);
+		break;
+	case PROC_SET_TAIL_TAG:
+		sw_cfg(sw, S_TAIL_TAG_CTRL, SW_TAIL_TAG_ENABLE, num);
+		if (num)
+			sw->overrides |= TAIL_TAGGING;
+		else
+			sw->overrides &= ~TAIL_TAGGING;
+		break;
+	case PROC_SET_AGGR_BACKOFF:
+		sw_cfg(sw, REG_SW_CTRL_1, SW_AGGR_BACKOFF, num);
+		break;
+	case PROC_SET_NO_EXC_DROP:
+		sw_cfg(sw, REG_SW_CTRL_2, NO_EXC_COLLISION_DROP, num);
+		break;
+	case PROC_SET_VLAN_BOUNDARY:
+		sw_cfg(sw, REG_SW_CTRL_2, UNICAST_VLAN_BOUNDARY, num);
+		break;
+	case PROC_SET_HUGE_PACKET:
+		sw_cfg(sw, S_HUGE_PACKET_CTRL, SW_HUGE_PACKET, num);
+		break;
+	case PROC_SET_LEGAL_PACKET:
+		sw_cfg(sw, REG_SW_CTRL_2, SW_LEGAL_PACKET, num);
+		break;
+	case PROC_SET_LENGTH_CHECK:
+		sw_cfg(sw, REG_SW_CTRL_1, SW_CHECK_LENGTH, num);
+		break;
+	case PROC_SET_BACK_PRESSURE_MODE:
+		sw_cfg(sw, REG_SW_CTRL_2, SW_BACK_PRESSURE, num);
+		break;
+	case PROC_SET_SWITCH_FLOW_CTRL:
+		sw_cfg(sw, S_REPLACE_VID_CTRL, SW_FLOW_CTRL, num);
+		break;
+	case PROC_SET_SWITCH_HALF_DUPLEX:
+		sw_cfg(sw, S_REPLACE_VID_CTRL, SW_HALF_DUPLEX, num);
+		break;
+#ifdef SW_10_MBIT
+	case PROC_SET_SWITCH_10_MBIT:
+		sw_cfg(sw, S_REPLACE_VID_CTRL, SW_10_MBIT, num);
+		break;
+#endif
+	case PROC_SET_RX_FLOW_CTRL:
+		sw_cfg(sw, REG_SW_CTRL_1, SW_RX_FLOW_CTRL_DISABLE, !num);
+		break;
+	case PROC_SET_TX_FLOW_CTRL:
+		sw_cfg(sw, REG_SW_CTRL_1, SW_TX_FLOW_CTRL_DISABLE, !num);
+		break;
+	case PROC_SET_FAIR_FLOW_CTRL:
+		sw_cfg(sw, REG_SW_CTRL_2, FAIR_FLOW_CTRL, num);
+		break;
+	case PROC_SET_FORWARD_UNKNOWN_UNICAST:
+		sw_cfg_unk_dest(sw, REG_SW_UNK_UCAST_CTRL, num);
+		break;
+	case PROC_SET_FORWARD_UNKNOWN_UNICAST_PORTS:
+		num = get_phy_mask_from_log(sw, num);
+		sw_cfg_unk_def_port(sw, REG_SW_UNK_UCAST_CTRL, num, 2);
+		break;
+	case PROC_SET_FORWARD_UNKNOWN_MULTICAST:
+		sw_cfg_unk_dest(sw, REG_SW_UNK_MCAST_CTRL, num);
+		break;
+	case PROC_SET_FORWARD_UNKNOWN_MULTICAST_PORTS:
+		num = get_phy_mask_from_log(sw, num);
+		sw_cfg_unk_def_port(sw, REG_SW_UNK_MCAST_CTRL, num, 2);
+		break;
+	case PROC_SET_FORWARD_UNKNOWN_VID:
+		sw_cfg_unk_dest(sw, REG_SW_UNK_VID_CTRL, num);
+		break;
+	case PROC_SET_FORWARD_UNKNOWN_VID_PORTS:
+		num = get_phy_mask_from_log(sw, num);
+		sw_cfg_unk_def_port(sw, REG_SW_UNK_VID_CTRL, num, 2);
+		break;
+	case PROC_SET_FORWARD_UNKNOWN_IP_MULTICAST:
+		sw_cfg_unk_dest(sw, REG_SW_UNK_IP_MCAST_CTRL, num);
+		break;
+	case PROC_SET_FORWARD_UNKNOWN_IP_MULTICAST_PORTS:
+		num = get_phy_mask_from_log(sw, num);
+		sw_cfg_unk_def_port(sw, REG_SW_UNK_IP_MCAST_CTRL, num, 2);
+		break;
+	case PROC_SET_SELF_ADDR_FILTER:
+		sw_cfg_self_filter(sw, num);
+		break;
+	case PROC_SET_INS_TAG:
+		sw_cfg_ins_tag(sw, num);
+		break;
+	case PROC_SET_PASS_ALL:
+		sw_cfg(sw, REG_SW_CTRL_1, SW_PASS_ALL, num);
+		break;
+	case PROC_SET_PASS_PAUSE:
+		sw_cfg(sw, S_PASS_PAUSE_CTRL, SW_PASS_PAUSE, num);
+		break;
+	case PROC_SET_HI_PRIO_QUEUES:
+		sw_set_hi_prio_queues(sw, num);
+		break;
+	default:
+		processed = false;
+		break;
+	}
+	return processed;
+}  /* sysfs_sw_write */
+
+static ssize_t sysfs_port_read(struct ksz_sw *sw, int proc_num, uint n,
+	ssize_t len, char *buf)
+{
+	struct ksz_port_cfg *port_cfg;
+	struct ksz_port_info *port_info;
+	uint port;
+	int chk = 0;
+	int type = SHOW_HELP_NONE;
+	char note[40];
+
+	note[0] = '\0';
+	port = get_sysfs_port(sw, n);
+	port_cfg = get_port_cfg(sw, port);
+	port_info = get_port_info(sw, port);
+	switch (proc_num) {
+	case PROC_SET_PORT_DUPLEX:
+		if (media_connected == port_info->state) {
+			if (1 == port_info->duplex)
+				len += sprintf(buf + len, "half-duplex\n");
+			else if (2 == port_info->duplex)
+				len += sprintf(buf + len, "full-duplex\n");
+		} else
+			len += sprintf(buf + len, "unlinked\n");
+		break;
+	case PROC_SET_PORT_SPEED:
+		if (media_connected == port_info->state)
+			len += sprintf(buf + len, "%u\n",
+				port_info->tx_rate / TX_RATE_UNIT);
+		else
+			len += sprintf(buf + len, "unlinked\n");
+		break;
+	case PROC_SET_PORT_MIB:
+		port = get_log_port(sw, port);
+		len += display_sw_mib_counters(sw, port, 1, buf + len);
+		break;
+	case PROC_SET_LINK_MD:
+		len += sprintf(buf + len, "%u:%u %u:%u %u:%u\n",
+			port_info->length[0], port_info->status[0],
+			port_info->length[1], port_info->status[1],
+			port_info->length[2], port_info->status[2]);
+		if (sw->verbose)
+			len += sprintf(buf + len,
+				"(%d=unknown; %d=normal; %d=open; %d=short)\n",
+				CABLE_UNKNOWN, CABLE_GOOD, CABLE_OPEN,
+				CABLE_SHORT);
+		break;
+	case PROC_SET_PORT_BASED:
+		chk = port_cfg->port_prio;
+		type = SHOW_HELP_NUM;
+		break;
+	case PROC_SET_DEF_VID:
+		chk = port_cfg->vid;
+		type = SHOW_HELP_HEX_4;
+		break;
+	case PROC_SET_MEMBER:
+		chk = port_cfg->member;
+		chk = get_log_mask_from_phy(sw, chk);
+		type = SHOW_HELP_HEX_2;
+		break;
+	case PROC_SET_TX_Q0_CTRL:
+		chk = (port_cfg->rate_ctrl[0] & RATE_CTRL_ENABLE) != 0;
+		type = SHOW_HELP_ON_OFF;
+		break;
+	case PROC_SET_TX_Q1_CTRL:
+		chk = (port_cfg->rate_ctrl[1] & RATE_CTRL_ENABLE) != 0;
+		type = SHOW_HELP_ON_OFF;
+		break;
+	case PROC_SET_TX_Q2_CTRL:
+		chk = (port_cfg->rate_ctrl[2] & RATE_CTRL_ENABLE) != 0;
+		type = SHOW_HELP_ON_OFF;
+		break;
+	case PROC_SET_TX_Q3_CTRL:
+		chk = (port_cfg->rate_ctrl[3] & RATE_CTRL_ENABLE) != 0;
+		type = SHOW_HELP_ON_OFF;
+		break;
+	case PROC_SET_TX_Q0_RATIO:
+		chk = port_cfg->rate_ctrl[0] & RATE_RATIO_M;
+		type = SHOW_HELP_NUM;
+		break;
+	case PROC_SET_TX_Q1_RATIO:
+		chk = port_cfg->rate_ctrl[1] & RATE_RATIO_M;
+		type = SHOW_HELP_NUM;
+		break;
+	case PROC_SET_TX_Q2_RATIO:
+		chk = port_cfg->rate_ctrl[2] & RATE_RATIO_M;
+		type = SHOW_HELP_NUM;
+		break;
+	case PROC_SET_TX_Q3_RATIO:
+		chk = port_cfg->rate_ctrl[3] & RATE_RATIO_M;
+		type = SHOW_HELP_NUM;
+		break;
+	case PROC_SET_RX_LIMIT:
+		chk = ((port_cfg->rate_limit >> PORT_IN_LIMIT_MODE_S) &
+			PORT_IN_LIMIT_MODE_M);
+		if (sw->verbose) {
+			switch (chk) {
+			case 1:
+				strcpy(note, " (flooded unicast)");
+				break;
+			case 2:
+				strcpy(note, " (multicast)");
+				break;
+			case 3:
+				strcpy(note, " (broadcast)");
+				break;
+			default:
+				strcpy(note, " (all)");
+			}
+		}
+		type = SHOW_HELP_SPECIAL;
+		break;
+	case PROC_SET_RX_LIMIT_FLOW_CTRL:
+		chk = ((port_cfg->rate_limit >> PORT_IN_FLOW_CTRL_S) & 1);
+		type = SHOW_HELP_ON_OFF;
+		break;
+	case PROC_SET_CNT_IFG:
+		chk = ((port_cfg->rate_limit >> PORT_COUNT_IFG_S) & 1);
+		type = SHOW_HELP_ON_OFF;
+		break;
+	case PROC_SET_CNT_PRE:
+		chk = ((port_cfg->rate_limit >> PORT_COUNT_PREAMBLE_S) & 1);
+		type = SHOW_HELP_ON_OFF;
+		break;
+	case PROC_SET_RX_P0_RATE:
+		chk = port_cfg->rx_rate[0];
+		if (sw->verbose)
+			strcpy(note, " (bps)");
+		type = SHOW_HELP_SPECIAL;
+		break;
+	case PROC_SET_RX_P1_RATE:
+		chk = port_cfg->rx_rate[1];
+		if (sw->verbose)
+			strcpy(note, " (bps)");
+		type = SHOW_HELP_SPECIAL;
+		break;
+	case PROC_SET_RX_P2_RATE:
+		chk = port_cfg->rx_rate[2];
+		if (sw->verbose)
+			strcpy(note, " (bps)");
+		type = SHOW_HELP_SPECIAL;
+		break;
+	case PROC_SET_RX_P3_RATE:
+		chk = port_cfg->rx_rate[3];
+		if (sw->verbose)
+			strcpy(note, " (bps)");
+		type = SHOW_HELP_SPECIAL;
+		break;
+	case PROC_SET_TX_Q0_RATE:
+		chk = port_cfg->tx_rate[0];
+		if (sw->verbose)
+			strcpy(note, " (bps)");
+		type = SHOW_HELP_SPECIAL;
+		break;
+	case PROC_SET_TX_Q1_RATE:
+		chk = port_cfg->tx_rate[1];
+		if (sw->verbose)
+			strcpy(note, " (bps)");
+		type = SHOW_HELP_SPECIAL;
+		break;
+	case PROC_SET_TX_Q2_RATE:
+		chk = port_cfg->tx_rate[2];
+		if (sw->verbose)
+			strcpy(note, " (bps)");
+		type = SHOW_HELP_SPECIAL;
+		break;
+	case PROC_SET_TX_Q3_RATE:
+		chk = port_cfg->tx_rate[3];
+		if (sw->verbose)
+			strcpy(note, " (bps)");
+		type = SHOW_HELP_SPECIAL;
+		break;
+	}
+	return sysfs_show(len, buf, type, chk, note, sw->verbose);
+}  /* sysfs_port_read */
+
+static ssize_t sysfs_port_read_hw(struct ksz_sw *sw, int proc_num, uint n,
+	ssize_t len, char *buf)
+{
+	uint port;
+	int chk = 0;
+	int type = SHOW_HELP_ON_OFF;
+	char note[40];
+
+	note[0] = '\0';
+	port = get_sysfs_port(sw, n);
+	switch (proc_num) {
+	case PROC_ENABLE_BROADCAST_STORM:
+		chk = port_chk_broad_storm(sw, port);
+		break;
+	case PROC_ENABLE_DIFFSERV:
+		chk = port_chk_diffserv(sw, port);
+		break;
+	case PROC_ENABLE_802_1P:
+		chk = port_chk_802_1p(sw, port);
+		break;
+	case PROC_ENABLE_PRIO_QUEUE:
+		chk = port_get_prio_queue(sw, port);
+		type = SHOW_HELP_NUM;
+		break;
+	case PROC_SET_MIRROR_PORT:
+		chk = port_chk_mirror_sniffer(sw, port);
+		break;
+	case PROC_SET_MIRROR_RX:
+		chk = port_chk_mirror_rx(sw, port);
+		break;
+	case PROC_SET_MIRROR_TX:
+		chk = port_chk_mirror_tx(sw, port);
+		break;
+	case PROC_SET_RX:
+		chk = port_chk_rx(sw, port);
+		break;
+	case PROC_SET_TX:
+		chk = port_chk_tx(sw, port);
+		break;
+	case PROC_SET_LEARN:
+		chk = !port_chk_dis_learn(sw, port);
+		break;
+	case PROC_SET_INSERT_TAG:
+		chk = port_chk_ins_tag(sw, port);
+		break;
+	case PROC_SET_REMOVE_TAG:
+		chk = port_chk_rmv_tag(sw, port);
+		break;
+	case PROC_SET_DROP_TAG:
+		chk = port_chk_drop_tag(sw, port);
+		break;
+	case PROC_SET_REPLACE_PRIO:
+		chk = port_chk_replace_prio(sw, port);
+		break;
+	case PROC_ENABLE_RX_PRIO_RATE:
+		chk = sw_chk_rx_prio_rate(sw, port);
+		break;
+	case PROC_ENABLE_TX_PRIO_RATE:
+		chk = sw_chk_tx_prio_rate(sw, port);
+		break;
+	case PROC_SET_DIS_NON_VID:
+		chk = port_chk_dis_non_vid(sw, port);
+		break;
+	case PROC_SET_INGRESS:
+		chk = port_chk_in_filter(sw, port);
+		break;
+	case PROC_SET_BACK_PRESSURE:
+		chk = port_chk_back_pressure(sw, port);
+		break;
+	case PROC_SET_FORCE_FLOW_CTRL:
+		chk = port_chk_force_flow_ctrl(sw, port);
+		break;
+	case PROC_SET_INS_TAG_0:
+		chk = sw_chk_ins(sw, port, 0);
+		break;
+	case PROC_SET_INS_TAG_1:
+		chk = sw_chk_ins(sw, port, 1);
+		break;
+	case PROC_SET_INS_TAG_2:
+		chk = sw_chk_ins(sw, port, 2);
+		break;
+	case PROC_SET_INS_TAG_3:
+		chk = sw_chk_ins(sw, port, 3);
+		break;
+	case PROC_SET_INS_TAG_4:
+		chk = sw_chk_ins(sw, port, 4);
+		break;
+	case PROC_SET_UNKNOWN_UNICAST_PORT:
+		chk = sw_chk_unk_def_port(sw, REG_SW_UNK_UCAST_CTRL, port);
+		break;
+	case PROC_SET_UNKNOWN_MULTICAST_PORT:
+		chk = sw_chk_unk_def_port(sw, REG_SW_UNK_MCAST_CTRL, port);
+		break;
+	case PROC_SET_UNKNOWN_VID_PORT:
+		chk = sw_chk_unk_def_port(sw, REG_SW_UNK_VID_CTRL, port);
+		break;
+	case PROC_SET_UNKNOWN_IP_MULTICAST_PORT:
+		chk = sw_chk_unk_def_port(sw, REG_SW_UNK_IP_MCAST_CTRL, port);
+		break;
+	}
+	return sysfs_show(len, buf, type, chk, note, sw->verbose);
+}  /* sysfs_port_read_hw */
+
+static int sysfs_port_write(struct ksz_sw *sw, int proc_num, uint n,
+	int num, const char *buf)
+{
+	uint port;
+	int processed = true;
+
+	port = get_sysfs_port(sw, n);
+	switch (proc_num) {
+	case PROC_SET_PORT_DUPLEX:
+	case PROC_SET_PORT_SPEED:
+	{
+		struct ksz_port phy_port;
+		struct ksz_port_info *port_info = get_port_info(sw, port);
+
+		if ((PROC_SET_PORT_DUPLEX == proc_num && num > 2) ||
+		    (PROC_SET_PORT_SPEED == proc_num &&
+		    num != 0 && num != 10 && num != 100))
+			break;
+
+		if (port == sw->HOST_PORT)
+			break;
+		phy_port.sw = sw;
+		phy_port.port_cnt = 1;
+		phy_port.first_port = get_log_port(sw, port);
+		phy_port.flow_ctrl = port_info->own_flow_ctrl;
+		phy_port.duplex = port_info->own_duplex;
+		phy_port.speed = port_info->own_speed;
+		if (PROC_SET_PORT_DUPLEX == proc_num)
+			phy_port.duplex = (u8) num;
+		else
+			phy_port.speed = (u16) num;
+		port_set_link_speed(&phy_port);
+		break;
+	}
+	case PROC_SET_PORT_MIB:
+		sw_cfg_mib_counter_ctrl(sw, num, port);
+		break;
+	case PROC_ENABLE_BROADCAST_STORM:
+		if (!num)
+			sw_dis_broad_storm(sw, port);
+		else
+			sw_ena_broad_storm(sw, port);
+		break;
+	case PROC_ENABLE_DIFFSERV:
+		if (!num)
+			sw_dis_diffserv(sw, port);
+		else
+			sw_ena_diffserv(sw, port);
+		break;
+	case PROC_ENABLE_802_1P:
+		if (!num)
+			sw_dis_802_1p(sw, port);
+		else
+			sw_ena_802_1p(sw, port);
+		break;
+	case PROC_SET_PORT_BASED:
+		sw_cfg_port_based(sw, port, num);
+		break;
+	case PROC_SET_DEF_VID:
+		sw_cfg_def_vid(sw, port, num);
+		break;
+	case PROC_SET_MEMBER:
+		num = get_phy_mask_from_log(sw, num);
+		sw_cfg_port_base_vlan(sw, port, (u8) num);
+		break;
+	case PROC_ENABLE_PRIO_QUEUE:
+		if (0 <= num && num <= 4)
+			port_set_prio_queue(sw, port, num);
+		break;
+	case PROC_SET_INS_TAG_0:
+		sw_cfg_ins(sw, port, 0, num);
+		break;
+	case PROC_SET_INS_TAG_1:
+		sw_cfg_ins(sw, port, 1, num);
+		break;
+	case PROC_SET_INS_TAG_2:
+		sw_cfg_ins(sw, port, 2, num);
+		break;
+	case PROC_SET_INS_TAG_3:
+		sw_cfg_ins(sw, port, 3, num);
+		break;
+	case PROC_SET_INS_TAG_4:
+		sw_cfg_ins(sw, port, 4, num);
+		break;
+	case PROC_SET_TX_Q0_CTRL:
+		hw_cfg_rate_ctrl(sw, port, 0, num);
+		break;
+	case PROC_SET_TX_Q1_CTRL:
+		hw_cfg_rate_ctrl(sw, port, 1, num);
+		break;
+	case PROC_SET_TX_Q2_CTRL:
+		hw_cfg_rate_ctrl(sw, port, 2, num);
+		break;
+	case PROC_SET_TX_Q3_CTRL:
+		hw_cfg_rate_ctrl(sw, port, 3, num);
+		break;
+	case PROC_SET_TX_Q0_RATIO:
+		hw_cfg_rate_ratio(sw, port, 0, (u8) num);
+		break;
+	case PROC_SET_TX_Q1_RATIO:
+		hw_cfg_rate_ratio(sw, port, 1, (u8) num);
+		break;
+	case PROC_SET_TX_Q2_RATIO:
+		hw_cfg_rate_ratio(sw, port, 2, (u8) num);
+		break;
+	case PROC_SET_TX_Q3_RATIO:
+		hw_cfg_rate_ratio(sw, port, 3, (u8) num);
+		break;
+	case PROC_SET_RX_LIMIT:
+		hw_cfg_rx_limit(sw, port, (u8) num);
+		break;
+	case PROC_SET_RX_LIMIT_FLOW_CTRL:
+		hw_cfg_in_flow_ctrl(sw, port, num);
+		break;
+	case PROC_SET_CNT_IFG:
+		hw_cfg_cnt_ifg(sw, port, num);
+		break;
+	case PROC_SET_CNT_PRE:
+		hw_cfg_cnt_pre(sw, port, num);
+		break;
+	case PROC_SET_RX_P0_RATE:
+		hw_cfg_rx_prio_rate(sw, port, 0, num);
+		break;
+	case PROC_SET_RX_P1_RATE:
+		hw_cfg_rx_prio_rate(sw, port, 1, num);
+		break;
+	case PROC_SET_RX_P2_RATE:
+		hw_cfg_rx_prio_rate(sw, port, 2, num);
+		break;
+	case PROC_SET_RX_P3_RATE:
+		hw_cfg_rx_prio_rate(sw, port, 3, num);
+		break;
+	case PROC_SET_TX_Q0_RATE:
+		hw_cfg_tx_prio_rate(sw, port, 0, num);
+		break;
+	case PROC_SET_TX_Q1_RATE:
+		hw_cfg_tx_prio_rate(sw, port, 1, num);
+		break;
+	case PROC_SET_TX_Q2_RATE:
+		hw_cfg_tx_prio_rate(sw, port, 2, num);
+		break;
+	case PROC_SET_TX_Q3_RATE:
+		hw_cfg_tx_prio_rate(sw, port, 3, num);
+		break;
+	case PROC_SET_MIRROR_PORT:
+		port_cfg_mirror_sniffer(sw, port, num);
+		break;
+	case PROC_SET_MIRROR_RX:
+		port_cfg_mirror_rx(sw, port, num);
+		break;
+	case PROC_SET_MIRROR_TX:
+		port_cfg_mirror_tx(sw, port, num);
+		break;
+	case PROC_SET_RX:
+		port_cfg_rx(sw, port, num);
+		break;
+	case PROC_SET_TX:
+		port_cfg_tx(sw, port, num);
+		break;
+	case PROC_SET_LEARN:
+		port_cfg_dis_learn(sw, port, !num);
+		if (!num)
+			sw_cfg(sw, S_FLUSH_TABLE_CTRL,
+				SW_FLUSH_DYN_MAC_TABLE, 1);
+		break;
+	case PROC_SET_INSERT_TAG:
+		sw_vlan_cfg_ins_tag(sw, port, num);
+		break;
+	case PROC_SET_REMOVE_TAG:
+		sw_vlan_cfg_rmv_tag(sw, port, num);
+		break;
+	case PROC_SET_DROP_TAG:
+		sw_vlan_cfg_drop_tag(sw, port, num);
+		break;
+	case PROC_SET_REPLACE_PRIO:
+		sw_cfg_replace_prio(sw, port, num);
+		break;
+	case PROC_ENABLE_RX_PRIO_RATE:
+		if (!num)
+			sw_dis_rx_prio_rate(sw, port);
+		else
+			sw_ena_rx_prio_rate(sw, port);
+		break;
+	case PROC_ENABLE_TX_PRIO_RATE:
+		if (!num)
+			sw_dis_tx_prio_rate(sw, port);
+		else
+			sw_ena_tx_prio_rate(sw, port);
+		break;
+	case PROC_SET_DIS_NON_VID:
+		sw_vlan_cfg_dis_non_vid(sw, port, num);
+		break;
+	case PROC_SET_INGRESS:
+		sw_vlan_cfg_in_filter(sw, port, num);
+		break;
+	case PROC_SET_BACK_PRESSURE:
+		port_cfg_back_pressure(sw, port, num);
+		break;
+	case PROC_SET_FORCE_FLOW_CTRL:
+		port_cfg_force_flow_ctrl(sw, port, num);
+		break;
+	case PROC_SET_UNKNOWN_UNICAST_PORT:
+		sw_cfg_unk_def_port(sw, REG_SW_UNK_UCAST_CTRL, port, num);
+		break;
+	case PROC_SET_UNKNOWN_MULTICAST_PORT:
+		sw_cfg_unk_def_port(sw, REG_SW_UNK_MCAST_CTRL, port, num);
+		break;
+	case PROC_SET_UNKNOWN_VID_PORT:
+		sw_cfg_unk_def_port(sw, REG_SW_UNK_VID_CTRL, port, num);
+		break;
+	case PROC_SET_UNKNOWN_IP_MULTICAST_PORT:
+		sw_cfg_unk_def_port(sw, REG_SW_UNK_IP_MCAST_CTRL, port, num);
+		break;
+	case PROC_SET_LINK_MD:
+		sw_get_link_md(sw, port);
+		break;
+	default:
+		processed = false;
+		break;
+	}
+	return processed;
+}  /* sysfs_port_write */
+
+static ssize_t sysfs_mac_read(struct ksz_sw *sw, int proc_num, ssize_t len,
+	char *buf)
+{
+	struct ksz_mac_table *entry;
+	u8 ports;
+
+	entry = &sw->info->mac_entry;
+	switch (proc_num) {
+	case PROC_SET_STATIC_FID:
+		len += sprintf(buf + len, "0x%02x\n", entry->fid);
+		break;
+	case PROC_SET_STATIC_USE_FID:
+		len += sprintf(buf + len, "%u\n", entry->use_fid);
+		break;
+	case PROC_SET_STATIC_OVERRIDE:
+		len += sprintf(buf + len, "%u\n", entry->override);
+		break;
+	case PROC_SET_STATIC_VALID:
+		len += sprintf(buf + len, "%u\n", entry->valid);
+		break;
+	case PROC_SET_STATIC_PORTS:
+		ports = entry->ports;
+		ports = get_log_mask_from_phy(sw, entry->ports);
+		len += sprintf(buf + len, "0x%02x\n", ports);
+		break;
+	case PROC_SET_STATIC_MAC_ADDR:
+		len += sprintf(buf + len, "%02x:%02x:%02x:%02x:%02x:%02x\n",
+			entry->addr[0], entry->addr[1],
+			entry->addr[2], entry->addr[3],
+			entry->addr[4], entry->addr[5]);
+		break;
+	case PROC_SET_STATIC_INDEX:
+		len += sprintf(buf + len, "0x%02x\n", sw->mac_index);
+		break;
+	case PROC_GET_STATIC_INFO:
+		if (sw->mac_dirty) {
+			if (!entry->dirty)
+				sw_r_sta_mac_table(sw, sw->mac_index, entry);
+			sw->mac_dirty = 0;
+		}
+		ports = entry->ports;
+		ports = get_log_mask_from_phy(sw, ports);
+		len += sprintf(buf + len,
+			"%2x: %02X:%02X:%02X:%02X:%02X:%02X "
+			"%02x o:%u %u:%02x [%u]\n",
+			sw->mac_index,
+			entry->addr[0], entry->addr[1], entry->addr[2],
+			entry->addr[3], entry->addr[4], entry->addr[5],
+			ports, entry->override,
+			entry->use_fid, entry->fid,
+			entry->dirty ? 2 : entry->valid);
+		break;
+	}
+	return len;
+}  /* sysfs_mac_read */
+
+static int sysfs_mac_write(struct ksz_sw *sw, int proc_num, int num,
+	const char *buf)
+{
+	struct ksz_mac_table *entry;
+	u8 ports;
+	int processed = true;
+
+	entry = &sw->info->mac_entry;
+	switch (proc_num) {
+	case PROC_SET_STATIC_FID:
+		if (0 <= num && num < 128) {
+			entry->fid = num;
+			entry->dirty = 1;
+		}
+		break;
+	case PROC_SET_STATIC_USE_FID:
+		if (num)
+			entry->use_fid = 1;
+		else
+			entry->use_fid = 0;
+		entry->dirty = 1;
+		break;
+	case PROC_SET_STATIC_OVERRIDE:
+		if (num)
+			entry->override = 1;
+		else
+			entry->override = 0;
+		entry->dirty = 1;
+		break;
+	case PROC_SET_STATIC_VALID:
+		if (num)
+			entry->valid = 1;
+		else
+			entry->valid = 0;
+		sw_w_sta_mac_table(sw, sw->mac_index, entry);
+		break;
+	case PROC_SET_STATIC_PORTS:
+		if (0 <= num && num <= sw->PORT_MASK) {
+			ports = num;
+			ports = get_phy_mask_from_log(sw, ports);
+			entry->ports = ports;
+			entry->dirty = 1;
+		}
+		break;
+	case PROC_SET_STATIC_MAC_ADDR:
+	{
+		int i;
+		int n[6];
+
+		i = sscanf(buf, "%x:%x:%x:%x:%x:%x",
+			&n[0], &n[1], &n[2], &n[3], &n[4], &n[5]);
+		if (6 == i) {
+			for (i = 0; i < 6; i++)
+				entry->addr[i] = (u8) n[i];
+			entry->dirty = 1;
+		}
+		break;
+	}
+	case PROC_SET_STATIC_INDEX:
+		if (0 <= num && num < STATIC_MAC_TABLE_ENTRIES) {
+			sw->mac_index = num;
+			sw->mac_dirty = 1;
+		}
+		break;
+	default:
+		processed = false;
+		break;
+	}
+	return processed;
+}  /* sysfs_mac_write */
+
+static ssize_t sysfs_vlan_read(struct ksz_sw *sw, int proc_num,	ssize_t len,
+	char *buf)
+{
+	struct ksz_vlan_table *entry;
+
+	entry = &sw->info->vlan_entry;
+	switch (proc_num) {
+	case PROC_SET_VLAN_VALID:
+		len += sprintf(buf + len, "%u\n", entry->valid);
+		break;
+	case PROC_SET_VLAN_MEMBER:
+		len += sprintf(buf + len, "0x%02x\n", entry->member);
+		break;
+	case PROC_SET_VLAN_FID:
+		len += sprintf(buf + len, "0x%02x\n", entry->fid);
+		break;
+	case PROC_SET_VLAN_VID:
+		len += sprintf(buf + len, "0x%03x\n", sw->vlan_index);
+		break;
+	case PROC_GET_VLAN_INFO:
+		if (sw->vlan_dirty) {
+			if (!entry->dirty) {
+				sw_r_vlan_table(sw, sw->vlan_index, entry);
+				entry->member =
+					get_log_mask_from_phy(sw, entry->member);
+			}
+			sw->vlan_dirty = 0;
+		}
+		len += sprintf(buf + len,
+			"%3x: 0x%02x %02x [%u]\n",
+			sw->vlan_index, entry->fid, entry->member,
+			entry->dirty ? 2 : entry->valid);
+		break;
+	}
+	return len;
+}  /* sysfs_vlan_read */
+
+static int sysfs_vlan_write(struct ksz_sw *sw, int proc_num, int num)
+{
+	struct ksz_vlan_table *entry;
+	u8 ports;
+	int processed = true;
+
+	entry = &sw->info->vlan_entry;
+	switch (proc_num) {
+	case PROC_SET_VLAN_VALID:
+		if (num)
+			entry->valid = 1;
+		else
+			entry->valid = 0;
+		ports = entry->member;
+		entry->member = get_phy_mask_from_log(sw, entry->member);
+		sw_w_vlan_table(sw, sw->vlan_index, entry);
+		entry->member = ports;
+		sw->vlan_dirty = 0;
+		break;
+	case PROC_SET_VLAN_MEMBER:
+		if (0 <= num && num <= sw->PORT_MASK) {
+			entry->member = num;
+			entry->dirty = 1;
+		}
+		break;
+	case PROC_SET_VLAN_FID:
+		if (0 <= num && num < 128) {
+			entry->fid = num;
+			entry->dirty = 1;
+		}
+		break;
+	case PROC_SET_VLAN_VID:
+		if (0 <= num && num < 0x1000) {
+			sw->vlan_index = num;
+			sw->vlan_dirty = 1;
+		}
+		break;
+	default:
+		processed = false;
+		break;
+	}
+	return processed;
+}  /* sysfs_vlan_write */
+
+static void sw_cfg_mac(struct ksz_sw *sw, u8 index, u8 *dest, u32 ports,
+	int override, int use_fid, u16 fid)
+{
+	struct ksz_mac_table mac;
+
+	if (index >= STATIC_MAC_TABLE_ENTRIES)
+		return;
+	memset(&mac, 0, sizeof(struct ksz_mac_table));
+	memcpy(mac.addr, dest, ETH_ALEN);
+	mac.ports = (u8) ports;
+	mac.override = override;
+	mac.use_fid = use_fid;
+	mac.fid = (u8) fid;
+	mac.valid = mac.ports != 0;
+	if (!mac.valid && mac.override) {
+		mac.override = 0;
+		mac.valid = 1;
+	}
+	sw_w_sta_mac_table(sw, index, &mac);
+}  /* sw_cfg_mac */
+
+static void sw_cfg_vlan(struct ksz_sw *sw, u8 index, u16 vid, u16 fid,
+	u32 ports)
+{
+	struct ksz_vlan_table vlan;
+
+	if (0xffff == ports)
+		ports = sw->PORT_MASK;
+	memset(&vlan, 0, sizeof(struct ksz_vlan_table));
+	vlan.vid = vid;
+	vlan.fid = (u8) fid;
+	vlan.member = (u8)(ports & sw->PORT_MASK);
+	vlan.valid = ports != 0;
+	sw_w_vlan_table(sw, vlan.vid, &vlan);
+}  /* sw_cfg_vlan */
+
+static u8 sw_alloc_mac(struct ksz_sw *sw)
+{
+	int i;
+
+	for (i = 1; i < STATIC_MAC_TABLE_ENTRIES; i++) {
+		if (!(sw->info->mac_table_used & (1 << i))) {
+			sw->info->mac_table_used |= (1 << i);
+			return i;
+		}
+	}
+
+	/* Reject request. */
+	return 0;
+}  /* sw_alloc_mac */
+
+static void sw_free_mac(struct ksz_sw *sw, u8 index)
+{
+	sw->info->mac_table_used &= ~(1 << index);
+}  /* sw_free_mac */
+
+static u8 sw_alloc_vlan(struct ksz_sw *sw)
+{
+	return 1;
+}  /* sw_alloc_vlan */
+
+static void sw_free_vlan(struct ksz_sw *sw, u8 index)
+{
+}  /* sw_free_vlan */
+
+static u16 sw_alloc_fid(struct ksz_sw *sw, u16 vid)
+{
+	int x;
+	int y;
+	u16 fid;
+
+	if (sw->info->fid_cnt + 2 == FID_ENTRIES)
+		return 0;
+	fid = vid & (FID_ENTRIES - 1);
+	if (vid < 2)
+		fid = 100;
+	x = fid / FID_IN_DATA;
+	y = fid % FID_IN_DATA;
+	while (sw->info->fid[x] & (1 << y)) {
+		++fid;
+		++y;
+		if (y >= FID_IN_DATA) {
+			y = 0;
+			++x;
+		}
+	}
+	sw->info->fid[x] |= (1 << y);
+	++sw->info->fid_cnt;
+	return fid;
+}  /* sw_alloc_fid */
+
+static void sw_free_fid(struct ksz_sw *sw, u16 fid)
+{
+	int x;
+	int y;
+
+	x = fid / FID_IN_DATA;
+	y = fid % FID_IN_DATA;
+	if (sw->info->fid[x] & (1 << y)) {
+		sw->info->fid[x] &= ~(1 << y);
+		--sw->info->fid_cnt;
+	}
+}  /* sw_free_fid */
+
+static const u8 *sw_get_br_id(struct ksz_sw *sw)
+{
+	static u8 id[8];
+	const u8* ret = id;
+
+	memcpy(&id[2], sw->info->mac_addr, ETH_ALEN);
+	id[0] = 0x80;
+	id[1] = 0x00;
+
+#ifdef CONFIG_KSZ_STP
+	ret = stp_br_id(&sw->info->rstp);
+#endif
+	return ret;
+}  /* sw_get_br_id */
+
+static void sw_from_backup(struct ksz_sw *sw, uint p)
+{
+#ifdef CONFIG_KSZ_MRP
+	if (sw->features & MRP_SUPPORT) {
+		struct mrp_info *mrp = &sw->mrp;
+
+		mrp->ops->from_backup(mrp, p);
+	}
+#endif
+}  /* sw_from_backup */
+
+static void sw_to_backup(struct ksz_sw *sw, uint p)
+{
+#ifdef CONFIG_KSZ_MRP
+	if (sw->features & MRP_SUPPORT) {
+		struct mrp_info *mrp = &sw->mrp;
+
+		mrp->ops->to_backup(mrp, p);
+	}
+#endif
+}  /* sw_to_backup */
+
+static void sw_from_designated(struct ksz_sw *sw, uint p, bool alt)
+{
+#ifdef CONFIG_KSZ_MRP
+	if (sw->features & MRP_SUPPORT) {
+		struct mrp_info *mrp = &sw->mrp;
+
+		mrp->ops->from_designated(mrp, p, alt);
+	}
+#endif
+}  /* sw_from_designated */
+
+static void sw_to_designated(struct ksz_sw *sw, uint p)
+{
+#ifdef CONFIG_KSZ_MRP
+	if (sw->features & MRP_SUPPORT) {
+		struct mrp_info *mrp = &sw->mrp;
+
+		mrp->ops->to_designated(mrp, p);
+	}
+#endif
+}  /* sw_to_designated */
+
+static void sw_tc_detected(struct ksz_sw *sw, uint p)
+{
+#ifdef CONFIG_KSZ_MRP
+	if (sw->features & MRP_SUPPORT) {
+		struct mrp_info *mrp = &sw->mrp;
+
+		mrp->ops->tc_detected(mrp, p);
+	}
+#endif
+}  /* sw_tc_detected */
+
+static int sw_get_tcDetected(struct ksz_sw *sw, uint p)
+{
+	int ret = false;
+
+#ifdef CONFIG_KSZ_STP
+	if (sw->features & STP_SUPPORT) {
+		struct ksz_stp_info *info = &sw->info->rstp;
+
+		ret = info->ops->get_tcDetected(info, p);
+	}
+#endif
+	return ret;
+}  /* sw_get_tcDetected */
+
+static void sw_ena_intr(struct ksz_sw *sw)
+{
+	struct sw_priv *ks = container_of(sw, struct sw_priv, sw);
+
+	sw->reg->w8(sw, REG_INT_ENABLE, ks->intr_mask);
+}  /* sw_ena_intr */
+
+enum {
+	KSZ8895_SW_CHIP,
+	KSZ8864_SW_CHIP,
+};
+
+static void sw_cfg_tail_tag(struct ksz_sw *sw, bool enable)
+{
+	sw_cfg(sw, S_TAIL_TAG_CTRL, SW_TAIL_TAG_ENABLE, enable);
+}
+
+static void sw_set_multi(struct ksz_sw *sw, struct net_device *dev,
+	struct ksz_port *priv)
+{
+	struct ksz_mac_table *entry;
+	struct ksz_alu_table *alu;
+	struct netdev_hw_addr *ha;
+	int i;
+	int found;
+	int owner;
+	int port = 0;
+	struct ksz_sw_info *info = sw->info;
+
+	/* This device is associated with a switch port. */
+	if (1 == priv->port_cnt)
+		port = priv->first_port;
+	owner = 1 << port;
+
+	/* Remove old multicast entries. */
+	for (i = SWITCH_MAC_TABLE_ENTRIES; i < info->multi_net; i++) {
+		entry = &info->mac_table[i];
+		alu = &info->alu_table[i];
+		if (alu->valid && (alu->owner & owner)) {
+			/* Remove device ownership. */
+			alu->owner &= ~owner;
+			if (!port)
+				alu->forward &= ~FWD_MAIN_DEV;
+			else if (alu->owner <= 1)
+				alu->forward &= ~FWD_STP_DEV;
+			if (!alu->owner) {
+				alu->valid = 0;
+				entry->ports = 0;
+				entry->valid = 0;
+			}
+		}
+	}
+	netdev_for_each_mc_addr(ha, dev) {
+		if (!(*ha->addr & 1))
+			continue;
+		if (info->multi_net == info->multi_sys)
+			break;
+		found = 0;
+		for (i = 0; i < MULTI_MAC_TABLE_ENTRIES; i++) {
+			entry = &info->mac_table[i];
+			alu = &info->alu_table[i];
+			if (alu->valid &&
+			    !memcmp(entry->addr, ha->addr, ETH_ALEN)) {
+				found = i + 1;
+				break;
+			}
+			if (!alu->valid && !found &&
+			    i >= SWITCH_MAC_TABLE_ENTRIES &&
+			    i < info->multi_net)
+				found = i + 1;
+		}
+		if (!found) {
+			info->multi_net++;
+			found = info->multi_net;
+		}
+		found--;
+		if (found >= SWITCH_MAC_TABLE_ENTRIES &&
+		    found < info->multi_net) {
+			entry = &info->mac_table[found];
+			alu = &info->alu_table[found];
+			if (port)
+				alu->forward |= FWD_STP_DEV;
+			else
+				alu->forward |= FWD_MAIN_DEV;
+			alu->owner |= owner;
+			alu->valid = 1;
+			memcpy(entry->addr, ha->addr, ETH_ALEN);
+			entry->ports = sw->PORT_MASK;
+			entry->valid = 1;
+		}
+	}
+}  /* sw_set_multi */
+
+static struct net_device *sw_rx_dev(struct ksz_sw *sw, u8 *data, u32 *len,
+	int *tag, int *port)
+{
+	struct vlan_ethhdr *vlan = (struct vlan_ethhdr *) data;
+	struct net_device *dev;
+	u16 *proto_loc;
+	u16 proto;
+	int index = -1;
+	int vid = 0;
+
+	proto_loc = &vlan->h_vlan_proto;
+	proto = htons(*proto_loc);
+
+	/* Ignore PAUSE frame sent by switch. */
+	if (!memcmp(vlan->h_source, sw->info->mac_addr, ETH_ALEN) &&
+	    proto == ETH_P_PAUSE)
+		return NULL;
+
+	/* Get received port number. */
+	if (sw->overrides & TAIL_TAGGING) {
+		(*len)--;
+		*tag = data[*len];
+
+		/* In case tagging is not working right. */
+		if (*tag >= SWITCH_PORT_NUM)
+			*tag = 0;
+
+		/* Save receiving port. */
+		*port = *tag;
+		index = sw->info->port_cfg[*tag].index;
+	}
+
+	/* Determine network device from VLAN id. */
+	if (index < 0) {
+		index = 0;
+		if (vlan->h_vlan_proto == htons(ETH_P_8021Q)) {
+			u16 vlan_tci = ntohs(vlan->h_vlan_TCI);
+
+			vid = vlan_tci & VLAN_VID_MASK;
+		}
+		if (vid && (sw->features & SW_VLAN_DEV)) {
+			struct ksz_dev_map *map;
+			int p;
+
+			for (p = 0; p < sw->eth_cnt; p++) {
+				map = &sw->eth_maps[p];
+				if (vid == map->vlan) {
+					*port = map->first;
+					p = get_phy_port(sw, *port);
+					*port = p;
+					index = sw->info->port_cfg[p].index;
+					break;
+				}
+			}
+		}
+	}
+	if (index >= sw->dev_count + sw->dev_offset) {
+		printk(KERN_INFO "  [%s] netdev not correct\n", __func__);
+		BUG();
+	}
+	dev = sw->netdev[index];
+	if (!(sw->features & VLAN_PORT_TAGGING) ||
+	    !(sw->vlan_id & (1 << *tag))) {
+		*tag = 0;
+	}
+	return dev;
+}  /* sw_rx_dev */
+
+static int pkt_matched(struct sk_buff *skb, struct net_device *dev, void *ptr,
+	int (*match_multi)(void *ptr, u8 *addr), u8 h_promiscuous)
+{
+	int drop = false;
+	u8 bcast_addr[] = { 0xff, 0xff, 0xff, 0xff, 0xff, 0xff };
+
+	if (skb->data[0] & 0x01) {
+		if (memcmp(skb->data, bcast_addr, ETH_ALEN))
+			drop = match_multi(ptr, skb->data);
+	} else if (h_promiscuous && memcmp(skb->data, dev->dev_addr, ETH_ALEN))
+		drop = true;
+	if (drop)
+		return 0;
+	return skb->len;
+}  /* pkt_matched */
+
+static int sw_match_pkt(struct ksz_sw *sw, struct net_device **dev,
+	void **priv, int (*get_promiscuous)(void *ptr),
+	int (*match_multi)(void *ptr, u8 *data), struct sk_buff *skb,
+	u8 h_promiscuous)
+{
+	int s_promiscuous;
+
+	if (sw->dev_count <= 1)
+		return true;
+	s_promiscuous = get_promiscuous(*priv);
+	if (!s_promiscuous && !pkt_matched(skb, *dev, *priv, match_multi,
+			h_promiscuous)) {
+		int matched = false;
+
+		/* There is a parent network device. */
+		if (sw->dev_offset) {
+			matched = true;
+			*dev = sw->netdev[0];
+			*priv = netdev_priv(*dev);
+			s_promiscuous = get_promiscuous(*priv);
+			if (!s_promiscuous && !pkt_matched(skb, *dev, *priv,
+					match_multi, h_promiscuous))
+				matched = false;
+		}
+		return matched;
+	}
+	return true;
+}  /* sw_match_pkt */
+
+static struct net_device *sw_parent_rx(struct ksz_sw *sw,
+				       struct net_device *dev, int *forward)
+{
+	if (sw->dev_offset && dev != sw->netdev[0]) {
+		if (!*forward)
+			*forward = FWD_MAIN_DEV;
+		if (!(*forward & FWD_STP_DEV))
+			dev = sw->netdev[0];
+		else
+			*forward &= ~FWD_VLAN_DEV;
+	}
+	return dev;
+}  /* sw_parent_rx */
+
+static int sw_port_vlan_rx(struct sk_buff *skb, int forward, int tag)
+{
+	/* Add VLAN tag manually. */
+	if (!(forward & FWD_VLAN_DEV) || !tag)
+		return false;
+
+	tag += VLAN_PORT_START;
+
+	/* Only forward to one network device. */
+	__vlan_hwaccel_put_tag(skb, htons(ETH_P_8021Q), tag);
+	return true;
+}  /* sw_port_vlan_rx */
+
+static int sw_drv_rx(struct ksz_sw *sw, struct sk_buff *skb, uint port)
+{
+	int ret = 1;
+
+#ifdef CONFIG_KSZ_STP
+	if (sw->features & STP_SUPPORT) {
+		ret = stp_rcv(&sw->info->rstp, skb, port);
+		if (!ret)
+			return ret;
+	}
+#endif
+#ifdef PROC_MRP
+	if (sw->features & MRP_SUPPORT) {
+		ret = mrp_rcv(&sw->mrp, skb, port);
+		if (!ret)
+			return ret;
+	}
+#endif
+
+	/* Need to remove VLAN tag if not using tail tag. */
+	if (sw->dev_count > 1 && (sw->features & SW_VLAN_DEV) &&
+	    !(sw->overrides & TAIL_TAGGING)) {
+		struct vlan_ethhdr *vlan = (struct vlan_ethhdr *) skb->data;
+
+		if (vlan->h_vlan_proto == htons(ETH_P_8021Q)) {
+			int p;
+			int vid;
+			struct ethhdr *eth;
+			u16 vlan_tci = ntohs(vlan->h_vlan_TCI);
+
+			vid = vlan_tci & VLAN_VID_MASK;
+			for (p = 0; p < sw->eth_cnt; p++) {
+				if (vid == sw->eth_maps[p].vlan) {
+					eth = (struct ethhdr *)
+						skb_pull(skb, VLAN_HLEN);
+					memmove(eth, vlan, 12);
+					break;
+				}
+			}
+		}
+	}
+	return ret;
+}  /* sw_drv_rx */
+
+static int sw_get_mtu(struct ksz_sw *sw)
+{
+	int need_tail_tag = false;
+	int header = 0;
+	int mtu = 0;
+
+	if (sw->dev_count > 1 && !(sw->features & SW_VLAN_DEV))
+		need_tail_tag = true;
+	if (sw->features & VLAN_PORT_TAGGING)
+		need_tail_tag = true;
+	if (sw->features & STP_SUPPORT)
+		need_tail_tag = true;
+	if (need_tail_tag)
+		mtu += 1;
+	if (sw->features & SW_VLAN_DEV)
+		if (header < VLAN_HLEN)
+			header = VLAN_HLEN;
+	mtu += header;
+	return mtu;
+}  /* sw_get_mtu */
+
+static int sw_get_tx_len(struct ksz_sw *sw, struct sk_buff *skb, uint port,
+	int *header)
+{
+	int len = skb->len;
+	int hlen = 0;
+
+	if (sw->features & SW_VLAN_DEV)
+		hlen = VLAN_HLEN;
+	*header += hlen;
+	if (!(sw->overrides & TAIL_TAGGING))
+		return len;
+	if (len < 60)
+		len = 60;
+	len += 1;
+	return len;
+}  /* sw_get_tx_len */
+
+static void sw_add_tail_tag(struct ksz_sw *sw, struct sk_buff *skb, uint ports)
+{
+	u8 *trailer;
+	int len = 1;
+
+	trailer = skb_put(skb, len);
+	if (!ports)
+		ports = TAIL_TAG_LOOKUP;
+	else if (ports & TAIL_TAG_SET_OVERRIDE) {
+		ports &= ~TAIL_TAG_SET_OVERRIDE;
+		ports |= TAIL_TAG_OVERRIDE;
+	}
+	trailer[0] = (u8) ports;
+}  /* sw_add_tail_tag */
+
+static int sw_get_tail_tag(u8 *trailer, int *port)
+{
+	int len = 1;
+
+	*port = *trailer;
+	return len;
+}  /* sw_get_tail_tag */
+
+static void sw_add_vid(struct ksz_sw *sw, u16 vid)
+{
+	if ((sw->features & VLAN_PORT) && vid >= VLAN_PORT_START) {
+		vid -= VLAN_PORT_START;
+		if (vid <= SWITCH_PORT_NUM)
+			sw->vlan_id |= (1 << vid);
+	}
+}  /* sw_add_vid */
+
+static void sw_kill_vid(struct ksz_sw *sw, u16 vid)
+{
+	if ((sw->features & VLAN_PORT) && vid >= VLAN_PORT_START) {
+		vid -= VLAN_PORT_START;
+		if (vid <= SWITCH_PORT_NUM)
+			sw->vlan_id &= ~(1 << vid);
+	}
+}  /* sw_kill_vid */
+
+static struct sk_buff *sw_ins_vlan(struct ksz_sw *sw, uint port,
+	struct sk_buff *skb)
+{
+	port = get_phy_port(sw, port);
+
+	/* Need to insert VLAN tag. */
+	if (sw->dev_count > 1 && (sw->features & SW_VLAN_DEV)) {
+		u16 vid;
+		struct vlan_ethhdr *vlan;
+		struct ethhdr *eth;
+		struct sk_buff *nskb = NULL;
+
+		/*
+		 * Bridge uses clones of socket buffer to send to both
+		 * devices!
+		 */
+		if (!skb_cloned(skb) && skb_headroom(skb) >= VLAN_HLEN)
+			nskb = skb;
+		if (!nskb) {
+			nskb = skb_copy_expand(skb, VLAN_HLEN, skb->len,
+					       GFP_ATOMIC);
+			if (!nskb)
+				return skb;
+			dev_kfree_skb_irq(skb);
+			skb = nskb;
+		}
+		eth = (struct ethhdr *) skb->data;
+
+		vid = sw->info->port_cfg[port].vid;
+		vlan = (struct vlan_ethhdr *) skb_push(skb, VLAN_HLEN);
+		memmove(vlan, eth, 12);
+		vlan->h_vlan_TCI = htons(vid);
+		vlan->h_vlan_proto = htons(ETH_P_8021Q);
+	}
+	return skb;
+}  /* sw_ins_vlan */
+
+static struct sk_buff *sw_check_skb(struct ksz_sw *sw, struct sk_buff *skb,
+	struct ksz_port *priv, void *ptr,
+	int (*update_msg)(u8 *data, u32 port, u32 overrides))
+{
+	bool need_new_copy = false;
+	int len;
+	int padlen = 0;
+	uint port;
+	u8 dest;
+	struct sk_buff *org_skb;
+	int update_dst = (sw->overrides & TAIL_TAGGING);
+	int headlen = 0;
+
+	if (!update_dst)
+		return sw_ins_vlan(sw, priv->first_port, skb);
+
+#ifdef CONFIG_KSZ_STP
+	if (skb->protocol == htons(STP_TAG_TYPE))
+		return skb;
+#endif
+
+	if (sw->features & SW_VLAN_DEV)
+		headlen = VLAN_HLEN;
+	org_skb = skb;
+	port = 0;
+
+	/* This device is associated with a switch port. */
+	if (1 == priv->port_cnt)
+		port = priv->first_port;
+
+#if 0
+	do {
+		u16 prio;
+		u16 vid;
+		uint i;
+		uint p;
+		u32 features = sw->features;
+
+		p = get_phy_port(sw, priv->first_port);
+		i = sw->info->port_cfg[p].index;
+		if (sw->features & SW_VLAN_DEV)
+			features = sw->eth_maps[i].proto;
+		if (!(features & VLAN_PORT) || port || vlan_get_tag(skb, &vid))
+			break;
+		prio = vid & VLAN_PRIO_MASK;
+		vid &= VLAN_VID_MASK;
+		if (vid < VLAN_PORT_START)
+			break;
+		vid -= VLAN_PORT_START;
+		if (!vid || vid > SWITCH_PORT_NUM)
+			break;
+		port = vid;
+
+		if (sw->vid || prio) {
+			struct vlan_ethhdr *vlan =
+				(struct vlan_ethhdr *) skb->data;
+			u16 vlan_tci = ntohs(vlan->h_vlan_TCI);
+
+			vlan_tci &= ~VLAN_VID_MASK;
+			vlan_tci |= sw->vid;
+			vlan->h_vlan_TCI = htons(vlan_tci);
+
+		/* Need to remove VLAN tag manually. */
+		} else if (!(sw->overrides & TAG_REMOVE)) {
+			u8 *data;
+
+			len = VLAN_ETH_HLEN - 2;
+			data = &skb->data[len];
+			memmove(data - VLAN_HLEN, data, skb->len - len);
+			skb->len -= VLAN_HLEN;
+		}
+	} while (0);
+#endif
+
+	dest = 0;
+	if (port) {
+		port = get_phy_port(sw, port);
+		dest = 1 << port;
+	}
+
+	/* Check the socket buffer length is enough to hold the tail tag. */
+	if (skb->len < ETH_ZLEN)
+		padlen = ETH_ZLEN - skb->len;
+	len = skb_tailroom(skb);
+	if (len < 1 + padlen) {
+		need_new_copy = true;
+		len = (skb->len + padlen + 4) & ~3;
+	}
+	if (skb_headroom(skb) < headlen) {
+		need_new_copy = true;
+	}
+	if (need_new_copy) {
+		int headerlen = skb_headroom(skb);
+
+		if (headerlen < headlen)
+			headerlen = headlen;
+		skb = skb_copy_expand(org_skb, headerlen, len, GFP_ATOMIC);
+		if (!skb)
+			return NULL;
+		consume_skb(org_skb);
+	}
+
+	/* skb_put requires tail pointer set first. */
+	skb_set_tail_pointer(skb, skb->len);
+	if (padlen) {
+		if (__skb_put_padto(skb, skb->len + padlen, false))
+			return NULL;
+	}
+	len = skb->len;
+
+	if (!dest) {
+		dest = TAIL_TAG_LOOKUP;
+	}
+	if (dest == TAIL_TAG_LOOKUP) {
+		/* Use VLAN for port forwarding if not specified directly. */
+		skb = sw_ins_vlan(sw, priv->first_port, skb);
+		if (len != skb->len)
+			len = skb->len;
+	}
+	skb->data[len] = dest;
+	skb_put(skb, 1);
+
+	/* Need to compensate checksum for some devices. */
+	if (skb->ip_summed != CHECKSUM_PARTIAL)
+		dest = 0;
+	if (dest && (sw->overrides & UPDATE_CSUM)) {
+		__sum16 *csum_loc = (__sum16 *)
+			(skb->head + skb->csum_start + skb->csum_offset);
+
+		/* Checksum is cleared by driver to be filled by hardware. */
+		if (!*csum_loc) {
+			__sum16 new_csum;
+
+			new_csum = dest << 8;
+			*csum_loc = ~htons(new_csum);
+		}
+	}
+	return skb;
+}  /* sw_check_skb */
+
+static struct sk_buff *sw_check_tx(struct ksz_sw *sw, struct net_device *dev,
+	struct sk_buff *skb, struct ksz_port *priv)
+{
+	void *ptr = NULL;
+	int (*update_msg)(u8 *data, u32 port, u32 overrides) = NULL;
+
+	return sw_check_skb(sw, skb, priv, ptr, update_msg);
+}  /* sw_check_tx */
+
+static struct sk_buff *sw_final_skb(struct ksz_sw *sw, struct sk_buff *skb,
+	struct net_device *dev, struct ksz_port *port)
+{
+	skb = sw->net_ops->check_tx(sw, dev, skb, port);
+	if (!skb)
+		return NULL;
+	return skb;
+}  /* sw_final_skb */
+
+static void sw_start(struct ksz_sw *sw, u8 *addr)
+{
+	int need_tail_tag = false;
+	int need_vlan = false;
+
+	sw->ops->acquire(sw);
+	sw_setup(sw);
+	sw_enable(sw);
+	sw_set_addr(sw, addr);
+#if 0
+	if (1 == sw->dev_count)
+		sw_cfg_self_filter(sw, true);
+#endif
+	if (sw->dev_count > 1 && !(sw->features & SW_VLAN_DEV))
+		need_tail_tag = true;
+	if (sw->features & VLAN_PORT) {
+		if (sw->features & VLAN_PORT_REMOVE_TAG) {
+			uint n;
+			uint p;
+
+			for (n = 1; n <= sw->mib_port_cnt; n++) {
+				p = get_phy_port(sw, n);
+				port_cfg_rmv_tag(sw, p, true);
+			}
+			sw->overrides |= TAG_REMOVE;
+		}
+		if (sw->features & VLAN_PORT_TAGGING)
+			need_tail_tag = true;
+	}
+	if (sw->features & SW_VLAN_DEV) {
+		struct ksz_vlan_table entry;
+		struct ksz_dev_map *map;
+		int i;
+		int p;
+		uint port;
+		uint q;
+
+		memset(&entry, 0, sizeof(struct ksz_vlan_table));
+		for (p = 0; p < sw->eth_cnt; p++) {
+
+			/* Not really using VLAN. */
+			if (1 == sw->eth_maps[p].vlan)
+				continue;
+
+			map = &sw->eth_maps[p];
+
+			/*
+			 * Setting FID allows same MAC address in different
+			 * VLANs.
+			 */
+			entry.fid = map->vlan & (FID_ENTRIES - 1);
+			entry.member = sw->HOST_MASK | map->mask;
+			entry.valid = 1;
+			sw->ops->release(sw);
+			sw_w_vlan_table(sw, map->vlan, &entry);
+			sw->ops->acquire(sw);
+			for (i = 0, q = map->first;
+			     i < map->cnt; i++, q++) {
+				port = get_phy_port(sw, q);
+				sw_cfg_def_vid(sw, port, map->vlan);
+				port_cfg_rmv_tag(sw, port, true);
+			}
+		}
+
+		/* Use VLAN tag to determine the network device. */
+		if (!need_tail_tag)
+			port_cfg_ins_tag(sw, sw->HOST_PORT, true);
+		need_vlan = true;
+	}
+	if (sw->features & STP_SUPPORT)
+		need_tail_tag = true;
+	if (need_tail_tag) {
+		sw_cfg_tail_tag(sw, true);
+		sw->overrides |= TAIL_TAGGING;
+	}
+	if (need_vlan) {
+		struct ksz_vlan_table entry;
+
+		/* Default VID 1 is not in VLAN table. */
+		sw->ops->release(sw);
+		entry.fid = 0;
+		entry.member = sw->PORT_MASK;
+		entry.valid = 1;
+		sw_w_vlan_table(sw, 1, &entry);
+		entry.fid = 0;
+		entry.member = sw->PORT_MASK;
+		entry.valid = 1;
+		sw_w_vlan_table(sw, 0, &entry);
+		sw->ops->acquire(sw);
+		sw_ena_vlan(sw);
+	}
+	sw_ena_intr(sw);
+	sw->ops->release(sw);
+
+#ifdef CONFIG_KSZ_STP
+	if (sw->features & STP_SUPPORT) {
+		struct ksz_stp_info *stp = &sw->info->rstp;
+
+		if (stp->br.bridgeEnabled)
+			stp_start(stp);
+	} else
+		stp_set_addr(&sw->info->rstp, sw->info->mac_addr);
+#endif
+}  /* sw_start */
+
+static int sw_stop(struct ksz_sw *sw, int complete)
+{
+	int reset = false;
+
+#ifdef CONFIG_KSZ_STP
+	if (sw->features & STP_SUPPORT) {
+		struct ksz_stp_info *stp = &sw->info->rstp;
+
+		if (stp->br.bridgeEnabled)
+			stp_stop(stp, true);
+	}
+#endif
+
+#ifdef CONFIG_KSZ_MRP
+	if (sw->features & MRP_SUPPORT) {
+		struct mrp_info *mrp = &sw->mrp;
+
+		mrp_stop(mrp);
+	}
+#endif
+
+	sw->ops->acquire(sw);
+	if (!reset)
+		sw_reset(sw);
+	reset = true;
+	sw_init(sw);
+	sw->ops->release(sw);
+
+	/* Clean out static MAC table when the switch shutdown. */
+	if (complete)
+		sw_clr_sta_mac_table(sw);
+	return reset;
+}  /* sw_stop */
+
+static void sw_init_mib(struct ksz_sw *sw)
+{
+	unsigned long interval;
+	uint n;
+	uint p;
+
+	interval = MIB_READ_INTERVAL * 2 / (sw->mib_port_cnt + 1);
+	for (n = 0; n <= sw->mib_port_cnt; n++) {
+		p = get_phy_port(sw, n);
+		sw->port_mib[p].mib_start = 0;
+		if (sw->next_jiffies < jiffies)
+			sw->next_jiffies = jiffies + HZ * 2;
+		else
+			sw->next_jiffies += interval;
+		sw->counter[p].time = sw->next_jiffies;
+		sw->port_state[p].state = media_disconnected;
+		port_init_cnt(sw, p);
+	}
+	sw->port_state[sw->HOST_PORT].state = media_connected;
+}  /* sw_init_mib */
+
+#ifdef CONFIG_PHYLINK
+static void sw_set_phylink_support(struct ksz_sw *sw, struct ksz_port *port,
+				   unsigned long *supported,
+				   struct phylink_link_state *state)
+{
+	__ETHTOOL_DECLARE_LINK_MODE_MASK(mask) = { 0, };
+
+	phylink_set(mask, TP);
+	phylink_set(mask, MII);
+
+	phylink_set(mask, Autoneg);
+
+	phylink_set(mask, 10baseT_Half);
+	phylink_set(mask, 10baseT_Full);
+	phylink_set(mask, 100baseT_Half);
+	phylink_set(mask, 100baseT_Full);
+
+	switch (port->flow_ctrl) {
+	case PHY_NO_FLOW_CTRL:
+		phylink_clear(mask, Pause);
+		phylink_clear(mask, Asym_Pause);
+		break;
+	case PHY_RX_ONLY:
+		/* Depend on link partner to only advertise Asym_Pause. */
+		phylink_set(mask, Pause);
+		phylink_set(mask, Asym_Pause);
+		break;
+	case PHY_TX_ONLY:
+		phylink_set(mask, Asym_Pause);
+		phylink_clear(mask, Pause);
+		break;
+	default:
+		phylink_set(mask, Pause);
+		phylink_clear(mask, Asym_Pause);
+	}
+
+	bitmap_and(supported, supported, mask, __ETHTOOL_LINK_MODE_MASK_NBITS);
+	bitmap_and(state->advertising, state->advertising, mask,
+		   __ETHTOOL_LINK_MODE_MASK_NBITS);
+	linkmode_copy(port->phydev->supported, supported);
+	linkmode_copy(port->phydev->advertising, state->advertising);
+}  /* sw_set_phylink_support */
+
+static void sw_port_phylink_get_fixed_state(struct phylink_config *config,
+					    struct phylink_link_state *s)
+{
+	struct ksz_port *p = container_of(config, struct ksz_port, pl_config);
+	struct ksz_sw *sw = p->sw;
+	struct ksz_port_info *info = get_port_info(sw, sw->HOST_PORT);
+
+	s->interface = sw->interface;
+	s->speed = info->tx_rate / TX_RATE_UNIT;
+	s->duplex = 1;
+	s->pause = 3;
+	s->link = 1;
+	s->an_enabled = 0;
+	s->an_complete = 0;
+}  /* sw_port_phylink_get_fixed_state */
+
+static void sw_port_phylink_validate(struct phylink_config *config,
+				     unsigned long *supported,
+				     struct phylink_link_state *state)
+{
+	struct ksz_port *p = container_of(config, struct ksz_port, pl_config);
+	struct ksz_sw *sw = p->sw;
+
+dbg_msg(" validate: %d\n", state->interface);
+	if ((sw->dev_offset && p->port_cnt > 1) ||
+	    (!sw->dev_offset && !sw->phy_offset)) {
+		if (sw->phylink_ops && sw->phylink_ops->validate)
+			sw->phylink_ops->validate(config, supported, state);
+	} else {
+		sw_set_phylink_support(sw, p, supported, state);
+	}
+}
+
+static int sw_port_phylink_mac_prepare(struct phylink_config *config,
+				       unsigned int mode,
+				       phy_interface_t interface)
+{
+	struct ksz_port *p = container_of(config, struct ksz_port, pl_config);
+	struct ksz_sw *sw = p->sw;
+
+	if (sw->phylink_ops && sw->phylink_ops->mac_prepare)
+		return sw->phylink_ops->mac_prepare(config, mode, interface);
+	return 0;
+}
+
+static void sw_port_phylink_mac_config(struct phylink_config *config,
+				       unsigned int mode,
+				       const struct phylink_link_state *state)
+{
+	/* The switch is always connected to the MAC. */
+}
+
+static void sw_port_phylink_mac_link_down(struct phylink_config *config,
+					  unsigned int mode,
+					  phy_interface_t interface)
+{
+	struct ksz_port *p = container_of(config, struct ksz_port, pl_config);
+	struct ksz_sw *sw = p->sw;
+
+	/* Tell MAC driver to turn off transmit queues. */
+	interface = PHY_INTERFACE_MODE_INTERNAL;
+	if (sw->phylink_ops && sw->phylink_ops->mac_link_down)
+		sw->phylink_ops->mac_link_down(config, mode, interface);
+}
+
+static void sw_port_phylink_mac_link_up(struct phylink_config *config,
+					struct phy_device *phydev,
+					unsigned int mode,
+					phy_interface_t interface,
+					int speed, int duplex,
+					bool tx_pause, bool rx_pause)
+{
+	struct ksz_port *p = container_of(config, struct ksz_port, pl_config);
+	struct ksz_sw *sw = p->sw;
+
+	/* Tell MAC driver to turn on transmit queues. */
+	interface = PHY_INTERFACE_MODE_INTERNAL;
+	if (sw->phylink_ops && sw->phylink_ops->mac_link_up)
+		sw->phylink_ops->mac_link_up(config, phydev, mode,
+					     interface, speed, duplex,
+					     tx_pause, rx_pause);
+}
+
+static const struct phylink_mac_ops sw_port_phylink_mac_ops = {
+	.validate = sw_port_phylink_validate,
+	.mac_prepare = sw_port_phylink_mac_prepare,
+	.mac_config = sw_port_phylink_mac_config,
+	.mac_link_down = sw_port_phylink_mac_link_down,
+	.mac_link_up = sw_port_phylink_mac_link_up,
+};
+
+static int setup_phylink(struct ksz_port *port)
+{
+	struct device_node *dn = port->dn;
+	phy_interface_t mode;
+	int ret;
+
+	ret = of_get_phy_mode(dn, &mode);
+	if (ret)
+		mode = PHY_INTERFACE_MODE_NA;
+
+	port->pl_config.dev = &port->netdev->dev;
+
+	/* netif_carrier_on is called automatically for netdevice. */
+	port->pl_config.type = PHYLINK_NETDEV;
+
+#if 0
+	/* netif_carrier_on is not called for base device. */
+	port->pl_config.type = PHYLINK_DEV;
+#endif
+	port->pl_config.get_fixed_state = sw_port_phylink_get_fixed_state;
+
+	port->pl = phylink_create(&port->pl_config, of_fwnode_handle(dn), mode,
+				  &sw_port_phylink_mac_ops);
+	if (IS_ERR(port->pl)) {
+		netdev_err(port->netdev,
+			   "error creating PHYLINK: %ld\n", PTR_ERR(port->pl));
+		return PTR_ERR(port->pl);
+	}
+
+	return ret;
+}
+
+static void sw_exit_phylink(struct ksz_sw *sw, struct ksz_port *port)
+{
+	const struct phylink_mac_ops *ops = sw->phylink_ops;
+
+	if (ops && port) {
+		unsigned int mode = MLO_AN_PHY;
+		struct phylink_config *config;
+
+		config = &port->pl_config;
+		ops->mac_link_down(config, mode, sw->interface);
+	}
+}  /* sw_exit_phylink */
+
+static void sw_init_phylink(struct ksz_sw *sw, struct ksz_port *port)
+{
+	const struct phylink_mac_ops *ops = sw->phylink_ops;
+
+	if (ops && port) {
+		struct phylink_link_state *state;
+		unsigned int mode = MLO_AN_PHY;
+		struct phylink_config *config;
+		bool rx_pause = true;
+		bool tx_pause = true;
+
+		config = &port->pl_config;
+		state = &port->pl_state;
+		sw_port_phylink_get_fixed_state(config, state);
+		ops->mac_config(config, mode, state);
+		ops->mac_link_up(config, port->phydev, mode,
+				 sw->interface,
+				 state->speed, state->duplex,
+				 tx_pause, rx_pause);
+	}
+}  /* sw_init_phylink */
+#endif
+
+static void setup_device_node(struct ksz_sw *sw)
+{
+	struct sw_priv *ks = sw->dev;
+	struct device_node *np;
+	struct device_node *ports, *port;
+	struct device_node *ethernet;
+	const char *name;
+	int err;
+	u32 reg;
+
+	if (!ks->of_dev)
+		return;
+	np = ks->of_dev->of_node;
+	if (np) {
+		ports = of_get_child_by_name(np, "ports");
+		if (ports) {
+			for_each_available_child_of_node(ports, port) {
+				err = of_property_read_u32(port, "reg", &reg);
+				if (err)
+					break;
+dbg_msg(" reg: %d\n", reg);
+				ethernet = of_parse_phandle(port, "ethernet", 0);
+				if (ethernet)
+dbg_msg(" found eth\n");
+				name = of_get_property(port, "label", NULL);
+				if (name)
+dbg_msg(" name: %s\n", name);
+				/* Save the device node. */
+				sw->devnode[reg] = port;
+			}
+		}
+	}
+}
+
+static int sw_open_dev(struct ksz_sw *sw, struct net_device *dev,
+	struct ksz_port *port, u8 *addr)
+{
+	int mode = 0;
+
+	sw_init_mib(sw);
+
+	sw->main_dev = dev;
+	sw->main_port = port;
+	sw->net_ops->start(sw, addr);
+	if (sw->dev_count > 1)
+		mode |= 1;
+	if (sw->features & DIFF_MAC_ADDR)
+		mode |= 2;
+	return mode;
+}  /* sw_open_dev */
+
+static void sw_open_port(struct ksz_sw *sw, struct net_device *dev,
+	struct ksz_port *port)
+{
+	uint i;
+	uint n;
+	uint p;
+	struct ksz_port_info *info;
+	struct ksz_port_info *host;
+
+	host = get_port_info(sw, sw->HOST_PORT);
+	for (i = 0, n = port->first_port; i < port->port_cnt; i++, n++) {
+		p = get_phy_port(sw, n);
+		info = get_port_info(sw, p);
+		/*
+		 * Initialize to invalid value so that link detection
+		 * is done.
+		 */
+		info->link = 0xFF;
+		info->state = media_unknown;
+		info->tx_rate = host->tx_rate;
+		info->duplex = host->duplex;
+		if (port->port_cnt == 1) {
+			if (sw->netdev[0]) {
+				struct ksz_port *sw_port = sw->netport[0];
+
+				port->speed = sw_port->speed;
+				port->duplex = sw_port->duplex;
+				port->flow_ctrl = sw_port->flow_ctrl;
+			}
+			if (info->own_speed != port->speed ||
+			    info->own_duplex != port->duplex) {
+				if (info->own_speed)
+					port->speed = info->own_speed;
+				if (info->own_duplex)
+					port->duplex = info->own_duplex;
+			}
+		}
+	}
+	port->opened = true;
+	port->report = true;
+
+	sw->ops->acquire(sw);
+
+	/* Need to open the port in multiple device interfaces mode. */
+	if (sw->dev_count > 1 && (!sw->dev_offset || dev != sw->netdev[0])) {
+		port->state = STP_STATE_SIMPLE;
+		if (sw->dev_offset && !(sw->features & STP_SUPPORT)) {
+			port->state = STP_STATE_FORWARDING;
+		}
+		if (sw->features & SW_VLAN_DEV) {
+			p = get_phy_port(sw, port->first_port);
+			i = sw->info->port_cfg[p].index;
+			port->state = STP_STATE_FORWARDING;
+		}
+		for (i = 0, n = port->first_port; i < port->port_cnt;
+		     i++, n++) {
+			p = get_phy_port(sw, n);
+			sw->dev_ports |= (1 << p);
+#ifdef CONFIG_KSZ_STP
+			if (sw->features & STP_SUPPORT) {
+				stp_enable_port(&sw->info->rstp, p,
+						&port->state);
+			}
+#endif
+			port_set_stp_state(sw, p, port->state);
+		}
+	} else if (sw->dev_count == 1) {
+		sw->dev_ports = sw->PORT_MASK;
+	}
+
+	sw->phy_intr = sw->PORT_MASK;
+	if (port->force_link)
+		port_force_link_speed(port);
+	else
+		port_set_link_speed(port);
+	port_get_link_speed(port);
+	if (port->link_ports)
+		schedule_delayed_work(&port->link_update, 0);
+	sw->phy_intr = 0;
+	sw->ops->release(sw);
+}  /* sw_open_port */
+
+static void sw_close_port(struct ksz_sw *sw, struct net_device *dev,
+	struct ksz_port *port)
+{
+	int i;
+	uint p;
+#ifdef CONFIG_KSZ_MRP
+	struct mrp_info *mrp = &sw->mrp;
+#endif
+
+	port->opened = false;
+
+	/* Need to shut the port manually in multiple device interfaces mode. */
+	if (sw->dev_count > 1 && (!sw->dev_offset || dev != sw->netdev[0])) {
+		uint n;
+
+		sw->ops->acquire(sw);
+		for (i = 0, n = port->first_port; i < port->port_cnt;
+		     i++, n++) {
+			p = get_phy_port(sw, n);
+			if (p == sw->HOST_PORT)
+				continue;
+#ifdef CONFIG_KSZ_STP
+			if (sw->features & STP_SUPPORT)
+				stp_disable_port(&sw->info->rstp, p);
+#endif
+#ifdef CONFIG_KSZ_MRP
+			if (sw->features & MRP_SUPPORT) {
+				mrp_close_port(mrp, p);
+			}
+#endif
+			sw->dev_ports &= ~(1 << p);
+			port_set_stp_state(sw, p, STP_STATE_DISABLED);
+		}
+		sw->ops->release(sw);
+	} else if (sw->dev_count == 1) {
+#ifdef CONFIG_KSZ_MRP
+		uint n;
+
+		if (sw->features & MRP_SUPPORT) {
+			for (n = 0; n <= sw->mib_port_cnt; n++) {
+				p = get_phy_port(sw, n);
+				mrp_close_port(mrp, p);
+			}
+		}
+#endif
+		sw->dev_ports = 0;
+	}
+}  /* sw_close_port */
+
+static void sw_open(struct ksz_sw *sw)
+{
+#ifdef CONFIG_PHYLINK
+	sw_init_phylink(sw, sw->main_port);
+#endif
+#ifdef CONFIG_KSZ_MRP
+	if (sw->features & MRP_SUPPORT) {
+		struct ksz_port_info *info = get_port_info(sw, sw->HOST_PORT);
+		u32 speed = info->tx_rate / TX_RATE_UNIT;
+
+		mrp_set_speed(&sw->mrp, sw->HOST_PORT, speed, true);
+		mrp_open(&sw->mrp);
+		schedule_delayed_work(&sw->set_mrp, msecs_to_jiffies(1000));
+	}
+#endif
+	/* Timer may already be started by the SPI device. */
+	if (!sw->monitor_timer_info->max)
+		ksz_start_timer(sw->monitor_timer_info,
+			sw->monitor_timer_info->period);
+}  /* sw_open */
+
+static void sw_close(struct ksz_sw *sw)
+{
+#ifdef CONFIG_PHYLINK
+	sw_exit_phylink(sw, sw->main_port);
+#endif
+#ifdef CONFIG_KSZ_MRP
+	if (sw->features & MRP_SUPPORT)
+		mrp_close(&sw->mrp, true);
+#endif
+	ksz_stop_timer(sw->monitor_timer_info);
+	cancel_delayed_work_sync(sw->link_read);
+}  /* sw_close */
+
+static u8 sw_set_mac_addr(struct ksz_sw *sw, struct net_device *dev,
+	u8 promiscuous, uint port)
+{
+	int n;
+
+	/* See if different MAC addresses are used. */
+	if (sw->dev_count > 1) {
+		int i;
+		int dev_count = sw->dev_count + sw->dev_offset;
+
+		for (i = 0; i < dev_count; i++) {
+			if (dev == sw->netdev[i])
+				continue;
+			if (memcmp(sw->netdev[i]->dev_addr,
+			    dev->dev_addr, ETH_ALEN))
+				break;
+		}
+		if (sw->features & DIFF_MAC_ADDR) {
+			struct ksz_port *priv;
+			struct ksz_port_info *info;
+			uint p;
+
+			/* All addresses are the same. */
+			if (i == dev_count) {
+				sw->features &= ~DIFF_MAC_ADDR;
+				--promiscuous;
+			} else if (sw->dev_offset && dev == sw->netdev[0]) {
+				for (n = 1; n < dev_count; n++) {
+					priv = sw->netport[n];
+					p = get_phy_port(sw, priv->first_port);
+					info = get_port_info(sw, p);
+					inc_mac_addr(info->mac_addr,
+						dev->dev_addr, n);
+					inc_mac_addr(sw->netdev[n]->dev_addr,
+						dev->dev_addr, n);
+				}
+			}
+		} else {
+			if (dev == sw->netdev[0] && i < dev_count) {
+
+				/* Make MAC address the same in all devices. */
+				for (i = 1; i < dev_count; i++) {
+					memcpy(sw->netdev[i]->dev_addr,
+						dev->dev_addr, ETH_ALEN);
+				}
+			} else {
+				if (i < dev_count) {
+					sw->features |= DIFF_MAC_ADDR;
+					++promiscuous;
+				}
+			}
+		}
+	}
+	if (dev == sw->netdev[0]) {
+		sw->ops->acquire(sw);
+		sw_set_addr(sw, dev->dev_addr);
+		sw->ops->release(sw);
+	}
+	for (n = 0; n < sw->eth_cnt; n++) {
+		if (sw->netdev[n] != dev)
+			continue;
+#ifdef CONFIG_KSZ_STP
+		if (sw->features & STP_SUPPORT) {
+			struct ksz_stp_info *stp = &sw->info->rstp;
+
+			stp->ops->change_addr(stp, dev->dev_addr);
+		}
+#endif
+	}
+	return promiscuous;
+}  /* sw_set_mac_addr */
+
+static struct ksz_dev_major sw_majors[MAX_SW_DEVICES];
+
+static struct file_dev_info *alloc_sw_dev_info(struct ksz_sw *sw, uint minor)
+{
+	struct file_dev_info *info;
+
+	info = kzalloc(sizeof(struct file_dev_info), GFP_KERNEL);
+	if (info) {
+		info->dev = sw;
+		sema_init(&info->sem, 1);
+		mutex_init(&info->lock);
+		init_waitqueue_head(&info->wait_msg);
+		info->read_max = 60000;
+		info->read_tmp = MAX_SW_LEN;
+		info->read_buf = kzalloc(info->read_max + info->read_tmp,
+			GFP_KERNEL);
+		info->read_in = &info->read_buf[info->read_max];
+		info->write_len = 1000;
+		info->write_buf = kzalloc(info->write_len, GFP_KERNEL);
+
+		info->minor = minor;
+		info->next = sw->dev_list[minor];
+		sw->dev_list[minor] = info;
+	}
+	return info;
+}  /* alloc_sw_dev_info */
+
+static void free_sw_dev_info(struct file_dev_info *info)
+{
+	if (info) {
+		struct ksz_sw *sw = info->dev;
+		uint minor = info->minor;
+
+		file_dev_clear_notify(sw->dev_list[minor], info, DEV_MOD_BASE,
+				      &sw->notifications);
+		file_gen_dev_release(info, &sw->dev_list[minor]);
+	}
+}  /* free_sw_dev_info */
+
+static int sw_dev_open(struct inode *inode, struct file *filp)
+{
+	struct file_dev_info *info = (struct file_dev_info *)
+		filp->private_data;
+	uint minor = MINOR(inode->i_rdev);
+	uint major = MAJOR(inode->i_rdev);
+	struct ksz_sw *sw = NULL;
+	int i;
+
+	if (minor > 1)
+		return -ENODEV;
+	for (i = 0; i < MAX_SW_DEVICES; i++) {
+		if (sw_majors[i].major == major) {
+			sw = sw_majors[i].dev;
+			break;
+		}
+	}
+	if (!sw)
+		return -ENODEV;
+	if (!info) {
+		info = alloc_sw_dev_info(sw, minor);
+		if (info)
+			filp->private_data = info;
+		else
+			return -ENOMEM;
+	}
+	return 0;
+}  /* sw_dev_open */
+
+static int sw_dev_release(struct inode *inode, struct file *filp)
+{
+	struct file_dev_info *info = (struct file_dev_info *)
+		filp->private_data;
+
+	free_sw_dev_info(info);
+	filp->private_data = NULL;
+	return 0;
+}  /* sw_dev_release */
+
+static int sw_get_attrib(struct ksz_sw *sw, int subcmd, int size,
+	int *req_size, size_t *len, u8 *data, int *output)
+{
+	struct ksz_info_opt *opt = (struct ksz_info_opt *) data;
+	struct ksz_info_cfg *cfg = &opt->data.cfg;
+	uint i;
+	uint n;
+	uint p;
+
+	*len = 0;
+	*output = 0;
+	switch (subcmd) {
+	case DEV_SW_CFG:
+		n = opt->num;
+		p = opt->port;
+		if (!n)
+			n = 1;
+		*len = 2 + n * sizeof(struct ksz_info_cfg);
+		if (n > sw->mib_port_cnt ||
+		    p > sw->mib_port_cnt)
+			return DEV_IOC_INVALID_CMD;
+		break;
+	}
+	if (!*len)
+		return DEV_IOC_INVALID_CMD;
+	if (size < *len) {
+		*req_size = *len + SIZEOF_ksz_request;
+		return DEV_IOC_INVALID_LEN;
+	}
+	switch (subcmd) {
+	case DEV_SW_CFG:
+		n = opt->port;
+		sw->ops->acquire(sw);
+		for (i = 0; i < opt->num; i++, n++) {
+			p = get_phy_port(sw, n);
+			cfg->on_off = 0;
+			if (cfg->set & SP_LEARN) {
+				if (!port_chk_dis_learn(sw, p))
+					cfg->on_off |= SP_LEARN;
+			}
+			if (cfg->set & SP_RX) {
+				if (port_chk_rx(sw, p))
+					cfg->on_off |= SP_RX;
+			}
+			if (cfg->set & SP_TX) {
+				if (port_chk_tx(sw, p))
+					cfg->on_off |= SP_TX;
+			}
+			if (p == sw->HOST_PORT)
+				continue;
+#if 0
+			if (cfg->set & SP_PHY_POWER) {
+				if (port_chk_power(sw, p))
+					cfg->on_off |= SP_PHY_POWER;
+			}
+#endif
+			cfg++;
+		}
+		sw->ops->release(sw);
+		break;
+	}
+	return DEV_IOC_OK;
+}  /* sw_get_attrib */
+
+static int sw_set_attrib(struct ksz_sw *sw, int subcmd, int size,
+	int *req_size, u8 *data, int *output)
+{
+	struct ksz_info_opt *opt = (struct ksz_info_opt *) data;
+	struct ksz_info_cfg *cfg = &opt->data.cfg;
+	int len;
+	uint i;
+	uint n;
+	uint p;
+
+	*output = 0;
+	switch (subcmd) {
+	case DEV_SW_CFG:
+		n = opt->num;
+		p = opt->port;
+		if (!n)
+			n = 1;
+		len = 2 + n * sizeof(struct ksz_info_cfg);
+		if (size < len)
+			goto not_enough;
+		if (n > sw->mib_port_cnt ||
+		    p > sw->mib_port_cnt)
+			return DEV_IOC_INVALID_CMD;
+		n = opt->port;
+		sw->ops->acquire(sw);
+		for (i = 0; i < opt->num; i++, n++) {
+			p = get_phy_port(sw, n);
+			if (cfg->set & SP_LEARN)
+				port_cfg_dis_learn(sw, p,
+					!(cfg->on_off & SP_LEARN));
+			if (cfg->set & SP_RX)
+				port_cfg_rx(sw, p,
+					!!(cfg->on_off & SP_RX));
+			if (cfg->set & SP_TX)
+				port_cfg_tx(sw, p,
+					!!(cfg->on_off & SP_TX));
+			if (p == sw->HOST_PORT)
+				continue;
+#if 0
+			if (cfg->set & SP_PHY_POWER)
+				port_cfg_power(sw, p,
+					!!(cfg->on_off & SP_PHY_POWER));
+#endif
+			cfg++;
+		}
+		sw->ops->release(sw);
+		break;
+	default:
+		return DEV_IOC_INVALID_CMD;
+	}
+	return DEV_IOC_OK;
+
+not_enough:
+	*req_size = len + SIZEOF_ksz_request;
+	return DEV_IOC_INVALID_LEN;
+}  /* sw_set_attrib */
+
+static int sw_get_info(struct ksz_sw *sw, int subcmd, int size,
+	int *req_size, size_t *len, u8 *data)
+{
+	struct ksz_info_opt *opt = (struct ksz_info_opt *) data;
+	struct ksz_info_speed *speed = &opt->data.speed;
+	struct ksz_port_info *info;
+	uint i;
+	uint n;
+	uint p;
+
+	*len = 0;
+	switch (subcmd) {
+	case DEV_INFO_SW_LINK:
+		n = opt->num;
+		p = opt->port;
+		if (!n)
+			n = 1;
+		*len = 2 + n * sizeof(struct ksz_info_speed);
+		if (n > sw->mib_port_cnt ||
+		    p > sw->mib_port_cnt)
+			return DEV_IOC_INVALID_CMD;
+		break;
+	}
+	if (!*len)
+		return DEV_IOC_INVALID_CMD;
+	if (size < *len) {
+		*req_size = *len + SIZEOF_ksz_request;
+		return DEV_IOC_INVALID_LEN;
+	}
+	switch (subcmd) {
+	case DEV_INFO_SW_LINK:
+		n = p;
+		for (i = 0; i < opt->num; i++, n++) {
+			p = get_phy_port(sw, n);
+			info = get_port_info(sw, p);
+			if (info->state == media_connected) {
+				speed->tx_rate = info->tx_rate;
+				speed->duplex = info->duplex;
+				speed->flow_ctrl = info->flow_ctrl;
+			} else {
+				memset(speed, 0, sizeof(struct ksz_info_speed));
+			}
+			++speed;
+		}
+		break;
+	}
+	return DEV_IOC_OK;
+}  /* sw_get_info */
+
+static int base_dev_req(struct ksz_sw *sw, char *arg, void *info)
+{
+	struct ksz_request *req = (struct ksz_request *) arg;
+	int len;
+	int maincmd;
+	int req_size;
+	int subcmd;
+	int output;
+	size_t param_size;
+	u8 data[PARAM_DATA_SIZE];
+	struct ksz_resp_msg *msg = (struct ksz_resp_msg *) data;
+	int err = 0;
+	int result = 0;
+
+	get_user_data(&req_size, &req->size, info);
+	get_user_data(&maincmd, &req->cmd, info);
+	get_user_data(&subcmd, &req->subcmd, info);
+	get_user_data(&output, &req->output, info);
+	len = req_size - SIZEOF_ksz_request;
+
+	maincmd &= 0xffff;
+	switch (maincmd) {
+	case DEV_CMD_INFO:
+		switch (subcmd) {
+		case DEV_INFO_INIT:
+			req_size = SIZEOF_ksz_request + 4;
+			if (len >= 4) {
+				data[0] = 'M';
+				data[1] = 'i';
+				data[2] = 'c';
+				data[3] = 'r';
+				data[4] = 1;
+				data[5] = sw->mib_port_cnt;
+				err = write_user_data(data, req->param.data,
+					6, info);
+				if (err)
+					goto dev_ioctl_done;
+			} else
+				result = DEV_IOC_INVALID_LEN;
+			break;
+		case DEV_INFO_EXIT:
+			fallthrough;
+
+		case DEV_INFO_QUIT:
+
+			/* Not called through char device. */
+			if (!info)
+				break;
+			file_dev_clear_notify(sw->dev_list[0], info,
+					      DEV_MOD_BASE,
+					      &sw->notifications);
+			msg->module = DEV_MOD_BASE;
+			msg->cmd = DEV_INFO_QUIT;
+			msg->resp.data[0] = 0;
+			file_dev_setup_msg(info, msg, 8, NULL, NULL);
+			break;
+		case DEV_INFO_NOTIFY:
+			if (len >= 4) {
+				struct file_dev_info *dev_info = info;
+				uint *notify = (uint *) data;
+
+				_chk_ioctl_size(len, 4, 0, &req_size, &result,
+					&req->param, data, info);
+				dev_info->notifications[DEV_MOD_BASE] =
+					*notify;
+				sw->notifications |= *notify;
+			}
+			break;
+		case DEV_INFO_SW_LINK:
+			if (_chk_ioctl_size(len, len, 0, &req_size, &result,
+			    &req->param, data, info))
+				goto dev_ioctl_resp;
+			result = sw_get_info(sw, subcmd, len, &req_size,
+					     &param_size, data);
+			if (result)
+				goto dev_ioctl_resp;
+			err = write_user_data(data, req->param.data,
+					      param_size, info);
+			if (err)
+				goto dev_ioctl_done;
+			req_size = param_size + SIZEOF_ksz_request;
+			break;
+		default:
+			result = DEV_IOC_INVALID_CMD;
+			break;
+		}
+		break;
+	case DEV_CMD_PUT:
+		if (_chk_ioctl_size(len, len, 0, &req_size, &result,
+		    &req->param, data, info))
+			goto dev_ioctl_resp;
+		result = sw_set_attrib(sw, subcmd, len, &req_size, data,
+			&output);
+		if (result)
+			goto dev_ioctl_resp;
+		put_user_data(&output, &req->output, info);
+		break;
+	case DEV_CMD_GET:
+		if (_chk_ioctl_size(len, len, 0, &req_size, &result,
+		    &req->param, data, info))
+			goto dev_ioctl_resp;
+		result = sw_get_attrib(sw, subcmd, len, &req_size,
+			&param_size, data, &output);
+		if (result)
+			goto dev_ioctl_resp;
+		err = write_user_data(data, req->param.data, param_size, info);
+		if (err)
+			goto dev_ioctl_done;
+		req_size = param_size + SIZEOF_ksz_request;
+		put_user_data(&output, &req->output, info);
+		break;
+	default:
+		result = DEV_IOC_INVALID_CMD;
+		break;
+	}
+
+dev_ioctl_resp:
+	put_user_data(&req_size, &req->size, info);
+	put_user_data(&result, &req->result, info);
+
+	/* Return ERESTARTSYS so that the system call is called again. */
+	if (result < 0)
+		err = result;
+
+dev_ioctl_done:
+	return err;
+}  /* base_dev_req */
+
+static int sw_dev_req(struct ksz_sw *sw, char *arg,
+	struct file_dev_info *info)
+{
+	struct ksz_request *req = (struct ksz_request *) arg;
+	int maincmd;
+	int req_size;
+	int err = 0;
+	int result = DEV_IOC_OK;
+
+	/* Check request size. */
+	get_user_data(&req_size, &req->size, info);
+	if (chk_ioctl_size(req_size, SIZEOF_ksz_request, 0, &req_size,
+	    &result, NULL, NULL))
+		goto dev_ioctl_resp;
+
+	result = -EOPNOTSUPP;
+	get_user_data(&maincmd, &req->cmd, info);
+	maincmd >>= 16;
+	switch (maincmd) {
+	case DEV_MOD_BASE:
+		err = base_dev_req(sw, arg, info);
+		result = 0;
+		break;
+	default:
+		break;
+	}
+
+	/* Processed by specific module. */
+	if (!result)
+		return err;
+	if (result < 0)
+		goto dev_ioctl_done;
+
+dev_ioctl_resp:
+	put_user_data(&req_size, &req->size, info);
+	put_user_data(&result, &req->result, info);
+
+dev_ioctl_done:
+
+	/* Return ERESTARTSYS so that the system call is called again. */
+	if (result < 0)
+		err = result;
+
+	return err;
+}  /* sw_dev_req */
+
+static ssize_t sw_dev_read(struct file *filp, char *buf, size_t count,
+	loff_t *offp)
+{
+	struct file_dev_info *info = (struct file_dev_info *)
+		filp->private_data;
+	ssize_t result = 0;
+	int rc;
+
+	if (!info->read_len) {
+		*offp = 0;
+		rc = wait_event_interruptible(info->wait_msg,
+			0 != info->read_len);
+
+		/* Cannot continue if ERESTARTSYS. */
+		if (rc < 0)
+			return 0;
+	}
+
+	if (down_interruptible(&info->sem))
+		return -ERESTARTSYS;
+
+	mutex_lock(&info->lock);
+	if (*offp >= info->read_len) {
+		info->read_len = 0;
+		count = 0;
+		*offp = 0;
+		goto dev_read_done;
+	}
+
+	if (*offp + count > info->read_len) {
+		count = info->read_len - *offp;
+		info->read_len = 0;
+	}
+
+	if (copy_to_user(buf, &info->read_buf[*offp], count)) {
+		result = -EFAULT;
+		goto dev_read_done;
+	}
+	if (info->read_len)
+		*offp += count;
+	else
+		*offp = 0;
+	result = count;
+
+dev_read_done:
+	mutex_unlock(&info->lock);
+	up(&info->sem);
+	return result;
+}  /* sw_dev_read */
+
+static long sw_dev_ioctl(struct file *filp, unsigned int cmd,
+	unsigned long arg)
+{
+	struct file_dev_info *info = (struct file_dev_info *)
+		filp->private_data;
+	struct ksz_sw *sw = info->dev;
+	int err = 0;
+
+	if (_IOC_TYPE(cmd) != DEV_IOC_MAGIC)
+		return -ENOTTY;
+	if (_IOC_NR(cmd) > DEV_IOC_MAX)
+		return -ENOTTY;
+	if (_IOC_DIR(cmd) & _IOC_READ)
+		err = !access_ok((void *) arg, _IOC_SIZE(cmd));
+	else if (_IOC_DIR(cmd) & _IOC_WRITE)
+		err = !access_ok((void *) arg, _IOC_SIZE(cmd));
+	if (err) {
+		printk(KERN_ALERT "err fault\n");
+		return -EFAULT;
+	}
+	if (down_interruptible(&info->sem))
+		return -ERESTARTSYS;
+
+	err = sw_dev_req(sw, (char *) arg, info);
+	up(&info->sem);
+	return err;
+}  /* sw_dev_ioctl */
+
+static ssize_t sw_dev_write(struct file *filp, const char *buf, size_t count,
+	loff_t *offp)
+{
+	struct file_dev_info *info = (struct file_dev_info *)
+		filp->private_data;
+	ssize_t result = 0;
+	size_t size;
+	int rc;
+
+	if (!count)
+		return result;
+
+	if (down_interruptible(&info->sem))
+		return -ERESTARTSYS;
+
+	if (*offp >= info->write_len) {
+		result = -ENOSPC;
+		goto dev_write_done;
+	}
+	if (*offp + count > info->write_len)
+		count = info->write_len - *offp;
+	if (copy_from_user(info->write_buf, buf, count)) {
+		result = -EFAULT;
+		goto dev_write_done;
+	}
+	size = 0;
+	result = size;
+	rc = 0;
+	if (rc)
+		result = rc;
+
+dev_write_done:
+	up(&info->sem);
+	return result;
+}  /* sw_dev_write */
+
+static const struct file_operations sw_dev_fops = {
+	.read		= sw_dev_read,
+	.write		= sw_dev_write,
+	.unlocked_ioctl	= sw_dev_ioctl,
+	.open		= sw_dev_open,
+	.release	= sw_dev_release,
+};
+
+static struct class *sw_class[MAX_SW_DEVICES];
+
+static int init_sw_dev(int id, int dev_major, char *dev_name)
+{
+	int result;
+
+	result = register_chrdev(dev_major, dev_name, &sw_dev_fops);
+	if (result < 0) {
+		printk(KERN_WARNING "%s: can't get major %d\n", dev_name,
+			dev_major);
+		return result;
+	}
+	if (0 == dev_major)
+		dev_major = result;
+	sw_class[id] = class_create(THIS_MODULE, dev_name);
+	if (IS_ERR(sw_class[id])) {
+		unregister_chrdev(dev_major, dev_name);
+		return -ENODEV;
+	}
+	device_create(sw_class[id], NULL, MKDEV(dev_major, 0), NULL, dev_name);
+	return dev_major;
+}  /* init_sw_dev */
+
+static void exit_sw_dev(int id, int dev_major, char *dev_name)
+{
+	device_destroy(sw_class[id], MKDEV(dev_major, 0));
+	class_destroy(sw_class[id]);
+	unregister_chrdev(dev_major, dev_name);
+}  /* exit_sw_dev */
+
+static void sw_init_dev(struct ksz_sw *sw)
+{
+	sprintf(sw->dev_name, "sw_dev");
+	if (sw->id)
+		sprintf(sw->dev_name, "sw_dev_%u", sw->id);
+	sw->dev_major = init_sw_dev(sw->id, 0, sw->dev_name);
+	sw->msg_buf = kzalloc(MAX_SW_LEN, GFP_KERNEL);
+	sw_majors[sw->id].dev = sw;
+	sw_majors[sw->id].major = sw->dev_major;
+}  /* sw_init_dev */
+
+static void sw_exit_dev(struct ksz_sw *sw)
+{
+	kfree(sw->msg_buf);
+	if (sw->dev_major >= 0)
+		exit_sw_dev(sw->id, sw->dev_major, sw->dev_name);
+}  /* sw_exit_dev */
+
+static void sw_report_link(struct ksz_sw *sw, struct ksz_port *port,
+			   struct ksz_port_info *info)
+{
+	struct ksz_port_info *linked = port->linked;
+	struct phy_device *phydev = port->phydev;
+	struct net_device *dev = port->netdev;
+	int lpa = info->lpa;
+	int phy_link = 0;
+	int link;
+
+	phydev->link = (info->state == media_connected);
+	phydev->speed = info->tx_rate / TX_RATE_UNIT;
+	if (!phydev->speed)
+		phydev->speed = 10;
+	phydev->duplex = (info->duplex == 2);
+
+#ifdef CONFIG_PHYLINK
+	/* Not started yet. */
+	if (sw->phylink_ops && port != sw->main_port &&
+	    phydev->state == PHY_READY)
+		return;
+#endif
+	if (phydev->link)
+		phy_link = (linked->state == media_connected);
+	link = netif_carrier_ok(dev);
+	if (port->report) {
+		port->report = false;
+		link = !phy_link;
+	}
+	if (phy_link == link)
+		return;
+
+	/* Update link partner capabilities. */
+	if (lpa) {
+		mii_lpa_mod_linkmode_lpa_t(phydev->lp_advertising, lpa);
+#if 0
+dbg_msg(" %*pb\n", __ETHTOOL_LINK_MODE_MASK_NBITS, phydev->advertising);
+dbg_msg(" %*pb\n", __ETHTOOL_LINK_MODE_MASK_NBITS, phydev->lp_advertising);
+#endif
+	}
+	if (netif_msg_link(sw))
+		pr_info("%s link %s"NL,
+			dev->name,
+			phy_link ? "on" : "off");
+	if (phydev->phy_link_change) {
+		phydev->phy_link_change(phydev, phy_link);
+	} else if (phy_link != link) {
+		if (phy_link)
+			netif_carrier_on(dev);
+		else
+			netif_carrier_off(dev);
+	}
+}  /* sw_report_link */
+
+static void link_update_work(struct work_struct *work)
+{
+	struct delayed_work *dwork = to_delayed_work(work);
+	struct ksz_port *port =
+		container_of(dwork, struct ksz_port, link_update);
+	struct ksz_sw *sw = port->sw;
+	struct ksz_port_info *info;
+
+	/* Netdevice associated with port was closed. */
+	if (!port->opened)
+		goto do_main;
+
+	if (sw->dev_offset && sw->netport[0]) {
+		int dev_cnt = sw->dev_count + sw->dev_offset;
+		struct ksz_port *sw_port = sw->netport[0];
+		struct ksz_port *dev_port;
+		int i;
+
+		for (i = sw->dev_offset; i < dev_cnt; i++) {
+			struct phy_priv *phydata;
+
+			dev_port = sw->netport[i];
+			if (!dev_port) {
+				phydata = &sw->phydata[i];
+				dev_port = phydata->port;
+			}
+			if (media_connected == dev_port->linked->state) {
+				sw_port->linked = dev_port->linked;
+				break;
+			}
+		}
+	}
+
+	sw_notify_link_change(sw, port->link_ports);
+
+	if ((!sw->dev_offset || port != sw->netport[0]) && port->netdev)
+		sw_report_link(sw, port, port->linked);
+
+#ifdef CONFIG_KSZ_STP
+	if (sw->features & STP_SUPPORT) {
+		struct ksz_stp_info *stp = &sw->info->rstp;
+
+		stp->ops->link_change(stp, true);
+	}
+#endif
+
+do_main:
+	port->link_ports = 0;
+
+	/* There is an extra network device for the main device. */
+	/* The switch is always linked; speed and duplex are also fixed. */
+	if (sw->dev_offset) {
+		port = sw->netport[0];
+		if (port && port->opened && netif_running(port->netdev)) {
+			info = get_port_info(sw, sw->HOST_PORT);
+			sw_report_link(sw, port, info);
+		}
+	}
+}  /* link_update_work */
+
+static void set_phy_support(struct ksz_port *port, struct phy_device *phydev)
+{
+	switch (port->flow_ctrl) {
+	case PHY_NO_FLOW_CTRL:
+		linkmode_clear_bit(ETHTOOL_LINK_MODE_Pause_BIT,
+				   phydev->supported);
+		linkmode_clear_bit(ETHTOOL_LINK_MODE_Asym_Pause_BIT,
+				   phydev->supported);
+		break;
+	case PHY_RX_ONLY:
+		/* Depend on link partner to only advertise Asym_Pause. */
+		linkmode_set_bit(ETHTOOL_LINK_MODE_Pause_BIT,
+				 phydev->supported);
+		linkmode_set_bit(ETHTOOL_LINK_MODE_Asym_Pause_BIT,
+				 phydev->supported);
+		break;
+	case PHY_TX_ONLY:
+		linkmode_clear_bit(ETHTOOL_LINK_MODE_Pause_BIT,
+				   phydev->supported);
+		linkmode_set_bit(ETHTOOL_LINK_MODE_Asym_Pause_BIT,
+				 phydev->supported);
+		break;
+	default:
+		linkmode_set_bit(ETHTOOL_LINK_MODE_Pause_BIT,
+				 phydev->supported);
+		linkmode_clear_bit(ETHTOOL_LINK_MODE_Asym_Pause_BIT,
+				   phydev->supported);
+		break;
+	}
+}  /* set_phy_support */
+
+/*
+ * This enables multiple network device mode for the switch, which contains at
+ * least two physical ports.  Some users like to take control of the ports for
+ * running Spanning Tree Protocol.  The driver will create an additional eth?
+ * device for each port depending on the mode.
+ *
+ * Some limitations are the network devices cannot have different MTU and
+ * multicast hash tables.
+ */
+static int multi_dev = -1;
+
+static int stp = -1;
+
+/*
+ * This enables fast aging in the switch.  Not sure what situation requires
+ * that.  However, fast aging is used to flush the dynamic MAC table when STP
+ * support is enabled.
+ */
+static int fast_aging;
+
+static int eth1_ports;
+static int eth2_ports;
+static int eth3_ports;
+static int eth4_ports;
+
+static int eth1_vlan;
+static int eth2_vlan;
+static int eth3_vlan;
+static int eth4_vlan;
+
+static char *eth1_proto = " ";
+static char *eth2_proto = " ";
+static char *eth3_proto = " ";
+static char *eth4_proto = " ";
+
+static int *eth_ports[] = {
+	&eth1_ports,
+	&eth2_ports,
+	&eth3_ports,
+	&eth4_ports,
+	NULL
+};
+
+static int *eth_vlans[] = {
+	&eth1_vlan,
+	&eth2_vlan,
+	&eth3_vlan,
+	&eth4_vlan,
+	NULL
+};
+
+static char **eth_proto[] = {
+	&eth1_proto,
+	&eth2_proto,
+	&eth3_proto,
+	&eth4_proto,
+	NULL
+};
+
+struct ksz_port_mapping {
+	u8 id;
+	u8 cnt;
+	u8 fiber;
+	u8 map[8];
+};
+
+enum {
+	KSZ8895_SKU,
+	KSZ8864_SKU,
+};
+
+static struct ksz_port_mapping port_mappings[] = {
+	{ KSZ8895_SKU,   5, 0, { 1, 2, 3, 4, 5, 0, 0, 0 }},
+	{ KSZ8864_SKU,   5, 0, { 2, 3, 4, 5, 0, 0, 0, 0 }},
+};
+
+static u8 port_map[8];
+
+static void ksz_setup_logical_ports(struct ksz_sw *sw, u8 id, uint ports)
+{
+	struct ksz_port_mapping *map;
+	struct ksz_port_info *info;
+	struct ksz_port_info *pinfo;
+	uint i;
+	uint l;
+	uint n;
+	uint p;
+	uint cnt = 0;
+
+	for (i = 0; i < ARRAY_SIZE(port_mappings); i++) {
+		map = &port_mappings[i];
+		if (id == map->id) {
+			cnt = map->cnt;
+			memset(port_map, 0, 8 * sizeof(u8));
+			memcpy(port_map, map->map, cnt * sizeof(u8));
+			for (n = 0; n < sw->port_cnt; n++) {
+				if (n == sw->HOST_PORT)
+					break;
+				info = &sw->port_info[n];
+				info->phy = 1;
+				info->fiber = map->fiber;
+			}
+			break;
+		}
+	}
+	if (!cnt)
+		return;
+
+	for (n = 0; n < cnt; n++) {
+		p = map->map[n];
+		if (!p)
+			break;
+		--p;
+		info = &sw->port_info[p];
+		if (p == sw->HOST_PORT) {
+			l = 0;
+		} else {
+			l = p + 1;
+			if (p > sw->HOST_PORT)
+				--l;
+		}
+		info->log_p = l;
+		info->log_m = 0;
+	}
+	n = (1 << cnt) - 1;
+	ports &= n;
+	for (i = 0, n = 0; n <= sw->port_cnt; n++) {
+		if (n > 0) {
+			p = n - 1;
+			if (!(ports & BIT(p)))
+				continue;
+			p = map->map[p] - 1;
+			if (p == sw->HOST_PORT)
+				continue;
+			l = i;
+			++i;
+		} else {
+			p = sw->HOST_PORT;
+			l = 0;
+		}
+		info = &sw->port_info[i];
+		info->phy_p = p;
+		info->phy_m = BIT(p);
+		info = &sw->port_info[p];
+		info->log_p = i;
+		info->log_m = BIT(l);
+		info->phy_id = p + 1;
+	}
+	info = &sw->port_info[sw->HOST_PORT];
+	info->log_m = BIT(i);
+
+	ports = 0;
+	for (n = 0; n <= i; n++) {
+		info = &sw->port_info[n];
+		ports |= info->phy_m;
+	}
+dbg_msg("ports: %d %x\n", i, ports);
+
+	for (n = 0; n <= i; n++) {
+		info = &sw->port_info[n];
+		pinfo = &sw->port_info[info->phy_p];
+dbg_msg("%d= %d:%02x %d:%02x %d:%02x %d\n", n,
+info->phy_p, info->phy_m, info->log_p, info->log_m,
+pinfo->log_p, pinfo->log_m, pinfo->fiber);
+	}
+#if 1
+	if (i + 1 < sw->port_cnt) {
+		for (n = 0; n < sw->port_cnt; n++) {
+			info = &sw->port_info[n];
+			if (!info->log_m)
+dbg_msg(" %d= %d %d\n", n, info->log_p, info->fiber);
+			if (!info->log_m)
+				info->log_p = sw->port_cnt;
+		}
+	}
+#endif
+	sw->PORT_MASK = ports;
+dbg_msg("mask: %x %x\n", sw->HOST_MASK, sw->PORT_MASK);
+	sw->mib_port_cnt = i;
+	if (sw->mib_port_cnt + 1 < sw->port_cnt)
+		sw->features |= USE_FEWER_PORTS;
+	for (i = 0; i < sw->eth_cnt; i++) {
+dbg_msg(" eth_maps: %d=%x\n", i, sw->eth_maps[i].mask);
+		sw->eth_maps[i].mask = 0;
+		for (l = 0, n = sw->eth_maps[i].first;
+		     l < sw->eth_maps[i].cnt; l++, n++) {
+			p = get_phy_port(sw, n);
+			sw->eth_maps[i].mask |= BIT(p);
+		}
+dbg_msg("   eth_maps: %d=%x\n", i, sw->eth_maps[i].mask);
+	}
+}  /* ksz_setup_logical_ports */
+
+static uint sw_setup_zone(struct ksz_sw *sw, uint in_ports)
+{
+	int c;
+	int f;
+	int limit;
+	int m;
+	uint p;
+	uint q;
+	int w;
+	int *v;
+	char **s;
+	uint features;
+	struct ksz_dev_map *map;
+	uint ports;
+	uint used = 0;
+	int last_log_port = 0;
+	int last_phy_port = 0;
+	int last_vlan = 0;
+	uint left = (1 << sw->port_cnt) - 1;
+
+	if (multi_dev > 2) {
+		ports = in_ports;
+		goto setup_next;
+	}
+	q = 0;
+	ports = 0;
+	for (p = 0; p < sw->port_cnt - 1; p++) {
+		v = eth_ports[p];
+
+		/* No more port setting. */
+		if (!v || !*v)
+			break;
+		m = *v;
+		if (!(m & left)) {
+			left = 0;
+			break;
+		}
+
+		/* Find out how the ports are to be used. */
+		limit = 0;
+		w = last_vlan;
+		features = 0;
+		s = eth_proto[p];
+#ifdef CONFIG_KSZ_STP
+		if (!strcmp(*s, "stp")) {
+			features = STP_SUPPORT;
+		}
+#endif
+
+		m &= ~((1 << last_phy_port) - 1);
+		m &= left;
+
+		/* No more legimate port. */
+		if (!m)
+			break;
+
+		v = eth_vlans[p];
+		if (!w && (!v || !*v))
+			break;
+		if (*v)
+			w = *v;
+
+		/* Check VLAN id is unused. */
+		for (q = 0; q < p; q++) {
+			if (w > 1 && w == sw->eth_maps[q].vlan)
+				w = last_vlan + 1;
+		}
+		c = 0;
+		f = -1;
+		for (q = p; q < sw->port_cnt - 1; q++) {
+			if (m & (1 << q)) {
+				if (f < 0)
+					f = last_log_port;
+				++c;
+				++last_log_port;
+
+				/* Limit to certain ports. */
+				if (limit && c >= limit) {
+					if (!(used & features)) {
+						used |= features;
+						++q;
+						last_phy_port = q;
+						break;
+					}
+					features = 0;
+				}
+				last_phy_port = q + 1;
+			} else if (f >= 0) {
+				if (limit && c < limit)
+					features = 0;
+				break;
+			}
+		}
+		if (!c)
+			continue;
+		m &= (1 << q) - 1;
+		if (!p && c > 1)
+			used |= (features & VLAN_PORT);
+#ifdef CONFIG_KSZ_STP
+		if ((features & STP_SUPPORT) && c > 1) {
+			used |= (features & STP_SUPPORT);
+			stp = m;
+		}
+#endif
+		++f;
+		map = &sw->eth_maps[p];
+		map->cnt = c;
+		map->mask = m;
+		map->first = f;
+		map->phy_id = f;
+		map->vlan = w & (4096 - 1);
+		map->proto = features;
+		if (last_vlan < w)
+			last_vlan = w;
+		ports |= m;
+	}
+
+	/* No VLAN devices specified. */
+	if (!p) {
+		ports = in_ports;
+		goto setup_next;
+	}
+
+	/* Not all ports are used. */
+	left &= ~((1 << last_phy_port) - 1);
+	if (multi_dev != 1)
+		left = 0;
+	features = 0;
+	s = eth_proto[p];
+#ifdef CONFIG_KSZ_STP
+	if (s && !strcmp(*s, "stp"))
+		features = STP_SUPPORT;
+#endif
+	if (left) {
+		m = left;
+		c = 0;
+		f = -1;
+		for (q = 0; q < sw->mib_port_cnt - 1; q++) {
+			if (m & (1 << q)) {
+				if (f < 0)
+					f = last_log_port;
+				++c;
+			}
+		}
+		m &= (1 << q) - 1;
+		if ((features & STP_SUPPORT) && c > 1) {
+			used |= (features & STP_SUPPORT);
+			stp = m;
+		}
+		++f;
+		map = &sw->eth_maps[p];
+		map->cnt = c;
+		map->mask = m;
+		map->first = f;
+		map->phy_id = f;
+		map->vlan = ++last_vlan & (4096 - 1);
+		map->proto = features;
+		ports |= m;
+		p++;
+	}
+	if (p > 1)
+		sw->features |= SW_VLAN_DEV;
+	sw->eth_cnt = p;
+	for (p = 0; p < sw->eth_cnt; p++) {
+		map = &sw->eth_maps[p];
+		dbg_msg("%d: %d:%d %04x %03x %08x\n",
+			p, map->first, map->cnt,
+			map->mask, map->vlan, map->proto);
+	}
+
+setup_next:
+	if (sw->eth_cnt > 1) {
+		if (stp <= 1)
+			stp = 0;
+	}
+#ifdef CONFIG_KSZ_STP
+	if (stp > 0) {
+		sw->features |= STP_SUPPORT;
+	}
+#endif
+
+dbg_msg("features: %x m:%d s:%x\n", sw->features, multi_dev, stp);
+	return ports;
+}  /* sw_setup_zone */
+
+static int phy_offset;
+
+static void sw_setup_special(struct ksz_sw *sw, int *port_cnt,
+	int *mib_port_cnt, int *dev_cnt,
+	const void *phylink_ops)
+{
+#ifdef CONFIG_PHYLINK
+	sw->phylink_ops = phylink_ops;
+#endif
+	phy_offset = 0;
+	sw->dev_offset = 0;
+	sw->phy_offset = 0;
+
+	/* Multiple network device interfaces are required. */
+	if (1 == sw->multi_dev) {
+		sw->dev_count = sw->mib_port_cnt;
+		sw->phy_offset = 1;
+	} else if (2 == sw->multi_dev)
+		sw->features |= VLAN_PORT | VLAN_PORT_TAGGING;
+	else if (3 == sw->multi_dev) {
+		sw->dev_count = sw->mib_port_cnt;
+		sw->dev_offset = 1;
+	} else if (4 == sw->multi_dev)
+		sw->features |= VLAN_PORT;
+	else if (5 == sw->multi_dev) {
+		sw->dev_count = sw->mib_port_cnt;
+		sw->dev_offset = 1;
+		sw->features |= VLAN_PORT | VLAN_PORT_TAGGING;
+	}
+
+	/* Single network device has multiple ports. */
+	if (1 == sw->dev_count) {
+		*port_cnt = sw->mib_port_cnt;
+		*mib_port_cnt = sw->mib_port_cnt;
+	}
+	if (1 == sw->multi_dev && sw->eth_cnt)
+		sw->dev_count = sw->eth_cnt;
+	*dev_cnt = sw->dev_count;
+	if (3 == sw->multi_dev || 5 == sw->multi_dev)
+		(*dev_cnt)++;
+}  /* sw_setup_special */
+
+static void sw_leave_dev(struct ksz_sw *sw)
+{
+	int dev_count = sw->dev_count + sw->dev_offset;
+	struct sw_priv *ks = sw->dev;
+	struct phy_priv *phydata;
+#ifdef CONFIG_PHYLINK
+	struct ksz_port *port;
+#endif
+	int i;
+
+#ifdef CONFIG_KSZ_STP
+	if (sw->features & STP_SUPPORT)
+		leave_stp(&sw->info->rstp);
+#endif
+#ifdef CONFIG_KSZ_MRP
+	if (sw->features & MRP_SUPPORT)
+		leave_mrp(&sw->mrp);
+#endif
+	for (i = 0; i < dev_count; i++) {
+#ifdef CONFIG_PHYLINK
+		port = sw->netport[i];
+		if (port && port->pl) {
+		       phylink_destroy(port->pl);
+		       port->pl = NULL;
+		}
+#endif
+		sw->netdev[i] = NULL;
+		sw->netport[i] = NULL;
+	}
+
+	/* Reset port pointer as it is pointed to one from device. */
+	for (i = 0; i <= sw->port_cnt; i++) {
+		phydata = &sw->phydata[i];
+		phydata->port = &ks->ports[i];
+	}
+	sw->eth_cnt = 0;
+	sw->dev_count = 1;
+	sw->dev_offset = 0;
+	sw->phy_offset = 0;
+}  /* sw_leave_dev */
+
+static int sw_setup_dev(struct ksz_sw *sw, struct net_device *dev,
+	char *dev_name, struct ksz_port *port, int i, uint port_cnt,
+	uint mib_port_cnt)
+{
+	struct ksz_port_info *info;
+	uint cnt;
+	uint n;
+	uint p;
+	uint pi;
+	int phy_id;
+	u32 features;
+	struct ksz_dev_map *map;
+
+	if (!phy_offset)
+		phy_offset = sw->phy_offset;
+
+	/* dev_offset is ether 0 or 1. */
+	p = i;
+	if (p)
+		p -= sw->dev_offset;
+
+	if (sw->dev_offset) {
+		/*
+		 * First device associated with switch has been
+		 * created.
+		 */
+		if (i) {
+			snprintf(dev->name, IFNAMSIZ, "%s.10%%d", dev_name);
+			pi = get_phy_port(sw, p + 1);
+			memcpy(dev->dev_addr, sw->port_info[pi].mac_addr,
+				ETH_ALEN);
+		} else {
+			port_cnt = sw->mib_port_cnt;
+			mib_port_cnt = sw->mib_port_cnt;
+			sw->ops->acquire(sw);
+			sw_set_addr(sw, dev->dev_addr);
+			sw->ops->release(sw);
+		}
+	}
+
+	map = &sw->eth_maps[i];
+	if (1 == sw->multi_dev && sw->eth_cnt) {
+		port_cnt = map->cnt;
+		p = map->first - 1;
+		mib_port_cnt = port_cnt;
+	}
+
+	port->port_cnt = port_cnt;
+	port->mib_port_cnt = mib_port_cnt;
+	port->first_port = p + 1;
+	port->flow_ctrl = PHY_FLOW_CTRL;
+
+#ifdef CONFIG_KSZ_STP
+	if (!i && (sw->features & STP_SUPPORT))
+		prep_stp_mcast(dev);
+#endif
+#ifdef CONFIG_KSZ_MRP
+	if (!i && (sw->features & MRP_SUPPORT))
+		setup_mrp(&sw->mrp, dev);
+#endif
+
+	p = get_phy_port(sw, port->first_port);
+	port->sw = sw;
+	port->linked = get_port_info(sw, p);
+
+	/* Point to port under netdev. */
+	if (phy_offset)
+		phy_id = port->linked->phy_id;
+	else
+		phy_id = 0;
+
+	/* Replace virtual port with one from network device. */
+	do {
+		struct phy_device *phydev;
+		struct phy_priv *priv;
+		struct sw_priv *hw_priv = container_of(sw, struct sw_priv, sw);
+
+		phydev = mdiobus_get_phy(hw_priv->bus, phy_id);
+		priv = phydev->priv;
+		priv->port = port;
+		set_phy_support(port, phydev);
+	} while (0);
+
+	if (!phy_offset)
+		phy_offset = 1;
+
+	for (cnt = 0, n = port->first_port; cnt < port_cnt; cnt++, n++) {
+		pi = get_phy_port(sw, n);
+		info = get_port_info(sw, pi);
+		info->state = media_disconnected;
+		sw->info->port_cfg[pi].index = i;
+	}
+	sw->netdev[i] = dev;
+	sw->netport[i] = port;
+	port->netdev = dev;
+	port->phydev = sw->phy[phy_id];
+	if (phy_id)
+		port->dn = sw->devnode[phy_id - 1];
+#ifdef CONFIG_PHYLINK
+	setup_phylink(port);
+#endif
+	if (sw->dev_count > 1 && i && !(sw->features & DIFF_MAC_ADDR)) {
+		if (memcmp(dev->dev_addr, sw->netdev[0]->dev_addr, ETH_ALEN))
+			sw->features |= DIFF_MAC_ADDR;
+	}
+
+	INIT_DELAYED_WORK(&port->link_update, link_update_work);
+	features = sw->features;
+	if (sw->features & SW_VLAN_DEV)
+		features = map->proto;
+
+	if (features & VLAN_PORT)
+		dev->features |= NETIF_F_HW_VLAN_CTAG_FILTER;
+
+	/* Needed for inserting VLAN tag. */
+	if (sw->features & SW_VLAN_DEV)
+		dev->hard_header_len += VLAN_HLEN;
+dbg_msg("%s %d:%d phy:%d\n", __func__, port->first_port, port->port_cnt, phy_id);
+
+	return phy_id;
+}  /* sw_setup_dev */
+
+static int netdev_chk_running(struct net_device *dev)
+{
+	return netif_running(dev);
+}
+
+static int netdev_chk_stopped(struct net_device *dev)
+{
+	return netif_running(dev) && netif_queue_stopped(dev);
+}
+
+static void netdev_start_queue(struct net_device *dev)
+{
+	netif_start_queue(dev);
+}
+
+static void netdev_stop_queue(struct net_device *dev)
+{
+	netif_stop_queue(dev);
+}
+
+static void netdev_wake_queue(struct net_device *dev)
+{
+	netif_wake_queue(dev);
+}
+
+static void sw_netdev_oper(struct ksz_sw *sw, struct net_device *dev,
+	int (*netdev_chk)(struct net_device *dev),
+	void (*netdev_oper)(struct net_device *dev))
+{
+	uint port;
+	int dev_count = 1;
+
+	dev_count = sw->dev_count + sw->dev_offset;
+	if (dev_count <= 1) {
+		netdev_oper(dev);
+		return;
+	}
+	for (port = 0; port < dev_count; port++) {
+		dev = sw->netdev[port];
+		if (!dev)
+			continue;
+		if (!netdev_chk || netdev_chk(dev))
+			netdev_oper(dev);
+	}
+}  /* sw_netdev_oper */
+
+static void sw_netdev_open_port(struct ksz_sw *sw, struct net_device *dev)
+{
+	struct ksz_port *port;
+	int p;
+	int dev_count = 1;
+
+	dev_count = sw->dev_count + sw->dev_offset;
+	if (dev_count <= 1) {
+		port = sw->netport[0];
+		sw->net_ops->open_port(sw, dev, port);
+		return;
+	}
+	for (p = 0; p < dev_count; p++) {
+		dev = sw->netdev[p];
+		if (!dev)
+			continue;
+		port = sw->netport[p];
+		sw->net_ops->open_port(sw, dev, port);
+	}
+}  /* sw_netdev_open_port */
+
+static void sw_netdev_start_queue(struct ksz_sw *sw, struct net_device *dev)
+{
+	sw_netdev_oper(sw, dev, netdev_chk_running, netdev_start_queue);
+}
+
+static void sw_netdev_stop_queue(struct ksz_sw *sw, struct net_device *dev)
+{
+	sw_netdev_oper(sw, dev, netdev_chk_running, netdev_stop_queue);
+}
+
+static void sw_netdev_wake_queue(struct ksz_sw *sw, struct net_device *dev)
+{
+	sw_netdev_oper(sw, dev, netdev_chk_stopped, netdev_wake_queue);
+}
+
+static struct ksz_sw_net_ops sw_net_ops = {
+	.setup_special		= sw_setup_special,
+	.setup_dev		= sw_setup_dev,
+	.leave_dev		= sw_leave_dev,
+
+	.start			= sw_start,
+	.stop			= sw_stop,
+	.open_dev		= sw_open_dev,
+	.open_port		= sw_open_port,
+	.close_port		= sw_close_port,
+	.open			= sw_open,
+	.close			= sw_close,
+
+	.netdev_start_queue	= sw_netdev_start_queue,
+	.netdev_stop_queue	= sw_netdev_stop_queue,
+	.netdev_wake_queue	= sw_netdev_wake_queue,
+	.netdev_open_port	= sw_netdev_open_port,
+
+	.set_mac_addr		= sw_set_mac_addr,
+
+	.get_mtu		= sw_get_mtu,
+	.get_tx_len		= sw_get_tx_len,
+	.add_tail_tag		= sw_add_tail_tag,
+	.get_tail_tag		= sw_get_tail_tag,
+	.add_vid		= sw_add_vid,
+	.kill_vid		= sw_kill_vid,
+	.check_tx		= sw_check_tx,
+	.rx_dev			= sw_rx_dev,
+	.match_pkt		= sw_match_pkt,
+	.parent_rx		= sw_parent_rx,
+	.port_vlan_rx		= sw_port_vlan_rx,
+	.final_skb		= sw_final_skb,
+	.drv_rx			= sw_drv_rx,
+	.set_multi		= sw_set_multi,
+
+};
+
+static struct ksz_sw_ops sw_ops = {
+	.init			= sw_init_dev,
+	.exit			= sw_exit_dev,
+	.dev_req		= sw_dev_req,
+
+	.get_phy_port		= get_phy_port,
+	.get_log_port		= get_log_port,
+
+	.acquire		= sw_acquire,
+	.release		= sw_release,
+
+	.chk			= sw_chk,
+	.cfg			= sw_cfg,
+
+	.port_get_link_speed	= port_get_link_speed,
+	.port_set_link_speed	= port_set_link_speed,
+	.port_force_link_speed	= port_force_link_speed,
+
+	.port_r_cnt		= port_r_cnt,
+	.get_mib_counters	= get_sw_mib_counters,
+
+	.sysfs_read		= sysfs_sw_read,
+	.sysfs_read_hw		= sysfs_sw_read_hw,
+	.sysfs_write		= sysfs_sw_write,
+	.sysfs_port_read	= sysfs_port_read,
+	.sysfs_port_read_hw	= sysfs_port_read_hw,
+	.sysfs_port_write	= sysfs_port_write,
+	.sysfs_mac_read		= sysfs_mac_read,
+	.sysfs_mac_write	= sysfs_mac_write,
+	.sysfs_vlan_read	= sysfs_vlan_read,
+	.sysfs_vlan_write	= sysfs_vlan_write,
+
+#ifdef CONFIG_KSZ_STP
+	.sysfs_stp_read		= sysfs_stp_read,
+	.sysfs_stp_write	= sysfs_stp_write,
+	.sysfs_stp_port_read	= sysfs_stp_port_read,
+	.sysfs_stp_port_write	= sysfs_stp_port_write,
+#endif
+
+#ifdef CONFIG_KSZ_MRP
+	.sysfs_mrp_read		= sysfs_mrp_read,
+	.sysfs_mrp_write	= sysfs_mrp_write,
+	.sysfs_mrp_port_read	= sysfs_mrp_port_read,
+	.sysfs_mrp_port_write	= sysfs_mrp_port_write,
+#endif
+
+	.cfg_mac		= sw_cfg_mac,
+	.cfg_vlan		= sw_cfg_vlan,
+	.alloc_mac		= sw_alloc_mac,
+	.free_mac		= sw_free_mac,
+	.alloc_vlan		= sw_alloc_vlan,
+	.free_vlan		= sw_free_vlan,
+	.alloc_fid		= sw_alloc_fid,
+	.free_fid		= sw_free_fid,
+
+	.get_br_id		= sw_get_br_id,
+	.from_backup		= sw_from_backup,
+	.to_backup		= sw_to_backup,
+	.from_designated	= sw_from_designated,
+	.to_designated		= sw_to_designated,
+	.tc_detected		= sw_tc_detected,
+	.get_tcDetected		= sw_get_tcDetected,
+
+	.cfg_src_filter		= sw_cfg_src_filter,
+	.flush_table		= sw_flush_dyn_mac_table,
+	.fwd_unk_vid		= sw_fwd_unk_vid,
+
+};
+
+/* -------------------------------------------------------------------------- */
+
+/* debugfs code */
+static int state_show(struct seq_file *seq, void *v)
+{
+	int i;
+	int j;
+	SW_D data[16 / SW_SIZE];
+	struct sw_priv *priv = seq->private;
+	struct ksz_sw *sw = &priv->sw;
+
+	for (i = 0; i < 0x100; i += 16) {
+		seq_printf(seq, SW_SIZE_STR":\t", i);
+		mutex_lock(&priv->lock);
+		for (j = 0; j < 16 / SW_SIZE; j++)
+			data[j] = sw->reg->r8(sw, i + j * SW_SIZE);
+		mutex_unlock(&priv->lock);
+		for (j = 0; j < 16 / SW_SIZE; j++)
+			seq_printf(seq, SW_SIZE_STR" ", data[j]);
+		seq_printf(seq, "\n");
+	}
+	return 0;
+}
+
+static int state_open(struct inode *inode, struct file *file)
+{
+	return single_open(file, state_show, inode->i_private);
+}
+
+static const struct file_operations state_fops = {
+	.owner	= THIS_MODULE,
+	.open	= state_open,
+	.read	= seq_read,
+	.llseek	= seq_lseek,
+	.release = single_release,
+};
+
+/**
+ * create_debugfs - create debugfs directory and files
+ * @priv:	The switch device structure.
+ *
+ * Create the debugfs entries for the specific device.
+ */
+static void create_debugfs(struct sw_priv *priv)
+{
+	struct dentry *root;
+	char root_name[32];
+
+	snprintf(root_name, sizeof(root_name), "%s",
+		 dev_name(priv->dev));
+
+	root = debugfs_create_dir(root_name, NULL);
+	if (IS_ERR(root)) {
+		pr_err("cannot create debugfs root\n");
+		return;
+	}
+
+	priv->debug_root = root;
+	priv->debug_file = debugfs_create_file("state", 0444, root,
+		priv, &state_fops);
+	if (IS_ERR(priv->debug_file))
+		pr_err("cannot create debugfs state file\n");
+}
+
+static void delete_debugfs(struct sw_priv *priv)
+{
+	debugfs_remove(priv->debug_file);
+	debugfs_remove(priv->debug_root);
+}
+
+/* -------------------------------------------------------------------------- */
+
+#define USE_SPEED_LINK
+#define USE_MIB
+#include "ksz_sw_sysfs_8895.c"
+
+static irqreturn_t sw_interrupt(int irq, void *phy_dat)
+{
+	struct sw_priv *ks = phy_dat;
+
+	ks->sw.intr_using += 1;
+	ks->irq_work.func(&ks->irq_work);
+
+	return IRQ_HANDLED;
+}  /* sw_interrupt */
+
+static void sw_change(struct work_struct *work)
+{
+	struct sw_priv *ks =
+		container_of(work, struct sw_priv, irq_work);
+	struct ksz_sw *sw = &ks->sw;
+	SW_D status;
+
+	mutex_lock(&ks->hwlock);
+	mutex_lock(&ks->lock);
+	sw->intr_using++;
+	status = sw->reg->r8(sw, REG_INT_STATUS);
+	status &= ks->intr_mask;
+	if (status & ks->intr_mask) {
+		sw->phy_intr = status;
+		sw->reg->w8(sw, REG_INT_STATUS, status);
+		status &= ~ks->intr_mask;
+		schedule_delayed_work(&ks->link_read, 0);
+		ks->intr_working |= 0x80000000;
+	} else
+		ks->intr_working &= ~0x80000000;
+	if (!(ks->intr_working & 0x80000000))
+		ks->intr_working = 0;
+	else
+		ks->intr_working |= 1;
+	sw->intr_using--;
+	mutex_unlock(&ks->lock);
+	if (status) {
+		mutex_lock(&ks->lock);
+		sw->reg->w8(sw, REG_INT_STATUS, status);
+		mutex_unlock(&ks->lock);
+	}
+	mutex_unlock(&ks->hwlock);
+	sw->intr_using = 0;
+}  /* sw_change */
+
+static int sw_start_interrupt(struct sw_priv *ks, const char *name)
+{
+	int err = 0;
+
+	INIT_WORK(&ks->irq_work, sw_change);
+
+	err = request_threaded_irq(ks->irq, NULL, sw_interrupt,
+		ks->intr_mode, name, ks);
+	if (err < 0) {
+		printk(KERN_WARNING "%s: Can't get IRQ %d (PHY)\n",
+			name,
+			ks->irq);
+		ks->irq = 0;
+	}
+
+	return err;
+}  /* sw_start_interrupt */
+
+static void sw_stop_interrupt(struct sw_priv *ks)
+{
+	free_irq(ks->irq, ks);
+	cancel_work_sync(&ks->irq_work);
+}  /* sw_stop_interrupt */
+
+/* -------------------------------------------------------------------------- */
+
+static char *kszsw_phy_driver_names[] = {
+	"Microchip KSZ8895 Switch",
+	"Microchip KSZ8864 Switch",
+};
+
+static int kszphy_probe(struct phy_device *phydev)
+{
+	struct mii_bus *bus = phydev->mdio.bus;
+	struct sw_priv *sw_priv = bus->priv;
+	struct ksz_sw *sw = &sw_priv->sw;
+	uint p;
+
+	p = phydev->mdio.addr;
+	phydev->priv = &sw->phydata[p];
+	phydev->interface = sw->interface;
+	return 0;
+}
+
+static int kszphy_get_features(struct phy_device *phydev)
+{
+	struct phy_priv *priv = phydev->priv;
+	struct ksz_port *port = priv->port;
+	int ret;
+
+	ret = genphy_read_abilities(phydev);
+	if (ret < 0)
+		return ret;
+
+	set_phy_support(port, phydev);
+
+	/* Special for first PHY connected to MAC. */
+	if (phydev->mdio.addr == 0) {
+		linkmode_clear_bit(ETHTOOL_LINK_MODE_Autoneg_BIT,
+				   phydev->supported);
+	}
+	return 0;
+}
+
+static int kszphy_config_init(struct phy_device *phydev)
+{
+	return 0;
+}
+
+static struct phy_driver kszsw_phy_driver = {
+	.phy_id		= PHY_ID_KSZ_SW,
+	.phy_id_mask	= 0x00ffffff,
+	.name		= "Microchip KSZ8895 Switch",
+	.probe		= kszphy_probe,
+	.get_features	= kszphy_get_features,
+	.config_init	= kszphy_config_init,
+	.config_aneg	= genphy_config_aneg,
+	.read_status	= genphy_read_status,
+};
+
+static int ksz_mii_read(struct mii_bus *bus, int phy_id, int regnum)
+{
+	struct sw_priv *ks = bus->priv;
+	struct ksz_sw *sw = &ks->sw;
+	int ret = 0xffff;
+
+	/* Last port can be a PHY. */
+	if (phy_id > sw->port_cnt)
+		return 0xffff;
+	if (phy_id && get_log_port(sw, phy_id - 1) > sw->mib_port_cnt)
+		return 0xffff;
+
+	mutex_lock(&ks->lock);
+	ret = 0;
+	if (regnum < 6) {
+		u16 data;
+
+		sw_r_phy(sw, phy_id, regnum, &data);
+		ret = data;
+	}
+	mutex_unlock(&ks->lock);
+	return ret;
+}  /* ksz_mii_read */
+
+static int ksz_mii_write(struct mii_bus *bus, int phy_id, int regnum, u16 val)
+{
+	struct sw_priv *ks = bus->priv;
+	struct ksz_sw *sw = &ks->sw;
+
+	/* Last port can be a PHY. */
+	if (phy_id > sw->port_cnt)
+		return -EINVAL;
+
+	/* Zero is used for the whole switch. */
+	if ((sw->multi_dev & 1) && phy_id == 0)
+		return 0;
+
+	mutex_lock(&ks->lock);
+	if (regnum < 6) {
+		uint i;
+		uint p;
+		int first;
+		int last;
+
+		if (0 == phy_id) {
+			first = 1;
+			last = sw->mib_port_cnt;
+		} else {
+			bool found;
+			int n;
+			int f;
+			int l;
+			struct ksz_dev_map *map;
+
+			first = phy_id;
+			last = phy_id;
+			found = false;
+			for (n = 0; n < sw->eth_cnt; n++) {
+				map = &sw->eth_maps[n];
+				f = map->first;
+				l = f + map->cnt - 1;
+				for (i = f; i <= l; i++) {
+					p = get_phy_port(sw, i);
+					if (phy_id == p + 1) {
+						found = true;
+						break;
+					}
+				}
+				if (found) {
+					first = map->first;
+					last = first + map->cnt - 1;
+					break;
+				}
+			}
+dbg_msg(" %d f:%d l:%d\n", phy_id, first, last);
+		}
+
+		/* PHY device driver resets or powers down the PHY. */
+		if (0 == regnum &&
+		    (val & (PHY_RESET | PHY_POWER_DOWN)))
+			goto done;
+		for (i = first; i <= last; i++) {
+			p = get_phy_port(sw, i) + 1;
+			sw_w_phy(sw, p, regnum, val);
+		}
+		if (PHY_REG_CTRL == regnum &&
+		    !(val & PHY_AUTO_NEG_ENABLE))
+			schedule_delayed_work(&ks->link_read, 1);
+	}
+done:
+	mutex_unlock(&ks->lock);
+	return 0;
+}  /* ksz_mii_write */
+
+static void sw_init_phy_priv(struct sw_priv *ks)
+{
+	struct phy_priv *phydata;
+	struct ksz_port *port;
+	struct ksz_sw *sw = &ks->sw;
+	uint n;
+	uint p;
+
+	for (n = 0; n <= sw->port_cnt; n++) {
+		phydata = &sw->phydata[n];
+		port = &ks->ports[n];
+		phydata->port = port;
+		port->sw = sw;
+		port->phydev = &sw->phy_map[n];
+		port->flow_ctrl = PHY_FLOW_CTRL;
+		port->port_cnt = 1;
+		port->mib_port_cnt = 1;
+		p = n;
+		if (!n) {
+			port->port_cnt = sw->mib_port_cnt;
+			port->mib_port_cnt = sw->mib_port_cnt;
+			p = 1;
+		}
+		port->first_port = p;
+		p = get_phy_port(sw, p);
+		port->linked = get_port_info(sw, p);
+dbg_msg(" %s %d=p:%d; f:%d c:%d i:%d\n", __func__, n, p,
+port->first_port, port->port_cnt, port->linked->phy_id);
+		INIT_DELAYED_WORK(&port->link_update, link_update_work);
+		sw->phy_map[n].priv = phydata;
+	}
+}  /* sw_init_phy_priv */
+
+static void sw_init_phydev(struct ksz_sw *sw, struct phy_device *phydev)
+{
+	struct ksz_port_info *info = get_port_info(sw, sw->HOST_PORT);
+
+	phydev->interface = sw->interface;
+	phydev->link = 1;
+	phydev->speed = info->tx_rate / TX_RATE_UNIT;
+	phydev->duplex = (info->duplex == 2);
+	phydev->pause = 1;
+}  /* sw_init_phydev */
+
+static int driver_installed;
+
+static int ksz_mii_init(struct sw_priv *ks)
+{
+	struct platform_device *pdev;
+	struct mii_bus *bus;
+	struct phy_device *phydev;
+	int err;
+	int i;
+
+	pdev = platform_device_register_simple("Switch MII bus", ks->sw.id,
+		NULL, 0);
+	if (!pdev)
+		return -ENOMEM;
+
+	bus = mdiobus_alloc();
+	if (bus == NULL) {
+		err = -ENOMEM;
+		goto mii_init_reg;
+	}
+
+	if (!driver_installed) {
+		kszsw_phy_driver.name =
+			kszsw_phy_driver_names[ks->sw.chip_id];
+		err = phy_driver_register(&kszsw_phy_driver, THIS_MODULE);
+		if (err)
+			goto mii_init_free_mii_bus;
+		driver_installed = true;
+	}
+
+	bus->name = "Switch MII bus",
+	bus->read = ksz_mii_read;
+	bus->write = ksz_mii_write;
+	snprintf(bus->id, MII_BUS_ID_SIZE, "sw.%d", ks->sw.id);
+	bus->parent = &pdev->dev;
+	bus->phy_mask = ~((1 << (ks->sw.port_cnt + 1)) - 1);
+	bus->priv = ks;
+
+	for (i = 0; i < PHY_MAX_ADDR; i++)
+		bus->irq[i] = -1;
+
+	err = mdiobus_register(bus);
+	if (err < 0)
+		goto mii_init_free_mii_bus;
+
+	for (i = 0; i < PHY_MAX_ADDR; i++) {
+		phydev = mdiobus_get_phy(bus, i);
+		if (phydev) {
+			struct phy_priv *priv = phydev->priv;
+
+			priv->state = phydev->state;
+		}
+	}
+
+	ks->bus = bus;
+	ks->pdev = pdev;
+	phydev = mdiobus_get_phy(bus, 0);
+	ks->phydev = phydev;
+	sw_init_phydev(&ks->sw, phydev);
+
+	return 0;
+
+mii_init_free_mii_bus:
+	if (driver_installed) {
+		phy_driver_unregister(&kszsw_phy_driver);
+		driver_installed = false;
+	}
+	mdiobus_free(bus);
+
+mii_init_reg:
+	platform_device_unregister(pdev);
+
+	return err;
+}  /* ksz_mii_init */
+
+static void ksz_mii_exit(struct sw_priv *ks)
+{
+	int i;
+	struct phy_device *phydev;
+	struct platform_device *pdev = ks->pdev;
+	struct mii_bus *bus = ks->bus;
+
+	if (ks->irq > 0) {
+		struct ksz_sw *sw = &ks->sw;
+
+		mutex_lock(&ks->lock);
+		sw->reg->w8(sw, REG_INT_ENABLE, 0);
+		mutex_unlock(&ks->lock);
+		sw_stop_interrupt(ks);
+	}
+	for (i = 0; i < PHY_MAX_ADDR; i++) {
+		phydev = mdiobus_get_phy(bus, i);
+		if (phydev) {
+			struct ksz_port *port;
+
+			port = &ks->ports[i];
+			cancel_delayed_work_sync(&port->link_update);
+		}
+	}
+	mdiobus_unregister(bus);
+	if (driver_installed) {
+		phy_driver_unregister(&kszsw_phy_driver);
+		driver_installed = false;
+	}
+	mdiobus_free(bus);
+	platform_device_unregister(pdev);
+}  /* ksz_mii_exit */
+
+/* driver bus management functions */
+
+static void determine_rate(struct ksz_sw *sw, struct ksz_port_mib *mib)
+{
+	int j;
+
+	for (j = 0; j < 2; j++) {
+		if (mib->rate[j].last) {
+			int offset;
+			u64 cnt;
+			u64 last_cnt;
+			unsigned long diff = jiffies - mib->rate[j].last;
+
+			if (0 == j)
+				offset = MIB_RX_LO_PRIO;
+			else
+				offset = MIB_TX_LO_PRIO;
+			cnt = mib->counter[offset] + mib->counter[offset + 1];
+			last_cnt = cnt;
+			cnt -= mib->rate[j].last_cnt;
+			if (cnt > 1000000 && diff >= HZ) {
+				u64 rate = cnt;
+
+				rate *= 8;
+				diff *= 1000 * 100 / HZ;
+				rate = div_u64_u32(rate, diff);
+				mib->rate[j].last = jiffies;
+				mib->rate[j].last_cnt = last_cnt;
+				if (mib->rate[j].peak < (u32) rate)
+					mib->rate[j].peak = (u32) rate;
+			}
+		} else
+			mib->rate[j].last = jiffies;
+	}
+}  /* determine_rate */
+
+static void ksz8895_mib_read_work(struct work_struct *work)
+{
+	struct sw_priv *hw_priv =
+		container_of(work, struct sw_priv, mib_read);
+	struct ksz_sw *sw = &hw_priv->sw;
+	struct ksz_port_mib *mib;
+	unsigned long interval;
+	uint n;
+	uint p;
+	int cnt = 0;
+
+	/* Find out how many ports are connected. */
+	for (n = 1; n <= sw->mib_port_cnt; n++) {
+		p = get_phy_port(sw, n);
+		if (media_connected == sw->port_state[p].state)
+			++cnt;
+	}
+	cnt++;
+	interval = MIB_READ_INTERVAL * 2 / cnt;
+	if (time_before(sw->next_jiffies, jiffies)) {
+		sw->next_jiffies = jiffies;
+		sw->next_jiffies += interval;
+	}
+	for (n = 0; n <= sw->mib_port_cnt; n++) {
+		p = get_phy_port(sw, n);
+		mib = get_port_mib(sw, p);
+
+		if (mib->cnt_ptr || 1 == hw_priv->counter[p].read) {
+		} else if (time_after_eq(jiffies, hw_priv->counter[p].time)) {
+			hw_priv->counter[p].time = sw->next_jiffies;
+			/* Only read MIB counters when the port is connected. */
+			if (media_connected == sw->port_state[p].state ||
+			    p == sw->HOST_PORT)
+				hw_priv->counter[p].read = 1;
+
+			/* Read the dropped counters. */
+			else
+				mib->cnt_ptr = SWITCH_COUNTER_NUM;
+			sw->next_jiffies += interval;
+
+		/* Port is just disconnected. */
+		} else if (sw->port_state[p].link_down) {
+			int j;
+
+			for (j = 0; j < TOTAL_SWITCH_COUNTER_NUM; j++)
+				mib->read_cnt[j] += mib->read_max[j];
+			sw->port_state[p].link_down = 0;
+
+			/* Read counters one last time after link is lost. */
+			hw_priv->counter[p].read = 1;
+		}
+
+		/* Reading MIB counters or requested to read. */
+		if (mib->cnt_ptr || 1 == hw_priv->counter[p].read) {
+
+			/* Need to process interrupt. */
+			if (port_r_cnt(sw, p))
+				return;
+			hw_priv->counter[p].read = 0;
+
+			/* Finish reading counters. */
+			if (0 == mib->cnt_ptr) {
+				hw_priv->counter[p].read = 2;
+				wake_up_interruptible(
+					&hw_priv->counter[p].counter);
+				if (p != sw->HOST_PORT)
+					determine_rate(sw, mib);
+			}
+		}
+	}
+}  /* ksz8895_mib_read_work */
+
+static void copy_port_status(struct ksz_port *src, struct ksz_port *dst)
+{
+	dst->duplex = src->duplex;
+	dst->speed = src->speed;
+	dst->force_link = src->force_link;
+	dst->linked = src->linked;
+}
+
+static void link_read_work(struct work_struct *work)
+{
+	struct delayed_work *dwork = to_delayed_work(work);
+	struct sw_priv *hw_priv =
+		container_of(dwork, struct sw_priv, link_read);
+	struct ksz_sw *sw = &hw_priv->sw;
+	struct ksz_port *port = NULL;
+	struct ksz_port *sw_port = NULL;
+	int i;
+	int s = 1;
+	int dev_cnt = sw->dev_count + sw->dev_offset;
+
+	if (1 == sw->dev_count || 1 == sw->dev_offset)
+		s = 0;
+
+	/* Check main device when child devices are used. */
+	if (sw->dev_offset)
+		sw_port = sw->netport[0];
+	sw->ops->acquire(sw);
+	if (!sw->phy_intr) {
+		sw->phy_intr = sw->reg->r8(sw, REG_INT_STATUS);
+		if (sw->phy_intr)
+			sw->reg->w8(sw, REG_INT_STATUS, sw->phy_intr);
+	}
+	for (i = sw->dev_offset; i < dev_cnt; i++) {
+		struct phy_priv *phydata;
+		int n = i + s;
+
+		port = sw->netport[i];
+		phydata = &sw->phydata[n];
+		if (!port)
+			port = phydata->port;
+		port_get_link_speed(port);
+
+		/* Copy all port information for user access. */
+		if (port != phydata->port)
+			copy_port_status(port, phydata->port);
+	}
+
+	/* Check last port. */
+	port = sw->phydata[sw->port_cnt].port;
+	port_get_link_speed(port);
+
+	sw->phy_intr = 0;
+	sw->ops->release(sw);
+
+	/* Need to invoke link_update_work before sw_port->linked is updated
+	 * as link_update_work can be called before link_read_work is
+	 * finished if the delay is not long enough.
+	 */
+	for (i = sw->dev_offset; i < dev_cnt; i++) {
+		struct phy_priv *phydata;
+
+		port = sw->netport[i];
+		if (!port) {
+			phydata = &sw->phydata[i];
+			port = phydata->port;
+		}
+		if (port->link_ports)
+			schedule_delayed_work(&port->link_update, 0);
+	}
+	port = sw->phydata[sw->port_cnt].port;
+	if (port->link_ports)
+		schedule_delayed_work(&port->link_update, 0);
+}  /* link_read_work */
+
+/*
+ * Hardware monitoring
+ */
+
+static void ksz8895_mib_monitor(struct timer_list *t)
+{
+	struct ksz_timer_info *info = from_timer(info, t, timer);
+	struct sw_priv *hw_priv = info->dev;
+
+	schedule_work(&hw_priv->mib_read);
+
+	ksz_update_timer(&hw_priv->mib_timer_info);
+}  /* ksz8895_mib_monitor */
+
+static void ksz8895_dev_monitor(struct timer_list *t)
+{
+	struct ksz_timer_info *info = from_timer(info, t, timer);
+	struct sw_priv *hw_priv = info->dev;
+	struct phy_device *phydev;
+	struct phy_priv *priv;
+	int i;
+
+	/* For MAC driver that does not add code to device open function. */
+	for (i = 0; i <= TOTAL_PORT_NUM; i++) {
+		phydev = mdiobus_get_phy(hw_priv->bus, i);
+		if (!phydev)
+			continue;
+		priv = phydev->priv;
+		if (priv->state != phydev->state) {
+			priv->state = phydev->state;
+			if (PHY_UP == phydev->state)
+				schedule_delayed_work(&priv->port->link_update, 
+						      0);
+		}
+	}
+	if (!(hw_priv->intr_working & 1))
+		schedule_delayed_work(&hw_priv->link_read, 0);
+
+	ksz_update_timer(&hw_priv->monitor_timer_info);
+}  /* ksz8895_dev_monitor */
+
+static int intr_mode;
+static int ports;
+
+static int sw_device_present;
+
+static int ksz_probe(struct sw_priv *ks)
+{
+	struct ksz_sw *sw = &ks->sw;
+	struct ksz_port_info *info;
+	u16 id;
+	u8 id1;
+	u8 id2;
+	u8 sku;
+	int i;
+	uint mib_port_count;
+	uint pi;
+	uint port_count;
+	int ret;
+
+	if (sw_device_present >= MAX_SW_DEVICES)
+		return -ENODEV;
+
+	ks->intr_mode = intr_mode ? IRQF_TRIGGER_FALLING :
+		IRQF_TRIGGER_LOW;
+	ks->intr_mode |= IRQF_ONESHOT;
+
+	dev_set_drvdata(ks->dev, ks);
+
+	mutex_init(&ks->hwlock);
+	mutex_init(&ks->lock);
+
+	sw = &ks->sw;
+	mutex_init(&sw->lock);
+	sw->hwlock = &ks->hwlock;
+	sw->reglock = &ks->lock;
+	sw->dev = ks;
+
+	sw->net_ops = &sw_net_ops;
+	sw->ops = &sw_ops;
+	init_waitqueue_head(&sw->queue);
+
+	/* simple check for a valid chip being connected to the bus */
+	mutex_lock(&ks->lock);
+#ifdef CONFIG_SPI_ATMEL
+	/* Switch may not be accessible the very first time when SPI mode is
+	 * not 0 in newer kernels where Atmel SPI was changed to use standard
+	 * SPI transfer function.
+	 */
+	if (ks->spi_mode & 2)
+		sw->reg->w8(sw, REG_CHIP_ID0, 0);
+#endif
+	id = sw->reg->r16(sw, REG_CHIP_ID0);
+	mutex_unlock(&ks->lock);
+	id1 = id >> 8;
+	id2 = id & SW_CHIP_ID_M;
+	if (id1 != FAMILY_ID ||
+	    (id2 != CHIP_ID_95 && id2 != CHIP_ID_95R)) {
+		dev_err(ks->dev, "failed to read device ID(0x%x)\n", id);
+		ret = -ENODEV;
+		goto err_sw;
+	}
+	dev_info(ks->dev, "chip id 0x%04x\n", id);
+
+	sw->info = kzalloc(sizeof(struct ksz_sw_info), GFP_KERNEL);
+	if (!sw->info) {
+		ret = -ENOMEM;
+		goto err_sw;
+	}
+
+	port_count = TOTAL_PORT_NUM;
+	mib_port_count = TOTAL_PORT_NUM;
+
+	sku = KSZ8895_SKU;
+	sw->chip_id = KSZ8895_SW_CHIP;
+
+	mutex_lock(&ks->lock);
+	id2 = sw->reg->r8(sw, REG_KSZ8864_CHIP_ID);
+	mutex_unlock(&ks->lock);
+	if (id2 & SW_KSZ8864) {
+		sku = KSZ8864_SKU;
+		sw->chip_id = KSZ8864_SW_CHIP;
+		mib_port_count--;
+	}
+	sw->PORT_INTR_MASK = (1 << port_count) - 1;
+	sw->PORT_MASK = (1 << mib_port_count) - 1;
+
+	sw->id = sw_device_present;
+
+	sw->HOST_PORT = port_count - 1;
+	sw->HOST_MASK = (1 << sw->HOST_PORT);
+
+	sw->dev_count = 1;
+
+	sw->mib_cnt = TOTAL_SWITCH_COUNTER_NUM;
+	sw->mib_port_cnt = mib_port_count;
+	sw->port_cnt = port_count;
+
+#ifdef DEBUG
+	sw->verbose = 1;
+#endif
+	if (multi_dev < 0)
+		multi_dev = 0;
+	if (stp < 0)
+		stp = 0;
+
+	/* No specific ports are specified. */
+	if (!ports)
+		ports = sw->PORT_MASK;
+dbg_msg("ports: %x\n", ports);
+
+	ports = sw_setup_zone(sw, ports);
+
+	ksz_setup_logical_ports(sw, sku, ports);
+
+	sw->PORT_MASK |= sw->HOST_MASK;
+
+	dbg_msg("%s\n", kszsw_phy_driver_names[ks->sw.chip_id]);
+
+	INIT_DELAYED_WORK(&ks->link_read, link_read_work);
+
+	for (pi = 0; pi < SWITCH_PORT_NUM; pi++) {
+		/*
+		 * Initialize to invalid value so that link detection
+		 * is done.
+		 */
+		info = get_port_info(sw, pi);
+		info->link = 0xFF;
+		info->state = media_disconnected;
+	}
+	sw->interface = PHY_INTERFACE_MODE_RMII;
+	pi = sw->HOST_PORT;
+	info = get_port_info(sw, pi);
+	info->tx_rate = 100 * TX_RATE_UNIT;
+	info->duplex = 2;
+	info->flow_ctrl = 0x33;
+	info->lpa = 0x05e1;
+
+	sw_init_phy_priv(ks);
+	setup_device_node(sw);
+
+	ret = ksz_mii_init(ks);
+	if (ret)
+		goto err_mii;
+
+	if (ks->bus) {
+		for (i = 0; i <= sw->port_cnt; i++)
+			sw->phy[i] = mdiobus_get_phy(ks->bus, i);
+	} else {
+		for (i = 0; i <= sw->port_cnt; i++)
+			sw->phy[i] = &sw->phy_map[i];
+	}
+
+	sw->multi_dev |= multi_dev;
+	sw->stp |= stp;
+	sw->fast_aging |= fast_aging;
+	if (sw->stp)
+		sw->features |= STP_SUPPORT;
+	if (sw->fast_aging)
+		sw->overrides |= FAST_AGING;
+
+	sw->counter = ks->counter;
+	sw->monitor_timer_info = &ks->monitor_timer_info;
+	sw->link_read = &ks->link_read;
+
+	sw_init_mib(sw);
+
+	for (i = 0; i < TOTAL_PORT_NUM; i++)
+		init_waitqueue_head(&ks->counter[i].counter);
+
+	create_debugfs(ks);
+
+#ifdef CONFIG_KSZ_STP
+	ksz_stp_init(&sw->info->rstp, sw);
+#endif
+	sw->ops->acquire(sw);
+	sw_reset(sw);
+	sw_init(sw);
+	sw_setup(sw);
+	sw_enable(sw);
+	sw->ops->release(sw);
+	sw->ops->init(sw);
+
+#ifndef CONFIG_KSZ_SWITCH_EMBEDDED
+	init_sw_sysfs(sw, &ks->sysfs, ks->dev);
+#endif
+#ifdef KSZSW_REGS_SIZE
+	ret = sysfs_create_bin_file(&ks->dev->kobj,
+		&kszsw_registers_attr);
+#endif
+	sema_init(&ks->proc_sem, 1);
+
+	INIT_WORK(&ks->mib_read, ksz8895_mib_read_work);
+
+	/* 500 ms timeout */
+	ksz_init_timer(&ks->mib_timer_info, 500 * HZ / 1000,
+		ksz8895_mib_monitor, ks);
+	ksz_init_timer(&ks->monitor_timer_info, 100 * HZ / 1000,
+		ksz8895_dev_monitor, ks);
+
+	ksz_start_timer(&ks->mib_timer_info, ks->mib_timer_info.period);
+	if (!(sw->multi_dev & 1) && !sw->stp)
+		ksz_start_timer(&ks->monitor_timer_info,
+			ks->monitor_timer_info.period * 10);
+
+	sw_device_present++;
+
+#ifdef CONFIG_KSZ_MRP
+	if (sw->features & MRP_SUPPORT) {
+		struct mrp_info *mrp = &sw->mrp;
+
+		INIT_DELAYED_WORK(&sw->set_mrp, sw_set_mrp);
+		mrp->ops = &mrp_ops;
+		mrp->ops->init(mrp);
+	}
+#endif
+
+	if (ks->irq <= 0)
+		return 0;
+	ks->intr_mask = INT_PORT_1 | INT_PORT_2 |
+		INT_PORT_3 | INT_PORT_4 | INT_PORT_5;
+	mutex_lock(&ks->lock);
+	sw->reg->w8(sw, REG_INT_ENABLE, 0);
+	sw->reg->w8(sw, REG_INT_STATUS, ks->intr_mask);
+	mutex_unlock(&ks->lock);
+	ret = sw_start_interrupt(ks, dev_name(ks->dev));
+	if (ret < 0)
+		printk(KERN_WARNING "No switch interrupt\n");
+	else {
+		mutex_lock(&ks->lock);
+		sw->reg->w8(sw, REG_INT_ENABLE, ks->intr_mask);
+		mutex_unlock(&ks->lock);
+	}
+
+	return 0;
+
+err_mii:
+	kfree(sw->info);
+
+err_sw:
+	kfree(ks->hw_dev);
+	kfree(ks);
+
+	return ret;
+}
+
+static int ksz_remove(struct sw_priv *ks)
+{
+	struct ksz_sw *sw = &ks->sw;
+
+	ksz_stop_timer(&ks->monitor_timer_info);
+	ksz_stop_timer(&ks->mib_timer_info);
+	flush_work(&ks->mib_read);
+	cancel_delayed_work_sync(&ks->link_read);
+	ksz_mii_exit(ks);
+
+#ifdef KSZSW_REGS_SIZE
+	sysfs_remove_bin_file(&ks->dev->kobj, &kszsw_registers_attr);
+#endif
+
+#ifndef CONFIG_KSZ_SWITCH_EMBEDDED
+	exit_sw_sysfs(sw, &ks->sysfs, ks->dev);
+#endif
+	sw->ops->exit(sw);
+
+#ifdef CONFIG_KSZ_STP
+	ksz_stp_exit(&sw->info->rstp);
+#endif
+	delete_debugfs(ks);
+
+	kfree(sw->info);
+	kfree(ks->hw_dev);
+	kfree(ks);
+
+	return 0;
+}
+
+module_param(intr_mode, int, 0);
+MODULE_PARM_DESC(intr_mode,
+	"Configure which interrupt mode to use(0=level low, 1=falling)");
+
+module_param(fast_aging, int, 0);
+module_param(multi_dev, int, 0);
+module_param(stp, int, 0);
+MODULE_PARM_DESC(fast_aging, "Fast aging");
+MODULE_PARM_DESC(multi_dev, "Multiple device interfaces");
+MODULE_PARM_DESC(stp, "STP support");
+
+module_param(ports, int, 0);
+MODULE_PARM_DESC(ports,
+	"Configure number of ports");
+
+module_param(eth1_ports, int, 0);
+module_param(eth2_ports, int, 0);
+module_param(eth3_ports, int, 0);
+module_param(eth4_ports, int, 0);
+MODULE_PARM_DESC(eth1_ports, "Ports to use on device 1.");
+MODULE_PARM_DESC(eth2_ports, "Ports to use on device 2.");
+MODULE_PARM_DESC(eth3_ports, "Ports to use on device 3.");
+MODULE_PARM_DESC(eth4_ports, "Ports to use on device 4.");
+
+module_param(eth1_vlan, int, 0);
+module_param(eth2_vlan, int, 0);
+module_param(eth3_vlan, int, 0);
+module_param(eth4_vlan, int, 0);
+MODULE_PARM_DESC(eth1_vlan, "VLAN to use on device 1.");
+MODULE_PARM_DESC(eth2_vlan, "VLAN to use on device 2.");
+MODULE_PARM_DESC(eth3_vlan, "VLAN to use on device 3.");
+MODULE_PARM_DESC(eth4_vlan, "VLAN to use on device 4.");
+
+module_param(eth1_proto, charp, 0);
+module_param(eth2_proto, charp, 0);
+module_param(eth3_proto, charp, 0);
+module_param(eth4_proto, charp, 0);
+MODULE_PARM_DESC(eth1_proto, "Protocol to use on device 1.");
+MODULE_PARM_DESC(eth2_proto, "Protocol to use on device 2.");
+MODULE_PARM_DESC(eth3_proto, "Protocol to use on device 3.");
+MODULE_PARM_DESC(eth4_proto, "Protocol to use on device 4.");
+
diff -Napur linux-linux4microchip-2022.10-orig/drivers/net/ethernet/micrel/ksz_sw_8895.h linux-linux4microchip-2022.10/drivers/net/ethernet/micrel/ksz_sw_8895.h
--- linux-linux4microchip-2022.10-orig/drivers/net/ethernet/micrel/ksz_sw_8895.h	1969-12-31 16:00:00.000000000 -0800
+++ linux-linux4microchip-2022.10/drivers/net/ethernet/micrel/ksz_sw_8895.h	2023-12-05 18:41:39.000000000 -0800
@@ -0,0 +1,879 @@
+/**
+ * Microchip KSZ8895 switch common header
+ *
+ * Copyright (c) 2015-2023 Microchip Technology Inc.
+ *	Tristram Ha <Tristram.Ha@microchip.com>
+ *
+ * Copyright (c) 2010-2015 Micrel, Inc.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ */
+
+#ifndef KSZ_SW_8895_H
+#define KSZ_SW_8895_H
+
+
+#ifdef CONFIG_PHYLINK
+#include <linux/phylink.h>
+#endif
+
+/* These definitions should be defined before this header file. */
+#ifndef PRIO_QUEUES
+#define PRIO_QUEUES			4
+#endif
+
+#ifndef KS_PRIO_IN_REG
+#define KS_PRIO_IN_REG			4
+#endif
+
+#ifndef TOTAL_PORT_NUM
+#define TOTAL_PORT_NUM			5
+#endif
+
+#ifndef SWITCH_COUNTER_NUM
+#define SWITCH_COUNTER_NUM		0x20
+#endif
+#ifndef TOTAL_SWITCH_COUNTER_NUM
+#define TOTAL_SWITCH_COUNTER_NUM	(SWITCH_COUNTER_NUM + 2)
+#endif
+
+#ifndef SW_D
+#error "SW_D and other data bus parameters need to be defined."
+#endif
+
+/* Host port can only be last of them. */
+#define SWITCH_PORT_NUM			(TOTAL_PORT_NUM - 1)
+
+#define MAX_SW_DEVICES			2
+
+
+#include "ksz_sw_api.h"
+#ifdef CONFIG_KSZ_STP
+#include "ksz_stp.h"
+#endif
+#ifdef CONFIG_KSZ_MRP
+#include "ksz_mrp.h"
+#endif
+
+
+#define LEARNED_MAC_TABLE_ENTRIES	1024
+#define STATIC_MAC_TABLE_ENTRIES	32
+#define SWITCH_MAC_TABLE_ENTRIES	32
+#define MULTI_MAC_TABLE_ENTRIES		56
+
+#define RX_TABLE_ENTRIES		128
+#define TX_TABLE_ENTRIES		8
+
+
+/**
+ * struct ksz_mac_table - Static MAC table data structure
+ * @mac_addr:	MAC address to filter.
+ * @vid:	VID value.
+ * @fid:	FID value.
+ * @ports:	Port membership.
+ * @override:	Override setting.
+ * @use_fid:	FID use setting.
+ * @valid:	Valid setting indicating the entry is being used.
+ */
+struct ksz_mac_table {
+	u8 addr[ETH_ALEN];
+	u8 fid;
+	u8 ports;
+	u8 override:1;
+	u8 use_fid:1;
+	u8 valid:1;
+	u8 dirty:1;
+};
+
+#define FWD_HOST_OVERRIDE		(1 << 0)
+#define FWD_HOST			(1 << 1)
+#define FWD_STP_DEV			(1 << 2)
+#define FWD_MAIN_DEV			(1 << 3)
+#define FWD_VLAN_DEV			(1 << 4)
+
+struct ksz_alu_table {
+	u8 owner;
+	u8 forward;
+	u8 valid:1;
+};
+
+#define VLAN_TABLE_ENTRIES		(4096 / 4)
+#define FID_ENTRIES			128
+#define FID_IN_DATA			32
+
+/**
+ * struct ksz_vlan_table - VLAN table data structure
+ * @vid:	VID value.
+ * @fid:	FID value.
+ * @member:	Port membership.
+ * @valid:	Valid setting indicating the entry is being used.
+ */
+struct ksz_vlan_table {
+	u16 vid;
+	u8 fid;
+	u8 member;
+	u8 valid:1;
+	u8 dirty:1;
+};
+
+#define PRIO_802_1P_ENTRIES		8
+
+#define DIFFSERV_ENTRIES		64
+
+/**
+ * struct ksz_port_mib - Port MIB data structure
+ * @cnt_ptr:	Current pointer to MIB counter index.
+ * @mib_start:	The starting counter index.  Some ports do not start at 0.
+ * @counter:	64-bit MIB counter value.
+ * @dropped:	Temporary buffer to remember last read packet dropped values.
+ * @read_cnt:	Used to signal when to read the MIB counter.
+ * @read_max:	Used to indicate how often to read the MIB counter.
+ *
+ * MIB counters needs to be read periodically so that counters do not get
+ * overflowed and give incorrect values.  A right balance is needed to
+ * satisfy this condition and not waste too much CPU time.
+ */
+struct ksz_port_mib {
+	u8 cnt_ptr;
+	u8 mib_start;
+	u8 reserved[2];
+
+	u64 counter[TOTAL_SWITCH_COUNTER_NUM];
+	u32 dropped[2];
+	u8 read_cnt[TOTAL_SWITCH_COUNTER_NUM];
+	u8 read_max[TOTAL_SWITCH_COUNTER_NUM];
+	struct {
+		unsigned long last;
+		u64 last_cnt;
+		u32 peak;
+	} rate[2];
+};
+
+enum {
+	STP_STATE_DISABLED = 0,
+	STP_STATE_LISTENING,
+	STP_STATE_LEARNING,
+	STP_STATE_FORWARDING,
+	STP_STATE_BLOCKED,
+	STP_STATE_SIMPLE
+};
+
+/**
+ * struct ksz_port_cfg - Port configuration data structure
+ * @vid:	VID value.
+ * @member:	Port membership.
+ * @port_prio:	Port priority.
+ * @rate_ctrl:	Priority rate control.
+ * @rx_rate:	Receive priority rate.
+ * @tx_rate:	Transmit priority rate.
+ * @rate_limit: Priority rate limit value.
+ * @vid_member:	VLAN membership.
+ * @index:	Net device pointer.
+ * @stp_state:	Current Spanning Tree Protocol state.
+ */
+struct ksz_port_cfg {
+	u16 vid;
+	u8 member;
+	u8 port_prio;
+	u8 rate_ctrl[PRIO_QUEUES];
+	u32 rx_rate[PRIO_QUEUES];
+	u32 tx_rate[PRIO_QUEUES];
+	u8 rate_limit;
+	u8 vid_member;
+	int index;
+	int stp_state;
+};
+
+/**
+ * struct ksz_sw_info - KSZ8895 switch information data structure
+ * @mac_table:	MAC table entries information.
+ * @multi_net:	Network multicast addresses used.
+ * @multi_sys:	System multicast addresses used.
+ * @blocked_rx:	Blocked receive addresses.
+ * @blocked_rx_cnt: Blocked receive addresses count.
+ * @vlan_table:	VLAN table entries information.
+ * @port_cfg:	Port configuration information.
+ * @rx_table:	Receive frame information.
+ * @tx_table:	Transmit frame information.
+ * @diffserv:	DiffServ priority settings.  Possible values from 6-bit of ToS
+ *		(bit7 ~ bit2) field.
+ * @p_802_1p:	802.1P priority settings.  Possible values from 3-bit of 802.1p
+ *		Tag priority field.
+ * @br_addr:	Bridge address.  Used for STP.
+ * @mac_addr:	Switch MAC address.
+ * @broad_per:	Broadcast storm percentage.
+ * @member:	Current port membership.  Used for STP.
+ * @phy_addr:	PHY address used by first port.
+ */
+struct ksz_sw_info {
+	struct ksz_mac_table mac_table[MULTI_MAC_TABLE_ENTRIES];
+	struct ksz_alu_table alu_table[MULTI_MAC_TABLE_ENTRIES];
+	u32 mac_table_used;
+	int multi_net;
+	int multi_sys;
+	struct ksz_port_cfg port_cfg[TOTAL_PORT_NUM];
+#ifdef CONFIG_KSZ_STP
+	struct ksz_stp_info rstp;
+#endif
+	struct ksz_mac_table mac_entry;
+	struct ksz_vlan_table vlan_entry;
+
+	SW_D diffserv[DIFFSERV_ENTRIES / KS_PRIO_IN_REG];
+	SW_D p_802_1p[PRIO_802_1P_ENTRIES / KS_PRIO_IN_REG];
+
+	u8 br_addr[ETH_ALEN];
+	u8 mac_addr[ETH_ALEN];
+
+	u32 fid[FID_ENTRIES / FID_IN_DATA];
+	u16 fid_cnt;
+
+	u8 broad_per;
+	u8 member[1];
+	u8 phy_addr;
+};
+
+/**
+ * struct ksz_port_state - Port state information data structure
+ * @state:	Connection status of the port.
+ * @link_down:	Indication the link has just gone down.
+ *
+ * It is pointless to read MIB counters when the port is disconnected.  The
+ * @state provides the connection status so that MIB counters are read only
+ * when the port is connected.  The @link_down indicates the port is just
+ * disconnected so that all MIB counters are read one last time to update the
+ * information.
+ */
+struct ksz_port_state {
+	uint state;
+	u8 link_down;
+};
+
+#define TX_RATE_UNIT			10000
+
+/**
+ * struct ksz_port_info - Port information data structure
+ * @state:	Connection status of the port.
+ * @tx_rate:	Transmit rate divided by 10000 to get Mbit.
+ * @duplex:	Duplex mode.
+ * @flow_ctrl:	Flow control.
+ * @advertised:	Advertised auto-negotiation setting.  Used to determine link.
+ * @partner:	Auto-negotiation partner setting.  Used to determine link.
+ * @link:	Link status.  Used to determine link.
+ * @status:	LinkMD status values.
+ * @length:	LinkMD length values.
+ * @mac_addr:	MAC address of the port.
+ * @phy_id:	PHY id used by the port.
+ */
+struct ksz_port_info {
+	uint state;
+	uint tx_rate;
+	u8 duplex;
+	u8 flow_ctrl;
+	u8 advertised;
+	u8 partner;
+	u8 link;
+	u16 lpa;
+	u32 status[3];
+	u32 length[3];
+	u8 mac_addr[ETH_ALEN];
+	u8 own_flow_ctrl;
+	u8 own_duplex;
+	u16 own_speed;
+	u8 phy_id;
+	u32 phy:1;
+	u32 fiber:1;
+
+	u8 phy_p;
+	u8 log_p;
+	u16 phy_m;
+	u16 log_m;
+};
+
+struct ksz_sw;
+struct ksz_port;
+
+struct ksz_sw_reg_ops {
+	u8 (*r8)(struct ksz_sw *sw, unsigned reg);
+	u16 (*r16)(struct ksz_sw *sw, unsigned reg);
+	u32 (*r32)(struct ksz_sw *sw, unsigned reg);
+	void (*w8)(struct ksz_sw *sw, unsigned reg, unsigned val);
+	void (*w16)(struct ksz_sw *sw, unsigned reg, unsigned val);
+	void (*w32)(struct ksz_sw *sw, unsigned reg, unsigned val);
+
+	void (*r)(struct ksz_sw *sw, unsigned reg, void *buf, size_t cnt);
+	void (*w)(struct ksz_sw *sw, unsigned reg, void *buf, size_t cnt);
+
+	int (*get)(struct ksz_sw *sw, u32 reg, size_t count, char *buf);
+	int (*set)(struct ksz_sw *sw, u32 reg, size_t count, char *buf);
+};
+
+struct ksz_sw_net_ops {
+	void (*setup_special)(struct ksz_sw *sw, int *port_cnt,
+		int *mib_port_cnt, int *dev_cnt,
+		const void *ops);
+	void (*setup_mdiobus)(struct ksz_sw *sw, void *bus);
+	int (*setup_dev)(struct ksz_sw *sw, struct net_device *dev,
+		char *dev_name, struct ksz_port *port, int i, uint port_cnt,
+		uint mib_port_cnt);
+	void (*leave_dev)(struct ksz_sw *sw);
+
+	void (*start)(struct ksz_sw *sw, u8 *addr);
+	int (*stop)(struct ksz_sw *sw, int complete);
+	int (*open_dev)(struct ksz_sw *sw, struct net_device *dev,
+		struct ksz_port *port, u8 *addr);
+	void (*open_port)(struct ksz_sw *sw, struct net_device *dev,
+		struct ksz_port *port);
+	void (*close_port)(struct ksz_sw *sw, struct net_device *dev,
+		struct ksz_port *port);
+	void (*open)(struct ksz_sw *sw);
+	void (*close)(struct ksz_sw *sw);
+
+	void (*netdev_start_queue)(struct ksz_sw *sw, struct net_device *dev);
+	void (*netdev_stop_queue)(struct ksz_sw *sw, struct net_device *dev);
+	void (*netdev_wake_queue)(struct ksz_sw *sw, struct net_device *dev);
+	void (*netdev_open_port)(struct ksz_sw *sw, struct net_device *dev);
+
+	u8 (*set_mac_addr)(struct ksz_sw *sw, struct net_device *dev,
+		u8 promiscuous, uint port);
+
+	int (*get_mtu)(struct ksz_sw *sw);
+	int (*get_tx_len)(struct ksz_sw *sw, struct sk_buff *skb, uint port,
+		int *header);
+	void (*add_tail_tag)(struct ksz_sw *sw, struct sk_buff *skb, uint dst);
+	int (*get_tail_tag)(u8 *trailer, int *port);
+	void (*add_vid)(struct ksz_sw *sw, u16 vid);
+	void (*kill_vid)(struct ksz_sw *sw, u16 vid);
+	struct sk_buff *(*check_tx)(struct ksz_sw *sw, struct net_device *dev,
+		struct sk_buff *skb, struct ksz_port *priv);
+	struct net_device *(*rx_dev)(struct ksz_sw *sw, u8 *data, u32 *len,
+		int *tag, int *port);
+	int (*match_pkt)(struct ksz_sw *sw, struct net_device **dev,
+		void **priv, int (*get_promiscuous)(void *ptr),
+		int (*match_multi)(void *ptr, u8 *data),
+		struct sk_buff *skb, u8 h_promiscuous);
+	struct net_device *(*parent_rx)(struct ksz_sw *sw,
+		struct net_device *dev, int *forward);
+	int (*port_vlan_rx)(struct sk_buff *skb, int forward, int tag);
+	struct sk_buff *(*final_skb)(struct ksz_sw *sw, struct sk_buff *skb,
+		struct net_device *dev, struct ksz_port *port);
+	int (*drv_rx)(struct ksz_sw *sw, struct sk_buff *skb, uint port);
+	void (*set_multi)(struct ksz_sw *sw, struct net_device *dev,
+		struct ksz_port *priv);
+};
+
+struct ksz_sw_ops {
+	void (*init)(struct ksz_sw *sw);
+	void (*exit)(struct ksz_sw *sw);
+	int (*dev_req)(struct ksz_sw *sw, char *arg,
+		struct file_dev_info *info);
+
+	uint (*get_phy_port)(struct ksz_sw *sw, uint n);
+	uint (*get_log_port)(struct ksz_sw *sw, uint p);
+
+	void (*acquire)(struct ksz_sw *sw);
+	void (*release)(struct ksz_sw *sw);
+
+	int (*chk)(struct ksz_sw *sw, u32 addr, SW_D bits);
+	void (*cfg)(struct ksz_sw *sw, u32 addr, SW_D bits, bool set);
+
+	int (*port_get_link_speed)(struct ksz_port *port);
+	void (*port_set_link_speed)(struct ksz_port *port);
+	void (*port_force_link_speed)(struct ksz_port *port);
+
+	int (*port_r_cnt)(struct ksz_sw *sw, uint port);
+	void (*get_mib_counters)(struct ksz_sw *sw, int first, int cnt,
+		u64 *counter);
+
+	ssize_t (*sysfs_read)(struct ksz_sw *sw, int proc_num,
+		struct ksz_port *port, ssize_t len, char *buf);
+	ssize_t (*sysfs_read_hw)(struct ksz_sw *sw, int proc_num, ssize_t len,
+		char *buf);
+	int (*sysfs_write)(struct ksz_sw *sw, int proc_num,
+		struct ksz_port *port, int num, const char *buf);
+	ssize_t (*sysfs_port_read)(struct ksz_sw *sw, int proc_num, uint port,
+		ssize_t len, char *buf);
+	ssize_t (*sysfs_port_read_hw)(struct ksz_sw *sw, int proc_num,
+		uint port, ssize_t len, char *buf);
+	int (*sysfs_port_write)(struct ksz_sw *sw, int proc_num, uint port,
+		int num, const char *buf);
+	ssize_t (*sysfs_mac_read)(struct ksz_sw *sw, int proc_num, ssize_t len,
+		char *buf);
+	int (*sysfs_mac_write)(struct ksz_sw *sw, int proc_num, int num,
+		const char *buf);
+	ssize_t (*sysfs_vlan_read)(struct ksz_sw *sw, int proc_num,
+		ssize_t len, char *buf);
+	int (*sysfs_vlan_write)(struct ksz_sw *sw, int proc_num, int num);
+
+#ifdef CONFIG_KSZ_STP
+	ssize_t (*sysfs_stp_read)(struct ksz_sw *sw, int proc_num, ssize_t len,
+		char *buf);
+	int (*sysfs_stp_write)(struct ksz_sw *sw, int proc_num, int num,
+		const char *buf);
+	ssize_t (*sysfs_stp_port_read)(struct ksz_sw *sw, int proc_num,
+		uint port, ssize_t len, char *buf);
+	int (*sysfs_stp_port_write)(struct ksz_sw *sw, int proc_num, uint port,
+		int num, const char *buf);
+#endif
+
+#ifdef CONFIG_KSZ_MRP
+	ssize_t (*sysfs_mrp_read)(struct ksz_sw *sw, int proc_num, ssize_t len,
+		char *buf);
+	int (*sysfs_mrp_write)(struct ksz_sw *sw, int proc_num, int num,
+		const char *buf);
+	ssize_t (*sysfs_mrp_port_read)(struct ksz_sw *sw, int proc_num,
+		uint port, ssize_t len, char *buf);
+	int (*sysfs_mrp_port_write)(struct ksz_sw *sw, int proc_num, uint port,
+		int num, const char *buf);
+#endif
+
+	void (*cfg_mac)(struct ksz_sw *sw, u8 index, u8 *dest, u32 ports,
+		int override, int use_fid, u16 fid);
+	void (*cfg_vlan)(struct ksz_sw *sw, u8 index, u16 vid, u16 fid,
+		u32 ports);
+	u8 (*alloc_mac)(struct ksz_sw *sw);
+	void (*free_mac)(struct ksz_sw *sw, u8 index);
+	u8 (*alloc_vlan)(struct ksz_sw *sw);
+	void (*free_vlan)(struct ksz_sw *sw, u8 index);
+	u16 (*alloc_fid)(struct ksz_sw *sw, u16 vid);
+	void (*free_fid)(struct ksz_sw *sw, u16 fid);
+
+	const u8 *(*get_br_id)(struct ksz_sw *sw);
+	void (*from_backup)(struct ksz_sw *sw, uint p);
+	void (*to_backup)(struct ksz_sw *sw, uint p);
+	void (*from_designated)(struct ksz_sw *sw, uint p, bool alt);
+	void (*to_designated)(struct ksz_sw *sw, uint p);
+	void (*tc_detected)(struct ksz_sw *sw, uint p);
+	int (*get_tcDetected)(struct ksz_sw *sw, uint p);
+
+	void (*cfg_src_filter)(struct ksz_sw *sw, bool set);
+	void (*flush_table)(struct ksz_sw *sw, uint port);
+	void (*fwd_unk_vid)(struct ksz_sw *sw);
+};
+
+struct ksz_dev_map {
+	u8 cnt;
+	u8 mask;
+	u8 first;
+	u8 phy_id;
+	u16 vlan;
+	uint proto;
+};
+
+struct phy_priv {
+	struct ksz_port *port;
+	enum phy_state state;
+};
+
+/* Switch features and bug fixes. */
+#define STP_SUPPORT			(1 << 0)
+#define VLAN_PORT			(1 << 1)
+#define VLAN_PORT_REMOVE_TAG		(1 << 2)
+#define VLAN_PORT_TAGGING		(1 << 3)
+#define VLAN_PORT_START			200
+#define SW_VLAN_DEV			(1 << 4)
+#define MRP_SUPPORT			(1 << 5)
+
+#define USE_FEWER_PORTS			(1 << 18)
+
+#define DSA_SUPPORT			(1 << 28)
+#define DIFF_MAC_ADDR			(1 << 30)
+
+/* Software overrides. */
+#define PAUSE_FLOW_CTRL			(1 << 0)
+#define FAST_AGING			(1 << 1)
+#define UPDATE_CSUM			(1 << 2)
+
+#define SYSFS_PHY_PORT			(1 << 18)
+
+#define TAIL_PRP_0			(1 << 24)
+#define TAIL_PRP_1			(1 << 25)
+
+#define TAG_REMOVE			(1 << 30)
+#define TAIL_TAGGING			(1 << 31)
+
+#define TAIL_TAG_SET_OVERRIDE		BIT(31)
+#define TAIL_TAG_SET_QUEUE		BIT(30)
+
+/**
+ * struct ksz_sw - Virtual switch data structure
+ * @dev:		Pointer to hardware device.
+ * @phydev:		Pointer to PHY device interface.
+ * @interface:		The R/G/MII interface used.
+ * @msg_enable:		The message flags controlling driver output.
+ * @hwlock:		Pointer to hardware lock.
+ * @reglock:		Pointer to register lock.
+ * @lock		Software lock to switch structure.
+ * @locked:		locked status.
+ * @info:		Pointer to switch information structure.
+ * @port_info:		Port information.
+ * @netdev:		Pointer to OS dependent network devices.
+ * @phy:		Pointer to OS dependent PHY devices.
+ * @dev_offset:		Indication of a switch associated network device.
+ * @phy_offset:		Indication of a port associated PHY device.
+ * @port_state:		Port state information.
+ * @port_mib:		Port MIB information.
+ * @mib_cnt:		Number of MIB counters this switch has.
+ * @mib_port_cnt:	Number of ports with MIB counters.
+ * @port_cnt:		Number of ports to support.
+ * @monitor_timer_info:	Timer information for monitoring ports.
+ * @counter:		Pointer to OS dependent MIB counter information.
+ * @link_read:		Workqueue for link monitoring.
+ * @ops:		Switch function access.
+ * @reg:		Switch register access.
+ * @net_ops:		Network related switch function access.
+ * @HOST_PORT:		A predefined value indicating the host port.
+ * @HOST_MASK:		A predefined value indicating the host port mask.
+ * @PORT_MASK:		A predefined value indicating the port mask.
+ * @rx_ports:		Bitmap of ports with receive enabled.
+ * @tx_ports:		Bitmap of ports with transmit enabled.
+ * @dev_count:		Number of network devices this switch supports.
+ * @id:			Hardware ID.  Used for display only.
+ * @vlan_id		Used for the VLAN port forwarding feature.
+ * @vid:		Used for the VLAN port forwarding feature.
+ * @features:		Switch features to enable.
+ * @overrides:		Switch features to override.
+ * @multi_dev:		Used to specify multiple devices mode.
+ * @stp:		Used to enable STP.
+ * @fast_aging:		Used to enable fast aging.
+ */
+struct ksz_sw {
+	void *dev;
+	phy_interface_t interface;
+	u32 msg_enable;
+	wait_queue_head_t queue;
+	struct mutex *hwlock;
+	struct mutex *reglock;
+	struct mutex lock;
+	int intr_using;
+
+	struct ksz_sw_info *info;
+	struct ksz_port_info port_info[TOTAL_PORT_NUM];
+	struct net_device *main_dev;
+	struct ksz_port *main_port;
+	struct net_device *netdev[TOTAL_PORT_NUM];
+	struct ksz_port *netport[TOTAL_PORT_NUM];
+	struct device_node *devnode[TOTAL_PORT_NUM];
+	struct phy_device phy_map[TOTAL_PORT_NUM + 1];
+	struct phy_device *phy[TOTAL_PORT_NUM + 1];
+	struct phy_priv phydata[TOTAL_PORT_NUM + 1];
+	int dev_offset;
+	int phy_offset;
+	struct ksz_port_state port_state[TOTAL_PORT_NUM];
+	struct ksz_port_mib port_mib[TOTAL_PORT_NUM];
+	unsigned long next_jiffies;
+	int mib_cnt;
+	int mib_port_cnt;
+	int dsa_port_cnt;
+	int port_cnt;
+	struct ksz_timer_info *monitor_timer_info;
+	struct ksz_counter_info *counter;
+	struct delayed_work *link_read;
+
+#ifdef CONFIG_PHYLINK
+	const struct phylink_mac_ops *phylink_ops;
+#endif
+
+	const struct ksz_sw_ops *ops;
+	const struct ksz_sw_reg_ops *reg;
+	struct ksz_sw_net_ops *net_ops;
+
+	int HOST_PORT;
+	u16 HOST_MASK;
+	u16 PORT_MASK;
+	u16 PORT_INTR_MASK;
+	u8 phy_intr;
+	u16 dev_ports;
+	u16 rx_ports;
+	u16 tx_ports;
+	u8 tx_pad[60];
+	int tx_start;
+
+	int dev_major;
+	u8 *msg_buf;
+	struct file_dev_info *dev_list[2];
+	uint notifications;
+	char dev_name[20];
+
+	int chip_id;
+	int dev_count;
+	int id;
+	u32 vlan_id;
+	u16 vid;
+	u16 mac_index;
+	u16 vlan_index;
+	u8 mac_dirty;
+	u8 vlan_dirty;
+	u8 verbose;
+
+	uint features;
+	uint overrides;
+
+	int multi_dev;
+	int stp;
+	int fast_aging;
+	struct ksz_dev_map eth_maps[SWITCH_PORT_NUM];
+	int eth_cnt;
+
+#ifdef CONFIG_KSZ_MRP
+	struct delayed_work set_mrp;
+	struct mrp_info mrp;
+#endif
+};
+
+struct ksz_sw_sysfs {
+	struct ksz_dev_attr *ksz_port_attrs[TOTAL_PORT_NUM];
+	struct attribute **port_attrs[TOTAL_PORT_NUM];
+};
+
+/**
+ * struct ksz_port - Virtual port data structure
+ * @first_port:		Index of first port this port supports.
+ * @mib_port_cnt:	Number of ports with MIB counters.
+ * @port_cnt:		Number of ports this port supports.
+ * @flow_ctrl:		Flow control setting.  PHY_NO_FLOW_CTRL for no flow
+ *			control, and PHY_FLOW_CTRL for flow control.
+ *			PHY_TX_ONLY and PHY_RX_ONLY are not supported for 100
+ *			Mbit PHY.
+ * @duplex:		Duplex mode setting.  1 for half duplex, 2 for full
+ *			duplex, and 0 for auto, which normally results in full
+ *			duplex.
+ * @speed:		Speed setting.  10 for 10 Mbit, 100 for 100 Mbit, and
+ *			0 for auto, which normally results in 100 Mbit.
+ * @force_link:		Force link setting.  0 for auto-negotiation, and 1 for
+ *			force.
+ * @linked:		Pointer to port information linked to this port.
+ * @sw:			Pointer to virtual switch structure.
+ */
+struct ksz_port {
+	int first_port;
+	int mib_port_cnt;
+	int port_cnt;
+
+	u8 flow_ctrl;
+	u8 duplex;
+	u8 speed;
+	u8 force_link;
+	u8 state;
+	uint opened:1;
+	uint ready:1;
+	uint report:1;
+	u16 link_ports;
+
+	struct ksz_port_info *linked;
+
+	struct ksz_sw *sw;
+
+	struct delayed_work link_update;
+	struct net_device *netdev;
+	struct phy_device *phydev;
+	struct device_node *dn;
+#ifdef CONFIG_PHYLINK
+	struct phylink *pl;
+	struct phylink_config pl_config;
+	struct phylink_link_state pl_state;
+#endif
+};
+
+static inline void sw_update_csum(struct ksz_sw *sw)
+{
+	sw->overrides |= UPDATE_CSUM;
+}
+
+#ifdef CONFIG_KSZ_HSR
+static inline bool using_hsr(struct ksz_sw *sw)
+{
+	return (sw->features & HSR_HW);
+}
+#endif
+
+static inline bool using_tail_tag(struct ksz_sw *sw)
+{
+	return (sw->overrides & TAIL_TAGGING);
+}
+
+struct lan_attributes {
+	int info;
+	int version;
+	int duplex;
+	int speed;
+	int force;
+	int flow_ctrl;
+	int features;
+	int overrides;
+	int mib;
+	int reg;
+	int vid;
+	int dynamic_table;
+	int static_table;
+	int vlan_table;
+	int aging;
+	int fast_aging;
+	int link_aging;
+	int bcast_per;
+	int mcast_storm;
+	int tx_queue_based;
+	int diffserv_map;
+	int p_802_1p_map;
+	int vlan;
+	int null_vid;
+	int macaddr;
+	int mirror_mode;
+	int tail_tag;
+	int igmp_snoop;
+	int aggr_backoff;
+	int no_exc_drop;
+	int huge_packet;
+	int legal_packet;
+	int length_check;
+	int back_pressure;
+	int sw_flow_ctrl;
+	int sw_half_duplex;
+	int sw_10_mbit;
+	int rx_flow_ctrl;
+	int tx_flow_ctrl;
+	int fair_flow_ctrl;
+	int vlan_bound;
+
+	int fw_unk_ucast_dest;
+	int fw_unk_ucast_ports;
+	int fw_unk_mcast_dest;
+	int fw_unk_mcast_ports;
+	int fw_inv_vid;
+	int fw_inv_vid_ports;
+	int fw_unk_ip_mcast_dest;
+	int fw_unk_ip_mcast_ports;
+	int self_filter;
+	int ins_tag;
+
+	int pass_all;
+	int pass_pause;
+	int hi_prio_queues;
+
+	int ports;
+	int dev_start;
+	int vlan_start;
+	int stp;
+
+	int mac_fid;
+	int mac_use_fid;
+	int mac_override;
+	int mac_valid;
+	int mac_ports;
+	int mac_addr;
+	int mac_index;
+	int mac_info;
+
+	int vlan_valid;
+	int vlan_ports;
+	int vlan_fid;
+	int vlan_index;
+	int vlan_info;
+
+#ifdef CONFIG_KSZ_STP
+	int stp_br_info;
+	int stp_br_on;
+	int stp_br_prio;
+	int stp_br_fwd_delay;
+	int stp_br_max_age;
+	int stp_br_hello_time;
+	int stp_br_tx_hold;
+	int stp_version;
+#endif
+};
+
+struct sw_attributes {
+	int mib;
+	int vid;
+	int member;
+	int bcast_storm;
+	int diffserv;
+	int p_802_1p;
+	int port_based;
+	int non_vid;
+	int ingress;
+	int ins_tag;
+	int rmv_tag;
+	int drop_tagged;
+	int replace_prio;
+	int rx;
+	int tx;
+	int learn;
+	int ins_tag_0;
+	int ins_tag_1;
+	int ins_tag_2;
+	int ins_tag_3;
+	int ins_tag_4;
+	int prio_queue;
+	int tx_q0_ctrl;
+	int tx_q1_ctrl;
+	int tx_q2_ctrl;
+	int tx_q3_ctrl;
+	int tx_q0_ratio;
+	int tx_q1_ratio;
+	int tx_q2_ratio;
+	int tx_q3_ratio;
+	int rx_prio_rate;
+	int tx_prio_rate;
+	int rx_limit;
+	int rx_limit_flow_ctrl;
+	int cnt_ifg;
+	int cnt_pre;
+	int rx_p0_rate;
+	int rx_p1_rate;
+	int rx_p2_rate;
+	int rx_p3_rate;
+	int tx_q0_rate;
+	int tx_q1_rate;
+	int tx_q2_rate;
+	int tx_q3_rate;
+	int mirror_port;
+	int mirror_rx;
+	int mirror_tx;
+	int back_pressure;
+	int force_flow_ctrl;
+	int fw_unk_ucast_dest;
+	int fw_unk_mcast_dest;
+	int fw_inv_vid;
+	int fw_unk_ip_mcast_dest;
+
+	int duplex;
+	int speed;
+	int linkmd;
+
+#ifdef CONFIG_KSZ_STP
+	int stp_info;
+	int stp_on;
+	int stp_prio;
+	int stp_admin_path_cost;
+	int stp_path_cost;
+	int stp_admin_edge;
+	int stp_auto_edge;
+	int stp_mcheck;
+	int stp_admin_p2p;
+#endif
+};
+
+struct static_mac_attributes {
+	int fid;
+	int use_fid;
+	int override;
+	int valid;
+	int ports;
+	int addr;
+};
+
+struct vlan_attributes {
+	int valid;
+	int member;
+	int fid;
+	int vid;
+};
+
+#endif
diff -Napur linux-linux4microchip-2022.10-orig/drivers/net/ethernet/micrel/ksz_sw_9897.c linux-linux4microchip-2022.10/drivers/net/ethernet/micrel/ksz_sw_9897.c
--- linux-linux4microchip-2022.10-orig/drivers/net/ethernet/micrel/ksz_sw_9897.c	1969-12-31 16:00:00.000000000 -0800
+++ linux-linux4microchip-2022.10/drivers/net/ethernet/micrel/ksz_sw_9897.c	2024-06-27 17:25:14.843298969 -0700
@@ -0,0 +1,19598 @@
+/**
+ * Microchip gigabit switch common code
+ *
+ * Copyright (c) 2015-2024 Microchip Technology Inc.
+ *	Tristram Ha <Tristram.Ha@microchip.com>
+ *
+ * Copyright (c) 2010-2015 Micrel, Inc.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ */
+
+
+/* -------------------------------------------------------------------------- */
+
+/* (288 - 24 - 4) / (8 * 3) */
+#define MAX_IBA_MIB_ENTRIES		10
+
+#define READ_MIB_ENTRY_SIZE		2
+
+/* (288 - 24 - 4) / (8 * 6) */
+#define MAX_IBA_MAC_ENTRIES		5
+
+/* (288 - 24 - 4) / (8 * 6) */
+#define MAX_IBA_VLAN_ENTRIES		5
+
+#define READ_VLAN_ENTRY_SIZE		3
+#define WRITE_VLAN_ENTRY_SIZE		4
+
+#define MAX_SYSFS_BUF_SIZE		(4080 - 80)
+
+#if 0
+#define USE_SAME_ADDR
+#endif
+
+enum {
+	PROC_SW_INFO,
+	PROC_SW_VERSION,
+
+	PROC_SET_SW_DUPLEX,
+	PROC_SET_SW_SPEED,
+	PROC_SET_SW_FORCE,
+	PROC_SET_SW_FLOW_CTRL,
+
+	PROC_SET_SW_FEATURES,
+	PROC_SET_SW_OVERRIDES,
+	PROC_SET_SW_MIB,
+
+	PROC_SET_SW_REG,
+	PROC_SET_SW_VID,
+
+	PROC_DYNAMIC,
+	PROC_STATIC,
+	PROC_VLAN,
+	PROC_HSR,
+
+	PROC_SET_AGING,
+	PROC_SET_FAST_AGING,
+	PROC_SET_LINK_AGING,
+
+	PROC_SET_BROADCAST_STORM,
+	PROC_SET_MULTICAST_STORM,
+	PROC_SET_TX_RATE_QUEUE_BASED,
+	PROC_SET_DIFFSERV,
+	PROC_SET_802_1P,
+
+	PROC_ENABLE_VLAN,
+	PROC_SET_REPLACE_NULL_VID,
+	PROC_SET_DROP_INVALID_VID,
+	PROC_SET_MAC_ADDR,
+	PROC_SET_MIRROR_MODE,
+
+	PROC_SET_IGMP_SNOOP,
+	PROC_SET_IPV6_MLD_SNOOP,
+	PROC_SET_IPV6_MLD_OPTION,
+
+	PROC_SET_AGGR_BACKOFF,
+	PROC_SET_NO_EXC_DROP,
+
+	PROC_SET_JUMBO_PACKET,
+	PROC_SET_LEGAL_PACKET,
+	PROC_SET_LENGTH_CHECK,
+
+	PROC_SET_BACK_PRESSURE_MODE,
+	PROC_SET_SWITCH_FLOW_CTRL,
+	PROC_SET_SWITCH_HALF_DUPLEX,
+	PROC_SET_SWITCH_10_MBIT,
+
+	PROC_SET_FAIR_FLOW_CTRL,
+	PROC_SET_VLAN_BOUNDARY,
+	PROC_SET_DOUBLE_TAG,
+	PROC_SET_ISP_TAG,
+	PROC_SET_HSR_TAG,
+	PROC_SET_HSR_REDBOX_ID,
+	PROC_SET_HSR_NET_ID,
+	PROC_SET_MTU,
+	PROC_SET_FORWARD_UNKNOWN_UNICAST,
+	PROC_SET_UNKNOWN_UNICAST_PORTS,
+	PROC_SET_FORWARD_UNKNOWN_MULTICAST,
+	PROC_SET_UNKNOWN_MULTICAST_PORTS,
+	PROC_SET_FORWARD_UNKNOWN_VID,
+	PROC_SET_UNKNOWN_VID_PORTS,
+
+	PROC_SET_PASS_PAUSE,
+	PROC_ENABLE_PME,
+	PROC_ENABLE_PME_POLARITY,
+
+	PROC_GET_HOST_PORT,
+	PROC_GET_PORTS,
+	PROC_GET_DEV_START,
+	PROC_GET_PORT_START,
+	PROC_GET_VLAN_START,
+	PROC_GET_AVB,
+	PROC_GET_STP,
+	PROC_GET_TWO_DEV,
+	PROC_SET_AUTHEN,
+
+	PROC_SET_STATIC_FID,
+	PROC_SET_STATIC_USE_FID,
+	PROC_SET_STATIC_OVERRIDE,
+	PROC_SET_STATIC_VALID,
+	PROC_SET_STATIC_MSTP,
+	PROC_SET_STATIC_PRIO,
+	PROC_SET_STATIC_SRC,
+	PROC_SET_STATIC_DST,
+	PROC_SET_STATIC_PORTS,
+	PROC_SET_STATIC_MAC_ADDR,
+	PROC_SET_STATIC_TYPE,
+	PROC_SET_STATIC_INDEX,
+	PROC_SET_STATIC_INFO,
+
+	PROC_SET_VLAN_VALID,
+	PROC_SET_VLAN_PORTS,
+	PROC_SET_VLAN_UNTAG,
+	PROC_SET_VLAN_FID,
+	PROC_SET_VLAN_MSTP,
+	PROC_SET_VLAN_PRIO,
+	PROC_SET_VLAN_OPTION,
+	PROC_SET_VLAN_VID,
+	PROC_SET_VLAN_INFO,
+	PROC_SET_VID_2_FID,
+	PROC_SET_FID_2_MSTID,
+
+#ifdef CONFIG_KSZ_STP
+	PROC_GET_STP_BR_INFO,
+	PROC_SET_STP_BR_ON,
+	PROC_SET_STP_BR_PRIO,
+	PROC_SET_STP_BR_FWD_DELAY,
+	PROC_SET_STP_BR_MAX_AGE,
+	PROC_SET_STP_BR_HELLO_TIME,
+	PROC_SET_STP_BR_TX_HOLD,
+	PROC_SET_STP_VERSION,
+#ifdef CONFIG_KSZ_MSTP
+	PROC_SET_STP_BR_MAX_HOPS,
+	PROC_SET_STP_MSTI,
+	PROC_SET_STP_MSTI_VID,
+	PROC_SET_STP_MSTP_CFG,
+	PROC_SET_STP_MSTP_NAME,
+#endif
+#endif
+
+#ifdef CONFIG_KSZ_MSRP
+	PROC_GET_MSRP_INFO,
+	PROC_SET_MSRP_ENABLED,
+#endif
+#ifdef CONFIG_KSZ_AVB
+	PROC_SET_MSRP_SR_A,
+#endif
+
+#ifdef CONFIG_KSZ_HSR
+	PROC_SET_HSR_VALID,
+	PROC_SET_HSR_AGE_CNT,
+	PROC_SET_HSR_PATH_ID,
+	PROC_SET_HSR_SRC_MAC_ADDR,
+	PROC_SET_HSR_INDEX,
+	PROC_SET_HSR_INFO,
+	PROC_GET_HSR_STATE,
+#endif
+
+	PROC_SET_NO_COLOR,
+	PROC_SET_COLOR_RED,
+	PROC_SET_COLOR_YELLOW,
+	PROC_SET_COLOR_GREEN,
+
+	PROC_SET_VLAN_FILTERING_DYNAMIC,
+	PROC_SET_VLAN_FILTERING_STATIC,
+
+	PROC_SW_LAST,
+};
+
+enum {
+	PROC_SET_PORT_MIB,
+
+	PROC_SET_DEF_VID,
+	PROC_SET_MEMBER,
+
+	PROC_ENABLE_BROADCAST_STORM,
+	PROC_ENABLE_DIFFSERV,
+	PROC_ENABLE_802_1P,
+	PROC_ENABLE_VLAN_PRIO,
+	PROC_ENABLE_MAC_PRIO,
+	PROC_ENABLE_ACL_PRIO,
+	PROC_SET_HIGHEST_PRIO,
+	PROC_SET_OR_PRIO,
+
+	PROC_SET_PORT_BASED,
+
+	PROC_SET_DIS_NON_VID,
+	PROC_SET_INGRESS,
+	PROC_SET_DROP_NON_VLAN,
+	PROC_SET_DROP_TAG,
+	PROC_SET_REPLACE_VID,
+	PROC_SET_REPLACE_PRIO,
+	PROC_SET_MAC_BASED_802_1X,
+	PROC_SET_SRC_ADDR_FILTER,
+	PROC_SET_VLAN_LOOKUP_0,
+
+	PROC_SET_MSTP,
+	PROC_SET_RX,
+	PROC_SET_TX,
+	PROC_SET_LEARN,
+	PROC_SET_POWER,
+
+	PROC_ENABLE_PRIO_QUEUE,
+
+	PROC_ENABLE_RX_PRIO_RATE,
+	PROC_ENABLE_TX_PRIO_RATE,
+	PROC_SET_LIMIT,
+	PROC_SET_LIMIT_PORT_BASED,
+	PROC_SET_LIMIT_PACKET_BASED,
+	PROC_SET_LIMIT_FLOW_CTRL,
+	PROC_SET_LIMIT_CNT_IFG,
+	PROC_SET_LIMIT_CNT_PRE,
+	PROC_SET_RX_P0_RATE,
+	PROC_SET_RX_P1_RATE,
+	PROC_SET_RX_P2_RATE,
+	PROC_SET_RX_P3_RATE,
+	PROC_SET_RX_P4_RATE,
+	PROC_SET_RX_P5_RATE,
+	PROC_SET_RX_P6_RATE,
+	PROC_SET_RX_P7_RATE,
+	PROC_SET_TX_Q0_RATE,
+	PROC_SET_TX_Q1_RATE,
+	PROC_SET_TX_Q2_RATE,
+	PROC_SET_TX_Q3_RATE,
+
+	PROC_SET_COLOR_MAP,
+	PROC_SET_TC_MAP,
+
+	PROC_SET_MIRROR_PORT,
+	PROC_SET_MIRROR_RX,
+	PROC_SET_MIRROR_TX,
+
+	PROC_SET_BACK_PRESSURE,
+	PROC_SET_FORCE_FLOW_CTRL,
+	PROC_SET_PASS_ALL,
+	PROC_SET_TAIL_TAG,
+
+	PROC_SET_CUSTOM_VID,
+	PROC_SET_SR_1_VID,
+	PROC_SET_SR_2_VID,
+	PROC_SET_SR_1_TYPE,
+	PROC_SET_SR_2_TYPE,
+	PROC_SET_PME_CTRL,
+	PROC_SET_PME_STATUS,
+
+	PROC_SET_AUTHEN_MODE,
+	PROC_SET_ACL,
+	PROC_SET_ACL_FIRST_RULE,
+	PROC_SET_ACL_RULESET,
+	PROC_SET_ACL_MODE,
+	PROC_SET_ACL_ENABLE,
+	PROC_SET_ACL_SRC,
+	PROC_SET_ACL_EQUAL,
+	PROC_SET_ACL_MAC_ADDR,
+	PROC_SET_ACL_TYPE,
+	PROC_SET_ACL_CNT,
+	PROC_SET_ACL_MSEC,
+	PROC_SET_ACL_INTR_MODE,
+	PROC_SET_ACL_IP_ADDR,
+	PROC_SET_ACL_IP_MASK,
+	PROC_SET_ACL_PROTOCOL,
+	PROC_SET_ACL_SEQNUM,
+	PROC_SET_ACL_PORT_MODE,
+	PROC_SET_ACL_MAX_PORT,
+	PROC_SET_ACL_MIN_PORT,
+	PROC_SET_ACL_TCP_FLAG_ENABLE,
+	PROC_SET_ACL_TCP_FLAG,
+	PROC_SET_ACL_TCP_FLAG_MASK,
+	PROC_SET_ACL_PRIO_MODE,
+	PROC_SET_ACL_PRIO,
+	PROC_SET_ACL_VLAN_PRIO_REPLACE,
+	PROC_SET_ACL_VLAN_PRIO,
+	PROC_SET_ACL_MAP_MODE,
+	PROC_SET_ACL_PORTS,
+	PROC_SET_ACL_INDEX,
+	PROC_SET_ACL_ACTION_INDEX,
+	PROC_SET_ACL_ACTION,
+	PROC_SET_ACL_RULE_INDEX,
+	PROC_SET_ACL_INFO,
+	PROC_GET_ACL_TABLE,
+
+	PROC_SET_P_INDEX,
+	PROC_SET_Q_INDEX,
+
+	PROC_SET_POLICE_PACKET_TYPE,
+	PROC_SET_NON_DSCP_COLOR,
+	PROC_ENABLE_POLICE_DROP_ALL,
+	PROC_ENABLE_PORT_BASED_POLICING,
+	PROC_ENABLE_COLOR_MARK,
+	PROC_ENABLE_COLOR_REMAP,
+	PROC_ENABLE_DROP_SRP,
+	PROC_ENABLE_COLOR_AWARE,
+	PROC_ENABLE_POLICE,
+
+	PROC_SET_Q_CIR,
+	PROC_SET_Q_PIR,
+	PROC_SET_Q_CBS,
+	PROC_SET_Q_PBS,
+
+	PROC_SET_WRED_MAX_THRESHOLD,
+	PROC_SET_WRED_MIN_THRESHOLD,
+	PROC_SET_WRED_MULTIPLIER,
+	PROC_GET_WRED_AVG_SIZE,
+
+	PROC_SET_WRED_Q_MAX_THRESHOLD,
+	PROC_SET_WRED_Q_MIN_THRESHOLD,
+	PROC_SET_WRED_Q_MULTIPLIER,
+	PROC_GET_WRED_Q_AVG_SIZE,
+
+	PROC_SET_WRED_RANDOM_DROP,
+	PROC_SET_WRED_DROP_GYR,
+	PROC_SET_WRED_DROP_YR,
+	PROC_SET_WRED_DROP_R,
+	PROC_SET_WRED_DROP_ALL,
+	PROC_GET_WRED_PMON,
+
+	PROC_SET_QUEUE_SCHEDULING,
+	PROC_SET_QUEUE_SHAPING,
+#ifdef MTI_PREEMPT_ENABLE
+	PROC_SET_PREEMPT,
+#endif
+	PROC_SET_TX_RATIO,
+	PROC_SET_CREDIT_HI_WATER_MARK,
+	PROC_SET_CREDIT_LO_WATER_MARK,
+	PROC_SET_CREDIT_INCREMENT,
+	PROC_SET_SRP,
+
+	PROC_SET_QM_DROP,
+	PROC_SET_QM_BURST_SIZE,
+	PROC_SET_QM_RESV_SPACE,
+	PROC_SET_QM_HI_WATER_MARK,
+	PROC_SET_QM_LO_WATER_MARK,
+	PROC_GET_QM_TX_USED,
+	PROC_GET_QM_TX_AVAIL,
+	PROC_GET_QM_TX_CALCULATED,
+
+	PROC_SET_MMD_ID,
+	PROC_SET_MMD_REG,
+	PROC_SET_MMD_VAL,
+
+	PROC_GET_RX_FLOW_CTRL,
+	PROC_GET_TX_FLOW_CTRL,
+
+	PROC_SET_PORT_DUPLEX,
+	PROC_SET_PORT_SPEED,
+
+	PROC_SET_MAC_OPERATIONAL,
+	PROC_SET_VLAN_RESTRICTED,
+	PROC_SET_VLAN_UNTAGGED,
+
+#ifdef CONFIG_KSZ_STP
+	PROC_GET_STP_INFO,
+	PROC_SET_STP_ON,
+	PROC_SET_STP_PRIO,
+	PROC_SET_STP_ADMIN_PATH_COST,
+	PROC_SET_STP_PATH_COST,
+	PROC_SET_STP_ADMIN_EDGE,
+	PROC_SET_STP_AUTO_EDGE,
+	PROC_SET_STP_MCHECK,
+	PROC_SET_STP_ADMIN_P2P,
+	PROC_SET_STP_AUTO_ISOLATE,
+#endif
+
+#ifdef CONFIG_KSZ_MRP
+	PROC_SET_PORT_MMRP_ENABLED,
+	PROC_SET_PORT_MMRP_MAC,
+	PROC_SET_PORT_MMRP_SVC,
+	PROC_SET_PORT_MMRP_REG,
+	PROC_SET_PORT_MVRP_ENABLED,
+	PROC_SET_PORT_MVRP_VID,
+	PROC_SET_PORT_MVRP_REG,
+#endif
+
+#ifdef CONFIG_KSZ_MSRP
+	PROC_SET_PORT_MSRP_ENABLED,
+#endif
+
+#ifdef CONFIG_KSZ_AVB
+	PROC_SET_PORT_ASCAPABLE,
+	PROC_SET_TC_DELTA_BANDWIDTH,
+	PROC_SET_TC_ADMIN_IDLE_MBPS,
+	PROC_SET_TC_ADMIN_IDLE_SLOPE,
+	PROC_GET_TC_OPER_IDLE_SLOPE,
+	PROC_SET_TC_ALGORITHM,
+	PROC_GET_SR_A_RX_PRIO,
+	PROC_SET_SR_A_TX_PRIO,
+	PROC_GET_SR_A_SRP_DOMAIN_BOUNDARY,
+	PROC_SET_SR_A_LATENCY,
+	PROC_GET_SR_B_RX_PRIO,
+	PROC_SET_SR_B_TX_PRIO,
+	PROC_GET_SR_B_SRP_DOMAIN_BOUNDARY,
+	PROC_SET_SR_B_LATENCY,
+	PROC_SET_MAX_FRAME_SIZE,
+	PROC_SET_MAX_INTERVAL_FRAMES,
+	PROC_SET_CLASS_PRIO,
+#endif
+
+	PROC_SET_LINK_MD,
+	PROC_SET_SQI,
+	PROC_SET_MAC_LOOPBACK,
+	PROC_SET_PHY_LOOPBACK,
+	PROC_SET_REMOTE_LOOPBACK,
+
+	PROC_SW_PORT_LAST,
+};
+
+/* -------------------------------------------------------------------------- */
+
+static uint get_phy_port(struct ksz_sw *sw, uint n)
+{
+	if (n >= sw->mib_port_cnt + 1)
+		n = 0;
+	return sw->port_info[n].phy_p;
+}
+
+static uint get_log_port(struct ksz_sw *sw, uint p)
+{
+	return sw->port_info[p].log_p;
+}
+
+static u16 get_phy_mask(struct ksz_sw *sw, uint n)
+{
+	if (n >= sw->mib_port_cnt + 1)
+		n = 0;
+	return sw->port_info[n].phy_m;
+}
+
+static uint get_phy_mask_from_log(struct ksz_sw *sw, uint log_m)
+{
+	struct ksz_port_info *info;
+	uint n;
+	uint p;
+	uint phy_m = 0;
+
+	for (n = 0; n <= sw->mib_port_cnt; n++) {
+		info = &sw->port_info[n];
+		p = info->phy_p;
+		if (log_m & sw->port_info[p].log_m)
+			phy_m |= info->phy_m;
+	}
+	return phy_m;
+}
+
+static uint get_log_mask_from_phy(struct ksz_sw *sw, uint phy_m)
+{
+	struct ksz_port_info *info;
+	uint n;
+	uint p;
+	uint log_m = 0;
+
+	for (n = 0; n <= sw->mib_port_cnt; n++) {
+		info = &sw->port_info[n];
+		p = info->phy_p;
+		if (phy_m & info->phy_m)
+			log_m |= sw->port_info[p].log_m;
+	}
+	return log_m;
+}
+
+static uint get_sysfs_port(struct ksz_sw *sw, uint n)
+{
+	uint p;
+
+	if (sw->overrides & SYSFS_1_BASE)
+		n--;
+	p = n;
+	if (!(sw->overrides & SYSFS_PHY_PORT)) {
+		n++;
+		if (n > sw->mib_port_cnt)
+			n = 0;
+		p = get_phy_port(sw, n);
+	}
+	return p;
+}
+
+static inline struct ksz_port_cfg *get_port_cfg(struct ksz_sw *sw, uint p)
+{
+	return &sw->info->port_cfg[p];
+}
+
+static inline struct ksz_port_info *get_port_info(struct ksz_sw *sw, uint p)
+{
+	return &sw->port_info[p];
+}
+
+static inline struct ksz_port_mib *get_port_mib(struct ksz_sw *sw, uint p)
+{
+	return &sw->port_mib[p];
+}
+
+static void sw_acquire(struct ksz_sw *sw)
+{
+	mutex_lock(&sw->lock);
+	sw->reg->lock(sw);
+	mutex_unlock(&sw->lock);
+}  /* sw_acquire */
+
+static void sw_release(struct ksz_sw *sw)
+{
+	sw->reg->unlock(sw);
+}  /* sw_release */
+
+/* -------------------------------------------------------------------------- */
+
+/* Common routines used by both SPI and IBA accesses. */
+
+/**
+ * get_mac_table_info - Get MAC table information
+ * @mac:	Buffer to store the MAC table entry.
+ * @data:	Buffer holding MAC table information.
+ *
+ * This helper routine retrieves information from MAC table entry data.
+ */
+static void get_mac_table_info(struct ksz_mac_table *mac, u32 data[])
+{
+	mac->valid = !!(data[0] & ALU_V_STATIC_VALID);
+	mac->src = !!(data[0] & ALU_V_SRC_FILTER);
+	mac->dst = !!(data[0] & ALU_V_DST_FILTER);
+	mac->prio = (data[0] >> ALU_V_PRIO_AGE_CNT_S) &
+		ALU_V_PRIO_AGE_CNT_M;
+	mac->mstp = data[0] & ALU_V_MSTP_M;
+	mac->override = !!(data[1] & ALU_V_OVERRIDE);
+	mac->use_fid = !!(data[1] & ALU_V_USE_FID);
+	mac->ports = data[1] & ALU_V_PORT_MAP;
+	mac->fid = data[2] >> ALU_V_FID_S;
+	mac->addr[1] = (u8) data[2];
+	mac->addr[0] = (u8)(data[2] >> 8);
+	mac->addr[5] = (u8) data[3];
+	mac->addr[4] = (u8)(data[3] >> 8);
+	mac->addr[3] = (u8)(data[3] >> 16);
+	mac->addr[2] = (u8)(data[3] >> 24);
+}  /* get_mac_table_info */
+
+/**
+ * set_mac_table_info - Set MAC table information
+ * @mac:	The MAC table entry.
+ * @data:	Buffer to hold MAC table information.
+ *
+ * This helper routine puts information to MAC table entry.
+ */
+static void set_mac_table_info(struct ksz_mac_table *mac, u32 data[])
+{
+	data[0] = (u32)(mac->prio & ALU_V_PRIO_AGE_CNT_M) <<
+		ALU_V_PRIO_AGE_CNT_S;
+	data[0] |= mac->mstp & ALU_V_MSTP_M;
+	if (mac->src)
+		data[0] |= ALU_V_SRC_FILTER;
+	if (mac->dst)
+		data[0] |= ALU_V_DST_FILTER;
+	if (mac->valid)
+		data[0] |= ALU_V_STATIC_VALID;
+	data[1] = mac->ports & ALU_V_PORT_MAP;
+	if (mac->override)
+		data[1] |= ALU_V_OVERRIDE;
+	if (mac->use_fid)
+		data[1] |= ALU_V_USE_FID;
+	data[2] = (u32) mac->fid << ALU_V_FID_S;
+	data[2] |= ((u32) mac->addr[0] << 8) | mac->addr[1];
+	data[3] = ((u32) mac->addr[2] << 24) |
+		((u32) mac->addr[3] << 16) |
+		((u32) mac->addr[4] << 8) | mac->addr[5];
+}  /* set_mac_table_info */
+
+/**
+ * wait_for_dyn_mac_table - Wait for dynamic MAC table to be ready
+ * @sw:		The switch instance.
+ *
+ * This helper function waits for dynamic MAC table to be ready for access.
+ */
+static int wait_for_dyn_mac_table(struct ksz_sw *sw)
+{
+	u32 ctrl;
+	int timeout = 100;
+
+	do {
+		--timeout;
+		if (!timeout)
+			return 1;
+		ctrl = sw->reg->r32(sw, REG_SW_ALU_CTRL__4);
+	} while ((ctrl & ALU_START) || ALU_SEARCH == (ctrl & ALU_ACTION));
+	return 0;
+}  /* wait_for_dyn_mac_table */
+
+/**
+ * wait_for_sta_mac_table - Wait for static MAC table to be ready
+ * @sw:		The switch instance.
+ *
+ * This helper function waits for static MAC table to be ready for access.
+ */
+static int wait_for_sta_mac_table(struct ksz_sw *sw)
+{
+	u32 ctrl;
+	int timeout = 100;
+
+	do {
+		--timeout;
+		if (!timeout)
+			return 1;
+		ctrl = sw->reg->r32(sw, REG_SW_ALU_STAT_CTRL__4);
+	} while (ctrl & ALU_STAT_START);
+	return 0;
+}  /* wait_for_sta_mac_table */
+
+/**
+ * get_vlan_table_info - Get VLAN table information
+ * @vlan:	Buffer to store the VLAN table entry.
+ * @data:	Buffer holding VLAN table information.
+ *
+ * This helper routine retrieves information from VLAN table entry data.
+ */
+static void get_vlan_table_info(struct ksz_vlan_table *vlan, u32 data[])
+{
+	vlan->valid = !!(data[0] & VLAN_VALID);
+	vlan->fid = (data[0] & VLAN_FID_M);
+	vlan->mstp = (data[0] >> VLAN_MSTP_S) & VLAN_MSTP_M;
+	vlan->prio = (data[0] >> VLAN_PRIO_S) & VLAN_PRIO_M;
+	vlan->option = !!(data[0] & VLAN_FORWARD_OPTION);
+	vlan->untag = data[1];
+	vlan->ports = data[2];
+}  /* get_vlan_table_info */
+
+/**
+ * set_vlan_table_info - set VLAN table information
+ * @vlan:	The VLAN table entry.
+ * @data:	Buffer to hold VLAN table information.
+ *
+ * This helper routine puts information to VLAN table entry.
+ */
+static void set_vlan_table_info(struct ksz_vlan_table *vlan, u32 data[])
+{
+	data[0] = vlan->fid & VLAN_FID_M;
+	data[0] |= (u32)(vlan->mstp & VLAN_MSTP_M) << VLAN_MSTP_S;
+	data[0] |= (u32)(vlan->prio & VLAN_PRIO_M) << VLAN_PRIO_S;
+	if (vlan->option)
+		data[0] |= VLAN_FORWARD_OPTION;
+	if (vlan->valid)
+		data[0] |= VLAN_VALID;
+	data[1] = vlan->untag;
+	data[2] = vlan->ports;
+}  /* set_vlan_table_info */
+
+/**
+ * wait_for_vlan_table - Wait for VLAN table to be ready
+ * @sw:		The switch instance.
+ *
+ * This helper function waits for VLAN table to be ready for access.
+ */
+static int wait_for_vlan_table(struct ksz_sw *sw)
+{
+	u32 ctrl;
+	int timeout = 100;
+
+	do {
+		--timeout;
+		if (!timeout)
+			return 1;
+		ctrl = sw->reg->r8(sw, REG_SW_VLAN_CTRL);
+	} while (ctrl & VLAN_START);
+	return 0;
+}  /* wait_for_vlan_table */
+
+#ifdef CONFIG_KSZ_HSR
+/**
+ * get_hsr_table_info - Get HSR table information
+ * @hsr:	Buffer to store the HSR table entry.
+ * @data:	Buffer holding HSR table information.
+ *
+ * This helper routine retrieves information from HSR table entry data.
+ */
+static void get_hsr_table_info(struct ksz_hsr_table *hsr, u32 data[])
+{
+	u8 *byte;
+	u16 *word;
+
+	hsr->valid = !!(data[0] & HSR_V_STATIC_VALID);
+	hsr->age_cnt = (data[0] >> HSR_V_AGE_CNT_S) & HSR_V_AGE_CNT_M;
+	hsr->path_id = (data[0] & HSR_V_PATH_ID_M);
+	data[1] = htonl(data[1]);
+	memcpy(hsr->dst_mac, &data[1], 4);
+	byte = (u8 *) &data[2];
+	data[2] = htonl(data[2]);
+	memcpy(&hsr->dst_mac[4], &byte[0], 2);
+	memcpy(&hsr->src_mac[0], &byte[2], 2);
+	data[3] = htonl(data[3]);
+	memcpy(&hsr->src_mac[2], &data[3], 4);
+	word = (u16 *) &data[4];
+	memcpy(&hsr->start_seq[1], word, 2);
+	word++;
+	memcpy(&hsr->start_seq[0], word, 2);
+	word++;
+	memcpy(&hsr->exp_seq[1], word, 2);
+	word++;
+	memcpy(&hsr->exp_seq[0], word, 2);
+	word++;
+	memcpy(&hsr->seq_cnt[1], word, 2);
+	word++;
+	memcpy(&hsr->seq_cnt[0], word, 2);
+	word++;
+}  /* get_hsr_table_info */
+
+/**
+ * set_hsr_table_info - Set HSR table information
+ * @hsr:	The HSR table entry.
+ * @data:	Buffer to hold HSR table information.
+ *
+ * This helper routine puts information to HSR table entry.
+ */
+static void set_hsr_table_info(struct ksz_hsr_table *hsr, u32 data[])
+{
+	u8 *byte;
+	u16 *word;
+
+	data[0] = (u32)(hsr->age_cnt & HSR_V_AGE_CNT_M) << HSR_V_AGE_CNT_S;
+	data[0] |= hsr->path_id & HSR_V_PATH_ID_M;
+	if (hsr->valid)
+		data[0] |= HSR_V_STATIC_VALID;
+	memcpy(&data[1], hsr->dst_mac, 4);
+	data[1] = htonl(data[1]);
+	byte = (u8 *) &data[2];
+	memcpy(&byte[0], &hsr->dst_mac[4], 2);
+	memcpy(&byte[2], &hsr->src_mac[0], 2);
+	data[2] = htonl(data[2]);
+	memcpy(&data[3], &hsr->src_mac[2], 4);
+	data[3] = htonl(data[3]);
+	word = (u16 *) &data[4];
+	memcpy(word, &hsr->start_seq[1], 2);
+	word++;
+	memcpy(word, &hsr->start_seq[0], 2);
+	word++;
+	memcpy(word, &hsr->exp_seq[1], 2);
+	word++;
+	memcpy(word, &hsr->exp_seq[0], 2);
+	word++;
+	memcpy(word, &hsr->seq_cnt[1], 2);
+	word++;
+	memcpy(word, &hsr->seq_cnt[0], 2);
+	word++;
+}  /* set_hsr_table_info */
+
+/**
+ * wait_for_hsr_table - Wait for HSR table to be ready
+ * @sw:		The switch instance.
+ *
+ * This helper function waits for HSR table to be ready for access.
+ */
+static int wait_for_hsr_table(struct ksz_sw *sw)
+{
+	u32 ctrl;
+	int timeout = 100;
+
+	do {
+		--timeout;
+		if (!timeout)
+			return 1;
+		ctrl = sw->reg->r32(sw, REG_HSR_ALU_CTRL__4);
+	} while ((ctrl & HSR_START) || HSR_SEARCH == (ctrl & HSR_ACTION));
+	return 0;
+}  /* wait_for_hsr_table */
+#endif
+
+/* -------------------------------------------------------------------------- */
+
+/* Switch functions */
+
+#ifndef CONFIG_KSZ_IBA_ONLY
+/**
+ * sw_r_mac_table - read from MAC table
+ * @sw:		The switch instance.
+ * @mac:	Buffer to store the MAC table entry.
+ *
+ * This helper function reads an entry of the MAC table of the switch.
+ */
+static void sw_r_mac_table(struct ksz_sw *sw, struct ksz_mac_table *mac)
+{
+	u32 data[4];
+
+	sw->reg->r(sw, REG_SW_ALU_VAL_A, data, 4 * 4);
+	data[0] = be32_to_cpu(data[0]);
+	data[1] = be32_to_cpu(data[1]);
+	data[2] = be32_to_cpu(data[2]);
+	data[3] = be32_to_cpu(data[3]);
+	get_mac_table_info(mac, data);
+}  /* sw_r_mac_table */
+
+/**
+ * sw_w_mac_table - write to MAC table
+ * @sw:		The switch instance.
+ * @mac:	The MAC table entry.
+ *
+ * This helper function writes an entry of the MAC table of the switch.
+ */
+static void sw_w_mac_table(struct ksz_sw *sw, struct ksz_mac_table *mac)
+{
+	u32 data[4];
+
+	set_mac_table_info(mac, data);
+	data[0] = cpu_to_be32(data[0]);
+	data[1] = cpu_to_be32(data[1]);
+	data[2] = cpu_to_be32(data[2]);
+	data[3] = cpu_to_be32(data[3]);
+	sw->reg->w(sw, REG_SW_ALU_VAL_A, data, 4 * 4);
+}  /* sw_w_mac_table */
+
+/**
+ * sw_s_dyn_mac_table - prepare dynmaic MAC table for access
+ * @sw:		The switch instance.
+ * @addr:	The address of the table entry.
+ * @src_addr:	The source address.
+ * @src_fid:	The source FID.
+ *
+ * This helper function prepares dynmaic MAC table for access.
+ */
+static u32 sw_s_dyn_mac_table(struct ksz_sw *sw, u16 addr, u8 *src_addr,
+	u16 src_fid)
+{
+	u32 ctrl;
+	u32 data;
+
+	ctrl = 0;
+	if (addr) {
+		data = (addr - 1) & ALU_DIRECT_INDEX_M;
+		sw->reg->w32(sw, REG_SW_ALU_INDEX_1, data);
+		ctrl |= ALU_DIRECT;
+	} else {
+		data = (u32) src_fid << ALU_FID_INDEX_S;
+		data |= ((u32) src_addr[0] << 8) | src_addr[1];
+		sw->reg->w32(sw, REG_SW_ALU_INDEX_0, data);
+		data = ((u32) src_addr[2] << 24) |
+			((u32) src_addr[3] << 16) |
+			((u32) src_addr[4] << 8) | src_addr[5];
+		sw->reg->w32(sw, REG_SW_ALU_INDEX_1, data);
+	}
+	ctrl |= ALU_START;
+	return ctrl;
+}  /* sw_s_dyn_mac_table */
+
+/**
+ * sw_r_dyn_mac_hw - read from dynamic MAC table using default access
+ * @sw:		The switch instance.
+ * @addr:	The address of the table entry.
+ * @src_addr:	The source address.
+ * @src_fid:	The source FID.
+ * @mac:	Buffer to store the MAC table entry.
+ * @entry:	Buffer to store the actual entry if available.
+ *
+ * This function reads an entry of the dynamic MAC table using default access.
+ */
+static int sw_r_dyn_mac_hw(struct ksz_sw *sw, u16 addr, u8 *src_addr,
+	u16 src_fid, struct ksz_mac_table *mac, u16 *entry)
+{
+	u32 ctrl;
+
+	ctrl = sw_s_dyn_mac_table(sw, addr, src_addr, src_fid);
+	ctrl |= ALU_READ;
+
+	/* Dynamic MAC table does not use USE_FID bit and does not clear it. */
+	sw->reg->w32(sw, REG_SW_ALU_VAL_B, 0);
+	sw->reg->w32(sw, REG_SW_ALU_CTRL__4, ctrl);
+	do {
+		ctrl = sw->reg->r32(sw, REG_SW_ALU_CTRL__4);
+	} while (ctrl & ALU_START);
+	sw_r_mac_table(sw, mac);
+
+	/* Hash read. */
+	if (!addr && entry)
+		*entry = (sw->reg->r16(sw, REG_SW_LUE_INDEX_0__2) &
+			ENTRY_INDEX_M) + 1;
+	return 1;
+}  /* sw_r_dyn_mac_hw */
+
+/**
+ * sw_w_dyn_mac_hw - write to dynamic MAC table using default access
+ * @sw:		The switch instance.
+ * @addr:	The address of the table entry.
+ * @src_addr:	The source address.
+ * @src_fid:	The source FID.
+ * @mac:	The MAC table entry.
+ *
+ * This function writes an entry of the dynamic MAC table using default access.
+ */
+static int sw_w_dyn_mac_hw(struct ksz_sw *sw, u16 addr, u8 *src_addr,
+	u16 src_fid, struct ksz_mac_table *mac)
+{
+	u32 ctrl;
+
+	ctrl = sw_s_dyn_mac_table(sw, addr, src_addr, src_fid);
+	ctrl |= ALU_WRITE;
+	sw_w_mac_table(sw, mac);
+	sw->reg->w32(sw, REG_SW_ALU_CTRL__4, ctrl);
+	return 1;
+}  /* sw_w_dyn_mac_hw */
+
+/**
+ * sw_start_dyn_mac_hw - start dynamic MAC table search using default access
+ * @sw:		The switch instance.
+ *
+ * This function starts dynamic MAC table search using default access.
+ */
+static int sw_start_dyn_mac_hw(struct ksz_sw *sw)
+{
+	u32 ctrl;
+
+	ctrl = ALU_SEARCH;
+	ctrl |= ALU_START;
+
+	/* Dynamic MAC table does not use USE_FID bit and does not clear it. */
+	sw->reg->w32(sw, REG_SW_ALU_VAL_B, 0);
+	sw->reg->w32(sw, REG_SW_ALU_CTRL__4, ctrl);
+	return 1;
+}  /* sw_start_dyn_mac_hw */
+
+/**
+ * sw_g_dyn_mac_hw - retrieve dynamic MAC table result using default access
+ * @sw:		The switch instance.
+ * @mac:	Buffer to store the MAC table entry.
+ *
+ * This function retrieves dynamic MAC table result using default access.
+ */
+static int sw_g_dyn_mac_hw(struct ksz_sw *sw, struct ksz_mac_table *mac)
+{
+	sw_r_mac_table(sw, mac);
+	return 1;
+}  /* sw_g_dyn_mac_hw */
+
+/**
+ * sw_stop_dyn_mac_hw - stop dynamic MAC table search using default access
+ * @sw:		The switch instance.
+ *
+ * This function stops dynamic MAC table search using default access.
+ *
+ * Return the last MAC table control.
+ */
+static u32 sw_stop_dyn_mac_hw(struct ksz_sw *sw)
+{
+	u32 ctrl;
+
+	ctrl = 0;
+	sw->reg->w32(sw, REG_SW_ALU_CTRL__4, ctrl);
+	ctrl = sw->reg->r32(sw, REG_SW_ALU_CTRL__4);
+	return ctrl;
+}  /* sw_stop_dyn_mac_hw */
+
+/**
+ * sw_r_sta_mac_hw - read from static MAC table using default access
+ * @sw:		The switch instance.
+ * @ctrl:	The control values for read operation.
+ * @num:	Number of entries to read.
+ * @mac:	Buffer to hold the MAC table entries.
+ *
+ * This function reads from static MAC table using default access.
+ */
+static int sw_r_sta_mac_hw(struct ksz_sw *sw, u32 ctrl[], int num,
+	struct ksz_mac_table *mac)
+{
+	u32 status;
+	int cnt = 0;
+
+	do {
+		sw->reg->w32(sw, REG_SW_ALU_STAT_CTRL__4, *ctrl);
+		do {
+			status = sw->reg->r32(sw, REG_SW_ALU_STAT_CTRL__4);
+		} while (status & ALU_STAT_START);
+		sw_r_mac_table(sw, mac);
+		++cnt;
+		++ctrl;
+		++mac;
+	} while (cnt < num);
+	return 1;
+}  /* sw_r_sta_mac_hw */
+
+/**
+ * sw_w_sta_mac_hw - write to static MAC table using default access
+ * @sw:		The switch instance.
+ * @ctrl:	The control values for write operation.
+ * @num:	Number of entries to write.
+ * @mac:	MAC table entries.
+ *
+ * This function writes to static MAC table using default access.
+ */
+static int sw_w_sta_mac_hw(struct ksz_sw *sw, u32 ctrl[], int num,
+	struct ksz_mac_table *mac)
+{
+	u32 status;
+	int cnt;
+
+	for (cnt = 0; cnt < num; cnt++, ctrl++, mac++) {
+		sw_w_mac_table(sw, mac);
+		sw->reg->w32(sw, REG_SW_ALU_STAT_CTRL__4, *ctrl);
+		do {
+			status = sw->reg->r32(sw, REG_SW_ALU_STAT_CTRL__4);
+		} while (status & ALU_STAT_START);
+	}
+	return 1;
+}  /* sw_w_sta_mac_hw */
+#endif
+
+/**
+ * sw_r_dyn_mac_table - read from dynamic MAC table
+ * @sw:		The switch instance.
+ * @addr:	The address of the table entry.
+ * @src_addr:	The source address.
+ * @src_fid:	The source FID.
+ * @mac:	Buffer to store the MAC table entry.
+ * @entry:	Buffer to store the actual entry if available.
+ *
+ * This routine reads an entry of the dynamic MAC table of the switch.
+ * ALU table is locked to prevent corruption of read data.
+ */
+static void sw_r_dyn_mac_table(struct ksz_sw *sw, u16 addr, u8 *src_addr,
+	u16 src_fid, struct ksz_mac_table *mac, u16 *entry)
+{
+	if (entry)
+		*entry = 0;
+	mutex_lock(&sw->alulock);
+	sw->ops->acquire(sw);
+	if (wait_for_dyn_mac_table(sw))
+		goto r_dyn_exit;
+	sw->reg->r_dyn_mac_hw(sw, addr, src_addr, src_fid, mac, entry);
+
+r_dyn_exit:
+	sw->ops->release(sw);
+	mutex_unlock(&sw->alulock);
+	mac->dirty = 0;
+}  /* sw_r_dyn_mac_table */
+
+/**
+ * sw_w_dyn_mac_table - write to dynamic MAC table
+ * @sw:		The switch instance.
+ * @addr:	The address of the table entry.
+ * @src_addr:	The source address.
+ * @src_fid:	The source FID.
+ * @mac:	The MAC table entry.
+ *
+ * This routine writes an entry of the dynamic MAC table of the switch.
+ * ALU table is locked to prevent corruption of read data.
+ */
+static void sw_w_dyn_mac_table(struct ksz_sw *sw, u16 addr, u8 *src_addr,
+	u16 src_fid, struct ksz_mac_table *mac)
+{
+	mutex_lock(&sw->alulock);
+	sw->ops->acquire(sw);
+	if (wait_for_dyn_mac_table(sw))
+		goto w_dyn_exit;
+
+	/* Dynamic MAC table does not use USE_FID bit. */
+	mac->use_fid = 0;
+	sw->reg->w_dyn_mac_hw(sw, addr, src_addr, src_fid, mac);
+
+w_dyn_exit:
+	sw->ops->release(sw);
+	mutex_unlock(&sw->alulock);
+	mac->dirty = 0;
+}  /* sw_w_dyn_mac_table */
+
+/**
+ * sw_start_dyn_mac_table - start dynamic MAC table search
+ * @sw:		The switch instance.
+ *
+ * This routine starts dynamic MAC table search.
+ * ALU table is locked to prevent corruption of read data.
+ */
+static void sw_start_dyn_mac_table(struct ksz_sw *sw)
+{
+	mutex_lock(&sw->alulock);
+	sw->ops->acquire(sw);
+	if (wait_for_dyn_mac_table(sw))
+		goto s_dyn_exit;
+	sw->reg->start_dyn_mac_hw(sw);
+
+s_dyn_exit:
+	sw->ops->release(sw);
+}  /* sw_start_dyn_mac_table */
+
+/**
+ * sw_g_dyn_mac_table - retrieve dynamic MAC table result
+ * @sw:		The switch instance.
+ * @addr:	The address of the table entry.
+ * @mac:	Buffer to store the MAC table entry.
+ *
+ * This function retrieves an entry of the dynamic MAC table search result.
+ *
+ * Return 0 if the entry is successfully read; otherwise -1.
+ */
+static int sw_g_dyn_mac_table(struct ksz_sw *sw, u16 *addr,
+	struct ksz_mac_table *mac)
+{
+	u32 ctrl;
+	int rc = 0;
+
+	sw->ops->acquire(sw);
+	do {
+		ctrl = sw->reg->r32(sw, REG_SW_ALU_CTRL__4);
+	} while (!(ctrl & ALU_VALID) && (ctrl & ALU_START));
+	if (ctrl & ALU_VALID) {
+		ctrl >>= ALU_VALID_CNT_S;
+		ctrl &= ALU_VALID_CNT_M;
+		rc = !sw->reg->g_dyn_mac_hw(sw, mac);
+		if (ctrl != *addr + 1)
+			*addr = ctrl - 1;
+	} else
+		rc = -1;
+	sw->ops->release(sw);
+	return rc;
+}  /* sw_g_dyn_mac_table */
+
+/**
+ * sw_stop_dyn_mac_table - stop dynamic MAC table search
+ * @sw:		The switch instance.
+ * @addr:	The address of the last table entry.
+ *
+ * This routine stops dynamic MAC table search.
+ */
+static void sw_stop_dyn_mac_table(struct ksz_sw *sw, u16 addr)
+{
+	u32 ctrl;
+
+	sw->ops->acquire(sw);
+	ctrl = sw->reg->stop_dyn_mac_hw(sw);
+	ctrl >>= ALU_VALID_CNT_S;
+	ctrl &= ALU_VALID_CNT_M;
+if (ctrl != addr)
+dbg_msg(" ?stop dyn: %x %x"NL, ctrl, addr);
+	sw->ops->release(sw);
+	mutex_unlock(&sw->alulock);
+}  /* sw_stop_dyn_mac_table */
+
+/**
+ * sw_d_dyn_mac_table - dump dynamic MAC table
+ * @sw:		The switch instance.
+ *
+ * This routine dumps dynamic MAC table contents.
+ */
+static ssize_t sw_d_dyn_mac_table(struct ksz_sw *sw, char *buf, ssize_t len)
+{
+	u16 i;
+	u32 ports;
+	struct ksz_mac_table mac;
+	int first_break = true;
+
+	sw_start_dyn_mac_table(sw);
+	i = 0;
+	do {
+		if (sw_g_dyn_mac_table(sw, &i, &mac))
+			break;
+		ports = mac.ports;
+		ports = get_log_mask_from_phy(sw, ports);
+		if (len >= MAX_SYSFS_BUF_SIZE && first_break) {
+			first_break = false;
+			len += sprintf(buf + len, "..."NL);
+		}
+		if (len < MAX_SYSFS_BUF_SIZE)
+		len += sprintf(buf + len,
+			"%4x: %02X:%02X:%02X:%02X:%02X:%02X  "
+			"%04x  m:%u  t:%u  s:%u  d:%u  o:%u  %02x  [%u]"NL,
+			i, mac.addr[0], mac.addr[1], mac.addr[2],
+			mac.addr[3], mac.addr[4], mac.addr[5],
+			ports, mac.mstp, mac.prio, mac.src, mac.dst,
+			mac.override, mac.fid, mac.valid);
+		else {
+		printk(KERN_INFO
+			"%4x: %02X:%02X:%02X:%02X:%02X:%02X  "
+			"%04x  m:%u  t:%u  s:%u  d:%u  o:%u  %02x  [%u]"NL,
+			i, mac.addr[0], mac.addr[1], mac.addr[2],
+			mac.addr[3], mac.addr[4], mac.addr[5],
+			ports, mac.mstp, mac.prio, mac.src, mac.dst,
+			mac.override, mac.fid, mac.valid);
+		yield();
+		}
+		i++;
+	} while (1);
+	sw_stop_dyn_mac_table(sw, i);
+	return len;
+}  /* sw_d_dyn_mac_table */
+
+static u8 mcast_reserved_map[RESERVED_MCAST_TABLE_ENTRIES] = {
+	0, 1, 6, 2, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,
+	3, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,
+	4, 5, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,
+};
+
+static u32 get_mac_table_ctrl(u16 addr, int mcast)
+{
+	u32 ctrl;
+
+	ctrl = addr;
+	if (mcast)
+		ctrl &= ALU_RESV_MCAST_INDEX_M;
+	else
+		ctrl &= ALU_STAT_INDEX_M;
+	ctrl <<= ALU_STAT_INDEX_S;
+	if (mcast)
+		ctrl |= ALU_RESV_MCAST_ADDR;
+	ctrl |= ALU_STAT_START;
+	return ctrl;
+}  /* get_mac_table_ctrl */
+
+/**
+ * sw_r_sta_mac_table - read from static MAC table
+ * @sw:		The switch instance.
+ * @addr:	The address of the table entry.
+ * @mcast:	Multicast reserved table indication.
+ * @mac:	Buffer to store the MAC table entry.
+ *
+ * This routine reads an entry of the static MAC table of the switch.
+ */
+static void sw_r_sta_mac_table(struct ksz_sw *sw, u16 addr, int mcast,
+	struct ksz_mac_table *mac)
+{
+	u32 ctrl;
+
+	mutex_lock(&sw->alulock);
+	sw->ops->acquire(sw);
+	if (wait_for_sta_mac_table(sw))
+		goto r_sta_exit;
+	ctrl = get_mac_table_ctrl(addr, mcast);
+	ctrl |= ALU_STAT_READ;
+	sw->reg->r_sta_mac_hw(sw, &ctrl, 1, mac);
+
+r_sta_exit:
+	sw->ops->release(sw);
+	mutex_unlock(&sw->alulock);
+	if (mcast) {
+		mac->addr[0] = 0x01;
+		mac->addr[1] = 0x80;
+		mac->addr[2] = 0xC2;
+		mac->addr[3] = 0x00;
+		mac->addr[4] = 0x00;
+		mac->addr[5] = addr;
+		mac->valid = 1;
+	}
+	mac->dirty = 0;
+}  /* sw_r_sta_mac_table */
+
+/**
+ * sw_r_m_sta_mac_table - read many from static MAC table
+ * @sw:		The switch instance.
+ * @addr:	The addresses of the table entries.
+ * @mcast:	Multicast reserved table indication.
+ * @mac:	Buffer to store the MAC table entries.
+ *
+ * This routine reads several entries of the static MAC table of the switch.
+ */
+static void sw_r_m_sta_mac_table(struct ksz_sw *sw, u16 addr[], int mcast,
+	int num, struct ksz_mac_table *mac)
+{
+	u32 ctrl[MAX_IBA_MAC_ENTRIES];
+	u16 max_addr;
+	int i;
+	int j;
+	int rc;
+	int cnt = MAX_IBA_MAC_ENTRIES;
+
+	if (mcast)
+		max_addr = RESERVED_MCAST_TABLE_ENTRIES;
+	else
+		max_addr = STATIC_MAC_TABLE_ENTRIES;
+	memset(mac, 0, sizeof(struct ksz_mac_table) * num);
+	mutex_lock(&sw->alulock);
+	sw->ops->acquire(sw);
+	if (wait_for_sta_mac_table(sw))
+		goto r_m_sta_exit;
+	for (i = 0; i < num; i += MAX_IBA_MAC_ENTRIES) {
+#if 1
+		if (addr[i] >= max_addr)
+			break;
+#endif
+		if (cnt > max_addr - addr[i])
+			cnt = max_addr - addr[i];
+		if (cnt > num - i)
+			cnt = num - i;
+		for (j = 0; j < cnt; j++) {
+			ctrl[j] = get_mac_table_ctrl(addr[i + j], mcast);
+			ctrl[j] |= ALU_STAT_READ;
+		}
+		rc = sw->reg->r_sta_mac_hw(sw, ctrl, cnt, mac);
+		if (!rc)
+			break;
+		for (j = 0; j < cnt; j++) {
+			if (mcast) {
+				mac->addr[0] = 0x01;
+				mac->addr[1] = 0x80;
+				mac->addr[2] = 0xC2;
+				mac->addr[3] = 0x00;
+				mac->addr[4] = 0x00;
+				mac->addr[5] = addr[i + j];
+				mac->valid = 1;
+			}
+			++mac;
+		}
+	}
+
+r_m_sta_exit:
+	sw->ops->release(sw);
+	mutex_unlock(&sw->alulock);
+}  /* sw_r_m_sta_mac_table */
+
+/**
+ * sw_w_sta_mac_table - write to static MAC table
+ * @sw:		The switch instance.
+ * @addr:	The address of the table entry.
+ * @mcast:	Multicast reserved table indication.
+ * @mac:	The MAC table entry.
+ *
+ * This routine writes an entry of the static MAC table of the switch.
+ */
+static void sw_w_sta_mac_table(struct ksz_sw *sw, u16 addr, int mcast,
+	struct ksz_mac_table *mac)
+{
+	u32 ctrl;
+
+	mutex_lock(&sw->alulock);
+	sw->ops->acquire(sw);
+	if (wait_for_sta_mac_table(sw))
+		goto w_sta_exit;
+	ctrl = get_mac_table_ctrl(addr, mcast);
+	sw->reg->w_sta_mac_hw(sw, &ctrl, 1, mac);
+
+w_sta_exit:
+	sw->ops->release(sw);
+	mutex_unlock(&sw->alulock);
+	mac->dirty = 0;
+}  /* sw_w_sta_mac_table */
+
+/**
+ * sw_w_m_sta_mac_table - write many to static MAC table
+ * @sw:		The switch instance.
+ * @addr:	The addresses of the table entries.
+ * @mcast:	Multicast reserved table indication.
+ * num:		Numbe of entries.
+ * @mac:	The MAC table entries.
+ *
+ * This routine writes several entries of the static MAC table of the switch.
+ */
+static void sw_w_m_sta_mac_table(struct ksz_sw *sw, u16 addr[], int mcast,
+	int num, struct ksz_mac_table *mac)
+{
+	u32 ctrl[MAX_IBA_MAC_ENTRIES];
+	int i;
+	int j;
+	int rc;
+	int cnt = MAX_IBA_MAC_ENTRIES;
+
+	mutex_lock(&sw->alulock);
+	sw->ops->acquire(sw);
+	if (wait_for_sta_mac_table(sw))
+		goto w_m_sta_exit;
+	for (i = 0; i < num; i += MAX_IBA_MAC_ENTRIES) {
+		if (cnt > num - i)
+			cnt = num - i;
+		for (j = 0; j < cnt; j++) {
+			ctrl[j] = get_mac_table_ctrl(addr[i + j], mcast);
+		}
+		rc = sw->reg->w_sta_mac_hw(sw, ctrl, cnt, mac);
+		if (!rc)
+			break;
+		mac += MAX_IBA_MAC_ENTRIES;
+	}
+
+w_m_sta_exit:
+	sw->ops->release(sw);
+	mutex_unlock(&sw->alulock);
+}  /* sw_w_m_sta_mac_table */
+
+static int get_mcast_reserved_addr(u8 group)
+{
+	int i;
+
+	for (i = 0; i < RESERVED_MCAST_TABLE_ENTRIES; i++)
+		if (group == mcast_reserved_map[i])
+			return i;
+	return -1;
+}  /* get_mcast_reserved_addr */
+
+/**
+ * sw_d_sta_mac_table - dump static MAC table
+ * @sw:		The switch instance.
+ *
+ * This routine dumps static MAC table contents.
+ */
+static ssize_t sw_d_sta_mac_table(struct ksz_sw *sw, char *sbuf, ssize_t slen)
+{
+	int i;
+	int j;
+	int seen;
+	int len;
+	char buf[120];
+	u16 addr[8];
+	u16 ports;
+	struct ksz_mac_table *mac;
+	struct ksz_mac_table table[8];
+	int first_static = true;
+
+	for (j = 0; j < 8; j++)
+		addr[j] = get_mcast_reserved_addr(j);
+	sw_r_m_sta_mac_table(sw, addr, 1, 8, table);
+	for (j = 0; j < 8; j++) {
+		mac = &table[j];
+		if (!mac->valid)
+			continue;
+		ports = mac->ports;
+		ports = get_log_mask_from_phy(sw, ports);
+		len = 0;
+		len += sprintf(buf + len,
+			"%2x: %02X:%02X:%02X:%02X:%02X:%02X  "
+			"%04x  m:%u  p:%u  s:%u  d:%u  o:%u  %u:%02x  <",
+			j,
+			mac->addr[0], mac->addr[1], mac->addr[2],
+			mac->addr[3], mac->addr[4], mac->addr[5],
+			ports, mac->mstp, mac->prio,
+			mac->src, mac->dst,
+			mac->override, mac->use_fid, mac->fid);
+
+		seen = 0;
+		for (i = 0; i < RESERVED_MCAST_TABLE_ENTRIES; i++) {
+			if (j == mcast_reserved_map[i]) {
+				if (!seen) {
+					if (i != mac->addr[5])
+						len += sprintf(buf + len, " ");
+					len += sprintf(buf + len,
+						"%02X", i);
+					seen = i + 1;
+				}
+			} else if (seen) {
+				if (i != seen)
+					len += sprintf(buf + len,
+						"..%02X", i - 1);
+				seen = 0;
+			}
+		}
+		if (seen && i != seen)
+			len += sprintf(buf + len, "..%02X", i - 1);
+		len += sprintf(buf + len, ">");
+		slen += sprintf(sbuf + slen, "%s"NL, buf);
+	}
+	i = 0;
+	do {
+		for (j = 0; j < MAX_IBA_MAC_ENTRIES; j++)
+			addr[j] = i + j;
+		sw_r_m_sta_mac_table(sw, addr, 0, MAX_IBA_MAC_ENTRIES,
+			table);
+		for (j = 0; j < MAX_IBA_MAC_ENTRIES; j++) {
+			mac = &table[j];
+			if (!mac->valid)
+				continue;
+			ports = mac->ports;
+			ports = get_log_mask_from_phy(sw, ports);
+			if (first_static) {
+				first_static = false;
+				slen += sprintf(sbuf + slen, NL);
+			}
+			slen += sprintf(sbuf + slen,
+				"%2x: %02X:%02X:%02X:%02X:%02X:%02X  "
+				"%04x  m:%u  p:%u  s:%u  d:%u  o:%u  %u:%02x"NL,
+				i + j,
+				mac->addr[0], mac->addr[1], mac->addr[2],
+				mac->addr[3], mac->addr[4], mac->addr[5],
+				ports, mac->mstp, mac->prio,
+				mac->src, mac->dst,
+				mac->override, mac->use_fid, mac->fid);
+		}
+		i += MAX_IBA_MAC_ENTRIES;
+	} while (i < STATIC_MAC_TABLE_ENTRIES);
+	return slen;
+}  /* sw_d_sta_mac_table */
+
+static ssize_t sw_d_mac_table(struct ksz_sw *sw, char *buf, ssize_t len)
+{
+	struct ksz_mac_table *mac;
+	struct ksz_alu_table *alu;
+	u16 ports;
+	int i;
+	int first_static = true;
+	int first_break = true;
+
+	i = STATIC_MAC_TABLE_ENTRIES;
+	do {
+		alu = &sw->info->alu_table[i];
+		if (alu->valid) {
+			if (len >= MAX_SYSFS_BUF_SIZE && first_break) {
+				first_break = false;
+				len += sprintf(buf + len, "..."NL);
+			}
+			if (first_static) {
+				first_static = false;
+				if (len < MAX_SYSFS_BUF_SIZE)
+					len += sprintf(buf + len, NL);
+				else
+					printk(KERN_INFO NL);
+			}
+			mac = &sw->info->mac_table[i];
+			ports = mac->ports;
+			ports = get_log_mask_from_phy(sw, ports);
+			if (len < MAX_SYSFS_BUF_SIZE)
+			len += sprintf(buf + len,
+				"%2x: %02X:%02X:%02X:%02X:%02X:%02X  "
+				"%04x  m:%u  p:%u  s:%u  d:%u  o:%u  %u:%02x  "
+				"%02x:%02x"NL,
+				i,
+				mac->addr[0], mac->addr[1], mac->addr[2],
+				mac->addr[3], mac->addr[4], mac->addr[5],
+				ports, mac->mstp, mac->prio,
+				mac->src, mac->dst,
+				mac->override, mac->use_fid, mac->fid,
+				alu->forward, alu->owner);
+			else
+			printk(KERN_INFO
+				"%2x: %02X:%02X:%02X:%02X:%02X:%02X  "
+				"%04x  m:%u  p:%u  s:%u  d:%u  o:%u  %u:%02x  "
+				"%02x:%02x"NL,
+				i,
+				mac->addr[0], mac->addr[1], mac->addr[2],
+				mac->addr[3], mac->addr[4], mac->addr[5],
+				ports, mac->mstp, mac->prio,
+				mac->src, mac->dst,
+				mac->override, mac->use_fid, mac->fid,
+				alu->forward, alu->owner);
+		}
+		i++;
+		if (SWITCH_MAC_TABLE_ENTRIES == i)
+			first_static = true;
+	} while (i < MULTI_MAC_TABLE_ENTRIES);
+	return len;
+}  /* sw_d_mac_table */
+
+/* -------------------------------------------------------------------------- */
+
+#ifndef CONFIG_KSZ_IBA_ONLY
+/**
+ * sw_r_vlan_hw - read from VLAN table using default access
+ * @sw:		The switch instance.
+ * @data:	Buffer to hold the VLAN data.
+ * @num:	Number of entries to read.
+ *
+ * This function reads from VLAN table using default access.
+ */
+static int sw_r_vlan_hw(struct ksz_sw *sw, u32 data[], int num)
+{
+	u8 ctrl;
+	int cnt = 0;
+	u16 addr = data[READ_VLAN_ENTRY_SIZE];
+
+	do {
+		sw->reg->w16(sw, REG_SW_VLAN_ENTRY_INDEX__2, addr);
+		ctrl = VLAN_READ;
+		ctrl |= VLAN_START;
+		sw->reg->w8(sw, REG_SW_VLAN_CTRL, ctrl);
+		do {
+			ctrl = sw->reg->r8(sw, REG_SW_VLAN_CTRL);
+		} while (ctrl & VLAN_START);
+		data[0] = sw->reg->r32(sw, REG_SW_VLAN_ENTRY__4);
+		data[1] = 0;
+		data[2] = 0;
+		if (data[0] & VLAN_VALID) {
+			sw->reg->r(sw, REG_SW_VLAN_ENTRY_UNTAG__4, &data[1],
+				   4 * 2);
+			data[1] = be32_to_cpu(data[1]);
+			data[2] = be32_to_cpu(data[2]);
+		}
+		++cnt;
+		++addr;
+		data += READ_VLAN_ENTRY_SIZE;
+	} while (cnt < num);
+	return 1;
+}  /* sw_r_vlan_hw */
+
+/**
+ * sw_w_vlan_hw - write to VLAN table using default access
+ * @sw:		The switch instance.
+ * @data:	VLAN data to write.
+ * @num:	Number of entries to write.
+ *
+ * This function writes to VLAN table using default access.
+ */
+static int sw_w_vlan_hw(struct ksz_sw *sw, u32 data[], int num)
+{
+	u8 ctrl;
+	int cnt;
+
+	for (cnt = 0; cnt < num; cnt++) {
+		data[0] = cpu_to_be32(data[0]);
+		data[1] = cpu_to_be32(data[1]);
+		data[2] = cpu_to_be32(data[2]);
+		data[3] = cpu_to_be16(data[3]);
+		sw->reg->w(sw, REG_SW_VLAN_ENTRY__4, data, 4 * 3 + 4);
+		ctrl = VLAN_WRITE;
+		ctrl |= VLAN_START;
+		sw->reg->w8(sw, REG_SW_VLAN_CTRL, ctrl);
+		do {
+			ctrl = sw->reg->r8(sw, REG_SW_VLAN_CTRL);
+		} while (ctrl & VLAN_START);
+		data += WRITE_VLAN_ENTRY_SIZE;
+	}
+	return 1;
+}  /* sw_w_vlan_hw */
+#endif
+
+/**
+ * sw_r_vlan_table - read from VLAN table
+ * @sw:		The switch instance.
+ * @addr:	The address of the table entry.
+ * @vlan:	Buffer to store the VLAN table entry.
+ *
+ * This function reads an entry of the VLAN table of the switch.
+ *
+ * Return 0 if the entry is valid; otherwise -1.
+ */
+static int sw_r_vlan_table(struct ksz_sw *sw, u16 addr,
+	struct ksz_vlan_table *vlan)
+{
+	u32 data[READ_VLAN_ENTRY_SIZE + 1];
+	int rc = -1;
+
+	mutex_lock(&sw->vlanlock);
+	sw->ops->acquire(sw);
+	if (wait_for_vlan_table(sw))
+		goto r_vlan_exit;
+	data[READ_VLAN_ENTRY_SIZE] = addr;
+	sw->reg->r_vlan_hw(sw, data, 1);
+	get_vlan_table_info(vlan, data);
+
+r_vlan_exit:
+	sw->ops->release(sw);
+	mutex_unlock(&sw->vlanlock);
+	if (vlan->valid)
+		rc = 0;
+	vlan->vid = addr;
+	vlan->dirty = 0;
+	return rc;
+}  /* sw_r_vlan_table */
+
+/**
+ * sw_r_m_vlan_table - read many from VLAN table
+ * @sw:		The switch instance.
+ * @addr:	The starting address of the table entries.
+ * @num:	Number of entries.
+ * @vlan:	Buffer to store the VLAN table entries.
+ *
+ * This routine reads several entries of the VLAN table of the switch.
+ */
+static void sw_r_m_vlan_table(struct ksz_sw *sw, u16 addr, int num,
+	struct ksz_vlan_table *vlan)
+{
+	u32 data[READ_VLAN_ENTRY_SIZE * MAX_IBA_VLAN_ENTRIES + 1];
+	int i;
+	int j;
+	int rc;
+	int cnt = MAX_IBA_VLAN_ENTRIES;
+
+	memset(vlan, 0, sizeof(struct ksz_vlan_table) * num);
+	mutex_lock(&sw->vlanlock);
+	sw->ops->acquire(sw);
+	if (wait_for_vlan_table(sw))
+		goto r_m_vlan_exit;
+	for (i = 0; i < num; i += MAX_IBA_VLAN_ENTRIES,
+	     addr += MAX_IBA_VLAN_ENTRIES) {
+		if (addr >= 4096)
+			break;
+		if (cnt > 4096 - addr)
+			cnt = 4096 - addr;
+		if (cnt > num - i)
+			cnt = num - i;
+		data[READ_VLAN_ENTRY_SIZE] = addr;
+		rc = sw->reg->r_vlan_hw(sw, data, cnt);
+		if (!rc)
+			break;
+		for (j = 0; j < cnt; j++) {
+			get_vlan_table_info(vlan, &data[j *
+				READ_VLAN_ENTRY_SIZE]);
+			vlan->vid = addr + j;
+			vlan++;
+		}
+	}
+
+r_m_vlan_exit:
+	sw->ops->release(sw);
+	mutex_unlock(&sw->vlanlock);
+}  /* sw_r_m_vlan_table */
+
+/**
+ * sw_w_vlan_table - write to VLAN table
+ * @sw:		The switch instance.
+ * @addr:	The address of the table entry.
+ * @vlan:	The VLAN table entry.
+ *
+ * This routine writes an entry of the VLAN table of the switch.
+ */
+static void sw_w_vlan_table(struct ksz_sw *sw, u16 addr,
+	struct ksz_vlan_table *vlan)
+{
+	u32 data[4];
+	int bit;
+	int index;
+
+	mutex_lock(&sw->vlanlock);
+	sw->ops->acquire(sw);
+	if (wait_for_vlan_table(sw))
+		goto w_vlan_exit;
+	set_vlan_table_info(vlan, data);
+	data[3] = addr;
+	sw->reg->w_vlan_hw(sw, data, 1);
+	index = addr / VID_IN_DATA;
+	bit = addr % VID_IN_DATA;
+	if (vlan->valid) {
+		uint m;
+		uint n;
+		uint p;
+		struct ksz_port_cfg *cfg;
+
+		for (n = 1; n <= sw->mib_port_cnt; n++) {
+			p = get_phy_port(sw, n);
+			cfg = get_port_cfg(sw, p);
+			m = BIT(p);
+			if (vlan->untag & m)
+				cfg->untagged[index] |= (1 << bit);
+			else
+				cfg->untagged[index] &= ~(1 << bit);
+		}
+		sw->info->vid[index] |= (1 << bit);
+	} else {
+		sw->info->vid[index] &= ~(1 << bit);
+	}
+
+w_vlan_exit:
+	sw->ops->release(sw);
+	mutex_unlock(&sw->vlanlock);
+	vlan->dirty = 0;
+}  /* sw_w_vlan_table */
+
+/**
+ * sw_d_vlan_table - dump VLAN table
+ * @sw:		The switch instance.
+ *
+ * This routine dumps the VLAN table of the switch.
+ */
+static ssize_t sw_d_vlan_table(struct ksz_sw *sw, char *buf, ssize_t len)
+{
+	u16 i;
+	int j;
+	u16 ports;
+	u16 untag;
+	struct ksz_vlan_table *vlan;
+	struct ksz_vlan_table table[8];
+	int first_break = true;
+
+	i = 0;
+	do {
+		sw_r_m_vlan_table(sw, i, MAX_IBA_VLAN_ENTRIES, table);
+		for (j = 0; j < MAX_IBA_VLAN_ENTRIES; j++) {
+			vlan = &table[j];
+			if (!vlan->valid)
+				continue;
+			ports = vlan->ports;
+			untag = vlan->untag;
+			ports = get_log_mask_from_phy(sw, ports);
+			untag = get_log_mask_from_phy(sw, untag);
+			if (len >= MAX_SYSFS_BUF_SIZE && first_break) {
+				first_break = false;
+				len += sprintf(buf + len, "..."NL);
+			}
+			if (len < MAX_SYSFS_BUF_SIZE)
+			len += sprintf(buf + len,
+				"%3x: 0x%03x  m:%x  p:%x  o:%u  %04x  %04x"NL,
+				vlan->vid,
+				vlan->fid, vlan->mstp, vlan->prio,
+				vlan->option, untag, ports);
+			else
+			printk(KERN_INFO
+				"%3x: 0x%03x  m:%x  p:%x  o:%u  %04x  %04x"NL,
+				vlan->vid,
+				vlan->fid, vlan->mstp, vlan->prio,
+				vlan->option, untag, ports);
+		}
+		if (len >= MAX_SYSFS_BUF_SIZE)
+		yield();
+		i += MAX_IBA_VLAN_ENTRIES;
+	} while (i < VLAN_TABLE_ENTRIES);
+	return len;
+}  /* sw_d_vlan_table */
+
+/* -------------------------------------------------------------------------- */
+
+#ifdef CONFIG_KSZ_HSR
+#ifndef CONFIG_KSZ_IBA_ONLY
+/**
+ * sw_s_hsr_table - prepare HSR table for access
+ * @sw:		The switch instance.
+ * @addr:	The address of the table entry.
+ * @src_addr:	The source address.
+ * @path_id:	The path ID.
+ *
+ * This helper function prepares HSR table for access.
+ */
+static u32 sw_s_hsr_table(struct ksz_sw *sw, u16 addr, u8 *src_addr,
+	u8 path_id)
+{
+	u32 ctrl;
+	u32 data;
+	u16 *word;
+
+	ctrl = 0;
+	if (addr) {
+		data = (addr - 1) & HSR_DIRECT_INDEX_M;
+		sw->reg->w32(sw, REG_HSR_ALU_INDEX_2, data);
+		ctrl |= HSR_DIRECT;
+	} else {
+		data = 0;
+		sw->reg->w32(sw, REG_HSR_ALU_INDEX_0, data);
+		word = (u16 *) &data;
+		word++;
+		memcpy(word, &src_addr[0], 2);
+		data = htonl(data);
+		sw->reg->w32(sw, REG_HSR_ALU_INDEX_1, data);
+		memcpy(&data, &src_addr[2], 4);
+		data = htonl(data);
+		sw->reg->w32(sw, REG_HSR_ALU_INDEX_2, data);
+		data = path_id & HSR_PATH_INDEX_M;
+		sw->reg->w32(sw, REG_HSR_ALU_INDEX_3, data);
+	}
+	ctrl |= HSR_START;
+	return ctrl;
+}  /* sw_s_hsr_table */
+
+static void sw_get_hsr_table(struct ksz_sw *sw, struct ksz_hsr_table *hsr)
+{
+	int i;
+	u32 data[7];
+
+	sw->reg->r(sw, REG_HSR_ALU_VAL_A, data, 7 * 4);
+	for (i = 0; i < 7; i++)
+		data[i] = be32_to_cpu(data[i]);
+	get_hsr_table_info(hsr, data);
+}  /* sw_get_hsr_table */
+
+/**
+ * sw_r_hsr_hw - read from HSR table using default access
+ * @sw:		The switch instance.
+ * @addr:	The address of the table entry.
+ * @hsr:	Buffer to store the HSR table entry.
+ *
+ * This function reads an entry of the HSR table using default access.
+ */
+static int sw_r_hsr_hw(struct ksz_sw *sw, u16 addr,
+	struct ksz_hsr_table *hsr)
+{
+	u32 ctrl;
+
+	ctrl = sw_s_hsr_table(sw, addr, hsr->src_mac, hsr->path_id);
+	ctrl |= HSR_READ;
+	sw->reg->w32(sw, REG_HSR_ALU_CTRL__4, ctrl);
+	do {
+		ctrl = sw->reg->r32(sw, REG_HSR_ALU_CTRL__4);
+	} while (ctrl & HSR_START);
+	sw_get_hsr_table(sw, hsr);
+	return 1;
+}  /* sw_r_hsr_hw */
+
+/**
+ * sw_w_hsr_hw - write to HSR table using default access
+ * @sw:		The switch instance.
+ * @addr:	The address of the table entry.
+ * @hsr:	The HSR table entry.
+ *
+ * This function writes an entry of the HSR table using default access.
+ */
+static int sw_w_hsr_hw(struct ksz_sw *sw, u16 addr,
+	struct ksz_hsr_table *hsr)
+{
+	u32 ctrl;
+	u32 data[7];
+	int i;
+
+	ctrl = sw_s_hsr_table(sw, addr, hsr->src_mac, hsr->path_id);
+	ctrl |= HSR_WRITE;
+	set_hsr_table_info(hsr, data);
+	for (i = 0; i < 7; i++)
+		data[i] = cpu_to_be32(data[i]);
+	sw->reg->w(sw, REG_HSR_ALU_VAL_A, data, 7 * 4);
+	sw->reg->w32(sw, REG_HSR_ALU_CTRL__4, ctrl);
+	return 1;
+}  /* sw_w_hsr_hw */
+
+/**
+ * sw_start_hsr_hw - start HSR table search using default access
+ * @sw:		The switch instance.
+ *
+ * This function starts HSR table search using default access.
+ */
+static int sw_start_hsr_hw(struct ksz_sw *sw)
+{
+	u32 ctrl;
+
+	ctrl = HSR_SEARCH;
+	ctrl |= HSR_START;
+	sw->reg->w32(sw, REG_HSR_ALU_CTRL__4, ctrl);
+	return 1;
+}  /* sw_start_hsr_hw */
+
+/**
+ * sw_g_hsr_hw - retrieve HSR table result using default access
+ * @sw:		The switch instance.
+ * @hsr:	Buffer to store the HSR table entry.
+ *
+ * This function retrieves HSR table result using default access.
+ */
+static int sw_g_hsr_hw(struct ksz_sw *sw, struct ksz_hsr_table *hsr)
+{
+	sw_get_hsr_table(sw, hsr);
+	return 1;
+}  /* sw_g_hsr_hw */
+
+/**
+ * sw_stop_hsr_hw - stop HSR table search using default access
+ * @sw:		The switch instance.
+ *
+ * This function stops HSR table search using default access.
+ *
+ * Return the last HSR table control.
+ */
+static u32 sw_stop_hsr_hw(struct ksz_sw *sw)
+{
+	u32 ctrl;
+
+	ctrl = 0;
+	sw->reg->w32(sw, REG_HSR_ALU_CTRL__4, ctrl);
+	ctrl = sw->reg->r32(sw, REG_HSR_ALU_CTRL__4);
+	return ctrl;
+}  /* sw_stop_hsr_hw */
+#endif
+
+/**
+ * sw_r_hsr_table - read from HSR table
+ * @sw:		The switch instance.
+ * @addr:	The address of the table entry.
+ * @hsr:	Buffer to store the HSR table entry.
+ *
+ * This routine reads an entry of the HSR table of the switch.
+ * HSR table is locked to prevent corruption of read data.
+ */
+static void sw_r_hsr_table(struct ksz_sw *sw, u16 addr,
+	struct ksz_hsr_table *hsr)
+{
+	mutex_lock(&sw->hsrlock);
+	sw->ops->acquire(sw);
+	if (wait_for_hsr_table(sw))
+		goto r_hsr_exit;
+	sw->reg->r_hsr_hw(sw, addr, hsr);
+
+r_hsr_exit:
+	sw->ops->release(sw);
+	mutex_unlock(&sw->hsrlock);
+	hsr->dirty = 0;
+}  /* sw_r_hsr_table */
+
+/**
+ * sw_w_hsr_table - write to HSR table
+ * @sw:		The switch instance.
+ * @addr:	The address of the table entry.
+ * @hsr:	The HSR table entry.
+ *
+ * This routine writes an entry of the HSR table of the switch.
+ * HSR table is locked to prevent corruption of read data.
+ */
+static void sw_w_hsr_table(struct ksz_sw *sw, u16 addr,
+	struct ksz_hsr_table *hsr)
+{
+	mutex_lock(&sw->hsrlock);
+	sw->ops->acquire(sw);
+	if (wait_for_hsr_table(sw))
+		goto w_hsr_exit;
+	sw->reg->w_hsr_hw(sw, addr, hsr);
+
+w_hsr_exit:
+	sw->ops->release(sw);
+	mutex_unlock(&sw->hsrlock);
+	hsr->dirty = 0;
+}  /* sw_w_hsr_table */
+
+/**
+ * sw_start_hsr_table - start HSR table search
+ * @sw:		The switch instance.
+ *
+ * This routine starts HSR table search.
+ * HSR table is locked to prevent corruption of read data.
+ */
+static void sw_start_hsr_table(struct ksz_sw *sw)
+{
+	mutex_lock(&sw->hsrlock);
+	sw->ops->acquire(sw);
+	if (wait_for_hsr_table(sw))
+		goto s_hsr_exit;
+	sw->reg->start_hsr_hw(sw);
+
+s_hsr_exit:
+	sw->ops->release(sw);
+}  /* sw_start_hsr_table */
+
+/**
+ * sw_g_hsr_table - retrieve HSR table result
+ * @sw:		The switch instance.
+ * @addr:	The address of the table entry.
+ * @hsr:	Buffer to store the HSR table entry.
+ *
+ * This function retrieves an entry of the HSR table search result.
+ *
+ * Return 0 if the entry is successfully read; otherwise -1.
+ */
+static int sw_g_hsr_table(struct ksz_sw *sw, u16 *addr,
+	struct ksz_hsr_table *hsr)
+{
+	u32 ctrl;
+	int rc = 0;
+
+	sw->ops->acquire(sw);
+	do {
+		ctrl = sw->reg->r32(sw, REG_HSR_ALU_CTRL__4);
+	} while (!(ctrl & HSR_VALID) && (ctrl & HSR_START));
+	if (ctrl & HSR_VALID) {
+		ctrl >>= HSR_VALID_CNT_S;
+		ctrl &= HSR_VALID_CNT_M;
+		rc = !sw->reg->g_hsr_hw(sw, hsr);
+		if (ctrl != *addr + 1)
+			*addr = ctrl - 1;
+	} else
+		rc = -1;
+	sw->ops->release(sw);
+	return rc;
+}  /* sw_g_hsr_table */
+
+/**
+ * sw_stop_hsr_table - stop HSR table search
+ * @sw:		The switch instance.
+ * @addr:	The address of the last table entry.
+ *
+ * This routine stops HSR table search.
+ */
+static void sw_stop_hsr_table(struct ksz_sw *sw, u16 addr)
+{
+	u32 ctrl;
+
+	sw->ops->acquire(sw);
+	ctrl = sw->reg->stop_hsr_hw(sw);
+	ctrl >>= HSR_VALID_CNT_S;
+	ctrl &= HSR_VALID_CNT_M;
+if (ctrl != addr)
+dbg_msg(" ?stop hsr: %x %x"NL, ctrl, addr);
+	sw->ops->release(sw);
+	mutex_unlock(&sw->hsrlock);
+}  /* sw_stop_hsr_table */
+
+/**
+ * sw_d_hsr_table - dump HSR table
+ * @sw:		The switch instance.
+ *
+ * This routine dumps HSR table contents.
+ */
+static ssize_t sw_d_hsr_table(struct ksz_sw *sw, char *buf, ssize_t len)
+{
+	u16 i;
+	struct ksz_hsr_table hsr;
+	int first_break = true;
+
+	sw_start_hsr_table(sw);
+	i = 0;
+	do {
+		if (sw_g_hsr_table(sw, &i, &hsr))
+			break;
+		if (len >= MAX_SYSFS_BUF_SIZE && first_break) {
+			first_break = false;
+			len += sprintf(buf + len, "..."NL);
+		}
+		if (len < MAX_SYSFS_BUF_SIZE)
+		len += sprintf(buf + len,
+			"%3x: %02X:%02X:%02X:%02X:%02X:%02X - %x "
+			" c=%x  %04x:%04x  %04x:%04x  %04x:%04x  [%u]"NL,
+			i, hsr.src_mac[0], hsr.src_mac[1], hsr.src_mac[2],
+			hsr.src_mac[3], hsr.src_mac[4], hsr.src_mac[5],
+			hsr.path_id >> 1, hsr.age_cnt,
+			hsr.start_seq[0], hsr.start_seq[1],
+			hsr.exp_seq[0], hsr.exp_seq[1],
+			hsr.seq_cnt[0], hsr.seq_cnt[1], hsr.valid);
+		else {
+		printk(KERN_INFO
+			"%3x: %02X:%02X:%02X:%02X:%02X:%02X - %x "
+			" c=%x  %04x:%04x  %04x:%04x  %04x:%04x  [%u]"NL,
+			i, hsr.src_mac[0], hsr.src_mac[1], hsr.src_mac[2],
+			hsr.src_mac[3], hsr.src_mac[4], hsr.src_mac[5],
+			hsr.path_id >> 1, hsr.age_cnt,
+			hsr.start_seq[0], hsr.start_seq[1],
+			hsr.exp_seq[0], hsr.exp_seq[1],
+			hsr.seq_cnt[0], hsr.seq_cnt[1], hsr.valid);
+		yield();
+		}
+		i++;
+	} while (1);
+	sw_stop_hsr_table(sw, i);
+#if 1
+	if (len < MAX_SYSFS_BUF_SIZE)
+		len += sprintf(buf + len, NL);
+	else
+		printk(KERN_INFO NL);
+	for (i = 0; i < HSR_INDEX_MAX; i++) {
+		sw_r_hsr_table(sw, i + 1, &hsr);
+		if (!hsr.valid && !hsr.age_cnt)
+			continue;
+		if (len >= MAX_SYSFS_BUF_SIZE && first_break) {
+			first_break = false;
+			len += sprintf(buf + len, "..."NL);
+		}
+		if (len < MAX_SYSFS_BUF_SIZE)
+		len += sprintf(buf + len,
+			"%3x: %02X:%02X:%02X:%02X:%02X:%02X - %x "
+			" c=%x  %04x:%04x  %04x:%04x  %04x:%04x  [%u]"NL,
+			i, hsr.src_mac[0], hsr.src_mac[1], hsr.src_mac[2],
+			hsr.src_mac[3], hsr.src_mac[4], hsr.src_mac[5],
+			hsr.path_id >> 1, hsr.age_cnt,
+			hsr.start_seq[0], hsr.start_seq[1],
+			hsr.exp_seq[0], hsr.exp_seq[1],
+			hsr.seq_cnt[0], hsr.seq_cnt[1], hsr.valid);
+		else {
+		printk(KERN_INFO
+			"%3x: %02X:%02X:%02X:%02X:%02X:%02X - %x "
+			" c=%x  %04x:%04x  %04x:%04x  %04x:%04x  [%u]"NL,
+			i, hsr.src_mac[0], hsr.src_mac[1], hsr.src_mac[2],
+			hsr.src_mac[3], hsr.src_mac[4], hsr.src_mac[5],
+			hsr.path_id >> 1, hsr.age_cnt,
+			hsr.start_seq[0], hsr.start_seq[1],
+			hsr.exp_seq[0], hsr.exp_seq[1],
+			hsr.seq_cnt[0], hsr.seq_cnt[1], hsr.valid);
+		yield();
+		}
+	}
+#endif
+	return len;
+}  /* sw_d_hsr_table */
+
+/* -------------------------------------------------------------------------- */
+#endif
+
+/*
+ * Port functions
+ */
+
+static void port_phy_w(struct ksz_sw *sw, u32 addr, u16 val)
+{
+	int shift;
+	u32 data;
+
+	shift = addr & 3;
+	shift *= 8;
+	shift = 16 - shift;
+	addr &= ~3;
+	data = sw->reg->r32(sw, addr);
+	data &= ~(0xffff << shift);
+	data |= (val << shift);
+	sw->reg->w32(sw, addr, data);
+}  /* port_phy_w */
+
+/**
+ * port_chk - check port register bits
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ * @offset:	The offset of the port register.
+ * @bits:	The data bits to check.
+ *
+ * This function checks whether the specified bits of the port register are set
+ * or not.
+ *
+ * Return 0 if the bits are not set.
+ */
+static int port_chk(struct ksz_sw *sw, uint port, uint offset, SW_D bits)
+{
+	u32 addr;
+	SW_D data;
+
+	addr = PORT_CTRL_ADDR(port, offset);
+	data = SW_R(sw, addr);
+	return (data & bits) == bits;
+}  /* port_chk */
+
+/**
+ * port_cfg - set port register bits
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ * @offset:	The offset of the port register.
+ * @bits:	The data bits to set.
+ * @set:	The flag indicating whether the bits are to be set or not.
+ *
+ * This routine sets or resets the specified bits of the port register.
+ */
+static void port_cfg(struct ksz_sw *sw, uint port, uint offset, SW_D bits,
+	bool set)
+{
+	u32 addr;
+	SW_D data;
+
+	addr = PORT_CTRL_ADDR(port, offset);
+	data = SW_R(sw, addr);
+	if (set)
+		data |= bits;
+	else
+		data &= ~bits;
+	SW_W(sw, addr, data);
+}  /* port_cfg */
+
+/**
+ * port_chk16 - check port register bits
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ * @offset:	The offset of the port register.
+ * @bits:	The data bits to check.
+ *
+ * This function checks whether the specified bits of the port register are set
+ * or not.
+ *
+ * Return 0 if the bits are not set.
+ */
+static int port_chk16(struct ksz_sw *sw, uint port, uint offset, u16 bits)
+{
+	u32 addr;
+	u16 data;
+
+	addr = PORT_CTRL_ADDR(port, offset);
+	data = sw->reg->r16(sw, addr);
+	return (data & bits) == bits;
+}  /* port_chk32 */
+
+/**
+ * port_cfg16 - set port register bits
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ * @offset:	The offset of the port register.
+ * @bits:	The data bits to set.
+ * @set:	The flag indicating whether the bits are to be set or not.
+ *
+ * This routine sets or resets the specified bits of the port register.
+ */
+static void port_cfg16(struct ksz_sw *sw, uint port, uint offset, u16 bits,
+	bool set)
+{
+	u32 addr;
+	u16 data;
+
+	addr = PORT_CTRL_ADDR(port, offset);
+	data = sw->reg->r16(sw, addr);
+	if (set)
+		data |= bits;
+	else
+		data &= ~bits;
+	if (0x120 <= offset && offset <= 0x13F)
+		port_phy_w(sw, addr, data);
+	else
+		sw->reg->w16(sw, addr, data);
+}  /* port_cfg16 */
+
+/**
+ * port_chk32 - check port register bits
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ * @offset:	The offset of the port register.
+ * @bits:	The data bits to check.
+ *
+ * This function checks whether the specified bits of the port register are set
+ * or not.
+ *
+ * Return 0 if the bits are not set.
+ */
+static int port_chk32(struct ksz_sw *sw, uint port, uint offset, u32 bits)
+{
+	u32 addr;
+	u32 data;
+
+	addr = PORT_CTRL_ADDR(port, offset);
+	data = sw->reg->r32(sw, addr);
+	return (data & bits) == bits;
+}  /* port_chk32 */
+
+/**
+ * port_cfg32 - set port register bits
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ * @offset:	The offset of the port register.
+ * @bits:	The data bits to set.
+ * @set:	The flag indicating whether the bits are to be set or not.
+ *
+ * This routine sets or resets the specified bits of the port register.
+ */
+static void port_cfg32(struct ksz_sw *sw, uint port, uint offset, u32 bits,
+	bool set)
+{
+	u32 addr;
+	u32 data;
+
+	addr = PORT_CTRL_ADDR(port, offset);
+	data = sw->reg->r32(sw, addr);
+	if (set)
+		data |= bits;
+	else
+		data &= ~bits;
+	sw->reg->w32(sw, addr, data);
+}  /* port_cfg32 */
+
+/**
+ * port_r8 - read byte from port register
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ * @offset:	The offset of the port register.
+ * @data:	Buffer to store the data.
+ *
+ * This routine reads a byte from the port register.
+ */
+static void port_r8(struct ksz_sw *sw, uint port, uint offset, u8 *data)
+{
+	u32 addr;
+
+	addr = PORT_CTRL_ADDR(port, offset);
+	*data = sw->reg->r8(sw, addr);
+}  /* port_r8 */
+
+/**
+ * port_w8 - write byte to port register
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ * @offset:	The offset of the port register.
+ * @data:	Data to write.
+ *
+ * This routine writes a byte to the port register.
+ */
+static void port_w8(struct ksz_sw *sw, uint port, uint offset, u8 data)
+{
+	u32 addr;
+
+	addr = PORT_CTRL_ADDR(port, offset);
+	sw->reg->w8(sw, addr, data);
+}  /* port_w8 */
+
+/**
+ * port_r16 - read word from port register.
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ * @offset:	The offset of the port register.
+ * @data:	Buffer to store the data.
+ *
+ * This routine reads a word from the port register.
+ */
+static void port_r16(struct ksz_sw *sw, uint port, uint offset, u16 *data)
+{
+	u32 addr;
+
+	addr = PORT_CTRL_ADDR(port, offset);
+	*data = sw->reg->r16(sw, addr);
+}  /* port_r16 */
+
+/**
+ * port_w16 - write word to port register.
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ * @offset:	The offset of the port register.
+ * @data:	Data to write.
+ *
+ * This routine writes a word to the port register.
+ */
+static void port_w16(struct ksz_sw *sw, uint port, uint offset, u16 data)
+{
+	u32 addr;
+
+	addr = PORT_CTRL_ADDR(port, offset);
+	if (0x120 <= offset && offset <= 0x13F)
+		port_phy_w(sw, addr, data);
+	else
+		sw->reg->w16(sw, addr, data);
+}  /* port_w16 */
+
+/**
+ * port_r32 - read dword from port register.
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ * @offset:	The offset of the port register.
+ * @data:	Buffer to store the data.
+ *
+ * This routine reads a dword from the port register.
+ */
+static void port_r32(struct ksz_sw *sw, uint port, uint offset, u32 *data)
+{
+	u32 addr;
+
+	addr = PORT_CTRL_ADDR(port, offset);
+	*data = sw->reg->r32(sw, addr);
+}  /* port_r32 */
+
+/**
+ * port_w32 - write dword to port register.
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ * @offset:	The offset of the port register.
+ * @data:	Data to write.
+ *
+ * This routine writes a dword to the port register.
+ */
+static void port_w32(struct ksz_sw *sw, uint port, uint offset, u32 data)
+{
+	u32 addr;
+
+	addr = PORT_CTRL_ADDR(port, offset);
+	sw->reg->w32(sw, addr, data);
+}  /* port_w32 */
+
+#ifndef CONFIG_KSZ_IBA_ONLY
+static void port_get(struct ksz_sw *sw, uint port, uint offset, void *buf,
+	size_t cnt)
+{
+	u32 addr;
+
+	addr = PORT_CTRL_ADDR(port, offset);
+	sw->reg->r(sw, addr, buf, cnt);
+}
+
+#if 0
+static void port_set(struct ksz_sw *sw, uint port, uint offset, void *buf,
+	size_t cnt)
+{
+	u32 addr;
+
+	addr = PORT_CTRL_ADDR(port, offset);
+	sw->reg->w(sw, addr, buf, cnt);
+}
+#endif
+#endif
+
+/**
+ * port_r_s - read bits with shift from port register.
+ * @sw:		The switch instance.
+ * @p:		The port index.
+ * @reg:	The port register.
+ * @mask:	The mask to apply.
+ * @shift:	The shift to use.
+ *
+ * This function reads bits from the port register.
+ */
+static u8 port_r_s(struct ksz_sw *sw, uint p, u32 reg, u8 mask, u8 shift)
+{
+	u8 data;
+
+	port_r8(sw, p, reg, &data);
+	data >>= shift;
+	data &= mask;
+	return data;
+}  /* port_r_s */
+
+/**
+ * port_w_s - write bits with shift to port register.
+ * @sw:		The switch instance.
+ * @p:		The port index.
+ * @reg:	The port register.
+ * @mask:	The mask to apply.
+ * @shift:	The shift to use.
+ *
+ * This routine writes bits to the port register.
+ */
+static void port_w_s(struct ksz_sw *sw, uint p, u32 reg, u8 mask, u8 shift,
+	u8 val)
+{
+	u8 data;
+
+	port_r8(sw, p, reg, &data);
+	data &= ~(mask << shift);
+	data |= (val & mask) << shift;
+	port_w8(sw, p, reg, data);
+}  /* port_w_s */
+
+static u32 port_r_s_32(struct ksz_sw *sw, uint p, u32 reg, u32 mask,
+	u32 shift)
+{
+	u32 data;
+
+	port_r32(sw, p, reg, &data);
+	data >>= shift;
+	data &= mask;
+	return data;
+}
+
+static void port_w_s_32(struct ksz_sw *sw, uint p, u32 reg, u32 mask,
+	u32 shift, u32 val)
+{
+	u32 data;
+
+	port_r32(sw, p, reg, &data);
+	data &= ~(mask << shift);
+	data |= (val & mask) << shift;
+	port_w32(sw, p, reg, data);
+}
+
+/**
+ * sw_chk - check switch register bits
+ * @sw:		The switch instance.
+ * @addr:	The address of the switch register.
+ * @bits:	The data bits to check.
+ *
+ * This function checks whether the specified bits of the switch register are
+ * set or not.
+ *
+ * Return 0 if the bits are not set.
+ */
+static int sw_chk(struct ksz_sw *sw, u32 addr, SW_D bits)
+{
+	SW_D data;
+
+	data = SW_R(sw, addr);
+	return (data & bits) == bits;
+}  /* sw_chk */
+
+/**
+ * sw_cfg - set switch register bits
+ * @sw:		The switch instance.
+ * @addr:	The address of the switch register.
+ * @bits:	The data bits to set.
+ * @set:	The flag indicating whether the bits are to be set or not.
+ *
+ * This function sets or resets the specified bits of the switch register.
+ */
+static void sw_cfg(struct ksz_sw *sw, u32 addr, SW_D bits, bool set)
+{
+	SW_D data;
+
+	data = SW_R(sw, addr);
+	if (set)
+		data |= bits;
+	else
+		data &= ~bits;
+	SW_W(sw, addr, data);
+}  /* sw_cfg */
+
+static SW_D sw_r_shift(struct ksz_sw *sw, u32 addr, u32 mask,
+	u32 shift)
+{
+	SW_D data;
+
+	data = SW_R(sw, addr);
+	data >>= shift;
+	data &= mask;
+	return data;
+}
+
+static void sw_w_shift(struct ksz_sw *sw, u32 addr, u32 mask, u32 shift,
+	SW_D val)
+{
+	SW_D data;
+
+	data = SW_R(sw, addr);
+	data &= ~(mask << shift);
+	data |= (val & mask) << shift;
+	SW_W(sw, addr, data);
+}
+
+/* -------------------------------------------------------------------------- */
+
+#define contain_reg(addr, len, reg)	\
+	(addr <= (reg) && (reg) <= (addr + len - 1))
+
+#ifdef CONFIG_KSZ_IBA
+static void sw_set_spi(struct ksz_sw *sw, struct ksz_iba_info *iba);
+
+#include "ksz_iba.c"
+#endif
+#ifdef CONFIG_1588_PTP
+#include "ksz_ptp_9897.c"
+#endif
+
+/* -------------------------------------------------------------------------- */
+
+#ifdef PORT_OUT_RATE_ADDR
+/**
+ * port_out_rate_r8 - read byte from port register
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ * @offset:	The offset of the port register.
+ * @data:	Buffer to store the data.
+ *
+ * This routine reads a byte from the port register.
+ */
+static void port_out_rate_r8(struct ksz_sw *sw, uint port, uint offset, u8 *data)
+{
+	u32 addr;
+
+	PORT_OUT_RATE_ADDR(port, addr);
+	addr += offset;
+	*data = sw->reg->r8(sw, addr);
+}
+
+/**
+ * port_out_rate_w8 - write byte to port register
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ * @offset:	The offset of the port register.
+ * @data:	Data to write.
+ *
+ * This routine writes a byte to the port register.
+ */
+static void port_out_rate_w8(struct ksz_sw *sw, uint port, uint offset, u8 data)
+{
+	u32 addr;
+
+	PORT_OUT_RATE_ADDR(port, addr);
+	addr += offset;
+	sw->reg->w8(sw, addr, data);
+}
+#endif
+
+/* -------------------------------------------------------------------------- */
+
+/* ACL */
+
+static inline void port_cfg_acl(struct ksz_sw *sw, uint p, bool set)
+{
+	port_cfg(sw, p,
+		REG_PORT_MRI_AUTHEN_CTRL, PORT_ACL_ENABLE, set);
+}
+
+static inline int port_chk_acl(struct ksz_sw *sw, uint p)
+{
+	return port_chk(sw, p,
+		REG_PORT_MRI_AUTHEN_CTRL, PORT_ACL_ENABLE);
+}
+
+static inline u8 port_get_authen_mode(struct ksz_sw *sw, uint p)
+{
+	return port_r_s(sw, p,
+		REG_PORT_MRI_AUTHEN_CTRL, PORT_AUTHEN_MODE, 0);
+}
+
+static void port_set_authen_mode(struct ksz_sw *sw, uint p, u8 mode)
+{
+	port_w_s(sw, p,
+		REG_PORT_MRI_AUTHEN_CTRL, PORT_AUTHEN_MODE, 0, mode);
+}
+
+/**
+ * get_acl_action_info - Get ACL action field information
+ * @acl:	The ACL entry.
+ * @data:	The ACL data.
+ *
+ * This helper routine gets ACL action field information.
+ */
+static void get_acl_action_info(struct ksz_acl_table *acl, u8 data[])
+{
+	acl->prio_mode = (data[10] >> ACL_PRIO_MODE_S) & ACL_PRIO_MODE_M;
+	acl->prio = (data[10] >> ACL_PRIO_S) & ACL_PRIO_M;
+	acl->vlan_prio_replace = !!(data[10] & ACL_VLAN_PRIO_REPLACE);
+	acl->vlan_prio = data[11] >> ACL_VLAN_PRIO_S;
+	acl->vlan_prio |= (data[10] & ACL_VLAN_PRIO_HI_M) << 1;
+	acl->map_mode = (data[11] >> ACL_MAP_MODE_S) & ACL_MAP_MODE_M;
+
+	/* byte 12 is not used at all. */
+	acl->ports = data[13];
+}  /* get_acl_action_info */
+
+/**
+ * get_acl_table_info - Get ACL table information
+ * @acl:	The ACL entry.
+ * @data:	The ACL data.
+ *
+ * This helper routine gets ACL table information.
+ */
+static void get_acl_table_info(struct ksz_acl_table *acl, u8 data[])
+{
+	u16 *ptr_16;
+	u32 *ptr_32;
+	int i;
+	int j;
+	int cnt = 0;
+
+	acl->first_rule = data[0] & ACL_FIRST_RULE_M;
+	acl->mode = (data[1] >> ACL_MODE_S) & ACL_MODE_M;
+	acl->enable = (data[1] >> ACL_ENABLE_S) & ACL_ENABLE_M;
+	acl->src = !!(data[1] & ACL_SRC);
+	acl->equal = !!(data[1] & ACL_EQUAL);
+	switch (acl->mode) {
+	case ACL_MODE_LAYER_2:
+		for (i = 0; i < 6; i++)
+			acl->mac[i] = data[2 + i];
+		ptr_16 = (u16 *) &data[8];
+		acl->eth_type = be16_to_cpu(*ptr_16);
+		if (ACL_ENABLE_2_COUNT == acl->enable) {
+			cnt = 1;
+			ptr_16 = (u16 *) &data[10];
+			acl->cnt = (be16_to_cpu(*ptr_16) >> ACL_CNT_S) &
+				ACL_CNT_M;
+			acl->msec =
+				!!(data[ACL_INTR_CNT_START] & ACL_MSEC_UNIT);
+			acl->intr_mode =
+				!!(data[ACL_INTR_CNT_START] & ACL_INTR_MODE);
+		}
+		break;
+	case ACL_MODE_LAYER_3:
+		j = 2;
+		for (i = 0; i < 4; i++, j++)
+			acl->ip4_addr[i] = data[j];
+		for (i = 0; i < 4; i++, j++)
+			acl->ip4_mask[i] = data[j];
+		break;
+	case ACL_MODE_LAYER_4:
+		switch (acl->enable) {
+		case ACL_ENABLE_4_TCP_SEQN_COMP:
+			ptr_32 = (u32 *) &data[2];
+			acl->seqnum = be32_to_cpu(*ptr_32);
+			break;
+		default:
+			ptr_16 = (u16 *) &data[2];
+			acl->max_port = be16_to_cpu(*ptr_16);
+			++ptr_16;
+			acl->min_port = be16_to_cpu(*ptr_16);
+			acl->port_mode = (data[6] >> ACL_PORT_MODE_S) &
+				ACL_PORT_MODE_M;
+		}
+		acl->protocol = (data[6] & 1) << 7;
+		acl->protocol |= (data[7] >> 1);
+		acl->tcp_flag_enable = !!(data[7] & ACL_TCP_FLAG_ENABLE);
+		acl->tcp_flag_mask = data[8];
+		acl->tcp_flag = data[9];
+		break;
+	default:
+		break;
+	}
+	if (!cnt)
+		get_acl_action_info(acl, data);
+	ptr_16 = (u16 *) &data[ACL_RULESET_START];
+	acl->ruleset = be16_to_cpu(*ptr_16);
+}  /* get_acl_table_info */
+
+/**
+ * set_acl_action_info - Set ACL action field information
+ * @acl:	The ACL entry.
+ * @data:	The ACL data.
+ *
+ * This helper routine sets ACL action field information.
+ */
+static void set_acl_action_info(struct ksz_acl_table *acl, u8 data[])
+{
+	if (acl->data[0] != 0xff)
+		memcpy(data, acl->data, ACL_TABLE_LEN);
+	data[10] = (acl->prio_mode & ACL_PRIO_MODE_M) << ACL_PRIO_MODE_S;
+	data[10] |= (acl->prio & ACL_PRIO_M) << ACL_PRIO_S;
+	if (acl->vlan_prio_replace)
+		data[10] |= ACL_VLAN_PRIO_REPLACE;
+	data[10] |= (acl->vlan_prio >> 1);
+	data[11] = acl->vlan_prio << ACL_VLAN_PRIO_S;
+	data[11] |= (acl->map_mode & ACL_MAP_MODE_M) << ACL_MAP_MODE_S;
+
+	/* byte 12 is not used at all. */
+	data[13] = acl->ports;
+}  /* set_acl_action_info */
+
+/**
+ * set_acl_ruleset_info - Set ACL ruleset field information
+ * @acl:	The ACL entry.
+ * @data:	The ACL data.
+ *
+ * This helper routine sets ACL ruleset field information.
+ */
+static void set_acl_ruleset_info(struct ksz_acl_table *acl, u8 data[])
+{
+	u16 *ptr_16;
+
+	if (acl->data[0] != 0xff)
+		memcpy(data, acl->data, ACL_TABLE_LEN);
+	data[0] = acl->first_rule & ACL_FIRST_RULE_M;
+	ptr_16 = (u16 *) &data[ACL_RULESET_START];
+	*ptr_16 = cpu_to_be16(acl->ruleset);
+}  /* set_acl_ruleset_info */
+
+/**
+ * set_acl_table_info - Set ACL table information
+ * @acl:	The ACL entry.
+ * @data:	The ACL data.
+ *
+ * This helper routine sets ACL table information.
+ */
+static void set_acl_table_info(struct ksz_acl_table *acl, u8 data[])
+{
+	u16 *ptr_16;
+	u32 *ptr_32;
+	int i;
+	int j;
+
+	if (acl->data[0] != 0xff)
+		memcpy(data, acl->data, ACL_TABLE_LEN);
+	data[1] = (acl->mode & ACL_MODE_M) << ACL_MODE_S;
+	data[1] |= (acl->enable & ACL_ENABLE_M) << ACL_ENABLE_S;
+	if (acl->src)
+		data[1] |= ACL_SRC;
+	if (acl->equal)
+		data[1] |= ACL_EQUAL;
+	switch (acl->mode) {
+	case ACL_MODE_LAYER_2:
+		for (i = 0; i < 6; i++)
+			data[2 + i] = acl->mac[i];
+		ptr_16 = (u16 *) &data[8];
+		*ptr_16 = cpu_to_be16(acl->eth_type);
+		if (ACL_ENABLE_2_COUNT == acl->enable) {
+			data[ACL_INTR_CNT_START] = 0;
+			ptr_16 = (u16 *) &data[10];
+			*ptr_16 = cpu_to_be16((acl->cnt & ACL_CNT_M) <<
+				ACL_CNT_S);
+			data[12] = 0;
+			if (acl->msec)
+				data[ACL_INTR_CNT_START] |= ACL_MSEC_UNIT;
+			if (acl->intr_mode)
+				data[ACL_INTR_CNT_START] |= ACL_INTR_MODE;
+		}
+		break;
+	case ACL_MODE_LAYER_3:
+		j = 2;
+		for (i = 0; i < 4; i++, j++)
+			data[j] = acl->ip4_addr[i];
+		for (i = 0; i < 4; i++, j++)
+			data[j] = acl->ip4_mask[i];
+		break;
+	case ACL_MODE_LAYER_4:
+		switch (acl->enable) {
+		case ACL_ENABLE_4_TCP_SEQN_COMP:
+			ptr_32 = (u32 *) &data[2];
+			*ptr_32 = cpu_to_be32(acl->seqnum);
+			data[6] = 0;
+			break;
+		default:
+			ptr_16 = (u16 *) &data[2];
+			*ptr_16 = cpu_to_be16(acl->max_port);
+			++ptr_16;
+			*ptr_16 = cpu_to_be16(acl->min_port);
+			data[6] = (acl->port_mode & ACL_PORT_MODE_M) <<
+				ACL_PORT_MODE_S;
+		}
+		data[6] |= (acl->protocol >> 7);
+		data[7] = (acl->protocol << 1);
+		if (acl->tcp_flag_enable)
+			data[7] |= ACL_TCP_FLAG_ENABLE;
+		data[8] = acl->tcp_flag_mask;
+		data[9] = acl->tcp_flag;
+		break;
+	default:
+		break;
+	}
+}  /* set_acl_table_info */
+
+/**
+ * wait_for_acl_table - Wait for ACL table
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ *
+ * This helper function waits for ACL table to be ready for access.
+ */
+static int wait_for_acl_table(struct ksz_sw *sw, uint port)
+{
+	u8 ctrl;
+	int timeout = 100;
+
+	if (!port_chk_acl(sw, port))
+		dbg_msg("acl not on: %d"NL, port);
+	do {
+		--timeout;
+		if (!timeout)
+			return 1;
+		port_r(sw, port, REG_PORT_ACL_CTRL_0, &ctrl);
+	} while (!(ctrl & (PORT_ACL_WRITE_DONE | PORT_ACL_READ_DONE)));
+	return 0;
+}  /* wait_for_acl_table */
+
+#ifndef CONFIG_KSZ_IBA_ONLY
+/**
+ * sw_r_acl_hw - read from ACL table using default access
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ * @addr:	The ACL index.
+ * @data:	Buffer to hold the ACL data.
+ *
+ * This function reads from ACL table of the port using default access.
+ */
+static int sw_r_acl_hw(struct ksz_sw *sw, uint port, u16 addr, u8 data[])
+{
+	u8 ctrl = (addr & PORT_ACL_INDEX_M);
+
+	port_w(sw, port, REG_PORT_ACL_CTRL_0, ctrl);
+	do {
+		port_r(sw, port, REG_PORT_ACL_CTRL_0, &ctrl);
+	} while (!(ctrl & PORT_ACL_READ_DONE));
+	sw->reg->r(sw, PORT_CTRL_ADDR(port, REG_PORT_ACL_0), data,
+		   ACL_TABLE_LEN);
+	return 1;
+}  /* sw_r_acl_hw */
+
+/**
+ * sw_w_acl_hw - write to ACL table using default access
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ * @addr:	The ACL index.
+ * @data:	The ACL data to write.
+ *
+ * This function writes to ACL table of the port using default access.
+ */
+static int sw_w_acl_hw(struct ksz_sw *sw, uint port, u16 addr, u8 data[])
+{
+	u8 ctrl = (addr & PORT_ACL_INDEX_M) | PORT_ACL_WRITE;
+
+	sw->reg->w(sw, PORT_CTRL_ADDR(port, REG_PORT_ACL_0), data,
+		   ACL_TABLE_LEN);
+	port_w(sw, port, REG_PORT_ACL_CTRL_0, ctrl);
+	do {
+		port_r(sw, port, REG_PORT_ACL_CTRL_0, &ctrl);
+	} while (!(ctrl & PORT_ACL_WRITE_DONE));
+	return 1;
+}  /* sw_w_acl_hw */
+#endif
+
+/**
+ * sw_r_acl_table - read from ACL table
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ * @addr:	The address of the table entry.
+ * @acl:	Buffer to store the ACL entry.
+ *
+ * This function reads an entry of the ACL table of the port.
+ *
+ * Return 0 if the entry is valid; otherwise -1.
+ */
+static int sw_r_acl_table(struct ksz_sw *sw, uint port, u16 addr,
+	struct ksz_acl_table *acl)
+{
+	u8 data[20];
+	int rc = -1;
+
+	sw->ops->acquire(sw);
+	if (wait_for_acl_table(sw, port))
+		goto r_acl_exit;
+	sw->reg->r_acl_hw(sw, port, addr, data);
+	get_acl_table_info(acl, data);
+	memcpy(acl->data, data, ACL_TABLE_LEN);
+
+r_acl_exit:
+	sw->ops->release(sw);
+	if (acl->mode)
+		rc = 0;
+	acl->changed = 0;
+	return rc;
+}  /* sw_r_acl_table */
+
+/**
+ * sw_w_acl_action - write to ACL action field
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ * @addr:	The address of the table entry.
+ * @acl:	The ACL entry.
+ *
+ * This routine writes to the action field of an entry of the ACL table.
+ */
+static void sw_w_acl_action(struct ksz_sw *sw, uint port, u16 addr,
+	struct ksz_acl_table *acl)
+{
+	u8 data[20];
+
+	sw->ops->acquire(sw);
+	if (wait_for_acl_table(sw, port))
+		goto w_acl_act_exit;
+	set_acl_action_info(acl, data);
+	port_w16(sw, port, REG_PORT_ACL_BYTE_EN_MSB, ACL_ACTION_ENABLE);
+	sw->reg->w_acl_hw(sw, port, addr, data);
+	memcpy(&acl->data[ACL_ACTION_START], &data[ACL_ACTION_START],
+		ACL_ACTION_LEN);
+
+w_acl_act_exit:
+	sw->ops->release(sw);
+	acl->action_changed = 0;
+}  /* sw_w_acl_action */
+
+/**
+ * sw_w_acl_ruleset - write to ACL ruleset field
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ * @addr:	The address of the table entry.
+ * @acl:	The ACL entry.
+ *
+ * This routine writes to the ruleset field of an entry of the ACL table.
+ */
+static void sw_w_acl_ruleset(struct ksz_sw *sw, uint port, u16 addr,
+	struct ksz_acl_table *acl)
+{
+	u8 data[20];
+
+	sw->ops->acquire(sw);
+	if (wait_for_acl_table(sw, port))
+		goto w_acl_ruleset_exit;
+	set_acl_ruleset_info(acl, data);
+	port_w16(sw, port, REG_PORT_ACL_BYTE_EN_MSB, ACL_RULESET_ENABLE);
+	sw->reg->w_acl_hw(sw, port, addr, data);
+
+	/* First rule */
+	acl->data[0] = data[0];
+	memcpy(&acl->data[ACL_RULESET_START], &data[ACL_RULESET_START],
+		ACL_RULESET_LEN);
+
+w_acl_ruleset_exit:
+	sw->ops->release(sw);
+	acl->ruleset_changed = 0;
+}  /* sw_w_acl_ruleset */
+
+/**
+ * sw_w_acl_rule - write to ACL matching and process fields
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ * @addr:	The address of the table entry.
+ * @acl:	The ACL entry.
+ *
+ * This routine writes to the matching and process fields of an entry of the
+ * ACL table of the port.
+ */
+static void sw_w_acl_rule(struct ksz_sw *sw, uint port, u16 addr,
+	struct ksz_acl_table *acl)
+{
+	u8 data[20];
+	u16 byte_enable = ACL_MATCH_ENABLE;
+	int len = ACL_ACTION_START;
+
+	if (ACL_MODE_LAYER_2 == acl->mode &&
+	    ACL_ENABLE_2_COUNT == acl->enable) {
+		byte_enable |= ACL_ACTION_ENABLE;
+		len += ACL_ACTION_LEN;
+	}
+	sw->ops->acquire(sw);
+	if (wait_for_acl_table(sw, port))
+		goto w_acl_rule_exit;
+	set_acl_table_info(acl, data);
+	port_w16(sw, port, REG_PORT_ACL_BYTE_EN_MSB, byte_enable);
+	sw->reg->w_acl_hw(sw, port, addr, data);
+	memcpy(acl->data, data, len);
+
+w_acl_rule_exit:
+	sw->ops->release(sw);
+	acl->changed = 0;
+}  /* sw_w_acl_rule */
+
+/**
+ * sw_w_acl_table - write to ACL table
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ * @addr:	The address of the table entry.
+ * @acl:	The ACL entry.
+ *
+ * This routine writes an entry of the ACL table of the port.
+ */
+static void sw_w_acl_table(struct ksz_sw *sw, uint port, u16 addr,
+	struct ksz_acl_table *acl)
+{
+	u8 data[20];
+
+	acl->data[0] = 0xff;
+	memset(data, 0, sizeof(data));
+	sw->ops->acquire(sw);
+	if (wait_for_acl_table(sw, port))
+		goto w_acl_exit;
+	if (!(ACL_MODE_LAYER_2 == acl->mode &&
+	    ACL_ENABLE_2_COUNT == acl->enable))
+		set_acl_action_info(acl, data);
+	set_acl_table_info(acl, data);
+	set_acl_ruleset_info(acl, data);
+	port_w16(sw, port, REG_PORT_ACL_BYTE_EN_MSB, ACL_BYTE_ENABLE);
+	sw->reg->w_acl_hw(sw, port, addr, data);
+	memcpy(acl->data, data, ACL_TABLE_LEN);
+
+w_acl_exit:
+	sw->ops->release(sw);
+}  /* sw_w_acl_table */
+
+/**
+ * acl_action_info - format ACL action field information
+ * @sw:		The switch instance.
+ * @acl:	The ACL entry.
+ * @index;	The entry index.
+ * @buf:	Buffer to store the strings.
+ * @len:	Lenght of buffer.
+ *
+ * This helper routine formats the ACL action field information.
+ */
+static int acl_action_info(struct ksz_sw *sw, struct ksz_acl_table *acl,
+			   u16 index, char *buf, int len)
+{
+	char prio = 'p';
+	char vlan = 'v';
+	uint ports = acl->ports;
+
+	ports = get_log_mask_from_phy(sw, ports);
+	if (acl->prio_mode != ACL_PRIO_MODE_DISABLE)
+		prio = 'P';
+	if (acl->vlan_prio_replace)
+		vlan = 'V';
+	len += sprintf(buf + len,
+		"%x: %c:%u=%u %c:%u=%u %u=%04x [%u]"NL,
+		index,
+		prio, acl->prio_mode, acl->prio,
+		vlan, acl->vlan_prio_replace, acl->vlan_prio,
+		acl->map_mode, ports,
+		acl->action_changed ? 8 : 1);
+	return len;
+}  /* acl_action_info */
+
+/**
+ * acl_ruleset_info - format ACL ruleset field information
+ * @acl:	The ACL entry.
+ * @index;	The entry index.
+ * @buf:	Buffer to store the strings.
+ * @len:	Lenght of buffer.
+ *
+ * This helper routine formats the ACL ruleset field information.
+ */
+static int acl_ruleset_info(struct ksz_acl_table *acl, u16 index, char *buf,
+	int len)
+{
+	len += sprintf(buf + len,
+		"%x: %x:%04x [%u]"NL,
+		index,
+		acl->first_rule, acl->ruleset,
+		acl->ruleset_changed ? 8 : 1);
+	return len;
+}  /* acl_ruleset_info */
+
+/**
+ * acl_info - format ACL matching and process field information
+ * @acl:	The ACL entry.
+ * @index;	The entry index.
+ * @buf:	Buffer to store the strings.
+ * @len:	Lenght of buffer.
+ *
+ * This helper routine formats the ACL matching and process field information.
+ */
+static int acl_info(struct ksz_acl_table *acl, u16 index, char *buf, int len)
+{
+	char enable = 'e';
+	char equal = 'q';
+	char src = 's';
+	char cnt = 'c';
+	char protocol = 'x';
+	char flag = 'f';
+	char seqnum = 's';
+	char msec[4];
+
+	switch (acl->mode) {
+	case ACL_MODE_LAYER_2:
+		enable = 'E';
+		*msec = 0;
+		if (ACL_ENABLE_2_COUNT == acl->enable) {
+			equal = 'Q';
+			src = 'S';
+			cnt = 'C';
+			if (acl->intr_mode)
+				*msec = 0;
+			else if (acl->msec)
+				strcpy(msec, "ms ");
+			else
+				strcpy(msec, "us ");
+		} else {
+			equal = 'Q';
+			if (ACL_ENABLE_2_TYPE != acl->enable)
+				src = 'S';
+		}
+		len += sprintf(buf + len,
+			"%x: %02X:%02X:%02X:%02X:%02X:%02X-%04x "
+			"%c:%u.%u %s"
+			"%c:%u %c:%u %c:%u "
+			"[%u]"NL,
+			index,
+			acl->mac[0], acl->mac[1], acl->mac[2],
+			acl->mac[3], acl->mac[4], acl->mac[5],
+			acl->eth_type,
+			cnt, acl->intr_mode, acl->cnt, msec,
+			enable, acl->enable,
+			src, acl->src, equal, acl->equal,
+			acl->changed ? 8 : acl->mode);
+		break;
+	case ACL_MODE_LAYER_3:
+		if (ACL_ENABLE_3_IP == acl->enable ||
+		    ACL_ENABLE_3_SRC_DST_COMP == acl->enable)
+			enable = 'E';
+		if (ACL_ENABLE_3_IP == acl->enable) {
+			equal = 'Q';
+			src = 'S';
+		}
+		len += sprintf(buf + len,
+			"%x: %u.%u.%u.%u:%u.%u.%u.%u "
+			"%c:%u %c:%u %c:%u "
+			"[%u]"NL,
+			index,
+			acl->ip4_addr[0], acl->ip4_addr[1],
+			acl->ip4_addr[2], acl->ip4_addr[3],
+			acl->ip4_mask[0], acl->ip4_mask[1],
+			acl->ip4_mask[2], acl->ip4_mask[3],
+			enable, acl->enable,
+			src, acl->src, equal, acl->equal,
+			acl->changed ? 8 : acl->mode);
+		break;
+	case ACL_MODE_LAYER_4:
+		enable = 'E';
+		if (ACL_ENABLE_4_PROTOCOL == acl->enable) {
+			protocol = 'X';
+			equal = 'Q';
+		} else if (ACL_ENABLE_4_TCP_SEQN_COMP == acl->enable) {
+			seqnum = 'S';
+			equal = 'Q';
+			if (acl->tcp_flag_enable)
+				flag = 'F';
+		} else if (ACL_ENABLE_4_TCP_PORT_COMP == acl->enable) {
+			src = 'S';
+			if (acl->tcp_flag_enable)
+				flag = 'F';
+		} else
+			src = 'S';
+		len += sprintf(buf + len,
+			"%x: %u=%4x-%4x 0%c%x %c:%08x %c:%u=%x:%x "
+			"%c:%u %c:%u %c:%u "
+			"[%u]"NL,
+			index,
+			acl->port_mode, acl->min_port, acl->max_port,
+			protocol, acl->protocol, seqnum, acl->seqnum,
+			flag, acl->tcp_flag_enable,
+			acl->tcp_flag, acl->tcp_flag_mask,
+			enable, acl->enable,
+			src, acl->src, equal, acl->equal,
+			acl->changed ? 8 : acl->mode);
+		break;
+	default:
+		len += sprintf(buf + len,
+			"%x: "
+			"%c:%u %c:%u %c:%u "
+			"[%u]"NL,
+			index,
+			enable, acl->enable,
+			src, acl->src, equal, acl->equal,
+			acl->changed ? 8 : acl->mode);
+		break;
+	}
+	return len;
+}  /* acl_info */
+
+/**
+ * sw_d_acl_table - dump ACL table
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ *
+ * This routine dumps ACL table of the port.
+ */
+static ssize_t sw_d_acl_table(struct ksz_sw *sw, uint port, char *buf,
+	ssize_t len)
+{
+	struct ksz_port_cfg *cfg = get_port_cfg(sw, port);
+	struct ksz_acl_table *acl;
+	int i;
+	int acl_on;
+	int min = 0;
+
+	sw->ops->acquire(sw);
+	acl_on = port_chk_acl(sw, port);
+	if (!acl_on) {
+		printk(KERN_INFO "ACL not on for port %d"NL, port);
+		port_cfg_acl(sw, port, true);
+	}
+	sw->ops->release(sw);
+	for (i = 0; i < ACL_TABLE_ENTRIES; i++) {
+		acl = &cfg->acl_info[i];
+		acl->action_selected = false;
+		sw_r_acl_table(sw, port, i, acl);
+	}
+	for (i = 0; i < ACL_TABLE_ENTRIES; i++) {
+		acl = &cfg->acl_info[i];
+		if (!acl->mode)
+			continue;
+		if (!min)
+			len += sprintf(buf + len, "rules:"NL);
+		len = acl_info(acl, i, buf, len);
+		min = 1;
+	}
+	if (min)
+		len += sprintf(buf + len, NL);
+	min = 0;
+	for (i = 0; i < ACL_TABLE_ENTRIES; i++) {
+		acl = &cfg->acl_info[i];
+		if (!acl->ruleset)
+			continue;
+		if (!min)
+			len += sprintf(buf + len, "rulesets:"NL);
+		cfg->acl_info[acl->first_rule].action_selected = true;
+		len = acl_ruleset_info(acl, i, buf, len);
+		min = 1;
+	}
+	if (min)
+		len += sprintf(buf + len, NL);
+	min = 0;
+	for (i = 0; i < ACL_TABLE_ENTRIES; i++) {
+		acl = &cfg->acl_info[i];
+		if (ACL_PRIO_MODE_DISABLE == acl->prio_mode &&
+		    ACL_MAP_MODE_DISABLE == acl->map_mode &&
+		    !acl->ports &&
+		    !acl->vlan_prio_replace && !acl->action_selected)
+			continue;
+		if (ACL_MODE_LAYER_2 == acl->mode &&
+		    ACL_ENABLE_2_COUNT == acl->enable)
+			continue;
+		if (!min)
+			len += sprintf(buf + len, "actions:"NL);
+		len = acl_action_info(sw, acl, i, buf, len);
+		min = 1;
+	}
+	if (!acl_on) {
+		sw->ops->acquire(sw);
+		port_cfg_acl(sw, port, false);
+		sw->ops->release(sw);
+	}
+	return len;
+}  /* sw_d_acl_table */
+
+static void sw_reset_acl(struct ksz_sw *sw)
+{
+	struct ksz_port_cfg *cfg;
+	uint n;
+	uint port;
+
+	for (n = 0; n <= sw->mib_port_cnt; n++) {
+		port = get_phy_port(sw, n);
+		cfg = get_port_cfg(sw, port);
+		memset(cfg->acl_info, 0, sizeof(struct ksz_acl_table) *
+			ACL_TABLE_ENTRIES);
+		cfg->acl_index = cfg->acl_act_index = cfg->acl_rule_index = 0;
+	}
+}  /* sw_reset_acl */
+
+static void sw_init_acl(struct ksz_sw *sw)
+{
+	struct ksz_port_cfg *cfg;
+	struct ksz_acl_table *acl;
+	int i;
+	uint n;
+	uint port;
+
+	for (n = 0; n <= sw->mib_port_cnt; n++) {
+		port = get_phy_port(sw, n);
+		port_cfg_acl(sw, port, 1);
+		cfg = get_port_cfg(sw, port);
+		sw->ops->release(sw);
+		for (i = 0; i < ACL_TABLE_ENTRIES; i++) {
+			acl = &cfg->acl_info[i];
+			sw_r_acl_table(sw, port, i, acl);
+		}
+		sw->ops->acquire(sw);
+
+		/* Turn off ACL after reset. */
+		port_cfg_acl(sw, port, 0);
+	}
+}  /* sw_init_acl */
+
+/* -------------------------------------------------------------------------- */
+
+/* 36-bit counts. */
+#define MIB_RX_HI_PRIO			0x00
+#define MIB_RX_UNDERSIZE		0x01
+#define MIB_RX_FRAGMENT			0x02
+#define MIB_RX_OVERSIZE			0x03
+#define MIB_RX_JABBER			0x04
+#define MIB_RX_SYMBOL_ERR		0x05
+#define MIB_RX_CRC_ERR			0x06
+#define MIB_RX_ALIGNMENT_ERR		0x07
+#define MIB_RX_CTRL_8808		0x08
+#define MIB_RX_PAUSE			0x09
+#define MIB_RX_BROADCAST		0x0A
+#define MIB_RX_MULTICAST		0x0B
+#define MIB_RX_UNICAST			0x0C
+#define MIB_RX_OCTET_64			0x0D
+#define MIB_RX_OCTET_65_127		0x0E
+#define MIB_RX_OCTET_128_255		0x0F
+#define MIB_RX_OCTET_256_511		0x10
+#define MIB_RX_OCTET_512_1023		0x11
+#define MIB_RX_OCTET_1024_1522		0x12
+#define MIB_RX_OCTET_1523_2000		0x13
+#define MIB_RX_OCTET_2001		0x14
+#define MIB_TX_HI_PRIO			0x15
+#define MIB_TX_LATE_COLLISION		0x16
+#define MIB_TX_PAUSE			0x17
+#define MIB_TX_BROADCAST		0x18
+#define MIB_TX_MULTICAST		0x19
+#define MIB_TX_UNICAST			0x1A
+#define MIB_TX_DEFERRED			0x1B
+#define MIB_TX_TOTAL_COLLISION		0x1C
+#define MIB_TX_EXCESS_COLLISION		0x1D
+#define MIB_TX_SINGLE_COLLISION		0x1E
+#define MIB_TX_MULTI_COLLISION		0x1F
+
+#define MIB_RX_TOTAL			0x20
+#define MIB_TX_TOTAL			0x21
+
+#define MIB_RX_DROPS			0x22
+#define MIB_TX_DROPS			0x23
+
+/* Actual locations. */
+#define MIB_9897_RX_BYTE_CNT		0x80
+#define MIB_9897_TX_BYTE_CNT		0x81
+
+#define MIB_9897_RX_DROPPED_PACKET	0x82
+#define MIB_9897_TX_DROPPED_PACKET	0x83
+
+static struct {
+	char string[20];
+} mib_names[TOTAL_SWITCH_COUNTER_NUM] = {
+	{ "rx_hi        " },
+	{ "rx_undersize" },
+	{ "rx_fragments" },
+	{ "rx_oversize" },
+	{ "rx_jabbers" },
+	{ "rx_symbol_err" },
+	{ "rx_crc_err" },
+	{ "rx_align_err" },
+	{ "rx_mac_ctrl" },
+	{ "rx_pause" },
+	{ "rx_bcast" },
+	{ "rx_mcast" },
+	{ "rx_ucast" },
+	{ "rx_64_or_less" },
+	{ "rx_65_127" },
+	{ "rx_128_255" },
+	{ "rx_256_511" },
+	{ "rx_512_1023" },
+	{ "rx_1024_1522" },
+	{ "rx_1523_2000" },
+	{ "rx_2001     " },
+
+	{ "tx_hi        " },
+	{ "tx_late_col" },
+	{ "tx_pause" },
+	{ "tx_bcast" },
+	{ "tx_mcast" },
+	{ "tx_ucast" },
+	{ "tx_deferred" },
+	{ "tx_total_col" },
+	{ "tx_exc_col" },
+	{ "tx_single_col" },
+	{ "tx_mult_col" },
+
+	{ "rx_total" },
+	{ "tx_total" },
+
+	{ "rx_discards" },
+	{ "tx_discards" },
+};
+
+/*
+ * Some counters do not need to be read too often because they are less likely
+ * to increase much.
+ */
+static u8 mib_read_max[SWITCH_COUNTER_NUM] = {
+	1,
+	4,
+	4,
+	4,
+	4,
+	4,
+	4,
+	4,
+	4,
+	1,
+	1,
+	1,
+	1,
+	2,
+	2,
+	2,
+	2,
+	2,
+	2,
+	2,
+	2,
+
+	1,
+	4,
+	1,
+	1,
+	1,
+	1,
+	4,
+	4,
+	4,
+	4,
+	4,
+};
+
+static u8 mib_start[TOTAL_SWITCH_COUNTER_NUM];
+static u8 mib_index[TOTAL_SWITCH_COUNTER_NUM];
+
+static u8 sw_fill_mib_index(struct ksz_sw *sw, u8 index, u8 interval)
+{
+	int i;
+
+	for (i = 0; i < SWITCH_COUNTER_NUM; i++) {
+		if (interval == mib_read_max[i])
+			mib_start[index++] = i;
+	}
+	return index;
+}  /* sw_fill_mib_index */
+
+static void sw_setup_mib(struct ksz_sw *sw)
+{
+	int i;
+	int j;
+	u8 index = 0;
+
+	for (i = 0; i < 3; i++) {
+		j = 2 - i;
+		sw->mib_interval_start[j] = index;
+		index = sw_fill_mib_index(sw, index, 1 << j);
+	}
+	memcpy(mib_index, mib_start, SWITCH_COUNTER_NUM);
+	j = MIB_9897_RX_BYTE_CNT;
+	for (i = 0; i < 4; i++, j++, index++) {
+		mib_start[index] = SWITCH_COUNTER_NUM + i;
+		mib_index[index] = j;
+	}
+}  /* sw_setup_mib */
+
+static void get_mib_cnt_info(u64 *cnt, u32 data[])
+{
+	u64 num;
+
+	if (data[0] & MIB_COUNTER_OVERFLOW) {
+		num = 1;
+		num <<= 32 + 4;
+		*cnt += num;
+	}
+	num = (data[0] & MIB_COUNTER_DATA_HI_M);
+	num <<= 32;
+	num |= data[1];
+	*cnt += num;
+}  /* get_mib_cnt_info */
+
+#ifndef CONFIG_KSZ_IBA_ONLY
+static int dbg_mib;
+
+/**
+ * sw_r_mib_cnt_hw - read MIB counters using default access
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ * @addr:	The addresses of the counters.
+ * @num:	Number of entries to read.
+ * @data:	Buffer to store the counters.
+ *
+ * This function reads MIB counters of the port using default access.
+ */
+static int sw_r_mib_cnt_hw(struct ksz_sw *sw, uint port, u32 addr[], int num,
+			   u32 data[])
+{
+	int i;
+	u32 ctrl_addr;
+	struct ksz_port_cfg *cfg = get_port_cfg(sw, port);
+	u32 freeze = cfg->freeze ? MIB_COUNTER_FLUSH_FREEZE : 0;
+
+	for (i = 0; i < num; i++) {
+		ctrl_addr = (addr[i] & MIB_COUNTER_INDEX_M);
+		ctrl_addr <<= MIB_COUNTER_INDEX_S;
+		ctrl_addr |= MIB_COUNTER_READ;
+		ctrl_addr |= freeze;
+
+#if 1
+		/*
+		 * First KSZ956X revision chip has a bug that writing 32-bit
+		 * data to the register does not trigger the read operation.
+		 */
+		port_w16(sw, port, REG_PORT_MIB_CTRL_STAT__4, ctrl_addr >> 16);
+#else
+		port_w32(sw, port, REG_PORT_MIB_CTRL_STAT__4, ctrl_addr);
+#endif
+		/*
+		 * Need to check the valid bit, but SPI access is slow enough
+		 * to have that bit always set when reading.
+		 */
+		do {
+			port_get(sw, port, REG_PORT_MIB_CTRL_STAT__4, data, 8);
+
+			data[0] = be32_to_cpu(data[0]);
+			if (!(data[0] & MIB_COUNTER_VALID) && dbg_mib++ < 5)
+				dbg_msg(" !valid: %08x"NL, data[0]);
+			data[1] = be32_to_cpu(data[1]);
+		} while (!(data[0] & MIB_COUNTER_VALID));
+		data += READ_MIB_ENTRY_SIZE;
+	}
+	return 1;
+}  /* sw_r_mib_cnt_hw */
+#endif
+
+static int exit_mib_read(struct ksz_sw *sw)
+{
+	if (sw->intr_using)
+		return true;
+	return false;
+}  /* exit_mib_read */
+
+/**
+ * port_r_cnt - read MIB counters periodically
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ *
+ * This routine is used to read the counters of the port periodically to avoid
+ * counter overflow.  The hardware should be acquired first before calling this
+ * routine.
+ *
+ * Return non-zero when not all counters not read.
+ */
+static int port_r_cnt(struct ksz_sw *sw, uint port)
+{
+	struct ksz_port_mib *mib = get_port_mib(sw, port);
+	u32 index[MAX_IBA_MIB_ENTRIES];
+	u32 data[MAX_IBA_MIB_ENTRIES * READ_MIB_ENTRY_SIZE];
+	u8 start;
+	int cnt;
+	int i;
+	int rc;
+
+	/* First read in this interval. */
+	if (!mib->cnt_ptr) {
+		u8 interval;
+
+		++mib->interval;
+		switch (mib->interval) {
+		case 2:
+			interval = 1;
+			break;
+		case 4:
+			interval = 2;
+			mib->interval = 0;
+			break;
+		default:
+			interval = 0;
+		}
+
+		/* Determine the starting index in this interval. */
+		mib->cnt_ptr = sw->mib_interval_start[interval];
+	}
+	while (mib->cnt_ptr < TOTAL_SWITCH_COUNTER_NUM) {
+		cnt = MAX_IBA_MIB_ENTRIES;
+		if (cnt > TOTAL_SWITCH_COUNTER_NUM - mib->cnt_ptr)
+			cnt = TOTAL_SWITCH_COUNTER_NUM - mib->cnt_ptr;
+		for (i = 0; i < cnt; i++)
+			index[i] = mib_index[mib->cnt_ptr + i];
+		sw->ops->acquire(sw);
+		rc = sw->reg->r_mib_cnt_hw(sw, port, index, cnt, data);
+		sw->ops->release(sw);
+		if (!rc)
+			return mib->cnt_ptr;
+		for (i = 0; i < cnt; i++, mib->cnt_ptr++) {
+			start = mib_start[mib->cnt_ptr];
+			get_mib_cnt_info(&mib->counter[start],
+				&data[i * READ_MIB_ENTRY_SIZE]);
+		}
+		if (exit_mib_read(sw))
+			return mib->cnt_ptr;
+	}
+	mib->cnt_ptr = 0;
+	return 0;
+}  /* port_r_cnt */
+
+static void port_freeze_mib(struct ksz_sw *sw, uint port, bool freeze)
+{
+	u32 ctrl = freeze ? MIB_COUNTER_FLUSH_FREEZE : 0;
+	struct ksz_port_cfg *cfg = get_port_cfg(sw, port);
+
+	cfg->freeze = !!freeze;
+	port_w32(sw, port, REG_PORT_MIB_CTRL_STAT__4, ctrl);
+}  /* port_freeze_mib */
+
+static void sw_freeze_mib(struct ksz_sw *sw, bool freeze)
+{
+	sw_cfg(sw, REG_SW_MAC_CTRL_6, SW_MIB_COUNTER_FREEZE, freeze);
+}  /* sw_freeze_mib */
+
+/**
+ * port_init_cnt - initialize MIB counter values
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ *
+ * This routine is used to initialize all counters to zero if the hardware
+ * cannot do it after reset.
+ */
+static inline void port_init_cnt(struct ksz_sw *sw, uint port)
+{
+	struct ksz_port_mib *mib = get_port_mib(sw, port);
+	u32 index[MAX_IBA_MIB_ENTRIES];
+	u32 data[MAX_IBA_MIB_ENTRIES * READ_MIB_ENTRY_SIZE];
+	int cnt;
+	int i;
+	int rc;
+
+	sw->ops->acquire(sw);
+	mib->cnt_ptr = 0;
+	mib->interval = 0;
+	do {
+		cnt = MAX_IBA_MIB_ENTRIES;
+		if (cnt > TOTAL_SWITCH_COUNTER_NUM - mib->cnt_ptr)
+			cnt = TOTAL_SWITCH_COUNTER_NUM - mib->cnt_ptr;
+		for (i = 0; i < cnt; i++, mib->cnt_ptr++)
+			index[i] = mib_index[mib->cnt_ptr];
+		rc = sw->reg->r_mib_cnt_hw(sw, port, index, cnt, data);
+		if (!rc)
+			break;
+	} while (mib->cnt_ptr < TOTAL_SWITCH_COUNTER_NUM);
+	memset((void *) mib->counter, 0, sizeof(u64) *
+		TOTAL_SWITCH_COUNTER_NUM);
+	mib->cnt_ptr = 0;
+	mib->rate[0].last = mib->rate[1].last = 0;
+	mib->rate[0].last_cnt = mib->rate[1].last_cnt = 0;
+	mib->rate[0].peak = mib->rate[1].peak = 0;
+	sw->ops->release(sw);
+}  /* port_init_cnt */
+
+/* -------------------------------------------------------------------------- */
+
+/* Bandwidth */
+
+static inline void port_cfg_broad_storm(struct ksz_sw *sw, uint p, bool set)
+{
+	port_cfg(sw, p,
+		P_BCAST_STORM_CTRL, PORT_BROADCAST_STORM, set);
+}
+
+static inline int port_chk_broad_storm(struct ksz_sw *sw, uint p)
+{
+	return port_chk(sw, p,
+		P_BCAST_STORM_CTRL, PORT_BROADCAST_STORM);
+}
+
+/* Driver set switch broadcast storm protection at 10% rate. */
+#define BROADCAST_STORM_PROTECTION_RATE	10
+
+/* 148,800 frames * 50 ms / 100 */
+#define BROADCAST_STORM_VALUE		7440
+
+/**
+ * sw_cfg_broad_storm - configure broadcast storm threshold
+ * @sw:		The switch instance.
+ * @percent:	Broadcast storm threshold in percent of transmit rate.
+ *
+ * This routine configures the broadcast storm threshold of the switch.
+ */
+static void sw_cfg_broad_storm(struct ksz_sw *sw, u8 percent)
+{
+	u16 data;
+	u32 value = ((u32) BROADCAST_STORM_VALUE * (u32) percent / 100);
+
+	if (value > BROADCAST_STORM_RATE)
+		value = BROADCAST_STORM_RATE;
+
+	data = sw->reg->r16(sw, S_REPLACE_VID_CTRL);
+	data &= ~BROADCAST_STORM_RATE;
+	data |= value;
+	sw->reg->w16(sw, S_REPLACE_VID_CTRL, data);
+}  /* sw_cfg_broad_storm */
+
+/**
+ * sw_get_board_storm - get broadcast storm threshold
+ * @sw:		The switch instance.
+ * @percent:	Buffer to store the broadcast storm threshold percentage.
+ *
+ * This routine retrieves the broadcast storm threshold of the switch.
+ */
+static void sw_get_broad_storm(struct ksz_sw *sw, u8 *percent)
+{
+	int num;
+	u16 data;
+
+	data = sw->reg->r16(sw, S_REPLACE_VID_CTRL);
+	num = (data & BROADCAST_STORM_RATE);
+	num = (num * 100 + BROADCAST_STORM_VALUE / 2) / BROADCAST_STORM_VALUE;
+	*percent = (u8) num;
+}  /* sw_get_broad_storm */
+
+/**
+ * sw_dis_broad_storm - disable broadcast storm protection
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ *
+ * This routine disables the broadcast storm limit function of the switch.
+ */
+static void sw_dis_broad_storm(struct ksz_sw *sw, uint port)
+{
+	port_cfg_broad_storm(sw, port, 0);
+}  /* sw_dis_broad_storm */
+
+/**
+ * sw_ena_broad_storm - enable broadcast storm protection
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ *
+ * This routine enables the broadcast storm limit function of the switch.
+ */
+static void sw_ena_broad_storm(struct ksz_sw *sw, uint port)
+{
+	sw_cfg_broad_storm(sw, sw->info->broad_per);
+	port_cfg_broad_storm(sw, port, 1);
+}  /* sw_ena_broad_storm */
+
+/**
+ * sw_init_broad_storm - initialize broadcast storm
+ * @sw:		The switch instance.
+ *
+ * This routine initializes the broadcast storm limit function of the switch.
+ */
+static void sw_init_broad_storm(struct ksz_sw *sw)
+{
+	u8 percent;
+
+	sw_get_broad_storm(sw, &percent);
+	sw->info->broad_per = percent;
+}  /* sw_init_broad_storm */
+
+/**
+ * hw_cfg_broad_storm - configure broadcast storm
+ * @sw:		The switch instance.
+ * @percent:	Broadcast storm threshold in percent of transmit rate.
+ *
+ * This routine configures the broadcast storm threshold of the switch.
+ * It is called by user functions.  The hardware should be acquired first.
+ */
+static void hw_cfg_broad_storm(struct ksz_sw *sw, u8 percent)
+{
+	if (percent > 100)
+		percent = 100;
+
+	sw_cfg_broad_storm(sw, percent);
+	sw_init_broad_storm(sw);
+}  /* hw_cfg_broad_storm */
+
+/**
+ * sw_setup_broad_storm - setup broadcast storm
+ * @sw:		The switch instance.
+ *
+ * This routine setup the broadcast storm limit function of the switch.
+ */
+static void sw_setup_broad_storm(struct ksz_sw *sw)
+{
+	uint n;
+	uint port;
+
+	/* Enable switch broadcast storm protection at 10% percent rate. */
+	hw_cfg_broad_storm(sw, BROADCAST_STORM_PROTECTION_RATE);
+	for (n = 0; n <= sw->mib_port_cnt; n++) {
+		port = get_phy_port(sw, n);
+		sw_ena_broad_storm(sw, port);
+	}
+	sw_cfg(sw, REG_SW_MAC_CTRL_1, MULTICAST_STORM_DISABLE, 1);
+}  /* sw_setup_broad_storm */
+
+/* -------------------------------------------------------------------------- */
+
+/* Rate Limit */
+
+/**
+ * hw_cfg_rate_limit - configure port rate limit modes
+ * @sw:		The switch instance.
+ * @cfg:	The port config structure.
+ * @p:		The base port index.
+ * @mask:	The mask value.
+ * @shift:	The shift position.
+ * @mode:	The rate limit mode.
+ *
+ * This helper routine configures the rate limit modes of the port.
+ */
+static void hw_cfg_rate_limit(struct ksz_sw *sw, struct ksz_port_cfg *cfg,
+			      uint p, u8 mask, u8 shift, u8 mode)
+{
+	u8 data;
+	u8 saved;
+
+	port_r8(sw, p, P_RATE_LIMIT_CTRL, &data);
+	saved = data;
+	data &= ~(mask << shift);
+	data |= mode << shift;
+	if (data != saved)
+		port_w8(sw, p, P_RATE_LIMIT_CTRL, data);
+	cfg->rate_limit = data;
+}  /* hw_cfg_rate_limit */
+
+static void hw_cfg_in_port_based(struct ksz_sw *sw, struct ksz_port_cfg *cfg,
+				 uint p, bool set)
+{
+	hw_cfg_rate_limit(sw, cfg, p, 1, PORT_IN_PORT_BASED_S, set);
+}
+
+static void hw_cfg_in_flow_ctrl(struct ksz_sw *sw, struct ksz_port_cfg *cfg,
+				uint p, bool set)
+{
+	hw_cfg_rate_limit(sw, cfg, p, 1, PORT_IN_FLOW_CTRL_S, set);
+}
+
+/**
+ * hw_cfg_cnt_ifg - configure port rate limit count IFG control
+ * @sw:		The switch instance.
+ * @cfg:	The port config structure.
+ * @p:		The base port index.
+ * @set:	The flag indicating whether the count control is set or not.
+ *
+ * This routine configures the rate limit count IFG control of the port.
+ */
+static void hw_cfg_cnt_ifg(struct ksz_sw *sw, struct ksz_port_cfg *cfg,
+			   uint p, bool set)
+{
+	hw_cfg_rate_limit(sw, cfg, p, 1, PORT_COUNT_IFG_S, set);
+}  /* hw_cfg_cnt_ifg */
+
+/**
+ * hw_cfg_cnt_pre - configure port rate limit count preamble control
+ * @sw:		The switch instance.
+ * @cfg:	The port config structure.
+ * @p:		The base port index.
+ * @set:	The flag indicating whether the count control is set or not.
+ *
+ * This routine configures the rate limit count preamble control of the port.
+ */
+static void hw_cfg_cnt_pre(struct ksz_sw *sw, struct ksz_port_cfg *cfg,
+			   uint p, bool set)
+{
+	hw_cfg_rate_limit(sw, cfg, p, 1, PORT_COUNT_PREAMBLE_S, set);
+}  /* hw_cfg_cnt_pre */
+
+/**
+ * hw_cfg_rx_limit - configure port rate limit mode
+ * @sw:		The switch instance.
+ * @cfg:	The port config structure.
+ * @p:		The base port index.
+ * @mode:	The rate limit mode.
+ *
+ * This routine configures the rate limit mode of the port.
+ */
+static void hw_cfg_rx_limit(struct ksz_sw *sw, struct ksz_port_cfg *cfg,
+			    uint p, u8 mode)
+{
+	if (mode > PORT_IN_LIMIT_MODE_M)
+		return;
+
+	hw_cfg_rate_limit(sw, cfg, p, PORT_IN_LIMIT_MODE_M,
+		PORT_IN_LIMIT_MODE_S, mode);
+}  /* hw_cfg_rx_limit */
+
+/**
+ * hw_get_rate_limit - get port rate limit control
+ * @sw:		The switch instance.
+ * @cfg:	The port config structure.
+ * @p:		The base port index.
+ *
+ * This routine retrieves the rate limit of the port.
+ */
+static void hw_get_rate_limit(struct ksz_sw *sw, struct ksz_port_cfg *cfg,
+			      uint p)
+{
+	u8 data;
+
+	port_r8(sw, p, P_RATE_LIMIT_CTRL, &data);
+	cfg->rate_limit = data;
+	cfg->packet_based = (data >> PORT_RATE_PACKET_BASED_S) & 1;
+}  /* hw_get_rate_limit */
+
+/* -------------------------------------------------------------------------- */
+
+static uint get_rate_from_val(u8 val)
+{
+	uint i;
+
+	if (0 == val)
+		i = 0;
+	else if (val <= 100)
+		i = 1000 * val;
+	else
+		i = 64 * (val - 100);
+	return i;
+}
+
+static int get_rate_to_val(uint rate)
+{
+	int i;
+
+	if (rate >= 1000) {
+		i = (rate + 500) / 1000;
+		if (i > 100)
+			i = 100;
+	} else if (0 == rate) {
+		i = 0;
+	} else {
+		i = (rate + 32) / 64;
+		if (0 == i)
+			i = 1;
+		else if (i > 15)
+			i = 15;
+		i += 100;
+	}
+	return i;
+}
+
+static uint get_packet_from_val(u8 val)
+{
+	uint i;
+
+	if (0 == val)
+		i = 0;
+	else if (val <= 100)
+		i = 1920 * val;
+	else if (101 == val)
+		i = 64;
+	else
+		i = 128 * (val - 101);
+	return i;
+}
+
+static int get_packet_to_val(uint rate)
+{
+	int i;
+
+	if (rate >= 1920) {
+		i = (rate + 960) / 1920;
+		if (i > 100)
+			i = 100;
+	} else if (0 == rate) {
+		i = 0;
+	} else if (rate <= 64) {
+		i = 101;
+	} else {
+		i = (rate + 64) / 128;
+		if (0 == i)
+			i = 1;
+		else if (i > 14)
+			i = 14;
+		i += 101;
+	}
+	return i;
+}
+
+/**
+ * port_cfg_rate - configure port priority rate
+ * @sw:		The switch instance.
+ * @p:		The base port index.
+ * @prio:	The priority index to configure.
+ * @offset:	The receive or transmit rate offset.
+ * @shift:	The shift position to set the value.
+ * @rate:	The rate limit in number of Kbps (or Kpps).
+ * @gigabit:	Gigabit indication.
+ * @packet:	Packet indication.
+ *
+ * This helper routine configures the priority rate of the port.
+ */
+static void port_cfg_rate(struct ksz_sw *sw, uint p, uint prio, uint offset,
+			  uint rate, bool gigabit, bool packet)
+{
+	u8 factor;
+
+	if (gigabit)
+		rate /= 10;
+	if (packet)
+		factor = (u8) get_packet_to_val(rate);
+	else
+		factor = (u8) get_rate_to_val(rate);
+
+	port_w8(sw, p, offset + prio, factor);
+}  /* port_cfg_rate */
+
+/**
+ * port_get_rate - get port priority rate
+ * @sw:		The switch instance.
+ * @p:		The base port index.
+ * @prio:	The priority index to configure.
+ * @offset:	The receive or transmit rate offset.
+ * @shift:	The shift position to get the value.
+ * @rate:	Buffer to store the data rate in number of Kbps (or Kpps).
+ * @gigabit:	Gigabit indication.
+ * @packet:	Packet indication.
+ *
+ * This helper routine retrieves the priority rate of the port.
+ */
+static void port_get_rate(struct ksz_sw *sw, uint p, uint prio, uint offset,
+			  uint *rate, bool gigabit, bool packet)
+{
+	u8 data;
+
+	port_r8(sw, p, offset + prio, &data);
+	if (packet)
+		*rate = get_packet_from_val(data);
+	else
+		*rate = get_rate_from_val(data);
+	if (gigabit)
+		*rate *= 10;
+}  /* port_get_rate */
+
+/**
+ * hw_cfg_prio_rate - configure port priority
+ * @sw:		The switch instance.
+ * @p:		The base port index.
+ * @prio:	The priority index to configure.
+ * @rate:	The rate limit in number of Kbps (or Kpps).
+ * @offset:	The receive or transmit rate offset.
+ * @result:	Buffer to store the data rate in number of Kbps (or Kpps).
+ * @gigabit:	Gigabit indication.
+ * @packet:	Packet indication.
+ *
+ * This helper routine configures the priority rate of the port and retrieves
+ * the actual rate number.
+ */
+static void hw_cfg_prio_rate(struct ksz_sw *sw, uint p, uint prio, uint rate,
+			     uint offset, uint *result, bool gigabit,
+			     bool packet)
+{
+	port_cfg_rate(sw, p, prio, offset, rate, gigabit, packet);
+	port_get_rate(sw, p, prio, offset, result, gigabit, packet);
+}  /* hw_cfg_prio_rate */
+
+/*
+ * THa  2016/02/24
+ * The receive rate limit does not take effect until the last priority is also
+ * written!  It can be turned off without writing the last priority.  Setting
+ * it turns on rate limiting but the hardware seems to use the last value.
+ */
+static void hw_set_rx_prio(struct ksz_sw *sw, uint p)
+{
+	u8 data;
+
+	port_r8(sw, p, REG_PORT_IN_RATE_0 + RX_PRIO_QUEUES - 1, &data);
+	port_w8(sw, p, REG_PORT_IN_RATE_0 + RX_PRIO_QUEUES - 1, data);
+}  /* hw_set_rx_prio */
+
+/**
+ * hw_cfg_rx_prio_rate - configure port receive priority
+ * @sw:		The switch instance.
+ * @cfg:	The port config structure.
+ * @p:		The base port index.
+ * @prio:	The priority index to configure.
+ * @rate:	The rate limit in number of Kbps (or Kpps).
+ *
+ * This routine configures the receive priority rate of the port.
+ * It is called by user functions.  The hardware should be acquired first.
+ */
+static void hw_cfg_rx_prio_rate(struct ksz_sw *sw, struct ksz_port_cfg *cfg,
+				uint p, uint prio, uint rate)
+{
+	uint *result;
+
+	if (cfg->packet_based)
+		result = &cfg->rx_packet[prio];
+	else
+		result = &cfg->rx_rate[prio];
+	hw_cfg_prio_rate(sw, p, prio, rate, REG_PORT_IN_RATE_0, result,
+			 cfg->gigabit, cfg->packet_based);
+}  /* hw_cfg_rx_prio_rate */
+
+/**
+ * hw_cfg_tx_prio_rate - configure port transmit priority
+ * @sw:		The switch instance.
+ * @cfg:	The port config structure.
+ * @p:		The base port index.
+ * @prio:	The priority index to configure.
+ * @rate:	The rate limit in number of Kbps (or Kpps).
+ *
+ * This routine configures the transmit priority rate of the port.
+ * It is called by user functions.  The hardware should be acquired first.
+ */
+static void hw_cfg_tx_prio_rate(struct ksz_sw *sw, struct ksz_port_cfg *cfg,
+				uint p, uint prio, uint rate)
+{
+	uint *result;
+
+	if (cfg->packet_based)
+		result = &cfg->tx_packet[prio];
+	else
+		result = &cfg->tx_rate[prio];
+	hw_cfg_prio_rate(sw, p, prio, rate, REG_PORT_OUT_RATE_0, result,
+			 cfg->gigabit, cfg->packet_based);
+}  /* hw_cfg_tx_prio_rate */
+
+/**
+ * sw_chk_rx_prio_rate - check switch rx priority rate
+ * @sw:		The switch instance.
+ * @p:		The base port index.
+ *
+ * This function checks whether the rx priority rate function of the switch is
+ * enabled.
+ *
+ * Return 0 if not enabled.
+ */
+static int sw_chk_rx_prio_rate(struct ksz_sw *sw, uint p)
+{
+	u32 rate_addr;
+	u32 in_rate0;
+	u32 in_rate1;
+
+	rate_addr = PORT_CTRL_ADDR(p, REG_PORT_IN_RATE_0);
+	in_rate0 = sw->reg->r32(sw, rate_addr);
+	in_rate1 = sw->reg->r32(sw, rate_addr + 4);
+	return (in_rate0 | in_rate1) != 0;
+}  /* sw_chk_rx_prio_rate */
+
+/**
+ * sw_chk_tx_prio_rate - check switch tx priority rate
+ * @sw:		The switch instance.
+ * @p:		The base port index.
+ *
+ * This function checks whether the tx priority rate function of the switch is
+ * enabled.
+ *
+ * Return 0 if not enabled.
+ */
+static int sw_chk_tx_prio_rate(struct ksz_sw *sw, uint p)
+{
+	u32 rate_addr;
+	u32 out_rate;
+
+	rate_addr = PORT_CTRL_ADDR(p, REG_PORT_OUT_RATE_0);
+	if (sw_chk(sw, REG_SW_MAC_CTRL_5, SW_OUT_RATE_LIMIT_QUEUE_BASED))
+		out_rate = sw->reg->r32(sw, rate_addr);
+
+	/* Only need to check first priority as the others do not matter. */
+	else
+		out_rate = sw->reg->r8(sw, rate_addr);
+	return (out_rate) != 0;
+}  /* sw_chk_tx_prio_rate */
+
+/**
+ * sw_dis_rx_prio_rate - disable switch rx priority rate
+ * @sw:		The switch instance.
+ * @p:		The base port index.
+ *
+ * This routine disables the rx priority rate function of the switch.
+ */
+static void sw_dis_rx_prio_rate(struct ksz_sw *sw, uint p)
+{
+	u32 rate_addr;
+
+	rate_addr = PORT_CTRL_ADDR(p, REG_PORT_IN_RATE_0);
+	sw->reg->w32(sw, rate_addr, 0);
+	sw->reg->w32(sw, rate_addr + 4, 0);
+}  /* sw_dis_rx_prio_rate */
+
+/**
+ * sw_dis_tx_prio_rate - disable switch tx priority rate
+ * @sw:		The switch instance.
+ * @p:		The base port index.
+ *
+ * This routine disables the tx priority rate function of the switch.
+ */
+static void sw_dis_tx_prio_rate(struct ksz_sw *sw, uint p)
+{
+	u32 rate_addr;
+
+	rate_addr = PORT_CTRL_ADDR(p, REG_PORT_OUT_RATE_0);
+	sw->reg->w32(sw, rate_addr, 0);
+}  /* sw_dis_tx_prio_rate */
+
+/**
+ * sw_ena_rx_prio_rate - enable switch rx priority rate
+ * @sw:		The switch instance.
+ * @cfg:	The port config structure.
+ * @p:		The base port index.
+ *
+ * This routine enables the rx priority rate function of the switch.
+ */
+static void sw_ena_rx_prio_rate(struct ksz_sw *sw, struct ksz_port_cfg *cfg,
+				uint p)
+{
+	int prio;
+	uint *rate;
+
+	if (cfg->packet_based)
+		rate = cfg->rx_packet;
+	else
+		rate = cfg->rx_rate;
+/*
+ * THa  2016/02/24
+ * The receive rate limit does not take effect until the last priority is also
+ * written!  It can be turned off without writing the last priority.  Setting
+ * it turns on rate limiting but the hardware seems to use the last value.
+ */
+	for (prio = 0; prio < RX_PRIO_QUEUES; prio++, rate++)
+		hw_cfg_rx_prio_rate(sw, cfg, p, prio, *rate);
+}  /* sw_ena_rx_prio_rate */
+
+/**
+ * sw_ena_tx_prio_rate - enable switch tx priority rate
+ * @sw:		The switch instance.
+ * @cfg:	The port config structure.
+ * @p:		The base port index.
+ *
+ * This routine enables the tx priority rate function of the switch.
+ */
+static void sw_ena_tx_prio_rate(struct ksz_sw *sw, struct ksz_port_cfg *cfg,
+				uint p)
+{
+	int prio;
+	uint *rate;
+
+	if (cfg->packet_based)
+		rate = cfg->tx_packet;
+	else
+		rate = cfg->tx_rate;
+	for (prio = 0; prio < PRIO_QUEUES; prio++, rate++)
+		hw_cfg_tx_prio_rate(sw, cfg, p, prio, *rate);
+}  /* sw_ena_tx_prio_rate */
+
+static void hw_cfg_rate_packet_based(struct ksz_sw *sw,
+				     struct ksz_port_cfg *cfg, uint p,
+				     bool set)
+{
+	int prio;
+	uint *rx_rate;
+	uint *tx_rate;
+
+	cfg->packet_based = set;
+	hw_cfg_rate_limit(sw, cfg, p, 1, PORT_RATE_PACKET_BASED_S, set);
+	if (cfg->packet_based) {
+		rx_rate = cfg->rx_packet;
+		tx_rate = cfg->tx_packet;
+	} else {
+		rx_rate = cfg->rx_rate;
+		tx_rate = cfg->tx_rate;
+	}
+	for (prio = 0; prio < PRIO_QUEUES; prio++, rx_rate++, tx_rate++) {
+
+		/* Rate limiting is not enabled. */
+		if (!cfg->rx_rate[prio] && !cfg->rx_packet[prio])
+			continue;
+		hw_cfg_rx_prio_rate(sw, cfg, p, prio, *rx_rate);
+
+		/* Rate limiting is not enabled. */
+		if (!cfg->tx_rate[prio] && !cfg->tx_packet[prio])
+			continue;
+		hw_cfg_tx_prio_rate(sw, cfg, p, prio, *tx_rate);
+	}
+	for (; prio < RX_PRIO_QUEUES; prio++, rx_rate++) {
+
+/* See issue above about configuring rx priority rates. */
+#if 0
+		/* Rate limiting is not enabled. */
+		if (!cfg->rx_rate[prio] && !cfg->rx_packet[prio])
+			continue;
+#endif
+		hw_cfg_rx_prio_rate(sw, cfg, p, prio, *rx_rate);
+	}
+}  /* hw_cfg_rate_packet_based */
+
+/**
+ * sw_init_prio_rate - initialize switch prioirty rate
+ * @sw:		The switch instance.
+ *
+ * This routine initializes the priority rate function of the switch.
+ */
+static void sw_init_prio_rate(struct ksz_sw *sw)
+{
+	uint n;
+	uint offset;
+	uint port;
+	uint prio;
+	struct ksz_port_cfg *cfg;
+	struct ksz_port_info *info;
+
+	for (n = 0; n <= sw->mib_port_cnt; n++) {
+		port = get_phy_port(sw, n);
+		cfg = get_port_cfg(sw, port);
+		info = get_port_info(sw, port);
+		cfg->gigabit = (info->tx_rate / TX_RATE_UNIT == 1000);
+		hw_get_rate_limit(sw, cfg, port);
+		for (prio = 0; prio < RX_PRIO_QUEUES; prio++) {
+			offset = REG_PORT_IN_RATE_0;
+			port_get_rate(sw, port, prio, offset,
+				      &cfg->rx_rate[prio], cfg->gigabit,
+				      false);
+			port_get_rate(sw, port, prio, offset,
+				      &cfg->rx_packet[prio], cfg->gigabit,
+				      true);
+		}
+		for (prio = 0; prio < PRIO_QUEUES; prio++) {
+			offset = REG_PORT_OUT_RATE_0;
+			port_get_rate(sw, port, prio, offset,
+				      &cfg->tx_rate[prio], cfg->gigabit,
+				      false);
+			port_get_rate(sw, port, prio, offset,
+				      &cfg->tx_packet[prio], cfg->gigabit,
+				      true);
+		}
+	}
+}  /* sw_init_prio_rate */
+
+/* -------------------------------------------------------------------------- */
+
+/* Communication */
+
+static inline void port_cfg_back_pressure(struct ksz_sw *sw, uint p, bool set)
+{
+	port_cfg(sw, p,
+		REG_PORT_MAC_CTRL_1, PORT_BACK_PRESSURE, set);
+}
+
+static inline void port_cfg_force_flow_ctrl(struct ksz_sw *sw, uint p, bool set)
+{
+	port_cfg(sw, p,
+		REG_PORT_CTRL_0,
+		PORT_FORCE_TX_FLOW_CTRL | PORT_FORCE_RX_FLOW_CTRL, set);
+}
+
+static inline void port_cfg_mac_loopback(struct ksz_sw *sw, uint p, bool set)
+{
+	port_cfg(sw, p,
+		REG_PORT_CTRL_0, PORT_MAC_LOOPBACK, set);
+}
+
+static void port_cfg_phy_loopback(struct ksz_sw *sw, uint p, bool set)
+{
+	u16 data;
+	struct ksz_port_cfg *cfg = get_port_cfg(sw, p);
+
+	port_r16(sw, p, REG_PORT_PHY_CTRL, &data);
+	if (set) {
+		if (!(data & PORT_PHY_LOOPBACK)) {
+			cfg->phy_ctrl = data;
+			data &= ~PORT_AUTO_NEG_ENABLE;
+			data |= PORT_PHY_LOOPBACK;
+		}
+	} else {
+		if (data & PORT_PHY_LOOPBACK)
+			data = cfg->phy_ctrl;
+	}
+	port_w16(sw, p, REG_PORT_PHY_CTRL, data);
+}
+
+static inline void port_cfg_remote_loopback(struct ksz_sw *sw, uint p, bool set)
+{
+	port_cfg16(sw, p,
+		REG_PORT_PHY_REMOTE_LB_LED, PORT_REMOTE_LOOPBACK, set);
+}
+
+static inline void port_cfg_tail_tag(struct ksz_sw *sw, uint p, bool set)
+{
+	port_cfg(sw, p,
+		REG_PORT_CTRL_0, PORT_TAIL_TAG_ENABLE, set);
+	if (p == sw->HOST_PORT) {
+		if (set)
+			sw->overrides |= TAIL_TAGGING;
+		else
+			sw->overrides &= ~TAIL_TAGGING;
+	}
+}
+
+static inline int port_chk_back_pressure(struct ksz_sw *sw, uint p)
+{
+	return port_chk(sw, p,
+		REG_PORT_MAC_CTRL_1, PORT_BACK_PRESSURE);
+}
+
+static inline int port_chk_force_flow_ctrl(struct ksz_sw *sw, uint p)
+{
+	return port_chk(sw, p,
+		REG_PORT_CTRL_0,
+		PORT_FORCE_TX_FLOW_CTRL | PORT_FORCE_RX_FLOW_CTRL);
+}
+
+static inline int port_chk_mac_loopback(struct ksz_sw *sw, uint p)
+{
+	return port_chk(sw, p,
+		REG_PORT_CTRL_0, PORT_MAC_LOOPBACK);
+}
+
+static inline int port_chk_phy_loopback(struct ksz_sw *sw, uint p)
+{
+	return port_chk16(sw, p,
+		REG_PORT_PHY_CTRL, PORT_PHY_LOOPBACK);
+}
+
+static inline int port_chk_remote_loopback(struct ksz_sw *sw, uint p)
+{
+	return port_chk16(sw, p,
+		REG_PORT_PHY_REMOTE_LB_LED, PORT_REMOTE_LOOPBACK);
+}
+
+static inline int port_chk_tail_tag(struct ksz_sw *sw, uint p)
+{
+	return port_chk(sw, p,
+		REG_PORT_CTRL_0, PORT_TAIL_TAG_ENABLE);
+}
+
+/* -------------------------------------------------------------------------- */
+
+/* Spanning Tree */
+
+static inline void port_cfg_mstp(struct ksz_sw *sw, uint p, u8 mstp)
+{
+	struct ksz_port_cfg *cfg = get_port_cfg(sw, p);
+
+	if (cfg->mstp != mstp) {
+		port_w(sw, p, REG_PORT_LUE_MSTP_INDEX, mstp);
+		cfg->mstp = mstp;
+	}
+}
+
+static inline void port_cfg_dis_learn(struct ksz_sw *sw, uint p, bool set)
+{
+	port_cfg(sw, p,
+		P_STP_CTRL, PORT_LEARN_DISABLE, set);
+}
+
+static inline void port_cfg_rx(struct ksz_sw *sw, uint p, bool set)
+{
+	struct ksz_port_cfg *cfg = get_port_cfg(sw, p);
+	uint m = BIT(p);
+
+	port_cfg(sw, p,
+		 P_STP_CTRL, PORT_RX_ENABLE, set);
+	if (set)
+		sw->rx_ports[cfg->mstp] |= m;
+	else
+		sw->rx_ports[cfg->mstp] &= ~m;
+}
+
+static inline void port_cfg_tx(struct ksz_sw *sw, uint p, bool set)
+{
+	struct ksz_port_cfg *cfg = get_port_cfg(sw, p);
+	uint m = BIT(p);
+
+	port_cfg(sw, p,
+		 P_STP_CTRL, PORT_TX_ENABLE, set);
+	if (set)
+		sw->tx_ports[cfg->mstp] |= m;
+	else
+		sw->tx_ports[cfg->mstp] &= ~m;
+}
+
+static inline u8 port_chk_mstp(struct ksz_sw *sw, uint p)
+{
+	SW_D mstp;
+
+	port_r(sw, p, REG_PORT_LUE_MSTP_INDEX, &mstp);
+	return mstp;
+}
+
+static inline int port_chk_dis_learn(struct ksz_sw *sw, uint p)
+{
+	return port_chk(sw, p,
+		P_STP_CTRL, PORT_LEARN_DISABLE);
+}
+
+static inline int port_chk_rx(struct ksz_sw *sw, uint p)
+{
+	return port_chk(sw, p,
+		P_STP_CTRL, PORT_RX_ENABLE);
+}
+
+static inline int port_chk_tx(struct ksz_sw *sw, uint p)
+{
+	return port_chk(sw, p,
+		P_STP_CTRL, PORT_TX_ENABLE);
+}
+
+static void port_cfg_rx_special(struct ksz_sw *sw, uint p, bool set)
+{
+	int hsr = false;
+
+#ifdef CONFIG_KSZ_HSR
+	/* Have HSR hardware. */
+	if (sw->features & REDUNDANCY_SUPPORT)
+		hsr = true;
+#endif
+	if (!hsr)
+		port_cfg_rx(sw, p, set);
+#ifdef CONFIG_KSZ_HSR
+	else {
+		if (set)
+			port_cfg_rx(sw, p, set);
+		do {
+			int n;
+			u16 mask = 0;
+
+			for (n = 0; n < sw->eth_cnt; n++) {
+				if (sw->eth_maps[n].proto & HSR_HW) {
+					mask = sw->eth_maps[n].mask;
+					break;
+				}
+			}
+			if (mask & (1 << p)) {
+				u32 val;
+
+				val = sw->reg->r32(sw, REG_HSR_PORT_MAP__4);
+				if (set)
+					val |= mask & ~(1 << p);
+				else
+					val &= (1 << p);
+				sw->reg->w32(sw, REG_HSR_PORT_MAP__4, val);
+			}
+		} while (0);
+		if (!set)
+			port_cfg_rx(sw, p, set);
+	}
+#endif
+}  /* port_cfg_rx_special */
+
+static int port_chk_power(struct ksz_sw *sw, uint p)
+{
+	u16 ctrl;
+
+	port_r16(sw, p, REG_PORT_PHY_CTRL, &ctrl);
+	return !(ctrl & PORT_POWER_DOWN);
+}  /* port_chk_power */
+
+static inline void sw_cfg_fast_aging(struct ksz_sw *sw, bool set)
+{
+	sw_cfg(sw, REG_SW_LUE_CTRL_1, SW_FAST_AGING, set);
+}
+
+static void sw_flush_dyn_mac_table(struct ksz_sw *sw, uint port)
+{
+	int cnt;
+	int first;
+	int index;
+	int learn_disable[TOTAL_PORT_NUM];
+	SW_D data;
+
+	data = SW_R(sw, REG_SW_LUE_CTRL_2);
+	data &= ~(SW_FLUSH_OPTION_M << SW_FLUSH_OPTION_S);
+	data |= (SW_FLUSH_OPTION_DYN_MAC << SW_FLUSH_OPTION_S);
+	SW_W(sw, REG_SW_LUE_CTRL_2, data);
+	if (port < sw->port_cnt) {
+		first = port;
+		cnt = port + 1;
+		for (index = first; index < cnt; index++) {
+			learn_disable[index] = port_chk_dis_learn(sw, index);
+			if (!learn_disable[index])
+				port_cfg_dis_learn(sw, index, 1);
+		}
+		sw_cfg(sw, S_FLUSH_TABLE_CTRL, SW_FLUSH_DYN_MAC_TABLE, 1);
+		for (index = first; index < cnt; index++) {
+			if (!learn_disable[index])
+				port_cfg_dis_learn(sw, index, 0);
+		}
+	} else {
+		sw_cfg(sw, S_FLUSH_TABLE_CTRL, SW_FLUSH_STP_TABLE, 1);
+	}
+}  /* sw_flush_dyn_mac_table */
+
+/* -------------------------------------------------------------------------- */
+
+/* VLAN */
+
+static inline void port_cfg_drop_non_vlan(struct ksz_sw *sw, uint p, bool set)
+{
+	port_cfg(sw, p,
+		REG_PORT_MRI_MAC_CTRL, PORT_DROP_NON_VLAN, set);
+}
+
+/**
+ * port_cfg_drop_tag - configure 802.1q tagged packet drop
+ * @sw:		The switch instance.
+ * @p:		The base port index.
+ * @set:	The flag to disable or enable.
+ *
+ */
+static inline void port_cfg_drop_tag(struct ksz_sw *sw, uint p, bool set)
+{
+	port_cfg(sw, p,
+		REG_PORT_MRI_MAC_CTRL, PORT_DROP_TAG, set);
+}
+
+static inline int port_chk_drop_non_vlan(struct ksz_sw *sw, uint p)
+{
+	return port_chk(sw, p,
+		REG_PORT_MRI_MAC_CTRL, PORT_DROP_NON_VLAN);
+}
+
+static inline int port_chk_drop_tag(struct ksz_sw *sw, uint p)
+{
+	return port_chk(sw, p,
+		REG_PORT_MRI_MAC_CTRL, PORT_DROP_TAG);
+}
+
+/**
+ * port_cfg_dis_non_vid - configure discard non VID
+ * @sw:		The switch instance.
+ * @p:		The base port index.
+ * @set:	The flag to disable or enable.
+ *
+ * This routine configures Discard Non VID packets of the switch port.
+ * If enabled, the device will discard packets whose VLAN id does not match
+ * ingress port-based default VLAN id.
+ */
+static inline void port_cfg_dis_non_vid(struct ksz_sw *sw, uint p, bool set)
+{
+	port_cfg(sw, p,
+		REG_PORT_LUE_CTRL, PORT_DISCARD_NON_VID, set);
+}  /* port_cfg_dis_non_vid */
+
+/**
+ * port_cfg_in_filter - configure ingress VLAN filtering
+ * @sw:		The switch instance.
+ * @p:		The base port index.
+ * @set:	The flag to disable or enable.
+ *
+ * This routine configures Ingress VLAN filtering of the switch port.
+ * If enabled, the device will discard packets whose VLAN id membership	in the
+ * VLAN table receive ports does not include the ingress port that received
+ * this packet.
+ */
+static inline void port_cfg_in_filter(struct ksz_sw *sw, uint p, bool set)
+{
+	port_cfg(sw, p,
+		REG_PORT_LUE_CTRL, PORT_INGRESS_FILTER, set);
+}  /* port_cfg_in_filter */
+
+static inline int port_chk_dis_non_vid(struct ksz_sw *sw, uint p)
+{
+	return port_chk(sw, p,
+		REG_PORT_LUE_CTRL, PORT_DISCARD_NON_VID);
+}
+
+static inline int port_chk_in_filter(struct ksz_sw *sw, uint p)
+{
+	return port_chk(sw, p,
+		REG_PORT_LUE_CTRL, PORT_INGRESS_FILTER);
+}
+
+/* -------------------------------------------------------------------------- */
+
+/* Mirroring */
+
+static inline void port_cfg_mirror_sniffer(struct ksz_sw *sw, uint p, bool set)
+{
+	port_cfg(sw, p,
+		P_MIRROR_CTRL, PORT_MIRROR_SNIFFER, set);
+}
+
+static inline void port_cfg_mirror_rx(struct ksz_sw *sw, uint p, bool set)
+{
+	port_cfg(sw, p,
+		P_MIRROR_CTRL, PORT_MIRROR_RX, set);
+}
+
+static inline void port_cfg_mirror_tx(struct ksz_sw *sw, uint p, bool set)
+{
+	port_cfg(sw, p,
+		P_MIRROR_CTRL, PORT_MIRROR_TX, set);
+}
+
+static inline void sw_cfg_mirror_rx_tx(struct ksz_sw *sw, bool set)
+{
+	sw_cfg(sw, S_MIRROR_CTRL, SW_MIRROR_RX_TX, set);
+}
+
+static inline int port_chk_mirror_sniffer(struct ksz_sw *sw, uint p)
+{
+	return port_chk(sw, p,
+		P_MIRROR_CTRL, PORT_MIRROR_SNIFFER);
+}
+
+static inline int port_chk_mirror_rx(struct ksz_sw *sw, uint p)
+{
+	return port_chk(sw, p,
+		P_MIRROR_CTRL, PORT_MIRROR_RX);
+}
+
+static inline int port_chk_mirror_tx(struct ksz_sw *sw, uint p)
+{
+	return port_chk(sw, p,
+		P_MIRROR_CTRL, PORT_MIRROR_TX);
+}
+
+static inline int sw_chk_mirror_rx_tx(struct ksz_sw *sw)
+{
+	return sw_chk(sw, S_MIRROR_CTRL, SW_MIRROR_RX_TX);
+}
+
+static void sw_setup_mirror(struct ksz_sw *sw)
+{
+	uint n;
+	uint port;
+
+	/*
+	 * The mirror sniffer port requires it to be in the port membership
+	 * of the receive and transmit ports.
+	 * For example, port 3 is the mirror port of traffic between ports 1
+	 * and 2.  Port 3 needs only to turn sniffer on; its port membership
+	 * can be 0.  Ordinarily the port membership of ports 1 and 2 is 3 for
+	 * just commnunicating with eath other.  It has to be set to 7 to pass
+	 * the frames to port 3.  Only one of the ports needs to turn on
+	 * receive and transmit mirroring.
+	 * The mirror receive and transmit mode requires at least two ports to
+	 * turn on receive and transmit mirroring.
+	 */
+	for (n = 0; n <= sw->mib_port_cnt; n++) {
+		port = get_phy_port(sw, n);
+		port_cfg_mirror_sniffer(sw, port, 0);
+		port_cfg_mirror_rx(sw, port, 0);
+		port_cfg_mirror_tx(sw, port, 0);
+	}
+	sw_cfg_mirror_rx_tx(sw, 0);
+}
+
+/* -------------------------------------------------------------------------- */
+
+/* Priority */
+
+static inline void port_cfg_diffserv(struct ksz_sw *sw, uint p, bool set)
+{
+	port_cfg(sw, p,
+		P_PRIO_CTRL, PORT_DIFFSERV_PRIO_ENABLE, set);
+}
+
+static inline void port_cfg_802_1p(struct ksz_sw *sw, uint p, bool set)
+{
+	port_cfg(sw, p,
+		P_PRIO_CTRL, PORT_802_1P_PRIO_ENABLE, set);
+}
+
+static inline void port_cfg_vlan_prio(struct ksz_sw *sw, uint p, bool set)
+{
+	port_cfg(sw, p,
+		P_PRIO_CTRL, PORT_VLAN_PRIO_ENABLE, set);
+}
+
+static inline void port_cfg_mac_prio(struct ksz_sw *sw, uint p, bool set)
+{
+	port_cfg(sw, p,
+		P_PRIO_CTRL, PORT_MAC_PRIO_ENABLE, set);
+}
+
+static inline void port_cfg_acl_prio(struct ksz_sw *sw, uint p, bool set)
+{
+	port_cfg(sw, p,
+		P_PRIO_CTRL, PORT_ACL_PRIO_ENABLE, set);
+}
+
+static inline void port_cfg_highest_prio(struct ksz_sw *sw, uint p, bool set)
+{
+	port_cfg(sw, p,
+		P_PRIO_CTRL, PORT_HIGHEST_PRIO, set);
+}
+
+static inline void port_cfg_or_prio(struct ksz_sw *sw, uint p, bool set)
+{
+	port_cfg(sw, p,
+		P_PRIO_CTRL, PORT_OR_PRIO, set);
+}
+
+static inline void port_cfg_replace_prio(struct ksz_sw *sw, uint p, bool set)
+{
+	port_cfg(sw, p,
+		REG_PORT_MRI_MAC_CTRL, PORT_USER_PRIO_CEILING, set);
+}
+
+static inline void port_set_prio_queue(struct ksz_sw *sw, uint p, uint queue)
+{
+	port_w_s(sw, p,
+		REG_PORT_CTRL_0, PORT_QUEUE_SPLIT_ENABLE, 0, queue);
+}
+
+static inline int port_chk_diffserv(struct ksz_sw *sw, uint p)
+{
+	return port_chk(sw, p,
+		P_PRIO_CTRL, PORT_DIFFSERV_PRIO_ENABLE);
+}
+
+static inline int port_chk_802_1p(struct ksz_sw *sw, uint p)
+{
+	return port_chk(sw, p,
+		P_PRIO_CTRL, PORT_802_1P_PRIO_ENABLE);
+}
+
+static inline int port_chk_vlan_prio(struct ksz_sw *sw, uint p)
+{
+	return port_chk(sw, p,
+		P_PRIO_CTRL, PORT_VLAN_PRIO_ENABLE);
+}
+
+static inline int port_chk_mac_prio(struct ksz_sw *sw, uint p)
+{
+	return port_chk(sw, p,
+		P_PRIO_CTRL, PORT_MAC_PRIO_ENABLE);
+}
+
+static inline int port_chk_acl_prio(struct ksz_sw *sw, uint p)
+{
+	return port_chk(sw, p,
+		P_PRIO_CTRL, PORT_ACL_PRIO_ENABLE);
+}
+
+static inline int port_chk_highest_prio(struct ksz_sw *sw, uint p)
+{
+	return port_chk(sw, p,
+		P_PRIO_CTRL, PORT_HIGHEST_PRIO);
+}
+
+static inline int port_chk_or_prio(struct ksz_sw *sw, uint p)
+{
+	return port_chk(sw, p,
+		P_PRIO_CTRL, PORT_OR_PRIO);
+}
+
+static inline int port_chk_replace_prio(struct ksz_sw *sw, uint p)
+{
+	return port_chk(sw, p,
+		REG_PORT_MRI_MAC_CTRL, PORT_USER_PRIO_CEILING);
+}
+
+static inline int port_get_prio_queue(struct ksz_sw *sw, uint p)
+{
+	return port_r_s(sw, p,
+		REG_PORT_CTRL_0, PORT_QUEUE_SPLIT_ENABLE, 0);
+}
+
+/* -------------------------------------------------------------------------- */
+
+/* Policing */
+
+static void port_cfg_index(struct ksz_sw *sw, uint port, uint p, uint q)
+{
+	u32 data;
+
+	data = (p & MRI_INDEX_P_M) << MRI_INDEX_P_S;
+	data |= (q & MRI_INDEX_Q_M) << MRI_INDEX_Q_S;
+	port_w32(sw, port, REG_PORT_MRI_INDEX__4, data);
+}
+
+static inline void port_cfg_police(struct ksz_sw *sw, uint p, bool set)
+{
+	port_cfg32(sw, p,
+		REG_PORT_MRI_POLICE_CTRL__4, POLICE_ENABLE, set);
+}
+
+static inline void port_cfg_color_aware(struct ksz_sw *sw, uint p, bool set)
+{
+	port_cfg32(sw, p,
+		REG_PORT_MRI_POLICE_CTRL__4, POLICE_COLOR_NOT_AWARE, !set);
+}
+
+static inline void port_cfg_drop_srp(struct ksz_sw *sw, uint p, bool set)
+{
+	port_cfg32(sw, p,
+		REG_PORT_MRI_POLICE_CTRL__4, POLICE_DROP_SRP, set);
+}
+
+static inline void port_cfg_color_mark(struct ksz_sw *sw, uint p, bool set)
+{
+	port_cfg32(sw, p,
+		REG_PORT_MRI_POLICE_CTRL__4, COLOR_MARK_ENABLE, set);
+}
+
+static inline void port_cfg_color_remap(struct ksz_sw *sw, uint p, bool set)
+{
+	port_cfg32(sw, p,
+		REG_PORT_MRI_POLICE_CTRL__4, COLOR_REMAP_ENABLE, set);
+}
+
+static inline void port_cfg_port_based_policing(struct ksz_sw *sw, uint p,
+	bool set)
+{
+	port_cfg32(sw, p,
+		REG_PORT_MRI_POLICE_CTRL__4, PORT_BASED_POLICING, set);
+}
+
+static inline void port_cfg_police_drop_all(struct ksz_sw *sw, uint p, bool set)
+{
+	port_cfg32(sw, p,
+		REG_PORT_MRI_POLICE_CTRL__4, POLICE_DROP_ALL, set);
+}
+
+static inline void port_set_police_packet_type(struct ksz_sw *sw, uint p,
+	u32 type)
+{
+	port_w_s_32(sw, p,
+		REG_PORT_MRI_POLICE_CTRL__4, POLICE_PACKET_TYPE_M,
+			POLICE_PACKET_TYPE_S, type);
+}
+
+static inline void port_set_non_dscp_color(struct ksz_sw *sw, uint p, u32 color)
+{
+	port_w_s_32(sw, p,
+		REG_PORT_MRI_POLICE_CTRL__4, SW_COLOR_M,
+			NON_DSCP_COLOR_S, color);
+}
+
+static inline int port_chk_police(struct ksz_sw *sw, uint p)
+{
+	return port_chk32(sw, p,
+		REG_PORT_MRI_POLICE_CTRL__4, POLICE_ENABLE);
+}
+
+static inline int port_chk_color_aware(struct ksz_sw *sw, uint p)
+{
+	return !port_chk32(sw, p,
+		REG_PORT_MRI_POLICE_CTRL__4, POLICE_COLOR_NOT_AWARE);
+}
+
+static inline int port_chk_drop_srp(struct ksz_sw *sw, uint p)
+{
+	return port_chk32(sw, p,
+		REG_PORT_MRI_POLICE_CTRL__4, POLICE_DROP_SRP);
+}
+
+static inline int port_chk_color_mark(struct ksz_sw *sw, uint p)
+{
+	return port_chk32(sw, p,
+		REG_PORT_MRI_POLICE_CTRL__4, COLOR_MARK_ENABLE);
+}
+
+static inline int port_chk_color_remap(struct ksz_sw *sw, uint p)
+{
+	return port_chk32(sw, p,
+		REG_PORT_MRI_POLICE_CTRL__4, COLOR_REMAP_ENABLE);
+}
+
+static inline int port_chk_port_based_policing(struct ksz_sw *sw, uint p)
+{
+	return port_chk32(sw, p,
+		REG_PORT_MRI_POLICE_CTRL__4, PORT_BASED_POLICING);
+}
+
+static inline int port_chk_police_drop_all(struct ksz_sw *sw, uint p)
+{
+	return port_chk32(sw, p,
+		REG_PORT_MRI_POLICE_CTRL__4, POLICE_DROP_ALL);
+}
+
+static inline u32 port_get_police_packet_type(struct ksz_sw *sw, uint p)
+{
+	return port_r_s_32(sw, p,
+		REG_PORT_MRI_POLICE_CTRL__4, POLICE_PACKET_TYPE_M,
+			POLICE_PACKET_TYPE_S);
+}
+
+static inline u32 port_get_non_dscp_color(struct ksz_sw *sw, uint p)
+{
+	return port_r_s_32(sw, p,
+		REG_PORT_MRI_POLICE_CTRL__4, SW_COLOR_M,
+			NON_DSCP_COLOR_S);
+}
+
+static inline u16 port_get_cir(struct ksz_sw *sw, uint p)
+{
+	return port_r_s_32(sw, p,
+		REG_PORT_POLICE_RATE__4, 0xffff, POLICE_CIR_S);
+}
+
+static inline u16 port_get_pir(struct ksz_sw *sw, uint p)
+{
+	return port_r_s_32(sw, p,
+		REG_PORT_POLICE_RATE__4, 0xffff, POLICE_PIR_S);
+}
+
+static inline void port_set_cir(struct ksz_sw *sw, uint p, u16 rate)
+{
+	port_w_s_32(sw, p,
+		REG_PORT_POLICE_RATE__4, 0xffff, POLICE_CIR_S, rate);
+}
+
+static inline void port_set_pir(struct ksz_sw *sw, uint p, u16 rate)
+{
+	port_w_s_32(sw, p,
+		REG_PORT_POLICE_RATE__4, 0xffff, POLICE_PIR_S, rate);
+}
+
+static inline u16 port_get_cbs(struct ksz_sw *sw, uint p)
+{
+	return port_r_s_32(sw, p,
+		REG_PORT_POLICE_BURST_SIZE__4, POLICE_BURST_SIZE_M,
+			POLICE_CBS_S);
+}
+
+static inline u16 port_get_pbs(struct ksz_sw *sw, uint p)
+{
+	return port_r_s_32(sw, p,
+		REG_PORT_POLICE_BURST_SIZE__4, POLICE_BURST_SIZE_M,
+			POLICE_PBS_S);
+}
+
+static inline void port_set_cbs(struct ksz_sw *sw, uint p, u16 size)
+{
+	port_w_s_32(sw, p,
+		REG_PORT_POLICE_BURST_SIZE__4, POLICE_BURST_SIZE_M,
+			POLICE_CBS_S, size);
+}
+
+static inline void port_set_pbs(struct ksz_sw *sw, uint p, u16 size)
+{
+	port_w_s_32(sw, p,
+		REG_PORT_POLICE_BURST_SIZE__4, POLICE_BURST_SIZE_M,
+			POLICE_PBS_S, size);
+}
+
+static inline u16 port_get_wred_max(struct ksz_sw *sw, uint p)
+{
+	return port_r_s_32(sw, p,
+		REG_PORT_WRED_PM_CTRL_0__4, WRED_PM_CTRL_M,
+			WRED_PM_MAX_THRESHOLD_S);
+}
+
+static inline u16 port_get_wred_min(struct ksz_sw *sw, uint p)
+{
+	return port_r_s_32(sw, p,
+		REG_PORT_WRED_PM_CTRL_0__4, WRED_PM_CTRL_M,
+			WRED_PM_MIN_THRESHOLD_S);
+}
+
+static inline u16 port_get_wred_multiplier(struct ksz_sw *sw, uint p)
+{
+	return port_r_s_32(sw, p,
+		REG_PORT_WRED_PM_CTRL_1__4, WRED_PM_CTRL_M,
+			WRED_PM_MULTIPLIER_S);
+}
+
+static inline u16 port_get_wred_avg_size(struct ksz_sw *sw, uint p)
+{
+	return port_r_s_32(sw, p,
+		REG_PORT_WRED_PM_CTRL_1__4, WRED_PM_CTRL_M,
+			WRED_PM_AVG_QUEUE_SIZE_S);
+}
+
+static inline void port_set_wred_max(struct ksz_sw *sw, uint p, u16 threshold)
+{
+	port_w_s_32(sw, p,
+		REG_PORT_WRED_PM_CTRL_0__4, WRED_PM_CTRL_M,
+			WRED_PM_MAX_THRESHOLD_S, threshold);
+}
+
+static inline void port_set_wred_min(struct ksz_sw *sw, uint p, u16 threshold)
+{
+	port_w_s_32(sw, p,
+		REG_PORT_WRED_PM_CTRL_0__4, WRED_PM_CTRL_M,
+			WRED_PM_MIN_THRESHOLD_S, threshold);
+}
+
+static inline void port_set_wred_multiplier(struct ksz_sw *sw, uint p, u16 val)
+{
+	port_w_s_32(sw, p,
+		REG_PORT_WRED_PM_CTRL_1__4, WRED_PM_CTRL_M,
+			WRED_PM_MULTIPLIER_S, val);
+}
+
+static inline u16 port_get_wred_q_max(struct ksz_sw *sw, uint p)
+{
+	return port_r_s_32(sw, p,
+		REG_PORT_WRED_QUEUE_CTRL_0__4, WRED_PM_CTRL_M,
+			WRED_PM_MAX_THRESHOLD_S);
+}
+
+static inline u16 port_get_wred_q_min(struct ksz_sw *sw, uint p)
+{
+	return port_r_s_32(sw, p,
+		REG_PORT_WRED_QUEUE_CTRL_0__4, WRED_PM_CTRL_M,
+			WRED_PM_MIN_THRESHOLD_S);
+}
+
+static inline u16 port_get_wred_q_multiplier(struct ksz_sw *sw, uint p)
+{
+	return port_r_s_32(sw, p,
+		REG_PORT_WRED_QUEUE_CTRL_1__4, WRED_PM_CTRL_M,
+			WRED_PM_MULTIPLIER_S);
+}
+
+static inline u16 port_get_wred_q_avg_size(struct ksz_sw *sw, uint p)
+{
+	return port_r_s_32(sw, p,
+		REG_PORT_WRED_QUEUE_CTRL_1__4, WRED_PM_CTRL_M,
+			WRED_PM_AVG_QUEUE_SIZE_S);
+}
+
+static inline void port_set_wred_q_max(struct ksz_sw *sw, uint p, u16 threshold)
+{
+	port_w_s_32(sw, p,
+		REG_PORT_WRED_QUEUE_CTRL_0__4, WRED_PM_CTRL_M,
+			WRED_PM_MAX_THRESHOLD_S, threshold);
+}
+
+static inline void port_set_wred_q_min(struct ksz_sw *sw, uint p, u16 threshold)
+{
+	port_w_s_32(sw, p,
+		REG_PORT_WRED_QUEUE_CTRL_0__4, WRED_PM_CTRL_M,
+			WRED_PM_MIN_THRESHOLD_S, threshold);
+}
+
+static inline void port_set_wred_q_multiplier(struct ksz_sw *sw, uint p, u16 val)
+{
+	port_w_s_32(sw, p,
+		REG_PORT_WRED_QUEUE_CTRL_1__4, WRED_PM_CTRL_M,
+			WRED_PM_MULTIPLIER_S, val);
+}
+
+static inline void port_cfg_wred_random_drop(struct ksz_sw *sw, uint p, bool set)
+{
+	port_cfg32(sw, p,
+		REG_PORT_WRED_QUEUE_PMON__4, WRED_RANDOM_DROP_ENABLE, set);
+}
+
+static inline void port_cfg_wred_drop_gyr(struct ksz_sw *sw, uint p, bool set)
+{
+	port_cfg32(sw, p,
+		REG_PORT_WRED_QUEUE_PMON__4, WRED_DROP_GYR_DISABLE, !set);
+}
+
+static inline void port_cfg_wred_drop_yr(struct ksz_sw *sw, uint p, bool set)
+{
+	port_cfg32(sw, p,
+		REG_PORT_WRED_QUEUE_PMON__4, WRED_DROP_YR_DISABLE, !set);
+}
+
+static inline void port_cfg_wred_drop_r(struct ksz_sw *sw, uint p, bool set)
+{
+	port_cfg32(sw, p,
+		REG_PORT_WRED_QUEUE_PMON__4, WRED_DROP_R_DISABLE, !set);
+}
+
+static inline void port_cfg_wred_drop_all(struct ksz_sw *sw, uint p, bool set)
+{
+	port_cfg32(sw, p,
+		REG_PORT_WRED_QUEUE_PMON__4, WRED_DROP_ALL, set);
+}
+
+static inline int port_chk_wred_random_drop(struct ksz_sw *sw, uint p)
+{
+	return port_chk32(sw, p,
+		REG_PORT_WRED_QUEUE_PMON__4, WRED_RANDOM_DROP_ENABLE);
+}
+
+static inline int port_chk_wred_drop_gyr(struct ksz_sw *sw, uint p)
+{
+	return !port_chk32(sw, p,
+		REG_PORT_WRED_QUEUE_PMON__4, WRED_DROP_GYR_DISABLE);
+}
+
+static inline int port_chk_wred_drop_yr(struct ksz_sw *sw, uint p)
+{
+	return !port_chk32(sw, p,
+		REG_PORT_WRED_QUEUE_PMON__4, WRED_DROP_YR_DISABLE);
+}
+
+static inline int port_chk_wred_drop_r(struct ksz_sw *sw, uint p)
+{
+	return !port_chk32(sw, p,
+		REG_PORT_WRED_QUEUE_PMON__4, WRED_DROP_R_DISABLE);
+}
+
+static inline int port_chk_wred_drop_all(struct ksz_sw *sw, uint p)
+{
+	return port_chk32(sw, p,
+		REG_PORT_WRED_QUEUE_PMON__4, WRED_DROP_ALL);
+}
+
+static u32 port_get_wred_pmon(struct ksz_sw *sw, uint p)
+{
+	return port_r_s_32(sw, p,
+		REG_PORT_WRED_QUEUE_PMON__4, WRED_PMON_M, 0);
+}
+
+/* -------------------------------------------------------------------------- */
+
+/* Rate Control */
+
+#ifdef MTI_PREEMPT_ENABLE
+static inline void port_cfg_preempt(struct ksz_sw *sw, uint p, uint q, bool set)
+{
+	port_w32(sw, p, REG_PORT_MTI_QUEUE_INDEX__4, q);
+	port_cfg(sw, p,
+		REG_PORT_MTI_QUEUE_CTRL_0, MTI_PREEMPT_ENABLE, set);
+}
+
+static inline int port_chk_preempt(struct ksz_sw *sw, uint p, uint q)
+{
+	port_w32(sw, p, REG_PORT_MTI_QUEUE_INDEX__4, q);
+	return port_chk(sw, p,
+		REG_PORT_MTI_QUEUE_CTRL_0, MTI_PREEMPT_ENABLE);
+}
+#endif
+
+static inline u8 port_get_schedule_mode(struct ksz_sw *sw, uint p, uint q)
+{
+	port_w32(sw, p, REG_PORT_MTI_QUEUE_INDEX__4, q);
+	return port_r_s(sw, p,
+		REG_PORT_MTI_QUEUE_CTRL_0, MTI_SCHEDULE_MODE_M,
+			MTI_SCHEDULE_MODE_S);
+}
+
+static inline u8 port_get_shaping(struct ksz_sw *sw, uint p, uint q)
+{
+	port_w32(sw, p, REG_PORT_MTI_QUEUE_INDEX__4, q);
+	return port_r_s(sw, p,
+		REG_PORT_MTI_QUEUE_CTRL_0, MTI_SHAPING_M,
+			MTI_SHAPING_S);
+}
+
+static inline u8 port_get_tx_ratio(struct ksz_sw *sw, uint p, uint q)
+{
+	u8 data;
+
+	port_w32(sw, p, REG_PORT_MTI_QUEUE_INDEX__4, q);
+	port_r(sw, p, REG_PORT_MTI_QUEUE_CTRL_1, &data);
+	return data;
+}
+
+/**
+ * port_set_schedule_mode - configure port rate control
+ * @sw:		The switch instance.
+ * @p:		The port index.
+ * @q:		The priority queue.
+ * @mode:	The schedule mode to specify strict priority or WRR.
+ *
+ * This routine configures the priority queue rate control of the port.
+ */
+static inline void port_set_schedule_mode(struct ksz_sw *sw, uint p, uint q,
+	u8 mode)
+{
+	port_w32(sw, p, REG_PORT_MTI_QUEUE_INDEX__4, q);
+	port_w_s(sw, p,
+		REG_PORT_MTI_QUEUE_CTRL_0, MTI_SCHEDULE_MODE_M,
+			MTI_SCHEDULE_MODE_S, mode);
+}  /* port_set_schedule_mode */
+
+static inline void port_set_shaping(struct ksz_sw *sw, uint p, uint q,
+	u8 shaping)
+{
+	port_w32(sw, p, REG_PORT_MTI_QUEUE_INDEX__4, q);
+	port_w_s(sw, p,
+		REG_PORT_MTI_QUEUE_CTRL_0, MTI_SHAPING_M,
+			MTI_SHAPING_S, shaping);
+}
+
+/**
+ * port_set_tx_ratio - configure port rate ratio
+ * @sw:		The switch instance.
+ * @p:		The port index.
+ * @q:		The priority queue.
+ * @ratio:	The rate ratio.
+ *
+ * This routine configures the priority queue rate ratio of the port.
+ */
+static inline void port_set_tx_ratio(struct ksz_sw *sw, uint p, uint q, u8 ratio)
+{
+	port_w32(sw, p, REG_PORT_MTI_QUEUE_INDEX__4, q);
+	port_w(sw, p, REG_PORT_MTI_QUEUE_CTRL_1, ratio & MTI_TX_RATIO_M);
+}  /* port_set_tx_ratio */
+
+static u16 port_get_hi_water_mark(struct ksz_sw *sw, uint p, uint q)
+{
+	u16 data;
+
+	port_w32(sw, p, REG_PORT_MTI_QUEUE_INDEX__4, q);
+	port_r16(sw, p, REG_PORT_MTI_QUEUE_CTRL_2__2, &data);
+	return data;
+}  /* port_get_hi_water_mark */
+
+static u16 port_get_lo_water_mark(struct ksz_sw *sw, uint p, uint q)
+{
+	u16 data;
+
+	port_w32(sw, p, REG_PORT_MTI_QUEUE_INDEX__4, q);
+	port_r16(sw, p, REG_PORT_MTI_QUEUE_CTRL_3__2, &data);
+	return data;
+}  /* port_get_lo_water_mark */
+
+static u16 port_get_increment(struct ksz_sw *sw, uint p, uint q)
+{
+	u16 data;
+
+	port_w32(sw, p, REG_PORT_MTI_QUEUE_INDEX__4, q);
+	port_r16(sw, p, REG_PORT_MTI_QUEUE_CTRL_4__2, &data);
+	return data;
+}  /* port_get_increment */
+
+static u8 port_get_srp(struct ksz_sw *sw, uint p)
+{
+	u8 data;
+
+	port_r(sw, p, REG_PORT_CTRL_1, &data);
+	return data & PORT_SRP_ENABLE;
+}  /* port_get_srp */
+
+static void port_set_hi_water_mark(struct ksz_sw *sw, uint p, uint q, u16 val)
+{
+	port_w32(sw, p, REG_PORT_MTI_QUEUE_INDEX__4, q);
+	port_w16(sw, p, REG_PORT_MTI_QUEUE_CTRL_2__2, val);
+}  /* port_set_hi_water_mark */
+
+static void port_set_lo_water_mark(struct ksz_sw *sw, uint p, uint q, u16 val)
+{
+	port_w32(sw, p, REG_PORT_MTI_QUEUE_INDEX__4, q);
+	port_w16(sw, p, REG_PORT_MTI_QUEUE_CTRL_3__2, val);
+}  /* port_set_lo_water_mark */
+
+static void port_set_increment(struct ksz_sw *sw, uint p, uint q, u16 val)
+{
+	port_w32(sw, p, REG_PORT_MTI_QUEUE_INDEX__4, q);
+	port_w16(sw, p, REG_PORT_MTI_QUEUE_CTRL_4__2, val);
+}  /* port_set_increment */
+
+static void port_set_srp(struct ksz_sw *sw, uint p, u8 srp)
+{
+	port_w(sw, p, REG_PORT_CTRL_1, srp & PORT_SRP_ENABLE);
+}  /* port_set_srp */
+
+/* -------------------------------------------------------------------------- */
+
+/* Queue Management */
+
+static inline u8 port_get_qm_drop(struct ksz_sw *sw, uint p)
+{
+	return (u8) port_r_s_32(sw, p, REG_PORT_QM_CTRL__4,
+		PORT_QM_DROP_PRIO_M, 0);
+}
+
+static u8 port_get_qm_burst_size(struct ksz_sw *sw, uint p)
+{
+	return (u8) port_r_s_32(sw, p, REG_PORT_QM_QUEUE_INDEX__4,
+		3, PORT_QM_BURST_SIZE_S);
+}
+
+static u16 port_get_qm_resv_space(struct ksz_sw *sw, uint p)
+{
+	return (u16) port_r_s_32(sw, p, REG_PORT_QM_QUEUE_INDEX__4,
+		PORT_QM_MIN_RESV_SPACE_M, 0);
+}
+
+static u16 port_get_qm_hi_water_mark(struct ksz_sw *sw, uint p, uint q)
+{
+	port_w_s_32(sw, p, REG_PORT_QM_QUEUE_INDEX__4,
+		PRIO_QUEUES_M, PORT_QM_QUEUE_INDEX_S, q);
+	return (u16) port_r_s_32(sw, p, REG_PORT_QM_WATER_MARK__4,
+		PORT_QM_WATER_MARK_M, PORT_QM_HI_WATER_MARK_S);
+}
+
+static u16 port_get_qm_lo_water_mark(struct ksz_sw *sw, uint p, uint q)
+{
+	port_w_s_32(sw, p, REG_PORT_QM_QUEUE_INDEX__4,
+		PRIO_QUEUES_M, PORT_QM_QUEUE_INDEX_S, q);
+	return (u16) port_r_s_32(sw, p, REG_PORT_QM_WATER_MARK__4,
+		PORT_QM_WATER_MARK_M, PORT_QM_LO_WATER_MARK_S);
+}
+
+static u16 port_get_qm_tx_used(struct ksz_sw *sw, uint p, uint q)
+{
+	port_w_s_32(sw, p, REG_PORT_QM_QUEUE_INDEX__4,
+		PRIO_QUEUES_M, PORT_QM_QUEUE_INDEX_S, q);
+	return (u16) port_r_s_32(sw, p, REG_PORT_QM_TX_CNT_0__4,
+		PORT_QM_TX_CNT_M, PORT_QM_TX_CNT_USED_S);
+}
+
+static u16 port_get_qm_tx_avail(struct ksz_sw *sw, uint p, uint q)
+{
+	port_w_s_32(sw, p, REG_PORT_QM_QUEUE_INDEX__4,
+		PRIO_QUEUES_M, PORT_QM_QUEUE_INDEX_S, q);
+	return (u16) port_r_s_32(sw, p, REG_PORT_QM_TX_CNT_1__4,
+		PORT_QM_TX_CNT_M, PORT_QM_TX_CNT_AVAIL_S);
+}
+
+static u16 port_get_qm_tx_calculated(struct ksz_sw *sw, uint p, uint q)
+{
+	port_w_s_32(sw, p, REG_PORT_QM_QUEUE_INDEX__4,
+		PRIO_QUEUES_M, PORT_QM_QUEUE_INDEX_S, q);
+	return (u16) port_r_s_32(sw, p, REG_PORT_QM_TX_CNT_1__4,
+		PORT_QM_TX_CNT_M, PORT_QM_TX_CNT_CALCULATED_S);
+}
+
+static inline void port_set_qm_drop(struct ksz_sw *sw, uint p, u8 drop)
+{
+	port_w_s_32(sw, p, REG_PORT_QM_CTRL__4,
+		PORT_QM_DROP_PRIO_M, 0, drop);
+}
+
+static inline void port_set_qm_burst_size(struct ksz_sw *sw, uint p, u8 burst)
+{
+	port_w_s_32(sw, p, REG_PORT_QM_QUEUE_INDEX__4,
+		3, PORT_QM_BURST_SIZE_S, burst);
+}
+
+static inline void port_set_qm_resv_space(struct ksz_sw *sw, uint p, u16 space)
+{
+	port_w_s_32(sw, p, REG_PORT_QM_QUEUE_INDEX__4,
+		PORT_QM_MIN_RESV_SPACE_M, 0, space);
+}
+
+static void port_set_qm_hi_water_mark(struct ksz_sw *sw, uint p, uint q, u16 val)
+{
+	port_w_s_32(sw, p, REG_PORT_QM_QUEUE_INDEX__4,
+		PRIO_QUEUES_M, PORT_QM_QUEUE_INDEX_S, q);
+	port_w_s_32(sw, p, REG_PORT_QM_WATER_MARK__4,
+		PORT_QM_WATER_MARK_M, PORT_QM_HI_WATER_MARK_S, val);
+}
+
+static void port_set_qm_lo_water_mark(struct ksz_sw *sw, uint p, uint q, u16 val)
+{
+	port_w_s_32(sw, p, REG_PORT_QM_QUEUE_INDEX__4,
+		PRIO_QUEUES_M, PORT_QM_QUEUE_INDEX_S, q);
+	port_w_s_32(sw, p, REG_PORT_QM_WATER_MARK__4,
+		PORT_QM_WATER_MARK_M, PORT_QM_LO_WATER_MARK_S, val);
+}
+
+/* -------------------------------------------------------------------------- */
+
+/**
+ * sw_set_tos_prio - program switch TOS priority
+ * @sw:		The switch instance.
+ * @tos:	ToS value from 6-bit (bit7 ~ bit2) of ToS field, ranging from 0
+ *		to 63.
+ * @prio:	Priority to be assigned.
+ *
+ * This routine programs the TOS priority into the switch registers.
+ */
+static void sw_set_tos_prio(struct ksz_sw *sw, u8 tos, SW_D prio)
+{
+	SW_W(sw, S_TOS_PRIO_CTRL + tos * SW_SIZE, prio);
+}  /* sw_set_tos_prio */
+
+/**
+ * sw_dis_diffserv - disable switch DiffServ priority
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ *
+ * This routine disables the DiffServ priority function of the switch.
+ */
+static void sw_dis_diffserv(struct ksz_sw *sw, uint port)
+{
+	port_cfg_diffserv(sw, port, 0);
+}  /* sw_dis_diffserv */
+
+/**
+ * sw_ena_diffserv - enable switch DiffServ priority
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ *
+ * This routine enables the DiffServ priority function of the switch.
+ */
+static void sw_ena_diffserv(struct ksz_sw *sw, uint port)
+{
+	port_cfg_diffserv(sw, port, 1);
+}  /* sw_ena_diffserv */
+
+/**
+ * hw_cfg_tos_prio - configure TOS priority
+ * @sw:		The switch instance.
+ * @tos:	ToS value from 6-bit (bit7 ~ bit2) of ToS field, ranging from 0
+ *		to 63.
+ * @prio:	Priority to be assigned.
+ *
+ * This routine configures the TOS priority in the hardware.
+ * DiffServ Value 0 ~ 63 is mapped to Priority Queue Number 0 ~ 7.
+ * It is called by user functions.  The hardware should be acquired first.
+ */
+static void hw_cfg_tos_prio(struct ksz_sw *sw, u8 tos, SW_D prio)
+{
+	int shift;
+	SW_D data = prio;
+	SW_D mask = KS_PRIO_M;
+	SW_D regmask = KS_PRIO_M;
+
+	if (tos >= DIFFSERV_ENTRIES)
+		return;
+
+	if (prio >= 0x10)
+		mask = 0xff;
+	else if (prio > KS_PRIO_M)
+		mask = 0xf;
+	if (prio >= 0x10)
+		regmask = (KS_PRIO_M << KS_PRIO_S) | KS_PRIO_M;
+	shift = (tos & (KS_PRIO_IN_REG - 1)) * KS_PRIO_S;
+	prio = prio << shift;
+	if (prio >> shift != data)
+		return;
+	mask <<= shift;
+	regmask <<= shift;
+	prio &= regmask;
+	tos /= KS_PRIO_IN_REG;
+
+	sw->info->diffserv[tos] &= ~mask;
+	sw->info->diffserv[tos] |= prio;
+
+	sw_set_tos_prio(sw, tos, sw->info->diffserv[tos]);
+}  /* hw_cfg_tos_prio */
+
+/* -------------------------------------------------------------------------- */
+
+/**
+ * sw_set_802_1p_prio - program switch 802.1p priority
+ * @sw:		The switch instance.
+ * @prio:	Priority to be assigned.
+ *
+ * This routine programs the 802.1p priority into the switch register.
+ */
+static void sw_set_802_1p_prio(struct ksz_sw *sw, u8 tag, SW_D prio)
+{
+	SW_W(sw, S_802_1P_PRIO_CTRL + tag / SW_SIZE, prio);
+}  /* sw_set_802_1p_prio */
+
+/**
+ * sw_dis_802_1p - disable switch 802.1p priority
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ *
+ * This routine disables the 802.1p priority function of the switch.
+ */
+static void sw_dis_802_1p(struct ksz_sw *sw, uint port)
+{
+	port_cfg_802_1p(sw, port, 0);
+}  /* sw_dis_802_1p */
+
+/**
+ * sw_ena_802_1p - enable switch 802.1p priority
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ *
+ * This routine enables the 802.1p priority function of the switch.
+ */
+static void sw_ena_802_1p(struct ksz_sw *sw, uint port)
+{
+	port_cfg_802_1p(sw, port, 1);
+}  /* sw_ena_802_1p */
+
+/**
+ * hw_cfg_802_1p_prio - configure 802.1p priority
+ * @sw:		The switch instance.
+ * @tag:	The 802.1p tag priority value, ranging from 0 to 7.
+ * @prio:	Priority to be assigned.
+ *
+ * This routine configures the 802.1p priority in the hardware.
+ * 802.1p Tag priority value 0 ~ 7 is mapped to Priority Queue Number 0 ~ 7.
+ * It is called by user functions.  The hardware should be acquired first.
+ */
+static void hw_cfg_802_1p_prio(struct ksz_sw *sw, u8 tag, SW_D prio)
+{
+	int shift;
+	SW_D data = prio;
+	SW_D mask = KS_PRIO_M;
+	SW_D regmask = KS_PRIO_M;
+
+	if (tag >= PRIO_802_1P_ENTRIES)
+		return;
+
+	if (prio >= 0x10)
+		mask = 0xff;
+	else if (prio > KS_PRIO_M)
+		mask = 0xf;
+	if (prio >= 0x10)
+		regmask = (KS_PRIO_M << KS_PRIO_S) | KS_PRIO_M;
+	shift = (tag & (KS_PRIO_IN_REG - 1)) * KS_PRIO_S;
+	prio = prio << shift;
+	if (prio >> shift != data)
+		return;
+	mask <<= shift;
+	regmask <<= shift;
+	prio &= regmask;
+	tag /= KS_PRIO_IN_REG;
+
+	sw->info->p_802_1p[tag] &= ~mask;
+	sw->info->p_802_1p[tag] |= prio;
+
+	sw_set_802_1p_prio(sw, tag, sw->info->p_802_1p[tag]);
+}  /* hw_cfg_802_1p_prio */
+
+/**
+ * sw_cfg_replace_null_vid - enable switch null VID replacement
+ * @sw:		The switch instance.
+ * @p:		The port index.
+ * @set:	The flag to disable or enable.
+ *
+ * This routine enables the VID to be replaced with port default VID if it is
+ * empty.
+ */
+static void sw_cfg_replace_null_vid(struct ksz_sw *sw, uint p, bool set)
+{
+	port_cfg32(sw, p, REG_PORT_MTI_QUEUE_CTRL_0__4, MTI_PVID_REPLACE, set);
+}  /* sw_cfg_replace_null_vid */
+
+/**
+ * sw_cfg_replace_prio - enable switch 802.1p priority re-mapping
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ * @set:	The flag to disable or enable.
+ *
+ * This routine enables the 802.1p priority re-mapping function of the switch.
+ * That allows 802.1p priority field to be replaced with the port's default
+ * tag's priority value if the ingress packet's 802.1p priority has a higher
+ * priority than port's default tag's priority.
+ */
+static void sw_cfg_replace_prio(struct ksz_sw *sw, uint port, bool set)
+{
+	port_cfg_replace_prio(sw, port, set);
+}  /* sw_cfg_replace_prio */
+
+/* -------------------------------------------------------------------------- */
+
+/**
+ * sw_cfg_port_based - configure switch port based priority
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ * @prio:	The priority to set.
+ *
+ * This routine configures the port based priority of the switch.
+ */
+static void sw_cfg_port_based(struct ksz_sw *sw, uint port, u8 prio)
+{
+	SW_D data;
+	struct ksz_port_cfg *cfg = get_port_cfg(sw, port);
+
+	if (prio > PORT_BASED_PRIO_M)
+		prio = PORT_BASED_PRIO_M;
+
+	port_r(sw, port, REG_PORT_MRI_MAC_CTRL, &data);
+	data &= ~(PORT_BASED_PRIO_M << PORT_BASED_PRIO_S);
+	data |= prio << PORT_BASED_PRIO_S;
+	port_w(sw, port, REG_PORT_MRI_MAC_CTRL, data);
+
+	cfg->port_prio = prio;
+}  /* sw_cfg_port_based */
+
+/* -------------------------------------------------------------------------- */
+
+/**
+ * sw_set_multi_queue - enable transmit multiple queues
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ * @queue:	The queue register setting.
+ *
+ * This routine enables the transmit multiple queues selection of the switch
+ * port.  The port transmit queue is split into two or four priority queues.
+ */
+static void sw_set_multi_queue(struct ksz_sw *sw, uint port, uint queue)
+{
+	port_set_prio_queue(sw, port, queue);
+
+	/* Default is port based for egress rate limit. */
+	if (queue)
+		sw_cfg(sw, REG_SW_MAC_CTRL_5, SW_OUT_RATE_LIMIT_QUEUE_BASED,
+			1);
+}  /* sw_set_multi_queue */
+
+/**
+ * sw_init_prio - initialize switch priority
+ * @sw:		The switch instance.
+ *
+ * This routine initializes the switch QoS priority functions.
+ */
+static void sw_init_prio(struct ksz_sw *sw)
+{
+	uint n;
+	uint port;
+	int tos;
+	SW_D data;
+	struct ksz_port_cfg *port_cfg;
+
+	for (tos = 0; tos < PRIO_802_1P_ENTRIES / KS_PRIO_IN_REG; tos++)
+		sw->info->p_802_1p[tos] =
+			SW_R(sw, S_802_1P_PRIO_CTRL + tos * SW_SIZE);
+
+	for (tos = 0; tos < DIFFSERV_ENTRIES / KS_PRIO_IN_REG; tos++)
+		sw->info->diffserv[tos] =
+			SW_R(sw, S_TOS_PRIO_CTRL + tos * SW_SIZE);
+
+	for (n = 0; n <= sw->mib_port_cnt; n++) {
+		port = get_phy_port(sw, n);
+		port_cfg = get_port_cfg(sw, port);
+		port_r(sw, port, REG_PORT_MRI_MAC_CTRL, &data);
+		data &= PORT_BASED_PRIO_M;
+		port_cfg->port_prio = data;
+		for (tos = 0; tos < PRIO_802_1P_ENTRIES / 8; tos++)
+			port_r32(sw, port, REG_PORT_MRI_TC_MAP__4 - tos * 4,
+				&port_cfg->tc_map[tos]);
+
+		for (tos = 0; tos < DIFFSERV_ENTRIES / 16; tos++)
+			port_r32(sw, port, REG_PORT_POLICE_COLOR_3__4 -
+				tos * 4, &port_cfg->color_map[tos]);
+	}
+}  /* sw_init_prio */
+
+static void port_set_color_map(struct ksz_sw *sw, uint port, u8 tos, u32 prio)
+{
+	port_w32(sw, port, REG_PORT_POLICE_COLOR_3__4 - tos * 4, prio);
+}  /* port_set_color_map */
+
+static void port_set_tc_map(struct ksz_sw *sw, uint port, u8 tos, u32 prio)
+{
+	port_w32(sw, port, REG_PORT_MRI_TC_MAP__4 + tos * 4, prio);
+}  /* port_set_color_map */
+
+/**
+ * port_cfg_color_map - configure port police color map
+ * @sw:		The switch instance.
+ * @tos:	ToS value.
+ * @prio:	Priority to be assigned.
+ *
+ * This routine configures the color map in the hardware.
+ */
+static void port_cfg_color_map(struct ksz_sw *sw, uint port, u8 tos, u32 prio)
+{
+	int shift;
+	u32 data = prio;
+	u32 mask = POLICE_COLOR_MAP_M;
+	struct ksz_port_cfg *port_cfg = get_port_cfg(sw, port);
+
+	if (tos >= DIFFSERV_ENTRIES)
+		return;
+
+	if (prio >= 0x10000)
+		mask = 0xffffffff;
+	else if (prio >= 0x100)
+		mask = 0xffff;
+	else if (prio >= 0x10)
+		mask = 0xff;
+	else if (prio > POLICE_COLOR_MAP_M)
+		mask = 0xf;
+	shift = (tos & (16 - 1)) * POLICE_COLOR_MAP_S;
+	prio = prio << shift;
+	if (prio >> shift != data)
+		return;
+	mask <<= shift;
+	tos /= 16;
+
+	port_cfg->color_map[tos] &= ~mask;
+	port_cfg->color_map[tos] |= prio;
+
+	port_set_color_map(sw, port, tos, port_cfg->color_map[tos]);
+}  /* port_cfg_color_map */
+
+/**
+ * port_cfg_tc_map - configure port traffic class map
+ * @sw:		The switch instance.
+ * @tc:		Traffic class.
+ * @prio:	Priority to be assigned.
+ *
+ * This routine configures the traffic class mapping in the hardware.
+ */
+static void port_cfg_tc_map(struct ksz_sw *sw, uint port, u8 tc, u32 prio)
+{
+	int shift;
+	u32 data = prio;
+	u32 mask = PORT_TC_MAP_M;
+	u32 regmask = PORT_TC_MAP_M;
+	struct ksz_port_cfg *port_cfg = get_port_cfg(sw, port);
+
+	if (tc >= PRIO_802_1P_ENTRIES)
+		return;
+
+	if (prio >= 0x10000)
+		mask = 0xffffffff;
+	else if (prio >= 0x100)
+		mask = 0xffff;
+	else if (prio >= 0x10)
+		mask = 0xff;
+	else if (prio > PORT_TC_MAP_M)
+		mask = 0xf;
+	if (prio >= 0x10000)
+		regmask = 0x33333333;
+	else if (prio >= 0x100)
+		regmask = 0x3333;
+	else if (prio >= 0x10)
+		regmask = 0x33;
+	shift = (tc & (8 - 1)) * PORT_TC_MAP_S;
+	prio = prio << shift;
+	if (prio >> shift != data)
+		return;
+	mask <<= shift;
+	regmask <<= shift;
+	prio &= regmask;
+	tc /= 8;
+
+	port_cfg->tc_map[tc] &= ~mask;
+	port_cfg->tc_map[tc] |= prio;
+
+	port_set_tc_map(sw, port, tc, port_cfg->tc_map[tc]);
+}  /* port_cfg_tc_map */
+
+/**
+ * sw_setup_prio - setup switch priority
+ * @sw:		The switch instance.
+ *
+ * This routine setup the switch QoS priority functions.
+ */
+static void sw_setup_prio(struct ksz_sw *sw)
+{
+	uint n;
+	uint port;
+	uint q;
+	struct ksz_port_cfg *cfg;
+
+	/* All QoS functions disabled. */
+	for (n = 0; n <= sw->mib_port_cnt; n++) {
+		port = get_phy_port(sw, n);
+		cfg = get_port_cfg(sw, port);
+
+		sw_set_multi_queue(sw, port, 2);
+		sw_dis_diffserv(sw, port);
+		sw_cfg_replace_prio(sw, port, 0);
+		sw_cfg_replace_null_vid(sw, port, 0);
+		sw_cfg_port_based(sw, port, cfg->port_prio);
+
+		sw_ena_802_1p(sw, port);
+		for (q = 0; q < PRIO_QUEUES; q++)
+			port_set_tx_ratio(sw, port, q, 1 << q);
+		if (sw->features & AVB_SUPPORT) {
+			cfg->tc_map[0] = 0x11113200;
+			port_set_tc_map(sw, port, 0, cfg->tc_map[0]);
+			cfg->port_prio = 4;
+			sw_cfg_port_based(sw, port, cfg->port_prio);
+		}
+		sw->reg->w8(sw, REG_PTP_EVENT_PRIO_CTRL, PTP_PRIO_ENABLE | 7);
+		sw->reg->w8(sw, REG_PTP_GENERAL_PRIO_CTRL, PTP_PRIO_ENABLE | 7);
+	}
+	sw->ctrl_queue = (1 << 2) - 1;
+	if (sw->features & AVB_SUPPORT)
+		sw->ctrl_queue = 1;
+}  /* sw_setup_prio */
+
+/* -------------------------------------------------------------------------- */
+
+/**
+ * port_cfg_def_vid - configure port default VID
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ * @vid:	The VID value.
+ *
+ * This routine configures the default VID of the port.
+ */
+static void port_cfg_def_vid(struct ksz_sw *sw, uint port, u16 vid)
+{
+	port_w16(sw, port, REG_PORT_DEFAULT_VID, vid);
+}  /* port_cfg_def_vid */
+
+/**
+ * port_get_def_vid - get port default VID.
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ * @vid:	Buffer to store the VID.
+ *
+ * This routine retrieves the default VID of the port.
+ */
+static void port_get_def_vid(struct ksz_sw *sw, uint port, u16 *vid)
+{
+	port_r16(sw, port, REG_PORT_DEFAULT_VID, vid);
+}  /* port_get_def_vid */
+
+/**
+ * sw_cfg_def_vid - configure switch port default VID
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ * @vid:	The VID value.
+ *
+ * This routine configures the default VID of the port.
+ */
+static void sw_cfg_def_vid(struct ksz_sw *sw, uint port, u16 vid)
+{
+	struct ksz_port_cfg *cfg = get_port_cfg(sw, port);
+
+	cfg->vid = vid;
+	port_cfg_def_vid(sw, port, vid);
+}  /* sw_cfg_def_vid */
+
+/**
+ * sw_cfg_port_base_vlan - configure port-based VLAN membership
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ * @member:	The port-based VLAN membership.
+ *
+ * This routine configures the port-based VLAN membership of the port.
+ */
+static void sw_cfg_port_base_vlan(struct ksz_sw *sw, uint port, u16 member)
+{
+	struct ksz_port_cfg *cfg = get_port_cfg(sw, port);
+
+	cfg->member = member;
+	port_w32(sw, port, REG_PORT_VLAN_MEMBERSHIP__4, member);
+}  /* sw_cfg_port_base_vlan */
+
+static void sw_cfg_default_vlan(struct ksz_sw *sw, int reset)
+{
+	struct ksz_vlan_table vlan;
+
+	if (sw->overrides & VLAN_SET)
+		return;
+	sw->ops->release(sw);
+	vlan.vid = 1;
+	sw_r_vlan_table(sw, vlan.vid, &vlan);
+	vlan.ports = sw->PORT_MASK;
+	if (reset)
+		vlan.untag = 0;
+	else
+		vlan.untag = vlan.ports;
+	vlan.valid = vlan.ports != 0;
+	sw_w_vlan_table(sw, vlan.vid, &vlan);
+	if (!reset) {
+		vlan.vid = 0;
+		vlan.ports = sw->PORT_MASK;
+		vlan.untag = 0;
+		vlan.valid = 1;
+		sw_w_vlan_table(sw, vlan.vid, &vlan);
+	}
+	sw->ops->acquire(sw);
+}  /* sw_cfg_default_vlan */
+
+/**
+ * sw_dis_vlan - disable switch VLAN
+ * @sw:		The switch instance.
+ *
+ * This routine disables the VLAN function of the switch.
+ */
+static void sw_dis_vlan(struct ksz_sw *sw)
+{
+	sw_cfg(sw, REG_SW_LUE_CTRL_0, SW_VLAN_ENABLE, 0);
+	sw_cfg_default_vlan(sw, true);
+}  /* sw_dis_vlan */
+
+/**
+ * sw_ena_vlan - enable switch VLAN
+ * @sw:		The switch instance.
+ *
+ * This routine enables the VLAN function of the switch.
+ */
+static void sw_ena_vlan(struct ksz_sw *sw)
+{
+	uint n;
+	uint p;
+	u32 val;
+
+	/* Hardware may remove priority tag with VID 0. */
+	for (n = 0; n <= sw->mib_port_cnt; n++) {
+		p = get_phy_port(sw, n);
+		port_cfg(sw, p, REG_PORT_LUE_CTRL, PORT_VLAN_LOOKUP_VID_0,
+			true);
+	}
+	sw_cfg_default_vlan(sw, false);
+
+	/* Enable 802.1q VLAN mode. */
+	val = sw->reg->r32(sw, REG_SW_QM_CTRL__4);
+	val |= UNICAST_VLAN_BOUNDARY;
+	sw->reg->w32(sw, REG_SW_QM_CTRL__4, val);
+	sw_cfg(sw, REG_SW_LUE_CTRL_0, SW_VLAN_ENABLE, 1);
+	for (n = 1; n <= sw->mib_port_cnt; n++) {
+		p = get_phy_port(sw, n);
+		port_cfg_in_filter(sw, p, true);
+	}
+}  /* sw_ena_vlan */
+
+/**
+ * sw_init_vlan - initialize switch VLAN
+ * @sw:		The switch instance.
+ *
+ * This routine initializes the VLAN function of the switch.
+ */
+static void sw_init_vlan(struct ksz_sw *sw)
+{
+	uint n;
+	uint port;
+	u32 data;
+	struct ksz_port_cfg *cfg;
+
+	for (n = 0; n <= sw->mib_port_cnt; n++) {
+		port = get_phy_port(sw, n);
+		cfg = get_port_cfg(sw, port);
+		port_get_def_vid(sw, port, &cfg->vid);
+		port_r32(sw, port, REG_PORT_VLAN_MEMBERSHIP__4, &data);
+		cfg->member = (u16) data;
+		cfg->vid_member = (u16) data;
+	}
+}  /* sw_init_vlan */
+
+static void inc_mac_addr(u8 *dst, u8 *src, u8 inc)
+{
+#ifdef USE_SAME_ADDR
+	inc = 0;
+#endif
+	memcpy(dst, src, ETH_ALEN);
+	dst[5] += inc;
+	if (dst[5] < src[5])
+		dst[4]++;
+	if (dst[4] < src[4])
+		dst[3]++;
+}  /* inc_mac_addr */
+
+/**
+ * sw_get_addr - get the switch MAC address.
+ * @sw:		The switch instance.
+ * @mac_addr:	Buffer to store the MAC address.
+ *
+ * This function retrieves the MAC address of the switch.
+ */
+static inline void sw_get_addr(struct ksz_sw *sw, u8 *mac_addr)
+{
+	sw->reg->r(sw, REG_SW_MAC_ADDR_0, mac_addr, 6);
+	memcpy(sw->info->mac_addr, mac_addr, 6);
+}  /* sw_get_addr */
+
+/**
+ * sw_set_addr - configure switch MAC address
+ * @sw:		The switch instance.
+ * @mac_addr:	The MAC address.
+ *
+ * This function configures the MAC address of the switch.
+ */
+static void sw_set_addr(struct ksz_sw *sw, u8 *mac_addr)
+{
+	uint i;
+	uint n;
+	struct ksz_port_info *info;
+
+	for (n = 0; n <= sw->mib_port_cnt; n++) {
+		i = get_phy_port(sw, n);
+		info = get_port_info(sw, i);
+		inc_mac_addr(info->mac_addr, mac_addr, n);
+	}
+	sw->reg->w(sw, REG_SW_MAC_ADDR_0, mac_addr, 6);
+	memcpy(sw->info->mac_addr, mac_addr, 6);
+#ifdef CONFIG_KSZ_IBA
+#ifndef CONFIG_KSZ_IBA_ONLY
+	prepare_iba(&sw->info->iba, mac_addr, sw->info->iba.src);
+#endif
+#endif
+}  /* sw_set_addr */
+
+static void sw_setup_reserved_multicast(struct ksz_sw *sw)
+{
+	struct ksz_mac_table table[8];
+	u16 addr[8];
+	int i;
+	bool update;
+
+	memset(table, 0, sizeof(struct ksz_mac_table) * 8);
+	for (i = 0; i < 8; i++)
+		addr[i] = get_mcast_reserved_addr(i);
+
+	sw_r_m_sta_mac_table(sw, addr, 1, 8, table);
+	update = table[6].ports != sw->HOST_MASK ||
+		 sw->PORT_MASK != (1 << sw->port_cnt) - 1 ||
+		 (sw->features & MRP_SUPPORT);
+	if (update) {
+		table[1].ports = 0;
+		table[2].override = true;
+		table[2].ports = sw->HOST_MASK;
+		table[3].ports = sw->PORT_MASK;
+		if (sw->features & MRP_SUPPORT) {
+			table[4].override = true;
+			table[4].ports = sw->HOST_MASK;
+			table[5].override = true;
+			table[5].ports = sw->HOST_MASK;
+		} else {
+			table[4].ports = sw->PORT_MASK;
+			table[5].ports = sw->PORT_MASK;
+		}
+		table[6].override = true;
+		table[6].ports = sw->HOST_MASK;
+		table[7].ports = sw->PORT_MASK & ~sw->HOST_MASK;
+		sw_w_m_sta_mac_table(sw, addr, true, 8, table);
+	}
+	memcpy(sw->info->reserved_table, table,
+	       sizeof(struct ksz_mac_table) * ACTUAL_MCAST_TABLE_ENTRIES);
+}  /* sw_setup_reserved_multicast */
+
+static int sw_get_gbit(struct ksz_sw *sw, u8 data)
+{
+	int gbit;
+
+	if (sw->features & NEW_XMII)
+		gbit = !(data & PORT_MII_NOT_1GBIT);
+	else
+		gbit = data & PORT_MII_1000MBIT_S1;
+	return gbit;
+}  /* sw_get_gbit */
+
+static void sw_set_gbit(struct ksz_sw *sw, int gbit, u8 *data)
+{
+	if (sw->features & NEW_XMII) {
+		if (gbit)
+			*data &= ~PORT_MII_NOT_1GBIT;
+		else
+			*data |= PORT_MII_NOT_1GBIT;
+	} else {
+		if (gbit)
+			*data |= PORT_MII_1000MBIT_S1;
+		else
+			*data &= ~PORT_MII_1000MBIT_S1;
+	}
+}  /* sw_set_gbit */
+
+static int sw_get_xmii(struct ksz_sw *sw, u8 data)
+{
+	int mode;
+
+	if (sw->features & NEW_XMII) {
+		switch (data & PORT_MII_SEL_M) {
+		case PORT_MII_SEL:
+			mode = 0;
+			break;
+		case PORT_RMII_SEL:
+			mode = 1;
+			break;
+		case PORT_GMII_SEL:
+			mode = 2;
+			break;
+		default:
+			mode = 3;
+		}
+	} else {
+		switch (data & PORT_MII_SEL_M) {
+		case PORT_MII_SEL_S1:
+			mode = 0;
+			break;
+		case PORT_RMII_SEL_S1:
+			mode = 1;
+			break;
+		case PORT_GMII_SEL_S1:
+			mode = 2;
+			break;
+		default:
+			mode = 3;
+		}
+	}
+	return mode;
+}  /* sw_get_xmii */
+
+static void sw_set_xmii(struct ksz_sw *sw, int mode, u8 *data)
+{
+	u8 xmii;
+
+	if (sw->features & NEW_XMII) {
+		switch (mode) {
+		case 0:
+			xmii = PORT_MII_SEL;
+			break;
+		case 1:
+			xmii = PORT_RMII_SEL;
+			break;
+		case 2:
+			xmii = PORT_GMII_SEL;
+			break;
+		default:
+			xmii = PORT_RGMII_SEL;
+		}
+	} else {
+		switch (mode) {
+		case 0:
+			xmii = PORT_MII_SEL_S1;
+			break;
+		case 1:
+			xmii = PORT_RMII_SEL_S1;
+			break;
+		case 2:
+			xmii = PORT_GMII_SEL_S1;
+			break;
+		default:
+			xmii = PORT_RGMII_SEL_S1;
+		}
+	}
+	*data &= ~PORT_MII_SEL_M;
+	*data |= xmii;
+}  /* sw_set_xmii */
+
+#if defined(CONFIG_KSZ_AVB) || defined(CONFIG_KSZ_MRP)
+#include "ksz_mrp.c"
+#endif
+
+#define STP_ENTRY			(MULTI_MAC_TABLE_ENTRIES - 2)
+#define DEV_0_ADDR_ENTRY		0
+#define DEV_1_ADDR_ENTRY		1
+#define BRIDGE_ADDR_ENTRY		2
+
+/**
+ * sw_set_global_ctrl - set switch global control
+ * @sw:		The switch instance.
+ *
+ * This routine sets the global control of the switch function.
+ */
+static void sw_set_global_ctrl(struct ksz_sw *sw)
+{
+	SW_D data;
+	bool setup_xmii = sw->HOST_PORT >= sw->phy_port_cnt;
+
+#ifdef CONFIG_KSZ_IBA
+	if (IBA_USE_CODE_PREPARE <= sw->info->iba.use_iba)
+		setup_xmii = 0;
+#endif
+	if (setup_xmii) {
+		struct phy_device *phydev = sw->phy[0];
+		struct ksz_port_info *info = get_port_info(sw, sw->HOST_PORT);
+		int interface = phydev->interface;
+		int duplex = 1;
+		int speed = 0;
+		int gbit;
+		int mode;
+
+		/* Allow slower speed to be used for testing purpose. */
+#ifdef USE_10_MBIT_MODE
+		phydev->speed = SPEED_10;
+		phydev->dev_flags |= 1;
+#endif
+#ifdef USE_HALF_DUPLEX
+		phydev->duplex = DUPLEX_HALF;
+		phydev->dev_flags |= 1;
+#endif
+#if defined(USE_MII_PHY) || defined(USE_RGMII_PHY)
+		phydev->dev_flags |= 2;
+#endif
+		if (sw->netdev[0]) {
+			struct ksz_port *sw_port = sw->netport[0];
+
+			if (sw_port->flow_ctrl == PHY_NO_FLOW_CTRL) {
+				if (sw->features & IS_9893) {
+					port_cfg_force_flow_ctrl(sw,
+						sw->HOST_PORT, 0);
+				} else {
+					port_r(sw, sw->HOST_PORT,
+						REG_PORT_XMII_CTRL_0, &data);
+					data &= ~(PORT_MII_TX_FLOW_CTRL |
+						  PORT_MII_RX_FLOW_CTRL);
+					port_w(sw, sw->HOST_PORT,
+						REG_PORT_XMII_CTRL_0, data);
+				}
+			} else {
+				if (sw->features & IS_9893) {
+					port_cfg_force_flow_ctrl(sw,
+						sw->HOST_PORT, 1);
+				} else {
+					port_r(sw, sw->HOST_PORT,
+						REG_PORT_XMII_CTRL_0, &data);
+					data |= (PORT_MII_TX_FLOW_CTRL |
+						 PORT_MII_RX_FLOW_CTRL);
+					port_w(sw, sw->HOST_PORT,
+						REG_PORT_XMII_CTRL_0, data);
+				}
+			}
+		}
+		if ((sw->features & NO_GLOBAL_RESET) ||
+		    (phydev->dev_flags & 1)) {
+			port_r(sw, sw->HOST_PORT, REG_PORT_XMII_CTRL_0, &data);
+			data |= PORT_MII_100MBIT;
+			data |= PORT_MII_FULL_DUPLEX;
+			if (phydev->dev_flags & 1) {
+				if (SPEED_10 == phydev->speed)
+					data &= ~PORT_MII_100MBIT;
+				if (DUPLEX_HALF == phydev->duplex)
+					data &= ~PORT_MII_FULL_DUPLEX;
+			}
+			if ((data & PORT_MII_100MBIT) &&
+			    phydev->speed < SPEED_100)
+				phydev->speed = SPEED_100;
+			if ((data & PORT_MII_FULL_DUPLEX) &&
+			    phydev->duplex < DUPLEX_FULL)
+				phydev->duplex = DUPLEX_FULL;
+			port_w(sw, sw->HOST_PORT, REG_PORT_XMII_CTRL_0, data);
+		}
+
+		port_r(sw, sw->HOST_PORT, REG_PORT_XMII_CTRL_1, &data);
+		data &= ~PORT_MII_MAC_MODE;
+		if (phydev->dev_flags & 2)
+			data |= PORT_MII_MAC_MODE;
+		switch (interface) {
+		case PHY_INTERFACE_MODE_MII:
+			sw_set_gbit(sw, false, &data);
+			mode = 0;
+			speed = 100;
+			break;
+		case PHY_INTERFACE_MODE_RMII:
+			sw_set_gbit(sw, false, &data);
+			mode = 1;
+			speed = 100;
+			break;
+		case PHY_INTERFACE_MODE_GMII:
+			sw_set_gbit(sw, true, &data);
+			mode = 2;
+			if (phydev->dev_flags & 1) {
+				if (SPEED_1000 != phydev->speed)
+					sw_set_gbit(sw, false, &data);
+			}
+			gbit = sw_get_gbit(sw, data);
+			if (gbit)
+				speed = 1000;
+			else
+				speed = 100;
+			break;
+		case PHY_INTERFACE_MODE_SGMII:
+			mode = 3;
+			speed = 1000;
+			break;
+		default:
+			data &= ~PORT_RGMII_ID_IG_ENABLE;
+			data &= ~PORT_RGMII_ID_EG_ENABLE;
+			if (PHY_INTERFACE_MODE_RGMII_ID == interface ||
+			    PHY_INTERFACE_MODE_RGMII_RXID == interface)
+				data |= PORT_RGMII_ID_IG_ENABLE;
+			if (PHY_INTERFACE_MODE_RGMII_ID == interface ||
+			    PHY_INTERFACE_MODE_RGMII_TXID == interface)
+				data |= PORT_RGMII_ID_EG_ENABLE;
+			sw_set_gbit(sw, true, &data);
+			mode = 3;
+			if (phydev->dev_flags & 1) {
+				if (SPEED_1000 != phydev->speed)
+					sw_set_gbit(sw, false, &data);
+			}
+			gbit = sw_get_gbit(sw, data);
+			if (gbit)
+				speed = 1000;
+			else
+				speed = 100;
+			break;
+		}
+		info->tx_rate = speed * TX_RATE_UNIT;
+		info->duplex = duplex + 1;
+		phydev->speed = speed;
+		phydev->duplex = duplex - 1;
+#ifdef USE_RGMII_PHY
+		data |= PORT_RGMII_ID_EG_ENABLE;
+		mode = 3;
+#endif
+		sw_set_xmii(sw, mode, &data);
+		port_w(sw, sw->HOST_PORT, REG_PORT_XMII_CTRL_1, data);
+		sw->cached.xmii[sw->HOST_PORT - sw->phy_port_cnt] &= ~0xff;
+		sw->cached.xmii[sw->HOST_PORT - sw->phy_port_cnt] |= data;
+dbg_msg(" cached: %02x"NL, sw->cached.xmii[sw->HOST_PORT - sw->phy_port_cnt]);
+	}
+
+	data = SW_R(sw, REG_SW_MAC_CTRL_0);
+
+	/* Enable aggressive back off algorithm in half duplex mode. */
+	data |= SW_AGGR_BACKOFF;
+
+/*
+ * THa  2016/10/17
+ * If no excessive collision drop is enabled the default backoff algorithm
+ * may cause both linked device to stop passing traffic completely.
+ */
+	data |= SW_NEW_BACKOFF;
+
+/*
+ * THa  2016/10/17
+ * Recommended to turn on this mode to pass UNH tests.
+ */
+	data |= SW_PAUSE_UNH_MODE;
+	SW_W(sw, REG_SW_MAC_CTRL_0, data);
+
+	data = SW_R(sw, REG_SW_MAC_CTRL_1);
+
+	/* Enable no excessive collision drop. */
+	data |= NO_EXC_COLLISION_DROP;
+	SW_W(sw, REG_SW_MAC_CTRL_1, data);
+
+	data = SW_R(sw, S_LINK_AGING_CTRL);
+
+	data |= SW_AGING_ENABLE;
+	if (sw->overrides & FAST_AGING)
+		data |= SW_FAST_AGING;
+	else
+		data &= ~SW_FAST_AGING;
+
+	/* Enable automatic fast aging when link changed detected. */
+	data |= SW_LINK_AUTO_AGING;
+
+#if 1
+/*
+ * THa  2014/10/08
+ * The host port also gets filtered if lookup is used!
+ */
+	if (sw->features & NEW_CAP)
+		data |= SW_SRC_ADDR_FILTER;
+#endif
+	SW_W(sw, S_LINK_AGING_CTRL, data);
+
+#ifdef IMX6_KSZ9567
+	/* SPI access becomes more stable. */
+	data = SW_R(sw, REG_SW_GLOBAL_OUTPUT_CTRL__1);
+	data |= SW_REFCLKO_IS_125MHZ;
+	SW_W(sw, REG_SW_GLOBAL_OUTPUT_CTRL__1, data);
+
+	data = SW_R(sw, REG_SW_IO_STRENGTH__1);
+	data &= ~(SW_DRIVE_STRENGTH_M << SW_HI_SPEED_DRIVE_STRENGTH_S);
+	data |= SW_DRIVE_STRENGTH_16MA << SW_HI_SPEED_DRIVE_STRENGTH_S;
+	SW_W(sw, REG_SW_IO_STRENGTH__1, data);
+#endif
+}  /* sw_set_global_ctrl */
+
+/* -------------------------------------------------------------------------- */
+
+/**
+ * port_set_stp_state - configure port spanning tree state
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ * @state:	The spanning tree state.
+ *
+ * This routine configures the spanning tree state of the port.
+ */
+static void port_set_stp_state(struct ksz_sw *sw, uint port, int state)
+{
+	u8 old_data;
+	SW_D data;
+	struct ksz_port_cfg *cfg = get_port_cfg(sw, port);
+	uint m = BIT(port);
+	int member = -1;
+
+#if 0
+dbg_msg("%s %d %d"NL, __func__, port, state);
+#endif
+	port_r(sw, port, P_STP_CTRL, &data);
+	old_data = data;
+	switch (state) {
+	case STP_STATE_DISABLED:
+		data &= ~(PORT_TX_ENABLE | PORT_RX_ENABLE);
+		data |= PORT_LEARN_DISABLE;
+		if (port != sw->HOST_PORT)
+			member = 0;
+		break;
+	case STP_STATE_LISTENING:
+/* No need to turn on transmit because of port direct mode.
+ * Turning on receive is required if static MAC table is not setup.
+ */
+		data &= ~PORT_TX_ENABLE;
+		data |= PORT_RX_ENABLE;
+		data |= PORT_LEARN_DISABLE;
+		if (port != sw->HOST_PORT &&
+		    STP_STATE_DISABLED == cfg->stp_state[cfg->mstp])
+			member = sw->HOST_MASK | cfg->vid_member;
+		break;
+	case STP_STATE_LEARNING:
+		data &= ~PORT_TX_ENABLE;
+		data |= PORT_RX_ENABLE;
+		data &= ~PORT_LEARN_DISABLE;
+		break;
+	case STP_STATE_FORWARDING:
+		data |= (PORT_TX_ENABLE | PORT_RX_ENABLE);
+		data &= ~PORT_LEARN_DISABLE;
+
+#ifdef CONFIG_KSZ_STP
+		/* Actual port membership setting is done in another RSTP
+		 * processing routine.
+		 */
+		if (sw->stp == 1 || (sw->stp && (sw->stp & (1 << port)))) {
+			struct ksz_stp_info *info = &sw->info->rstp;
+
+			if (info->br.bridgeEnabled)
+				break;
+		}
+#endif
+		if (((sw->features & (SW_VLAN_DEV | USE_FEWER_PORTS)) ||
+		    sw->dev_offset) && port != sw->HOST_PORT)
+			/* Set port-base vlan membership with host port. */
+			member = sw->HOST_MASK | cfg->vid_member;
+		break;
+	case STP_STATE_BLOCKED:
+/*
+ * Need to setup static MAC table with override to keep receiving BPDU
+ * messages.  See sw_setup_stp routine.
+ */
+		data &= ~(PORT_TX_ENABLE | PORT_RX_ENABLE);
+		data |= PORT_LEARN_DISABLE;
+		if (port != sw->HOST_PORT &&
+		    STP_STATE_DISABLED == cfg->stp_state[cfg->mstp])
+			member = sw->HOST_MASK | cfg->vid_member;
+		break;
+	case STP_STATE_SIMPLE:
+		data |= (PORT_TX_ENABLE | PORT_RX_ENABLE);
+		data |= PORT_LEARN_DISABLE;
+		if (port != sw->HOST_PORT)
+			/* Set port-base vlan membership with host port. */
+			member = sw->HOST_MASK | cfg->vid_member;
+		break;
+	}
+	port_w(sw, port, P_STP_CTRL, data);
+	cfg->stp_state[cfg->mstp] = state;
+	if (data & PORT_RX_ENABLE)
+		sw->rx_ports[cfg->mstp] |= m;
+	else
+		sw->rx_ports[cfg->mstp] &= ~m;
+	if (data & PORT_TX_ENABLE)
+		sw->tx_ports[cfg->mstp] |= m;
+	else
+		sw->tx_ports[cfg->mstp] &= ~m;
+	if ((sw->overrides & DELAY_UPDATE_LINK) &&
+	    !(old_data & PORT_TX_ENABLE) &&
+	    (data & PORT_TX_ENABLE)) {
+		struct ksz_port *netport;
+		int i;
+
+		for (i = sw->dev_offset; i < sw->dev_offset + sw->dev_count;
+		     i++) {
+			netport = sw->netport[i];
+			if (netport && netport->linked->phy_id == port + 1) {
+				schedule_delayed_work(&netport->link_update, 0);
+				break;
+			}
+		}
+	}
+
+	/* Port membership may share register with STP state. */
+	if (member >= 0)
+		sw_cfg_port_base_vlan(sw, port, (u8) member);
+}  /* port_set_stp_state */
+
+static void port_open(struct ksz_sw *sw, uint p, bool open)
+{
+	int acl_on;
+	uint n;
+	uint q;
+	u8 acl_rule;
+	u8 map_mode;
+	u8 member;
+	u16 ruleset;
+	struct ksz_acl_table *acl;
+
+dbg_msg("%s %d %d"NL, __func__, p, open);
+	acl_rule = 1;
+	if (open) {
+		sw->on_ports |= (1 << p);
+		map_mode = 0;
+		ruleset = 0;
+	} else {
+		sw->on_ports &= ~(1 << p);
+		map_mode = ACL_MAP_MODE_REPLACE;
+		ruleset = (1 << acl_rule);
+	}
+	acl_on = port_chk_acl(sw, p);
+	if (!acl_on)
+		port_cfg_acl(sw, p, true);
+	sw->ops->release(sw);
+	acl = &sw->info->port_cfg[p].acl_info[acl_rule];
+	acl->map_mode = map_mode;
+	acl->ruleset = ruleset;
+	sw_w_acl_table(sw, p, acl_rule, acl);
+	++acl_rule;
+	acl = &sw->info->port_cfg[p].acl_info[acl_rule];
+	if (ruleset)
+		ruleset = (1 << acl_rule);
+	acl->ruleset = ruleset;
+	sw_w_acl_table(sw, p, acl_rule, acl);
+	sw->ops->acquire(sw);
+	if (!acl_on)
+		port_cfg_acl(sw, p, false);
+	for (n = 1; n <= sw->mib_port_cnt; n++) {
+		q = get_phy_port(sw, n);
+		if (sw->on_ports & (1 << q))
+			member = (u8)(sw->HOST_MASK | sw->on_ports);
+		else
+			member = sw->HOST_MASK;
+		if (sw->info->port_cfg[q].member != member)
+			sw_cfg_port_base_vlan(sw, q, member);
+	}
+}  /* port_open */
+
+static void sw_setup_acl(struct ksz_sw *sw)
+{
+	struct ksz_acl_table *acl;
+	int acl_on;
+	uint n;
+	uint port;
+	u8 first_rule;
+	u8 acl_rule;
+
+	if (!(sw->overrides & USE_802_1X_AUTH))
+		return;
+	for (n = 1; n <= sw->mib_port_cnt; n++) {
+		port = get_phy_port(sw, n);
+		port_set_authen_mode(sw, port, PORT_AUTHEN_BLOCK);
+		sw_cfg_port_base_vlan(sw, port, sw->HOST_MASK);
+
+		acl_on = port_chk_acl(sw, port);
+		if (!acl_on)
+			port_cfg_acl(sw, port, true);
+
+		first_rule = 0;
+		acl_rule = 0;
+		acl = &sw->info->port_cfg[port].acl_info[acl_rule];
+		acl->mac[0] = 0x01;
+		acl->mac[1] = 0x80;
+		acl->mac[2] = 0xC2;
+		acl->mac[3] = 0x00;
+		acl->mac[4] = 0x00;
+		acl->mac[5] = 0x03;
+		acl->eth_type = ETH_P_PAE;
+		acl->src = 0;
+		acl->equal = 1;
+		acl->enable = ACL_ENABLE_2_BOTH;
+		acl->mode = ACL_MODE_LAYER_2;
+		acl->ruleset = (1 << acl_rule);
+		acl->first_rule = first_rule;
+		acl->map_mode = 0;
+		acl->ports = 0;
+		sw->ops->release(sw);
+		sw_w_acl_table(sw, port, acl_rule, acl);
+		sw->ops->acquire(sw);
+
+		first_rule = 1;
+		acl_rule++;
+		acl++;
+		acl->src = 0;
+		acl->min_port = 1645;
+		acl->max_port = 1812;
+		acl->port_mode = ACL_PORT_MODE_EITHER;
+		acl->enable = ACL_ENABLE_4_UDP_PORT_COMP;
+		acl->mode = ACL_MODE_LAYER_4;
+		acl->ruleset = (1 << acl_rule);
+		acl->first_rule = first_rule;
+		acl->map_mode = ACL_MAP_MODE_REPLACE;
+		acl->ports = sw->HOST_MASK;
+		sw->ops->release(sw);
+		sw_w_acl_table(sw, port, acl_rule, acl);
+		sw->ops->acquire(sw);
+		acl_rule++;
+		acl++;
+		acl->eth_type = ETH_P_ARP;
+		acl->equal = 1;
+		acl->enable = ACL_ENABLE_2_TYPE;
+		acl->mode = ACL_MODE_LAYER_2;
+		acl->ruleset = (1 << acl_rule);
+		acl->first_rule = first_rule;
+		sw->ops->release(sw);
+		sw_w_acl_table(sw, port, acl_rule, acl);
+		sw->ops->acquire(sw);
+		if (!acl_on)
+			port_cfg_acl(sw, port, false);
+	}
+}  /* sw_setup_acl */
+
+/**
+ * sw_clr_sta_mac_table - clear static MAC table
+ * @sw:		The switch instance.
+ *
+ * This routine clears the static MAC table.
+ */
+static void sw_clr_sta_mac_table(struct ksz_sw *sw)
+{
+	SW_D data;
+
+	data = SW_R(sw, REG_SW_LUE_CTRL_2);
+	data &= ~(SW_FLUSH_OPTION_M << SW_FLUSH_OPTION_S);
+	data |= (SW_FLUSH_OPTION_STA_MAC << SW_FLUSH_OPTION_S);
+	SW_W(sw, REG_SW_LUE_CTRL_2, data);
+	sw_cfg(sw, S_FLUSH_TABLE_CTRL, SW_FLUSH_STP_TABLE, 1);
+}  /* sw_clr_sta_mac_table */
+
+#ifdef CONFIG_KSZ_STP
+/**
+ * sw_setup_stp - setup switch spanning tree support
+ * @sw:		The switch instance.
+ *
+ * This routine initializes the spanning tree support of the switch.
+ */
+static void sw_setup_stp(struct ksz_sw *sw)
+{
+	struct ksz_mac_table *entry;
+	struct ksz_alu_table *alu;
+	struct ksz_sw_info *info = sw->info;
+
+	entry = &info->mac_table[STP_ENTRY];
+	entry->addr[0] = 0x01;
+	entry->addr[1] = 0x80;
+	entry->addr[2] = 0xC2;
+	entry->addr[3] = 0x00;
+	entry->addr[4] = 0x00;
+	entry->addr[5] = 0x00;
+	if (sw->stp) {
+		entry->ports = sw->HOST_MASK;
+		entry->override = 1;
+	} else {
+		entry->ports = sw->PORT_MASK & ~sw->HOST_MASK;
+	}
+	entry->valid = 1;
+	alu = &info->alu_table[STP_ENTRY];
+	alu->index = 0;
+	alu->type = 1;
+	alu->owner = 0x8000;
+	alu->valid = 1;
+	if (sw->stp)
+		alu->forward = FWD_STP_DEV | FWD_HOST | FWD_HOST_OVERRIDE;
+	sw_w_sta_mac_table(sw, alu->index, alu->type, entry);
+}  /* sw_setup_stp */
+#endif
+
+static void sw_set_mcast_table(struct ksz_sw *sw, int i, const u8 *addr)
+{
+	struct ksz_mac_table *entry;
+	struct ksz_alu_table *alu;
+
+	entry = &sw->info->mac_table[i];
+	alu = &sw->info->alu_table[i];
+	memcpy(entry->addr, addr, ETH_ALEN);
+	entry->fid = 0;
+	entry->ports = sw->PORT_MASK;
+	entry->valid = 1;
+	alu = &sw->info->alu_table[i];
+	alu->forward = FWD_MAIN_DEV | FWD_MCAST | FWD_KNOWN;
+	alu->owner = 0x8000;
+	alu->valid = 1;
+	alu->index = 0;
+	alu->type = 2;
+	sw_w_dyn_mac_table(sw, 0, entry->addr, entry->fid, entry);
+}  /* sw_set_mcast_table */
+
+#ifdef CONFIG_1588_PTP
+static void sw_setup_ptp(struct ksz_sw *sw)
+{
+	struct ksz_mac_table *entry;
+	struct ksz_alu_table *alu;
+	int i;
+	u8 forward;
+	struct ksz_sw_info *info = sw->info;
+
+	i = info->multi_sys;
+	forward = FWD_MAIN_DEV;
+	forward |= FWD_VLAN_DEV;
+
+#if 0
+	entry = &info->mac_table[--i];
+	entry->addr[0] = 0x01;
+	entry->addr[1] = 0x00;
+	entry->addr[2] = 0x5E;
+	entry->addr[3] = 0x00;
+	entry->addr[4] = 0x01;
+	entry->addr[5] = 0x81;
+	entry->ports = sw->PORT_MASK;
+	alu = &info->alu_table[i];
+	alu->forward = forward;
+	alu->owner = 0x8000;
+	alu->valid = 1;
+	alu->index = 0;
+	alu->type = 3;
+	entry = &info->mac_table[--i];
+	entry->addr[0] = 0x33;
+	entry->addr[1] = 0x33;
+	entry->addr[2] = 0x00;
+	entry->addr[3] = 0x00;
+	entry->addr[4] = 0x01;
+	entry->addr[5] = 0x81;
+	entry->ports = sw->PORT_MASK;
+	alu = &info->alu_table[i];
+	alu->forward = forward;
+	alu->owner = 0x8000;
+	alu->valid = 1;
+	alu->index = 0;
+	alu->type = 3;
+	entry = &info->mac_table[--i];
+	entry->addr[0] = 0x01;
+	entry->addr[1] = 0x1B;
+	entry->addr[2] = 0x19;
+	entry->addr[3] = 0x00;
+	entry->addr[4] = 0x00;
+	entry->addr[5] = 0x00;
+	entry->ports = sw->PORT_MASK;
+	alu = &info->alu_table[i];
+	alu->forward = forward;
+	alu->owner = 0x8000;
+	alu->valid = 1;
+	alu->index = 0;
+	alu->type = 3;
+#endif
+	entry = &info->mac_table[--i];
+	entry->addr[0] = 0x01;
+	entry->addr[1] = 0x00;
+	entry->addr[2] = 0x5E;
+	entry->addr[3] = 0x00;
+	entry->addr[4] = 0x00;
+	entry->addr[5] = 0x6B;
+	entry->ports = sw->PORT_MASK;
+	entry->override = 1;
+	alu = &info->alu_table[i];
+	alu->forward = forward | FWD_HOST_OVERRIDE;
+	alu->owner = 0x8000;
+	alu->valid = 1;
+	alu->index = 0;
+	alu->type = 3;
+	entry = &info->mac_table[--i];
+	entry->addr[0] = 0x33;
+	entry->addr[1] = 0x33;
+	entry->addr[2] = 0x00;
+	entry->addr[3] = 0x00;
+	entry->addr[4] = 0x00;
+	entry->addr[5] = 0x6B;
+	entry->ports = sw->PORT_MASK;
+	entry->override = 1;
+	alu = &info->alu_table[i];
+	alu->forward = forward | FWD_HOST_OVERRIDE;
+	alu->owner = 0x8000;
+	alu->valid = 1;
+	alu->index = 0;
+	alu->type = 3;
+	entry = &info->mac_table[--i];
+	entry->addr[0] = 0x01;
+	entry->addr[1] = 0x80;
+	entry->addr[2] = 0xC2;
+	entry->addr[3] = 0x00;
+	entry->addr[4] = 0x00;
+	entry->addr[5] = 0x0E;
+	entry->ports = sw->HOST_MASK;
+	entry->override = 1;
+	alu = &info->alu_table[i];
+	alu->forward = forward | FWD_HOST_OVERRIDE;
+	alu->owner = 0x8000;
+	alu->valid = 1;
+	alu->index = 0xE;
+	alu->type = 1;
+
+	info->multi_sys = i;
+}
+#endif
+
+static const u8 ipv6_neigh_mcast[] = {
+	0x33, 0x33, 0xFF,
+};
+
+static const u8 known_mcast_addr[][ETH_ALEN] = {
+	/* IGMP V2 */
+	{ 0x01, 0x00, 0x5E, 0x00, 0x00, 0x01 },
+	{ 0x33, 0x33, 0x00, 0x00, 0x00, 0x01 },
+	{ 0x01, 0x00, 0x5E, 0x00, 0x00, 0x02 },
+
+	/* ICMPv6 */
+	{ 0x33, 0x33, 0x00, 0x00, 0x00, 0x02 },
+
+	/* IGMP V3 */
+	{ 0x01, 0x00, 0x5E, 0x00, 0x00, 0x16 },
+	{ 0x33, 0x33, 0x00, 0x00, 0x00, 0x16 },
+
+	{ 0x01, 0x00, 0x5E, 0x00, 0x00, 0xFB },
+	{ 0x33, 0x33, 0x00, 0x00, 0x00, 0xFB },
+
+	/* Link-Local Multicast Name Resolution */
+	{ 0x01, 0x00, 0x5E, 0x00, 0x00, 0xFC },
+	{ 0x33, 0x33, 0x00, 0x01, 0x00, 0x03 },
+
+	/* Simple Service Discovery Protocol */
+	{ 0x01, 0x00, 0x5E, 0x7F, 0xFF, 0xFA },
+	{ 0x33, 0x33, 0x00, 0x00, 0x00, 0x0C },
+
+	{ 0x33, 0x33, 0x00, 0x01, 0x00, 0x02 },
+};
+
+static void sw_setup_multi(struct ksz_sw *sw)
+{
+	struct ksz_mac_table *entry;
+	struct ksz_alu_table *alu;
+	int i;
+	int n;
+	u8 forward;
+	u8 addr[ETH_ALEN];
+	struct ksz_sw_info *info = sw->info;
+
+	sw->ops->release(sw);
+	i = info->multi_sys;
+	forward = FWD_HOST;
+
+	addr[0] = 0x01;
+	addr[1] = 0x80;
+	addr[2] = 0xC2;
+	addr[3] = 0x00;
+	addr[4] = 0x00;
+	addr[5] = 0x00;
+
+	entry = &info->mac_table[--i];
+	memset(entry->addr, 0xFF, ETH_ALEN);
+	entry->ports = sw->PORT_MASK;
+	entry->fid = 0;
+	alu = &info->alu_table[i];
+	alu->forward = FWD_MAIN_DEV | FWD_MCAST | FWD_KNOWN;
+	alu->owner = sw->PORT_MASK;
+	alu->valid = 1;
+	alu->index = 0;
+	alu->type = 2;
+#if 0
+	if (sw->features & DLR_HW) {
+		entry->ports = sw->HOST_MASK;
+		alu->forward |= FWD_HOST;
+	}
+#endif
+
+	i = STP_ENTRY;
+
+#if 1
+	entry = &info->mac_table[--i];
+	memcpy(entry->addr, addr, ETH_ALEN);
+	entry->addr[5] = 0x01;
+	entry->ports = sw->HOST_MASK;
+	alu = &info->alu_table[i];
+	alu->forward = forward;
+	alu->owner = 0x8000;
+	alu->valid = 1;
+	alu->index = 0;
+	alu->type = 1;
+#endif
+
+	entry = &info->mac_table[--i];
+	memcpy(entry->addr, addr, ETH_ALEN);
+	entry->addr[5] = 0x02;
+	entry->ports = sw->HOST_MASK;
+	alu = &info->alu_table[i];
+	alu->forward = forward;
+	alu->owner = 0x8000;
+	alu->valid = 1;
+	alu->index = 0;
+	alu->type = 1;
+
+	entry = &info->mac_table[--i];
+	memcpy(entry->addr, addr, ETH_ALEN);
+	entry->addr[5] = 0x03;
+	entry->ports = sw->HOST_MASK;
+	alu = &info->alu_table[i];
+	alu->forward = forward;
+	alu->owner = 0x8000;
+	alu->valid = 1;
+	alu->index = 0;
+	alu->type = 1;
+
+	entry = &info->mac_table[--i];
+	memcpy(entry->addr, addr, ETH_ALEN);
+	entry->addr[5] = 0x04;
+	entry->ports = sw->HOST_MASK;
+	alu = &info->alu_table[i];
+	alu->forward = forward;
+	alu->owner = 0x8000;
+	alu->valid = 1;
+	alu->index = 0;
+	alu->type = 1;
+
+	entry = &info->mac_table[--i];
+	memcpy(entry->addr, addr, ETH_ALEN);
+	entry->addr[5] = 0x05;
+	entry->ports = sw->HOST_MASK;
+	alu = &info->alu_table[i];
+	alu->forward = forward;
+	alu->owner = 0x8000;
+	alu->valid = 1;
+	alu->index = 0;
+	alu->type = 1;
+
+	entry = &info->mac_table[--i];
+	memcpy(entry->addr, addr, ETH_ALEN);
+	entry->addr[5] = 0x06;
+	entry->ports = sw->HOST_MASK;
+	alu = &info->alu_table[i];
+	alu->forward = forward;
+	alu->owner = 0x8000;
+	alu->valid = 1;
+	alu->index = 0;
+	alu->type = 1;
+
+	entry = &info->mac_table[--i];
+	memcpy(entry->addr, addr, ETH_ALEN);
+	entry->addr[5] = 0x07;
+	entry->ports = sw->HOST_MASK;
+	alu = &info->alu_table[i];
+	alu->forward = forward;
+	alu->owner = 0x8000;
+	alu->valid = 1;
+	alu->index = 0;
+	alu->type = 1;
+
+	entry = &info->mac_table[--i];
+	memcpy(entry->addr, addr, ETH_ALEN);
+	entry->addr[5] = 0x08;
+	entry->ports = sw->HOST_MASK;
+	alu = &info->alu_table[i];
+	alu->forward = forward;
+	alu->owner = 0x8000;
+	alu->valid = 1;
+	alu->index = 0;
+	alu->type = 1;
+
+	entry = &info->mac_table[--i];
+	memcpy(entry->addr, addr, ETH_ALEN);
+	entry->addr[5] = 0x09;
+	entry->ports = sw->HOST_MASK;
+	alu = &info->alu_table[i];
+	alu->forward = forward;
+	alu->owner = 0x8000;
+	alu->valid = 1;
+	alu->index = 0;
+	alu->type = 1;
+
+	entry = &info->mac_table[--i];
+	memcpy(entry->addr, addr, ETH_ALEN);
+	entry->addr[5] = 0x0A;
+	entry->ports = sw->HOST_MASK;
+	alu = &info->alu_table[i];
+	alu->forward = forward;
+	alu->owner = 0x8000;
+	alu->valid = 1;
+	alu->index = 0;
+	alu->type = 1;
+
+	entry = &info->mac_table[--i];
+	memcpy(entry->addr, addr, ETH_ALEN);
+	entry->addr[5] = 0x0B;
+	entry->ports = sw->HOST_MASK;
+	alu = &info->alu_table[i];
+	alu->forward = forward;
+	alu->owner = 0x8000;
+	alu->valid = 1;
+	alu->index = 0;
+	alu->type = 1;
+
+	entry = &info->mac_table[--i];
+	memcpy(entry->addr, addr, ETH_ALEN);
+	entry->addr[5] = 0x0C;
+	entry->ports = sw->HOST_MASK;
+	alu = &info->alu_table[i];
+	alu->forward = forward;
+	alu->owner = 0x8000;
+	alu->valid = 1;
+	alu->index = 0;
+	alu->type = 1;
+
+	entry = &info->mac_table[--i];
+	memcpy(entry->addr, addr, ETH_ALEN);
+	entry->addr[5] = 0x0D;
+	entry->ports = sw->HOST_MASK;
+	alu = &info->alu_table[i];
+	alu->forward = forward;
+	alu->owner = 0x8000;
+	alu->valid = 1;
+	alu->index = 0;
+	alu->type = 1;
+
+	entry = &info->mac_table[--i];
+	memcpy(entry->addr, addr, ETH_ALEN);
+	entry->addr[5] = 0x0E;
+	entry->ports = sw->HOST_MASK;
+#ifdef CONFIG_1588_PTP
+	entry->override = 1;
+	alu = &info->alu_table[i];
+	alu->forward = forward | FWD_HOST_OVERRIDE;
+#else
+	alu = &info->alu_table[i];
+	alu->forward = forward;
+#endif
+	alu->owner = 0x8000;
+	alu->valid = 1;
+	alu->index = 0;
+	alu->type = 1;
+
+	entry = &info->mac_table[--i];
+	memcpy(entry->addr, addr, ETH_ALEN);
+	entry->addr[5] = 0x0F;
+	entry->ports = sw->HOST_MASK;
+	alu = &info->alu_table[i];
+	alu->forward = forward;
+	alu->owner = 0x8000;
+	alu->valid = 1;
+	alu->index = 0;
+	alu->type = 1;
+
+	entry = &info->mac_table[--i];
+	memcpy(entry->addr, addr, ETH_ALEN);
+	entry->addr[5] = 0x20;
+	entry->ports = sw->HOST_MASK;
+	alu = &info->alu_table[i];
+	alu->forward = forward;
+	alu->owner = 0x8000;
+	alu->valid = 1;
+	alu->index = 0;
+	alu->type = 1;
+
+	entry = &info->mac_table[--i];
+	memcpy(entry->addr, addr, ETH_ALEN);
+	entry->addr[5] = 0x21;
+	entry->ports = sw->HOST_MASK;
+	alu = &info->alu_table[i];
+	alu->forward = forward;
+	alu->owner = 0x8000;
+	alu->valid = 1;
+	alu->index = 0;
+	alu->type = 1;
+
+	forward = FWD_MAIN_DEV;
+	forward |= FWD_MCAST | FWD_KNOWN;
+
+	entry = &info->mac_table[--i];
+	memcpy(entry->addr, addr, ETH_ALEN);
+	entry->addr[5] = 0x10;
+	entry->ports = sw->PORT_MASK;
+	alu = &info->alu_table[i];
+	alu->forward = forward;
+	alu->owner = 0x8000;
+	alu->valid = 1;
+	alu->index = 0;
+	alu->type = 1;
+
+	for (n = 0; n < 13; n++) {
+		--i;
+		sw_set_mcast_table(sw, i, known_mcast_addr[n]);
+	}
+
+	sw->ops->acquire(sw);
+	info->multi_sys = i;
+}  /* sw_setup_multi */
+
+#ifdef CONFIG_KSZ_STP
+static void bridge_change(struct ksz_sw *sw)
+{
+	int i;
+	uint n;
+	uint port;
+	u8 m;
+	u8 member;
+	struct ksz_port_cfg *cfg;
+
+	for (n = 1; n <= sw->mib_port_cnt; n++) {
+		port = get_phy_port(sw, n);
+		cfg = get_port_cfg(sw, port);
+		member = 0;
+		for (i = 0; i < NUM_OF_MSTI; i++) {
+			if (STP_STATE_FORWARDING == cfg->stp_state[i])
+				m = sw->HOST_MASK | sw->info->member[i];
+			else if (STP_STATE_DISABLED == cfg->stp_state[i])
+				m = 0;
+			else
+				m = sw->HOST_MASK | (1 << port);
+			member |= m;
+		}
+		if (member != cfg->member)
+			sw_cfg_port_base_vlan(sw, port, member);
+	}
+}  /* bridge_change */
+#endif
+
+static int sw_match_multi(struct ksz_sw *sw, struct ksz_port *priv, u8 *addr)
+{
+	struct ksz_mac_table *entry;
+	struct ksz_alu_table *alu;
+	int i;
+	int owner;
+	uint port = 0;
+	struct ksz_sw_info *info = sw->info;
+
+	if (priv->port_cnt != sw->mib_port_cnt)
+		port = priv->first_port;
+#if 0
+dbg_msg("%s %d %d"NL, __func__, priv->port_cnt, port);
+#endif
+	owner = 1 << port;
+
+	for (i = SWITCH_MAC_TABLE_ENTRIES; i < info->multi_net; i++) {
+		entry = &info->mac_table[i];
+		alu = &info->alu_table[i];
+		if (alu->valid && (alu->owner & owner) &&
+		    !memcmp(addr, entry->addr, ETH_ALEN))
+			return false;
+	}
+	for (i = info->multi_sys; i < MULTI_MAC_TABLE_ENTRIES; i++) {
+		entry = &info->mac_table[i];
+		alu = &info->alu_table[i];
+		if (alu->valid && !memcmp(addr, entry->addr, ETH_ALEN))
+			return false;
+	}
+	return true;
+}  /* sw_match_multi */
+
+static void sw_set_multi(struct ksz_sw *sw, struct net_device *dev,
+	struct ksz_port *priv)
+{
+	struct ksz_mac_table *entry;
+	struct ksz_alu_table *alu;
+	struct netdev_hw_addr *ha;
+	int i;
+	int found;
+	int owner;
+	uint port = 0;
+	struct ksz_sw_info *info = sw->info;
+
+	if (priv->port_cnt != sw->mib_port_cnt)
+		port = priv->first_port;
+#if 0
+dbg_msg("%s %d %d"NL, __func__, priv->port_cnt, port);
+#endif
+	owner = 1 << port;
+
+	/* Remove old multicast entries. */
+	for (i = SWITCH_MAC_TABLE_ENTRIES; i < info->multi_net; i++) {
+		entry = &info->mac_table[i];
+		alu = &info->alu_table[i];
+		if (alu->valid && (alu->owner & owner)) {
+			/* Remove device ownership. */
+			alu->owner &= ~owner;
+
+			/* Mark as present as it is likely set again. */
+			alu->owner |= 0x4000;
+			if (!port)
+				alu->forward &= ~FWD_MAIN_DEV;
+			else if (alu->owner <= 1)
+				alu->forward &= ~FWD_STP_DEV;
+		}
+	}
+	netdev_for_each_mc_addr(ha, dev) {
+		if (!(*ha->addr & 1))
+			continue;
+		if (info->multi_net == info->multi_sys)
+			break;
+		found = 0;
+		for (i = 0; i < MULTI_MAC_TABLE_ENTRIES; i++) {
+			entry = &info->mac_table[i];
+			alu = &info->alu_table[i];
+			if (alu->valid &&
+			    !memcmp(entry->addr, ha->addr, ETH_ALEN)) {
+				found = i + 1;
+				break;
+			}
+			if (!alu->valid && !found &&
+			    i >= SWITCH_MAC_TABLE_ENTRIES &&
+			    i < info->multi_net)
+				found = i + 1;
+		}
+		if (!found) {
+			info->multi_net++;
+			found = info->multi_net;
+		}
+		found--;
+		if (found >= SWITCH_MAC_TABLE_ENTRIES &&
+		    found < info->multi_net) {
+			entry = &info->mac_table[found];
+			alu = &info->alu_table[found];
+			if (port)
+				alu->forward |= FWD_STP_DEV;
+			else
+				alu->forward |= FWD_MAIN_DEV;
+			alu->owner |= owner;
+			alu->valid = 1;
+			memcpy(entry->addr, ha->addr, ETH_ALEN);
+			entry->ports = sw->PORT_MASK;
+			entry->valid = 1;
+		}
+	}
+	for (i = SWITCH_MAC_TABLE_ENTRIES; i < info->multi_net; i++) {
+		entry = &info->mac_table[i];
+		alu = &info->alu_table[i];
+
+		/* Look for previously set entries. */
+		if ((alu->owner & 0x4000)) {
+			alu->owner &= ~0x4000;
+
+			/* Nobody owns this entry. */
+			if (!alu->owner) {
+				alu->valid = 0;
+				entry->ports = 0;
+				entry->valid = 0;
+			}
+		} else if (alu->valid && (alu->owner & owner)) {
+			/* Newly added entry. */
+		}
+	}
+}  /* sw_set_multi */
+
+static void sw_reset_multi(struct ksz_sw *sw, struct ksz_port *priv)
+{
+	struct ksz_mac_table *entry;
+	struct ksz_alu_table *alu;
+	int i;
+	int owner;
+	uint port = 0;
+	struct ksz_sw_info *info = sw->info;
+
+	if (priv->port_cnt != sw->mib_port_cnt)
+		port = priv->first_port;
+	owner = 1 << port;
+
+	for (i = SWITCH_MAC_TABLE_ENTRIES; i < info->multi_net; i++) {
+		entry = &info->mac_table[i];
+		alu = &info->alu_table[i];
+		if (alu->valid && (alu->owner & owner)) {
+			alu->owner &= ~owner;
+			if (!alu->owner) {
+				alu->valid = 0;
+				entry->valid = 0;
+			}
+		}
+	}
+}  /* sw_reset_multi */
+
+static void sw_reset_setup(struct ksz_sw *sw)
+{
+	struct ksz_mac_table *entry;
+	struct ksz_alu_table *alu;
+	int i;
+	struct ksz_sw_info *info = sw->info;
+
+	for (i = 0; i < MULTI_MAC_TABLE_ENTRIES; i++) {
+		entry = &info->mac_table[i];
+		alu = &info->alu_table[i];
+		if (alu->valid) {
+			alu->owner = 0;
+			alu->valid = 0;
+			entry->valid = 0;
+		}
+	}
+	for (i = 0; i < sw->port_cnt; i++)
+		sw->info->port_cfg[i].mstp = 0;
+}  /* sw_reset_setup */
+
+#define MAX_SW_LEN			1500
+
+#ifdef CONFIG_KSZ_MSTP
+#include "ksz_mstp.c"
+#elif defined(CONFIG_KSZ_STP)
+#define HAVE_VID2FID
+#include "ksz_stp.c"
+#endif
+#ifdef CONFIG_KSZ_DLR
+#include "ksz_dlr.c"
+#endif
+#ifdef CONFIG_KSZ_HSR
+#include "ksz_hsr.c"
+#endif
+
+/*
+ * Link detection routines
+ */
+
+static inline void dbp_link(struct ksz_port *port, struct ksz_sw *sw,
+	int change)
+{
+	struct ksz_port_info *info;
+	uint i;
+	uint n;
+	uint p;
+
+	for (i = 0, n = port->first_port; i < port->port_cnt; i++, n++) {
+		p = get_phy_port(sw, n);
+		info = get_port_info(sw, p);
+
+		/* Port does not have PHY. */
+		if (!info->phy)
+			continue;
+
+		if (media_connected == info->state) {
+			if (change & (1 << i)) {
+				printk(KERN_INFO "link %d-%d: %d, %d"NL,
+					sw->id, i + port->first_port,
+					info->tx_rate / TX_RATE_UNIT,
+					info->duplex);
+			}
+		} else {
+			if (change & (1 << i))
+				printk(KERN_INFO "link %d-%d disconnected"NL,
+					sw->id, i + port->first_port);
+		}
+	}
+}
+
+static u16 port_advertised_flow_ctrl(int flow_ctrl, u16 ctrl)
+{
+	ctrl &= ~PORT_AUTO_NEG_PAUSE;
+	switch (flow_ctrl) {
+	case PHY_FLOW_CTRL:
+		ctrl |= PORT_AUTO_NEG_SYM_PAUSE;
+		break;
+	case PHY_TX_ONLY:
+		ctrl |= PORT_AUTO_NEG_ASYM_PAUSE;
+		break;
+	case PHY_RX_ONLY:
+		ctrl |= PORT_AUTO_NEG_PAUSE;
+		break;
+	default:
+		break;
+	}
+	return ctrl;
+}  /* port_advertised_flow_ctrl */
+
+static u8 sw_determine_flow_ctrl(struct ksz_sw *sw, bool force_link,
+	u16 local, u16 remote)
+{
+	int rx;
+	int tx;
+	u8 flow = 0;
+
+	if (sw->overrides & PAUSE_FLOW_CTRL)
+		return flow;
+
+	rx = tx = 0;
+	if (force_link)
+		rx = tx = 1;
+	if (remote & PORT_REMOTE_SYM_PAUSE) {
+		if (local & PORT_AUTO_NEG_SYM_PAUSE)
+			rx = tx = 1;
+		else if ((remote & PORT_AUTO_NEG_ASYM_PAUSE) &&
+			 (local & PORT_AUTO_NEG_PAUSE) ==
+			 PORT_AUTO_NEG_ASYM_PAUSE)
+			tx = 1;
+	} else if (remote & PORT_AUTO_NEG_ASYM_PAUSE) {
+		if ((local & PORT_AUTO_NEG_PAUSE) == PORT_AUTO_NEG_PAUSE)
+			rx = 1;
+	}
+	if (rx)
+		flow |= 0x01;
+	if (tx)
+		flow |= 0x02;
+#ifdef DBG_LINK
+	printk(KERN_INFO "pause: %d, %d; %04x %04x"NL,
+		rx, tx, local, remote);
+#endif
+	return flow;
+}  /* sw_determine_flow_ctrl */
+
+static void sw_notify_link_change(struct ksz_sw *sw, uint ports)
+{
+	static u8 link_buf[sizeof(struct ksz_info_opt) +
+		sizeof(struct ksz_info_speed) * TOTAL_PORT_NUM +
+		sizeof(struct ksz_resp_msg)];
+
+	if ((sw->notifications & SW_INFO_LINK_CHANGE)) {
+		struct ksz_resp_msg *msg = (struct ksz_resp_msg *) link_buf;
+		struct ksz_info_opt *opt = (struct ksz_info_opt *)
+			&msg->resp.data;
+		struct ksz_port_info *info;
+		struct ksz_info_speed *speed;
+		struct file_dev_info *dev_info;
+		int c;
+		int n;
+		int p;
+		int q;
+
+		/* Check whether only 1 port has change. */
+		c = 0;
+		q = 0;
+		for (n = 1; n <= sw->mib_port_cnt; n++) {
+			p = get_phy_port(sw, n);
+			if (ports & (1 << p)) {
+				q = n;
+				c++;
+			}
+		}
+		if (c > 1) {
+			c = sw->mib_port_cnt;
+			q = 1;
+		}
+		msg->module = DEV_MOD_BASE;
+		msg->cmd = DEV_INFO_SW_LINK;
+		opt->num = (u8) c;
+		opt->port = (u8) q;
+		speed = &opt->data.speed;
+		for (n = 1; n <= opt->num; n++, q++) {
+			p = get_phy_port(sw, q);
+			info = get_port_info(sw, p);
+			if (info->state == media_connected) {
+				speed->tx_rate = info->tx_rate;
+				speed->duplex = info->duplex;
+				speed->flow_ctrl = info->flow_ctrl;
+			} else {
+				speed->tx_rate = 0;
+				speed->duplex = 0;
+				speed->flow_ctrl = 0;
+			}
+			++speed;
+		}
+		n = opt->num * sizeof(struct ksz_info_speed);
+		n += 2;
+		n += sizeof(struct ksz_resp_msg);
+		n -= 4;
+		dev_info = sw->dev_list[0];
+		while (dev_info) {
+			if ((dev_info->notifications[DEV_MOD_BASE] &
+			    SW_INFO_LINK_CHANGE))
+				file_dev_setup_msg(dev_info, msg, n, NULL,
+						   NULL);
+			dev_info = dev_info->next;
+		}
+	}
+}  /* sw_notify_link_change */
+
+static void port_sgmii_r(struct ksz_sw *sw, uint port, u16 devid, u16 reg,
+	u16 *buf, u16 len)
+{
+	u32 data;
+
+	data = devid & PORT_SGMII_DEVICE_ID_M;
+	data <<= PORT_SGMII_DEVICE_ID_S;
+	data |= reg;
+	if (len > 1)
+		data |= PORT_SGMII_AUTO_INCR;
+	port_w32(sw, port, REG_PORT_SGMII_ADDR__4, data);
+	while (len) {
+		port_r32(sw, port, REG_PORT_SGMII_DATA__4, &data);
+		*buf++ = (u16) data;
+		len--;
+	}
+}  /* port_sgmii_r */
+
+static void port_sgmii_w(struct ksz_sw *sw, uint port, u16 devid, u16 reg,
+	u16 *buf, u16 len)
+{
+	u32 data;
+
+	data = devid & PORT_SGMII_DEVICE_ID_M;
+	data <<= PORT_SGMII_DEVICE_ID_S;
+	data |= reg;
+	if (len > 1)
+		data |= PORT_SGMII_AUTO_INCR;
+	port_w32(sw, port, REG_PORT_SGMII_ADDR__4, data);
+	while (len) {
+		data = *buf++;
+		port_w32(sw, port, REG_PORT_SGMII_DATA__4, data);
+		len--;
+	}
+}  /* port_sgmii_w */
+
+static u16 port_sgmii_phy_r(struct ksz_sw *sw, uint port, u16 reg)
+{
+	u16 buf;
+
+	do {
+		port_sgmii_r(sw, port, SR_MII, MMD_SR_MII_PHY_CTRL, &buf, 1);
+	} while (buf & SR_MII_PHY_START_BUSY);
+	buf = reg;
+	port_sgmii_w(sw, port, SR_MII, MMD_SR_MII_PHY_ADDR, &buf, 1);
+	buf = 0;
+	port_sgmii_w(sw, port, SR_MII, MMD_SR_MII_PHY_CTRL, &buf, 1);
+	buf |= SR_MII_PHY_START_BUSY;
+	port_sgmii_w(sw, port, SR_MII, MMD_SR_MII_PHY_CTRL, &buf, 1);
+	do {
+		port_sgmii_r(sw, port, SR_MII, MMD_SR_MII_PHY_CTRL, &buf, 1);
+	} while (buf & SR_MII_PHY_START_BUSY);
+	port_sgmii_r(sw, port, SR_MII, MMD_SR_MII_PHY_DATA, &buf, 1);
+	return buf;
+}  /* port_sgmii_phy_r */
+
+static void port_sgmii_phy_w(struct ksz_sw *sw, uint port, u16 reg, u16 val)
+{
+	u16 buf;
+
+	do {
+		port_sgmii_r(sw, port, SR_MII, MMD_SR_MII_PHY_CTRL, &buf, 1);
+	} while (buf & SR_MII_PHY_START_BUSY);
+	buf = reg;
+	port_sgmii_w(sw, port, SR_MII, MMD_SR_MII_PHY_ADDR, &buf, 1);
+	buf = val;
+	port_sgmii_w(sw, port, SR_MII, MMD_SR_MII_PHY_DATA, &buf, 1);
+	buf = SR_MII_PHY_WRITE;
+	port_sgmii_w(sw, port, SR_MII, MMD_SR_MII_PHY_CTRL, &buf, 1);
+	buf |= SR_MII_PHY_START_BUSY;
+	port_sgmii_w(sw, port, SR_MII, MMD_SR_MII_PHY_CTRL, &buf, 1);
+	do {
+		port_sgmii_r(sw, port, SR_MII, MMD_SR_MII_PHY_CTRL, &buf, 1);
+	} while (buf & SR_MII_PHY_START_BUSY);
+}  /* port_sgmii_phy_w */
+
+static int port_sgmii_detect(struct ksz_sw *sw, uint p)
+{
+	u16 buf[6];
+	int ret = 0;
+
+	port_sgmii_phy_w(sw, p, SR_MII_PHY_JTAG_CHIP_ID_LO, 0x1234);
+	buf[0] = port_sgmii_phy_r(sw, p, SR_MII_PHY_JTAG_CHIP_ID_LO);
+	buf[1] = port_sgmii_phy_r(sw, p, SR_MII_PHY_JTAG_CHIP_ID_HI);
+dbg_msg("jtag: %04x %04x"NL, buf[1], buf[0]);
+	port_sgmii_r(sw, p, SR_MII, 0x8000, buf, 3);
+dbg_msg("%04x %04x %04x"NL,
+buf[0], buf[1], buf[2]);
+	port_sgmii_r(sw, p, SR_MII, 0, buf, 6);
+dbg_msg("%04x %04x %04x %04x %04x %04x"NL,
+buf[0], buf[1], buf[2], buf[3], buf[4], buf[5]);
+
+	/* Cannot detect whether the SGMII PHY is plugged in reliably. */
+	if (sw->sgmii_mode) {
+		ret = 1;
+		if (buf[5] & SR_MII_REMOTE_ACK) {
+			if (buf[5] & (SR_MII_REMOTE_HALF_DUPLEX |
+				      SR_MII_REMOTE_FULL_DUPLEX))
+				sw->port_info[p].fiber = 1;
+			else if (sw->sgmii_mode == 1)
+				sw->sgmii_mode = 2;
+		} else if (sw->sgmii_mode == 1)
+			sw->port_info[p].fiber = 1;
+	}
+dbg_msg("fiber: %d %d"NL, sw->port_info[p].fiber, sw->sgmii_mode);
+	return ret;
+}  /* port_sgmii_detect */
+
+static void port_sgmii_setup(struct ksz_sw *sw, uint p, bool pcs, bool master,
+	bool autoneg, int speed, int duplex)
+{
+	u16 cfg;
+	u16 ctrl;
+	u16 adv;
+
+dbg_msg("  sgmii_setup %d %d %d"NL, pcs, master, autoneg);
+	/* SGMII registers are not changed by reset. */
+	port_sgmii_r(sw, p, SR_MII, MMD_SR_MII_AUTO_NEG_CTRL, &cfg, 1);
+dbg_msg("  cfg: %04x"NL, cfg);
+	if (cfg & SR_MII_AUTO_NEG_COMPLETE_INTR)
+		return;
+	cfg = 0;
+	if (pcs)
+		cfg |= SR_MII_PCS_SGMII << SR_MII_PCS_MODE_S;
+	if (master) {
+		cfg |= SR_MII_TX_CFG_PHY_MASTER;
+		cfg |= SR_MII_SGMII_LINK_UP;
+	}
+	cfg |= SR_MII_AUTO_NEG_COMPLETE_INTR;
+dbg_msg("CFG: %04x"NL, cfg);
+	port_sgmii_w(sw, p, SR_MII, MMD_SR_MII_AUTO_NEG_CTRL, &cfg, 1);
+	port_sgmii_r(sw, p, SR_MII, MMD_SR_MII_CTRL, &ctrl, 1);
+	if (master || !autoneg) {
+		switch (speed) {
+		case 1:
+			ctrl |= SR_MII_SPEED_100MBIT;
+			break;
+		case 2:
+			ctrl |= SR_MII_SPEED_1000MBIT;
+			break;
+		}
+	}
+	if (!autoneg) {
+		ctrl &= ~SR_MII_AUTO_NEG_ENABLE;
+dbg_msg("CTRL: %04x"NL, ctrl);
+		port_sgmii_w(sw, p, SR_MII, MMD_SR_MII_CTRL, &ctrl, 1);
+		return;
+	} else if (!(ctrl & SR_MII_AUTO_NEG_ENABLE)) {
+		ctrl |= SR_MII_AUTO_NEG_ENABLE;
+dbg_msg("CTRL: %04x"NL, ctrl);
+		port_sgmii_w(sw, p, SR_MII, MMD_SR_MII_CTRL, &ctrl, 1);
+	}
+
+	/* Need to write to advertise register to send correct signal. */
+	/* Default value is 0x0020. */
+	port_sgmii_r(sw, p, SR_MII, MMD_SR_MII_AUTO_NEGOTIATION, &adv, 1);
+dbg_msg("  adv: %04x"NL, adv);
+	adv = SR_MII_AUTO_NEG_ASYM_PAUSE_RX << SR_MII_AUTO_NEG_PAUSE_S;
+	if (duplex)
+		adv |= SR_MII_AUTO_NEG_FULL_DUPLEX;
+	else
+		adv |= SR_MII_AUTO_NEG_HALF_DUPLEX;
+dbg_msg("ADV: %04x"NL, adv);
+	port_sgmii_w(sw, p, SR_MII, MMD_SR_MII_AUTO_NEGOTIATION, &adv, 1);
+	if (master && autoneg) {
+		ctrl |= SR_MII_AUTO_NEG_RESTART;
+dbg_msg("AUTO_NEG: %04x"NL, ctrl);
+		port_sgmii_w(sw, p, SR_MII, MMD_SR_MII_CTRL, &ctrl, 1);
+	}
+}  /* port_sgmii_setup */
+
+static int sgmii_port_get_speed(struct ksz_sw *sw, uint p, bool force_link)
+{
+	u16 data;
+	u16 speed;
+	u16 status;
+	u8 link;
+	int ret = 0;
+	struct ksz_port_info *info = get_port_info(sw, p);
+
+	port_sgmii_r(sw, p, SR_MII, MMD_SR_MII_STATUS, &status, 1);
+	port_sgmii_r(sw, p, SR_MII, MMD_SR_MII_STATUS, &status, 1);
+	port_sgmii_r(sw, p, SR_MII, MMD_SR_MII_AUTO_NEG_STATUS, &data, 1);
+
+	/* 10/100/1000: 1f0001 = 01ad  1f0005 = 4000  1f8002 = 0008
+	 *              1f0001 = 01bd  1f0005 = d000  1f8002 = 001a
+	 * 1000:        1f0001 = 018d  1f0005 = 0000  1f8002 = 0000
+	 *              1f0001 = 01ad  1f0005 = 40a0  1f8002 = 0000
+	 *              1f0001 = 01ad  1f0005 = 41a0  1f8002 = 0000
+	 * fiber:       1f0001 = 0189  1f0005 = 0000  1f8002 = 0000
+	 *              1f0001 = 01ad  1f0005 = 41a0  1f8002 = 0000
+	 */
+
+	/* Running in fiber mode. */
+	if (info->fiber && !data &&
+	    (status & (PORT_AUTO_NEG_ACKNOWLEDGE | PORT_LINK_STATUS)) ==
+	    (PORT_AUTO_NEG_ACKNOWLEDGE | PORT_LINK_STATUS)) {
+		data = SR_MII_STAT_LINK_UP |
+		       (SR_MII_STAT_1000_MBPS << SR_MII_STAT_S) |
+		       SR_MII_STAT_FULL_DUPLEX;
+	}
+	if (data & SR_MII_STAT_LINK_UP)
+		ret = 1;
+
+	link = data & ~SR_MII_AUTO_NEG_COMPLETE_INTR;
+	if (info->link == link)
+		return ret;
+dbg_msg(" sgmii %04x %04x"NL, status, data);
+	do {
+		u16 buf[6];
+
+		port_sgmii_r(sw, p, SR_MII, 0, buf, 6);
+dbg_msg("%04x %04x %04x %04x %04x %04x"NL,
+buf[0], buf[1], buf[2], buf[3], buf[4], buf[5]);
+	} while (0);
+
+	/* Need to update control register with same link setting. */
+	if (data & SR_MII_STAT_LINK_UP) {
+		u16 ctrl;
+
+		ctrl = SR_MII_AUTO_NEG_ENABLE;
+		speed = (data >> SR_MII_STAT_S) & SR_MII_STAT_M;
+		if (SR_MII_STAT_1000_MBPS == speed)
+			ctrl |= SR_MII_SPEED_1000MBIT;
+		else if (SR_MII_STAT_100_MBPS == speed)
+			ctrl |= SR_MII_SPEED_100MBIT;
+		if (data & SR_MII_STAT_FULL_DUPLEX)
+			ctrl |= SR_MII_FULL_DUPLEX;
+		port_sgmii_w(sw, p, SR_MII, MMD_SR_MII_CTRL, &ctrl, 1);
+	}
+	if (data & SR_MII_STAT_LINK_UP) {
+		speed = (data >> SR_MII_STAT_S) & SR_MII_STAT_M;
+		info->tx_rate = 10 * TX_RATE_UNIT;
+		if (SR_MII_STAT_1000_MBPS == speed)
+			info->tx_rate = 1000 * TX_RATE_UNIT;
+		else if (SR_MII_STAT_100_MBPS == speed)
+			info->tx_rate = 100 * TX_RATE_UNIT;
+
+		info->duplex = 1;
+		if (data & SR_MII_STAT_FULL_DUPLEX)
+			info->duplex = 2;
+
+		if (info->link != link) {
+			SW_D flow_ctrl;
+
+			port_r(sw, p, REG_PORT_STATUS_0, &flow_ctrl);
+#ifdef DBG_LINK
+			printk(KERN_INFO "flow_ctrl: "SW_SIZE_STR""NL,
+				flow_ctrl & (PORT_RX_FLOW_CTRL |
+				PORT_TX_FLOW_CTRL));
+#endif
+			info->flow_ctrl = 3;
+			if (flow_ctrl & PORT_RX_FLOW_CTRL)
+				info->flow_ctrl |= 0x10;
+			if (flow_ctrl & PORT_TX_FLOW_CTRL)
+				info->flow_ctrl |= 0x20;
+			ret |= 2;
+		}
+	} else {
+		ret = 2;
+	}
+	info->link = link;
+	return ret;
+}  /* sgmii_port_get_speed */
+
+static void sgmii_port_set_speed(struct ksz_sw *sw, uint p, int speed,
+				 int duplex, int flow_ctrl)
+{
+	u16 ctrl;
+	u16 cfg;
+	u16 adv;
+
+	port_sgmii_r(sw, p, SR_MII, MMD_SR_MII_AUTO_NEGOTIATION, &adv, 1);
+	cfg = adv;
+	switch (flow_ctrl) {
+	case PHY_FLOW_CTRL:
+		adv = SR_MII_AUTO_NEG_SYM_PAUSE;
+		break;
+	case PHY_TX_ONLY:
+		adv = SR_MII_AUTO_NEG_ASYM_PAUSE_TX;
+		break;
+	case PHY_RX_ONLY:
+		adv = SR_MII_AUTO_NEG_ASYM_PAUSE_RX;
+		break;
+	default:
+		adv = 0;
+	}
+	adv <<= SR_MII_AUTO_NEG_PAUSE_S;
+	adv |= SR_MII_AUTO_NEG_FULL_DUPLEX;
+	if (1 == duplex) {
+		adv &= ~SR_MII_AUTO_NEG_FULL_DUPLEX;
+		adv |= SR_MII_AUTO_NEG_HALF_DUPLEX;
+	}
+	if (adv != cfg) {
+dbg_msg("ADV: %04x"NL, adv);
+		port_sgmii_w(sw, p, SR_MII, MMD_SR_MII_AUTO_NEGOTIATION,
+			&adv, 1);
+		port_sgmii_r(sw, p, SR_MII, MMD_SR_MII_CTRL, &ctrl, 1);
+		ctrl |= SR_MII_AUTO_NEG_RESTART;
+		port_sgmii_w(sw, p, SR_MII, MMD_SR_MII_CTRL, &ctrl, 1);
+	}
+}  /* sgmii_port_set_speed */
+
+static int phy_port_get_speed(struct ksz_sw *sw, uint p, bool force_link)
+{
+	u16 data;
+	u16 link;
+	u16 dbg_link;
+	u16 status;
+	u32 local;
+	u32 remote;
+	int ret = 0;
+	struct ksz_port_info *info = get_port_info(sw, p);
+
+#ifdef NO_PHY_READ
+	if (sw) {
+		link = PORT_LINK_STATUS;
+		dbg_link = link;
+		status = PORT_STAT_SPEED_100MBIT |
+			 PORT_STAT_FULL_DUPLEX;
+		local = 0x10001000;
+		remote = 0x10001000;
+		goto get_cont1;
+	}
+#endif
+
+	if (sw->features & GIGABIT_SUPPORT)
+		port_r16(sw, p, REG_PORT_PHY_1000_CTRL, &data);
+	else
+		data = 0;
+	local = data;
+	local <<= 16;
+	port_r16(sw, p, REG_PORT_PHY_AUTO_NEGOTIATION, &data);
+	local |= data;
+	port_r16(sw, p, REG_PORT_PHY_1000_STATUS, &data);
+	remote = data;
+	remote &= PORT_PHY_1000_STATIC_STATUS;
+	remote <<= 16;
+	port_r16(sw, p, REG_PORT_PHY_REMOTE_CAPABILITY, &data);
+	remote |= data;
+	port_r16(sw, p, P_LINK_STATUS, &link);
+
+	/* Read second time in case the status is not latched. */
+	if (!(link & PORT_LINK_STATUS))
+		port_r16(sw, p, P_LINK_STATUS, &link);
+
+	/* SPI has problem reading the link status from the PHY. */
+	dbg_link = link;
+	port_r16(sw, p, P_SPEED_STATUS, &status);
+	if (!(sw->features & NEW_CAP) &&
+	    ((link & PORT_AUTO_NEG_ACKNOWLEDGE) && (status &
+	    (PORT_STAT_SPEED_1000MBIT |
+	     PORT_STAT_SPEED_100MBIT |
+	     PORT_STAT_SPEED_10MBIT))))
+		link |= PORT_LINK_STATUS;
+
+#ifdef NO_PHY_READ
+get_cont1:
+#endif
+	/* The partner capability register is updated but the
+	 * auto-negotiation is not completed yet.
+	 */
+	link &= (PORT_AUTO_NEG_ACKNOWLEDGE | PORT_LINK_STATUS);
+	status &= (PORT_STAT_SPEED_1000MBIT |
+		  PORT_STAT_SPEED_100MBIT |
+		  PORT_STAT_SPEED_10MBIT |
+		  PORT_STAT_FULL_DUPLEX);
+	link |= (status << 8);
+
+	if (link & PORT_LINK_STATUS)
+		ret = 1;
+
+	/* No change to status. */
+	if (local == info->advertised && link == info->link)
+		return ret;
+
+	if (!(dbg_link & PORT_LINK_STATUS) &&
+	    dbg_link & PORT_AUTO_NEG_ACKNOWLEDGE)
+dbg_msg(" link? %d=%04x"NL, p, dbg_link);
+#ifdef DBG_LINK
+	printk(KERN_INFO
+		"%d=advertised: %08X-%08X; partner: %08X-%08X"
+		"; link: %04X-%04X"NL, p,
+		local, info->advertised, remote, info->partner,
+		link, info->link);
+#endif
+	if (link & PORT_LINK_STATUS) {
+		info->tx_rate = 10 * TX_RATE_UNIT;
+		if (status & PORT_STAT_SPEED_100MBIT)
+			info->tx_rate = 100 * TX_RATE_UNIT;
+		else if (status & PORT_STAT_SPEED_1000MBIT)
+			info->tx_rate = 1000 * TX_RATE_UNIT;
+
+		info->duplex = 1;
+		if (status & PORT_STAT_FULL_DUPLEX)
+			info->duplex = 2;
+
+		if (link != info->link) {
+			SW_D flow_ctrl;
+
+			port_r(sw, p, REG_PORT_STATUS_0, &flow_ctrl);
+#ifdef DBG_LINK
+			printk(KERN_INFO "flow_ctrl: "SW_SIZE_STR""NL,
+				flow_ctrl & (PORT_RX_FLOW_CTRL |
+				PORT_TX_FLOW_CTRL));
+#endif
+			info->flow_ctrl = sw_determine_flow_ctrl(sw,
+				force_link, local, remote);
+			if (flow_ctrl & PORT_RX_FLOW_CTRL)
+				info->flow_ctrl |= 0x10;
+			if (flow_ctrl & PORT_TX_FLOW_CTRL)
+				info->flow_ctrl |= 0x20;
+			if (sw->info)
+				port_cfg_back_pressure(sw, p,
+					(1 == info->duplex));
+			ret |= 2;
+		}
+	} else {
+		ret = 2;
+	}
+	info->advertised = local;
+	info->partner = remote;
+	info->link = link;
+	return ret;
+}  /* phy_port_get_speed */
+
+static void phy_port_set_speed(struct ksz_sw *sw, uint p, int speed,
+			       int duplex, int flow_ctrl)
+{
+	struct ksz_port_cfg *port_cfg;
+	u16 data;
+	u16 ctrl;
+	u16 local;
+	u16 status;
+	u32 adv;
+	u32 cfg;
+
+	if (sw->features & GIGABIT_SUPPORT)
+		port_r16(sw, p, REG_PORT_PHY_1000_CTRL, &ctrl);
+	else
+		ctrl = 0;
+	port_r16(sw, p, REG_PORT_PHY_AUTO_NEGOTIATION, &local);
+	if (!(local & PORT_AUTO_NEG_SYM_PAUSE) &&
+	    (PHY_FLOW_CTRL == flow_ctrl ||
+	    PHY_RX_ONLY == flow_ctrl))
+		dbg_msg(" no sym pause: %d %04x"NL, p, local);
+	if ((local & PORT_AUTO_NEG_ASYM_PAUSE) &&
+	    (PHY_TX_ONLY != flow_ctrl &&
+	    PHY_RX_ONLY != flow_ctrl))
+		dbg_msg(" has asym pause: %d %04x"NL, p, local);
+	adv = ctrl;
+	adv <<= 16;
+	adv |= local;
+	port_r16(sw, p, P_SPEED_STATUS, &status);
+	port_r16(sw, p, P_NEG_RESTART_CTRL, &data);
+
+	cfg = 0;
+
+	/* Do not need to restart auto-negotiation if desired settings
+	 * are same.
+	 */
+	if ((data & PORT_AUTO_NEG_ENABLE) &&
+	    (status &
+	    (PORT_STAT_SPEED_1000MBIT |
+	     PORT_STAT_SPEED_100MBIT |
+	     PORT_STAT_SPEED_10MBIT)))
+		cfg = adv;
+
+	/* Need auto-negotiation restart. */
+	port_cfg = get_port_cfg(sw, p);
+	if (port_cfg->setup_time) {
+		cfg = 0;
+		port_cfg->setup_time = 0;
+	}
+
+	local = port_advertised_flow_ctrl(flow_ctrl, local);
+
+	/* 1000 half-duplex is not supported. */
+	if (sw->features & GIGABIT_SUPPORT)
+		ctrl |= PORT_AUTO_NEG_1000BT_FD;
+	local |= PORT_AUTO_NEG_100BTX_FD | PORT_AUTO_NEG_100BTX |
+		PORT_AUTO_NEG_10BT_FD | PORT_AUTO_NEG_10BT;
+
+	/* Check if manual configuration is specified by the user. */
+	if (speed || duplex) {
+		if (speed && speed != 1000)
+			ctrl &= ~(PORT_AUTO_NEG_1000BT_FD |
+				PORT_AUTO_NEG_1000BT);
+		if (10 == speed)
+			local &= ~(PORT_AUTO_NEG_100BTX_FD |
+				PORT_AUTO_NEG_100BTX);
+		else if (100 == speed)
+			local &= ~(PORT_AUTO_NEG_10BT_FD |
+				PORT_AUTO_NEG_10BT);
+		else if (1000 == speed)
+			local &= ~(PORT_AUTO_NEG_100BTX_FD |
+				PORT_AUTO_NEG_100BTX |
+				PORT_AUTO_NEG_10BT_FD |
+				PORT_AUTO_NEG_10BT);
+		if (1 == duplex) {
+			ctrl &= ~PORT_AUTO_NEG_1000BT_FD;
+			local &= ~(PORT_AUTO_NEG_100BTX_FD |
+				PORT_AUTO_NEG_10BT_FD);
+		} else if (2 == duplex) {
+			ctrl &= ~PORT_AUTO_NEG_1000BT;
+			local &= ~(PORT_AUTO_NEG_100BTX |
+				PORT_AUTO_NEG_10BT);
+		}
+	}
+	adv = ctrl;
+	adv <<= 16;
+	adv |= local;
+	if (adv != cfg) {
+		if (sw->features & GIGABIT_SUPPORT)
+			port_w16(sw, p, REG_PORT_PHY_1000_CTRL, ctrl);
+		port_w16(sw, p, REG_PORT_PHY_AUTO_NEGOTIATION, local);
+		port_r16(sw, p, P_NEG_RESTART_CTRL, &data);
+		data |= PORT_AUTO_NEG_ENABLE;
+		port_cfg->phy_ctrl = data;
+		data |= PORT_AUTO_NEG_RESTART;
+		port_w16(sw, p, P_NEG_RESTART_CTRL, data);
+
+		/* Link is going down. */
+		sw->port_state[p].state = media_disconnected;
+	}
+}  /* phy_port_set_speed */
+
+static void phy_port_force_speed(struct ksz_sw *sw, uint p, int speed,
+			         int duplex, int flow_ctrl)
+{
+	struct ksz_port_cfg *cfg;
+	u16 data;
+
+	port_r16(sw, p, P_PHY_CTRL, &data);
+	data &= ~(PORT_AUTO_NEG_ENABLE |
+		PORT_SPEED_100MBIT | PORT_SPEED_1000MBIT);
+	if (100 == speed)
+		data |= PORT_SPEED_100MBIT;
+	else if (1000 == speed)
+		data |= PORT_SPEED_1000MBIT;
+	if (1 == duplex)
+		data &= ~PORT_FULL_DUPLEX;
+	else if (2 == duplex)
+		data |= PORT_FULL_DUPLEX;
+	port_w16(sw, p, P_PHY_CTRL, data);
+	cfg = get_port_cfg(sw, p);
+	cfg->phy_ctrl = data;
+}  /* phy_port_force_speed */
+
+/**
+ * port_get_link_speed - get current link status
+ * @port:	The port instance.
+ *
+ * This routine reads PHY registers to determine the current link status of the
+ * switch ports.
+ */
+static int port_get_link_speed(struct ksz_port *port)
+{
+	struct ksz_port_info *info;
+	struct ksz_port_info *linked = NULL;
+	struct ksz_port_state *state;
+	struct ksz_sw *sw = port->sw;
+	int link;
+	uint i;
+	uint n;
+	uint p;
+	int change = 0;
+
+	for (i = 0, n = port->first_port; i < port->port_cnt; i++, n++) {
+		p = get_phy_port(sw, n);
+		info = get_port_info(sw, p);
+
+		/* Port does not have PHY. */
+		if (!info->phy) {
+
+			/* By itself. */
+			if (1 == port->port_cnt && port->report &&
+			    (sw->phy_intr & (1 << p)))
+				change |= 1 << p;
+			continue;
+		}
+
+		if (!(sw->phy_intr & (1 << p))) {
+			if (!linked && p != sw->HOST_PORT &&
+			    info->state == media_connected)
+				linked = info;
+			continue;
+		}
+		sw->phy_intr &= ~(1 << p);
+
+		state = &sw->port_state[p];
+
+		if (!info->get_link_speed)
+			continue;
+		link = info->get_link_speed(sw, p, port->force_link);
+		if (link & 1) {
+
+			/* Remember the first linked port. */
+			if (!linked && p != sw->HOST_PORT)
+				linked = info;
+		}
+
+		/* No change to status. */
+		if (!(link & 2))
+			continue;
+
+		if (link & 1) {
+			if (media_connected != info->state) {
+				change |= 1 << p;
+			}
+			if (link & 2)
+				change |= 1 << p;
+			info->state = media_connected;
+			state->tx_rate = info->tx_rate;
+		} else {
+			if (media_disconnected != info->state) {
+				change |= 1 << p;
+
+				/* Indicate the link just goes down. */
+				state->link_down = 1;
+
+				/* For 802.1X Authentication. */
+				if ((sw->overrides & USE_802_1X_AUTH) &&
+				    (sw->on_ports & (1 << p)))
+					port_open(sw, p, false);
+			}
+			info->state = media_disconnected;
+		}
+		if (media_disconnected == info->state) {
+			sw->live_ports &= ~(1 << p);
+			port->live_ports &= ~(1 << p);
+		} else {
+			sw->live_ports |= (1 << p);
+			port->live_ports |= (1 << p);
+		}
+		state->state = info->state;
+	}
+
+	if (linked && media_disconnected == port->linked->state)
+		port->linked = linked;
+
+#ifdef DBG_LINK
+	if (change)
+		dbp_link(port, sw, change);
+#endif
+	if (change) {
+		port->report = true;
+		port->link_ports |= change;
+	}
+	return change;
+}  /* port_get_link_speed */
+
+/**
+ * port_set_link_speed - set port speed
+ * @port:	The port instance.
+ *
+ * This routine sets the link speed of the switch ports.
+ */
+static void port_set_link_speed(struct ksz_port *port)
+{
+	struct ksz_sw *sw = port->sw;
+	struct ksz_port_info *info;
+	uint i;
+	uint n;
+	uint p;
+
+	for (i = 0, n = port->first_port; i < port->port_cnt; i++, n++) {
+		p = get_phy_port(sw, n);
+		info = get_port_info(sw, p);
+		if (!info->set_link_speed)
+			continue;
+
+		info->own_flow_ctrl = port->flow_ctrl;
+		info->own_duplex = port->duplex;
+		info->own_speed = port->speed;
+
+		info->set_link_speed(sw, p, port->speed, port->duplex,
+				     port->flow_ctrl);
+	}
+}  /* port_set_link_speed */
+
+/**
+ * port_force_link_speed - force port speed
+ * @port:	The port instance.
+ *
+ * This routine forces the link speed of the switch ports.
+ */
+static void port_force_link_speed(struct ksz_port *port)
+{
+	struct ksz_sw *sw = port->sw;
+	struct ksz_port_info *info;
+	uint i;
+	uint n;
+	uint p;
+
+	for (i = 0, n = port->first_port; i < port->port_cnt; i++, n++) {
+		p = get_phy_port(sw, n);
+		info = get_port_info(sw, p);
+		if (!info->force_link_speed)
+			continue;
+
+		info->own_flow_ctrl = port->flow_ctrl;
+		info->own_duplex = port->duplex;
+		info->own_speed = port->speed;
+
+		info->force_link_speed(sw, p, port->speed, port->duplex,
+				       port->flow_ctrl);
+	}
+}  /* port_force_link_speed */
+
+static void port_mmd_read(struct ksz_sw *sw, uint port, u16 devid, u16 reg,
+	u16 *buf, u16 len)
+{
+	port_w16(sw, port, REG_PORT_PHY_MMD_SETUP,
+		MMD_SETUP(PORT_MMD_OP_INDEX, devid));
+	port_w16(sw, port, REG_PORT_PHY_MMD_INDEX_DATA, reg);
+	if (len > 1)
+		port_w16(sw, port, REG_PORT_PHY_MMD_SETUP,
+			MMD_SETUP(PORT_MMD_OP_DATA_INCR_RW, devid));
+	else
+		port_w16(sw, port, REG_PORT_PHY_MMD_SETUP,
+			MMD_SETUP(PORT_MMD_OP_DATA_NO_INCR, devid));
+	while (len) {
+		port_r16(sw, port, REG_PORT_PHY_MMD_INDEX_DATA, buf);
+		buf++;
+		len--;
+	}
+}  /* port_mmd_read */
+
+static void port_mmd_write(struct ksz_sw *sw, uint port, u16 devid, u16 reg,
+	u16 *buf, u16 len)
+{
+	port_w16(sw, port, REG_PORT_PHY_MMD_SETUP,
+		MMD_SETUP(PORT_MMD_OP_INDEX, devid));
+	port_w16(sw, port, REG_PORT_PHY_MMD_INDEX_DATA, reg);
+	if (len > 1)
+		port_w16(sw, port, REG_PORT_PHY_MMD_SETUP,
+			MMD_SETUP(PORT_MMD_OP_DATA_INCR_W, devid));
+	else
+		port_w16(sw, port, REG_PORT_PHY_MMD_SETUP,
+			MMD_SETUP(PORT_MMD_OP_DATA_NO_INCR, devid));
+	while (len) {
+		port_w16(sw, port, REG_PORT_PHY_MMD_INDEX_DATA, *buf);
+		buf++;
+		len--;
+	}
+}  /* port_mmd_write */
+
+static void port_chk_sqi(struct ksz_sw *sw, uint port)
+{
+	u16 sqi[4];
+	int val;
+	int val2;
+	int i;
+	int n;
+	int cnt = 10;
+	int num = 4;
+	struct ksz_port_info *info = get_port_info(sw, port);
+
+	if (info->state == media_disconnected)
+		return;
+	if (info->tx_rate / TX_RATE_UNIT != 1000)
+		num = 1;
+	val = 0;
+	for (n = 0; n < cnt; n++) {
+		port_mmd_read(sw, port, MMD_DEVICE_ID_DSP, MMD_DSP_SQI_CHAN_A,
+			sqi, num);
+		val2 = 0;
+		for (i = 0; i < num; i++) {
+			if (sqi[i] & DSP_SQI_ERR_DETECTED)
+dbg_msg(" ?? %s %d %04x"NL, __func__, i, sqi[i]);
+			if (sqi[i] & DSP_SQI_ERR_DETECTED)
+				break;
+			val2 += sqi[i] & DSP_SQI_AVG_ERR;
+		}
+		val2 /= num;
+		val += val2;
+	}
+	val /= cnt;
+	val >>= 8;
+	if (val > 15)
+		val = 15;
+	val = 15 - val;
+	info->sqi = val;
+}  /* port_chk_sqi */
+
+#if 1
+#define SETUP_PHY_OLD
+#endif
+#if 0
+#define SETUP_PHY_NEW
+#endif
+
+struct ksz_phy_settings {
+	u16 mmd;
+	u16 reg;
+	u16 val;
+};
+
+static struct ksz_phy_settings ksz9893_phy_settings[] = {
+	{ MMD_DEVICE_ID_DSP, 0xa0, 0x3fff },
+};
+
+static void port_setup_eee(struct ksz_sw *sw, uint port)
+{
+	u16 val[0x20];
+
+	if (sw->features & NEW_CAP) {
+#ifdef DEBUG_PHY
+dbg_msg("%s %d"NL, __func__, port);
+if (0 == port) {
+int i;
+		port_r16(sw, port, REG_PORT_PHY_CTRL, val);
+dbg_msg("%04x=%04x"NL, REG_PORT_PHY_CTRL, val[0]);
+		port_r16(sw, port, REG_PORT_PHY_REMOTE_LB_LED, val);
+dbg_msg("%04x=%04x"NL, REG_PORT_PHY_REMOTE_LB_LED, val[0]);
+
+dbg_msg(" %x"NL, MMD_DEVICE_ID_DSP);
+		port_mmd_read(sw, port, MMD_DEVICE_ID_DSP, 0x6F, val, 1);
+dbg_msg(" %04x=%04x"NL, 0x6F, val[0]);
+		port_mmd_read(sw, port, MMD_DEVICE_ID_DSP, 0xCE, val, 1);
+dbg_msg(" %04x=%04x"NL, 0xCE, val[0]);
+		port_mmd_read(sw, port, MMD_DEVICE_ID_DSP, 0xCC, val, 1);
+dbg_msg(" %04x=%04x"NL, 0xCC, val[0]);
+		port_mmd_read(sw, port, MMD_DEVICE_ID_DSP, 0xCA, val, 1);
+dbg_msg(" %04x=%04x"NL, 0xCA, val[0]);
+		port_mmd_read(sw, port, MMD_DEVICE_ID_DSP, 0xCB, val, 1);
+dbg_msg(" %04x=%04x"NL, 0xCB, val[0]);
+		port_mmd_read(sw, port, MMD_DEVICE_ID_DSP, 0xC8, val, 1);
+dbg_msg(" %04x=%04x"NL, 0xC8, val[0]);
+		port_mmd_read(sw, port, MMD_DEVICE_ID_DSP, 0xD9, val, 1);
+dbg_msg(" %04x=%04x"NL, 0xD9, val[0]);
+		port_mmd_read(sw, port, MMD_DEVICE_ID_DSP, 0xC9, val, 1);
+dbg_msg(" %04x=%04x"NL, 0xC9, val[0]);
+
+		port_mmd_read(sw, port, MMD_DEVICE_ID_DSP, 0x79, &val[0x09],
+			18);
+for (i = 0; i < 18; i++)
+dbg_msg("%04x ", val[0x09 + i]);
+dbg_msg(NL);
+
+		port_mmd_read(sw, port, MMD_DEVICE_ID_DSP, 0x8F, val, 1);
+dbg_msg(" %04x=%04x"NL, 0x8F, val[0]);
+		port_mmd_read(sw, port, MMD_DEVICE_ID_DSP, 0x9D, val, 1);
+dbg_msg(" %04x=%04x"NL, 0x9D, val[0]);
+
+		port_mmd_read(sw, port, MMD_DEVICE_ID_DSP, 0x75, val, 1);
+dbg_msg(" %04x=%04x"NL, 0x75, val[0]);
+		port_mmd_read(sw, port, MMD_DEVICE_ID_DSP, 0xD3, val, 1);
+dbg_msg(" %04x=%04x"NL, 0xD3, val[0]);
+
+dbg_msg(" %x"NL, MMD_DEVICE_ID_AFED);
+		port_mmd_read(sw, port, 0x1C, 0x0, val, 1);
+dbg_msg(" %04x=%04x"NL, 0x0, val[0]);
+		port_mmd_read(sw, port, 0x1C, 0x4, val, 1);
+dbg_msg(" %04x=%04x"NL, 0x4, val[0]);
+		port_mmd_read(sw, port, 0x1C, 0x6, val, 1);
+dbg_msg(" %04x=%04x"NL, 0x6, val[0]);
+		port_mmd_read(sw, port, 0x1C, 0x8, val, 1);
+dbg_msg(" %04x=%04x"NL, 0x8, val[0]);
+		port_mmd_read(sw, port, 0x1C, 0x9, val, 1);
+dbg_msg(" %04x=%04x"NL, 0x9, val[0]);
+
+		port_mmd_read(sw, port, 0x1C, 0x13, &val[0x13], 12);
+for (i = 0; i < 12; i++)
+dbg_msg("%04x ", val[0x13 + i]);
+dbg_msg(NL);
+		port_mmd_read(sw, port, 0x1C, 0x20, val, 1);
+dbg_msg(" %04x=%04x"NL, 0x20, val[0]);
+}
+#endif
+
+		port_w16(sw, port, REG_PORT_PHY_CTRL, 0x2100);
+#ifdef SETUP_PHY_OLD
+		port_w16(sw, port, REG_PORT_PHY_REMOTE_LB_LED, 0x00f0);
+#endif
+
+#ifdef SETUP_PHY_NEW
+		val[0] = 0xDD0B;
+		port_mmd_write(sw, port, MMD_DEVICE_ID_DSP, 0x6F, val, 1);
+#else
+		val[0] = 0x0100;
+		port_mmd_write(sw, port, MMD_DEVICE_ID_DSP, 0xCE, val, 1);
+		val[0] = 0x0ff0;
+		port_mmd_write(sw, port, MMD_DEVICE_ID_DSP, 0xCC, val, 1);
+		val[0] = 0x0141;
+		port_mmd_write(sw, port, MMD_DEVICE_ID_DSP, 0xCA, val, 1);
+		val[0] = 0x0fcf;
+		port_mmd_write(sw, port, MMD_DEVICE_ID_DSP, 0xCB, val, 1);
+		val[0] = 0x0010;
+		port_mmd_write(sw, port, MMD_DEVICE_ID_DSP, 0xC8, val, 1);
+		val[0] = 0x0100;
+		port_mmd_write(sw, port, MMD_DEVICE_ID_DSP, 0xD9, val, 1);
+		val[0] = 0x0280;
+		port_mmd_write(sw, port, MMD_DEVICE_ID_DSP, 0xC9, val, 1);
+#endif
+
+#ifdef SETUP_PHY_OLD
+		val[0x09] = 0x010A;
+		val[0x0a] = 0x00ED;
+		val[0x0b] = 0x00D3;
+		val[0x0c] = 0x00BC;
+		val[0x0d] = 0x00A8;
+		val[0x0e] = 0x0096;
+		val[0x0f] = 0x0085;
+		val[0x10] = 0x0077;
+		val[0x11] = 0x006A;
+		val[0x12] = 0x005E;
+		val[0x13] = 0x0054;
+		val[0x14] = 0x004B;
+		val[0x15] = 0x0043;
+		val[0x16] = 0x003C;
+		val[0x17] = 0x0035;
+		val[0x18] = 0x002F;
+		val[0x19] = 0x002A;
+		val[0x1a] = 0x0026;
+		port_mmd_write(sw, port, MMD_DEVICE_ID_DSP, 0x79, &val[0x09],
+			18);
+#endif
+
+		val[0] = 0x6032;
+		port_mmd_write(sw, port, MMD_DEVICE_ID_DSP, 0x8F, val, 1);
+		val[0] = 0x248C;
+		port_mmd_write(sw, port, MMD_DEVICE_ID_DSP, 0x9D, val, 1);
+
+		val[0] = 0x0060;
+		port_mmd_write(sw, port, MMD_DEVICE_ID_DSP, 0x75, val, 1);
+		val[0] = 0x7777;
+		port_mmd_write(sw, port, MMD_DEVICE_ID_DSP, 0xD3, val, 1);
+
+#ifdef SETUP_PHY_OLD
+		val[0] = 0x9400;
+		port_mmd_write(sw, port, MMD_DEVICE_ID_AFED, 0x0, val, 1);
+#endif
+
+#ifdef SETUP_PHY_NEW
+		val[0] = 0x00d0;
+		port_mmd_write(sw, port, MMD_DEVICE_ID_AFED, 0x4, val, 1);
+		val[0] = 0x3008;
+		port_mmd_write(sw, port, MMD_DEVICE_ID_AFED, 0x6, val, 1);
+		val[0] = 0x2000;
+		port_mmd_write(sw, port, MMD_DEVICE_ID_AFED, 0x8, val, 1);
+#else
+/*
+ * THa  2016/10/20
+ * Use value 0x00E2 for improved 100BTX PMD Output Amplitude.
+ */
+		val[0] = 0x00e2;
+		port_mmd_write(sw, port, MMD_DEVICE_ID_AFED, 0x4, val, 1);
+		val[0] = 0x3100;
+		port_mmd_write(sw, port, MMD_DEVICE_ID_AFED, 0x6, val, 1);
+		val[0] = 0xe01c;
+		port_mmd_write(sw, port, MMD_DEVICE_ID_AFED, 0x9, val, 1);
+#endif
+
+		val[0x13] = 0x6eff;
+		val[0x14] = 0xe6ff;
+		val[0x15] = 0x6eff;
+		val[0x16] = 0xe6ff;
+		val[0x17] = 0x00ff;
+		val[0x18] = 0x43ff;
+		val[0x19] = 0xc3ff;
+		val[0x1a] = 0x6fff;
+		val[0x1b] = 0x07ff;
+		val[0x1c] = 0x0fff;
+		val[0x1d] = 0xe7ff;
+		val[0x1e] = 0xefff;
+		port_mmd_write(sw, port, MMD_DEVICE_ID_AFED, 0x13, &val[0x13],
+			12);
+#ifdef SETUP_PHY_NEW
+		val[0] = 0xeeee;
+		port_mmd_write(sw, port, MMD_DEVICE_ID_AFED, 0x20, val, 1);
+#endif
+
+		if (port == sw->HOST_PORT)
+			port_w16(sw, port, REG_PORT_PHY_CTRL, 0x1140);
+		else {
+			/* Avoid empty interrupt at beginning. */
+			port_w16(sw, port, REG_PORT_PHY_CTRL, 0x0140);
+			sw->info->port_cfg[port].setup_time = 4000;
+		}
+	} else {
+		memset(val, 0, sizeof(val));
+		port_mmd_write(sw, port, 0x1C, 0x10, val, 0x11);
+	}
+}  /* port_setup_eee */
+
+static void port_setup_9893(struct ksz_sw *sw, uint port)
+{
+	u16 val[1];
+	int i;
+	struct ksz_phy_settings *set;
+
+	for (i = 0; i < sizeof(ksz9893_phy_settings) /
+	     sizeof(struct ksz_phy_settings); i++) {
+		set = &ksz9893_phy_settings[i];
+		val[0] = set->val;
+		port_mmd_write(sw, port, set->mmd, set->reg, val, 1);
+	}
+}  /* port_setup_9893 */
+
+static void port_cfg_power(struct ksz_sw *sw, uint p, bool set)
+{
+	u16 ctrl;
+	u8 intr;
+	struct ksz_port_cfg *cfg = get_port_cfg(sw, p);
+
+	if (set) {
+		u32 data;
+
+		port_r16(sw, p, REG_PORT_PHY_CTRL, &ctrl);
+		if (!(ctrl & PORT_POWER_DOWN))
+			return;
+		ctrl = cfg->phy_ctrl;
+		port_w16(sw, p, REG_PORT_PHY_CTRL, cfg->phy_ctrl);
+
+		/* After IBA write the value may become zero. */
+		port_r16(sw, p, REG_PORT_PHY_CTRL, &ctrl);
+		if (!ctrl)
+			port_w16(sw, p, REG_PORT_PHY_CTRL, cfg->phy_ctrl);
+		port_r16(sw, p, REG_PORT_PHY_AUTO_NEGOTIATION, &ctrl);
+		if (ctrl != cfg->phy_adv)
+			port_w16(sw, p, REG_PORT_PHY_AUTO_NEGOTIATION,
+				cfg->phy_adv);
+		if (sw->features & GIGABIT_SUPPORT) {
+			port_r16(sw, p, REG_PORT_PHY_1000_CTRL, &ctrl);
+			if (ctrl != cfg->phy_adv_g)
+				port_w16(sw, p, REG_PORT_PHY_1000_CTRL,
+					cfg->phy_adv_g);
+		}
+		port_r32(sw, p, REG_PORT_PHY_INT_ENABLE & ~3, &data);
+		data &= 0xffff00ff;
+		data |= cfg->phy_intr << 8;
+		port_w32(sw, p, REG_PORT_PHY_INT_ENABLE & ~3, data);
+		if (sw->features & IS_9893)
+			port_setup_9893(sw, p);
+		else
+			port_setup_eee(sw, p);
+		cfg->setup_time = 0;
+		ctrl = cfg->phy_ctrl;
+		if (ctrl & PORT_AUTO_NEG_ENABLE)
+			ctrl |= PORT_AUTO_NEG_RESTART;
+		port_w16(sw, p, REG_PORT_PHY_CTRL, ctrl);
+	} else {
+		port_r8(sw, p, REG_PORT_PHY_INT_ENABLE, &intr);
+		cfg->phy_intr = intr;
+		port_r16(sw, p, REG_PORT_PHY_AUTO_NEGOTIATION, &ctrl);
+		cfg->phy_adv = ctrl;
+		if (sw->features & GIGABIT_SUPPORT) {
+			port_r16(sw, p, REG_PORT_PHY_1000_CTRL, &ctrl);
+			cfg->phy_adv_g = ctrl;
+		}
+		port_r16(sw, p, REG_PORT_PHY_CTRL, &ctrl);
+		ctrl &= ~PORT_POWER_DOWN;
+		cfg->phy_ctrl = ctrl;
+		port_w16(sw, p, REG_PORT_PHY_CTRL, ctrl | PORT_POWER_DOWN);
+	}
+}  /* port_cfg_power */
+
+static void sw_dis_intr(struct ksz_sw *sw)
+{
+	sw->reg->w32(sw, REG_SW_INT_MASK__4, SWITCH_INT_MASK);
+	sw->reg->w32(sw, REG_SW_PORT_INT_MASK__4, sw->PORT_INTR_MASK);
+}  /* sw_dis_intr */
+
+static void sw_ena_intr(struct ksz_sw *sw)
+{
+	uint n;
+	uint p;
+
+	sw->reg->w32(sw, REG_SW_INT_MASK__4,
+		     ~sw->intr_mask & SWITCH_INT_MASK);
+	sw->reg->w32(sw, REG_SW_PORT_INT_MASK__4,
+		     ~sw->port_intr_mask & sw->PORT_INTR_MASK);
+	sw->reg->w8(sw, REG_SW_LUE_INT_MASK__1,
+		     ~sw->lue_intr_mask & LUE_INT_MASK);
+	for (n = 0; n <= sw->mib_port_cnt; n++) {
+		p = get_phy_port(sw, n);
+		port_w(sw, p, REG_PORT_INT_MASK,
+			~sw->info->port_cfg[p].intr_mask & PORT_INT_MASK);
+	}
+}  /* sw_ena_intr */
+
+/**
+ * sw_enable - enable the switch
+ * @sw:		The switch instance.
+ *
+ * This routine enables the switch with a specific configuration.
+ */
+static void sw_enable(struct ksz_sw *sw)
+{
+	uint n;
+	uint port;
+	bool fewer;
+	struct ksz_port_cfg *cfg;
+	struct ksz_port_info *info;
+	int state = STP_STATE_FORWARDING;
+
+	/* Manually change default membership when not all ports are used. */
+	fewer = false;
+	for (port = 0; port < sw->port_cnt; port++) {
+		info = get_port_info(sw, port);
+		if (info->log_m) {
+			cfg = get_port_cfg(sw, port);
+			cfg->vid_member = sw->PORT_MASK;
+		} else {
+			fewer = true;
+			port_set_stp_state(sw, port, STP_STATE_DISABLED);
+			if (port < sw->phy_port_cnt)
+				port_cfg_power(sw, port, false);
+		}
+	}
+	if (fewer)
+dbg_msg(" fewer: %d %d"NL, fewer, sw->eth_cnt);
+	if (fewer)
+		sw_cfg_port_base_vlan(sw, sw->HOST_PORT, sw->PORT_MASK);
+	if ((sw->dev_count > 1 && !sw->dev_offset) ||
+	    (sw->features & STP_SUPPORT)) {
+		u16 member;
+
+		for (n = 1; n <= sw->mib_port_cnt; n++) {
+			port = get_phy_port(sw, n);
+			member = (1 << port);
+			if (sw->features & SW_VLAN_DEV) {
+				struct ksz_dev_map *map;
+				int q;
+
+				for (q = 0; q < sw->eth_cnt; q++) {
+					map = &sw->eth_maps[q];
+					if (map->first <= n &&
+					    n <= map->first + map->cnt - 1) {
+						member = map->mask;
+						break;
+					}
+				}
+			}
+			cfg = get_port_cfg(sw, port);
+			cfg->vid_member = member;
+		}
+	} else if (1 == sw->eth_cnt) {
+		u16 member;
+
+		for (n = 1; n <= sw->mib_port_cnt; n++) {
+			port = get_phy_port(sw, n);
+			member = 0;
+			if (sw->eth_maps[0].mask & (1 << port))
+				member = sw->eth_maps[0].mask;
+			cfg = get_port_cfg(sw, port);
+			cfg->vid_member = member;
+		}
+	}
+	for (n = 1; n <= sw->mib_port_cnt; n++) {
+		port = get_phy_port(sw, n);
+		if (sw->dev_count > 1 ||
+		    (sw->eth_maps[0].mask &&
+		    !(sw->eth_maps[0].mask & (1 << port))))
+			port_set_stp_state(sw, port, STP_STATE_DISABLED);
+		else if ((sw->eth_maps[0].mask & (1 << port)) &&
+			 (sw->eth_maps[0].proto & HSR_HW))
+			port_set_stp_state(sw, port, STP_STATE_SIMPLE);
+		else
+			port_set_stp_state(sw, port, state);
+	}
+	if (sw->dev_count > 1 && !sw->dev_offset && sw->eth_cnt < 2)
+		port_set_stp_state(sw, sw->HOST_PORT, STP_STATE_SIMPLE);
+	else
+		port_set_stp_state(sw, sw->HOST_PORT, state);
+
+	/*
+	 * There may be some entries in the dynamic MAC table before the
+	 * the learning is turned off.  Once the entries are in the table the
+	 * switch may keep updating them even learning is off.
+	 */
+	if (sw->dev_count > 1)
+		sw_flush_dyn_mac_table(sw, sw->port_cnt);
+}  /* sw_enable */
+
+static void sw_init_cached_regs(struct ksz_sw *sw)
+{
+	sw->cached.ptp_clk_ctrl = sw->reg->r16(sw, REG_PTP_CLK_CTRL);
+	sw->cached.ptp_unit_index = sw->reg->r32(sw, REG_PTP_UNIT_INDEX__4);
+}
+
+/**
+ * sw_init - initialize the switch
+ * @sw:		The switch instance.
+ *
+ * This routine initializes the hardware switch engine for default operation.
+ */
+static void sw_init(struct ksz_sw *sw)
+{
+	memset(sw->tx_pad, 0, 60);
+	sw->tx_start = 0;
+	sw_init_cached_regs(sw);
+	sw->open_ports = sw->PORT_MASK & ~sw->HOST_MASK;
+	sw->mtu = sw->reg->r16(sw, REG_SW_MTU__2);
+
+	sw_init_broad_storm(sw);
+
+	sw_init_prio(sw);
+
+	sw_init_prio_rate(sw);
+
+	sw_init_vlan(sw);
+
+	sw_init_acl(sw);
+#if 0
+	if (!sw_chk(sw, REG_SWITCH_CTRL_1,
+			SWITCH_TX_FLOW_CTRL | SWITCH_RX_FLOW_CTRL))
+		sw->overrides |= PAUSE_FLOW_CTRL;
+#endif
+}  /* sw_init */
+
+/**
+ * sw_setup - setup the switch
+ * @sw:		The switch instance.
+ *
+ * This routine setup the hardware switch engine for default operation.
+ */
+static void sw_setup(struct ksz_sw *sw)
+{
+	uint n;
+	uint port;
+	u16 val;
+	u32 data;
+	struct ksz_port_cfg *cfg;
+
+	/* Starting from stopped state will flush the ACL table. */
+	sw_cfg(sw, REG_SW_OPERATION, SW_START, 1);
+
+	sw->port_intr_mask = sw->PORT_INTR_MASK;
+	sw->intr_mask = SWITCH_INT_MASK;
+	sw->lue_intr_mask = LEARN_FAIL_INT | WRITE_FAIL_INT;
+	sw_set_global_ctrl(sw);
+	sw_cfg(sw, REG_SW_LUE_CTRL_0, SW_RESV_MCAST_ENABLE, 1);
+#if 0
+	sw->reg->w16(sw, REG_SW_ISP_TPID__2, 0x88A8);
+#endif
+/*
+ * THa  2015/12/03
+ * The new chip does not require legal packet check to be disabled for the tail
+ * tagging to work, but it still counts packets as oversized.
+ */
+#if 0
+	if (!(sw->features & NEW_CAP))
+#endif
+		sw_cfg(sw, REG_SW_MAC_CTRL_1, SW_LEGAL_PACKET_DISABLE, 1);
+	if (sw->features & IS_9893) {
+		sw_cfg(sw, REG_SW_MAC_CTRL_0, SW_CHECK_LENGTH, 0);
+		sw->reg->w16(sw, REG_AVB_STRATEGY__2,
+			SW_SHAPING_CREDIT_ACCT |
+			SW_POLICING_CREDIT_ACCT);
+	}
+
+	for (n = 0; n <= sw->mib_port_cnt; n++) {
+		port = get_phy_port(sw, n);
+		cfg = get_port_cfg(sw, port);
+		cfg->intr_mask = 0;
+		port_cfg_back_pressure(sw, port, 1);
+		if (port < sw->phy_port_cnt)
+			port_cfg_force_flow_ctrl(sw, port, 0);
+
+		/* Enable ACL only when needed. */
+		if (sw->features & (AVB_SUPPORT | DLR_HW))
+			port_cfg_acl(sw, port, true);
+		cfg->intr_mask |= PORT_ACL_INT;
+		if (port == sw->HOST_PORT)
+			continue;
+
+#ifdef CONFIG_1588_PTP
+		cfg->intr_mask |= PORT_PTP_INT;
+#endif
+	}
+/*
+ * THa  2015/10/01
+ * Increasing wait time in this register avoids the PTP transmit problem.
+ */
+	dbg_msg("eee txq wait: %04x"NL,
+		sw->reg->r16(sw, REG_SW_EEE_TXQ_WAIT_TIME__2));
+	sw->reg->w16(sw, REG_SW_EEE_TXQ_WAIT_TIME__2, 0x0040);
+
+	for (n = 1; n <= sw->mib_port_cnt; n++) {
+		port = get_phy_port(sw, n);
+		if (port >= sw->phy_port_cnt)
+			continue;
+
+		/*
+		 * Switch actually cannot do auto-negotiation with old 10Mbit
+		 * hub.
+		 */
+		port_r16(sw, port, P_PHY_CTRL, &val);
+		val &= ~PORT_FULL_DUPLEX;
+		port_w16(sw, port, P_PHY_CTRL, val);
+		if (sw->features & IS_9893)
+			port_setup_9893(sw, port);
+		else
+			port_setup_eee(sw, port);
+
+		/* Disable EEE for now. */
+		port_mmd_read(sw, port, MMD_DEVICE_ID_EEE_ADV, MMD_EEE_ADV,
+			&val, 1);
+/*
+ * THa  2015/09/30
+ * EEE in gigabit causes PTP messages not to be sent immediately.
+ * Just advertise EEE in 100 causes link not to be established.
+ * EEE in 100 has too much PTP jitter.
+ */
+		val = 0;
+		port_mmd_write(sw, port, MMD_DEVICE_ID_EEE_ADV, MMD_EEE_ADV,
+			&val, 1);
+	}
+	for (n = 0; n <= sw->mib_port_cnt; n++) {
+		port = get_phy_port(sw, n);
+		if (port >= sw->phy_port_cnt)
+			continue;
+/*
+ * THa  2015/10/07
+ * The S2 chip has a bug that writing to the 0xN13E register will cause the
+ * 100Mbit link to be unstable, with blinking LED that can only transmit.
+ * Enabling EEE will recover this error condition.
+ */
+		/* Enable port PHY interrupt. */
+		sw->info->port_cfg[port].intr_mask |= PORT_PHY_INT;
+
+		val = LINK_DOWN_INT | LINK_UP_INT;
+
+#if 1
+/*
+ * THa  2014/06/25
+ * SPI cannot just read PHY interrupt status register to clear interrupts.
+ * No way to clear the interrupts so cannot enable them here.
+ */
+		if (!(sw->features & NEW_CAP))
+			val = 0;
+#endif
+		port_r32(sw, port, REG_PORT_PHY_INT_ENABLE & ~3, &data);
+		data &= 0xffff00ff;
+		data |= val << 8;
+		port_w32(sw, port, REG_PORT_PHY_INT_ENABLE & ~3, data);
+	}
+
+	/* Setup SGMII only when the port is enabled. */
+	port = 6;
+	if (PHY_INTERFACE_MODE_SGMII == sw->port_info[port].interface &&
+	    get_log_port(sw, port) <= sw->mib_port_cnt) {
+		bool pcs = true;
+		bool master = false;
+		bool autoneg = true;
+
+		/* Possibly connect directly. */
+		if (sw->sgmii_mode == 0) {
+			master = true;
+			autoneg = false;
+
+		/* Using fiber. */
+		} else if (sw->sgmii_mode == 1) {
+			pcs = false;
+			master = true;
+		}
+		port_sgmii_setup(sw, port, pcs, master, autoneg, 2, 1);
+		sw->info->port_cfg[port].intr_mask |= PORT_SGMII_INT;
+	}
+
+	sw_setup_broad_storm(sw);
+
+	sw_setup_prio(sw);
+
+	sw_setup_mirror(sw);
+
+	sw->info->multi_sys = MULTI_MAC_TABLE_ENTRIES;
+	sw->info->multi_net = SWITCH_MAC_TABLE_ENTRIES;
+
+	/* Unknown multicast forwarding will not be used. */
+	if (sw->features & AVB_SUPPORT)
+		sw_setup_multi(sw);
+#ifdef CONFIG_KSZ_STP
+	sw->ops->release(sw);
+	sw_setup_stp(sw);
+	sw->ops->acquire(sw);
+#endif
+#ifdef CONFIG_1588_PTP
+	if (sw->features & PTP_HW)
+		sw_setup_ptp(sw);
+#endif
+#ifdef CONFIG_KSZ_IBA
+	if ((sw->features & IBA_SUPPORT) && !sw->info->iba.use_iba)
+		sw_setup_iba(sw);
+#endif
+#ifdef CONFIG_KSZ_DLR
+	if (sw->features & DLR_HW)
+		sw_setup_dlr(sw);
+#endif
+#ifdef CONFIG_KSZ_HSR
+	if (sw->features & HSR_HW)
+		sw_setup_hsr(sw);
+#endif
+#if defined(CONFIG_KSZ_AVB) || defined(CONFIG_KSZ_MRP)
+	if ((sw->features & (AVB_SUPPORT | MRP_SUPPORT))) {
+		sw->ops->release(sw);
+		sw_setup_mrp(sw);
+		sw->ops->acquire(sw);
+	}
+#endif
+	sw_setup_acl(sw);
+}  /* sw_setup */
+
+static void sw_reset_hw(struct ksz_sw *sw)
+{
+	sw_cfg(sw, REG_SW_OPERATION, SW_RESET, 1);
+	delay_micro(1);
+
+	/* Turn off SPI auto edge detection. */
+	sw->reg->w8(sw, REG_SW_GLOBAL_SERIAL_CTRL_0, 0x40);
+}  /* sw_reset_hw */
+
+static void sw_reset(struct ksz_sw *sw)
+{
+	uint p;
+	u8 byte_before;
+	u8 byte_after;
+
+	sw->overrides &= ~VLAN_SET;
+	sw_reset_setup(sw);
+
+	sw_reset_hw(sw);
+
+#if 1
+	if (sw->overrides & USE_802_1X_AUTH)
+		sw_cfg(sw, REG_SW_OPERATION, SW_START, 0);
+#endif
+
+/*
+ * THa  2016/10/03
+ * SGMII registers are not reset by hardware reset.
+ */
+	p = 6;
+	if (PHY_INTERFACE_MODE_SGMII == sw->port_info[p].interface) {
+		u16 ctrl;
+
+		port_sgmii_r(sw, p, SR_MII, MMD_SR_MII_CTRL, &ctrl, 1);
+		ctrl |= SR_MII_RESET;
+		port_sgmii_w(sw, p, SR_MII, MMD_SR_MII_CTRL, &ctrl, 1);
+	}
+	for (p = sw->phy_port_cnt; p < sw->port_cnt; p++) {
+		port_r(sw, p, REG_PORT_XMII_CTRL_1, &byte_before);
+		byte_after = byte_before ^(PORT_MII_NOT_1GBIT |
+			PORT_MII_MAC_MODE | PORT_MII_SEL_M);
+		port_w(sw, p, REG_PORT_XMII_CTRL_1, byte_after);
+		port_w(sw, p, REG_PORT_XMII_CTRL_1, byte_before);
+		port_w16(sw, p, REG_PORT_XMII_CTRL_0,
+			sw->cached.xmii[p - sw->phy_port_cnt]);
+	}
+
+#ifdef CONFIG_KSZ_IBA
+	do {
+		u32 iba;
+
+		iba = sw->reg->r32(sw, REG_SW_IBA__4);
+
+		/* IBA is strapped to enable. */
+		if (iba & SW_IBA_ENABLE) {
+			iba &= ~(SW_IBA_PORT_M << SW_IBA_PORT_S);
+			iba |= sw->HOST_PORT << SW_IBA_PORT_S;
+			sw->reg->w32(sw, REG_SW_IBA__4, iba);
+		}
+	} while (0);
+#endif
+
+	sw_reset_acl(sw);
+	sw->overrides &= ~TAIL_TAGGING;
+	sw->overrides &= ~PTP_TAG;
+	sw->overrides &= ~TAG_REMOVE;
+	sw_dis_intr(sw);
+}  /* sw_reset */
+
+static void sw_chk_regs(struct ksz_sw *sw, u32 addr, u8 *val, size_t txl)
+{
+	int i;
+	u32 port_reg;
+
+	port_reg = REG_PTP_MSG_CONF1 + 1;
+	if contain_reg(addr, txl, port_reg) {
+		i = port_reg - addr;
+		if (val[i] & PTP_ENABLE)
+			sw->overrides |= PTP_TAG;
+		else
+			sw->overrides &= ~PTP_TAG;
+
+/* KSZ9567 S1 needs to have PTP tag all the time. */
+#if 1
+		if (!(sw->features & NEW_CAP) &&
+		    sw->TAIL_TAG_LOOKUP >= 0x100)
+			sw->overrides |= PTP_TAG;
+#endif
+	}
+	port_reg = PORT_CTRL_ADDR(sw->HOST_PORT, REG_PORT_CTRL_0);
+	if contain_reg(addr, txl, port_reg) {
+		i = port_reg - addr;
+		if (val[i] & PORT_TAIL_TAG_ENABLE)
+			sw->overrides |= TAIL_TAGGING;
+		else
+			sw->overrides &= ~TAIL_TAGGING;
+	}
+}  /* sw_chk_regs */
+
+#define KSZSW_REGS_SIZE			0x8000
+
+#if !defined(CONFIG_KSZ_IBA_ONLY)
+static struct sw_regs {
+	int start;
+	int end;
+} sw_regs_range[] = {
+	{ 0x0000, 0x7FFF },
+	{ 0, 0 }
+};
+
+static int check_sw_reg_range(unsigned addr)
+{
+	struct sw_regs *range = sw_regs_range;
+
+	while (range->end > range->start) {
+		if (range->start <= addr && addr < range->end)
+			return true;
+		range++;
+	}
+	return false;
+}
+#endif
+
+/* -------------------------------------------------------------------------- */
+
+#ifdef KSZSW_REGS_SIZE
+static struct ksz_sw *get_sw_data(struct device *d)
+{
+	struct sw_priv *hw_priv = dev_get_drvdata(d);
+
+	return &hw_priv->sw;
+}
+
+static ssize_t kszsw_registers_read(struct file *filp, struct kobject *kobj,
+	struct bin_attribute *bin_attr, char *buf, loff_t off, size_t count)
+{
+	size_t i;
+	unsigned reg;
+	struct device *dev;
+	struct ksz_sw *sw;
+
+	if (unlikely(off > KSZSW_REGS_SIZE))
+		return 0;
+
+	if ((off + count) > KSZSW_REGS_SIZE)
+		count = KSZSW_REGS_SIZE - off;
+
+	if (unlikely(!count))
+		return count;
+
+	dev = container_of(kobj, struct device, kobj);
+	sw = get_sw_data(dev);
+
+	reg = off;
+	sw->ops->acquire(sw);
+	i = sw->reg->get(sw, reg, count, buf);
+	sw->ops->release(sw);
+
+	/* Linux 5.10 return -1 to application if sizes do not match. */
+	if (i > count)
+		i = count;
+	return i;
+}
+
+static ssize_t kszsw_registers_write(struct file *filp, struct kobject *kobj,
+	struct bin_attribute *bin_attr, char *buf, loff_t off, size_t count)
+{
+	size_t i;
+	unsigned reg;
+	unsigned phy_reg;
+	struct device *dev;
+	struct ksz_sw *sw;
+
+	if (unlikely(off >= KSZSW_REGS_SIZE))
+		return -EFBIG;
+
+	if ((off + count) > KSZSW_REGS_SIZE)
+		count = KSZSW_REGS_SIZE - off;
+
+	if (unlikely(!count))
+		return count;
+
+	dev = container_of(kobj, struct device, kobj);
+	sw = get_sw_data(dev);
+
+	reg = off;
+	phy_reg = reg & 0x1FF;
+	sw->ops->acquire(sw);
+	if ((reg & 0xF000) && (0x120 <= phy_reg && phy_reg <= 0x13F) &&
+	    ((reg & 3) || (count & 3))) {
+		char *tmp;
+		size_t cnt = (count + 3) & ~3;
+		int start = phy_reg & 3;
+
+#ifdef CONFIG_KSZ_IBA
+		if (sw->info->iba.use_iba)
+			start = 4 - ((phy_reg + count) & 3);
+#endif
+		tmp = kzalloc(cnt, GFP_KERNEL);
+		if (!tmp) {
+			i = 0;
+			goto write_done;
+		}
+		reg &= ~3;
+		i = sw->reg->get(sw, reg, cnt, tmp);
+		memcpy(&tmp[start], buf, count);
+		i = sw->reg->set(sw, reg, cnt, tmp);
+		kfree(tmp);
+	} else
+		i = sw->reg->set(sw, reg, count, buf);
+write_done:
+	sw->ops->release(sw);
+	return i;
+}
+
+static struct bin_attribute kszsw_registers_attr = {
+	.attr = {
+		.name	= "registers",
+		.mode	= S_IRUSR | S_IWUSR,
+	},
+	.size	= KSZSW_REGS_SIZE,
+	.read	= kszsw_registers_read,
+	.write	= kszsw_registers_write,
+};
+#endif
+
+#if !defined(CONFIG_KSZ_IBA_ONLY)
+static int sw_chk_reg(struct ksz_sw *sw, u32 reg, size_t count)
+{
+	size_t i;
+
+	for (i = 0; i < count; i += SW_SIZE, reg += SW_SIZE) {
+		if (!check_sw_reg_range(reg))
+			return false;
+	}
+	return true;
+}
+
+static int sw_reg_get(struct ksz_sw *sw, u32 reg, size_t count, void *buf)
+{
+	size_t i;
+	SW_D *addr;
+
+	addr = (SW_D *) buf;
+	if (sw_chk_reg(sw, reg, count)) {
+		sw->reg->r(sw, reg, buf, count);
+		return count;
+	}
+	for (i = 0; i < count; i += SW_SIZE, reg += SW_SIZE, addr++) {
+		*addr = 0;
+		if (check_sw_reg_range(reg))
+			*addr = SW_R(sw, reg);
+	}
+	return i;
+}
+
+static int sw_reg_set(struct ksz_sw *sw, u32 reg, size_t count, void *buf)
+{
+	size_t i;
+	SW_D *addr;
+
+	addr = (SW_D *) buf;
+	if (sw_chk_reg(sw, reg, count)) {
+		sw->reg->w(sw, reg, buf, count);
+		sw_chk_regs(sw, reg, buf, count);
+		return count;
+	}
+	for (i = 0; i < count; i += SW_SIZE, reg += SW_SIZE, addr++) {
+		if (check_sw_reg_range(reg))
+			SW_W(sw, reg, *addr);
+	}
+	return i;
+}
+#endif
+
+#ifdef CONFIG_KSZ_IBA
+/**
+ * sw_set_spi - use SPI for access
+ * @sw:		The switch instance.
+ * @iba:	The IBA instance.
+ *
+ * This routine uses default hardware access like SPI for register access.
+ */
+static void sw_set_spi(struct ksz_sw *sw, struct ksz_iba_info *iba)
+{
+	sw->reg = sw->old;
+#ifdef CONFIG_1588_PTP
+	if (sw->features & PTP_HW) {
+		struct ptp_info *ptp = &sw->ptp_hw;
+
+		ptp->reg = &ptp_reg_ops;
+	}
+#endif
+	iba->use_iba = IBA_USE_CODE_OFF;
+}  /* sw_set_spi */
+
+/**
+ * sw_set_ops - try to use SPI for access
+ * @sw:		The switch instance.
+ * @iba:	The IBA instance.
+ *
+ * This routine tries to use IBA for register access.
+ */
+static void sw_set_ops(struct work_struct *work)
+{
+	struct delayed_work *dwork = to_delayed_work(work);
+	struct ksz_sw *sw = container_of(dwork, struct ksz_sw, set_ops);
+	struct ksz_iba_info *iba = &sw->info->iba;
+#ifdef CONFIG_1588_PTP
+	struct ptp_info *ptp = NULL;
+#endif
+
+	if (sw->reg == &sw_iba_ops)
+		return;
+
+#ifdef CONFIG_KSZ_IBA
+	if (iba_stopped(sw))
+		return;
+#endif
+
+	/* Catch bug if triggered. */
+	if (!iba->dev || !netif_running(iba->dev)) {
+		dbg_msg("No IBA dev"NL);
+		return;
+	}
+	if (sw->net_ops->get_ready && !sw->net_ops->get_ready(iba->dev)) {
+		schedule_delayed_work(&sw->set_ops, 1);
+		return;
+	}
+	if (sw->HOST_PORT < sw->phy_port_cnt &&
+	    sw->port_info[sw->HOST_PORT].state != media_connected) {
+		schedule_delayed_work(&sw->set_ops, 1);
+		return;
+	}
+
+#ifdef CONFIG_1588_PTP
+	if (sw->features & PTP_HW)
+		ptp = &sw->ptp_hw;
+#endif
+	mutex_lock(&sw->lock);
+	mutex_lock(sw->hwlock);
+	mutex_lock(sw->reglock);
+	if (netif_running(iba->dev)) {
+#ifdef CONFIG_1588_PTP
+		if (ptp)
+			ptp->reg = &ptp_iba_ops;
+#endif
+		sw->reg = &sw_iba_ops;
+		iba->cnt = 0;
+		iba->use_iba = IBA_USE_CODE_ON;
+	}
+	if (iba->use_iba) {
+		u32 id;
+
+		sw->intr_using += 3;
+		iba->use_iba |= IBA_USE_CODE_TESTING;
+		id = sw->reg->r32(sw, REG_CHIP_ID0__1);
+		iba->use_iba &= ~IBA_USE_CODE_TESTING;
+dbg_msg("id = %08x"NL, id);
+#if 1
+/*
+ * THa  2016/01/03
+ * KSZ9563 S1 does not respond the very first time when using RGMII.
+ */
+		if (id == 0xdeadbeaf && !(sw->features & NEW_CAP)) {
+			id = sw->reg->r32(sw, REG_CHIP_ID0__1);
+dbg_msg("id = %08x"NL, id);
+		}
+#endif
+		sw->intr_using -= 3;
+		if (id == 0xdeadbeaf)
+			sw_set_spi(sw, iba);
+	}
+	mutex_unlock(sw->reglock);
+	mutex_unlock(sw->hwlock);
+	mutex_unlock(&sw->lock);
+
+	if (!iba->use_iba)
+		return;
+printk(KERN_INFO "Using IBA"NL);
+#if 1
+/*
+ * THa  2014/06/25
+ * SPI cannot just read PHY interrupt status register to clear interrupts.
+ *
+ * THa  2015/09/17
+ * The S2 chip has a strange bug that if PHY register 0x0136 is accessed last,
+ * either read or write, the global port interrupt status 0x0018 and individual
+ * port status register 0x001B do not indicate PHY interrupt even though the
+ * actual interrupt is triggered by plugging in or out the cable.
+ */
+	if (!(sw->features & NEW_CAP)) {
+		uint port;
+		u8 val = LINK_DOWN_INT | LINK_UP_INT;
+
+		sw->ops->acquire(sw);
+		for (port = 0; port < sw->phy_port_cnt; port++) {
+			port_w8(sw, port, REG_PORT_PHY_INT_ENABLE, val);
+		}
+		sw->ops->release(sw);
+	}
+#endif
+}  /* sw_set_ops */
+
+/**
+ * sw_set_dev - try to use SPI for access
+ * @sw:		The switch instance.
+ * @dev:	The network device.
+ * @mac_addr:	The MAC address used.
+ *
+ * This routine setup the IBA with the network device and its MAC address if
+ * the device exists.  Otherwise it uses the default access like SPI.
+ */
+static void sw_set_dev(struct ksz_sw *sw, struct net_device *dev, u8 *mac_addr)
+{
+	struct ksz_iba_info *iba = &sw->info->iba;
+	int delay_tick = msecs_to_jiffies(20);
+
+	if (!iba->use_iba && sw->HOST_PORT < sw->phy_port_cnt)
+		delay_tick = msecs_to_jiffies(100);
+	if (!dev) {
+		if (sw->reg != sw->old) {
+			mutex_lock(&sw->lock);
+			mutex_lock(sw->hwlock);
+			mutex_lock(sw->reglock);
+			sw_set_spi(sw, iba);
+			mutex_unlock(sw->reglock);
+			mutex_unlock(sw->hwlock);
+			mutex_unlock(&sw->lock);
+		}
+	} else if (sw->features & IBA_SUPPORT)
+		schedule_delayed_work(&sw->set_ops, delay_tick);
+	mutex_lock(sw->hwlock);
+	iba->dev = dev;
+#ifdef CONFIG_KSZ_IBA_ONLY
+	prepare_iba(iba, mac_addr, mac_addr);
+#else
+	prepare_iba(iba, iba->dst, mac_addr);
+#endif
+	mutex_unlock(sw->hwlock);
+}  /* sw_set_dev */
+#endif
+
+#ifdef CONFIG_KSZ_MRP
+static void sw_set_mrp(struct work_struct *work)
+{
+	struct delayed_work *dwork = to_delayed_work(work);
+	struct ksz_sw *sw = container_of(dwork, struct ksz_sw, set_mrp);
+
+#ifdef CONFIG_KSZ_IBA
+	if (iba_stopped(sw))
+		return;
+#endif
+
+	if (sw->HOST_PORT < sw->phy_port_cnt &&
+	    !netif_carrier_ok(sw->main_dev)) {
+		schedule_delayed_work(&sw->set_mrp, msecs_to_jiffies(500));
+		return;
+	}
+
+	mrp_start(&sw->mrp);
+}  /* sw_set_mrp */
+#endif
+
+/* -------------------------------------------------------------------------- */
+
+/*
+ * Microchip LinkMD routines
+ */
+
+enum {
+	CABLE_UNKNOWN,
+	CABLE_GOOD,
+	CABLE_CROSSED,
+	CABLE_REVERSED,
+	CABLE_CROSSED_REVERSED,
+	CABLE_OPEN,
+	CABLE_SHORT
+};
+
+#define STATUS_FULL_DUPLEX		0x01
+#define STATUS_CROSSOVER		0x02
+#define STATUS_REVERSED			0x04
+
+#define LINK_10MBPS_FULL		0x00000001
+#define LINK_10MBPS_HALF		0x00000002
+#define LINK_100MBPS_FULL		0x00000004
+#define LINK_100MBPS_HALF		0x00000008
+#define LINK_1GBPS_FULL			0x00000010
+#define LINK_1GBPS_HALF			0x00000020
+#define LINK_10GBPS_FULL		0x00000040
+#define LINK_10GBPS_HALF		0x00000080
+#define LINK_SYM_PAUSE			0x00000100
+#define LINK_ASYM_PAUSE			0x00000200
+
+#define LINK_AUTO_MDIX			0x00010000
+#define LINK_MDIX			0x00020000
+#define LINK_AUTO_POLARITY		0x00040000
+
+#define CABLE_LEN_MAXIMUM		15000
+#define CABLE_LEN_MULTIPLIER		8
+
+#define PHY_RESET_TIMEOUT		10
+
+/**
+ * sw_get_link_md -
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ *
+ * This routine is used to get the LinkMD status.
+ */
+static void sw_get_link_md(struct ksz_sw *sw, uint port)
+{
+	u16 ctrl;
+	u16 data;
+	u16 giga;
+	u16 link;
+	u16 len;
+	int i;
+	int timeout;
+	struct ksz_port_info *port_info = get_port_info(sw, port);
+
+	port_r16(sw, port, P_LINK_STATUS, &link);
+
+	/* Read second time in case the status is not latched. */
+	if (!(link & PORT_LINK_STATUS))
+		port_r16(sw, port, P_LINK_STATUS, &link);
+	port_r16(sw, port, REG_PORT_PHY_DIGITAL_STATUS, &data);
+	port_info->status[0] = CABLE_UNKNOWN;
+	if (link & PORT_LINK_STATUS) {
+		int stat = 0;
+
+		port_info->status[0] = CABLE_GOOD;
+		port_info->length[0] = 1;
+		port_info->status[1] = CABLE_GOOD;
+		port_info->length[1] = 1;
+		port_info->status[2] = CABLE_GOOD;
+		port_info->length[2] = 1;
+		port_info->status[3] = CABLE_GOOD;
+		port_info->length[3] = 1;
+		port_info->status[4] = CABLE_GOOD;
+		port_info->length[4] = 1;
+
+		if (!(data & PORT_PHY_STAT_MDI))
+			stat |= STATUS_CROSSOVER;
+#if 0
+		if (data & PORT_REVERSED_POLARITY)
+			stat |= STATUS_REVERSED;
+#endif
+		if ((stat & (STATUS_CROSSOVER | STATUS_REVERSED)) ==
+				(STATUS_CROSSOVER | STATUS_REVERSED))
+			port_info->status[0] = CABLE_CROSSED_REVERSED;
+		else if ((stat & STATUS_CROSSOVER) == STATUS_CROSSOVER)
+			port_info->status[0] = CABLE_CROSSED;
+		else if ((stat & STATUS_REVERSED) == STATUS_REVERSED)
+			port_info->status[0] = CABLE_REVERSED;
+		return;
+	}
+
+	/* Put in 1000 Mbps mode. */
+	port_r16(sw, port, P_PHY_CTRL, &ctrl);
+	data = PORT_FULL_DUPLEX | PORT_SPEED_1000MBIT;
+	port_w16(sw, port, P_PHY_CTRL, data);
+	port_r16(sw, port, REG_PORT_PHY_1000_CTRL, &giga);
+	data = PORT_AUTO_NEG_MANUAL;
+	port_w16(sw, port, REG_PORT_PHY_1000_CTRL, data);
+
+	port_r16(sw, port, REG_PORT_PHY_LINK_MD, &data);
+
+	/* Disable transmitter. */
+	data |= PORT_TX_DISABLE;
+	port_w16(sw, port, REG_PORT_PHY_LINK_MD, data);
+
+	/* Wait at most 1 second.*/
+	delay_milli(100);
+
+	/* Enable transmitter. */
+	data &= ~PORT_TX_DISABLE;
+	port_w16(sw, port, REG_PORT_PHY_LINK_MD, data);
+
+	for (i = 1; i <= 4; i++) {
+
+		/* Start cable diagnostic test. */
+		data |= PORT_START_CABLE_DIAG;
+		data |= (i - 1) << PORT_CABLE_DIAG_PAIR_S;
+		port_w16(sw, port, REG_PORT_PHY_LINK_MD, data);
+		timeout = PHY_RESET_TIMEOUT;
+		do {
+			if (!--timeout)
+				break;
+			delay_milli(10);
+			port_r16(sw, port, REG_PORT_PHY_LINK_MD, &data);
+		} while ((data & PORT_START_CABLE_DIAG));
+
+		port_info->length[i] = 0;
+		port_info->status[i] = CABLE_UNKNOWN;
+
+		if (!(data & PORT_START_CABLE_DIAG)) {
+			len = data & PORT_CABLE_FAULT_COUNTER;
+			if (len >= 22)
+				len -= 22;
+			else
+				len = 0;
+			len *= CABLE_LEN_MULTIPLIER;
+			len += 5;
+			len /= 10;
+			port_info->length[i] = len;
+			data >>= PORT_CABLE_DIAG_RESULT_S;
+			data &= PORT_CABLE_DIAG_RESULT_M;
+			switch (data) {
+			case PORT_CABLE_STAT_NORMAL:
+				port_info->status[i] = CABLE_GOOD;
+				port_info->length[i] = 1;
+				break;
+			case PORT_CABLE_STAT_OPEN:
+				port_info->status[i] = CABLE_OPEN;
+				break;
+			case PORT_CABLE_STAT_SHORT:
+				port_info->status[i] = CABLE_SHORT;
+				break;
+			}
+		}
+	}
+
+	port_w16(sw, port, REG_PORT_PHY_1000_CTRL, giga);
+	port_w16(sw, port, P_PHY_CTRL, ctrl);
+	if (ctrl & PORT_AUTO_NEG_ENABLE) {
+		ctrl |= PORT_AUTO_NEG_RESTART;
+		port_w16(sw, port, P_NEG_RESTART_CTRL, ctrl);
+	}
+
+	port_info->length[0] = port_info->length[1];
+	port_info->status[0] = port_info->status[1];
+	for (i = 2; i < 5; i++) {
+		if (CABLE_GOOD == port_info->status[0]) {
+			if (port_info->status[i] != CABLE_GOOD) {
+				port_info->status[0] = port_info->status[i];
+				port_info->length[0] = port_info->length[i];
+				break;
+			}
+		}
+	}
+}
+
+/* -------------------------------------------------------------------------- */
+
+static void get_sw_mib_counters(struct ksz_sw *sw, int first, int cnt,
+	u64 *counter)
+{
+	int i;
+	int mib;
+	uint n;
+	uint p;
+	struct ksz_port_mib *port_mib;
+
+	memset(counter, 0, sizeof(u64) * TOTAL_SWITCH_COUNTER_NUM);
+	for (i = 0, n = first; i < cnt; i++, n++) {
+		p = get_phy_port(sw, n);
+		port_mib = get_port_mib(sw, p);
+		for (mib = port_mib->mib_start; mib < sw->mib_cnt; mib++)
+			counter[mib] += port_mib->counter[mib];
+	}
+}
+
+static struct {
+	int rx;
+	int tx;
+} mib_display[TOTAL_SWITCH_COUNTER_NUM / 2] = {
+	{ MIB_RX_TOTAL, MIB_TX_TOTAL },
+	{ MIB_RX_HI_PRIO, MIB_TX_HI_PRIO },
+	{ MIB_RX_PAUSE, MIB_TX_PAUSE },
+	{ MIB_RX_BROADCAST, MIB_TX_BROADCAST },
+	{ MIB_RX_MULTICAST, MIB_TX_MULTICAST },
+	{ MIB_RX_UNICAST, MIB_TX_UNICAST },
+	{ MIB_RX_DROPS, MIB_TX_DROPS },
+	{ MIB_RX_OCTET_64, MIB_RX_OCTET_65_127 },
+	{ MIB_RX_OCTET_128_255, MIB_RX_OCTET_256_511 },
+	{ MIB_RX_OCTET_512_1023, MIB_RX_OCTET_1024_1522 },
+	{ MIB_RX_OCTET_1523_2000, MIB_RX_OCTET_2001 },
+	{ MIB_RX_UNDERSIZE, MIB_RX_OVERSIZE },
+	{ MIB_RX_FRAGMENT, MIB_RX_JABBER },
+	{ MIB_RX_SYMBOL_ERR, MIB_RX_CRC_ERR },
+	{ MIB_RX_ALIGNMENT_ERR, MIB_RX_CTRL_8808 },
+	{ MIB_TX_LATE_COLLISION, MIB_TX_DEFERRED },
+	{ MIB_TX_TOTAL_COLLISION, MIB_TX_EXCESS_COLLISION },
+	{ MIB_TX_SINGLE_COLLISION, MIB_TX_MULTI_COLLISION },
+};
+
+static int display_sw_mib_counters(struct ksz_sw *sw, int first, int cnt,
+	char *buf)
+{
+	int mib;
+	int n;
+	int len = 0;
+	u64 counter[TOTAL_SWITCH_COUNTER_NUM];
+
+	get_sw_mib_counters(sw, first, cnt, counter);
+	for (mib = 0; mib < TOTAL_SWITCH_COUNTER_NUM / 2; mib++) {
+		int rx = mib_display[mib].rx;
+		int tx = mib_display[mib].tx;
+		if (buf)
+			len += sprintf(buf + len,
+				"%s\t= %-20llu\t%s\t= %-20llu"NL,
+				mib_names[rx].string,
+				counter[rx],
+				mib_names[tx].string,
+				counter[tx]);
+		else
+			printk(KERN_INFO "%s\t= %-20llu\t%s\t= %-20llu"NL,
+				mib_names[rx].string,
+				counter[rx],
+				mib_names[tx].string,
+				counter[tx]);
+	}
+	for (n = 0, mib = first; n < cnt; n++, mib++) {
+		int j;
+		uint p = get_phy_port(sw, mib);
+		struct ksz_port_mib *port_mib = get_port_mib(sw, p);
+
+		for (j = 0; j < 2; j++) {
+			if (port_mib->rate[j].peak) {
+				u32 num;
+				u32 frac;
+
+				num = port_mib->rate[j].peak / 10;
+				frac = port_mib->rate[j].peak % 10;
+				if (buf)
+					len += sprintf(buf + len,
+						"%d:%d=%u.%u"NL, mib, j,
+						num, frac);
+				else
+					printk(KERN_INFO
+						"%d:%d=%u.%u"NL, mib, j,
+						num, frac);
+				port_mib->rate[j].peak = 0;
+			}
+		}
+	}
+	return len;
+}  /* display_sw_mib_counters */
+
+/* -------------------------------------------------------------------------- */
+
+static ssize_t display_sw_info(int cnt, char *buf, ssize_t len)
+{
+	len += sprintf(buf + len, "duplex:\t\t");
+	len += sprintf(buf + len, "0 for auto; ");
+	len += sprintf(buf + len,
+		"set to 1 for half-duplex; 2, full-duplex\n");
+	len += sprintf(buf + len, "speed:\t\t");
+	len += sprintf(buf + len,
+		"0 for auto; set to 10 or 100\n");
+	len += sprintf(buf + len, "force:\t\t");
+	len += sprintf(buf + len,
+		"set to 1 to force link to specific speed setting\n");
+	len += sprintf(buf + len, "flow_ctrl:\t");
+	len += sprintf(buf + len,
+		"set to 0 to disable flow control\n");
+	len += sprintf(buf + len, "mib:\t\t");
+	len += sprintf(buf + len,
+		"display the MIB table\n");
+	printk(KERN_INFO "%s", buf);
+	len = 0;
+
+	if (TOTAL_PORT_NUM != cnt)
+		return len;
+
+	len += sprintf(buf + len, "\ndynamic_table:\t");
+	len += sprintf(buf + len,
+		"display the switch's dynamic MAC table\n");
+	len += sprintf(buf + len, "static_table:\t");
+	len += sprintf(buf + len,
+		"display the switch's static MAC table\n");
+	len += sprintf(buf + len, "vlan_table:\t");
+	len += sprintf(buf + len,
+		"display the switch's VLAN table\n");
+
+	len += sprintf(buf + len, "\naging:\t\t");
+	len += sprintf(buf + len,
+		"disable/enable aging\n");
+	len += sprintf(buf + len, "fast_aging:\t");
+	len += sprintf(buf + len,
+		"disable/enable fast aging\n");
+	len += sprintf(buf + len, "link_aging:\t");
+	len += sprintf(buf + len,
+		"disable/enable link change auto aging\n");
+
+	len += sprintf(buf + len, "\nbcast_per:\t");
+	len += sprintf(buf + len,
+		"set broadcast storm percentage\n");
+	len += sprintf(buf + len, "mcast_storm:\t");
+	len += sprintf(buf + len,
+		"disable multicast storm protection\n");
+	len += sprintf(buf + len, "diffserv_map:\t");
+	len += sprintf(buf + len,
+		"set DiffServ value.  Use \"decimal=hexadecimal\" format\n");
+	len += sprintf(buf + len, "p_802_1p_map:\t");
+	len += sprintf(buf + len,
+		"set 802.1p value.  Use \"decimal=hexadecimal\" format\n");
+	printk(KERN_INFO "%s", buf);
+	len = 0;
+
+	len += sprintf(buf + len, "\nvlan:\t\t");
+	len += sprintf(buf + len,
+		"disable/enable 802.1Q VLAN\n");
+	len += sprintf(buf + len, "null_vid:\t");
+	len += sprintf(buf + len,
+		"set to 1 to replace null vid\n");
+	len += sprintf(buf + len, "macaddr:\t");
+	len += sprintf(buf + len,
+		"set switch MAC address\n");
+	len += sprintf(buf + len, "mirror_mode:\t");
+	len += sprintf(buf + len,
+		"set to 1 to use mirror rx AND tx mode\n");
+
+	len += sprintf(buf + len, "\nigmp_snoop:\t");
+	len += sprintf(buf + len,
+		"disable/enable IGMP snooping\n");
+	len += sprintf(buf + len, "ipv6_mld_snoop:\t");
+	len += sprintf(buf + len,
+		"disable/enable IPv6 MLD snooping\n");
+	len += sprintf(buf + len, "ipv6_mld_option:");
+	len += sprintf(buf + len,
+		"disable/enable IPv6 MLD option snooping\n");
+
+	len += sprintf(buf + len, "\naggr_backoff:\t");
+	len += sprintf(buf + len,
+		"disable/enable aggressive backoff in half-duplex mode\n");
+	len += sprintf(buf + len, "no_exc_drop:\t");
+	len += sprintf(buf + len,
+		"disable/enable no excessive collision drop\n");
+	len += sprintf(buf + len, "buf_reserve:\t");
+	len += sprintf(buf + len,
+		"disable/enable buffer reserve\n");
+	printk(KERN_INFO "%s", buf);
+	len = 0;
+
+	len += sprintf(buf + len, "\nhuge_packet:\t");
+	len += sprintf(buf + len,
+		"disable/enable huge packet support\n");
+	len += sprintf(buf + len, "legal_packet:\t");
+	len += sprintf(buf + len,
+		"disable/enable legal packet\n");
+	len += sprintf(buf + len, "length_check:\t");
+	len += sprintf(buf + len,
+		"disable/enable legal packet length check\n");
+
+	len += sprintf(buf + len, "\nback_pressure:\t");
+	len += sprintf(buf + len,
+		"set back pressure mode\n");
+	len += sprintf(buf + len, "sw_flow_ctrl:\t");
+	len += sprintf(buf + len,
+		"disable/enable switch host port flow control\n");
+	len += sprintf(buf + len, "sw_half_duplex:\t");
+	len += sprintf(buf + len,
+		"disable/enable switch host port half-duplex mode\n");
+#ifdef SWITCH_10_MBIT
+	len += sprintf(buf + len, "sw_10_mbit:\t");
+	len += sprintf(buf + len,
+		"disable/enable switch host port 10Mbit mode\n");
+#endif
+	len += sprintf(buf + len, "rx_flow_ctrl:\t");
+	len += sprintf(buf + len,
+		"disable/enable receive flow control\n");
+	len += sprintf(buf + len, "tx_flow_ctrl:\t");
+	len += sprintf(buf + len,
+		"disable/enable transmit flow control\n");
+	len += sprintf(buf + len, "fair_flow_ctrl:\t");
+	len += sprintf(buf + len,
+		"disable/enable fair flow control mode\n");
+	printk(KERN_INFO "%s", buf);
+	len = 0;
+
+	len += sprintf(buf + len, "vlan_bound:\t");
+	len += sprintf(buf + len,
+		"disable/enable unicast VLAN boundary\n");
+
+	len += sprintf(buf + len, "pass_pause:\t");
+	len += sprintf(buf + len,
+		"set to 1 to pass PAUSE frames for debugging\n");
+	printk(KERN_INFO "%s", buf);
+	len = 0;
+
+	len += sprintf(buf + len, "\nswitch port settings:\n");
+	len += sprintf(buf + len, "duplex:\t\t");
+	len += sprintf(buf + len,
+		"display the port's duplex setting\n");
+	len += sprintf(buf + len, "speed:\t\t");
+	len += sprintf(buf + len,
+		"display the port's link speed\n");
+	len += sprintf(buf + len, "linkmd:\t\t");
+	len += sprintf(buf + len,
+		"write to start LinkMD test.  read for result\n");
+	len += sprintf(buf + len, "mib:\t\t");
+	len += sprintf(buf + len,
+		"display the port's MIB table\n");
+	printk(KERN_INFO "%s", buf);
+	len = 0;
+
+	len += sprintf(buf + len, "vid:\t\t");
+	len += sprintf(buf + len,
+		"set default VID value\n");
+	len += sprintf(buf + len, "member:\t\t");
+	len += sprintf(buf + len,
+		"set VLAN membership\n");
+
+	len += sprintf(buf + len, "bcast_storm:\t");
+	len += sprintf(buf + len,
+		"disable/enable broadcast storm protection\n");
+	len += sprintf(buf + len, "diffserv:\t");
+	len += sprintf(buf + len,
+		"disable/enable DiffServ priority\n");
+	len += sprintf(buf + len, "p_802_1p:\t");
+	len += sprintf(buf + len,
+		"disable/enable 802.1p priority\n");
+
+	len += sprintf(buf + len, "port_based:\t");
+	len += sprintf(buf + len,
+		"disable/enable port-based priority\n");
+	printk(KERN_INFO "%s", buf);
+	len = 0;
+
+	len += sprintf(buf + len, "prio_queue:\t");
+	len += sprintf(buf + len,
+		"disable/enable priority queue\n");
+	len += sprintf(buf + len, "tx_p0_ctrl:\t");
+	len += sprintf(buf + len,
+		"set priority queue 0 control\n");
+	len += sprintf(buf + len, "tx_p1_ctrl:\t");
+	len += sprintf(buf + len,
+		"set priority queue 1 control\n");
+	len += sprintf(buf + len, "tx_p2_ctrl:\t");
+	len += sprintf(buf + len,
+		"set priority queue 2 control\n");
+	len += sprintf(buf + len, "tx_p3_ctrl:\t");
+	len += sprintf(buf + len,
+		"set priority queue 3 control\n");
+	len += sprintf(buf + len, "tx_p0_ratio:\t");
+	len += sprintf(buf + len,
+		"set priority queue 0 ratio\n");
+	len += sprintf(buf + len, "tx_p1_ratio:\t");
+	len += sprintf(buf + len,
+		"set priority queue 1 ratio\n");
+	len += sprintf(buf + len, "tx_p2_ratio:\t");
+	len += sprintf(buf + len,
+		"set priority queue 2 ratio\n");
+	len += sprintf(buf + len, "tx_p3_ratio:\t");
+	len += sprintf(buf + len,
+		"set priority queue 3 ratio\n");
+	len += sprintf(buf + len, "prio_rate:\t");
+	len += sprintf(buf + len,
+		"disable/enable priority queue rate limiting\n");
+	len += sprintf(buf + len, "rx_limit:\t");
+	len += sprintf(buf + len,
+		"set rx rate limiting mode\n");
+	len += sprintf(buf + len, "cnt_ifg:\t");
+	len += sprintf(buf + len,
+		"set to 1 to count IPG\n");
+	len += sprintf(buf + len, "cnt_pre:\t");
+	len += sprintf(buf + len,
+		"set to 1 to count preamble\n");
+	len += sprintf(buf + len, "rx_p0_rate:\t");
+	len += sprintf(buf + len,
+		"set rx priority queue 0 rate in 64Kbps unit\n");
+	len += sprintf(buf + len, "rx_p1_rate:\t");
+	len += sprintf(buf + len,
+		"set rx priority queue 1 rate in 64Kbps unit\n");
+	len += sprintf(buf + len, "rx_p2_rate:\t");
+	len += sprintf(buf + len,
+		"set rx priority queue 2 rate in 64Kbps unit\n");
+	len += sprintf(buf + len, "rx_p3_rate:\t");
+	len += sprintf(buf + len,
+		"set rx priority queue 3 rate in 64Kbps unit\n");
+	len += sprintf(buf + len, "tx_p0_rate:\t");
+	len += sprintf(buf + len,
+		"set tx priority queue 0 rate in 64Kbps unit\n");
+	len += sprintf(buf + len, "tx_p1_rate:\t");
+	len += sprintf(buf + len,
+		"set tx priority queue 1 rate in 64Kbps unit\n");
+	len += sprintf(buf + len, "tx_p2_rate:\t");
+	len += sprintf(buf + len,
+		"set tx priority queue 2 rate in 64Kbps unit\n");
+	len += sprintf(buf + len, "tx_p3_rate:\t");
+	len += sprintf(buf + len,
+		"set tx priority queue 3 rate in 64Kbps unit\n");
+	printk(KERN_INFO "%s", buf);
+	len = 0;
+
+	len += sprintf(buf + len, "\npass_all:\t");
+	len += sprintf(buf + len,
+		"set to 1 to pass all frames for debugging\n");
+	len += sprintf(buf + len, "tail_tag:\t");
+	len += sprintf(buf + len,
+		"disable/enable tail tagging\n");
+	len += sprintf(buf + len, "rx:\t\t");
+	len += sprintf(buf + len,
+		"disable/enable rx\n");
+	len += sprintf(buf + len, "tx:\t\t");
+	len += sprintf(buf + len,
+		"disable/enable tx\n");
+	len += sprintf(buf + len, "learn:\t\t");
+	len += sprintf(buf + len,
+		"disable/enable learning\n");
+
+	len += sprintf(buf + len, "mirror_port:\t");
+	len += sprintf(buf + len,
+		"disable/enable mirror port\n");
+	len += sprintf(buf + len, "mirror_rx:\t");
+	len += sprintf(buf + len,
+		"disable/enable mirror receive\n");
+	len += sprintf(buf + len, "mirror_tx:\t");
+	len += sprintf(buf + len,
+		"disable/enable mirror transmit\n");
+	printk(KERN_INFO "%s", buf);
+	len = 0;
+
+	len += sprintf(buf + len, "\nnon_vid:\t");
+	len += sprintf(buf + len,
+		"set to 1 to discard non-VID packets\n");
+	len += sprintf(buf + len, "ingress:\t");
+	len += sprintf(buf + len,
+		"disable/enable ingress VLAN filtering\n");
+	len += sprintf(buf + len, "drop_tagged:\t");
+	len += sprintf(buf + len,
+		"disable/enable drop tagged packet feature\n");
+	len += sprintf(buf + len, "replace_prio:\t");
+	len += sprintf(buf + len,
+		"disable/enable replace 802.1p priority feature\n");
+	len += sprintf(buf + len, "back_pressure:\t");
+	len += sprintf(buf + len,
+		"disable/enable back pressure in half-duplex mode\n");
+	len += sprintf(buf + len, "force_flow_ctrl:");
+	len += sprintf(buf + len,
+		"set to 1 to force flow control\n");
+	len += sprintf(buf + len, "\nmacaddr:\t");
+	len += sprintf(buf + len,
+		"set port MAC address\n");
+	printk(KERN_INFO "%s", buf);
+	len = 0;
+
+	len += sprintf(buf + len, "\nstatic MAC table:\n");
+	len += sprintf(buf + len, "addr:\t\t");
+	len += sprintf(buf + len,
+		"set MAC address\n");
+	len += sprintf(buf + len, "ports:\t\t");
+	len += sprintf(buf + len,
+		"set destination ports\n");
+	len += sprintf(buf + len, "override:\t");
+	len += sprintf(buf + len,
+		"set override bit\n");
+	len += sprintf(buf + len, "use_fid:\t");
+	len += sprintf(buf + len,
+		"set use FID bit\n");
+	len += sprintf(buf + len, "fid:\t\t");
+	len += sprintf(buf + len,
+		"set FID\n");
+	len += sprintf(buf + len, "valid:\t\t");
+	len += sprintf(buf + len,
+		"set valid bit and write to table\n");
+	len += sprintf(buf + len, "\nVLAN table:\n");
+	len += sprintf(buf + len, "vid:\t\t");
+	len += sprintf(buf + len,
+		"set VID\n");
+	len += sprintf(buf + len, "fid:\t\t");
+	len += sprintf(buf + len,
+		"set FID\n");
+	len += sprintf(buf + len, "member:\t\t");
+	len += sprintf(buf + len,
+		"set membership\n");
+	len += sprintf(buf + len, "valid:\t\t");
+	len += sprintf(buf + len,
+		"set valid bit and write to table\n");
+	printk(KERN_INFO "%s", buf);
+	len = 0;
+
+	return len;
+}  /* display_sw_info */
+
+static ssize_t sysfs_sw_read(struct ksz_sw *sw, int proc_num,
+	struct ksz_port *port, ssize_t len, char *buf)
+{
+	int i;
+	int j;
+	u16 map;
+	struct ksz_sw_info *info = sw->info;
+
+	switch (proc_num) {
+	case PROC_SW_INFO:
+		len = display_sw_info(TOTAL_PORT_NUM, buf, len);
+		break;
+	case PROC_SW_VERSION:
+		len += sprintf(buf + len, "%s  %s"NL,
+			SW_DRV_VERSION, SW_DRV_RELDATE);
+		break;
+	case PROC_SET_SW_DUPLEX:
+		if (!port)
+			break;
+		len += sprintf(buf + len, "%u; ", port->duplex);
+		if (media_connected == port->linked->state) {
+			if (1 == port->linked->duplex)
+				len += sprintf(buf + len, "half-duplex"NL);
+			else if (2 == port->linked->duplex)
+				len += sprintf(buf + len, "full-duplex"NL);
+		} else
+			len += sprintf(buf + len, "unlinked"NL);
+		break;
+	case PROC_SET_SW_SPEED:
+		if (!port)
+			break;
+		len += sprintf(buf + len, "%u; ", port->speed);
+		if (media_connected == port->linked->state)
+			len += sprintf(buf + len, "%u"NL,
+				port->linked->tx_rate / TX_RATE_UNIT);
+		else
+			len += sprintf(buf + len, "unlinked"NL);
+		break;
+	case PROC_SET_SW_FORCE:
+		if (!port)
+			break;
+		len += sprintf(buf + len, "%u"NL, port->force_link);
+		break;
+	case PROC_SET_SW_FLOW_CTRL:
+		if (!port)
+			break;
+		len += sprintf(buf + len, "%u; ", port->flow_ctrl);
+		switch (port->flow_ctrl) {
+		case PHY_FLOW_CTRL:
+			len += sprintf(buf + len, "flow control"NL);
+			break;
+		case PHY_TX_ONLY:
+			len += sprintf(buf + len, "tx only"NL);
+			break;
+		case PHY_RX_ONLY:
+			len += sprintf(buf + len, "rx only"NL);
+			break;
+		default:
+			len += sprintf(buf + len, "no flow control"NL);
+			break;
+		}
+		break;
+	case PROC_SET_SW_MIB:
+		if (!port)
+			break;
+		len += display_sw_mib_counters(sw, port->first_port,
+			port->mib_port_cnt, buf + len);
+		break;
+	case PROC_SET_BROADCAST_STORM:
+		len += sprintf(buf + len, "%u", info->broad_per);
+		if (sw->verbose)
+			len += sprintf(buf + len, PER_CHAR);
+		len += sprintf(buf + len, NL);
+		break;
+	case PROC_SET_DIFFSERV:
+		for (i = 0; i < DIFFSERV_ENTRIES / KS_PRIO_IN_REG; i++) {
+			len += sprintf(buf + len, "%2u=",
+				i * KS_PRIO_IN_REG);
+			map = info->diffserv[i];
+			for (j = 0; j < KS_PRIO_IN_REG; j++) {
+				len += sprintf(buf + len, "%u ",
+					map & KS_PRIO_M);
+				map >>= KS_PRIO_S;
+			}
+			len += sprintf(buf + len, "\t"SW_SIZE_STR""NL,
+				info->diffserv[i]);
+		}
+		break;
+	case PROC_SET_802_1P:
+		for (i = 0; i < PRIO_802_1P_ENTRIES / KS_PRIO_IN_REG; i++) {
+			len += sprintf(buf + len, "%u=",
+				i * KS_PRIO_IN_REG);
+			map = info->p_802_1p[i];
+			for (j = 0; j < KS_PRIO_IN_REG; j++) {
+				len += sprintf(buf + len, "%u ",
+					map & KS_PRIO_M);
+				map >>= KS_PRIO_S;
+			}
+			len += sprintf(buf + len, "\t"SW_SIZE_STR""NL,
+				info->p_802_1p[i]);
+		}
+		break;
+	case PROC_SET_SW_VID:
+		len += sprintf(buf + len, "0x%04x"NL, sw->vid);
+		break;
+	case PROC_GET_HOST_PORT:
+		len += sprintf(buf + len, "%u"NL, sw->HOST_PORT + 1);
+		break;
+	case PROC_GET_PORTS:
+	{
+		uint ports = sw->mib_port_cnt + 1;
+
+		if (sw->eth_cnt)
+			ports = sw->eth_maps[0].cnt + 1;
+		len += sprintf(buf + len, "%u"NL, ports);
+		break;
+	}
+	case PROC_GET_DEV_START:
+	{
+		int start = 0;
+
+		if (sw->dev_offset)
+			start = 100;
+		len += sprintf(buf + len, "%u"NL, start);
+		break;
+	}
+	case PROC_GET_PORT_START:
+	{
+		int start = 0;
+
+		if (sw->overrides & SYSFS_1_BASE)
+			start = 1;
+		len += sprintf(buf + len, "%u"NL, start);
+		break;
+	}
+	case PROC_GET_VLAN_START:
+	{
+		int start = 0;
+
+		if (sw->features & VLAN_PORT)
+			start = VLAN_PORT_START;
+		len += sprintf(buf + len, "%u"NL, start);
+		break;
+	}
+	case PROC_GET_AVB:
+		i = 0;
+		if (sw->features & AVB_SUPPORT) {
+			i = 2;
+			if (sw->features & MRP_SUPPORT)
+				i = 1;
+		}
+		len += sprintf(buf + len, "%u"NL, i);
+		break;
+	case PROC_GET_STP:
+		len += sprintf(buf + len, "0"NL);
+		break;
+	case PROC_GET_TWO_DEV:
+		i = 0;
+		if (2 == sw->dev_count && (sw->features & SW_VLAN_DEV))
+			i = 1;
+#ifdef CONFIG_KSZ_HSR
+		if ((sw->features & HSR_REDBOX) &&
+		    (sw->overrides & HSR_FORWARD))
+			i = 2;
+#endif
+		len += sprintf(buf + len, "%d"NL, i);
+		break;
+	case PROC_SET_SW_FEATURES:
+		len += sprintf(buf + len, "%08x:"NL, sw->features);
+		len += sprintf(buf + len, "\t%08lx = STP support"NL,
+			STP_SUPPORT);
+		len += sprintf(buf + len, "\t%08lx = VLAN port forwarding"NL,
+			VLAN_PORT);
+		len += sprintf(buf + len, "\t%08lx = VLAN port remove tag"NL,
+			VLAN_PORT_REMOVE_TAG);
+		len += sprintf(buf + len, "\t%08lx = VLAN port tag tailing"NL,
+			VLAN_PORT_TAGGING);
+		len += sprintf(buf + len, "\t%08lx = VLAN dev forwarding"NL,
+			SW_VLAN_DEV);
+		len += sprintf(buf + len, "\t%08lx = MRP support"NL,
+			MRP_SUPPORT);
+		len += sprintf(buf + len, "\t%08lx = Gigabit support"NL,
+			GIGABIT_SUPPORT);
+		len += sprintf(buf + len, "\t%08lx = IBA support"NL,
+			IBA_SUPPORT);
+		len += sprintf(buf + len, "\t%08lx = new capabilities"NL,
+			NEW_CAP);
+		len += sprintf(buf + len, "\t%08lx = AVB support"NL,
+			AVB_SUPPORT);
+		len += sprintf(buf + len, "\t%08lx = Redundancy support"NL,
+			REDUNDANCY_SUPPORT);
+#ifdef CONFIG_KSZ_DLR
+		len += sprintf(buf + len, "\t%08lx = DLR support"NL,
+			DLR_HW);
+#endif
+#ifdef CONFIG_KSZ_HSR
+		len += sprintf(buf + len, "\t%08lx = HSR support"NL,
+			HSR_HW);
+		len += sprintf(buf + len, "\t%08lx = HSR RedBox support"NL,
+			HSR_REDBOX);
+#endif
+		len += sprintf(buf + len, "\t%08lx = different MAC addresses"NL,
+			DIFF_MAC_ADDR);
+#ifdef CONFIG_1588_PTP
+		len += sprintf(buf + len, "\t%08lx = 1588 PTP"NL,
+			PTP_HW);
+#endif
+		break;
+	case PROC_SET_SW_OVERRIDES:
+		len += sprintf(buf + len, "%08x:"NL, sw->overrides);
+		len += sprintf(buf + len, "\t%08lx = flow control"NL,
+			PAUSE_FLOW_CTRL);
+		len += sprintf(buf + len, "\t%08lx = fast aging"NL,
+			FAST_AGING);
+		len += sprintf(buf + len, "\t%08lx = have >2 ports"NL,
+			HAVE_MORE_THAN_2_PORTS);
+#ifdef CONFIG_KSZ_HSR
+		len += sprintf(buf + len, "\t%08lx = HSR forward"NL,
+			HSR_FORWARD);
+#endif
+		len += sprintf(buf + len, "\t%08lx = unknown mcast blocked"NL,
+			UNK_MCAST_BLOCK);
+		len += sprintf(buf + len, "\t%08lx = update csum"NL,
+			UPDATE_CSUM);
+		len += sprintf(buf + len, "\t%08lx = delay update link"NL,
+			DELAY_UPDATE_LINK);
+#ifdef CONFIG_KSZ_IBA
+		len += sprintf(buf + len, "\t%08lx = IBA test"NL,
+			IBA_TEST);
+#endif
+		len += sprintf(buf + len, "\t%08lx = ACL intr monitor"NL,
+			ACL_INTR_MONITOR);
+		len += sprintf(buf + len, "\t%08lx = 802.1X Authentication"NL,
+			USE_802_1X_AUTH);
+		len += sprintf(buf + len, "\t%08lx = ptp tag"NL,
+			PTP_TAG);
+		len += sprintf(buf + len, "\t%08lx = tag is removed"NL,
+			TAG_REMOVE);
+		len += sprintf(buf + len, "\t%08lx = tail tagging"NL,
+			TAIL_TAGGING);
+		break;
+	case PROC_SET_AUTHEN:
+		len += sprintf(buf + len, "%u"NL,
+			!!(sw->overrides & USE_802_1X_AUTH));
+		break;
+	case PROC_DYNAMIC:
+		len = sw_d_dyn_mac_table(sw, buf, len);
+		break;
+	case PROC_STATIC:
+		len = sw_d_sta_mac_table(sw, buf, len);
+		len = sw_d_mac_table(sw, buf, len);
+		break;
+	case PROC_VLAN:
+		len = sw_d_vlan_table(sw, buf, len);
+		break;
+	case PROC_HSR:
+#ifdef CONFIG_KSZ_HSR
+		if (sw->features & REDUNDANCY_SUPPORT)
+			len = sw_d_hsr_table(sw, buf, len);
+#endif
+		break;
+	}
+	return len;
+}  /* sysfs_sw_read */
+
+static ssize_t sysfs_sw_read_hw(struct ksz_sw *sw, int proc_num, ssize_t len,
+	char *buf)
+{
+	u8 data[8];
+	u32 val;
+	int chk = 0;
+	int type = SHOW_HELP_ON_OFF;
+	char note[40];
+
+	note[0] = '\0';
+	switch (proc_num) {
+	case PROC_SET_AGING:
+		chk = sw_chk(sw, REG_SW_LUE_CTRL_1, SW_AGING_ENABLE);
+		break;
+	case PROC_SET_FAST_AGING:
+		chk = sw_chk(sw, REG_SW_LUE_CTRL_1, SW_FAST_AGING);
+		break;
+	case PROC_SET_LINK_AGING:
+		chk = sw_chk(sw, S_LINK_AGING_CTRL, SW_LINK_AUTO_AGING);
+		break;
+	case PROC_SET_MULTICAST_STORM:
+		chk = !sw_chk(sw, REG_SW_MAC_CTRL_1, MULTICAST_STORM_DISABLE);
+		break;
+	case PROC_SET_TX_RATE_QUEUE_BASED:
+		chk = sw_chk(sw, REG_SW_MAC_CTRL_5,
+			SW_OUT_RATE_LIMIT_QUEUE_BASED);
+		break;
+	case PROC_ENABLE_VLAN:
+		chk = sw_chk(sw, REG_SW_LUE_CTRL_0, SW_VLAN_ENABLE);
+		break;
+	case PROC_SET_REPLACE_NULL_VID:
+		chk = sw_chk(sw, S_REPLACE_VID_CTRL, SW_REPLACE_VID);
+		break;
+	case PROC_SET_DROP_INVALID_VID:
+		chk = sw_chk(sw, REG_SW_LUE_CTRL_0, SW_DROP_INVALID_VID);
+		break;
+	case PROC_SET_MAC_ADDR:
+		sw_get_addr(sw, data);
+		len += sprintf(buf + len, "%02X:%02X:%02X:%02X:%02X:%02X"NL,
+			data[0], data[1], data[2], data[3], data[4], data[5]);
+		type = SHOW_HELP_NONE;
+		break;
+	case PROC_SET_MIRROR_MODE:
+		chk = sw_chk_mirror_rx_tx(sw);
+		if (sw->verbose) {
+			if (chk)
+				strcpy(note, " (rx and tx)");
+			else
+				strcpy(note, " (rx or tx)");
+		}
+		type = SHOW_HELP_SPECIAL;
+		break;
+	case PROC_SET_IGMP_SNOOP:
+		chk = sw_chk(sw, S_MIRROR_CTRL, SW_IGMP_SNOOP);
+		break;
+#ifdef SW_IPV6_MLD_SNOOP
+	case PROC_SET_IPV6_MLD_SNOOP:
+		chk = sw_chk(sw, S_MIRROR_CTRL, SW_IPV6_MLD_SNOOP);
+		break;
+	case PROC_SET_IPV6_MLD_OPTION:
+		chk = sw_chk(sw, S_MIRROR_CTRL, SW_IPV6_MLD_OPTION);
+		break;
+#endif
+	case PROC_SET_AGGR_BACKOFF:
+		chk = sw_chk(sw, REG_SW_MAC_CTRL_0, SW_AGGR_BACKOFF);
+		break;
+	case PROC_SET_NO_EXC_DROP:
+		chk = sw_chk(sw, REG_SW_MAC_CTRL_1, NO_EXC_COLLISION_DROP);
+		break;
+	case PROC_SET_VLAN_BOUNDARY:
+		val = sw->reg->r32(sw, REG_SW_QM_CTRL__4);
+		chk = !!(val & UNICAST_VLAN_BOUNDARY);
+		break;
+	case PROC_SET_DOUBLE_TAG:
+		chk = sw_chk(sw, REG_SW_OPERATION, SW_DOUBLE_TAG);
+		break;
+	case PROC_SET_ISP_TAG:
+		chk = sw->reg->r16(sw, REG_SW_ISP_TPID__2);
+		type = SHOW_HELP_HEX_4;
+		break;
+	case PROC_SET_HSR_TAG:
+		chk = sw->reg->r16(sw, REG_SW_HSR_TPID__2);
+		type = SHOW_HELP_HEX_4;
+		break;
+	case PROC_SET_HSR_REDBOX_ID:
+#ifdef CONFIG_KSZ_HSR
+		if (sw->features & HSR_HW) {
+			struct ksz_hsr_info *hsr = &sw->info->hsr;
+
+			chk = hsr->ops->get_redbox_id(hsr);
+		}
+#endif
+		type = SHOW_HELP_NUM;
+		break;
+	case PROC_SET_HSR_NET_ID:
+#ifdef CONFIG_KSZ_HSR
+		if (sw->features & HSR_HW) {
+			struct ksz_hsr_info *hsr = &sw->info->hsr;
+
+			chk = hsr->ops->get_net_id(hsr);
+		}
+#endif
+		type = SHOW_HELP_NUM;
+		break;
+	case PROC_SET_MTU:
+		chk = sw->reg->r16(sw, REG_SW_MTU__2);
+		sw->mtu = chk;
+		type = SHOW_HELP_NUM;
+		break;
+	case PROC_SET_FORWARD_UNKNOWN_UNICAST:
+		val = sw->reg->r32(sw, REG_SW_LUE_UNK_UCAST_CTRL__4);
+		chk = !!(val & SW_UNK_UCAST_ENABLE);
+		break;
+	case PROC_SET_UNKNOWN_UNICAST_PORTS:
+		val = sw->reg->r32(sw, REG_SW_LUE_UNK_UCAST_CTRL__4);
+		val &= ~SW_UNK_UCAST_ENABLE;
+		val = get_log_mask_from_phy(sw, val);
+		chk = val;
+		type = SHOW_HELP_HEX;
+		break;
+	case PROC_SET_FORWARD_UNKNOWN_MULTICAST:
+		val = sw->reg->r32(sw, REG_SW_LUE_UNK_MCAST_CTRL__4);
+		chk = !!(val & SW_UNK_MCAST_ENABLE);
+		break;
+	case PROC_SET_UNKNOWN_MULTICAST_PORTS:
+		val = sw->reg->r32(sw, REG_SW_LUE_UNK_MCAST_CTRL__4);
+		val &= ~SW_UNK_MCAST_ENABLE;
+		val = get_log_mask_from_phy(sw, val);
+		chk = val;
+		type = SHOW_HELP_HEX;
+		break;
+	case PROC_SET_FORWARD_UNKNOWN_VID:
+		val = sw->reg->r32(sw, REG_SW_LUE_UNK_VID_CTRL__4);
+		chk = !!(val & SW_UNK_VID_ENABLE);
+		break;
+	case PROC_SET_UNKNOWN_VID_PORTS:
+		val = sw->reg->r32(sw, REG_SW_LUE_UNK_VID_CTRL__4);
+		val &= ~SW_UNK_VID_ENABLE;
+		val = get_log_mask_from_phy(sw, val);
+		chk = val;
+		type = SHOW_HELP_HEX;
+		break;
+	case PROC_SET_JUMBO_PACKET:
+		chk = sw_chk(sw, REG_SW_MAC_CTRL_1, SW_JUMBO_PACKET);
+		break;
+	case PROC_SET_LEGAL_PACKET:
+		chk = !sw_chk(sw, REG_SW_MAC_CTRL_1, SW_LEGAL_PACKET_DISABLE);
+		break;
+	case PROC_SET_LENGTH_CHECK:
+		chk = sw_chk(sw, REG_SW_MAC_CTRL_0, SW_CHECK_LENGTH);
+		break;
+	case PROC_SET_BACK_PRESSURE_MODE:
+		chk = sw_chk(sw, REG_SW_MAC_CTRL_1, SW_BACK_PRESSURE);
+		break;
+	case PROC_SET_FAIR_FLOW_CTRL:
+		chk = sw_chk(sw, REG_SW_MAC_CTRL_1, FAIR_FLOW_CTRL);
+		break;
+	case PROC_SET_PASS_PAUSE:
+		chk = sw_chk(sw, REG_SW_MAC_CTRL_4, SW_PASS_PAUSE);
+		break;
+	case PROC_ENABLE_PME:
+		chk = sw_chk(sw, REG_SW_PME_CTRL, PME_ENABLE);
+		break;
+	case PROC_ENABLE_PME_POLARITY:
+		chk = sw_chk(sw, REG_SW_PME_CTRL, PME_POLARITY);
+		break;
+	case PROC_SET_NO_COLOR:
+		chk = sw_r_shift(sw, REG_SW_MRI_CTRL_8, SW_COLOR_M,
+			SW_NO_COLOR_S);
+		break;
+	case PROC_SET_COLOR_RED:
+		chk = sw_r_shift(sw, REG_SW_MRI_CTRL_8, SW_COLOR_M,
+			SW_RED_COLOR_S);
+		break;
+	case PROC_SET_COLOR_YELLOW:
+		chk = sw_r_shift(sw, REG_SW_MRI_CTRL_8, SW_COLOR_M,
+			SW_YELLOW_COLOR_S);
+		break;
+	case PROC_SET_COLOR_GREEN:
+		chk = sw_r_shift(sw, REG_SW_MRI_CTRL_8, SW_COLOR_M,
+			SW_GREEN_COLOR_S);
+		break;
+	case PROC_SET_VLAN_FILTERING_DYNAMIC:
+		chk = sw_chk(sw, REG_SW_LUE_CTRL_2, SW_EGRESS_VLAN_FILTER_DYN);
+		break;
+	case PROC_SET_VLAN_FILTERING_STATIC:
+		chk = sw_chk(sw, REG_SW_LUE_CTRL_2, SW_EGRESS_VLAN_FILTER_STA);
+		break;
+	default:
+		return len;
+	}
+	return sysfs_show(len, buf, type, chk, note, sw->verbose);
+}  /* sysfs_sw_read_hw */
+
+static int sysfs_sw_write(struct ksz_sw *sw, int proc_num,
+	struct ksz_port *port, int num, const char *buf)
+{
+	int changes;
+	int count;
+	unsigned int val;
+	u8 data[8];
+	int processed = true;
+
+	switch (proc_num) {
+	case PROC_SW_INFO:
+		sw_init(sw);
+		sw->verbose = !!num;
+		break;
+	case PROC_SET_SW_DUPLEX:
+		if (!port)
+			break;
+		if (num <= 2)
+			port->duplex = (u8) num;
+		break;
+	case PROC_SET_SW_SPEED:
+		if (!port)
+			break;
+		if (!(sw->features & GIGABIT_SUPPORT) && num == 1000)
+			break;
+		if (0 == num || 10 == num || 100 == num || 1000 == num)
+			port->speed = (u16) num;
+		break;
+	case PROC_SET_SW_FORCE:
+		if (!port)
+			break;
+		port->force_link = (u8) num;
+		if (port->force_link) {
+			port_force_link_speed(port);
+			sw->phy_intr = sw->PORT_MASK;
+			port_get_link_speed(port);
+			if (port->link_ports)
+				schedule_delayed_work(&port->link_update, 0);
+			sw->phy_intr = 0;
+		} else
+			port_set_link_speed(port);
+		break;
+	case PROC_SET_SW_FLOW_CTRL:
+		if (!port)
+			break;
+		if (num <= PHY_RX_ONLY)
+			port->flow_ctrl = (u8) num;
+		break;
+	case PROC_SET_SW_MIB:
+		if (num >= 1 && num <= 2) {
+			sw_freeze_mib(sw, num - 1);
+			break;
+		}
+		for (count = 0; count <= sw->mib_port_cnt; count++) {
+			uint p = get_phy_port(sw, count);
+			struct ksz_port_mib *mib = get_port_mib(sw, p);
+
+			memset((void *) mib->counter, 0, sizeof(u64) *
+				TOTAL_SWITCH_COUNTER_NUM);
+			mib->rate[0].last = mib->rate[1].last = 0;
+			mib->rate[0].last_cnt = mib->rate[1].last_cnt = 0;
+			mib->rate[0].peak = mib->rate[1].peak = 0;
+		}
+		break;
+	case PROC_SET_SW_REG:
+		count = sscanf(buf, "%x=%x", (unsigned int *) &num, &val);
+		if (1 == count)
+			printk(KERN_INFO SW_SIZE_STR""NL,
+				SW_R(sw, num));
+		else if (2 == count)
+			SW_W(sw, num, val);
+		if (2 == count) {
+			u32 addr = num;
+			u8 buf[4];
+
+			buf[0] = (u8)val;
+			sw_chk_regs(sw, addr, buf, 1);
+		}
+		break;
+	case PROC_SET_SW_VID:
+		sw->vid = num;
+		break;
+	case PROC_SET_SW_FEATURES:
+		if ('0' != buf[0] || 'x' != buf[1])
+			sscanf(buf, "%x", &num);
+		changes = sw->features ^ num;
+		sw->features = num;
+		break;
+	case PROC_SET_SW_OVERRIDES:
+		if ('0' != buf[0] || 'x' != buf[1])
+			sscanf(buf, "%x", &num);
+		sw->overrides = num;
+		break;
+	case PROC_SET_AUTHEN:
+		if (num)
+			sw->overrides |= USE_802_1X_AUTH;
+		else if (sw->overrides & USE_802_1X_AUTH) {
+			struct ksz_acl_table *acl;
+			int acl_on;
+			int acl_rule;
+			uint n;
+			uint port;
+
+			for (n = 1; n <= sw->mib_port_cnt; n++) {
+				port = get_phy_port(sw, n);
+				port_set_authen_mode(sw, port,
+					PORT_AUTHEN_PASS);
+				sw_cfg_port_base_vlan(sw, port, sw->PORT_MASK);
+
+				acl_on = port_chk_acl(sw, port);
+				if (!acl_on)
+					port_cfg_acl(sw, port, true);
+				acl_rule = 0;
+				acl = &sw->info->port_cfg[port].
+					acl_info[acl_rule];
+				acl->ruleset = 0;
+				sw->ops->release(sw);
+				sw_w_acl_ruleset(sw, port, acl_rule, acl);
+				sw->ops->acquire(sw);
+				acl_rule++;
+				acl++;
+				acl->ruleset = 0;
+				sw->ops->release(sw);
+				sw_w_acl_ruleset(sw, port, acl_rule, acl);
+				sw->ops->acquire(sw);
+				acl_rule++;
+				acl++;
+				acl->ruleset = 0;
+				sw->ops->release(sw);
+				sw_w_acl_ruleset(sw, port, acl_rule, acl);
+				sw->ops->acquire(sw);
+				if (!acl_on)
+					port_cfg_acl(sw, port, false);
+			}
+			sw->overrides &= ~USE_802_1X_AUTH;
+		}
+		break;
+	case PROC_DYNAMIC:
+		sw_flush_dyn_mac_table(sw, sw->port_cnt);
+		break;
+	case PROC_STATIC:
+		sw_clr_sta_mac_table(sw);
+		break;
+	case PROC_SET_AGING:
+		sw_cfg(sw, REG_SW_LUE_CTRL_1, SW_AGING_ENABLE, num);
+		break;
+	case PROC_SET_FAST_AGING:
+		sw_cfg(sw, REG_SW_LUE_CTRL_1, SW_FAST_AGING, num);
+		break;
+	case PROC_SET_LINK_AGING:
+		sw_cfg(sw, S_LINK_AGING_CTRL, SW_LINK_AUTO_AGING, num);
+		break;
+	case PROC_SET_BROADCAST_STORM:
+		hw_cfg_broad_storm(sw, num);
+		break;
+	case PROC_SET_MULTICAST_STORM:
+		sw_cfg(sw, REG_SW_MAC_CTRL_1, MULTICAST_STORM_DISABLE,
+			!num);
+		break;
+	case PROC_SET_TX_RATE_QUEUE_BASED:
+		sw_cfg(sw, REG_SW_MAC_CTRL_5, SW_OUT_RATE_LIMIT_QUEUE_BASED,
+			num);
+		break;
+	case PROC_SET_DIFFSERV:
+		count = sscanf(buf, "%d=%x", (unsigned int *) &num, &val);
+		if (2 == count)
+			hw_cfg_tos_prio(sw, (u8) num, (u16) val);
+		break;
+	case PROC_SET_802_1P:
+		count = sscanf(buf, "%d=%x", (unsigned int *) &num, &val);
+		if (2 == count)
+			hw_cfg_802_1p_prio(sw, (u8) num, (u16) val);
+		break;
+	case PROC_ENABLE_VLAN:
+		if (!num)
+			sw_dis_vlan(sw);
+		else
+			sw_ena_vlan(sw);
+		break;
+	case PROC_SET_REPLACE_NULL_VID:
+		sw_cfg(sw, S_REPLACE_VID_CTRL, SW_REPLACE_VID, num);
+		break;
+	case PROC_SET_DROP_INVALID_VID:
+		sw_cfg(sw, REG_SW_LUE_CTRL_0, SW_DROP_INVALID_VID, num);
+		break;
+	case PROC_SET_MAC_ADDR:
+	{
+		int i;
+		int n[6];
+
+		i = sscanf(buf, "%x:%x:%x:%x:%x:%x",
+			&n[0], &n[1], &n[2], &n[3], &n[4], &n[5]);
+		if (6 == i) {
+			for (i = 0; i < 6; i++)
+				data[i] = (u8) n[i];
+			sw_set_addr(sw, data);
+		}
+		break;
+	}
+	case PROC_SET_MIRROR_MODE:
+		sw_cfg_mirror_rx_tx(sw, num);
+		break;
+	case PROC_SET_IGMP_SNOOP:
+		sw_cfg(sw, S_MIRROR_CTRL, SW_IGMP_SNOOP, num);
+		break;
+#ifdef SW_IPV6_MLD_SNOOP
+	case PROC_SET_IPV6_MLD_SNOOP:
+		sw_cfg(sw, S_MIRROR_CTRL, SW_IPV6_MLD_SNOOP, num);
+		break;
+	case PROC_SET_IPV6_MLD_OPTION:
+		sw_cfg(sw, S_MIRROR_CTRL, SW_IPV6_MLD_OPTION, num);
+		break;
+#endif
+	case PROC_SET_AGGR_BACKOFF:
+		sw_cfg(sw, REG_SW_MAC_CTRL_0, SW_AGGR_BACKOFF, num);
+		break;
+	case PROC_SET_NO_EXC_DROP:
+		sw_cfg(sw, REG_SW_MAC_CTRL_1, NO_EXC_COLLISION_DROP, num);
+		break;
+	case PROC_SET_VLAN_BOUNDARY:
+		val = sw->reg->r32(sw, REG_SW_QM_CTRL__4);
+		if (num)
+			val |= UNICAST_VLAN_BOUNDARY;
+		else
+			val &= ~UNICAST_VLAN_BOUNDARY;
+		sw->reg->w32(sw, REG_SW_QM_CTRL__4, val);
+		break;
+	case PROC_SET_DOUBLE_TAG:
+		sw_cfg(sw, REG_SW_OPERATION, SW_DOUBLE_TAG, num);
+		break;
+	case PROC_SET_ISP_TAG:
+		sw->reg->w16(sw, REG_SW_ISP_TPID__2, (u16) num);
+		break;
+	case PROC_SET_HSR_TAG:
+		sw->reg->w16(sw, REG_SW_HSR_TPID__2, (u16) num);
+		break;
+	case PROC_SET_HSR_REDBOX_ID:
+#ifdef CONFIG_KSZ_HSR
+		if (sw->features & HSR_HW) {
+			struct ksz_hsr_info *hsr = &sw->info->hsr;
+
+			if (0 <= num && num <= 7)
+				hsr->ops->set_redbox_id(hsr, (u8) num);
+		}
+#endif
+		break;
+	case PROC_SET_HSR_NET_ID:
+#ifdef CONFIG_KSZ_HSR
+		if (sw->features & HSR_HW) {
+			struct ksz_hsr_info *hsr = &sw->info->hsr;
+
+			if (0 <= num && num <= 7)
+				hsr->ops->set_net_id(hsr, (u8) num);
+		}
+#endif
+		break;
+	case PROC_SET_MTU:
+		if (64 <= num && num <= 9000) {
+			sw->reg->w16(sw, REG_SW_MTU__2, (u16) num);
+			sw->mtu = num;
+		}
+		break;
+	case PROC_SET_FORWARD_UNKNOWN_UNICAST:
+		sw_cfg(sw, REG_SW_LUE_UNK_UCAST_CTRL__4,
+			SW_UNK_UCAST_ENABLE >> 24, num);
+		break;
+	case PROC_SET_UNKNOWN_UNICAST_PORTS:
+		num &= ~SW_UNK_UCAST_ENABLE;
+		val = sw->reg->r32(sw, REG_SW_LUE_UNK_UCAST_CTRL__4);
+		val &= SW_UNK_UCAST_ENABLE;
+		num = get_phy_mask_from_log(sw, num);
+		val |= num;
+		sw->reg->w32(sw, REG_SW_LUE_UNK_UCAST_CTRL__4, val);
+		break;
+	case PROC_SET_FORWARD_UNKNOWN_MULTICAST:
+		sw_cfg(sw, REG_SW_LUE_UNK_MCAST_CTRL__4,
+			SW_UNK_MCAST_ENABLE >> 24, num);
+		if (num) {
+			val = sw->reg->r32(sw, REG_SW_LUE_UNK_MCAST_CTRL__4);
+			val &= ~SW_UNK_MCAST_ENABLE;
+			if (val == sw->HOST_MASK)
+				sw->overrides |= UNK_MCAST_BLOCK;
+		} else {
+			sw->overrides &= ~UNK_MCAST_BLOCK;
+		}
+		break;
+	case PROC_SET_UNKNOWN_MULTICAST_PORTS:
+		num &= ~SW_UNK_MCAST_ENABLE;
+		val = sw->reg->r32(sw, REG_SW_LUE_UNK_MCAST_CTRL__4);
+		val &= SW_UNK_MCAST_ENABLE;
+		num = get_phy_mask_from_log(sw, num);
+		val |= num;
+		sw->reg->w32(sw, REG_SW_LUE_UNK_MCAST_CTRL__4, val);
+		break;
+	case PROC_SET_FORWARD_UNKNOWN_VID:
+		sw_cfg(sw, REG_SW_LUE_UNK_VID_CTRL__4,
+			SW_UNK_VID_ENABLE >> 24, num);
+		break;
+	case PROC_SET_UNKNOWN_VID_PORTS:
+		num &= ~SW_UNK_VID_ENABLE;
+		val = sw->reg->r32(sw, REG_SW_LUE_UNK_VID_CTRL__4);
+		val &= SW_UNK_VID_ENABLE;
+		num = get_phy_mask_from_log(sw, num);
+		val |= num;
+		sw->reg->w32(sw, REG_SW_LUE_UNK_VID_CTRL__4, val);
+		break;
+	case PROC_SET_JUMBO_PACKET:
+		sw_cfg(sw, REG_SW_MAC_CTRL_1, SW_JUMBO_PACKET, num);
+		break;
+	case PROC_SET_LEGAL_PACKET:
+		sw_cfg(sw, REG_SW_MAC_CTRL_1, SW_LEGAL_PACKET_DISABLE,
+			!num);
+		break;
+	case PROC_SET_LENGTH_CHECK:
+		sw_cfg(sw, REG_SW_MAC_CTRL_0, SW_CHECK_LENGTH, num);
+		break;
+	case PROC_SET_BACK_PRESSURE_MODE:
+		sw_cfg(sw, REG_SW_MAC_CTRL_1, SW_BACK_PRESSURE, num);
+		break;
+	case PROC_SET_FAIR_FLOW_CTRL:
+		sw_cfg(sw, REG_SW_MAC_CTRL_1, FAIR_FLOW_CTRL, num);
+		break;
+	case PROC_SET_PASS_PAUSE:
+		sw_cfg(sw, REG_SW_MAC_CTRL_4, SW_PASS_PAUSE, num);
+		break;
+	case PROC_ENABLE_PME:
+		sw_cfg(sw, REG_SW_PME_CTRL, PME_ENABLE, num);
+		break;
+	case PROC_ENABLE_PME_POLARITY:
+		sw_cfg(sw, REG_SW_PME_CTRL, PME_POLARITY, num);
+		break;
+	case PROC_SET_NO_COLOR:
+		sw_w_shift(sw, REG_SW_MRI_CTRL_8,
+			SW_COLOR_M, SW_NO_COLOR_S, num);
+		break;
+	case PROC_SET_COLOR_RED:
+		sw_w_shift(sw, REG_SW_MRI_CTRL_8,
+			SW_COLOR_M, SW_RED_COLOR_S, num);
+		break;
+	case PROC_SET_COLOR_YELLOW:
+		sw_w_shift(sw, REG_SW_MRI_CTRL_8,
+			SW_COLOR_M, SW_YELLOW_COLOR_S, num);
+		break;
+	case PROC_SET_COLOR_GREEN:
+		sw_w_shift(sw, REG_SW_MRI_CTRL_8,
+			SW_COLOR_M, SW_GREEN_COLOR_S, num);
+		break;
+	case PROC_SET_VLAN_FILTERING_DYNAMIC:
+		sw_cfg(sw, REG_SW_LUE_CTRL_2, SW_EGRESS_VLAN_FILTER_DYN, num);
+		break;
+	case PROC_SET_VLAN_FILTERING_STATIC:
+		sw_cfg(sw, REG_SW_LUE_CTRL_2, SW_EGRESS_VLAN_FILTER_STA, num);
+		break;
+	default:
+		processed = false;
+		break;
+	}
+	return processed;
+}  /* sysfs_sw_write */
+
+static ssize_t sysfs_port_read(struct ksz_sw *sw, int proc_num, uint n,
+	ssize_t len, char *buf)
+{
+	struct ksz_port_cfg *port_cfg;
+	struct ksz_port_info *port_info;
+	int i;
+	int j;
+	uint port;
+	u32 map;
+	int chk = 0;
+	int type = SHOW_HELP_NONE;
+	char note[40];
+
+	note[0] = '\0';
+	port = get_sysfs_port(sw, n);
+	port_cfg = get_port_cfg(sw, port);
+	port_info = get_port_info(sw, port);
+	switch (proc_num) {
+	case PROC_SET_PORT_DUPLEX:
+		if (media_connected == port_info->state) {
+			if (1 == port_info->duplex)
+				len += sprintf(buf + len, "half-duplex"NL);
+			else if (2 == port_info->duplex)
+				len += sprintf(buf + len, "full-duplex"NL);
+		} else
+			len += sprintf(buf + len, "unlinked"NL);
+		break;
+	case PROC_SET_PORT_SPEED:
+		if (media_connected == port_info->state)
+			len += sprintf(buf + len, "%u"NL,
+				port_info->tx_rate / TX_RATE_UNIT);
+		else
+			len += sprintf(buf + len, "unlinked"NL);
+		break;
+	case PROC_SET_MAC_OPERATIONAL:
+		chk = !!(sw->dev_ports & (1 << port));
+		type = SHOW_HELP_ON_OFF;
+		break;
+	case PROC_SET_VLAN_RESTRICTED:
+		chk = port_cfg->restricted;
+		if (sw->verbose)
+			strcpy(note, " (0 = normal, 1 = restricted)");
+		type = SHOW_HELP_SPECIAL;
+		break;
+	case PROC_SET_VLAN_UNTAGGED:
+	{
+		int index;
+		int bit;
+
+		index = sw->vlan_index / VID_IN_DATA;
+		bit = sw->vlan_index % VID_IN_DATA;
+		chk = !!(port_cfg->untagged[index] & (1 << bit));
+		if (sw->verbose)
+			strcpy(note, " (0 = tagged, 1 = untagged)");
+		type = SHOW_HELP_SPECIAL;
+		break;
+	}
+	case PROC_SET_PORT_MIB:
+		port = get_log_port(sw, port);
+		len += display_sw_mib_counters(sw, port, 1, buf + len);
+		break;
+	case PROC_SET_LINK_MD:
+		len += sprintf(buf + len, "%u:%u %u:%u %u:%u %u:%u %u:%u"NL,
+			port_info->length[0], port_info->status[0],
+			port_info->length[1], port_info->status[1],
+			port_info->length[2], port_info->status[2],
+			port_info->length[3], port_info->status[3],
+			port_info->length[4], port_info->status[4]);
+		if (sw->verbose)
+			len += sprintf(buf + len,
+				"(%d=unknown; %d=normal; %d=open; %d=short)"NL,
+				CABLE_UNKNOWN, CABLE_GOOD, CABLE_OPEN,
+				CABLE_SHORT);
+		break;
+	case PROC_SET_SQI:
+		chk = port_info->sqi;
+		if (sw->verbose)
+			strcpy(note, " (0 = worst, 15 = best)");
+		type = SHOW_HELP_SPECIAL;
+		break;
+	case PROC_SET_PORT_BASED:
+		chk = port_cfg->port_prio;
+		type = SHOW_HELP_NUM;
+		break;
+	case PROC_SET_DEF_VID:
+		chk = port_cfg->vid;
+		type = SHOW_HELP_HEX_4;
+		break;
+	case PROC_SET_MEMBER:
+		chk = port_cfg->member;
+		chk = get_log_mask_from_phy(sw, chk);
+		type = SHOW_HELP_HEX_2;
+		break;
+	case PROC_SET_LIMIT:
+		chk = ((port_cfg->rate_limit >> PORT_IN_LIMIT_MODE_S) &
+			PORT_IN_LIMIT_MODE_M);
+		if (sw->verbose) {
+			switch (chk) {
+			case 1:
+				strcpy(note, " (flooded unicast)");
+				break;
+			case 2:
+				strcpy(note, " (multicast)");
+				break;
+			case 3:
+				strcpy(note, " (broadcast)");
+				break;
+			default:
+				strcpy(note, " (all)");
+			}
+		}
+		type = SHOW_HELP_SPECIAL;
+		break;
+	case PROC_SET_LIMIT_PORT_BASED:
+		chk = ((port_cfg->rate_limit >> PORT_IN_PORT_BASED_S) & 1);
+		type = SHOW_HELP_ON_OFF;
+		break;
+	case PROC_SET_LIMIT_PACKET_BASED:
+		chk = ((port_cfg->rate_limit >> PORT_RATE_PACKET_BASED_S) & 1);
+		type = SHOW_HELP_ON_OFF;
+		break;
+	case PROC_SET_LIMIT_FLOW_CTRL:
+		chk = ((port_cfg->rate_limit >> PORT_IN_FLOW_CTRL_S) & 1);
+		type = SHOW_HELP_ON_OFF;
+		break;
+	case PROC_SET_LIMIT_CNT_IFG:
+		chk = ((port_cfg->rate_limit >> PORT_COUNT_IFG_S) & 1);
+		type = SHOW_HELP_ON_OFF;
+		break;
+	case PROC_SET_LIMIT_CNT_PRE:
+		chk = ((port_cfg->rate_limit >> PORT_COUNT_PREAMBLE_S) & 1);
+		type = SHOW_HELP_ON_OFF;
+		break;
+	case PROC_SET_RX_P0_RATE:
+		len += sprintf(buf + len, "%u %s"NL, (int)
+			port_cfg->packet_based ?
+			port_cfg->rx_packet[0] : port_cfg->rx_rate[0],
+			port_cfg->packet_based ? "Kpps" : "Kbps");
+		break;
+	case PROC_SET_RX_P1_RATE:
+		len += sprintf(buf + len, "%u %s"NL, (int)
+			port_cfg->packet_based ?
+			port_cfg->rx_packet[1] : port_cfg->rx_rate[1],
+			port_cfg->packet_based ? "Kpps" : "Kbps");
+		break;
+	case PROC_SET_RX_P2_RATE:
+		len += sprintf(buf + len, "%u %s"NL, (int)
+			port_cfg->packet_based ?
+			port_cfg->rx_packet[2] : port_cfg->rx_rate[2],
+			port_cfg->packet_based ? "Kpps" : "Kbps");
+		break;
+	case PROC_SET_RX_P3_RATE:
+		len += sprintf(buf + len, "%u %s"NL, (int)
+			port_cfg->packet_based ?
+			port_cfg->rx_packet[3] : port_cfg->rx_rate[3],
+			port_cfg->packet_based ? "Kpps" : "Kbps");
+		break;
+	case PROC_SET_RX_P4_RATE:
+		len += sprintf(buf + len, "%u %s"NL, (int)
+			port_cfg->packet_based ?
+			port_cfg->rx_packet[4] : port_cfg->rx_rate[4],
+			port_cfg->packet_based ? "Kpps" : "Kbps");
+		break;
+	case PROC_SET_RX_P5_RATE:
+		len += sprintf(buf + len, "%u %s"NL, (int)
+			port_cfg->packet_based ?
+			port_cfg->rx_packet[5] : port_cfg->rx_rate[5],
+			port_cfg->packet_based ? "Kpps" : "Kbps");
+		break;
+	case PROC_SET_RX_P6_RATE:
+		len += sprintf(buf + len, "%u %s"NL, (int)
+			port_cfg->packet_based ?
+			port_cfg->rx_packet[6] : port_cfg->rx_rate[6],
+			port_cfg->packet_based ? "Kpps" : "Kbps");
+		break;
+	case PROC_SET_RX_P7_RATE:
+		len += sprintf(buf + len, "%u %s"NL, (int)
+			port_cfg->packet_based ?
+			port_cfg->rx_packet[7] : port_cfg->rx_rate[7],
+			port_cfg->packet_based ? "Kpps" : "Kbps");
+		break;
+	case PROC_SET_TX_Q0_RATE:
+		len += sprintf(buf + len, "%u %s"NL, (int)
+			port_cfg->packet_based ?
+			port_cfg->tx_packet[0] : port_cfg->tx_rate[0],
+			port_cfg->packet_based ? "Kpps" : "Kbps");
+		break;
+	case PROC_SET_TX_Q1_RATE:
+		len += sprintf(buf + len, "%u %s"NL, (int)
+			port_cfg->packet_based ?
+			port_cfg->tx_packet[1] : port_cfg->tx_rate[1],
+			port_cfg->packet_based ? "Kpps" : "Kbps");
+		break;
+	case PROC_SET_TX_Q2_RATE:
+		len += sprintf(buf + len, "%u %s"NL, (int)
+			port_cfg->packet_based ?
+			port_cfg->tx_packet[2] : port_cfg->tx_rate[2],
+			port_cfg->packet_based ? "Kpps" : "Kbps");
+		break;
+	case PROC_SET_TX_Q3_RATE:
+		len += sprintf(buf + len, "%u %s"NL, (int)
+			port_cfg->packet_based ?
+			port_cfg->tx_packet[3] : port_cfg->tx_rate[3],
+			port_cfg->packet_based ? "Kpps" : "Kbps");
+		break;
+	case PROC_SET_COLOR_MAP:
+		for (i = 0; i < DIFFSERV_ENTRIES / 16; i++) {
+			len += sprintf(buf + len, "%2u=",
+				i * 16);
+			map = port_cfg->color_map[i];
+			for (j = 0; j < 16; j++) {
+				len += sprintf(buf + len, "%u ",
+					map & POLICE_COLOR_MAP_M);
+				map >>= POLICE_COLOR_MAP_S;
+			}
+			len += sprintf(buf + len, "\t%08x"NL,
+				port_cfg->color_map[i]);
+		}
+		break;
+	case PROC_SET_TC_MAP:
+		for (i = 0; i < PRIO_802_1P_ENTRIES / 8; i++) {
+			len += sprintf(buf + len, "%u=",
+				i * 8);
+			map = port_cfg->tc_map[i];
+			for (j = 0; j < 8; j++) {
+				len += sprintf(buf + len, "%u ",
+					map & PORT_TC_MAP_M);
+				map >>= PORT_TC_MAP_S;
+			}
+			len += sprintf(buf + len, "\t%08x"NL,
+				port_cfg->tc_map[i]);
+		}
+		break;
+	case PROC_SET_MMD_ID:
+		chk = port_cfg->mmd_id;
+		type = SHOW_HELP_HEX;
+		break;
+	case PROC_SET_MMD_REG:
+		chk = port_cfg->mmd_reg;
+		type = SHOW_HELP_HEX;
+		break;
+	default:
+		return len;
+	}
+	return sysfs_show(len, buf, type, chk, note, sw->verbose);
+}  /* sysfs_port_read */
+
+static ssize_t sysfs_port_read_hw(struct ksz_sw *sw, int proc_num, uint n,
+	ssize_t len, char *buf)
+{
+	u16 val;
+	u16 data;
+	uint port;
+	struct ksz_port_cfg *cfg;
+	int chk = 0;
+	int type = SHOW_HELP_ON_OFF;
+	char note[40];
+
+	note[0] = '\0';
+	port = get_sysfs_port(sw, n);
+	cfg = get_port_cfg(sw, port);
+	switch (proc_num) {
+	case PROC_ENABLE_BROADCAST_STORM:
+		chk = port_chk_broad_storm(sw, port);
+		break;
+	case PROC_ENABLE_DIFFSERV:
+		chk = port_chk_diffserv(sw, port);
+		break;
+	case PROC_ENABLE_802_1P:
+		chk = port_chk_802_1p(sw, port);
+		break;
+	case PROC_ENABLE_VLAN_PRIO:
+		chk = port_chk_vlan_prio(sw, port);
+		break;
+	case PROC_ENABLE_MAC_PRIO:
+		chk = port_chk_mac_prio(sw, port);
+		break;
+	case PROC_ENABLE_ACL_PRIO:
+		chk = port_chk_acl_prio(sw, port);
+		break;
+	case PROC_SET_HIGHEST_PRIO:
+		chk = port_chk_highest_prio(sw, port);
+		break;
+	case PROC_SET_OR_PRIO:
+		chk = port_chk_or_prio(sw, port);
+		break;
+	case PROC_ENABLE_PRIO_QUEUE:
+		chk = port_get_prio_queue(sw, port);
+		if (sw->verbose) {
+			if (chk < 3)
+				sprintf(note, " (%u)", (1 << chk));
+			else
+				strcpy(note, " (invalid)");
+		}
+		type = SHOW_HELP_SPECIAL;
+		break;
+	case PROC_SET_REPLACE_VID:
+		chk = port_chk32(sw, port, REG_PORT_MTI_QUEUE_CTRL_0__4,
+			MTI_PVID_REPLACE);
+		break;
+	case PROC_SET_REPLACE_PRIO:
+		chk = port_chk_replace_prio(sw, port);
+		break;
+	case PROC_ENABLE_RX_PRIO_RATE:
+		chk = sw_chk_rx_prio_rate(sw, port);
+		break;
+	case PROC_ENABLE_TX_PRIO_RATE:
+		chk = sw_chk_tx_prio_rate(sw, port);
+		break;
+	case PROC_SET_DROP_NON_VLAN:
+		chk = port_chk_drop_non_vlan(sw, port);
+		break;
+	case PROC_SET_DROP_TAG:
+		chk = port_chk_drop_tag(sw, port);
+		break;
+	case PROC_SET_MIRROR_PORT:
+		chk = port_chk_mirror_sniffer(sw, port);
+		break;
+	case PROC_SET_MIRROR_RX:
+		chk = port_chk_mirror_rx(sw, port);
+		break;
+	case PROC_SET_MIRROR_TX:
+		chk = port_chk_mirror_tx(sw, port);
+		break;
+	case PROC_SET_DIS_NON_VID:
+		chk = port_chk_dis_non_vid(sw, port);
+		break;
+	case PROC_SET_INGRESS:
+		chk = port_chk_in_filter(sw, port);
+		break;
+	case PROC_SET_MAC_BASED_802_1X:
+		chk = port_chk(sw, port, REG_PORT_LUE_CTRL,
+			PORT_MAC_BASED_802_1X);
+		break;
+	case PROC_SET_SRC_ADDR_FILTER:
+		chk = port_chk(sw, port, REG_PORT_LUE_CTRL,
+			PORT_SRC_ADDR_FILTER);
+		break;
+	case PROC_SET_VLAN_LOOKUP_0:
+		chk = port_chk(sw, port, REG_PORT_LUE_CTRL,
+			PORT_VLAN_LOOKUP_VID_0);
+		break;
+	case PROC_SET_MSTP:
+		chk = port_chk_mstp(sw, port);
+		type = SHOW_HELP_NUM;
+		break;
+	case PROC_SET_RX:
+		chk = port_chk_rx(sw, port);
+		break;
+	case PROC_SET_TX:
+		chk = port_chk_tx(sw, port);
+		break;
+	case PROC_SET_LEARN:
+		chk = !port_chk_dis_learn(sw, port);
+		break;
+	case PROC_SET_POWER:
+		chk = port_chk_power(sw, port);
+		break;
+	case PROC_SET_BACK_PRESSURE:
+		chk = port_chk_back_pressure(sw, port);
+		break;
+	case PROC_SET_FORCE_FLOW_CTRL:
+		chk = port_chk_force_flow_ctrl(sw, port);
+		break;
+	case PROC_SET_PASS_ALL:
+		chk = port_chk(sw, port, REG_PORT_MAC_CTRL_1, PORT_PASS_ALL);
+		break;
+	case PROC_SET_TAIL_TAG:
+		chk = port_chk_tail_tag(sw, port);
+		break;
+	case PROC_SET_CUSTOM_VID:
+		port_r16(sw, port, REG_PORT_CUSTOM_VID, &data);
+		chk = data;
+		type = SHOW_HELP_HEX_4;
+		break;
+	case PROC_SET_SR_1_VID:
+		port_r16(sw, port, REG_PORT_AVB_SR_1_VID, &data);
+		chk = data;
+		type = SHOW_HELP_HEX_4;
+		break;
+	case PROC_SET_SR_2_VID:
+		port_r16(sw, port, REG_PORT_AVB_SR_2_VID, &data);
+		chk = data;
+		type = SHOW_HELP_HEX_4;
+		break;
+	case PROC_SET_SR_1_TYPE:
+		port_r16(sw, port, REG_PORT_AVB_SR_1_TYPE, &data);
+		chk = data;
+		type = SHOW_HELP_HEX_4;
+		break;
+	case PROC_SET_SR_2_TYPE:
+		port_r16(sw, port, REG_PORT_AVB_SR_2_TYPE, &data);
+		chk = data;
+		type = SHOW_HELP_HEX_4;
+		break;
+	case PROC_SET_PME_CTRL:
+		chk = port_r_s(sw, port, REG_PORT_PME_CTRL, 7, 0);
+		len += sprintf(buf + len, "%02x:"NL, chk);
+		len += sprintf(buf + len, "\t%02x = Magic Packet detect"NL,
+			PME_WOL_MAGICPKT);
+		len += sprintf(buf + len, "\t%02x = link up detect"NL,
+			PME_WOL_LINKUP);
+		len += sprintf(buf + len, "\t%02x = energy detect"NL,
+			PME_WOL_ENERGY);
+		type = SHOW_HELP_NONE;
+		break;
+	case PROC_SET_PME_STATUS:
+		chk = port_r_s(sw, port, REG_PORT_PME_STATUS, 7, 0);
+		len += sprintf(buf + len, "%02x:"NL, chk);
+		len += sprintf(buf + len, "\t%02x = Magic Packet detect"NL,
+			PME_WOL_MAGICPKT);
+		len += sprintf(buf + len, "\t%02x = link up detect"NL,
+			PME_WOL_LINKUP);
+		len += sprintf(buf + len, "\t%02x = energy detect"NL,
+			PME_WOL_ENERGY);
+		type = SHOW_HELP_NONE;
+		break;
+	case PROC_SET_AUTHEN_MODE:
+		len += sprintf(buf + len, "%u"NL,
+			port_get_authen_mode(sw, port));
+		type = SHOW_HELP_NONE;
+		break;
+	case PROC_SET_ACL:
+		chk = port_chk_acl(sw, port);
+		break;
+	case PROC_SET_P_INDEX:
+		chk = get_log_port(sw, cfg->p_index);
+		if (!chk)
+			chk = sw->mib_port_cnt + 1;
+		if (!(sw->features & SYSFS_1_BASE))
+			chk--;
+		type = SHOW_HELP_NUM;
+		break;
+	case PROC_SET_Q_INDEX:
+		chk = cfg->q_index;
+		type = SHOW_HELP_NUM;
+		break;
+	case PROC_SET_POLICE_PACKET_TYPE:
+		len += sprintf(buf + len, "%u"NL,
+			port_get_police_packet_type(sw, port));
+		type = SHOW_HELP_NONE;
+		break;
+	case PROC_SET_NON_DSCP_COLOR:
+		len += sprintf(buf + len, "%u"NL,
+			port_get_non_dscp_color(sw, port));
+		type = SHOW_HELP_NONE;
+		break;
+	case PROC_ENABLE_PORT_BASED_POLICING:
+		len += sprintf(buf + len, "%u"NL,
+			port_chk_port_based_policing(sw, port));
+		type = SHOW_HELP_NONE;
+		break;
+	case PROC_ENABLE_POLICE_DROP_ALL:
+		chk = port_chk_police_drop_all(sw, port);
+		break;
+	case PROC_ENABLE_COLOR_MARK:
+		chk = port_chk_color_mark(sw, port);
+		break;
+	case PROC_ENABLE_COLOR_REMAP:
+		chk = port_chk_color_remap(sw, port);
+		break;
+	case PROC_ENABLE_DROP_SRP:
+		chk = port_chk_drop_srp(sw, port);
+		break;
+	case PROC_ENABLE_COLOR_AWARE:
+		chk = port_chk_color_aware(sw, port);
+		break;
+	case PROC_ENABLE_POLICE:
+		chk = port_chk_police(sw, port);
+		break;
+	case PROC_SET_Q_CIR:
+		port_cfg_index(sw, port, cfg->p_index, cfg->q_index);
+		len += sprintf(buf + len, "%u"NL,
+			port_get_cir(sw, port));
+		type = SHOW_HELP_NONE;
+		break;
+	case PROC_SET_Q_PIR:
+		port_cfg_index(sw, port, cfg->p_index, cfg->q_index);
+		len += sprintf(buf + len, "%u"NL,
+			port_get_pir(sw, port));
+		type = SHOW_HELP_NONE;
+		break;
+	case PROC_SET_Q_CBS:
+		port_cfg_index(sw, port, cfg->p_index, cfg->q_index);
+		len += sprintf(buf + len, "%u"NL,
+			port_get_cbs(sw, port));
+		type = SHOW_HELP_NONE;
+		break;
+	case PROC_SET_Q_PBS:
+		port_cfg_index(sw, port, cfg->p_index, cfg->q_index);
+		len += sprintf(buf + len, "%u"NL,
+			port_get_pbs(sw, port));
+		type = SHOW_HELP_NONE;
+		break;
+	case PROC_SET_WRED_MAX_THRESHOLD:
+		len += sprintf(buf + len, "%u"NL,
+			port_get_wred_max(sw, port));
+		type = SHOW_HELP_NONE;
+		break;
+	case PROC_SET_WRED_MIN_THRESHOLD:
+		len += sprintf(buf + len, "%u"NL,
+			port_get_wred_min(sw, port));
+		type = SHOW_HELP_NONE;
+		break;
+	case PROC_SET_WRED_MULTIPLIER:
+		len += sprintf(buf + len, "%u"NL,
+			port_get_wred_multiplier(sw, port));
+		type = SHOW_HELP_NONE;
+		break;
+	case PROC_GET_WRED_AVG_SIZE:
+		len += sprintf(buf + len, "%u"NL,
+			port_get_wred_avg_size(sw, port));
+		type = SHOW_HELP_NONE;
+		break;
+	case PROC_SET_WRED_Q_MAX_THRESHOLD:
+		port_cfg_index(sw, port, cfg->p_index, cfg->q_index);
+		len += sprintf(buf + len, "%u"NL,
+			port_get_wred_q_max(sw, port));
+		type = SHOW_HELP_NONE;
+		break;
+	case PROC_SET_WRED_Q_MIN_THRESHOLD:
+		port_cfg_index(sw, port, cfg->p_index, cfg->q_index);
+		len += sprintf(buf + len, "%u"NL,
+			port_get_wred_q_min(sw, port));
+		type = SHOW_HELP_NONE;
+		break;
+	case PROC_SET_WRED_Q_MULTIPLIER:
+		port_cfg_index(sw, port, cfg->p_index, cfg->q_index);
+		len += sprintf(buf + len, "%u"NL,
+			port_get_wred_q_multiplier(sw, port));
+		type = SHOW_HELP_NONE;
+		break;
+	case PROC_GET_WRED_Q_AVG_SIZE:
+		port_cfg_index(sw, port, cfg->p_index, cfg->q_index);
+		len += sprintf(buf + len, "%u"NL,
+			port_get_wred_q_avg_size(sw, port));
+		type = SHOW_HELP_NONE;
+		break;
+	case PROC_SET_WRED_RANDOM_DROP:
+		len += sprintf(buf + len, "%u"NL,
+			port_chk_wred_random_drop(sw, port));
+		type = SHOW_HELP_NONE;
+		break;
+	case PROC_SET_WRED_DROP_GYR:
+		len += sprintf(buf + len, "%u"NL,
+			port_chk_wred_drop_gyr(sw, port));
+		type = SHOW_HELP_NONE;
+		break;
+	case PROC_SET_WRED_DROP_YR:
+		len += sprintf(buf + len, "%u"NL,
+			port_chk_wred_drop_yr(sw, port));
+		type = SHOW_HELP_NONE;
+		break;
+	case PROC_SET_WRED_DROP_R:
+		len += sprintf(buf + len, "%u"NL,
+			port_chk_wred_drop_r(sw, port));
+		type = SHOW_HELP_NONE;
+		break;
+	case PROC_SET_WRED_DROP_ALL:
+		len += sprintf(buf + len, "%u"NL,
+			port_chk_wred_drop_all(sw, port));
+		type = SHOW_HELP_NONE;
+		break;
+	case PROC_GET_WRED_PMON:
+		port_cfg_index(sw, port, cfg->p_index, cfg->q_index);
+		len += sprintf(buf + len, "%u"NL,
+			port_get_wred_pmon(sw, port));
+		type = SHOW_HELP_NONE;
+		break;
+	case PROC_SET_QUEUE_SCHEDULING:
+		chk = port_get_schedule_mode(sw, port, cfg->q_index);
+		if (sw->verbose) {
+			switch (chk) {
+			case 0:
+				strcpy(note, " (strict priority)");
+				break;
+			case 2:
+				strcpy(note, " (WRR)");
+				break;
+			default:
+				strcpy(note, " (invalid)");
+			}
+		}
+		type = SHOW_HELP_SPECIAL;
+		break;
+	case PROC_SET_QUEUE_SHAPING:
+		chk = port_get_shaping(sw, port, cfg->q_index);
+		if (sw->verbose) {
+			switch (chk) {
+			case 0:
+				strcpy(note, " (off)");
+				break;
+			case 1:
+				strcpy(note, " (on)");
+				break;
+			case 2:
+				strcpy(note, " (time aware)");
+				break;
+			default:
+				strcpy(note, " (invalid)");
+			}
+		}
+		type = SHOW_HELP_SPECIAL;
+		break;
+#ifdef MTI_PREEMPT_ENABLE
+	case PROC_SET_PREEMPT:
+		len += sprintf(buf + len, "%u"NL,
+			port_chk_preempt(sw, port, cfg->q_index));
+		type = SHOW_HELP_NONE;
+		break;
+#endif
+	case PROC_SET_TX_RATIO:
+		chk = port_get_tx_ratio(sw, port, cfg->q_index);
+		type = SHOW_HELP_NUM;
+		break;
+	case PROC_SET_CREDIT_HI_WATER_MARK:
+		chk = port_get_hi_water_mark(sw, port, cfg->q_index);
+		type = SHOW_HELP_NUM;
+		break;
+	case PROC_SET_CREDIT_LO_WATER_MARK:
+		chk = port_get_lo_water_mark(sw, port, cfg->q_index);
+		type = SHOW_HELP_NUM;
+		break;
+	case PROC_SET_CREDIT_INCREMENT:
+		chk = port_get_increment(sw, port, cfg->q_index);
+		type = SHOW_HELP_HEX;
+		break;
+	case PROC_SET_SRP:
+		chk = port_get_srp(sw, port);
+		if (sw->verbose) {
+			switch (chk) {
+			case 0:
+				strcpy(note, " (off)");
+				break;
+			case 1:
+				strcpy(note, " (insert SR_1 tag)");
+				break;
+			case 2:
+				strcpy(note, " (insert SR_2 tag)");
+				break;
+			default:
+				strcpy(note, " (invalid)");
+			}
+		}
+		type = SHOW_HELP_SPECIAL;
+		break;
+	case PROC_SET_QM_DROP:
+		len += sprintf(buf + len, "%u"NL,
+			port_get_qm_drop(sw, port));
+		type = SHOW_HELP_NONE;
+		break;
+	case PROC_SET_QM_BURST_SIZE:
+		len += sprintf(buf + len, "%u"NL,
+			port_get_qm_burst_size(sw, port));
+		type = SHOW_HELP_NONE;
+		break;
+	case PROC_SET_QM_RESV_SPACE:
+		len += sprintf(buf + len, "%u"NL,
+			port_get_qm_resv_space(sw, port));
+		type = SHOW_HELP_NONE;
+		break;
+	case PROC_SET_QM_HI_WATER_MARK:
+		len += sprintf(buf + len, "%u"NL,
+			port_get_qm_hi_water_mark(sw, port, cfg->q_index));
+		type = SHOW_HELP_NONE;
+		break;
+	case PROC_SET_QM_LO_WATER_MARK:
+		len += sprintf(buf + len, "%u"NL,
+			port_get_qm_lo_water_mark(sw, port, cfg->q_index));
+		type = SHOW_HELP_NONE;
+		break;
+	case PROC_GET_QM_TX_USED:
+		len += sprintf(buf + len, "%u"NL,
+			port_get_qm_tx_used(sw, port, cfg->q_index));
+		type = SHOW_HELP_NONE;
+		break;
+	case PROC_GET_QM_TX_AVAIL:
+		len += sprintf(buf + len, "%u"NL,
+			port_get_qm_tx_avail(sw, port, cfg->q_index));
+		type = SHOW_HELP_NONE;
+		break;
+	case PROC_GET_QM_TX_CALCULATED:
+		len += sprintf(buf + len, "%u"NL,
+			port_get_qm_tx_calculated(sw, port, cfg->q_index));
+		type = SHOW_HELP_NONE;
+		break;
+	case PROC_GET_RX_FLOW_CTRL:
+		chk = port_chk(sw, port, REG_PORT_STATUS_0,
+			PORT_RX_FLOW_CTRL);
+		break;
+	case PROC_GET_TX_FLOW_CTRL:
+		chk = port_chk(sw, port, REG_PORT_STATUS_0,
+			PORT_TX_FLOW_CTRL);
+		break;
+	case PROC_SET_MMD_VAL:
+		if (PHY_INTERFACE_MODE_SGMII == sw->port_info[port].interface)
+			port_sgmii_r(sw, port, cfg->mmd_id, cfg->mmd_reg,
+				&val, 1);
+		else
+			port_mmd_read(sw, port, cfg->mmd_id, cfg->mmd_reg,
+				&val, 1);
+		chk = val;
+		type = SHOW_HELP_HEX_4;
+		break;
+	case PROC_SET_MAC_LOOPBACK:
+		chk = port_chk_mac_loopback(sw, port);
+		break;
+	case PROC_SET_PHY_LOOPBACK:
+		chk = port_chk_phy_loopback(sw, port);
+		break;
+	case PROC_SET_REMOTE_LOOPBACK:
+		chk = port_chk_remote_loopback(sw, port);
+		break;
+	default:
+		return len;
+	}
+	return sysfs_show(len, buf, type, chk, note, sw->verbose);
+}  /* sysfs_port_read_hw */
+
+static int sysfs_port_write(struct ksz_sw *sw, int proc_num, uint n,
+	int num, const char *buf)
+{
+	int count;
+	uint m;
+	uint p;
+	uint port;
+	unsigned int val;
+	u16 mmd_val;
+	int processed = true;
+	struct ksz_port_cfg *cfg;
+	struct ksz_port_info *info;
+
+	port = get_sysfs_port(sw, n);
+	info = get_port_info(sw, port);
+	cfg = get_port_cfg(sw, port);
+	p = port;
+	switch (proc_num) {
+	case PROC_SET_PORT_DUPLEX:
+	case PROC_SET_PORT_SPEED:
+		if (sw->HOST_PORT == port)
+			break;
+		if ((PROC_SET_PORT_DUPLEX == proc_num && num > 2) ||
+		    (PROC_SET_PORT_SPEED == proc_num &&
+		    num != 0 && num != 10 && num != 100 && num != 1000))
+			break;
+		if (!(sw->features & GIGABIT_SUPPORT) && num == 1000)
+			break;
+
+		if (!info->set_link_speed)
+			break;
+		if (PROC_SET_PORT_DUPLEX == proc_num)
+			info->own_duplex = (u8) num;
+		else
+			info->own_speed = (u16) num;
+		info->set_link_speed(sw, p, info->own_speed,
+				     info->own_duplex, info->own_flow_ctrl);
+		break;
+	case PROC_SET_MAC_OPERATIONAL:
+#if 0
+		if (sw->overrides & USE_802_1X_AUTH)
+			break;
+#endif
+		m = BIT(port);
+		count = sw->dev_ports & m;
+		if (count && !num) {
+			sw->dev_ports &= ~m;
+			num = 1;
+		} else if (!count && num) {
+			if (sw->dev_count > 1)
+				break;
+			sw->dev_ports |= m;
+			num = 2;
+		} else
+			num = 0;
+
+#ifdef CONFIG_KSZ_MRP
+		if (sw->features & MRP_SUPPORT) {
+			struct mrp_info *mrp = &sw->mrp;
+
+			if (2 == num) {
+				mrp->rx_ports |= m;
+				mrp->tx_ports |= m;
+			} else if (1 == num) {
+				mrp->rx_ports &= ~m;
+				mrp->tx_ports &= ~m;
+			}
+		}
+#endif
+		break;
+	case PROC_SET_VLAN_RESTRICTED:
+		cfg->restricted = !!num;
+		break;
+	case PROC_SET_VLAN_UNTAGGED:
+	{
+		int bit;
+		int index;
+
+		index = sw->vlan_index / VID_IN_DATA;
+		bit = sw->vlan_index % VID_IN_DATA;
+		count = !!(cfg->untagged[index] & (1 << bit));
+		if (count != !!num) {
+			struct ksz_vlan_table vlan;
+
+			if (num)
+				cfg->untagged[index] |= (1 << bit);
+			else
+				cfg->untagged[index] &= ~(1 << bit);
+			sw->ops->release(sw);
+			if (!sw_r_vlan_table(sw, sw->vlan_index, &vlan)) {
+				if (num)
+					vlan.untag |= (1 << port);
+				else
+					vlan.untag &= ~(1 << port);
+				sw_w_vlan_table(sw, sw->vlan_index, &vlan);
+			}
+			sw->ops->acquire(sw);
+		}
+		break;
+	}
+	case PROC_SET_PORT_MIB:
+	{
+		struct ksz_port_mib *mib = get_port_mib(sw, port);
+
+		if (num >= 1 && num <= 2) {
+			port_freeze_mib(sw, p, num - 1);
+			break;
+		}
+		memset((void *) mib->counter, 0, sizeof(u64) *
+			TOTAL_SWITCH_COUNTER_NUM);
+		mib->rate[0].last = mib->rate[1].last = 0;
+		mib->rate[0].last_cnt = mib->rate[1].last_cnt = 0;
+		mib->rate[0].peak = mib->rate[1].peak = 0;
+		break;
+	}
+	case PROC_ENABLE_BROADCAST_STORM:
+		if (!num)
+			sw_dis_broad_storm(sw, port);
+		else
+			sw_ena_broad_storm(sw, port);
+		break;
+	case PROC_ENABLE_DIFFSERV:
+		if (!num)
+			sw_dis_diffserv(sw, port);
+		else
+			sw_ena_diffserv(sw, port);
+		break;
+	case PROC_ENABLE_802_1P:
+		if (!num)
+			sw_dis_802_1p(sw, port);
+		else
+			sw_ena_802_1p(sw, port);
+		break;
+	case PROC_ENABLE_VLAN_PRIO:
+		port_cfg_vlan_prio(sw, p, num);
+		break;
+	case PROC_ENABLE_MAC_PRIO:
+		port_cfg_mac_prio(sw, p, num);
+		break;
+	case PROC_ENABLE_ACL_PRIO:
+		port_cfg_acl_prio(sw, p, num);
+		break;
+	case PROC_SET_HIGHEST_PRIO:
+		port_cfg_highest_prio(sw, p, num);
+		break;
+	case PROC_SET_OR_PRIO:
+		port_cfg_or_prio(sw, p, num);
+		break;
+	case PROC_SET_PORT_BASED:
+		sw_cfg_port_based(sw, p, num);
+		break;
+	case PROC_SET_DEF_VID:
+		sw_cfg_def_vid(sw, p, num);
+		break;
+	case PROC_SET_MEMBER:
+		num = get_phy_mask_from_log(sw, num);
+		sw_cfg_port_base_vlan(sw, port, (u16) num);
+		break;
+	case PROC_ENABLE_PRIO_QUEUE:
+		if (0 <= num && num <= 2)
+			sw_set_multi_queue(sw, p, num);
+		break;
+	case PROC_SET_REPLACE_VID:
+		sw_cfg_replace_null_vid(sw, p, num);
+		break;
+	case PROC_SET_LIMIT:
+		hw_cfg_rx_limit(sw, cfg, p, (u8) num);
+		break;
+	case PROC_SET_LIMIT_PORT_BASED:
+		hw_cfg_in_port_based(sw, cfg, p, num);
+		break;
+	case PROC_SET_LIMIT_PACKET_BASED:
+		hw_cfg_rate_packet_based(sw, cfg, p, num);
+		break;
+	case PROC_SET_LIMIT_FLOW_CTRL:
+		hw_cfg_in_flow_ctrl(sw, cfg, p, num);
+		break;
+	case PROC_SET_LIMIT_CNT_IFG:
+		hw_cfg_cnt_ifg(sw, cfg, p, num);
+		break;
+	case PROC_SET_LIMIT_CNT_PRE:
+		hw_cfg_cnt_pre(sw, cfg, p, num);
+		break;
+	case PROC_SET_RX_P0_RATE:
+		hw_cfg_rx_prio_rate(sw, cfg, p, 0, num);
+		hw_set_rx_prio(sw, p);
+		break;
+	case PROC_SET_RX_P1_RATE:
+		hw_cfg_rx_prio_rate(sw, cfg, p, 1, num);
+		hw_set_rx_prio(sw, p);
+		break;
+	case PROC_SET_RX_P2_RATE:
+		hw_cfg_rx_prio_rate(sw, cfg, p, 2, num);
+		hw_set_rx_prio(sw, p);
+		break;
+	case PROC_SET_RX_P3_RATE:
+		hw_cfg_rx_prio_rate(sw, cfg, p, 3, num);
+		hw_set_rx_prio(sw, p);
+		break;
+	case PROC_SET_RX_P4_RATE:
+		hw_cfg_rx_prio_rate(sw, cfg, p, 4, num);
+		hw_set_rx_prio(sw, p);
+		break;
+	case PROC_SET_RX_P5_RATE:
+		hw_cfg_rx_prio_rate(sw, cfg, p, 5, num);
+		hw_set_rx_prio(sw, p);
+		break;
+	case PROC_SET_RX_P6_RATE:
+		hw_cfg_rx_prio_rate(sw, cfg, p, 6, num);
+		hw_set_rx_prio(sw, p);
+		break;
+	case PROC_SET_RX_P7_RATE:
+		hw_cfg_rx_prio_rate(sw, cfg, p, 7, num);
+		break;
+	case PROC_SET_TX_Q0_RATE:
+		hw_cfg_tx_prio_rate(sw, cfg, p, 0, num);
+		break;
+	case PROC_SET_TX_Q1_RATE:
+		hw_cfg_tx_prio_rate(sw, cfg, p, 1, num);
+		break;
+	case PROC_SET_TX_Q2_RATE:
+		hw_cfg_tx_prio_rate(sw, cfg, p, 2, num);
+		break;
+	case PROC_SET_TX_Q3_RATE:
+		hw_cfg_tx_prio_rate(sw, cfg, p, 3, num);
+		break;
+	case PROC_SET_REPLACE_PRIO:
+		sw_cfg_replace_prio(sw, p, num);
+		break;
+	case PROC_ENABLE_RX_PRIO_RATE:
+		if (!num)
+			sw_dis_rx_prio_rate(sw, p);
+		else
+			sw_ena_rx_prio_rate(sw, cfg, p);
+		break;
+	case PROC_ENABLE_TX_PRIO_RATE:
+		if (!num)
+			sw_dis_tx_prio_rate(sw, p);
+		else
+			sw_ena_tx_prio_rate(sw, cfg, p);
+		break;
+	case PROC_SET_LINK_MD:
+		sw_get_link_md(sw, port);
+		break;
+	case PROC_SET_SQI:
+		port_chk_sqi(sw, port);
+		break;
+	case PROC_SET_COLOR_MAP:
+		count = sscanf(buf, "%d=%x", (unsigned int *) &num, &val);
+		if (2 == count)
+			port_cfg_color_map(sw, p, (u8) num, (u32) val);
+		break;
+	case PROC_SET_TC_MAP:
+		count = sscanf(buf, "%d=%x", (unsigned int *) &num, &val);
+		if (2 == count)
+			port_cfg_tc_map(sw, p, (u8) num, (u32) val);
+		break;
+	case PROC_SET_DROP_NON_VLAN:
+		port_cfg_drop_non_vlan(sw, p, num);
+		break;
+	case PROC_SET_DROP_TAG:
+		port_cfg_drop_tag(sw, p, num);
+		break;
+	case PROC_SET_MIRROR_PORT:
+		port_cfg_mirror_sniffer(sw, p, num);
+		break;
+	case PROC_SET_MIRROR_RX:
+		port_cfg_mirror_rx(sw, p, num);
+		break;
+	case PROC_SET_MIRROR_TX:
+		port_cfg_mirror_tx(sw, p, num);
+		break;
+	case PROC_SET_DIS_NON_VID:
+		port_cfg_dis_non_vid(sw, p, num);
+		break;
+	case PROC_SET_INGRESS:
+		port_cfg_in_filter(sw, p, num);
+		break;
+	case PROC_SET_MAC_BASED_802_1X:
+		port_cfg(sw, p, REG_PORT_LUE_CTRL, PORT_MAC_BASED_802_1X,
+			num);
+		break;
+	case PROC_SET_SRC_ADDR_FILTER:
+		if (!num)
+			sw->open_ports |= (1 << port);
+		port_cfg(sw, p, REG_PORT_LUE_CTRL, PORT_SRC_ADDR_FILTER,
+			num);
+		if (num)
+			sw->open_ports &= ~(1 << port);
+		break;
+	case PROC_SET_VLAN_LOOKUP_0:
+		port_cfg(sw, p, REG_PORT_LUE_CTRL, PORT_VLAN_LOOKUP_VID_0,
+			num);
+		break;
+	case PROC_SET_MSTP:
+		port_cfg_mstp(sw, p, num);
+		break;
+	case PROC_SET_RX:
+		port_cfg_rx_special(sw, p, num);
+		break;
+	case PROC_SET_TX:
+		port_cfg_tx(sw, p, num);
+		break;
+	case PROC_SET_LEARN:
+		port_cfg_dis_learn(sw, p, !num);
+		if (!num)
+			sw_flush_dyn_mac_table(sw, port);
+		break;
+	case PROC_SET_POWER:
+		port_cfg_power(sw, p, num);
+		break;
+	case PROC_SET_BACK_PRESSURE:
+		port_cfg_back_pressure(sw, p, num);
+		break;
+	case PROC_SET_FORCE_FLOW_CTRL:
+		port_cfg_force_flow_ctrl(sw, p, num);
+		break;
+	case PROC_SET_PASS_ALL:
+		port_cfg(sw, p, REG_PORT_MAC_CTRL_1, PORT_PASS_ALL, num);
+		break;
+	case PROC_SET_TAIL_TAG:
+		port_cfg_tail_tag(sw, p, num);
+		break;
+	case PROC_SET_CUSTOM_VID:
+		port_w16(sw, p, REG_PORT_CUSTOM_VID, (u16) num);
+		break;
+	case PROC_SET_SR_1_VID:
+		port_w16(sw, p, REG_PORT_AVB_SR_1_VID, (u16) num);
+		break;
+	case PROC_SET_SR_2_VID:
+		port_w16(sw, p, REG_PORT_AVB_SR_2_VID, (u16) num);
+		break;
+	case PROC_SET_SR_1_TYPE:
+		port_w16(sw, p, REG_PORT_AVB_SR_1_TYPE, (u16) num);
+		break;
+	case PROC_SET_SR_2_TYPE:
+		port_w16(sw, p, REG_PORT_AVB_SR_2_TYPE, (u16) num);
+		break;
+	case PROC_SET_PME_CTRL:
+		port_w_s(sw, p, REG_PORT_PME_CTRL, 7, 0, (u8) num);
+		break;
+	case PROC_SET_PME_STATUS:
+		port_w_s(sw, p, REG_PORT_PME_STATUS, 7, 0, (u8) num);
+		break;
+	case PROC_SET_AUTHEN_MODE:
+		if (num > PORT_AUTHEN_TRAP)
+			break;
+		if (num == PORT_AUTHEN_NORMAL)
+			num = PORT_AUTHEN_PASS;
+		port_set_authen_mode(sw, p, num);
+		if (sw->overrides & USE_802_1X_AUTH)
+			port_open(sw, p, PORT_AUTHEN_PASS == num);
+		break;
+	case PROC_SET_ACL:
+		port_cfg_acl(sw, p, num);
+		break;
+	case PROC_SET_P_INDEX:
+		if (sw->features & SYSFS_1_BASE) {
+			if (num)
+				num--;
+			else
+				num = sw->mib_port_cnt;
+		}
+		if (0 <= num && num <= sw->mib_port_cnt)
+			cfg->p_index = (u8) get_phy_port(sw, num + 1);
+		break;
+	case PROC_SET_Q_INDEX:
+		if (0 <= num && num < PRIO_QUEUES)
+			cfg->q_index = (u8) num;
+		break;
+	case PROC_SET_POLICE_PACKET_TYPE:
+		port_set_police_packet_type(sw, p, num);
+		break;
+	case PROC_SET_NON_DSCP_COLOR:
+		port_set_non_dscp_color(sw, p, num);
+		break;
+	case PROC_ENABLE_PORT_BASED_POLICING:
+		port_cfg_port_based_policing(sw, p, num);
+		break;
+	case PROC_ENABLE_POLICE_DROP_ALL:
+		port_cfg_police_drop_all(sw, p, num);
+		break;
+	case PROC_ENABLE_COLOR_MARK:
+		port_cfg_color_mark(sw, p, num);
+		break;
+	case PROC_ENABLE_COLOR_REMAP:
+		port_cfg_color_remap(sw, p, num);
+		break;
+	case PROC_ENABLE_DROP_SRP:
+		port_cfg_drop_srp(sw, p, num);
+		break;
+	case PROC_ENABLE_COLOR_AWARE:
+		port_cfg_color_aware(sw, p, num);
+		break;
+	case PROC_ENABLE_POLICE:
+		port_cfg_police(sw, p, num);
+		break;
+	case PROC_SET_Q_CIR:
+		port_cfg_index(sw, p, cfg->p_index, cfg->q_index);
+		port_set_cir(sw, p, num);
+		break;
+	case PROC_SET_Q_PIR:
+		port_cfg_index(sw, p, cfg->p_index, cfg->q_index);
+		port_set_pir(sw, p, num);
+		break;
+	case PROC_SET_Q_CBS:
+		port_cfg_index(sw, p, cfg->p_index, cfg->q_index);
+		port_set_cbs(sw, p, num);
+		break;
+	case PROC_SET_Q_PBS:
+		port_cfg_index(sw, p, cfg->p_index, cfg->q_index);
+		port_set_pbs(sw, p, num);
+		break;
+	case PROC_SET_WRED_MAX_THRESHOLD:
+		port_set_wred_max(sw, p, num);
+		break;
+	case PROC_SET_WRED_MIN_THRESHOLD:
+		port_set_wred_min(sw, p, num);
+		break;
+	case PROC_SET_WRED_MULTIPLIER:
+		port_set_wred_multiplier(sw, p, num);
+		break;
+	case PROC_SET_WRED_Q_MAX_THRESHOLD:
+		port_cfg_index(sw, p, cfg->p_index, cfg->q_index);
+		port_set_wred_q_max(sw, p, num);
+		break;
+	case PROC_SET_WRED_Q_MIN_THRESHOLD:
+		port_cfg_index(sw, p, cfg->p_index, cfg->q_index);
+		port_set_wred_q_min(sw, p, num);
+		break;
+	case PROC_SET_WRED_Q_MULTIPLIER:
+		port_cfg_index(sw, p, cfg->p_index, cfg->q_index);
+		port_set_wred_q_multiplier(sw, p, num);
+		break;
+	case PROC_SET_WRED_RANDOM_DROP:
+		port_cfg_wred_random_drop(sw, p, num);
+		break;
+	case PROC_SET_WRED_DROP_GYR:
+		port_cfg_wred_drop_gyr(sw, p, num);
+		break;
+	case PROC_SET_WRED_DROP_YR:
+		port_cfg_wred_drop_yr(sw, p, num);
+		break;
+	case PROC_SET_WRED_DROP_R:
+		port_cfg_wred_drop_r(sw, p, num);
+		break;
+	case PROC_SET_WRED_DROP_ALL:
+		port_cfg_wred_drop_all(sw, p, num);
+		break;
+	case PROC_SET_QUEUE_SCHEDULING:
+		port_set_schedule_mode(sw, p, cfg->q_index, (u8) num);
+		break;
+	case PROC_SET_QUEUE_SHAPING:
+		port_set_shaping(sw, p, cfg->q_index, (u8) num);
+		break;
+#ifdef MTI_PREEMPT_ENABLE
+	case PROC_SET_PREEMPT:
+		port_cfg_preempt(sw, p, cfg->q_index, num);
+		break;
+#endif
+	case PROC_SET_TX_RATIO:
+		port_set_tx_ratio(sw, p, cfg->q_index, (u8) num);
+		break;
+	case PROC_SET_CREDIT_HI_WATER_MARK:
+		port_set_hi_water_mark(sw, p, cfg->q_index, (u16) num);
+		break;
+	case PROC_SET_CREDIT_LO_WATER_MARK:
+		port_set_lo_water_mark(sw, p, cfg->q_index, (u16) num);
+		break;
+	case PROC_SET_CREDIT_INCREMENT:
+		port_set_increment(sw, p, cfg->q_index, (u16) num);
+		break;
+	case PROC_SET_SRP:
+		port_set_srp(sw, p, (u8) num);
+		break;
+	case PROC_SET_QM_DROP:
+		port_set_qm_drop(sw, p, num);
+		break;
+	case PROC_SET_QM_BURST_SIZE:
+		port_set_qm_burst_size(sw, p, (u8) num);
+		break;
+	case PROC_SET_QM_RESV_SPACE:
+		port_set_qm_resv_space(sw, p, (u16) num);
+		break;
+	case PROC_SET_QM_HI_WATER_MARK:
+		port_set_qm_hi_water_mark(sw, p, cfg->q_index, (u16) num);
+		break;
+	case PROC_SET_QM_LO_WATER_MARK:
+		port_set_qm_lo_water_mark(sw, p, cfg->q_index, (u16) num);
+		break;
+	case PROC_SET_MMD_ID:
+		cfg->mmd_id = (u16) num;
+		break;
+	case PROC_SET_MMD_REG:
+		cfg->mmd_reg = (u16) num;
+		break;
+	case PROC_SET_MMD_VAL:
+		mmd_val = (u16) num;
+		if (PHY_INTERFACE_MODE_SGMII == info->interface)
+			port_sgmii_w(sw, p, cfg->mmd_id, cfg->mmd_reg,
+				&mmd_val, 1);
+		else
+			port_mmd_write(sw, p, cfg->mmd_id, cfg->mmd_reg,
+				&mmd_val, 1);
+		break;
+	case PROC_SET_MAC_LOOPBACK:
+		port_cfg_mac_loopback(sw, p, num);
+		break;
+	case PROC_SET_PHY_LOOPBACK:
+		port_cfg_phy_loopback(sw, p, num);
+		break;
+	case PROC_SET_REMOTE_LOOPBACK:
+		port_cfg_remote_loopback(sw, p, num);
+		break;
+	default:
+		processed = false;
+		break;
+	}
+	return processed;
+}  /* sysfs_port_write */
+
+static ssize_t sysfs_mac_read(struct ksz_sw *sw, int proc_num, ssize_t len,
+	char *buf)
+{
+	struct ksz_mac_table *entry;
+	u16 ports;
+
+	entry = &sw->info->mac_entry;
+	switch (proc_num) {
+	case PROC_SET_STATIC_FID:
+		len += sprintf(buf + len, "0x%03x"NL, entry->fid);
+		break;
+	case PROC_SET_STATIC_USE_FID:
+		len += sprintf(buf + len, "%u"NL, entry->use_fid);
+		break;
+	case PROC_SET_STATIC_MSTP:
+		len += sprintf(buf + len, "%u"NL, entry->mstp);
+		break;
+	case PROC_SET_STATIC_PRIO:
+		len += sprintf(buf + len, "%u"NL, entry->prio);
+		break;
+	case PROC_SET_STATIC_SRC:
+		len += sprintf(buf + len, "%u"NL, entry->src);
+		break;
+	case PROC_SET_STATIC_DST:
+		len += sprintf(buf + len, "%u"NL, entry->dst);
+		break;
+	case PROC_SET_STATIC_OVERRIDE:
+		len += sprintf(buf + len, "%u"NL, entry->override);
+		break;
+	case PROC_SET_STATIC_VALID:
+		len += sprintf(buf + len, "%u"NL, entry->valid);
+		break;
+	case PROC_SET_STATIC_PORTS:
+		ports = entry->ports;
+		ports = get_log_mask_from_phy(sw, entry->ports);
+		len += sprintf(buf + len, "0x%04x"NL, ports);
+		break;
+	case PROC_SET_STATIC_MAC_ADDR:
+		len += sprintf(buf + len, "%02x:%02x:%02x:%02x:%02x:%02x"NL,
+			entry->addr[0], entry->addr[1],
+			entry->addr[2], entry->addr[3],
+			entry->addr[4], entry->addr[5]);
+		break;
+	case PROC_SET_STATIC_TYPE:
+		len += sprintf(buf + len, "%u"NL, sw->alu_type);
+		break;
+	case PROC_SET_STATIC_INDEX:
+		len += sprintf(buf + len, "0x%03x"NL, sw->alu_index);
+		break;
+	case PROC_SET_STATIC_INFO:
+		if (sw->alu_dirty) {
+			if (2 == sw->alu_type) {
+				u8 mac_addr[ETH_ALEN];
+				u16 fid;
+				u16 mac_index;
+
+				memcpy(mac_addr, entry->addr, ETH_ALEN);
+				fid = entry->fid;
+				sw_r_dyn_mac_table(sw, sw->alu_index,
+					mac_addr, fid, entry, &mac_index);
+				if (!sw->alu_index && mac_index)
+					sw->alu_index = mac_index;
+			} else if (!entry->dirty)
+				sw_r_sta_mac_table(sw, sw->alu_index,
+					sw->alu_type, entry);
+			sw->alu_dirty = 0;
+		}
+		ports = entry->ports;
+		ports = get_log_mask_from_phy(sw, ports);
+		len += sprintf(buf + len,
+			"%3x.%u: %02X:%02X:%02X:%02X:%02X:%02X  "
+			"%04x  m:%u  ",
+			sw->alu_index, sw->alu_type,
+			entry->addr[0], entry->addr[1], entry->addr[2],
+			entry->addr[3], entry->addr[4], entry->addr[5],
+			ports, entry->mstp);
+		if (2 == sw->alu_type)
+			len += sprintf(buf + len,
+				"t:%u  s:%u  d:%u  o:%u  %02x  [%u]"NL,
+				entry->prio, entry->src, entry->dst,
+				entry->override, entry->fid,
+				entry->dirty ? 2 : entry->valid);
+		else
+			len += sprintf(buf + len,
+				"p:%u  s:%u  d:%u  o:%u  %u:%02x  [%u]"NL,
+				entry->prio, entry->src, entry->dst,
+				entry->override, entry->use_fid, entry->fid,
+				entry->dirty ? 2 : entry->valid);
+		break;
+	}
+	return len;
+}  /* sysfs_mac_read */
+
+static int sysfs_mac_write(struct ksz_sw *sw, int proc_num, int num,
+	const char *buf)
+{
+	struct ksz_mac_table *entry;
+	u16 ports;
+	int processed = true;
+
+	entry = &sw->info->mac_entry;
+	switch (proc_num) {
+	case PROC_SET_STATIC_FID:
+		if (0 <= num && num <= ALU_V_FID_M) {
+			entry->fid = num;
+			entry->dirty = 1;
+		}
+		break;
+	case PROC_SET_STATIC_USE_FID:
+		if (num)
+			entry->use_fid = 1;
+		else
+			entry->use_fid = 0;
+		entry->dirty = 1;
+		break;
+	case PROC_SET_STATIC_MSTP:
+		if (0 <= num && num <= ALU_V_MSTP_M) {
+			entry->mstp = num;
+			entry->dirty = 1;
+		}
+		break;
+	case PROC_SET_STATIC_PRIO:
+		if (0 <= num && num <= ALU_V_PRIO_AGE_CNT_M) {
+			entry->prio = num;
+			entry->dirty = 1;
+		}
+		break;
+	case PROC_SET_STATIC_SRC:
+		if (num)
+			entry->src = 1;
+		else
+			entry->src = 0;
+		entry->dirty = 1;
+		break;
+	case PROC_SET_STATIC_DST:
+		if (num)
+			entry->dst = 1;
+		else
+			entry->dst = 0;
+		entry->dirty = 1;
+		break;
+	case PROC_SET_STATIC_OVERRIDE:
+		if (num)
+			entry->override = 1;
+		else
+			entry->override = 0;
+		entry->dirty = 1;
+		break;
+	case PROC_SET_STATIC_VALID:
+		if (num)
+			entry->valid = 1;
+		else
+			entry->valid = 0;
+		if (2 == sw->alu_type)
+			sw_w_dyn_mac_table(sw, sw->alu_index,
+				entry->addr, entry->fid, entry);
+		else
+			sw_w_sta_mac_table(sw, sw->alu_index,
+				sw->alu_type, entry);
+		break;
+	case PROC_SET_STATIC_PORTS:
+		if (0 <= num && num <= sw->PORT_MASK) {
+			ports = num;
+			ports = get_phy_mask_from_log(sw, ports);
+			entry->ports = ports;
+			entry->dirty = 1;
+		}
+		break;
+	case PROC_SET_STATIC_MAC_ADDR:
+	{
+		int i;
+		int n[6];
+
+		i = sscanf(buf, "%x:%x:%x:%x:%x:%x",
+			&n[0], &n[1], &n[2], &n[3], &n[4], &n[5]);
+		if (6 == i) {
+			for (i = 0; i < 6; i++)
+				entry->addr[i] = (u8) n[i];
+			entry->dirty = 1;
+		}
+		break;
+	}
+	case PROC_SET_STATIC_TYPE:
+		if (0 <= num && num < 3) {
+			sw->alu_type = num;
+			sw->alu_dirty = 1;
+		}
+		break;
+	case PROC_SET_STATIC_INDEX:
+		if (0 <= num && num < 0x1000) {
+			sw->alu_index = num;
+			sw->alu_dirty = 1;
+		}
+		break;
+	case PROC_SET_STATIC_INFO:
+		sw->alu_dirty = 1;
+		entry->dirty = 0;
+		break;
+	default:
+		processed = false;
+		break;
+	}
+	return processed;
+}  /* sysfs_mac_write */
+
+static ssize_t sysfs_vlan_read(struct ksz_sw *sw, int proc_num, ssize_t len,
+	char *buf)
+{
+	struct ksz_vlan_table *entry;
+	int i;
+
+	entry = &sw->info->vlan_entry;
+	switch (proc_num) {
+	case PROC_SET_VLAN_VALID:
+		len += sprintf(buf + len, "%u"NL, entry->valid);
+		break;
+	case PROC_SET_VLAN_PORTS:
+		len += sprintf(buf + len, "0x%04x"NL, entry->ports);
+		break;
+	case PROC_SET_VLAN_UNTAG:
+		len += sprintf(buf + len, "0x%04x"NL, entry->untag);
+		break;
+	case PROC_SET_VLAN_FID:
+		len += sprintf(buf + len, "0x%03x"NL, entry->fid);
+		break;
+	case PROC_SET_VLAN_MSTP:
+		len += sprintf(buf + len, "0x%x"NL, entry->mstp);
+		break;
+	case PROC_SET_VLAN_PRIO:
+		len += sprintf(buf + len, "0x%x"NL, entry->prio);
+		break;
+	case PROC_SET_VLAN_OPTION:
+		len += sprintf(buf + len, "%u"NL, entry->option);
+		break;
+	case PROC_SET_VLAN_VID:
+		len += sprintf(buf + len, "0x%03x"NL, sw->vlan_index);
+		break;
+	case PROC_SET_VLAN_INFO:
+		if (sw->vlan_dirty) {
+			if (!entry->dirty) {
+				sw_r_vlan_table(sw, sw->vlan_index, entry);
+				entry->ports =
+					get_log_mask_from_phy(sw, entry->ports);
+			}
+			sw->vlan_dirty = 0;
+		}
+		len += sprintf(buf + len,
+			"%3x: 0x%03x  m:%x  p:%x  o:%u  %04x  %04x  [%u]"NL,
+			sw->vlan_index, entry->fid, entry->mstp, entry->prio,
+			entry->option, entry->untag, entry->ports,
+			entry->dirty ? 2 : entry->valid);
+		break;
+	case PROC_SET_VID_2_FID:
+		len += sprintf(buf + len,
+			"%03x=%02x"NL, sw->vlan_index,
+				sw->info->vid2fid[sw->vlan_index]);
+		break;
+	case PROC_SET_FID_2_MSTID:
+		if (entry->fid) {
+			len += sprintf(buf + len,
+				"%02x=%u"NL, entry->fid,
+					sw->info->fid2mstid[entry->fid]);
+			break;
+		}
+		for (i = 0; i < FID_ENTRIES; i++) {
+			if ((i % 32) == 0)
+				len += sprintf(buf + len,
+					"%02x: ", i);
+			len += sprintf(buf + len,
+				"%u ", sw->info->fid2mstid[i]);
+			if ((i % 32) == 31)
+				len += sprintf(buf + len,
+					NL);
+		}
+		break;
+	}
+	return len;
+}  /* sysfs_vlan_read */
+
+static int sysfs_vlan_write(struct ksz_sw *sw, int proc_num, int num)
+{
+	struct ksz_vlan_table *entry;
+	u16 ports;
+	u16 untag;
+	int processed = true;
+
+	entry = &sw->info->vlan_entry;
+	switch (proc_num) {
+	case PROC_SET_VLAN_VALID:
+		if (num)
+			entry->valid = 1;
+		else
+			entry->valid = 0;
+		ports = entry->ports;
+		untag = entry->untag;
+		entry->ports = get_phy_mask_from_log(sw, entry->ports);
+		entry->untag = get_phy_mask_from_log(sw, entry->untag);
+#ifdef CONFIG_KSZ_MRP
+		if (sw->features & MRP_SUPPORT) {
+			struct mrp_info *mrp = &sw->mrp;
+
+			mrp->ops->setup_vlan(mrp, sw->vlan_index, entry);
+		}
+#endif
+		sw_w_vlan_table(sw, sw->vlan_index, entry);
+		entry->ports = ports;
+		entry->untag = untag;
+		sw->vlan_dirty = 0;
+		sw->overrides |= VLAN_SET;
+		break;
+	case PROC_SET_VLAN_PORTS:
+		if (0 <= num && num <= sw->PORT_MASK) {
+			entry->ports = num;
+			entry->dirty = 1;
+		}
+		break;
+	case PROC_SET_VLAN_UNTAG:
+		if (0 <= num && num <= sw->PORT_MASK) {
+			entry->untag = num;
+			entry->dirty = 1;
+		}
+		break;
+	case PROC_SET_VLAN_FID:
+		if (0 <= num && num < FID_ENTRIES) {
+			entry->fid = num;
+			entry->dirty = 1;
+		}
+		break;
+	case PROC_SET_VLAN_VID:
+		if (0 <= num && num < 0x1000) {
+			sw->vlan_index = num;
+			sw->vlan_dirty = 1;
+		}
+		break;
+	case PROC_SET_VLAN_MSTP:
+		if (0 <= num && num <= VLAN_MSTP_M) {
+			entry->mstp = num;
+			entry->dirty = 1;
+		}
+		break;
+	case PROC_SET_VLAN_PRIO:
+		if (0 <= num && num <= VLAN_PRIO_M) {
+			entry->prio = num;
+			entry->dirty = 1;
+		}
+		break;
+	case PROC_SET_VLAN_OPTION:
+		if (num)
+			entry->option = 1;
+		else
+			entry->option = 0;
+		entry->dirty = 1;
+		break;
+	case PROC_SET_VLAN_INFO:
+		sw->vlan_dirty = 1;
+		entry->dirty = 0;
+		break;
+	case PROC_SET_VID_2_FID:
+		if (0 <= num && num < FID_ENTRIES) {
+			sw->info->vid2fid[sw->vlan_index] = num;
+			sw->info->fid_updated = 1;
+		}
+		break;
+	case PROC_SET_FID_2_MSTID:
+		if (0 <= num && num < NUM_OF_MSTI) {
+			sw->info->fid2mstid[entry->fid] = num;
+			sw->info->fid_updated = 1;
+		}
+		break;
+	default:
+		processed = false;
+		break;
+	}
+	return processed;
+}  /* sysfs_vlan_write */
+
+#ifdef CONFIG_KSZ_HSR
+static ssize_t sysfs_hsr_read(struct ksz_sw *sw, int proc_num, ssize_t len,
+	char *buf)
+{
+	struct ksz_hsr_table *entry;
+	u8 mac_addr[ETH_ALEN];
+	u8 path_id;
+	struct ksz_hsr_info *info = &sw->info->hsr;
+
+	entry = &sw->info->hsr_entry;
+	switch (proc_num) {
+	case PROC_SET_HSR_VALID:
+		len += sprintf(buf + len, "%u"NL, entry->valid);
+		break;
+	case PROC_SET_HSR_AGE_CNT:
+		len += sprintf(buf + len, "%u"NL, entry->age_cnt);
+		break;
+	case PROC_SET_HSR_PATH_ID:
+		len += sprintf(buf + len, "%x"NL, entry->path_id >> 1);
+		break;
+	case PROC_SET_HSR_SRC_MAC_ADDR:
+		len += sprintf(buf + len, "%02x:%02x:%02x:%02x:%02x:%02x"NL,
+			entry->src_mac[0], entry->src_mac[1],
+			entry->src_mac[2], entry->src_mac[3],
+			entry->src_mac[4], entry->src_mac[5]);
+		break;
+	case PROC_SET_HSR_INDEX:
+		len += sprintf(buf + len, "0x%03x"NL, sw->hsr_index);
+		break;
+	case PROC_SET_HSR_INFO:
+
+		/* MAC address and path ID will be wiped out if emtpy entry. */
+		memcpy(mac_addr, entry->src_mac, ETH_ALEN);
+		path_id = entry->path_id;
+		if (sw->hsr_dirty) {
+			sw_r_hsr_table(sw, sw->hsr_index, entry);
+			sw->hsr_dirty = 0;
+		}
+		len += sprintf(buf + len,
+			"%3x: %02X:%02X:%02X:%02X:%02X:%02X - %x "
+			" c:%u %04x:%04x %04x:%04x %04x:%04x [%u]"NL,
+			sw->hsr_index,
+			entry->src_mac[0], entry->src_mac[1], entry->src_mac[2],
+			entry->src_mac[3], entry->src_mac[4], entry->src_mac[5],
+			entry->path_id >> 1, entry->age_cnt,
+			entry->start_seq[0], entry->start_seq[1],
+			entry->exp_seq[0], entry->exp_seq[1],
+			entry->seq_cnt[0], entry->seq_cnt[1],
+			entry->dirty ? 2 : entry->valid);
+
+		memcpy(entry->src_mac, mac_addr, ETH_ALEN);
+		entry->path_id = path_id;
+		break;
+	case PROC_GET_HSR_STATE:
+		len += sprintf(buf + len,
+			"%u %u:%u %u:%u"NL, info->ring,
+			info->p1_down, info->p2_down,
+			info->p1_lost, info->p2_lost);
+		break;
+	}
+	return len;
+}  /* sysfs_hsr_read */
+
+static int sysfs_hsr_write(struct ksz_sw *sw, int proc_num, int num,
+	const char *buf)
+{
+	struct ksz_hsr_table *entry;
+	int processed = true;
+
+	entry = &sw->info->hsr_entry;
+	switch (proc_num) {
+	case PROC_SET_HSR_PATH_ID:
+		if (0 <= num && num <= 7) {
+			entry->path_id = num << 1;
+			if (!sw->hsr_index)
+				sw->hsr_dirty = 1;
+		}
+		break;
+	case PROC_SET_HSR_AGE_CNT:
+		if (0 <= num && num <= 3) {
+			entry->age_cnt = num;
+			entry->dirty = 1;
+		}
+		break;
+	case PROC_SET_HSR_VALID:
+		if (num)
+			entry->valid = 1;
+		else
+			entry->valid = 0;
+		sw_w_hsr_table(sw, sw->hsr_index, entry);
+		break;
+	case PROC_SET_HSR_SRC_MAC_ADDR:
+	{
+		int i;
+		int n[6];
+
+		i = sscanf(buf, "%x:%x:%x:%x:%x:%x",
+			&n[0], &n[1], &n[2], &n[3], &n[4], &n[5]);
+		if (6 == i) {
+			for (i = 0; i < 6; i++)
+				entry->src_mac[i] = (u8) n[i];
+			if (!sw->hsr_index)
+				sw->hsr_dirty = 1;
+		}
+		break;
+	}
+	case PROC_SET_HSR_INDEX:
+		if (0 <= num && num < 0x200) {
+			sw->hsr_index = num;
+			sw->hsr_dirty = 1;
+		}
+		break;
+	case PROC_SET_HSR_INFO:
+		sw->hsr_dirty = 1;
+		entry->dirty = 0;
+		break;
+	default:
+		processed = false;
+		break;
+	}
+	return processed;
+}  /* sysfs_hsr_write */
+#endif
+
+static ssize_t sysfs_acl_read(struct ksz_sw *sw, int proc_num, uint n,
+	ssize_t len, char *buf)
+{
+	struct ksz_port_cfg *cfg;
+	struct ksz_acl_table *acl;
+	struct ksz_acl_table *action;
+	struct ksz_acl_table *ruleset;
+	uint port;
+	int chk = 0;
+	int type = SHOW_HELP_NONE;
+	char note[40];
+
+	note[0] = '\0';
+	port = get_sysfs_port(sw, n);
+	cfg = get_port_cfg(sw, port);
+	acl = &cfg->acl_info[cfg->acl_index];
+	action = &cfg->acl_info[cfg->acl_act_index];
+	ruleset = &cfg->acl_info[cfg->acl_rule_index];
+	switch (proc_num) {
+	case PROC_SET_ACL_FIRST_RULE:
+		chk = ruleset->first_rule;
+		type = SHOW_HELP_HEX;
+		break;
+	case PROC_SET_ACL_RULESET:
+		len += acl_ruleset_info(ruleset, cfg->acl_rule_index, buf, len);
+		break;
+	case PROC_SET_ACL_MODE:
+		chk = acl->mode;
+		if (sw->verbose) {
+			switch (chk) {
+			case 1:
+				strcpy(note, " (layer 2)");
+				break;
+			case 2:
+				strcpy(note, " (layer 3)");
+				break;
+			case 3:
+				strcpy(note, " (layer 4)");
+				break;
+			default:
+				strcpy(note, " (off)");
+			}
+		}
+		type = SHOW_HELP_SPECIAL;
+		break;
+	case PROC_SET_ACL_ENABLE:
+		chk = acl->enable;
+		if (sw->verbose) {
+			switch (chk) {
+			case 1:
+				strcpy(note, " (type; ip; tcp port)");
+				break;
+			case 2:
+				strcpy(note, " (mac; src/dst; udp port)");
+				break;
+			case 3:
+				strcpy(note, " (both; -; tcp seq)");
+				break;
+			default:
+				strcpy(note, " (count; -; protocol)");
+			}
+		}
+		type = SHOW_HELP_SPECIAL;
+		break;
+	case PROC_SET_ACL_SRC:
+		chk = acl->src;
+		type = SHOW_HELP_ON_OFF;
+		break;
+	case PROC_SET_ACL_EQUAL:
+		chk = acl->equal;
+		type = SHOW_HELP_ON_OFF;
+		break;
+	case PROC_SET_ACL_PRIO_MODE:
+		chk = action->prio_mode;
+		if (sw->verbose) {
+			switch (chk) {
+			case 1:
+				strcpy(note, " (higher)");
+				break;
+			case 2:
+				strcpy(note, " (lower)");
+				break;
+			case 3:
+				strcpy(note, " (replace)");
+				break;
+			default:
+				strcpy(note, " (disabled)");
+			}
+		}
+		type = SHOW_HELP_SPECIAL;
+		break;
+	case PROC_SET_ACL_PRIO:
+		chk = action->prio;
+		type = SHOW_HELP_NUM;
+		break;
+	case PROC_SET_ACL_VLAN_PRIO_REPLACE:
+		chk = action->vlan_prio_replace;
+		type = SHOW_HELP_ON_OFF;
+		break;
+	case PROC_SET_ACL_VLAN_PRIO:
+		chk = action->vlan_prio;
+		type = SHOW_HELP_NUM;
+		break;
+	case PROC_SET_ACL_MAP_MODE:
+		chk = action->map_mode;
+		if (sw->verbose) {
+			switch (chk) {
+			case 1:
+				strcpy(note, " (or)");
+				break;
+			case 2:
+				strcpy(note, " (and)");
+				break;
+			case 3:
+				strcpy(note, " (replace)");
+				break;
+			default:
+				strcpy(note, " (disabled)");
+			}
+		}
+		type = SHOW_HELP_SPECIAL;
+		break;
+	case PROC_SET_ACL_PORTS:
+		chk = action->ports;
+		chk = get_log_mask_from_phy(sw, chk);
+		type = SHOW_HELP_HEX_4;
+		break;
+	case PROC_SET_ACL_MAC_ADDR:
+		len += sprintf(buf + len, "%02x:%02x:%02x:%02x:%02x:%02x"NL,
+			acl->mac[0], acl->mac[1], acl->mac[2],
+			acl->mac[3], acl->mac[4], acl->mac[5]);
+		break;
+	case PROC_SET_ACL_TYPE:
+		chk = acl->eth_type;
+		type = SHOW_HELP_HEX_4;
+		break;
+	case PROC_SET_ACL_CNT:
+		chk = acl->cnt;
+		type = SHOW_HELP_NUM;
+		break;
+	case PROC_SET_ACL_MSEC:
+		chk = acl->msec;
+		type = SHOW_HELP_ON_OFF;
+		break;
+	case PROC_SET_ACL_INTR_MODE:
+		chk = acl->intr_mode;
+		type = SHOW_HELP_ON_OFF;
+		break;
+	case PROC_SET_ACL_IP_ADDR:
+		len += sprintf(buf + len, "%u.%u.%u.%u"NL,
+			acl->ip4_addr[0], acl->ip4_addr[1],
+			acl->ip4_addr[2], acl->ip4_addr[3]);
+		break;
+	case PROC_SET_ACL_IP_MASK:
+		len += sprintf(buf + len, "%u.%u.%u.%u"NL,
+			acl->ip4_mask[0], acl->ip4_mask[1],
+			acl->ip4_mask[2], acl->ip4_mask[3]);
+		break;
+	case PROC_SET_ACL_PROTOCOL:
+		chk = acl->protocol;
+		type = SHOW_HELP_NUM;
+		break;
+	case PROC_SET_ACL_PORT_MODE:
+		chk = acl->port_mode;
+		if (sw->verbose) {
+			switch (chk) {
+			case 1:
+				strcpy(note, " (either)");
+				break;
+			case 2:
+				strcpy(note, " (in range)");
+				break;
+			case 3:
+				strcpy(note, " (out of range)");
+				break;
+			default:
+				strcpy(note, " (disabled)");
+			}
+		}
+		type = SHOW_HELP_SPECIAL;
+		break;
+	case PROC_SET_ACL_MAX_PORT:
+		chk = acl->max_port;
+		type = SHOW_HELP_NUM;
+		break;
+	case PROC_SET_ACL_MIN_PORT:
+		chk = acl->min_port;
+		type = SHOW_HELP_NUM;
+		break;
+	case PROC_SET_ACL_SEQNUM:
+		chk = acl->seqnum;
+		type = SHOW_HELP_NUM;
+		break;
+	case PROC_SET_ACL_TCP_FLAG_ENABLE:
+		chk = acl->tcp_flag_enable;
+		type = SHOW_HELP_ON_OFF;
+		break;
+	case PROC_SET_ACL_TCP_FLAG:
+		chk = acl->tcp_flag;
+		type = SHOW_HELP_HEX;
+		break;
+	case PROC_SET_ACL_TCP_FLAG_MASK:
+		chk = acl->tcp_flag_mask;
+		type = SHOW_HELP_HEX;
+		break;
+	case PROC_SET_ACL_INDEX:
+		chk = cfg->acl_index;
+		type = SHOW_HELP_HEX;
+		break;
+	case PROC_SET_ACL_ACTION_INDEX:
+		chk = cfg->acl_act_index;
+		type = SHOW_HELP_HEX;
+		break;
+	case PROC_SET_ACL_ACTION:
+		len += acl_action_info(sw, action, cfg->acl_act_index, buf,
+				       len);
+		break;
+	case PROC_SET_ACL_RULE_INDEX:
+		chk = cfg->acl_rule_index;
+		type = SHOW_HELP_HEX;
+		break;
+	case PROC_SET_ACL_INFO:
+		len += acl_info(acl, cfg->acl_index, buf, len);
+		break;
+	case PROC_GET_ACL_TABLE:
+		len = sw_d_acl_table(sw, port, buf, len);
+		break;
+	}
+	return sysfs_show(len, buf, type, chk, note, sw->verbose);
+}  /* sysfs_acl_read */
+
+static int sysfs_acl_write(struct ksz_sw *sw, int proc_num, uint n, int num,
+	const char *buf)
+{
+	struct ksz_port_cfg *cfg;
+	struct ksz_acl_table *acl;
+	struct ksz_acl_table *action;
+	struct ksz_acl_table *ruleset;
+	uint port;
+	int acl_on = 0;
+	int processed = true;
+
+	port = get_sysfs_port(sw, n);
+	cfg = get_port_cfg(sw, port);
+	acl = &cfg->acl_info[cfg->acl_index];
+	action = &cfg->acl_info[cfg->acl_act_index];
+	ruleset = &cfg->acl_info[cfg->acl_rule_index];
+	switch (proc_num) {
+	case PROC_SET_ACL_RULESET:
+	case PROC_SET_ACL_MODE:
+	case PROC_SET_ACL_ACTION:
+	case PROC_SET_ACL_INFO:
+		sw->ops->acquire(sw);
+		acl_on = port_chk_acl(sw, port);
+		if (!acl_on)
+			port_cfg_acl(sw, port, true);
+		sw->ops->release(sw);
+		break;
+	}
+	switch (proc_num) {
+	case PROC_SET_ACL_FIRST_RULE:
+		ruleset->first_rule = (u16) num;
+		ruleset->ruleset_changed = 1;
+		break;
+	case PROC_SET_ACL_RULESET:
+		sscanf(buf, "%x", &num);
+		ruleset->ruleset = (u16) num;
+		sw_w_acl_ruleset(sw, port, cfg->acl_rule_index, ruleset);
+		break;
+	case PROC_SET_ACL_MODE:
+		if (0 <= num && num < 4) {
+			acl->mode = num;
+			sw_w_acl_rule(sw, port, cfg->acl_index, acl);
+		}
+		break;
+	case PROC_SET_ACL_ENABLE:
+		if (0 <= num && num < 4) {
+			acl->enable = num;
+			acl->changed = 1;
+		}
+		break;
+	case PROC_SET_ACL_SRC:
+		if (num)
+			acl->src = 1;
+		else
+			acl->src = 0;
+		acl->changed = 1;
+		break;
+	case PROC_SET_ACL_EQUAL:
+		if (num)
+			acl->equal = 1;
+		else
+			acl->equal = 0;
+		acl->changed = 1;
+		break;
+	case PROC_SET_ACL_PRIO_MODE:
+		if (0 <= num && num < 4) {
+			action->prio_mode = num;
+			action->action_changed = 1;
+		}
+		break;
+	case PROC_SET_ACL_PRIO:
+		if (0 <= num && num <= KS_PRIO_M) {
+			action->prio = num;
+			action->action_changed = 1;
+		}
+		break;
+	case PROC_SET_ACL_VLAN_PRIO_REPLACE:
+		if (num)
+			action->vlan_prio_replace = 1;
+		else
+			action->vlan_prio_replace = 0;
+		action->action_changed = 1;
+		break;
+	case PROC_SET_ACL_VLAN_PRIO:
+		if (0 <= num && num <= KS_PRIO_M) {
+			action->vlan_prio = num;
+			action->action_changed = 1;
+		}
+		break;
+	case PROC_SET_ACL_MAP_MODE:
+		if (0 <= num && num < 4) {
+			action->map_mode = num;
+			action->action_changed = 1;
+		}
+		break;
+	case PROC_SET_ACL_PORTS:
+		sscanf(buf, "%x", &num);
+		if (0 <= num && num <= sw->PORT_MASK) {
+			num = get_phy_mask_from_log(sw, num);
+			action->ports = (u16) num;
+			action->action_changed = 1;
+		}
+		break;
+	case PROC_SET_ACL_MAC_ADDR:
+	{
+		int i;
+		int n[6];
+
+		i = sscanf(buf, "%x:%x:%x:%x:%x:%x",
+			&n[0], &n[1], &n[2], &n[3], &n[4], &n[5]);
+		if (6 == i) {
+			for (i = 0; i < 6; i++)
+				acl->mac[i] = (u8) n[i];
+			acl->changed = 1;
+		}
+		break;
+	}
+	case PROC_SET_ACL_TYPE:
+		sscanf(buf, "%x", &num);
+		acl->eth_type = (u16) num;
+		acl->changed = 1;
+		break;
+	case PROC_SET_ACL_CNT:
+		if (0 <= num && num <= ACL_CNT_M) {
+			acl->cnt = (u16) num;
+			acl->changed = 1;
+		}
+		break;
+	case PROC_SET_ACL_MSEC:
+		if (num)
+			acl->msec = 1;
+		else
+			acl->msec = 0;
+		acl->changed = 1;
+		break;
+	case PROC_SET_ACL_INTR_MODE:
+		if (num)
+			acl->intr_mode = 1;
+		else
+			acl->intr_mode = 0;
+		acl->changed = 1;
+		break;
+	case PROC_SET_ACL_IP_ADDR:
+	{
+		int i;
+		int n[4];
+
+		i = sscanf(buf, "%u.%u.%u.%u",
+			&n[0], &n[1], &n[2], &n[3]);
+		if (4 == i) {
+			for (i = 0; i < 4; i++)
+				acl->ip4_addr[i] = (u8) n[i];
+			acl->changed = 1;
+		}
+		break;
+	}
+	case PROC_SET_ACL_IP_MASK:
+	{
+		int i;
+		int n[4];
+
+		i = sscanf(buf, "%u.%u.%u.%u",
+			&n[0], &n[1], &n[2], &n[3]);
+		if (4 == i) {
+			for (i = 0; i < 4; i++)
+				acl->ip4_mask[i] = (u8) n[i];
+			acl->changed = 1;
+		}
+		break;
+	}
+	case PROC_SET_ACL_PROTOCOL:
+		acl->protocol = (u8) num;
+		acl->changed = 1;
+		break;
+	case PROC_SET_ACL_PORT_MODE:
+		if (0 <= num && num < 4) {
+			acl->port_mode = num;
+			acl->changed = 1;
+		}
+		break;
+	case PROC_SET_ACL_MAX_PORT:
+		acl->max_port = (u16) num;
+		acl->changed = 1;
+		break;
+	case PROC_SET_ACL_MIN_PORT:
+		acl->min_port = (u16) num;
+		acl->changed = 1;
+		break;
+	case PROC_SET_ACL_SEQNUM:
+		acl->seqnum = num;
+		acl->changed = 1;
+		break;
+	case PROC_SET_ACL_TCP_FLAG_ENABLE:
+		if (num)
+			acl->tcp_flag_enable = 1;
+		else
+			acl->tcp_flag_enable = 0;
+		acl->changed = 1;
+		break;
+	case PROC_SET_ACL_TCP_FLAG:
+		acl->tcp_flag = (u8) num;
+		acl->changed = 1;
+		break;
+	case PROC_SET_ACL_TCP_FLAG_MASK:
+		acl->tcp_flag_mask = (u8) num;
+		acl->changed = 1;
+		break;
+	case PROC_SET_ACL_INDEX:
+		if (0 <= num && num < ACL_TABLE_ENTRIES) {
+			cfg->acl_index = num;
+		}
+		break;
+	case PROC_SET_ACL_ACTION_INDEX:
+		if (0 <= num && num < ACL_TABLE_ENTRIES) {
+			cfg->acl_act_index = num;
+		}
+		break;
+	case PROC_SET_ACL_ACTION:
+		if (num)
+			sw_w_acl_action(sw, port, cfg->acl_act_index, action);
+		else
+			sw_r_acl_table(sw, port, cfg->acl_act_index, action);
+		break;
+	case PROC_SET_ACL_RULE_INDEX:
+		if (0 <= num && num < ACL_TABLE_ENTRIES) {
+			cfg->acl_rule_index = num;
+		}
+		break;
+	case PROC_SET_ACL_INFO:
+		sw_r_acl_table(sw, port, cfg->acl_index, acl);
+		break;
+	default:
+		processed = false;
+		break;
+	}
+	switch (proc_num) {
+	case PROC_SET_ACL_RULESET:
+	case PROC_SET_ACL_MODE:
+	case PROC_SET_ACL_ACTION:
+	case PROC_SET_ACL_INFO:
+		if (!acl_on) {
+			sw->ops->acquire(sw);
+			port_cfg_acl(sw, port, false);
+			sw->ops->release(sw);
+		}
+		break;
+	}
+	return processed;
+}  /* sysfs_acl_write */
+
+static void sw_cfg_mac(struct ksz_sw *sw, u8 index, const u8 *dest, u32 ports,
+	int override, int use_fid, u16 fid)
+{
+	struct ksz_mac_table mac;
+
+	memset(&mac, 0, sizeof(struct ksz_mac_table));
+	memcpy(mac.addr, dest, ETH_ALEN);
+	mac.ports = ports & sw->PORT_MASK;
+	mac.override = override;
+	mac.use_fid = use_fid;
+	mac.fid = fid;
+	mac.valid = ports != 0;
+	if (!mac.valid && mac.override) {
+		mac.override = 0;
+		mac.valid = 1;
+	}
+	sw_w_dyn_mac_table(sw, 0, mac.addr, mac.fid, &mac);
+}  /* sw_cfg_mac */
+
+static void sw_cfg_vlan(struct ksz_sw *sw, u8 index, u16 vid, u16 fid,
+	u32 ports)
+{
+	struct ksz_vlan_table vlan;
+
+	if (0xffff == ports)
+		ports = sw->PORT_MASK;
+	if (sw_r_vlan_table(sw, vid, &vlan)) {
+		memset(&vlan, 0, sizeof(struct ksz_vlan_table));
+		vlan.vid = vid;
+		vlan.fid = fid;
+	}
+	vlan.ports = ports & sw->PORT_MASK;
+	vlan.valid = ports != 0;
+	sw_w_vlan_table(sw, vlan.vid, &vlan);
+}  /* sw_cfg_vlan */
+
+static u8 sw_alloc_mac(struct ksz_sw *sw)
+{
+	return 1;
+}  /* sw_alloc_mac */
+
+static void sw_free_mac(struct ksz_sw *sw, u8 index)
+{
+}  /* sw_free_mac */
+
+static u8 sw_alloc_vlan(struct ksz_sw *sw)
+{
+	return 1;
+}  /* sw_alloc_vlan */
+
+static void sw_free_vlan(struct ksz_sw *sw, u8 index)
+{
+}  /* sw_free_vlan */
+
+static u16 sw_alloc_fid(struct ksz_sw *sw, u16 vid)
+{
+#if 0
+	int x;
+	int y;
+	u16 fid;
+
+	if (sw->info->fid_cnt + 2 == FID_ENTRIES)
+		return 0;
+	fid = vid & (FID_ENTRIES - 1);
+	if (vid < 2)
+		fid = 1000;
+	x = fid / FID_IN_DATA;
+	y = fid % FID_IN_DATA;
+	while (sw->info->fid[x] & (1 << y)) {
+		++fid;
+		++y;
+		if (y >= FID_IN_DATA) {
+			y = 0;
+			++x;
+		}
+	}
+	sw->info->fid[x] |= (1 << y);
+	++sw->info->fid_cnt;
+	return fid;
+#else
+	return sw->info->vid2fid[vid];
+#endif
+}  /* sw_alloc_fid */
+
+static void sw_free_fid(struct ksz_sw *sw, u16 fid)
+{
+#if 0
+	int x;
+	int y;
+
+	x = fid / FID_IN_DATA;
+	y = fid % FID_IN_DATA;
+	if (sw->info->fid[x] & (1 << y)) {
+		sw->info->fid[x] &= ~(1 << y);
+		--sw->info->fid_cnt;
+	}
+#endif
+}  /* sw_free_fid */
+
+static const u8 *sw_get_br_id(struct ksz_sw *sw)
+{
+	static u8 id[8];
+	const u8* ret = id;
+
+	memcpy(&id[2], sw->info->mac_addr, ETH_ALEN);
+	id[0] = 0x80;
+	id[1] = 0x00;
+
+#ifdef CONFIG_KSZ_STP
+	ret = stp_br_id(&sw->info->rstp);
+#endif
+	return ret;
+}  /* sw_get_br_id */
+
+static void sw_from_backup(struct ksz_sw *sw, uint p)
+{
+#ifdef CONFIG_KSZ_MRP
+	if (sw->features & MRP_SUPPORT) {
+		struct mrp_info *mrp = &sw->mrp;
+
+		mrp->ops->from_backup(mrp, p);
+	}
+#endif
+}  /* sw_from_backup */
+
+static void sw_to_backup(struct ksz_sw *sw, uint p)
+{
+#ifdef CONFIG_KSZ_MRP
+	if (sw->features & MRP_SUPPORT) {
+		struct mrp_info *mrp = &sw->mrp;
+
+		mrp->ops->to_backup(mrp, p);
+	}
+#endif
+}  /* sw_to_backup */
+
+static void sw_from_designated(struct ksz_sw *sw, uint p, bool alt)
+{
+#ifdef CONFIG_KSZ_MRP
+	if (sw->features & MRP_SUPPORT) {
+		struct mrp_info *mrp = &sw->mrp;
+
+		mrp->ops->from_designated(mrp, p, alt);
+	}
+#endif
+}  /* sw_from_designated */
+
+static void sw_to_designated(struct ksz_sw *sw, uint p)
+{
+#ifdef CONFIG_KSZ_MRP
+	if (sw->features & MRP_SUPPORT) {
+		struct mrp_info *mrp = &sw->mrp;
+
+		mrp->ops->to_designated(mrp, p);
+	}
+#endif
+}  /* sw_to_designated */
+
+static void sw_tc_detected(struct ksz_sw *sw, uint p)
+{
+#ifdef CONFIG_KSZ_MRP
+	if (sw->features & MRP_SUPPORT) {
+		struct mrp_info *mrp = &sw->mrp;
+
+		mrp->ops->tc_detected(mrp, p);
+	}
+#endif
+}  /* sw_tc_detected */
+
+static int sw_get_tcDetected(struct ksz_sw *sw, uint p)
+{
+	int ret = false;
+
+#ifdef CONFIG_KSZ_STP
+	if (sw->features & STP_SUPPORT) {
+		struct ksz_stp_info *info = &sw->info->rstp;
+
+		ret = info->ops->get_tcDetected(info, p);
+	}
+#endif
+	return ret;
+}  /* sw_get_tcDetected */
+
+enum {
+	KSZ9897_SW_CHIP,
+	KSZ9567_SW_CHIP,
+	KSZ9477_SW_CHIP,
+	KSZ9896_SW_CHIP,
+	KSZ9566_SW_CHIP,
+	KSZ8567_SW_CHIP,
+	KSZ8565_SW_CHIP,
+	KSZ9893_SW_CHIP,
+	KSZ9563_SW_CHIP,
+	KSZ8563_SW_CHIP,
+	LAN9646_SW_CHIP,
+	LAST_SW_CHIP
+};
+
+static void sw_cfg_src_filter(struct ksz_sw *sw, bool set)
+{
+	uint m;
+	uint n;
+	uint p;
+
+	if (!(sw->features & NEW_CAP))
+		return;
+	for (n = 1; n <= sw->mib_port_cnt; n++) {
+		p = get_phy_port(sw, n);
+		m = get_phy_mask(sw, n);
+		if (!set)
+			sw->open_ports |= m;
+		port_cfg(sw, p, REG_PORT_LUE_CTRL, PORT_SRC_ADDR_FILTER, set);
+		if (set)
+			sw->open_ports &= ~m;
+	}
+}  /* sw_cfg_src_filter */
+
+static void sw_fwd_unk_mcast(struct ksz_sw *sw, bool set)
+{
+	if (set) {
+		sw->reg->w32(sw, REG_SW_LUE_UNK_MCAST_CTRL__4,
+			sw->HOST_MASK | SW_UNK_MCAST_ENABLE);
+		sw->overrides |= UNK_MCAST_BLOCK;
+	} else {
+		sw_cfg(sw, REG_SW_LUE_UNK_MCAST_CTRL__4,
+			SW_UNK_MCAST_ENABLE >> 24, false);
+		sw->overrides &= ~UNK_MCAST_BLOCK;
+	}
+}  /* sw_fwd_unk_mcast */
+
+static void sw_fwd_unk_ucast(struct ksz_sw *sw)
+{
+	sw->reg->w32(sw, REG_SW_LUE_UNK_UCAST_CTRL__4,
+		sw->HOST_MASK | SW_UNK_UCAST_ENABLE);
+}  /* sw_fwd_unk_ucast */
+
+static void sw_fwd_unk_vid(struct ksz_sw *sw)
+{
+	sw->reg->w32(sw, REG_SW_LUE_UNK_VID_CTRL__4,
+		sw->HOST_MASK | SW_UNK_VID_ENABLE);
+}  /* sw_fwd_unk_vid */
+
+static int sw_need_dest(struct ksz_sw *sw, u8 *addr)
+{
+	int need = 0;
+
+	if (addr[0] & 0x01) {
+		int i;
+
+#ifdef USE_UNK_MCAST
+		struct ksz_mac_table *entry;
+		struct ksz_alu_table *alu;
+
+		for (i = MULTI_MAC_TABLE_ENTRIES - 1; i >= 0; i--) {
+			alu = &sw->info->alu_table[i];
+			entry = &sw->info->mac_table[i];
+			if (alu->valid &&
+			    !memcmp(addr, entry->addr, ETH_ALEN)) {
+				if (alu->forward & FWD_HOST)
+					need = 1;
+				if (alu->forward & FWD_HOST_OVERRIDE)
+					need = 2;
+				break;
+			}
+		}
+		if (i < 0 && (sw->overrides & UNK_MCAST_BLOCK))
+			need = 1;
+#else
+		if (!memcmp(addr, sw->info->reserved_table[0].addr, 5) &&
+		    addr[5] < RESERVED_MCAST_TABLE_ENTRIES) {
+			i = mcast_reserved_map[addr[5]];
+			if (sw->info->reserved_table[i].ports ==
+                            sw->HOST_MASK) {
+				need = 1;
+				if (sw->info->reserved_table[i].override)
+					need = 2;
+			}
+		}
+#endif
+	} else if (!memcmp(addr, sw->info->mac_addr, ETH_ALEN))
+		need = 1;
+	return need;
+}  /* sw_need_dest */
+
+static void sw_forward(struct ksz_sw *sw, u8 *addr, u8 *self, u16 proto,
+	int tag)
+{
+	int forward = 0;
+
+	/* Already set for PTP message. */
+	if (sw->info->forward)
+		return;
+
+	/* Check for multicast addresses that are not forwarding. */
+	if (addr[0] & 0x01) {
+		int i;
+		struct ksz_mac_table *entry;
+		struct ksz_alu_table *alu;
+
+		for (i = MULTI_MAC_TABLE_ENTRIES - 1; i >= 0; i--) {
+			alu = &sw->info->alu_table[i];
+			entry = &sw->info->mac_table[i];
+			if (alu->valid &&
+			    !memcmp(addr, entry->addr, ETH_ALEN)) {
+				forward = alu->forward;
+				if (!(forward & FWD_VLAN_DEV)) {
+					if (proto == 0x888E)
+						forward = FWD_STP_DEV |
+							  FWD_VLAN_DEV;
+				}
+				break;
+			}
+		}
+		if (!forward)
+			forward = FWD_MAIN_DEV | FWD_MCAST;
+
+	/* Check unicast address to host. */
+	} else if (!memcmp(addr, self, ETH_ALEN))
+		forward = FWD_HOST;
+	else
+		forward = FWD_MAIN_DEV | FWD_UCAST;
+	if (!tag)
+		forward &= ~FWD_VLAN_DEV;
+	sw->info->forward = forward;
+}  /* sw_forward */
+
+static void sw_tx_fwd(struct work_struct *work)
+{
+	int rc;
+	bool last;
+	struct sk_buff *skb;
+	struct ksz_sw *sw = container_of(work, struct ksz_sw, tx_fwd);
+	const struct net_device_ops *ops = sw->main_dev->netdev_ops;
+
+#ifdef CONFIG_KSZ_IBA
+	if (iba_stopped(sw))
+		return;
+#endif
+
+	last = skb_queue_empty(&sw->txq);
+	while (!last) {
+		skb = skb_dequeue(&sw->txq);
+		last = skb_queue_empty(&sw->txq);
+		if (!skb)
+			continue;
+		do {
+			rc = ops->ndo_start_xmit(skb, skb->dev);
+			if (NETDEV_TX_BUSY == rc) {
+				rc = wait_event_interruptible_timeout(sw->queue,
+					!netif_queue_stopped(sw->main_dev),
+					50 * HZ / 1000);
+
+				rc = NETDEV_TX_BUSY;
+			}
+		} while (NETDEV_TX_BUSY == rc);
+	}
+}  /* sw_tx_fwd */
+
+#if 1
+static u8 last_addr[6];
+#endif
+
+static struct net_device *sw_rx_dev(struct ksz_sw *sw, u8 *data, u32 *length,
+	int *tag, int *port)
+{
+	u16 proto;
+	u16* proto_loc;
+	struct net_device *dev;
+	struct ethhdr *eth = (struct ethhdr *) data;
+	struct vlan_ethhdr *vlan = (struct vlan_ethhdr *) data;
+	int index = -1;
+	int vid = 0;
+	u16 prio = 0;
+	size_t len = *length;
+
+	proto_loc = &vlan->h_vlan_proto;
+	proto = ntohs(*proto_loc);
+
+	/* Ignore PAUSE frame sent by switch. */
+	if (!memcmp(eth->h_source, sw->info->mac_addr, ETH_ALEN) &&
+	    proto == ETH_P_PAUSE)
+		return NULL;
+	if (eth->h_proto == htons(ETH_P_8021Q)) {
+		u16 vlan_tci = ntohs(vlan->h_vlan_TCI);
+
+		vid = vlan_tci & VLAN_VID_MASK;
+		prio = vlan_tci & VLAN_PRIO_MASK;
+		prio >>= VLAN_PRIO_SHIFT;
+		proto_loc = &vlan->h_vlan_encapsulated_proto;
+		proto = ntohs(*proto_loc);
+#if 0
+dbg_msg(" 1 vid: %x %d %04x"NL, vlan_tci, vid, proto);
+#endif
+		if (vlan->h_vlan_encapsulated_proto == htons(ETH_P_8021Q)) {
+			unsigned char *ptr = (unsigned char *) vlan;
+			u16 vlan_tci = ntohs(vlan->h_vlan_TCI);
+
+			ptr += VLAN_HLEN;
+dbg_msg(" 2 vid: %x"NL, vlan_tci);
+		}
+	}
+#ifdef CONFIG_KSZ_HSR
+	if (proto == ETH_P_HSR) {
+		proto_loc += HSR_HLEN / 2;
+		proto = ntohs(*proto_loc);
+	}
+#endif
+
+#if 0
+/*
+ * THa  2016/02/03
+ * A company switch is sending frames that causes the dropped count to
+ * increase.
+ */
+	if (proto == 0x8874 &&
+	    0x01 == vlan->h_source[0] &&
+	    0x80 == vlan->h_source[1] &&
+	    0xc2 == vlan->h_source[2] &&
+	    0xff == vlan->h_dest[0]) {
+		return NULL;
+	}
+#endif
+	if (eth->h_proto == htons(0x9100)) {
+		u16 vlan_tci = ntohs(vlan->h_vlan_TCI);
+
+dbg_msg(" 1 isp: %x"NL, vlan_tci);
+		if (vlan->h_vlan_encapsulated_proto == htons(ETH_P_8021Q)) {
+			unsigned char *ptr = (unsigned char *) vlan;
+			u16 vlan_tci = ntohs(vlan->h_vlan_TCI);
+
+			ptr += VLAN_HLEN;
+			vlan_tci = ntohs(vlan->h_vlan_TCI);
+dbg_msg(" 2 vid: %x"NL, vlan_tci);
+		}
+	}
+	sw->info->forward = 0;
+
+	/* Get received port number. */
+	if (sw->overrides & TAIL_TAGGING) {
+		u8 tail;
+
+		len--;
+		tail = data[len];
+		sw->tag.timestamp = 0;
+		sw->tag.ports = tail;
+		if (tail & TAIL_TAG_PTP) {
+			u32 rx_ts;
+
+			memcpy(&rx_ts, &data[len - 4], 4);
+			rx_ts = ntohl(rx_ts);
+			sw->tag.timestamp = rx_ts;
+			len -= 4;
+
+#ifdef CONFIG_1588_PTP
+			/* PTP message cannot be forwarded normally. */
+			if (sw->features & PTP_HW) {
+				struct ptp_info *ptp = &sw->ptp_hw;
+
+				sw->info->forward = ptp->forward;
+			} else
+#endif
+			if (sw->features & SW_VLAN_DEV)
+				sw->info->forward = FWD_VLAN_DEV;
+			else
+				sw->info->forward = FWD_MAIN_DEV;
+		}
+		tail &= ~TAIL_TAG_PTP;
+
+		/* In case tagging is not working right. */
+		if (tail >= sw->port_cnt)
+			tail = 0;
+		index = sw->info->port_cfg[tail].index;
+
+#ifdef CONFIG_KSZ_HSR
+		/* Always use HSR device for communication when forwarding
+		 * internally.
+		 */
+		if ((sw->features & HSR_REDBOX) &&
+		    (sw->overrides & HSR_FORWARD)) {
+			struct ksz_hsr_info *info = &sw->info->hsr;
+
+			index = info->hsr_index;
+		}
+#endif
+		*tag = tail;
+		*length = len;
+
+		/* Save receiving port. */
+		*port = tail;
+	}
+#ifdef CONFIG_KSZ_IBA
+	if (proto == IBA_TAG_TYPE)
+		return sw->netdev[0];
+	if (IBA_USE_CODE_PREPARE == sw->info->iba.use_iba &&
+	    !sw->info->iba.ready)
+		return NULL;
+#endif
+
+	/* Determine network device from VLAN id. */
+	if (index < 0) {
+		index = 0;
+		if (vid && (sw->features & SW_VLAN_DEV)) {
+			struct ksz_dev_map *map;
+			int p;
+
+			for (p = 0; p < sw->eth_cnt; p++) {
+				map = &sw->eth_maps[p];
+				if (vid == map->vlan) {
+					*port = map->first;
+					p = get_phy_port(sw, *port);
+					*port = p;
+					index = sw->info->port_cfg[p].index;
+					break;
+				}
+			}
+		}
+	}
+	if (index >= sw->dev_count + sw->dev_offset) {
+		printk(KERN_INFO "  [%s] netdev not correct"NL, __func__);
+		BUG();
+	}
+	dev = sw->netdev[index];
+	*tag = get_log_port(sw, *tag);
+	if (!(sw->features & VLAN_PORT_TAGGING) ||
+	    !(sw->vlan_id & (1 << *tag))) {
+		*tag = 0;
+	}
+	sw_forward(sw, data, dev->dev_addr, proto, *tag);
+#ifdef CONFIG_KSZ_MSRP
+	if (proto == 0x22F0)
+		mrp_proc_proto(&sw->mrp, data, *port);
+#endif
+	if ((sw->overrides & UNK_MCAST_BLOCK) &&
+	    (sw->info->forward & (FWD_MCAST | FWD_KNOWN)) == FWD_MCAST) {
+		struct sk_buff *skb;
+		u16 ports = sw->PORT_MASK & ~(1 << *port) & ~sw->HOST_MASK;
+
+		ports &= sw->live_ports;
+		if (!ports)
+			return dev;
+		if (memcmp(data, ipv6_neigh_mcast, 3)) {
+			if (memcmp(data, last_addr, 6)) {
+dbg_msg("%02x:%02x:%02x:%02x:%02x:%02x  %d=%d %x"NL,
+data[0], data[1], data[2], data[3], data[4], data[5], *port, len, ports);
+				memcpy(last_addr, data, 6);
+			}
+		}
+#ifdef CONFIG_KSZ_MSRP
+		/* Do not forward IPv6 packets as the test tool confuses them
+		 * for AVB traffic.
+		 */
+		else if ((sw->features & MRP_SUPPORT) && fqtss_hack)
+			return dev;
+		if (memcmp(data, ipv6_neigh_mcast, 3) &&
+		    (sw->features & AVB_SUPPORT)) {
+			int rc;
+
+			rc = mrp_chk_mcast(&sw->mrp, vlan->h_dest, vid, prio,
+					   proto, *port);
+			if (rc != 1)
+				return dev;
+		}
+#endif
+		skb = alloc_skb(len + 8, GFP_ATOMIC);
+		if (!skb)
+			return dev;
+		skb->dev = sw->main_dev;
+		skb_reset_network_header(skb);
+		skb_reset_transport_header(skb);
+		memcpy(skb->data, data, len);
+		skb_put(skb, len);
+		sw->net_ops->add_tail_tag(sw, skb, ports);
+		skb->protocol = htons(ETH_P_TRAILER);
+
+		skb_queue_tail(&sw->txq, skb);
+		schedule_work(&sw->tx_fwd);
+	}
+	return dev;
+}  /* sw_rx_dev */
+
+static struct ksz_port *sw_get_priv(struct ksz_sw *sw, struct net_device *dev)
+{
+	int dev_count = sw->dev_count + sw->dev_offset;
+	int i;
+
+	for (i = 0; i < dev_count; i++) {
+		if (sw->netdev[i] == dev)
+			return sw->netport[i];
+	}
+	return NULL;
+}
+
+static int pkt_matched(struct ksz_sw *sw, struct sk_buff *skb,
+	struct net_device *dev, void *ptr, int (*get_multi)(void *ptr),
+	u8 h_promiscuous)
+{
+	int drop = false;
+	u8 bcast_addr[] = { 0xff, 0xff, 0xff, 0xff, 0xff, 0xff };
+
+	if (skb->data[0] & 0x01) {
+		if (memcmp(skb->data, bcast_addr, ETH_ALEN) && !get_multi(ptr))
+			drop = sw_match_multi(sw, sw_get_priv(sw, dev),
+					      skb->data);
+	} else if (h_promiscuous && memcmp(skb->data, dev->dev_addr, ETH_ALEN))
+		drop = true;
+	if (drop)
+		return 0;
+	return skb->len;
+}  /* pkt_matched */
+
+static int sw_match_pkt(struct ksz_sw *sw, struct net_device **dev,
+	void **priv, int (*get_promiscuous)(void *ptr),
+	int (*get_multi)(void *ptr), struct sk_buff *skb,
+	u8 h_promiscuous)
+{
+	int s_promiscuous;
+
+	/* This function will return the child device if matched, and the
+	 * parent device if it is matched.
+	 */
+	if (sw->dev_count <= 1)
+		return true;
+	s_promiscuous = get_promiscuous(*priv);
+	if (!s_promiscuous && !pkt_matched(sw, skb, *dev, *priv, get_multi,
+	    h_promiscuous)) {
+		int matched = false;
+
+		/* There is a parent network device. */
+		if (sw->dev_offset) {
+			matched = true;
+			*dev = sw->netdev[0];
+			*priv = netdev_priv(*dev);
+			s_promiscuous = get_promiscuous(*priv);
+			if (!s_promiscuous && !pkt_matched(sw, skb, *dev,
+			    *priv, get_multi, h_promiscuous))
+				matched = false;
+		}
+		return matched;
+	}
+	return true;
+}  /* sw_match_pkt */
+
+static struct net_device *sw_parent_rx(struct ksz_sw *sw,
+				       struct net_device *dev, int *forward)
+{
+	if (sw->dev_offset && dev != sw->netdev[0]) {
+		if (!*forward)
+			*forward = FWD_MAIN_DEV;
+		if (!(*forward & FWD_STP_DEV))
+			dev = sw->netdev[0];
+		else
+			*forward &= ~FWD_VLAN_DEV;
+	}
+	return dev;
+}  /* sw_parent_rx */
+
+static int sw_port_vlan_rx(struct sk_buff *skb, int forward, int tag)
+{
+	/* Add VLAN tag manually. */
+	if (!(forward & FWD_VLAN_DEV) || !tag)
+		return false;
+
+	tag += VLAN_PORT_START;
+
+	/* Only forward to one network device. */
+	__vlan_hwaccel_put_tag(skb, htons(ETH_P_8021Q), tag);
+	return true;
+}  /* sw_port_vlan_rx */
+
+static int sw_drv_rx(struct ksz_sw *sw, struct sk_buff *skb, uint port)
+{
+	int ret = 1;
+
+#ifdef CONFIG_KSZ_IBA
+	if (sw->features & IBA_SUPPORT) {
+		ret = iba_rcv(&sw->info->iba, skb);
+		if (!ret)
+			return ret;
+
+#ifdef CONFIG_KSZ_IBA_ONLY
+		if (IBA_USE_CODE_PREPARE == sw->info->iba.use_iba) {
+			dev_kfree_skb_irq(skb);
+			return 0;
+		}
+#endif
+	}
+#endif
+#ifdef CONFIG_KSZ_STP
+	if (sw->features & STP_SUPPORT) {
+		ret = stp_rcv(&sw->info->rstp, skb, port);
+		if (!ret)
+			return ret;
+	}
+#endif
+#ifdef CONFIG_KSZ_MRP
+	if (sw->features & MRP_SUPPORT) {
+		ret = mrp_rcv(&sw->mrp, skb, port);
+		if (!ret)
+			return ret;
+	}
+#endif
+#ifdef CONFIG_KSZ_DLR
+	if (sw->features & DLR_HW) {
+		ret = dlr_rcv(&sw->info->dlr, skb, port);
+		if (!ret)
+			return ret;
+	}
+#endif
+#ifdef CONFIG_KSZ_HSR
+	if (sw->features & HSR_HW) {
+		ret = hsr_rcv(&sw->info->hsr, skb, port);
+
+		/* It is an HSR frame or consumed. */
+		if (ret < 2)
+			return ret;
+		if (sw->features & HSR_REDBOX) {
+			ret = hsr_chk(&sw->info->hsr, skb, port);
+			if (ret < 2)
+				return ret;
+		}
+	}
+#endif
+
+	/* Need to remove VLAN tag if not using tail tag. */
+	if (sw->dev_count > 1 && (sw->features & SW_VLAN_DEV) &&
+	    !(sw->overrides & TAIL_TAGGING)) {
+		struct vlan_ethhdr *vlan = (struct vlan_ethhdr *) skb->data;
+
+		if (vlan->h_vlan_proto == htons(ETH_P_8021Q)) {
+			int i;
+			int vid;
+			struct ethhdr *eth;
+			u16 vlan_tci = ntohs(vlan->h_vlan_TCI);
+
+			vid = vlan_tci & VLAN_VID_MASK;
+			for (i = 0; i < sw->eth_cnt; i++) {
+				if (vid == sw->eth_maps[i].vlan) {
+					eth = (struct ethhdr *)
+						skb_pull(skb, VLAN_HLEN);
+					memmove(eth, vlan, 12);
+					break;
+				}
+			}
+		}
+	}
+	return ret;
+}  /* sw_drv_rx */
+
+static int sw_get_mtu(struct ksz_sw *sw)
+{
+	int need_tail_tag = false;
+	int header = 0;
+	int mtu = 0;
+
+	if (sw->features & (PTP_HW | DLR_HW | HSR_HW))
+		need_tail_tag = true;
+	if (sw->dev_count > 1 && !(sw->features & SW_VLAN_DEV))
+		need_tail_tag = true;
+	if (sw->features & VLAN_PORT_TAGGING)
+		need_tail_tag = true;
+	if (sw->features & STP_SUPPORT)
+		need_tail_tag = true;
+	if (need_tail_tag) {
+		mtu += 2;
+		if (sw->TAIL_TAG_SHIFT != 7)
+			mtu -= 1;
+		if (sw->features & PTP_HW)
+			mtu += 4;
+	}
+#ifdef CONFIG_KSZ_HSR
+	if (sw->features & HSR_HW)
+		header = HSR_HLEN;
+#endif
+	if (sw->features & SW_VLAN_DEV)
+		if (header < VLAN_HLEN)
+			header = VLAN_HLEN;
+	mtu += header;
+dbg_msg("mtu: %d"NL, mtu);
+	return mtu;
+}  /* sw_get_mtu */
+
+static int sw_get_tx_len(struct ksz_sw *sw, struct sk_buff *skb, uint port,
+	int *header)
+{
+	int len = skb->len;
+	int hlen = 0;
+
+	if (sw->features & SW_VLAN_DEV)
+		hlen = VLAN_HLEN;
+#ifdef CONFIG_KSZ_HSR
+	do {
+		int i;
+
+		i = sw->info->port_cfg[port].index;
+		if (sw->eth_cnt && (sw->eth_maps[i].proto & HSR_HW))
+			hlen = HSR_HLEN;
+	} while (0);
+#endif
+	*header += hlen;
+	if (!(sw->overrides & TAIL_TAGGING))
+		return len;
+	if (len < 60)
+		len = 60;
+	len += 2;
+	if (sw->overrides & PTP_TAG)
+		len += 4;
+	return len;
+}  /* sw_get_tx_len */
+
+static bool chk_tag_lookup(struct ksz_sw *sw, struct ksz_sw_tx_tag *tag)
+{
+	return (tag->ports == sw->TAIL_TAG_LOOKUP);
+}  /* chk_tag_lookup */
+
+static void chk_tag_ports(struct ksz_sw *sw, struct ksz_sw_tx_tag *tag)
+{
+	/* Need lookup bit if no ports are set. */
+	if (!tag->ports)
+		tag->ports = sw->TAIL_TAG_LOOKUP;
+}  /* chk_tag_ports */
+
+static void set_tag_ports(struct ksz_sw *sw, struct ksz_sw_tx_tag *tag,
+			  uint ports, bool lookup, bool override)
+{
+	tag->ports = ports;
+	if (lookup && !ports) {
+		tag->ports |= sw->TAIL_TAG_LOOKUP;
+	} else {
+		tag->ports &= ~sw->TAIL_TAG_LOOKUP;
+		if (override && ports)
+			tag->ports |= sw->TAIL_TAG_OVERRIDE;
+	}
+}  /* set_tag_ports */
+
+static void set_tag_override(struct ksz_sw *sw, struct ksz_sw_tx_tag *tag)
+{
+	if (tag->ports)
+		tag->ports |= sw->TAIL_TAG_OVERRIDE;
+}  /* set_tag_override */
+
+static void set_tag_valid(struct ksz_sw *sw, struct ksz_sw_tx_tag *tag)
+{
+	tag->ports = htons(tag->ports);
+	tag->timestamp = htonl(tag->timestamp);
+}  /* set_tag_valid */
+
+#if defined(CONFIG_1588_PTP) && defined(CONFIG_KSZ_HSR)
+static bool ports_not_matched(struct ksz_sw *sw, struct ksz_sw_tx_tag *tag,
+			      uint ports)
+{
+	if (!(tag->ports & sw->TAIL_TAG_LOOKUP) &&
+	    !(tag->ports & ports))
+		return true;
+	return false;
+}
+#endif
+
+static void sw_add_tail_tag(struct ksz_sw *sw, struct sk_buff *skb, uint ports)
+{
+	struct ksz_sw_tx_tag tx_tag;
+	u8 *trailer;
+	u8 *tag;
+	int len = 2;
+	int ptp_len = 0;
+	bool override = !!(ports & TAIL_TAG_SET_OVERRIDE);
+	bool queue = !!(ports & TAIL_TAG_SET_QUEUE);
+
+	/* PTP is enabled and so requires extra 4 bytes. */
+	if (sw->overrides & PTP_TAG)
+		ptp_len = 4;
+	len += ptp_len;
+	if (sw->TAIL_TAG_SHIFT != 7)
+		len--;
+	memset(&tx_tag, 0, sizeof(struct ksz_sw_tx_tag));
+	set_tag_ports(sw, &tx_tag, ports, true, override);
+	if (queue)
+		set_tx_tag_queue(sw, &tx_tag, sw->ctrl_queue);
+	set_tag_valid(sw, &tx_tag);
+	tag = (u8 *) &tx_tag;
+	trailer = skb_put(skb, len);
+	memcpy(trailer, &tag[4 - ptp_len], ptp_len + 2);
+	if (sw->TAIL_TAG_SHIFT != 7)
+		trailer[ptp_len] = trailer[ptp_len + 1];
+}  /* sw_add_tail_tag */
+
+static int sw_get_tail_tag(struct ksz_sw *sw, u8 *trailer, uint *port)
+{
+	int len = 1;
+
+	if (*trailer & TAIL_TAG_PTP)
+		len += 4;
+	*port = *trailer & TAIL_TAG_RX_PORTS_M;
+	*port = get_log_port(sw, *port);
+	return len;
+}  /* sw_get_tail_tag */
+
+static void sw_add_vid(struct ksz_sw *sw, u16 vid)
+{
+	if ((sw->features & VLAN_PORT) && vid >= VLAN_PORT_START) {
+		vid -= VLAN_PORT_START;
+		if (vid <= SWITCH_PORT_NUM) {
+			sw->vlan_id |= (1 << vid);
+		}
+	}
+}  /* sw_add_vid */
+
+static void sw_kill_vid(struct ksz_sw *sw, u16 vid)
+{
+	if ((sw->features & VLAN_PORT) && vid >= VLAN_PORT_START) {
+		vid -= VLAN_PORT_START;
+		if (vid <= SWITCH_PORT_NUM) {
+			sw->vlan_id &= ~(1 << vid);
+		}
+	}
+}  /* sw_kill_vid */
+
+static int append_tag(u16 shift, u8 *pad, u8 *tag, int len, int ptp_len,
+	int addlen)
+{
+	memcpy(&pad[len], &tag[4 - ptp_len], ptp_len + 2);
+
+	/* Only one byte for the tag. */
+	if (shift != 7) {
+		pad[len + ptp_len] = pad[len + ptp_len + 1];
+		pad[len + ptp_len + 1] = 0;
+		addlen--;
+	}
+	return addlen;
+}
+
+static int adjust_tag(u8 *tag_data, u8 *skb_data, int skb_len, int tag_len)
+{
+	int tag_start = 0;
+
+	/* length is odd. */
+	if (skb_len & 1) {
+		tag_data[0] = 0;
+		tag_start = 1;
+	}
+	memcpy(&tag_data[tag_start], skb_data, tag_len);
+	tag_start += tag_len;
+	if (tag_start & 1)
+		tag_data[tag_start] = 0;
+	return tag_start;
+}
+
+static struct sk_buff *sw_ins_vlan(struct ksz_sw *sw, uint port,
+	struct sk_buff *skb)
+{
+#ifdef CONFIG_KSZ_IBA
+	if (skb->protocol == htons(ETH_P_IBA))
+		return skb;
+#endif
+	port = sw->priv_port;
+
+	/* Need to insert VLAN tag. */
+	if (sw->dev_count > 1 && (sw->features & SW_VLAN_DEV)) {
+		u16 vid;
+		struct vlan_ethhdr *vlan;
+		struct ethhdr *eth;
+		struct sk_buff *nskb = NULL;
+
+		/*
+		 * Bridge uses clones of socket buffer to send to both
+		 * devices!
+		 */
+		if (!skb_cloned(skb) && skb_headroom(skb) >= VLAN_HLEN)
+			nskb = skb;
+		if (!nskb) {
+			nskb = skb_copy_expand(skb, VLAN_HLEN, skb->len,
+					       GFP_ATOMIC);
+			if (!nskb)
+				return skb;
+			dev_kfree_skb_irq(skb);
+			skb = nskb;
+		}
+		eth = (struct ethhdr *) skb->data;
+
+		vid = sw->info->port_cfg[port].vid;
+		vlan = (struct vlan_ethhdr *) skb_push(skb, VLAN_HLEN);
+		memmove(vlan, eth, 12);
+		vlan->h_vlan_TCI = htons(vid);
+		vlan->h_vlan_proto = htons(ETH_P_8021Q);
+	}
+	return skb;
+}  /* sw_ins_vlan */
+
+#ifdef CONFIG_KSZ_HSR
+static struct sk_buff *sw_ins_hsr(struct ksz_sw *sw, uint n,
+	struct sk_buff *skb, struct ksz_sw_tx_tag *tag)
+{
+	int i;
+	uint p;
+	struct ksz_hsr_info *info = &sw->info->hsr;
+
+	p = sw->priv_port;
+	i = sw->info->port_cfg[p].index;
+
+	/* Internal forwarding when Redbox is up. */
+	if (info->redbox_up && info->redbox_fwd) {
+		struct net_device **dev = (struct net_device **)skb->cb;
+
+		/* Destination is Redbox. */
+		if (info->redbox && *dev == info->redbox) {
+			/* Do not show transmit count in eth1. */
+			skb->dev = info->dev;
+			return skb;
+		}
+
+		/* Using eth1 directly is not allowed. */
+		if (skb->dev == info->redbox) {
+			dev_kfree_skb_irq(skb);
+			return NULL;
+		}
+
+		/* Using eth0 to send and not forwarding from eth0. */
+		if (*dev != info->dev) {
+			int forward = hsr_fwd(info, skb);
+
+			/* Unicast frame to eth1 so get VLAN for eth1. */
+			if (forward == 2) {
+				struct ksz_port *sw_port =
+					sw->netport[info->redbox_index];
+				uint m = sw_port->first_port;
+
+				sw->priv_port = get_phy_port(sw, m);
+				return skb;
+			}
+		}
+	}
+	if (sw->eth_cnt && (sw->eth_maps[i].proto & HSR_HW)) {
+		struct hsr_port *from =
+			hsr_port_get_hsr(&info->hsr, HSR_PT_MASTER);
+#ifdef CONFIG_1588_PTP
+		struct ptp_msg *msg;
+		struct ptp_info *ptp = &sw->ptp_hw;
+
+		if (ptp->tx_msg_parsed) {
+			msg = ptp->tx_msg;
+		} else {
+			msg = check_ptp_msg(skb->data, NULL);
+			ptp->tx_msg_parsed = true;
+			ptp->tx_msg = msg;
+		}
+		if (msg) {
+			if (msg->hdr.messageType != SYNC_MSG &&
+			    msg->hdr.messageType != MANAGEMENT_MSG &&
+			    msg->hdr.messageType != SIGNALING_MSG &&
+			    msg->hdr.messageType != ANNOUNCE_MSG)
+				return skb;
+			if (ports_not_matched(sw, tag, (1 << info->ports[0]))) {
+				dev_kfree_skb_irq(skb);
+				return NULL;
+			}
+		}
+#endif
+		if (!hsr_forward_skb(skb, from))
+			return NULL;
+		set_tag_ports(sw, tag, info->member, false, false);
+	}
+	return skb;
+}  /* sw_ins_hsr */
+#endif
+
+static struct sk_buff *sw_check_skb(struct ksz_sw *sw, struct sk_buff *skb,
+	struct ksz_port *priv, void *ptr,
+	int (*update_msg)(u8 *data, u32 port, u32 overrides))
+{
+	bool need_new_copy = false;
+	int len;
+	int padlen = 0;
+	uint port;
+	struct sk_buff *org_skb;
+	struct ksz_sw_tx_tag tx_tag;
+	int tag_len;
+	int tag_start = 0;
+	u8 tag_data[8];
+	u8 *tag;
+	int update_dst = (sw->overrides & TAIL_TAGGING);
+	int ptp_len = 0;
+	int headlen = 0;
+
+#ifdef CONFIG_1588_PTP
+	struct ptp_info *ptp = ptr;
+#endif
+
+	if (!update_dst)
+		return sw_ins_vlan(sw, priv->first_port, skb);
+
+	if (skb->protocol == htons(ETH_P_TRAILER))
+		return skb;
+#ifdef CONFIG_KSZ_STP
+	if (skb->protocol == htons(STP_TAG_TYPE))
+		return skb;
+#endif
+#ifdef CONFIG_KSZ_DLR
+	if (skb->protocol == htons(DLR_TAG_TYPE))
+		return skb;
+#endif
+#ifdef CONFIG_KSZ_MRP
+	if (skb->protocol == htons(ETH_P_MVRP) ||
+#ifdef CONFIG_KSZ_MSRP
+	    skb->protocol == htons(ETH_P_MSRP) ||
+#endif
+	    skb->protocol == htons(ETH_P_MMRP)) {
+
+		/* MRP frame from application has 2 end marks. */
+		if (skb->data[skb->len - 1] || skb->data[skb->len - 2])
+			return skb;
+
+		mrp_rcv(&sw->mrp, skb, sw->HOST_PORT);
+		return NULL;
+	}
+#endif
+
+	/* PTP is enabled and so requires extra 4 bytes. */
+	if (sw->overrides & PTP_TAG)
+		ptp_len = 4;
+	tag_len = ptp_len + 2;
+	if (sw->TAIL_TAG_SHIFT != 7)
+		tag_len--;
+#ifdef CONFIG_KSZ_HSR
+	if (sw->features & HSR_HW)
+		headlen = HSR_HLEN;
+#endif
+	if (sw->features & SW_VLAN_DEV && !headlen)
+		headlen = VLAN_HLEN;
+
+	memset(&tx_tag, 0, sizeof(tx_tag));
+
+#ifdef CONFIG_KSZ_IBA
+	if (skb->protocol == htons(ETH_P_IBA)) {
+		set_tag_ports(sw, &tx_tag, 0, true, false);
+		len = skb->len;
+		goto add_tag;
+	}
+#endif
+
+	org_skb = skb;
+	port = 0;
+
+	/* This device is associated with a switch port. */
+	if (1 == priv->port_cnt)
+		port = priv->first_port;
+
+	do {
+		u16 prio;
+		u16 vid;
+		uint i;
+		uint p;
+		u32 features = sw->features;
+
+		p = get_phy_port(sw, priv->first_port);
+		i = sw->info->port_cfg[p].index;
+		if (sw->features & SW_VLAN_DEV)
+			features = sw->eth_maps[i].proto;
+		if (!(features & VLAN_PORT) || port || vlan_get_tag(skb, &vid))
+			break;
+		prio = vid & VLAN_PRIO_MASK;
+		vid &= VLAN_VID_MASK;
+		if (vid < VLAN_PORT_START)
+			break;
+		vid -= VLAN_PORT_START;
+		if (!vid || vid > sw->mib_port_cnt)
+			break;
+		port = vid;
+
+		if (sw->vid || prio) {
+			struct vlan_ethhdr *vlan =
+				(struct vlan_ethhdr *) skb->data;
+			u16 vlan_tci = ntohs(vlan->h_vlan_TCI);
+
+			vlan_tci &= ~VLAN_VID_MASK;
+			vlan_tci |= sw->vid;
+			vlan->h_vlan_TCI = htons(vlan_tci);
+
+		/* Need to remove VLAN tag manually. */
+		} else if (!(sw->overrides & TAG_REMOVE)) {
+			u8 *data;
+
+			len = VLAN_ETH_HLEN - 2;
+			data = &skb->data[len];
+			memmove(data - VLAN_HLEN, data, skb->len - len);
+			skb->len -= VLAN_HLEN;
+		}
+	} while (0);
+
+	if (port) {
+		port = get_phy_port(sw, port);
+		set_tag_ports(sw, &tx_tag, 1 << port, false, false);
+	}
+
+	/* Check the socket buffer length is enough to hold the tail tag. */
+	if (skb->len < ETH_ZLEN)
+		padlen = ETH_ZLEN - skb->len;
+	len = skb_tailroom(skb);
+	if (len < tag_len + padlen) {
+		need_new_copy = true;
+		len = (skb->len + tag_len + padlen + 4) & ~3;
+	}
+	if (skb_headroom(skb) < headlen) {
+		need_new_copy = true;
+	}
+	if (need_new_copy) {
+		int headerlen = skb_headroom(skb);
+
+		if (headerlen < headlen)
+			headerlen = headlen;
+
+		/* The new socket buffer has no fragments but still needs
+		 * hardware checksum generation.
+		 */
+		skb = skb_copy_expand(org_skb, headerlen, len, GFP_ATOMIC);
+		if (!skb)
+			return NULL;
+		consume_skb(org_skb);
+	}
+
+	/* skb_put requires tail pointer set first. */
+	skb_set_tail_pointer(skb, skb->len);
+	if (padlen) {
+		if (__skb_put_padto(skb, skb->len + padlen, false))
+			return NULL;
+	}
+	len = skb->len;
+
+	/* Remember original tag information as PTP may change the tag. */
+	port = get_tx_tag_ports(sw, &tx_tag);
+
+#ifdef CONFIG_1588_PTP
+	if (ptp && ptp->started)
+		ptp_set_tx_info(ptp, skb->data, &tx_tag);
+#endif
+	if (!port) {
+		/* Used to get VLAN for which device to send. */
+		sw->priv_port = get_phy_port(sw, priv->first_port);
+		chk_tag_ports(sw, &tx_tag);
+
+#ifdef CONFIG_KSZ_HSR
+		if (sw->features & HSR_HW) {
+			skb = sw_ins_hsr(sw, priv->first_port, skb, &tx_tag);
+			if (!skb)
+				return NULL;
+			len = skb->len;
+		}
+#endif
+	}
+	if (chk_tag_lookup(sw, &tx_tag)) {
+		/* Use VLAN for port forwarding if not specified directly. */
+		skb = sw_ins_vlan(sw, priv->first_port, skb);
+		if (len != skb->len)
+			len = skb->len;
+	}
+	do {
+		struct vlan_ethhdr *vlan = (struct vlan_ethhdr *)skb->data;
+		bool override;
+		int dest;
+
+		/* The MAC table was programmed to forward only to host.
+		 * Need destination ports to send out.
+		 * The port may be blocked.  Need override to send out.
+		 */
+		dest = sw_need_dest(sw, skb->data);
+
+		override = (2 == dest);
+		if (dest > 0 && chk_tag_lookup(sw, &tx_tag)) {
+			struct ksz_dev_map *map;
+			u16 ports;
+			int i;
+
+			/* Specify ports that are connected. */
+			ports = priv->live_ports;
+
+			/* Remove ports that are disabled unless override. */
+			if (!override)
+				ports &= sw->tx_ports[0];
+			ports &= ~sw->HOST_MASK;
+			if (!ports) {
+				dev_kfree_skb_irq(skb);
+				return NULL;
+			}
+			for (i = 0; i < sw->eth_cnt; i++) {
+				map = &sw->eth_maps[i];
+
+				/* Do not send if special. */
+				if (map->first == priv->first_port &&
+				    (map->proto & (DLR_HW | HSR_HW))) {
+					dev_kfree_skb_irq(skb);
+					return NULL;
+				}
+			}
+			set_tag_ports(sw, &tx_tag, ports, false, override);
+		} else {
+			if (override)
+				set_tag_override(sw, &tx_tag);
+		}
+
+		/* Honor the VLAN priority to put in different queue. */
+		if (vlan->h_vlan_proto == htons(ETH_P_8021Q)) {
+			u16 prio;
+			u16 vlan_tci = ntohs(vlan->h_vlan_TCI);
+			struct ksz_port_cfg *cfg =
+				&sw->info->port_cfg[sw->HOST_PORT];
+			u32 queue = cfg->tc_map[0];
+
+			prio = vlan_tci & VLAN_PRIO_MASK;
+			prio >>= VLAN_PRIO_SHIFT;
+			queue >>= prio * PORT_TC_MAP_S;
+			queue &= PORT_TC_MAP_M;
+			set_tx_tag_queue(sw, &tx_tag, queue);
+		}
+	} while (0);
+
+#ifdef CONFIG_KSZ_IBA
+add_tag:
+#endif
+	set_tag_valid(sw, &tx_tag);
+	tag = (u8 *) &tx_tag;
+
+	len = append_tag(sw->TAIL_TAG_SHIFT, skb->data, tag, len,
+		ptp_len, ptp_len + 2);
+
+	/* Need to compensate checksum. */
+	if (skb->ip_summed == CHECKSUM_PARTIAL)
+		tag_start = adjust_tag(tag_data, &skb->data[skb->len],
+				       skb->len, tag_len);
+	skb_put(skb, len);
+
+	/* Need to compensate checksum for some devices. */
+	if (tag_start && (sw->overrides & UPDATE_CSUM)) {
+		__sum16 *csum_loc = (__sum16 *)
+			(skb->head + skb->csum_start + skb->csum_offset);
+
+		/* Checksum is cleared by driver to be filled by hardware. */
+		if (!*csum_loc) {
+			u16 *tag_csum = (u16 *) &tag_data;
+			__sum16 new_csum;
+			int csum = 0;
+			int i;
+
+			/* Length may be odd. */
+			tag_start++;
+			for (i = 0; i < tag_start / 2; i++)
+				csum += ntohs(tag_csum[i]);
+			csum = (csum >> 16) + (csum & 0xffff);
+			csum += (csum >> 16);
+			new_csum = (__sum16) csum;
+			*csum_loc = ~htons(new_csum);
+		}
+	}
+	return skb;
+}  /* sw_check_skb */
+
+static struct sk_buff *sw_check_tx(struct ksz_sw *sw, struct net_device *dev,
+	struct sk_buff *skb, struct ksz_port *priv)
+{
+	void *ptr = NULL;
+	int (*update_msg)(u8 *data, u32 port, u32 overrides) = NULL;
+
+#ifdef CONFIG_1588_PTP
+	if (sw->features & PTP_HW) {
+		struct ptp_info *ptp = &sw->ptp_hw;
+
+		ptr = ptp;
+	}
+#endif
+
+	return sw_check_skb(sw, skb, priv, ptr, update_msg);
+}  /* sw_check_tx */
+
+static struct sk_buff *sw_final_skb(struct ksz_sw *sw, struct sk_buff *skb,
+	struct net_device *dev, struct ksz_port *port)
+{
+	skb = sw->net_ops->check_tx(sw, dev, skb, port);
+	if (!skb)
+		return NULL;
+
+#ifdef CONFIG_1588_PTP
+	if (sw->features & PTP_HW) {
+		struct ptp_info *ptp = &sw->ptp_hw;
+
+		if (skb_shinfo(skb)->tx_flags & SKBTX_HW_TSTAMP)
+			ptp->ops->get_tx_tstamp(ptp, skb);
+	}
+#endif
+	return skb;
+}  /* sw_final_skb */
+
+static void sw_start(struct ksz_sw *sw, u8 *addr)
+{
+	int need_tail_tag = false;
+	int need_vlan = false;
+	int setup = true;
+
+#ifdef CONFIG_KSZ_IBA
+	if (IBA_USE_CODE_PREPARE <= sw->info->iba.use_iba)
+		setup = false;
+#endif
+	sw->ops->acquire(sw);
+	if (setup)
+		sw_setup(sw);
+	sw_enable(sw);
+
+	sw_set_addr(sw, addr);
+
+	/* STP has its own mechanism to handle looping. */
+	if (!(sw->features & STP_SUPPORT))
+		sw_cfg_src_filter(sw, true);
+	if (sw->features & (PTP_HW | DLR_HW | HSR_HW))
+		need_tail_tag = true;
+	if (sw->dev_count > 1 && !(sw->features & SW_VLAN_DEV))
+		need_tail_tag = true;
+	if (sw->features & VLAN_PORT) {
+		if (sw->features & VLAN_PORT_REMOVE_TAG) {
+			struct ksz_vlan_table entry;
+			uint n;
+			uint p;
+
+			memset(&entry, 0, sizeof(struct ksz_vlan_table));
+			sw->ops->release(sw);
+			entry.fid = VLAN_PORT_START;
+			entry.untag = sw->PORT_MASK;
+			entry.ports = sw->PORT_MASK;
+			entry.valid = 1;
+			sw_w_vlan_table(sw, VLAN_PORT_START, &entry);
+			for (n = 1; n <= sw->mib_port_cnt; n++) {
+				p = get_phy_port(sw, n);
+				entry.fid = VLAN_PORT_START + p + 1;
+				entry.untag = (1 << p);
+				entry.ports = (1 << p) | (1 << sw->HOST_PORT);
+				entry.valid = 1;
+				sw_w_vlan_table(sw, VLAN_PORT_START + p + 1,
+					&entry);
+			}
+			sw->ops->acquire(sw);
+			for (n = 1; n <= sw->mib_port_cnt; n++) {
+				p = get_phy_port(sw, n);
+				sw_cfg_def_vid(sw, p, VLAN_PORT_START);
+			}
+			need_vlan = true;
+			sw->overrides |= TAG_REMOVE;
+		}
+		if (sw->features & VLAN_PORT_TAGGING)
+			need_tail_tag = true;
+	}
+	if (sw->features & SW_VLAN_DEV) {
+		struct ksz_vlan_table entry;
+		struct ksz_dev_map *map;
+		int i;
+		int p;
+		uint port;
+		uint q;
+
+		memset(&entry, 0, sizeof(struct ksz_vlan_table));
+		for (p = 0; p < sw->eth_cnt; p++) {
+
+			/* Not really using VLAN. */
+			if (1 == sw->eth_maps[p].vlan)
+				continue;
+			sw->ops->release(sw);
+
+			map = &sw->eth_maps[p];
+
+			/*
+			 * Setting FID allows same MAC address in different
+			 * VLANs.
+			 */
+			entry.fid = map->vlan & (FID_ENTRIES - 1);
+			entry.untag = map->mask;
+
+			/* Use tail tag to determine the network device. */
+			if (need_tail_tag)
+				entry.untag |= sw->HOST_MASK;
+			entry.ports = sw->HOST_MASK | map->mask;
+			entry.valid = 1;
+			sw_w_vlan_table(sw, map->vlan, &entry);
+			sw->ops->acquire(sw);
+			need_vlan = true;
+#ifdef CONFIG_KSZ_HSR
+			if (map->proto & HSR_HW)
+				continue;
+#endif
+			for (i = 0, q = map->first;
+			     i < map->cnt; i++, q++) {
+				port = get_phy_port(sw, q);
+				sw_cfg_def_vid(sw, port, map->vlan);
+			}
+		}
+	}
+	if (sw->features & STP_SUPPORT)
+		need_tail_tag = true;
+	if (sw->features & (DLR_HW | AVB_SUPPORT | MRP_SUPPORT))
+		need_vlan = true;
+
+#ifdef CONFIG_KSZ_MSTP
+	if (sw->features & STP_SUPPORT)
+		need_vlan = true;
+#endif
+	if (need_vlan)
+		sw_ena_vlan(sw);
+	if (need_tail_tag && !(sw->overrides & TAIL_TAGGING)) {
+		port_cfg_tail_tag(sw, sw->HOST_PORT, 1);
+if (!(sw->overrides & TAIL_TAGGING))
+dbg_msg(" ! tail tag not set"NL);
+	}
+	sw_ena_intr(sw);
+	sw->ops->release(sw);
+
+#ifdef CONFIG_KSZ_STP
+	if (sw->features & STP_SUPPORT) {
+		struct ksz_stp_info *stp = &sw->info->rstp;
+
+		if (stp->br.bridgeEnabled)
+			stp_start(stp);
+	} else
+		stp_set_addr(&sw->info->rstp, sw->info->mac_addr);
+#endif
+
+#ifdef CONFIG_1588_PTP
+	if (sw->features & PTP_HW) {
+		struct ptp_info *ptp = &sw->ptp_hw;
+
+		ptp->reg->start(ptp, true);
+if (!(sw->overrides & PTP_TAG))
+dbg_msg(" ! ptp tag not set"NL);
+	}
+#endif
+}  /* sw_start */
+
+static int sw_stop(struct ksz_sw *sw, int complete)
+{
+	int reset = false;
+	int hw_access = true;
+
+#ifdef CONFIG_KSZ_IBA
+	if (IBA_USE_CODE_PREPARE <= sw->info->iba.use_iba) {
+		hw_access = false;
+	}
+#endif
+
+#ifdef CONFIG_KSZ_STP
+	if (sw->features & STP_SUPPORT) {
+		struct ksz_stp_info *stp = &sw->info->rstp;
+
+		if (stp->br.bridgeEnabled)
+			stp_stop(stp, hw_access);
+	}
+#endif
+
+#ifdef CONFIG_KSZ_MRP
+	if (sw->features & MRP_SUPPORT) {
+		struct mrp_info *mrp = &sw->mrp;
+
+		mrp_stop(mrp);
+	}
+#endif
+
+#ifdef CONFIG_1588_PTP
+	if (sw->features & PTP_HW) {
+		struct ptp_info *ptp = &sw->ptp_hw;
+
+		reset = ptp->ops->stop(ptp, hw_access);
+	}
+#endif
+
+#ifdef CONFIG_KSZ_IBA
+	if (IBA_USE_CODE_PREPARE <= sw->info->iba.use_iba) {
+		if (sw->info->iba.use_iba < IBA_USE_CODE_HARD_RESET) {
+			sw->ops->acquire(sw);
+
+			/* Indicate no wait as there is no IBA response. */
+			sw->info->iba.use_iba |= IBA_USE_CODE_NO_WAIT;
+			sw->reg->w8(sw, REG_SW_OPERATION, SW_RESET);
+			sw->ops->release(sw);
+
+			/* Indicate no more hardware access. */
+			sw->info->iba.use_iba = IBA_USE_CODE_HARD_RESET;
+		} else if (sw->info->iba.use_iba == IBA_USE_CODE_SOFT_RESET) {
+			uint n, p;
+
+			sw->ops->acquire(sw);
+			sw->info->iba.use_iba = IBA_USE_CODE_ONLY;
+			sw_dis_intr(sw);
+			for (n = 1; n <= sw->mib_port_cnt; n++) {
+				p = get_phy_port(sw, n);
+				port_w8(sw, p, REG_PORT_LUE_MSTP_STATE, 6);
+				port_w32(sw, p, REG_PORT_VLAN_MEMBERSHIP__4,
+					 sw->PORT_MASK);
+			}
+			port_cfg_tail_tag(sw, sw->HOST_PORT, 0);
+			sw->reg->w16(sw, 0x514, 0x39);
+			sw->overrides &= ~PTP_TAG;
+			sw->info->iba.use_iba = IBA_USE_CODE_HARD_RESET;
+			sw_reset_setup(sw);
+			sw_reset_acl(sw);
+			sw->ops->release(sw);
+		}
+		return reset;
+	}
+#endif
+
+	sw->ops->acquire(sw);
+	if (!reset)
+		sw_reset(sw);
+	reset = hw_access;
+	if (sw->mtu > 2000) {
+		sw->reg->w16(sw, REG_SW_MTU__2, (u16) sw->mtu);
+		sw_cfg(sw, REG_SW_MAC_CTRL_1, SW_JUMBO_PACKET, true);
+	}
+	sw_init(sw);
+
+	/* Clean out static MAC table when the switch shutdown. */
+	if (complete)
+		sw_clr_sta_mac_table(sw);
+	sw->ops->release(sw);
+	return reset;
+}  /* sw_stop */
+
+static void sw_init_mib(struct ksz_sw *sw)
+{
+	unsigned long interval;
+	uint n;
+	uint p;
+
+	interval = MIB_READ_INTERVAL * 2 / (sw->mib_port_cnt + 1);
+	for (n = 0; n <= sw->mib_port_cnt; n++) {
+		p = get_phy_port(sw, n);
+		sw->port_mib[p].mib_start = 0;
+		if (sw->next_jiffies < jiffies)
+			sw->next_jiffies = jiffies + HZ * 2;
+		else
+			sw->next_jiffies += interval;
+		sw->counter[p].time = sw->next_jiffies;
+		sw->port_state[p].state = media_disconnected;
+		port_init_cnt(sw, p);
+	}
+	for (n = 0; n <= sw->mib_port_cnt; n++) {
+		p = get_phy_port(sw, n);
+		if (sw->port_info[p].phy)
+			continue;
+		sw->port_state[p].state = media_connected;
+	}
+	sw->port_state[sw->HOST_PORT].state = media_connected;
+}  /* sw_init_mib */
+
+#if defined(CONFIG_PHYLINK) || defined(CONFIG_PHYLINK_MODULE)
+static void sw_set_phylink_support(struct ksz_sw *sw, struct ksz_port *port,
+				   unsigned long *supported,
+				   struct phylink_link_state *state)
+{
+	__ETHTOOL_DECLARE_LINK_MODE_MASK(mask) = { 0, };
+	struct ksz_port_info *info;
+	bool basic_support = true;
+	uint flow_ctrl;
+	uint p;
+
+	p = get_phy_port(sw, port->first_port);
+	info = get_port_info(sw, p);
+
+	phylink_set(mask, TP);
+	phylink_set(mask, MII);
+
+	phylink_set(mask, Autoneg);
+
+	flow_ctrl = port->flow_ctrl;
+	switch (info->interface) {
+	case PHY_INTERFACE_MODE_RGMII:
+	case PHY_INTERFACE_MODE_RGMII_ID:
+	case PHY_INTERFACE_MODE_RGMII_RXID:
+	case PHY_INTERFACE_MODE_RGMII_TXID:
+		if (sw->features & GIGABIT_SUPPORT) {
+			phylink_set(mask, 1000baseT_Full);
+			phylink_set(mask, 1000baseX_Full);
+		}
+		break;
+	case PHY_INTERFACE_MODE_SGMII:
+		phylink_set(mask, 1000baseT_Full);
+		phylink_set(mask, 1000baseX_Full);
+		if (info->fiber) {
+			basic_support = false;
+		} else {
+			phylink_set(mask, 1000baseT_Half);
+		}
+		break;
+	default:
+		break;
+	}
+	if (basic_support) {
+		phylink_set(mask, 10baseT_Half);
+		phylink_set(mask, 10baseT_Full);
+		phylink_set(mask, 100baseT_Half);
+		phylink_set(mask, 100baseT_Full);
+	}
+
+	switch (flow_ctrl) {
+	case PHY_NO_FLOW_CTRL:
+		phylink_clear(mask, Pause);
+		phylink_clear(mask, Asym_Pause);
+		break;
+	case PHY_RX_ONLY:
+		/* Depend on link partner to only advertise Asym_Pause. */
+		phylink_set(mask, Pause);
+		phylink_set(mask, Asym_Pause);
+		break;
+	case PHY_TX_ONLY:
+		phylink_set(mask, Asym_Pause);
+		phylink_clear(mask, Pause);
+		break;
+	default:
+		phylink_set(mask, Pause);
+		phylink_clear(mask, Asym_Pause);
+	}
+
+	bitmap_and(supported, supported, mask, __ETHTOOL_LINK_MODE_MASK_NBITS);
+	bitmap_and(state->advertising, state->advertising, mask,
+		   __ETHTOOL_LINK_MODE_MASK_NBITS);
+	linkmode_copy(port->phydev->supported, supported);
+	linkmode_copy(port->phydev->advertising, state->advertising);
+}  /* sw_set_phylink_support */
+
+static void sw_port_phylink_get_fixed_state(struct phylink_config *config,
+					    struct phylink_link_state *s)
+{
+	struct ksz_port *p = container_of(config, struct ksz_port, pl_config);
+	struct ksz_sw *sw = p->sw;
+	struct ksz_port_info *info = get_port_info(sw, sw->HOST_PORT);
+
+	s->interface = sw->interface;
+	s->speed = info->tx_rate / TX_RATE_UNIT;
+	s->duplex = 1;
+	s->pause = 3;
+	s->link = 1;
+	s->an_enabled = 0;
+	s->an_complete = 0;
+dbg_msg(" fixed state: %d %d %d\n", sw->interface, info->interface, s->speed);
+}  /* sw_port_phylink_get_fixed_state */
+
+static void sw_port_phylink_validate(struct phylink_config *config,
+				     unsigned long *supported,
+				     struct phylink_link_state *state)
+{
+	struct ksz_port *p = container_of(config, struct ksz_port, pl_config);
+	struct ksz_sw *sw = p->sw;
+
+dbg_msg(" validate: %d\n", state->interface);
+	if ((sw->dev_offset && p->port_cnt > 1) ||
+	    (!sw->dev_offset && !sw->phy_offset)) {
+		if (sw->phylink_ops && sw->phylink_ops->validate)
+			sw->phylink_ops->validate(config, supported, state);
+	} else {
+		sw_set_phylink_support(sw, p, supported, state);
+	}
+}
+
+static int sw_port_phylink_mac_prepare(struct phylink_config *config,
+				       unsigned int mode,
+				       phy_interface_t interface)
+{
+	struct ksz_port *p = container_of(config, struct ksz_port, pl_config);
+	struct ksz_sw *sw = p->sw;
+
+	if (sw->phylink_ops && sw->phylink_ops->mac_prepare)
+		return sw->phylink_ops->mac_prepare(config, mode, interface);
+	return 0;
+}
+
+static void sw_port_phylink_mac_config(struct phylink_config *config,
+				       unsigned int mode,
+				       const struct phylink_link_state *state)
+{
+	/* The switch is always connected to the MAC. */
+}
+
+static void sw_port_phylink_mac_link_down(struct phylink_config *config,
+					  unsigned int mode,
+					  phy_interface_t interface)
+{
+	struct ksz_port *p = container_of(config, struct ksz_port, pl_config);
+	struct ksz_sw *sw = p->sw;
+
+	/* Cannot turn off transmit queue because of IBA. */
+	if (!sw || p == sw->main_port)
+		return;
+
+	/* Tell MAC driver to turn off transmit queues. */
+	interface = PHY_INTERFACE_MODE_INTERNAL;
+	if (sw->phylink_ops && sw->phylink_ops->mac_link_down)
+		sw->phylink_ops->mac_link_down(config, mode, interface);
+}
+
+static void sw_port_phylink_mac_link_up(struct phylink_config *config,
+					struct phy_device *phydev,
+					unsigned int mode,
+					phy_interface_t interface,
+					int speed, int duplex,
+					bool tx_pause, bool rx_pause)
+{
+	struct ksz_port *p = container_of(config, struct ksz_port, pl_config);
+	struct ksz_sw *sw = p->sw;
+
+	/* Tell MAC driver to turn on transmit queues. */
+	interface = PHY_INTERFACE_MODE_INTERNAL;
+	if (sw->phylink_ops && sw->phylink_ops->mac_link_up)
+		sw->phylink_ops->mac_link_up(config, phydev, mode,
+					     interface, speed, duplex,
+					     tx_pause, rx_pause);
+}
+
+static const struct phylink_mac_ops sw_port_phylink_mac_ops = {
+	.validate = sw_port_phylink_validate,
+	.mac_prepare = sw_port_phylink_mac_prepare,
+	.mac_config = sw_port_phylink_mac_config,
+	.mac_link_down = sw_port_phylink_mac_link_down,
+	.mac_link_up = sw_port_phylink_mac_link_up,
+};
+
+static int setup_phylink(struct ksz_port *port)
+{
+	struct device_node *dn = port->dn;
+	phy_interface_t mode;
+	int ret;
+
+	ret = of_get_phy_mode(dn, &mode);
+	if (ret)
+		mode = PHY_INTERFACE_MODE_NA;
+
+	port->pl_config.dev = &port->netdev->dev;
+
+	/* netif_carrier_on is called automatically for netdevice. */
+	port->pl_config.type = PHYLINK_NETDEV;
+
+#if 0
+	/* netif_carrier_on is not called for base device. */
+	port->pl_config.type = PHYLINK_DEV;
+#endif
+	port->pl_config.get_fixed_state = sw_port_phylink_get_fixed_state;
+
+	port->pl = phylink_create(&port->pl_config, of_fwnode_handle(dn), mode,
+				  &sw_port_phylink_mac_ops);
+	if (IS_ERR(port->pl)) {
+		netdev_err(port->netdev,
+			   "error creating PHYLINK: %ld\n", PTR_ERR(port->pl));
+		return PTR_ERR(port->pl);
+	}
+
+	return ret;
+}
+
+static void sw_exit_phylink(struct ksz_sw *sw, struct ksz_port *port)
+{
+	const struct phylink_mac_ops *ops = sw->phylink_ops;
+
+#ifdef CONFIG_KSZ_IBA_ONLY
+	if (IBA_USE_CODE_ONLY == sw->info->iba.use_iba) {
+		return;
+	}
+#endif
+	if (ops && port) {
+		unsigned int mode = MLO_AN_PHY;
+		struct phylink_config *config;
+
+		config = &port->pl_config;
+		ops->mac_link_down(config, mode, sw->interface);
+	}
+}  /* sw_exit_phylink */
+
+static void sw_init_phylink(struct ksz_sw *sw, struct ksz_port *port)
+{
+	const struct phylink_mac_ops *ops = sw->phylink_ops;
+
+#ifdef CONFIG_KSZ_IBA_ONLY
+	if (IBA_USE_CODE_PREPARE == sw->info->iba.use_iba) {
+		return;
+	}
+#endif
+	if (ops && port) {
+		struct phylink_link_state *state;
+		unsigned int mode = MLO_AN_PHY;
+		struct phylink_config *config;
+		bool rx_pause = true;
+		bool tx_pause = true;
+
+		config = &port->pl_config;
+		state = &port->pl_state;
+		sw_port_phylink_get_fixed_state(config, state);
+		ops->mac_config(config, mode, state);
+		ops->mac_link_up(config, port->phydev, mode,
+				 sw->interface,
+				 state->speed, state->duplex,
+				 tx_pause, rx_pause);
+	}
+}  /* sw_init_phylink */
+#endif
+
+static int setup_device_node(struct ksz_sw *sw)
+{
+	struct sw_priv *ks = sw->dev;
+	struct device_node *np;
+	int cnt = 0;
+
+	if (!ks->of_dev)
+		goto err;
+	np = ks->of_dev->of_node;
+	if (np) {
+		struct device_node *ports, *port;
+		struct device_node *ethernet;
+		struct ksz_port_info *info;
+		const char *name;
+		u32 mode, reg;
+		int err;
+
+		ports = of_get_child_by_name(np, "ports");
+		if (ports) {
+			for_each_available_child_of_node(ports, port) {
+				err = of_property_read_u32(port, "reg", &reg);
+				if (err)
+					break;
+dbg_msg(" reg: %d\n", reg);
+				ethernet = of_parse_phandle(port, "ethernet", 0);
+				if (ethernet)
+dbg_msg(" found eth\n");
+				if (ethernet) {
+					name = of_get_property(port,
+							       "phy-mode",
+							       NULL);
+					if (name && !strcmp(name, "rmii"))
+						sw->interface =
+							PHY_INTERFACE_MODE_RMII;
+					if (name && !strcmp(name, "rgmii-id"))
+						sw->interface =
+							PHY_INTERFACE_MODE_RGMII_ID;
+				}
+				name = of_get_property(port, "label", NULL);
+				if (name)
+dbg_msg(" name: %s\n", name);
+				/* Save the device node. */
+				sw->devnode[reg] = port;
+				cnt++;
+
+				err = of_property_read_u32(port, "mode", &mode);
+				if (err)
+					continue;
+dbg_msg(" mode: %d\n", mode);
+				info = get_port_info(sw, reg);
+
+				/* Expect only one port is SGMII. */
+				if (info->intf == INTF_SGMII)
+					sw->sgmii_mode = (u8) mode;
+			}
+		}
+
+		/* Will get kernel crash if devnode is not present. */
+		if (cnt > sw->mib_port_cnt)
+			return 0;
+	}
+
+err:
+	cnt = 0;
+#if defined(CONFIG_PHYLINK) || defined(CONFIG_PHYLINK_MODULE)
+	dev_err(ks->dev, "Please fix the device tree for correct ports.");
+	cnt = -1;
+#endif
+	return cnt;
+}
+
+static int sw_open_dev(struct ksz_sw *sw, struct net_device *dev,
+	struct ksz_port *port, u8 *addr)
+{
+	int mode = 0;
+
+	sw_init_mib(sw);
+
+	sw->main_dev = dev;
+	sw->main_port = port;
+#ifdef CONFIG_KSZ_IBA_ONLY
+	if (sw->info->iba.use_iba >= IBA_USE_CODE_PREPARE)
+		port->need_mac = true;
+#endif
+	sw->net_ops->start(sw, addr);
+	if (sw->features & AVB_SUPPORT)
+		mode |= 1;
+	if (sw->dev_count > 1)
+		mode |= 1;
+	if (sw->features & DIFF_MAC_ADDR)
+		mode |= 2;
+#ifdef CONFIG_KSZ_HSR
+	if (sw->features & HSR_REDBOX)
+		mode |= 2;
+#endif
+	return mode;
+}  /* sw_open_dev */
+
+static void sw_open_port(struct ksz_sw *sw, struct net_device *dev,
+	struct ksz_port *port)
+{
+	uint i;
+	uint n;
+	uint p;
+	struct ksz_port_info *info;
+	struct ksz_port_info *host;
+
+#ifdef CONFIG_KSZ_IBA
+	if (!sw->info->iba.use_iba && dev == sw->main_dev)
+		sw_set_dev(sw, sw->main_dev, sw->main_dev->dev_addr);
+#endif
+	host = get_port_info(sw, sw->HOST_PORT);
+	for (i = 0, n = port->first_port; i < port->port_cnt; i++, n++) {
+		p = get_phy_port(sw, n);
+		info = get_port_info(sw, p);
+		if (!info->phy)
+			continue;
+
+		/*
+		 * Initialize to invalid value so that link detection
+		 * is done.
+		 */
+		info->link = 0xFF;
+		info->state = media_unknown;
+		info->tx_rate = host->tx_rate;
+		info->duplex = host->duplex;
+		if (port->port_cnt == 1) {
+			if (sw->netdev[0]) {
+				struct ksz_port *sw_port = sw->netport[0];
+
+				port->speed = sw_port->speed;
+				port->duplex = sw_port->duplex;
+				port->flow_ctrl = sw_port->flow_ctrl;
+			}
+			if (info->own_speed != port->speed ||
+			    info->own_duplex != port->duplex) {
+				if (info->own_speed)
+					port->speed = info->own_speed;
+				if (info->own_duplex)
+					port->duplex = info->own_duplex;
+			}
+		}
+	}
+	port->opened = true;
+	port->report = true;
+
+	sw->ops->acquire(sw);
+
+	/* Need to open the port in multiple device interfaces mode. */
+	if (sw->dev_count > 1 && (!sw->dev_offset || dev != sw->netdev[0])) {
+		port->state = STP_STATE_SIMPLE;
+		if (sw->dev_offset && !(sw->features & STP_SUPPORT)) {
+			port->state = STP_STATE_FORWARDING;
+		}
+		if (sw->features & SW_VLAN_DEV) {
+			p = get_phy_port(sw, port->first_port);
+			i = sw->info->port_cfg[p].index;
+			if (!(sw->eth_maps[i].proto & HSR_HW))
+				port->state = STP_STATE_FORWARDING;
+		}
+		for (i = 0, n = port->first_port; i < port->port_cnt;
+		     i++, n++) {
+			p = get_phy_port(sw, n);
+			if (p == sw->HOST_PORT)
+				continue;
+			sw->dev_ports |= (1 << p);
+#ifdef CONFIG_KSZ_STP
+			if (sw->features & STP_SUPPORT) {
+				stp_enable_port(&sw->info->rstp, p,
+						&port->state);
+			}
+#endif
+			port_set_stp_state(sw, p, port->state);
+		}
+	} else if (sw->dev_count == 1) {
+		sw->dev_ports = sw->PORT_MASK;
+	}
+
+	sw->phy_intr = sw->PORT_MASK;
+	if (port->force_link)
+		port_force_link_speed(port);
+	else
+		port_set_link_speed(port);
+	port_get_link_speed(port);
+	if (port->link_ports)
+		schedule_delayed_work(&port->link_update, 0);
+	sw->phy_intr = 0;
+	sw->ops->release(sw);
+	for (i = 0; i < sw->eth_cnt; i++) {
+		if (dev != sw->netdev[i])
+			continue;
+#ifdef CONFIG_KSZ_DLR
+		if (sw->eth_maps[i].proto & DLR_HW) {
+			struct ksz_dlr_info *info = &sw->info->dlr;
+
+			p = get_phy_port(sw, port->first_port);
+			if (info->ports[0] == p)
+				prep_dlr(info, dev, dev->dev_addr);
+		}
+#endif
+#ifdef CONFIG_KSZ_HSR
+		if (sw->eth_maps[i].proto & HSR_HW) {
+			struct ksz_hsr_info *info = &sw->info->hsr;
+
+			p = get_phy_port(sw, port->first_port);
+			if (info->ports[0] == p)
+				prep_hsr(info, dev, dev->dev_addr);
+		}
+		if (sw->eth_maps[i].proto & HSR_REDBOX) {
+			struct ksz_hsr_info *info = &sw->info->hsr;
+
+			start_hsr_redbox(info, dev);
+		}
+#endif
+	}
+dbg_msg("%s %d:%d r:%x t:%x d:%x l:%x"NL, __func__,
+port->first_port, port->port_cnt,
+sw->rx_ports[0], sw->tx_ports[0], sw->dev_ports, sw->live_ports);
+}  /* sw_open_port */
+
+static void sw_close_port(struct ksz_sw *sw, struct net_device *dev,
+	struct ksz_port *port)
+{
+	int i;
+	uint p;
+#ifdef CONFIG_KSZ_MRP
+	struct mrp_info *mrp = &sw->mrp;
+#endif
+
+	port->opened = false;
+
+#ifdef CONFIG_KSZ_IBA
+#ifdef CONFIG_KSZ_IBA_ONLY
+	if (IBA_USE_CODE_PREPARE <= sw->info->iba.use_iba &&
+	    dev == sw->main_dev)
+		return;
+#endif
+	if (sw->info->iba.use_iba && dev == sw->main_dev)
+		sw_set_dev(sw, NULL, sw->main_dev->dev_addr);
+	if (port == sw->main_port)
+		port->need_mac = false;
+#endif
+
+	/* Need to shut the port manually in multiple device interfaces mode. */
+	if (sw->dev_count > 1 && (!sw->dev_offset || dev != sw->netdev[0])) {
+		uint n;
+
+		sw->ops->acquire(sw);
+		for (i = 0, n = port->first_port; i < port->port_cnt;
+		     i++, n++) {
+			p = get_phy_port(sw, n);
+			if (p == sw->HOST_PORT)
+				continue;
+#ifdef CONFIG_KSZ_STP
+			if (sw->features & STP_SUPPORT)
+				stp_disable_port(&sw->info->rstp, p);
+#endif
+#ifdef CONFIG_KSZ_MRP
+			if (sw->features & MRP_SUPPORT) {
+				mrp_close_port(mrp, p);
+			}
+#endif
+			sw->dev_ports &= ~(1 << p);
+			port_set_stp_state(sw, p, STP_STATE_DISABLED);
+		}
+		sw->ops->release(sw);
+		if (sw->netport[0])
+			schedule_delayed_work(&sw->netport[0]->link_update, 0);
+	} else if (sw->dev_count == 1) {
+#ifdef CONFIG_KSZ_MRP
+		uint n;
+
+		if (sw->features & MRP_SUPPORT) {
+			for (n = 0; n <= sw->mib_port_cnt; n++) {
+				p = get_phy_port(sw, n);
+				mrp_close_port(mrp, p);
+			}
+		}
+#endif
+		sw->dev_ports = 0;
+	}
+	for (i = 0; i < sw->eth_cnt; i++) {
+		if (dev != sw->netdev[i])
+			continue;
+#ifdef CONFIG_KSZ_HSR
+		if (sw->eth_maps[i].proto & HSR_HW) {
+			struct ksz_hsr_info *info = &sw->info->hsr;
+
+			p = get_phy_port(sw, port->first_port);
+			if (info->ports[0] == p)
+				stop_hsr(info);
+		}
+		if (sw->eth_maps[i].proto & HSR_REDBOX) {
+			struct ksz_hsr_info *info = &sw->info->hsr;
+
+			stop_hsr_redbox(info, dev);
+		}
+#endif
+	}
+	sw_reset_multi(sw, port);
+}  /* sw_close_port */
+
+static void sw_open(struct ksz_sw *sw)
+{
+	sw->running = true;
+#if defined(CONFIG_PHYLINK) || defined(CONFIG_PHYLINK_MODULE)
+	sw_init_phylink(sw, sw->main_port);
+#endif
+	sw_setup_reserved_multicast(sw);
+#ifdef CONFIG_KSZ_AVB
+	if (sw->features & AVB_SUPPORT) {
+		struct ksz_port_info *info = get_port_info(sw, sw->HOST_PORT);
+		u32 speed = info->tx_rate / TX_RATE_UNIT;
+
+		mrp_set_speed(&sw->mrp, sw->HOST_PORT, speed, true);
+	}
+#endif
+#if defined(CONFIG_KSZ_AVB) || defined(CONFIG_KSZ_MRP)
+	if (sw->features & (AVB_SUPPORT | MRP_SUPPORT)) {
+		mrp_open(&sw->mrp);
+	}
+#endif
+#ifdef CONFIG_KSZ_MRP
+	if (sw->features & MRP_SUPPORT) {
+		schedule_delayed_work(&sw->set_mrp, msecs_to_jiffies(1000));
+	}
+#endif
+	/* Timer may already be started by the SPI device. */
+	if (!sw->mib_timer_info->max)
+		ksz_start_timer(sw->mib_timer_info,
+			sw->mib_timer_info->period);
+	if (!sw->monitor_timer_info->max)
+		ksz_start_timer(sw->monitor_timer_info,
+			sw->monitor_timer_info->period);
+}  /* sw_open */
+
+static void sw_close(struct ksz_sw *sw)
+{
+	int hw_access = true;
+
+#ifdef CONFIG_KSZ_IBA
+	if (IBA_USE_CODE_PREPARE <= sw->info->iba.use_iba) {
+		hw_access = false;
+	}
+#endif
+	sw->running = false;
+	flush_work(&sw->set_addr);
+	flush_work(&sw->tx_fwd);
+#if defined(CONFIG_PHYLINK) || defined(CONFIG_PHYLINK_MODULE)
+	sw_exit_phylink(sw, sw->main_port);
+#endif
+#if defined(CONFIG_KSZ_AVB) || defined(CONFIG_KSZ_MRP)
+	if (sw->features & (AVB_SUPPORT | MRP_SUPPORT))
+		mrp_close(&sw->mrp, hw_access);
+#endif
+	if (!hw_access) {
+		struct sw_priv *hw_priv = sw->dev;
+
+		ksz_stop_timer(&hw_priv->mib_timer_info);
+		flush_work(&hw_priv->mib_read);
+	}
+	ksz_stop_timer(sw->monitor_timer_info);
+	cancel_delayed_work_sync(sw->link_read);
+}  /* sw_close */
+
+static void sw_delayed_set_addr(struct work_struct *work)
+{
+	struct ksz_sw *sw = container_of(work, struct ksz_sw, set_addr);
+
+#ifdef CONFIG_KSZ_IBA
+	if (iba_stopped(sw))
+		return;
+#endif
+
+	sw->ops->acquire(sw);
+	sw_set_addr(sw, sw->netdev[0]->dev_addr);
+	sw->ops->release(sw);
+}  /* sw_delayed_set_addr */
+
+static u8 sw_set_mac_addr(struct ksz_sw *sw, struct net_device *dev,
+	u8 promiscuous, uint port)
+{
+	int n;
+#ifdef CONFIG_KSZ_IBA
+	u8 promisc = promiscuous;
+#endif
+
+	/* See if different MAC addresses are used. */
+	if (sw->dev_count > 1) {
+		int i;
+		int dev_count = sw->dev_count + sw->dev_offset;
+
+		for (i = 0; i < dev_count; i++) {
+			if (dev == sw->netdev[i])
+				continue;
+			if (memcmp(sw->netdev[i]->dev_addr,
+			    dev->dev_addr, ETH_ALEN))
+				break;
+		}
+		if (sw->features & DIFF_MAC_ADDR) {
+			struct ksz_port *priv;
+			struct ksz_port_info *info;
+			uint p;
+
+			/* All addresses are the same. */
+			if (i == dev_count) {
+				sw->features &= ~DIFF_MAC_ADDR;
+				--promiscuous;
+			} else if (sw->dev_offset && dev == sw->netdev[0]) {
+				for (n = 1; n < dev_count; n++) {
+					priv = sw->netport[n];
+					p = get_phy_port(sw, priv->first_port);
+					info = get_port_info(sw, p);
+					inc_mac_addr(info->mac_addr,
+						dev->dev_addr, n);
+					inc_mac_addr(sw->netdev[n]->dev_addr,
+						dev->dev_addr, n);
+				}
+			}
+		} else {
+			if (dev == sw->netdev[0] && i < dev_count) {
+
+				/* Make MAC address the same in all devices. */
+				for (i = 1; i < dev_count; i++) {
+					memcpy(sw->netdev[i]->dev_addr,
+						dev->dev_addr, ETH_ALEN);
+				}
+			} else {
+				if (i < dev_count) {
+					sw->features |= DIFF_MAC_ADDR;
+					++promiscuous;
+				}
+			}
+		}
+	}
+	if (dev == sw->netdev[0])
+		schedule_work(&sw->set_addr);
+	for (n = 0; n < sw->eth_cnt; n++) {
+		if (sw->netdev[n] != dev)
+			continue;
+#ifdef CONFIG_KSZ_STP
+		if (sw->features & STP_SUPPORT) {
+			struct ksz_stp_info *stp = &sw->info->rstp;
+
+			stp->ops->change_addr(stp, dev->dev_addr);
+		}
+#endif
+#ifdef CONFIG_KSZ_DLR
+		if (sw->eth_maps[n].proto & DLR_HW) {
+			dlr_change_addr(&sw->info->dlr, dev->dev_addr);
+		}
+#endif
+#ifdef CONFIG_KSZ_HSR
+		if (sw->eth_maps[n].proto & (HSR_HW | HSR_REDBOX)) {
+			struct ksz_hsr_info *hsr = &sw->info->hsr;
+
+			hsr->ops->change_addr(hsr, dev);
+		}
+#endif
+	}
+	if (dev != sw->netdev[0])
+		return promiscuous;
+
+#ifdef CONFIG_1588_PTP
+	if (sw->features & PTP_HW) {
+		struct ptp_info *ptp = &sw->ptp_hw;
+
+		ptp->ops->set_identity(ptp, dev->dev_addr);
+	}
+#endif
+
+#ifdef CONFIG_KSZ_IBA
+	if (netif_running(dev)) {
+		sw_set_dev(sw, dev, dev->dev_addr);
+
+		/* A hack to accept IBA response. */
+		if (!promisc)
+			promiscuous = 2;
+	}
+#endif
+#ifdef CAPTURE_IBA
+promiscuous = 1;
+#endif
+	return promiscuous;
+}  /* sw_set_mac_addr */
+
+static struct ksz_dev_major sw_majors[MAX_SW_DEVICES];
+
+static struct file_dev_info *alloc_sw_dev_info(struct ksz_sw *sw, uint minor)
+{
+	struct file_dev_info *info;
+
+	info = kzalloc(sizeof(struct file_dev_info), GFP_KERNEL);
+	if (info) {
+		info->dev = sw;
+		sema_init(&info->sem, 1);
+		mutex_init(&info->lock);
+		init_waitqueue_head(&info->wait_msg);
+		info->read_max = 60000;
+		info->read_tmp = MAX_SW_LEN;
+		info->read_buf = kzalloc(info->read_max + info->read_tmp,
+			GFP_KERNEL);
+		info->read_in = &info->read_buf[info->read_max];
+		info->write_len = 1000;
+		info->write_buf = kzalloc(info->write_len, GFP_KERNEL);
+
+		info->minor = minor;
+		info->next = sw->dev_list[minor];
+		sw->dev_list[minor] = info;
+	}
+	return info;
+}  /* alloc_sw_dev_info */
+
+static void free_sw_dev_info(struct file_dev_info *info)
+{
+	if (info) {
+		struct ksz_sw *sw = info->dev;
+		uint minor = info->minor;
+
+		file_dev_clear_notify(sw->dev_list[minor], info, DEV_MOD_BASE,
+				      &sw->notifications);
+#ifdef CONFIG_KSZ_MRP_
+		do {
+			struct mrp_info *mrp = &sw->mrp;
+
+			file_dev_clear_notify(sw->dev_list[minor], info,
+					      DEV_MOD_MRP,
+					      &mrp->notifications);
+		} while (0);
+#endif
+#ifdef CONFIG_KSZ_DLR
+		do {
+			struct ksz_dlr_info *dlr = &sw->info->dlr;
+
+			file_dev_clear_notify(sw->dev_list[minor], info,
+					      DEV_MOD_DLR,
+					      &dlr->notifications);
+		} while (0);
+#endif
+#ifdef CONFIG_KSZ_HSR
+		do {
+			struct ksz_hsr_info *hsr = &sw->info->hsr;
+
+			file_dev_clear_notify(sw->dev_list[minor], info,
+					      DEV_MOD_HSR,
+					      &hsr->notifications);
+		} while (0);
+#endif
+		file_gen_dev_release(info, &sw->dev_list[minor]);
+	}
+}  /* free_sw_dev_info */
+
+static int sw_dev_open(struct inode *inode, struct file *filp)
+{
+	struct file_dev_info *info = (struct file_dev_info *)
+		filp->private_data;
+	uint minor = MINOR(inode->i_rdev);
+	uint major = MAJOR(inode->i_rdev);
+	struct ksz_sw *sw = NULL;
+	int i;
+
+	if (minor > 1)
+		return -ENODEV;
+	for (i = 0; i < MAX_SW_DEVICES; i++) {
+		if (sw_majors[i].major == major) {
+			sw = sw_majors[i].dev;
+			break;
+		}
+	}
+	if (!sw)
+		return -ENODEV;
+	if (!info) {
+		info = alloc_sw_dev_info(sw, minor);
+		if (info)
+			filp->private_data = info;
+		else
+			return -ENOMEM;
+	}
+	return 0;
+}  /* sw_dev_open */
+
+static int sw_dev_release(struct inode *inode, struct file *filp)
+{
+	struct file_dev_info *info = (struct file_dev_info *)
+		filp->private_data;
+
+	free_sw_dev_info(info);
+	filp->private_data = NULL;
+	return 0;
+}  /* sw_dev_release */
+
+static int sw_get_attrib(struct ksz_sw *sw, int subcmd, int size,
+	int *req_size, size_t *len, u8 *data, int *output)
+{
+	struct ksz_info_opt *opt = (struct ksz_info_opt *) data;
+	struct ksz_info_cfg *cfg = &opt->data.cfg;
+	uint i;
+	uint n;
+	uint p;
+
+	*len = 0;
+	*output = 0;
+	switch (subcmd) {
+	case DEV_SW_CFG:
+		n = opt->num;
+		p = opt->port;
+		if (!n)
+			n = 1;
+		*len = 2 + n * sizeof(struct ksz_info_cfg);
+		if (n > sw->mib_port_cnt ||
+		    p > sw->mib_port_cnt)
+			return DEV_IOC_INVALID_CMD;
+		break;
+	}
+	if (!*len)
+		return DEV_IOC_INVALID_CMD;
+	if (size < *len) {
+		*req_size = *len + SIZEOF_ksz_request;
+		return DEV_IOC_INVALID_LEN;
+	}
+	switch (subcmd) {
+	case DEV_SW_CFG:
+		n = opt->port;
+		sw->ops->acquire(sw);
+		for (i = 0; i < opt->num; i++, n++) {
+			p = get_phy_port(sw, n);
+			cfg->on_off = 0;
+			if (cfg->set & SP_LEARN) {
+				if (!port_chk_dis_learn(sw, p))
+					cfg->on_off |= SP_LEARN;
+			}
+			if (cfg->set & SP_RX) {
+				if (port_chk_rx(sw, p))
+					cfg->on_off |= SP_RX;
+			}
+			if (cfg->set & SP_TX) {
+				if (port_chk_tx(sw, p))
+					cfg->on_off |= SP_TX;
+			}
+			if (p == sw->HOST_PORT)
+				continue;
+			if (cfg->set & SP_PHY_POWER) {
+				if (port_chk_power(sw, p))
+					cfg->on_off |= SP_PHY_POWER;
+			}
+			cfg++;
+		}
+		sw->ops->release(sw);
+		break;
+	}
+	return DEV_IOC_OK;
+}  /* sw_get_attrib */
+
+static int sw_set_attrib(struct ksz_sw *sw, int subcmd, int size,
+	int *req_size, u8 *data, int *output)
+{
+	struct ksz_info_opt *opt = (struct ksz_info_opt *) data;
+	struct ksz_info_cfg *cfg = &opt->data.cfg;
+	int len;
+	uint i;
+	uint n;
+	uint p;
+
+	*output = 0;
+	switch (subcmd) {
+	case DEV_SW_CFG:
+		n = opt->num;
+		p = opt->port;
+		if (!n)
+			n = 1;
+		len = 2 + n * sizeof(struct ksz_info_cfg);
+		if (size < len)
+			goto not_enough;
+		if (n > sw->mib_port_cnt ||
+		    p > sw->mib_port_cnt)
+			return DEV_IOC_INVALID_CMD;
+		n = opt->port;
+		sw->ops->acquire(sw);
+		for (i = 0; i < opt->num; i++, n++) {
+			p = get_phy_port(sw, n);
+			if (cfg->set & SP_LEARN)
+				port_cfg_dis_learn(sw, p,
+					!(cfg->on_off & SP_LEARN));
+			if (cfg->set & SP_RX)
+				port_cfg_rx_special(sw, p,
+					!!(cfg->on_off & SP_RX));
+			if (cfg->set & SP_TX)
+				port_cfg_tx(sw, p,
+					!!(cfg->on_off & SP_TX));
+			if (p == sw->HOST_PORT)
+				continue;
+			if (cfg->set & SP_PHY_POWER)
+				port_cfg_power(sw, p,
+					!!(cfg->on_off & SP_PHY_POWER));
+			cfg++;
+		}
+		sw->ops->release(sw);
+		break;
+	default:
+		return DEV_IOC_INVALID_CMD;
+	}
+	return DEV_IOC_OK;
+
+not_enough:
+	*req_size = len + SIZEOF_ksz_request;
+	return DEV_IOC_INVALID_LEN;
+}  /* sw_set_attrib */
+
+static int sw_get_info(struct ksz_sw *sw, int subcmd, int size,
+	int *req_size, size_t *len, u8 *data)
+{
+	struct ksz_info_opt *opt = (struct ksz_info_opt *) data;
+	struct ksz_info_speed *speed = &opt->data.speed;
+	struct ksz_port_info *info;
+	uint i;
+	uint n;
+	uint p;
+
+	*len = 0;
+	switch (subcmd) {
+	case DEV_INFO_SW_LINK:
+		n = opt->num;
+		p = opt->port;
+		if (!n)
+			n = 1;
+		*len = 2 + n * sizeof(struct ksz_info_speed);
+		if (n > sw->mib_port_cnt ||
+		    p > sw->mib_port_cnt)
+			return DEV_IOC_INVALID_CMD;
+		break;
+	}
+	if (!*len)
+		return DEV_IOC_INVALID_CMD;
+	if (size < *len) {
+		*req_size = *len + SIZEOF_ksz_request;
+		return DEV_IOC_INVALID_LEN;
+	}
+	switch (subcmd) {
+	case DEV_INFO_SW_LINK:
+		n = p;
+		for (i = 0; i < opt->num; i++, n++) {
+			p = get_phy_port(sw, n);
+			info = get_port_info(sw, p);
+			if (info->state == media_connected) {
+				speed->tx_rate = info->tx_rate;
+				speed->duplex = info->duplex;
+				speed->flow_ctrl = info->flow_ctrl;
+			} else {
+				memset(speed, 0, sizeof(struct ksz_info_speed));
+			}
+			++speed;
+		}
+		break;
+	}
+	return DEV_IOC_OK;
+}  /* sw_get_info */
+
+static int base_dev_req(struct ksz_sw *sw, char *arg, void *info)
+{
+	struct ksz_request *req = (struct ksz_request *) arg;
+	int len;
+	int maincmd;
+	int req_size;
+	int subcmd;
+	int output;
+	size_t param_size;
+	u8 data[PARAM_DATA_SIZE];
+	struct ksz_resp_msg *msg = (struct ksz_resp_msg *) data;
+	int err = 0;
+	int result = 0;
+
+	get_user_data(&req_size, &req->size, info);
+	get_user_data(&maincmd, &req->cmd, info);
+	get_user_data(&subcmd, &req->subcmd, info);
+	get_user_data(&output, &req->output, info);
+	len = req_size - SIZEOF_ksz_request;
+
+	maincmd &= 0xffff;
+	switch (maincmd) {
+	case DEV_CMD_INFO:
+		switch (subcmd) {
+		case DEV_INFO_INIT:
+			req_size = SIZEOF_ksz_request + 4;
+			if (len >= 4) {
+				data[0] = 'M';
+				data[1] = 'i';
+				data[2] = 'c';
+				data[3] = 'r';
+				data[4] = 1;
+				data[5] = sw->mib_port_cnt;
+				err = write_user_data(data, req->param.data,
+					6, info);
+				if (err)
+					goto dev_ioctl_done;
+			} else
+				result = DEV_IOC_INVALID_LEN;
+			break;
+		case DEV_INFO_EXIT:
+			fallthrough;
+
+		case DEV_INFO_QUIT:
+
+			/* Not called through char device. */
+			if (!info)
+				break;
+			file_dev_clear_notify(sw->dev_list[0], info,
+					      DEV_MOD_BASE,
+					      &sw->notifications);
+			msg->module = DEV_MOD_BASE;
+			msg->cmd = DEV_INFO_QUIT;
+			msg->resp.data[0] = 0;
+			file_dev_setup_msg(info, msg, 8, NULL, NULL);
+			break;
+		case DEV_INFO_NOTIFY:
+			if (len >= 4) {
+				struct file_dev_info *dev_info = info;
+				uint *notify = (uint *) data;
+
+				_chk_ioctl_size(len, 4, 0, &req_size, &result,
+					&req->param, data, info);
+				dev_info->notifications[DEV_MOD_BASE] =
+					*notify;
+				sw->notifications |= *notify;
+			}
+			break;
+		case DEV_INFO_SW_LINK:
+			if (_chk_ioctl_size(len, len, 0, &req_size, &result,
+			    &req->param, data, info))
+				goto dev_ioctl_resp;
+			result = sw_get_info(sw, subcmd, len, &req_size,
+					     &param_size, data);
+			if (result)
+				goto dev_ioctl_resp;
+			err = write_user_data(data, req->param.data,
+					      param_size, info);
+			if (err)
+				goto dev_ioctl_done;
+			req_size = param_size + SIZEOF_ksz_request;
+			break;
+		default:
+			result = DEV_IOC_INVALID_CMD;
+			break;
+		}
+		break;
+	case DEV_CMD_PUT:
+		if (_chk_ioctl_size(len, len, 0, &req_size, &result,
+		    &req->param, data, info))
+			goto dev_ioctl_resp;
+		result = sw_set_attrib(sw, subcmd, len, &req_size, data,
+			&output);
+		if (result)
+			goto dev_ioctl_resp;
+		put_user_data(&output, &req->output, info);
+		break;
+	case DEV_CMD_GET:
+		if (_chk_ioctl_size(len, len, 0, &req_size, &result,
+		    &req->param, data, info))
+			goto dev_ioctl_resp;
+		result = sw_get_attrib(sw, subcmd, len, &req_size,
+			&param_size, data, &output);
+		if (result)
+			goto dev_ioctl_resp;
+		err = write_user_data(data, req->param.data, param_size, info);
+		if (err)
+			goto dev_ioctl_done;
+		req_size = param_size + SIZEOF_ksz_request;
+		put_user_data(&output, &req->output, info);
+		break;
+	default:
+		result = DEV_IOC_INVALID_CMD;
+		break;
+	}
+
+dev_ioctl_resp:
+	put_user_data(&req_size, &req->size, info);
+	put_user_data(&result, &req->result, info);
+
+	/* Return ERESTARTSYS so that the system call is called again. */
+	if (result < 0)
+		err = result;
+
+dev_ioctl_done:
+	return err;
+}  /* base_dev_req */
+
+static int sw_dev_req(struct ksz_sw *sw, char *arg, struct file_dev_info *info)
+{
+	struct ksz_request *req = (struct ksz_request *) arg;
+	int maincmd;
+	int req_size;
+	int err = 0;
+	int result = DEV_IOC_OK;
+
+	/* Check request size. */
+	get_user_data(&req_size, &req->size, info);
+	if (chk_ioctl_size(req_size, SIZEOF_ksz_request, 0, &req_size,
+	    &result, NULL, NULL))
+		goto dev_ioctl_resp;
+
+	result = -EOPNOTSUPP;
+	get_user_data(&maincmd, &req->cmd, info);
+	maincmd >>= 16;
+	switch (maincmd) {
+	case DEV_MOD_BASE:
+		err = base_dev_req(sw, arg, info);
+		result = 0;
+		break;
+#ifdef CONFIG_1588_PTP
+	case DEV_MOD_PTP:
+		if (sw->features & PTP_HW) {
+			struct ptp_info *ptp = &sw->ptp_hw;
+
+			err = ptp->ops->dev_req(ptp, arg, info);
+			result = 0;
+		}
+		break;
+#endif
+#ifdef CONFIG_KSZ_MRP
+	case DEV_MOD_MRP:
+		if (sw->features & MRP_SUPPORT) {
+			struct mrp_info *mrp = &sw->mrp;
+
+			err = mrp->ops->dev_req(mrp, arg);
+			result = 0;
+		}
+		break;
+#endif
+#ifdef CONFIG_KSZ_DLR
+	case DEV_MOD_DLR:
+		if (sw->features & DLR_HW) {
+			struct ksz_dlr_info *dlr = &sw->info->dlr;
+
+			err = dlr->ops->dev_req(dlr, arg, info);
+			result = 0;
+		}
+		break;
+#endif
+#ifdef CONFIG_KSZ_HSR
+	case DEV_MOD_HSR:
+		if (sw->features & HSR_HW) {
+			struct ksz_hsr_info *hsr = &sw->info->hsr;
+
+			err = hsr->ops->dev_req(hsr, arg, info);
+			result = 0;
+		}
+		break;
+#endif
+	default:
+		break;
+	}
+
+	/* Processed by specific module. */
+	if (!result)
+		return err;
+	if (result < 0)
+		goto dev_ioctl_done;
+
+dev_ioctl_resp:
+	put_user_data(&req_size, &req->size, info);
+	put_user_data(&result, &req->result, info);
+
+dev_ioctl_done:
+
+	/* Return ERESTARTSYS so that the system call is called again. */
+	if (result < 0)
+		err = result;
+
+	return err;
+}  /* sw_dev_req */
+
+static ssize_t sw_dev_read(struct file *filp, char *buf, size_t count,
+	loff_t *offp)
+{
+	struct file_dev_info *info = (struct file_dev_info *)
+		filp->private_data;
+	ssize_t result = 0;
+	int rc;
+
+	if (!info->read_len) {
+		*offp = 0;
+		rc = wait_event_interruptible(info->wait_msg,
+			0 != info->read_len);
+
+		/* Cannot continue if ERESTARTSYS. */
+		if (rc < 0)
+			return 0;
+	}
+
+	if (down_interruptible(&info->sem))
+		return -ERESTARTSYS;
+
+	mutex_lock(&info->lock);
+	if (*offp >= info->read_len) {
+		info->read_len = 0;
+		count = 0;
+		*offp = 0;
+		goto dev_read_done;
+	}
+
+	if (*offp + count > info->read_len) {
+		count = info->read_len - *offp;
+		info->read_len = 0;
+	}
+
+	if (copy_to_user(buf, &info->read_buf[*offp], count)) {
+		result = -EFAULT;
+		goto dev_read_done;
+	}
+	if (info->read_len)
+		*offp += count;
+	else
+		*offp = 0;
+	result = count;
+
+dev_read_done:
+	mutex_unlock(&info->lock);
+	up(&info->sem);
+	return result;
+}  /* sw_dev_read */
+
+static long sw_dev_ioctl(struct file *filp, unsigned int cmd,
+	unsigned long arg)
+{
+	struct file_dev_info *info = (struct file_dev_info *)
+		filp->private_data;
+	struct ksz_sw *sw = info->dev;
+	int err = 0;
+
+	if (_IOC_TYPE(cmd) != DEV_IOC_MAGIC)
+		return -ENOTTY;
+	if (_IOC_NR(cmd) > DEV_IOC_MAX)
+		return -ENOTTY;
+	if (_IOC_DIR(cmd) & _IOC_READ)
+		err = !access_ok((void *) arg, _IOC_SIZE(cmd));
+	else if (_IOC_DIR(cmd) & _IOC_WRITE)
+		err = !access_ok((void *) arg, _IOC_SIZE(cmd));
+	if (err) {
+		printk(KERN_ALERT "err fault"NL);
+		return -EFAULT;
+	}
+	if (down_interruptible(&info->sem))
+		return -ERESTARTSYS;
+
+	err = sw_dev_req(sw, (char *) arg, info);
+	up(&info->sem);
+	return err;
+}  /* sw_dev_ioctl */
+
+static ssize_t sw_dev_write(struct file *filp, const char *buf, size_t count,
+	loff_t *offp)
+{
+	struct file_dev_info *info = (struct file_dev_info *)
+		filp->private_data;
+	ssize_t result = 0;
+	size_t size;
+	int rc;
+
+	if (!count)
+		return result;
+
+	if (down_interruptible(&info->sem))
+		return -ERESTARTSYS;
+
+	if (*offp >= info->write_len) {
+		result = -ENOSPC;
+		goto dev_write_done;
+	}
+	if (*offp + count > info->write_len)
+		count = info->write_len - *offp;
+	if (copy_from_user(info->write_buf, buf, count)) {
+		result = -EFAULT;
+		goto dev_write_done;
+	}
+	size = 0;
+	result = size;
+	rc = 0;
+	if (rc)
+		result = rc;
+
+dev_write_done:
+	up(&info->sem);
+	return result;
+}  /* sw_dev_write */
+
+static const struct file_operations sw_dev_fops = {
+	.read		= sw_dev_read,
+	.write		= sw_dev_write,
+	.unlocked_ioctl	= sw_dev_ioctl,
+	.open		= sw_dev_open,
+	.release	= sw_dev_release,
+};
+
+static struct class *sw_class[MAX_SW_DEVICES];
+
+static int init_sw_dev(int id, int dev_major, char *dev_name)
+{
+	int result;
+
+	result = register_chrdev(dev_major, dev_name, &sw_dev_fops);
+	if (result < 0) {
+		printk(KERN_WARNING "%s: can't get major %d"NL, dev_name,
+			dev_major);
+		return result;
+	}
+	if (0 == dev_major)
+		dev_major = result;
+	sw_class[id] = class_create(THIS_MODULE, dev_name);
+	if (IS_ERR(sw_class[id])) {
+		unregister_chrdev(dev_major, dev_name);
+		return -ENODEV;
+	}
+	device_create(sw_class[id], NULL, MKDEV(dev_major, 0), NULL, dev_name);
+	return dev_major;
+}  /* init_sw_dev */
+
+static void exit_sw_dev(int id, int dev_major, char *dev_name)
+{
+	device_destroy(sw_class[id], MKDEV(dev_major, 0));
+	class_destroy(sw_class[id]);
+	unregister_chrdev(dev_major, dev_name);
+}  /* exit_sw_dev */
+
+static void sw_init_dev(struct ksz_sw *sw)
+{
+	sprintf(sw->dev_name, "sw_dev");
+	if (sw->id)
+		sprintf(sw->dev_name, "sw_dev_%u", sw->id);
+	sw->dev_major = init_sw_dev(sw->id, 0, sw->dev_name);
+	if (sw->dev_major < 0)
+		return;
+	sw->msg_buf = kzalloc(MAX_SW_LEN, GFP_KERNEL);
+	sw_majors[sw->id].dev = sw;
+	sw_majors[sw->id].major = sw->dev_major;
+}  /* sw_init_dev */
+
+static void sw_exit_dev(struct ksz_sw *sw)
+{
+	kfree(sw->msg_buf);
+	if (sw->dev_major >= 0)
+		exit_sw_dev(sw->id, sw->dev_major, sw->dev_name);
+}  /* sw_exit_dev */
+
+/* -------------------------------------------------------------------------- */
+
+static void sw_report_link(struct ksz_sw *sw, struct ksz_port *port,
+			   struct ksz_port_info *info)
+{
+	struct ksz_port_info *linked = port->linked;
+	struct phy_device *phydev = port->phydev;
+	struct net_device *dev = port->netdev;
+	int lpa = info->partner & 0xffff;
+	int lpagb = info->partner >> 16;
+	int phy_link = 0;
+	int link;
+
+	if (port == sw->netport[0]) {
+		struct ksz_port_info *host = get_port_info(sw, sw->HOST_PORT);
+
+		if (info != host)
+			info = host;
+	}
+	phydev->link = (info->state == media_connected);
+	phydev->speed = info->tx_rate / TX_RATE_UNIT;
+	if (!phydev->speed)
+		phydev->speed = 10;
+	phydev->duplex = (info->duplex == 2);
+
+	/* Not using PHY provided by the switch driver. */
+	if (dev->phydev && dev->phydev != phydev) {
+		dev->phydev->link = phydev->link;
+		if (!dev->phydev->is_pseudo_fixed_link) {
+			dev->phydev->speed = phydev->speed;
+			dev->phydev->duplex = phydev->duplex;
+		}
+	}
+
+#if defined(CONFIG_PHYLINK) || defined(CONFIG_PHYLINK_MODULE)
+	/* Not started yet. */
+	if (sw->phylink_ops && port != sw->main_port &&
+	    phydev->state == PHY_READY)
+		return;
+#endif
+	if (phydev->link)
+		phy_link = (linked->state == media_connected);
+	link = netif_carrier_ok(dev);
+	if (port->report) {
+		port->report = false;
+		link = !phy_link;
+	}
+	if (sw->overrides & DELAY_UPDATE_LINK) {
+		uint p = port->linked->phy_id - 1;
+		struct ksz_port_cfg *cfg;
+
+		cfg = get_port_cfg(sw, p);
+		if (phy_link && cfg->stp_state[0] != STP_STATE_FORWARDING)
+			link = phy_link;
+	}
+
+#ifdef CONFIG_KSZ_HSR
+	if (sw->features & HSR_HW) {
+		struct ksz_hsr_info *hsr = &sw->info->hsr;
+
+		if (dev == hsr->dev) {
+			hsr->hsr_up = phy_link;
+		} else if (dev == hsr->redbox) {
+			bool up = hsr->redbox_up;
+
+			hsr->redbox_up = phy_link;
+			if (up != hsr->redbox_up && up)
+				hsr_rmv_slaves(hsr);
+		}
+
+		/* Main device is used for communication so simulate link on
+		 * when Redbox is connected.
+		 */
+		if (sw->overrides & HSR_FORWARD) {
+			if (dev == hsr->dev) {
+				if (!phy_link)
+					phy_link = hsr->redbox_up;
+			} else if (!hsr->hsr_up) {
+				if (hsr->redbox_up &&
+				    !netif_carrier_ok(hsr->dev))
+					netif_carrier_on(hsr->dev);
+				else if (!hsr->redbox_up &&
+					 netif_carrier_ok(hsr->dev))
+					netif_carrier_off(hsr->dev);
+			}
+		}
+	}
+#endif
+	if (phy_link == link)
+		return;
+
+	/* Update link partner capabilities. */
+	if (lpa) {
+		mii_stat1000_mod_linkmode_lpa_t(phydev->lp_advertising,
+						lpagb);
+		mii_lpa_mod_linkmode_lpa_t(phydev->lp_advertising, lpa);
+#if 0
+dbg_msg(" %*pb\n", __ETHTOOL_LINK_MODE_MASK_NBITS, phydev->advertising);
+dbg_msg(" %*pb\n", __ETHTOOL_LINK_MODE_MASK_NBITS, phydev->lp_advertising);
+#endif
+	}
+	if (netif_msg_link(sw))
+		pr_info("%s link %s"NL,
+			dev->name,
+			phy_link ? "on" : "off");
+	if (phydev->phy_link_change) {
+		phydev->phy_link_change(phydev, phy_link);
+	} else if (phy_link != link) {
+		if (phy_link)
+			netif_carrier_on(dev);
+		else
+			netif_carrier_off(dev);
+	}
+}  /* sw_report_link */
+
+static void link_update_work(struct work_struct *work)
+{
+	struct delayed_work *dwork = to_delayed_work(work);
+	struct ksz_port *port =
+		container_of(dwork, struct ksz_port, link_update);
+	struct ksz_sw *sw = port->sw;
+	struct ksz_port_info *info;
+	uint i;
+	uint p;
+#ifdef CONFIG_KSZ_AVB
+	u32 speed;
+	bool duplex;
+#endif
+
+#ifdef CONFIG_KSZ_IBA
+	if (iba_stopped(sw))
+		return;
+#endif
+
+	/* Netdevice associated with port was closed. */
+	if (!port->opened)
+		goto do_main;
+
+#ifdef CONFIG_KSZ_DLR
+	if (sw->features & DLR_HW) {
+		struct ksz_dlr_info *dlr = &sw->info->dlr;
+
+		dlr->ops->link_change(dlr,
+			sw->port_info[dlr->ports[0]].state == media_connected,
+			sw->port_info[dlr->ports[1]].state == media_connected);
+	}
+#endif
+
+	sw_notify_link_change(sw, port->link_ports);
+
+	if (sw->dev_offset && sw->netport[0]) {
+		int dev_cnt = sw->dev_count + sw->dev_offset;
+		struct ksz_port *sw_port = sw->netport[0];
+		struct ksz_port_info *linked = NULL;
+		struct ksz_port *dev_port;
+
+		for (i = sw->dev_offset; i < dev_cnt; i++) {
+			struct phy_priv *phydata;
+
+			dev_port = sw->netport[i];
+			if (!dev_port) {
+				phydata = &sw->phydata[i];
+				dev_port = phydata->port;
+			}
+			if (media_connected == dev_port->linked->state &&
+			    dev_port->linked->phy) {
+
+				/* Indicate a linked port is found .*/
+				if (!linked)
+					linked = dev_port->linked;
+				if (sw->overrides & DELAY_UPDATE_LINK) {
+					struct ksz_port_cfg *cfg;
+					uint p;
+
+					p = dev_port->linked->phy_id - 1;
+					cfg = get_port_cfg(sw, p);
+					if (cfg->stp_state[0] !=
+					    STP_STATE_FORWARDING)
+						continue;
+				}
+				sw_port->linked = dev_port->linked;
+				linked = NULL;
+				break;
+			}
+		}
+
+		/* Linked port not set before because it is not ready. */
+		if (linked)
+			sw_port->linked = linked;
+	}
+
+	/* Report link for only device or child devices. */
+	if ((!sw->dev_offset || port != sw->netport[0]) && port->netdev)
+		sw_report_link(sw, port, port->linked);
+
+#ifdef CONFIG_KSZ_STP
+	if (sw->features & STP_SUPPORT) {
+		struct ksz_stp_info *stp = &sw->info->rstp;
+
+		stp->ops->link_change(stp, true);
+	}
+#endif
+
+	for (i = 1; i <= sw->mib_port_cnt; i++) {
+		p = get_phy_port(sw, i);
+		if (!(port->link_ports & (1 << p)))
+			continue;
+
+#ifdef CONFIG_KSZ_AVB
+		info = get_port_info(sw, p);
+		speed = (media_connected == info->state) ?
+			info->tx_rate / TX_RATE_UNIT : 0;
+		duplex = (info->duplex == 2);
+		if (sw->features & AVB_SUPPORT) {
+			struct mrp_info *mrp = &sw->mrp;
+
+			mrp_set_speed(mrp, p, speed, duplex);
+		}
+#endif
+	}
+	if (port->link_ports) {
+
+#ifdef CONFIG_1588_PTP
+		if (sw->features & PTP_HW) {
+			struct ptp_info *ptp = &sw->ptp_hw;
+
+			ptp->link_ports = port->link_ports;
+			if (ptp->started)
+				set_latency(&ptp->set_latency);
+		}
+#endif
+	}
+
+#ifdef CONFIG_KSZ_HSR
+	if (sw->features & HSR_HW) {
+		struct ksz_hsr_info *hsr = &sw->info->hsr;
+
+		p = get_phy_port(sw, port->first_port);
+		if (hsr->dev && hsr->ports[0] == p) {
+			hsr->ops->link_change(hsr,
+				sw->port_info[hsr->ports[0]].state ==
+				media_connected,
+				sw->port_info[hsr->ports[1]].state ==
+				media_connected);
+			hsr->ops->check_announce(hsr);
+		}
+	}
+#endif
+
+do_main:
+	port->link_ports = 0;
+
+	/* There is an extra network device for the main device. */
+	/* The switch is always linked; speed and duplex are also fixed. */
+	if (sw->dev_offset) {
+		port = sw->netport[0];
+		if (port && port->opened && netif_running(port->netdev)) {
+			info = get_port_info(sw, sw->HOST_PORT);
+			sw_report_link(sw, port, info);
+		}
+	}
+}  /* link_update_work */
+
+static void set_phy_support(struct ksz_port *port, struct phy_device *phydev)
+{
+	switch (port->flow_ctrl) {
+	case PHY_NO_FLOW_CTRL:
+		linkmode_clear_bit(ETHTOOL_LINK_MODE_Pause_BIT,
+				   phydev->supported);
+		linkmode_clear_bit(ETHTOOL_LINK_MODE_Asym_Pause_BIT,
+				   phydev->supported);
+		break;
+	case PHY_RX_ONLY:
+		/* Depend on link partner to only advertise Asym_Pause. */
+		linkmode_set_bit(ETHTOOL_LINK_MODE_Pause_BIT,
+				 phydev->supported);
+		linkmode_set_bit(ETHTOOL_LINK_MODE_Asym_Pause_BIT,
+				 phydev->supported);
+		break;
+	case PHY_TX_ONLY:
+		linkmode_clear_bit(ETHTOOL_LINK_MODE_Pause_BIT,
+				   phydev->supported);
+		linkmode_set_bit(ETHTOOL_LINK_MODE_Asym_Pause_BIT,
+				 phydev->supported);
+		break;
+	default:
+		linkmode_set_bit(ETHTOOL_LINK_MODE_Pause_BIT,
+				 phydev->supported);
+		linkmode_clear_bit(ETHTOOL_LINK_MODE_Asym_Pause_BIT,
+				   phydev->supported);
+		break;
+	}
+}  /* set_phy_support */
+
+
+#if defined(CONFIG_KSZ_DLR) && !defined(CONFIG_KSZ_MSRP) && !defined(CONFIG_KSZ_HSR)
+#define USE_DLR
+#endif
+
+#if defined(CONFIG_KSZ_HSR) && !defined(CONFIG_KSZ_MSRP) && !defined(CONFIG_KSZ_DLR)
+#define USE_HSR
+#define USE_HSR_REDBOX
+#endif
+
+#if 0
+#define USE_DLR
+#if 0
+#define USE_DLR_FORWARD
+#endif
+#endif
+#if 0
+#define USE_HSR
+#if 0
+#define USE_HSR_REDBOX
+#endif
+#endif
+
+/*
+ * This enables multiple network device mode for the switch, which contains at
+ * least two physical ports.  Some users like to take control of the ports for
+ * running Spanning Tree Protocol.  The driver will create an additional eth?
+ * device for each port depending on the mode.
+ *
+ * Some limitations are the network devices cannot have different MTU and
+ * multicast hash tables.
+ */
+#if defined(USE_HSR_REDBOX) || defined(USE_DLR_FORWARD)
+static int multi_dev = 1;
+#else
+static int multi_dev = -1;
+#endif
+
+static int stp = -1;
+
+/*
+ * This enables fast aging in the switch.  Not sure what situation requires
+ * that.  However, fast aging is used to flush the dynamic MAC table when STP
+ * support is enabled.
+ */
+static int fast_aging;
+
+static int sgmii = -1;
+
+static int authen;
+
+static int avb;
+#ifdef CONFIG_KSZ_MRP
+static int mrp;
+#endif
+
+#if defined(USE_DLR) || defined(USE_HSR)
+static int eth1_ports = 0x3;
+#else
+static int eth1_ports;
+#endif
+static int eth2_ports;
+static int eth3_ports;
+static int eth4_ports;
+static int eth5_ports;
+static int eth6_ports;
+
+#if defined(USE_HSR_REDBOX) || defined(USE_DLR_FORWARD)
+static int eth1_vlan = FID_ENTRIES - 2;
+#else
+static int eth1_vlan;
+#endif
+static int eth2_vlan;
+static int eth3_vlan;
+static int eth4_vlan;
+static int eth5_vlan;
+static int eth6_vlan;
+
+#if defined(USE_DLR)
+static char *eth1_proto = "dlr";
+#endif
+#if defined(USE_HSR)
+static char *eth1_proto = "hsr";
+#endif
+#if !defined(USE_DLR) && !defined(USE_HSR)
+static char *eth1_proto = " ";
+#endif
+#if defined(USE_HSR_REDBOX)
+static char *eth2_proto = "redbox";
+#else
+static char *eth2_proto = " ";
+#endif
+static char *eth3_proto = " ";
+static char *eth4_proto = " ";
+static char *eth5_proto = " ";
+static char *eth6_proto = " ";
+
+static int *eth_ports[] = {
+	&eth1_ports,
+	&eth2_ports,
+	&eth3_ports,
+	&eth4_ports,
+	&eth5_ports,
+	&eth6_ports,
+	NULL
+};
+
+static int *eth_vlans[] = {
+	&eth1_vlan,
+	&eth2_vlan,
+	&eth3_vlan,
+	&eth4_vlan,
+	&eth5_vlan,
+	&eth6_vlan,
+	NULL
+};
+
+static char **eth_proto[] = {
+	&eth1_proto,
+	&eth2_proto,
+	&eth3_proto,
+	&eth4_proto,
+	&eth5_proto,
+	&eth6_proto,
+	NULL
+};
+
+#ifdef CONFIG_KSZ_IBA
+static int iba = 1;
+#endif
+
+static uint sw_setup_zone(struct ksz_sw *sw, uint in_ports)
+{
+	int c;
+	int f;
+	int limit;
+	int m;
+	uint p;
+	uint q;
+	int w;
+	int *v;
+	char **s;
+	uint features;
+	struct ksz_dev_map *map;
+	uint ports;
+	uint used = 0;
+	int last_log_port = 0;
+	int last_phy_port = 0;
+	int last_vlan = 0;
+	uint left = (1 << sw->port_cnt) - 1;
+
+	if (multi_dev > 2) {
+		ports = in_ports;
+		goto setup_next;
+	}
+	q = 0;
+	ports = 0;
+	for (p = 0; p < sw->port_cnt - 1; p++) {
+		v = eth_ports[p];
+
+		/* No more port setting. */
+		if (!v || !*v)
+			break;
+		m = *v;
+		if (!(m & left)) {
+			left = 0;
+			break;
+		}
+
+		/* Find out how the ports are to be used. */
+		limit = 0;
+		w = last_vlan;
+		features = 0;
+		s = eth_proto[p];
+#ifdef CONFIG_KSZ_DLR
+		if (!strcmp(*s, "dlr")) {
+			features = DLR_HW;
+#ifdef CONFIG_1588_PTP
+			if (sw->features & PTP_HW) {
+				features |= VLAN_PORT;
+				sw->features |= VLAN_PORT | VLAN_PORT_TAGGING;
+			}
+#endif
+			limit = 2;
+			if (!w)
+				w = 1;
+		}
+#endif
+#ifdef CONFIG_KSZ_HSR
+		if (!strcmp(*s, "hsr")) {
+			features = HSR_HW;
+#ifdef CONFIG_1588_PTP
+			if (sw->features & PTP_HW) {
+				features |= VLAN_PORT;
+				sw->features |= VLAN_PORT | VLAN_PORT_TAGGING;
+			}
+#endif
+			limit = 2;
+			if (!w)
+				w = 1;
+		} else if (!strcmp(*s, "redbox")) {
+			features = HSR_REDBOX;
+			if (!w)
+				w = 1;
+			sw->overrides |= HSR_FORWARD;
+		} else if (!strcmp(*s, "redbox_")) {
+			features = HSR_REDBOX;
+			if (!w)
+				w = 1;
+		}
+#endif
+#ifdef CONFIG_KSZ_STP
+		if (!strcmp(*s, "stp")) {
+			features = STP_SUPPORT;
+		}
+#endif
+#ifdef CONFIG_1588_PTP
+		if (!features && !p) {
+			if (sw->features & PTP_HW) {
+				features |= VLAN_PORT;
+				sw->features |= VLAN_PORT | VLAN_PORT_TAGGING;
+			}
+		}
+#endif
+
+		m &= ~((1 << last_phy_port) - 1);
+		m &= left;
+
+		/* No more legitimate port. */
+		if (!m)
+			break;
+
+		v = eth_vlans[p];
+		if (!w && (!v || !*v))
+			break;
+		if (*v)
+			w = *v;
+
+		/* Check VLAN id is unused. */
+		for (q = 0; q < p; q++) {
+			if (w > 1 && w == sw->eth_maps[q].vlan)
+				w = last_vlan + 1;
+		}
+		c = 0;
+		f = -1;
+		for (q = p; q < sw->port_cnt - 1; q++) {
+			if (m & (1 << q)) {
+				if (f < 0)
+					f = last_log_port;
+				++c;
+				++last_log_port;
+
+				/* Limit to certain ports. */
+				if (limit && c >= limit) {
+					if (!(used & features)) {
+						used |= features;
+						++q;
+						last_phy_port = q;
+						break;
+					}
+					features = 0;
+				}
+				last_phy_port = q + 1;
+			} else if (f >= 0) {
+				if (limit && c < limit)
+					features = 0;
+				break;
+			}
+		}
+		if (!c)
+			continue;
+		m &= (1 << q) - 1;
+		if (!p && c > 1)
+			used |= (features & VLAN_PORT);
+#ifdef CONFIG_KSZ_STP
+		if ((features & STP_SUPPORT) && c > 1) {
+			used |= (features & STP_SUPPORT);
+			stp = m;
+		}
+#endif
+#ifdef CONFIG_KSZ_DLR
+		if ((features & DLR_HW) && !(used & DLR_HW))
+			features = 0;
+#endif
+#ifdef CONFIG_KSZ_HSR
+		if ((features & HSR_HW) && !(used & HSR_HW))
+			features = 0;
+		if (features == HSR_REDBOX)
+			used |= features;
+#endif
+		++f;
+		map = &sw->eth_maps[p];
+		map->cnt = c;
+		map->mask = m;
+		map->first = f;
+		map->phy_id = f;
+		map->vlan = w & (4096 - 1);
+		map->proto = features;
+		if (last_vlan < w)
+			last_vlan = w;
+		ports |= m;
+#ifdef CONFIG_KSZ_DLR
+		if (features & DLR_HW) {
+			struct ksz_dlr_info *dlr = &sw->info->dlr;
+
+			c = 0;
+			f = 0;
+			do {
+				if (m & 1) {
+					dlr->ports[c++] = f;
+				}
+				m >>= 1;
+				++f;
+			} while (m && c < map->cnt);
+		}
+#endif
+#ifdef CONFIG_KSZ_HSR
+		if (features & HSR_HW) {
+			struct ksz_hsr_info *hsr = &sw->info->hsr;
+
+			c = 0;
+			f = 0;
+			do {
+				if (m & 1) {
+					hsr->ports[c++] = f;
+				}
+				m >>= 1;
+				++f;
+			} while (m && c < map->cnt);
+		}
+#endif
+	}
+
+	/* No VLAN devices specified. */
+	if (!p) {
+		ports = in_ports;
+		goto setup_next;
+	}
+
+	/* Not all ports are used. */
+	if (last_phy_port < sw->mib_port_cnt)
+		left &= ~((1 << last_phy_port) - 1);
+	else
+		left = 0;
+	if (multi_dev != 1)
+		left = 0;
+	features = 0;
+	s = eth_proto[p];
+#ifdef CONFIG_KSZ_STP
+	if (s && !strcmp(*s, "stp"))
+		features = STP_SUPPORT;
+#endif
+#ifdef CONFIG_KSZ_HSR
+	if (left && (used & HSR_HW)) {
+
+		/* Redbox is explicitly specified. */
+		if (used & HSR_REDBOX)
+			left = 0;
+		if ((used & HSR_HW) && !(used & HSR_REDBOX)) {
+			s = eth_proto[1];
+			if (!strcmp(*s, "redbox")) {
+				features = HSR_REDBOX;
+				used |= HSR_REDBOX;
+				sw->overrides |= HSR_FORWARD;
+			} else if (!strcmp(*s, "redbox_")) {
+				features = HSR_REDBOX;
+				used |= HSR_REDBOX;
+			}
+		}
+		if (used & HSR_REDBOX)
+			sw->features |= HSR_REDBOX;
+		else if (used & HSR_HW)
+			left = 0;
+	}
+#endif
+	if (left) {
+		m = left;
+		c = 0;
+		f = -1;
+		for (q = 0; q < sw->mib_port_cnt; q++) {
+			if (m & (1 << q)) {
+				if (f < 0)
+					f = last_log_port;
+				++c;
+			}
+		}
+		m &= (1 << q) - 1;
+		if ((features & STP_SUPPORT) && c > 1) {
+			used |= (features & STP_SUPPORT);
+			stp = m;
+		}
+		++f;
+		map = &sw->eth_maps[p];
+		map->cnt = c;
+		map->mask = m;
+		map->first = f;
+		map->phy_id = f;
+		map->vlan = ++last_vlan & (4096 - 1);
+		map->proto = features;
+		ports |= m;
+		p++;
+	}
+	if (p > 1)
+		sw->features |= SW_VLAN_DEV;
+	else if (multi_dev == 1)
+		multi_dev = 0;
+	sw->eth_cnt = p;
+	for (p = 0; p < sw->eth_cnt; p++) {
+		map = &sw->eth_maps[p];
+		dbg_msg("%d: %d:%d:%d m=%04x v=%03x %08x"NL,
+			p, map->first, map->cnt, map->phy_id,
+			map->mask, map->vlan, map->proto);
+	}
+
+setup_next:
+#ifdef CONFIG_KSZ_DLR
+	if (!(used & DLR_HW))
+		sw->features &= ~DLR_HW;
+	else
+		sw->features |= DLR_HW;
+#endif
+#ifdef CONFIG_KSZ_HSR
+	if (!(used & HSR_HW))
+		sw->features &= ~HSR_HW;
+	else
+		sw->features |= HSR_HW;
+#endif
+	if ((sw->features & (DLR_HW | HSR_HW)) || sw->eth_cnt > 1) {
+		if (multi_dev < 0)
+			multi_dev = 0;
+		if (stp <= 1)
+			stp = 0;
+		avb = 0;
+		sw->overrides &= ~USE_802_1X_AUTH;
+	}
+#ifdef CONFIG_KSZ_STP
+	if (stp > 0) {
+		sw->features |= STP_SUPPORT;
+	}
+#endif
+#ifdef CONFIG_1588_PTP
+	if (!(used & VLAN_PORT))
+		sw->features &= ~(VLAN_PORT | VLAN_PORT_TAGGING);
+	if (!avb)
+		sw->features &= ~AVB_SUPPORT;
+#endif
+dbg_msg("features: %x m:%d a:%d s:%x"NL, sw->features, multi_dev, avb, stp);
+	return ports;
+}  /* sw_setup_zone */
+
+static int phy_offset;
+
+static void sw_setup_special(struct ksz_sw *sw, int *port_cnt,
+	int *mib_port_cnt, int *dev_cnt,
+	const void *phylink_ops)
+{
+#if defined(CONFIG_PHYLINK) || defined(CONFIG_PHYLINK_MODULE)
+	sw->phylink_ops = phylink_ops;
+#endif
+	*port_cnt = 1;
+	*mib_port_cnt = 1;
+	*dev_cnt = 1;
+
+	phy_offset = 0;
+	sw->dev_offset = 0;
+	sw->phy_offset = 0;
+	dbg_msg("%s s:%x m:%d f:%d"NL, __func__,
+		sw->stp, sw->multi_dev, sw->fast_aging);
+
+	/* Multiple network device interfaces are required. */
+	if (1 == sw->multi_dev) {
+		sw->dev_count = sw->mib_port_cnt;
+		sw->phy_offset = 1;
+	} else if (2 == sw->multi_dev)
+		sw->features |= VLAN_PORT | VLAN_PORT_TAGGING;
+	else if (3 == sw->multi_dev) {
+		sw->dev_count = sw->mib_port_cnt;
+		sw->dev_offset = 1;
+	} else if (4 == sw->multi_dev)
+		sw->features |= VLAN_PORT;
+	else if (5 == sw->multi_dev) {
+		sw->dev_count = sw->mib_port_cnt;
+		sw->dev_offset = 1;
+		sw->features |= VLAN_PORT | VLAN_PORT_TAGGING;
+	}
+
+	/* Single network device has multiple ports. */
+	if (1 == sw->dev_count) {
+		*port_cnt = sw->mib_port_cnt;
+		*mib_port_cnt = sw->mib_port_cnt;
+	}
+	if (1 == sw->multi_dev && sw->eth_cnt)
+		sw->dev_count = sw->eth_cnt;
+	*dev_cnt = sw->dev_count;
+	if (3 == sw->multi_dev || 5 == sw->multi_dev)
+		(*dev_cnt)++;
+#ifdef CONFIG_1588_PTP
+#ifdef CONFIG_PTP_1588_CLOCK
+/* THa  05/30/2018
+ * I2C driver is invoked before PPS and PTP, so the call to register the PTP
+ * clock needs to be called later.
+ */
+	if (sw->features & PTP_HW) {
+		struct ptp_info *ptp = &sw->ptp_hw;
+
+		micrel_ptp_probe(ptp);
+	}
+#endif
+	if (sw->features & VLAN_PORT) {
+		struct ptp_info *ptp = &sw->ptp_hw;
+
+		ptp->overrides |= PTP_PORT_FORWARD;
+	}
+#endif
+dbg_msg("%s d:%d c:%d"NL, __func__, *dev_cnt, sw->eth_cnt);
+}  /* sw_setup_special */
+
+static void sw_leave_dev(struct ksz_sw *sw)
+{
+	int dev_count = sw->dev_count + sw->dev_offset;
+	struct sw_priv *ks = sw->dev;
+	struct phy_priv *phydata;
+#if defined(CONFIG_PHYLINK) || defined(CONFIG_PHYLINK_MODULE)
+	struct ksz_port *port;
+#endif
+	int i;
+
+#ifdef CONFIG_KSZ_STP
+	if (sw->features & STP_SUPPORT)
+		leave_stp(&sw->info->rstp);
+#endif
+#ifdef CONFIG_KSZ_MRP
+	if (sw->features & MRP_SUPPORT)
+		leave_mrp(&sw->mrp);
+#endif
+	for (i = 0; i < dev_count; i++) {
+#if defined(CONFIG_PHYLINK) || defined(CONFIG_PHYLINK_MODULE)
+		port = sw->netport[i];
+		if (port && port->pl) {
+		       phylink_destroy(port->pl);
+		       port->pl = NULL;
+		}
+#endif
+		sw->netdev[i] = NULL;
+		sw->netport[i] = NULL;
+	}
+
+	/* Reset port pointer as it is pointed to one from device. */
+	for (i = 0; i <= sw->port_cnt; i++) {
+		phydata = &sw->phydata[i];
+		phydata->port = &ks->ports[i];
+	}
+	sw->eth_cnt = 0;
+	sw->dev_count = 1;
+	sw->dev_offset = 0;
+	sw->phy_offset = 0;
+}  /* sw_leave_dev */
+
+static int sw_setup_dev(struct ksz_sw *sw, struct net_device *dev,
+	char *dev_name, struct ksz_port *port, int i, uint port_cnt,
+	uint mib_port_cnt)
+{
+	struct ksz_port_info *info;
+	uint cnt;
+	uint n;
+	uint p;
+	uint pi;
+	int phy_id;
+	u32 features;
+	struct ksz_dev_map *map;
+	int header = 0;
+
+	if (!phy_offset)
+		phy_offset = sw->phy_offset;
+
+	/* dev_offset is ether 0 or 1. */
+	p = i;
+	if (p)
+		p -= sw->dev_offset;
+
+	if (sw->dev_offset) {
+		/*
+		 * First device associated with switch has been
+		 * created.
+		 */
+		if (i) {
+			snprintf(dev->name, IFNAMSIZ, "%s.10%%d", dev_name);
+			pi = get_phy_port(sw, p + 1);
+			memcpy(dev->dev_addr, sw->port_info[pi].mac_addr,
+				ETH_ALEN);
+		} else {
+			port_cnt = sw->mib_port_cnt;
+			mib_port_cnt = sw->mib_port_cnt;
+			sw->ops->acquire(sw);
+			sw_set_addr(sw, dev->dev_addr);
+			sw->ops->release(sw);
+		}
+	}
+
+	map = &sw->eth_maps[i];
+	if (1 == sw->multi_dev && sw->eth_cnt) {
+		port_cnt = map->cnt;
+		p = map->first - 1;
+		mib_port_cnt = port_cnt;
+	}
+
+	port->port_cnt = port_cnt;
+	port->mib_port_cnt = mib_port_cnt;
+	port->first_port = p + 1;
+	port->flow_ctrl = PHY_FLOW_CTRL;
+
+	/* S1 chips do not work well with asymmetric PAUSE. */
+	if ((sw->features & NEW_CAP) && (sw->features & GIGABIT_SUPPORT))
+		port->flow_ctrl = PHY_RX_ONLY;
+
+#ifdef CONFIG_KSZ_STP
+	if (!i && (sw->features & STP_SUPPORT))
+		prep_stp_mcast(dev);
+#endif
+
+#ifdef CONFIG_KSZ_DLR
+	/* Cannot flow control because of beacon timeout. */
+	if (sw->eth_cnt && (map->proto & DLR_HW)) {
+		if ((sw->features & NEW_CAP) &&
+		    (sw->features & GIGABIT_SUPPORT))
+			port->flow_ctrl = PHY_TX_ONLY;
+		prep_dlr_mcast(dev);
+	}
+#endif
+#ifdef CONFIG_KSZ_HSR
+	if (sw->eth_cnt && (map->proto & HSR_HW)) {
+#if 0
+		port->flow_ctrl = PHY_TX_ONLY;
+#endif
+		setup_hsr(&sw->info->hsr, dev, i);
+	}
+	if (sw->eth_cnt && (map->proto & HSR_REDBOX)) {
+		bool fwd = false;
+
+		if (sw->overrides & HSR_FORWARD)
+			fwd = true;
+		setup_hsr_redbox(&sw->info->hsr, dev, i, fwd);
+	}
+#endif
+#if defined(CONFIG_KSZ_AVB) || defined(CONFIG_KSZ_MRP)
+	if (!i && (sw->features & (AVB_SUPPORT | MRP_SUPPORT)))
+		setup_mrp(&sw->mrp, dev);
+#endif
+	if (sw->features & AVB_SUPPORT)
+		port->flow_ctrl = PHY_NO_FLOW_CTRL;
+
+	p = get_phy_port(sw, port->first_port);
+	port->sw = sw;
+	port->linked = get_port_info(sw, p);
+
+	/* Point to port under netdev. */
+	if (phy_offset)
+		phy_id = port->linked->phy_id;
+	else
+		phy_id = 0;
+
+	/* Replace virtual port with one from network device. */
+	do {
+		struct phy_device *phydev;
+		struct phy_priv *priv;
+		struct sw_priv *hw_priv = container_of(sw, struct sw_priv, sw);
+
+		phydev = mdiobus_get_phy(hw_priv->bus, phy_id);
+		priv = phydev->priv;
+		priv->port = port;
+		set_phy_support(port, phydev);
+	} while (0);
+
+	if (!phy_offset)
+		phy_offset = 1;
+
+	for (cnt = 0, n = port->first_port; cnt < port_cnt; cnt++, n++) {
+		pi = get_phy_port(sw, n);
+		info = get_port_info(sw, pi);
+		if (info->phy)
+			info->state = media_disconnected;
+		else if (info->intf == INTF_RGMII ||
+			 info->intf == INTF_SGMII)
+			port->live_ports |= (1 << pi);
+		if (pi == sw->HOST_PORT)
+			continue;
+		sw->info->port_cfg[pi].index = i;
+	}
+	sw->netdev[i] = dev;
+	sw->netport[i] = port;
+	port->netdev = dev;
+	port->phydev = sw->phy[phy_id];
+
+	/* Cannot point to host port that uses fixed-link. */
+	if (phy_id)
+		port->dn = sw->devnode[phy_id - 1];
+	if (dev->phydev && phy_is_internal(dev->phydev)) {
+		__ETHTOOL_DECLARE_LINK_MODE_MASK(supported);
+	
+		linkmode_set_bit(ETHTOOL_LINK_MODE_10baseT_Half_BIT,
+				 supported);
+		linkmode_set_bit(ETHTOOL_LINK_MODE_10baseT_Full_BIT,
+				 supported);
+		linkmode_set_bit(ETHTOOL_LINK_MODE_100baseT_Half_BIT,
+				 supported);
+		linkmode_set_bit(ETHTOOL_LINK_MODE_100baseT_Full_BIT,
+				 supported);
+		linkmode_set_bit(ETHTOOL_LINK_MODE_Pause_BIT,
+				 supported);
+		if (sw->features & GIGABIT_SUPPORT) {
+			linkmode_set_bit(ETHTOOL_LINK_MODE_1000baseT_Full_BIT,
+					 supported);
+			linkmode_set_bit(ETHTOOL_LINK_MODE_Asym_Pause_BIT,
+					 supported);
+		}
+		linkmode_copy(dev->phydev->supported, supported);
+		linkmode_copy(dev->phydev->advertising, supported);
+		dev->phydev->mdio.addr = phy_id;
+	}
+#if defined(CONFIG_PHYLINK) || defined(CONFIG_PHYLINK_MODULE)
+	setup_phylink(port);
+#endif
+	if (sw->dev_count > 1 && i && !(sw->features & DIFF_MAC_ADDR)) {
+		if (memcmp(dev->dev_addr, sw->netdev[0]->dev_addr, ETH_ALEN))
+			sw->features |= DIFF_MAC_ADDR;
+	}
+
+	INIT_DELAYED_WORK(&port->link_update, link_update_work);
+	features = sw->features;
+	if (sw->features & SW_VLAN_DEV)
+		features = map->proto;
+
+	if (features & VLAN_PORT)
+		dev->features |= NETIF_F_HW_VLAN_CTAG_FILTER;
+
+	/* Default headroom is 2.  Adding 1 to hard_header_len makes it 1.
+	 * Adding 2 makes it 16.  Then adding more will subtract from that
+	 * number.  So header len of 4 results in 14, and header len of 6
+	 * results in 12.  But somehow when sending raw PTP Sync, Follow_Up,
+	 * and Delay_Req messages (which have size of 58 bytes from 68 bytes
+	 * or more used by other messages) will result in network_header to
+	 * become 12 rather than 32 in newer kernels.
+	 * The best number to use is therefore 2 to have the largest headroom.
+	 */
+	if (sw->features & (SW_VLAN_DEV | HSR_HW))
+		header = 2;
+	if (dev->hard_header_len <= 14)
+		dev->hard_header_len += header;
+dbg_msg("%s %d:%d phy:%d l:%x"NL, __func__, port->first_port, port->port_cnt, phy_id,
+port->live_ports);
+
+	return phy_id;
+}  /* sw_setup_dev */
+
+static int netdev_chk_running(struct net_device *dev)
+{
+	return netif_running(dev);
+}
+
+static int netdev_chk_stopped(struct net_device *dev)
+{
+	return netif_running(dev) && netif_queue_stopped(dev);
+}
+
+static void netdev_start_queue(struct net_device *dev)
+{
+	netif_start_queue(dev);
+}
+
+static void netdev_stop_queue(struct net_device *dev)
+{
+	netif_stop_queue(dev);
+}
+
+static void netdev_wake_queue(struct net_device *dev)
+{
+	netif_wake_queue(dev);
+}
+
+static void sw_netdev_oper(struct ksz_sw *sw, struct net_device *dev,
+	int (*netdev_chk)(struct net_device *dev),
+	void (*netdev_oper)(struct net_device *dev))
+{
+	uint port;
+	int dev_count = 1;
+
+	dev_count = sw->dev_count + sw->dev_offset;
+	if (dev_count <= 1) {
+		netdev_oper(dev);
+		return;
+	}
+	for (port = 0; port < dev_count; port++) {
+		dev = sw->netdev[port];
+		if (!dev)
+			continue;
+		if (!netdev_chk || netdev_chk(dev))
+			netdev_oper(dev);
+	}
+}  /* sw_netdev_oper */
+
+static void sw_netdev_open_port(struct ksz_sw *sw, struct net_device *dev)
+{
+	struct ksz_port *port;
+	int p;
+	int dev_count = 1;
+
+	dev_count = sw->dev_count + sw->dev_offset;
+	if (dev_count <= 1) {
+		port = sw->netport[0];
+		sw->net_ops->open_port(sw, dev, port);
+		return;
+	}
+	for (p = 0; p < dev_count; p++) {
+		dev = sw->netdev[p];
+		if (!dev)
+			continue;
+		port = sw->netport[p];
+		sw->net_ops->open_port(sw, dev, port);
+	}
+}  /* sw_netdev_open_port */
+
+static void sw_netdev_start_queue(struct ksz_sw *sw, struct net_device *dev)
+{
+	sw_netdev_oper(sw, dev, netdev_chk_running, netdev_start_queue);
+}
+
+static void sw_netdev_stop_queue(struct ksz_sw *sw, struct net_device *dev)
+{
+	sw_netdev_oper(sw, dev, netdev_chk_running, netdev_stop_queue);
+}
+
+static void sw_netdev_wake_queue(struct ksz_sw *sw, struct net_device *dev)
+{
+	sw_netdev_oper(sw, dev, netdev_chk_stopped, netdev_wake_queue);
+}
+
+static struct ksz_sw_net_ops sw_net_ops = {
+	.setup_special		= sw_setup_special,
+	.setup_dev		= sw_setup_dev,
+	.leave_dev		= sw_leave_dev,
+
+	.start			= sw_start,
+	.stop			= sw_stop,
+	.open_dev		= sw_open_dev,
+	.open_port		= sw_open_port,
+	.close_port		= sw_close_port,
+	.open			= sw_open,
+	.close			= sw_close,
+
+	.netdev_start_queue	= sw_netdev_start_queue,
+	.netdev_stop_queue	= sw_netdev_stop_queue,
+	.netdev_wake_queue	= sw_netdev_wake_queue,
+	.netdev_open_port	= sw_netdev_open_port,
+
+	.set_mac_addr		= sw_set_mac_addr,
+
+	.get_mtu		= sw_get_mtu,
+	.get_tx_len		= sw_get_tx_len,
+	.add_tail_tag		= sw_add_tail_tag,
+	.get_tail_tag		= sw_get_tail_tag,
+	.add_vid		= sw_add_vid,
+	.kill_vid		= sw_kill_vid,
+	.check_tx		= sw_check_tx,
+	.rx_dev			= sw_rx_dev,
+	.match_pkt		= sw_match_pkt,
+	.parent_rx		= sw_parent_rx,
+	.port_vlan_rx		= sw_port_vlan_rx,
+	.final_skb		= sw_final_skb,
+	.drv_rx			= sw_drv_rx,
+	.set_multi		= sw_set_multi,
+
+};
+
+static struct ksz_sw_ops sw_ops = {
+	.init			= sw_init_dev,
+	.exit			= sw_exit_dev,
+	.dev_req		= sw_dev_req,
+
+	.get_phy_port		= get_phy_port,
+	.get_log_port		= get_log_port,
+	.get_phy_mask_from_log	= get_phy_mask_from_log,
+
+	.chk_regs		= sw_chk_regs,
+
+	.acquire		= sw_acquire,
+	.release		= sw_release,
+
+	.p_r8			= port_r8,
+	.p_w8			= port_w8,
+	.p_r16			= port_r16,
+	.p_w16			= port_w16,
+	.p_r32			= port_r32,
+	.p_w32			= port_w32,
+
+	.chk			= sw_chk,
+	.cfg			= sw_cfg,
+
+	.port_get_link_speed	= port_get_link_speed,
+	.port_set_link_speed	= port_set_link_speed,
+	.port_force_link_speed	= port_force_link_speed,
+
+	.port_r_cnt		= port_r_cnt,
+	.get_mib_counters	= get_sw_mib_counters,
+
+	.sysfs_read		= sysfs_sw_read,
+	.sysfs_read_hw		= sysfs_sw_read_hw,
+	.sysfs_write		= sysfs_sw_write,
+	.sysfs_port_read	= sysfs_port_read,
+	.sysfs_port_read_hw	= sysfs_port_read_hw,
+	.sysfs_port_write	= sysfs_port_write,
+	.sysfs_mac_read		= sysfs_mac_read,
+	.sysfs_mac_write	= sysfs_mac_write,
+	.sysfs_vlan_read	= sysfs_vlan_read,
+	.sysfs_vlan_write	= sysfs_vlan_write,
+
+#ifdef CONFIG_KSZ_STP
+	.sysfs_stp_read		= sysfs_stp_read,
+	.sysfs_stp_write	= sysfs_stp_write,
+	.sysfs_stp_port_read	= sysfs_stp_port_read,
+	.sysfs_stp_port_write	= sysfs_stp_port_write,
+#endif
+
+#if defined(CONFIG_KSZ_AVB) || defined(CONFIG_KSZ_MRP)
+	.sysfs_mrp_read		= sysfs_mrp_read,
+	.sysfs_mrp_write	= sysfs_mrp_write,
+	.sysfs_mrp_port_read	= sysfs_mrp_port_read,
+	.sysfs_mrp_port_write	= sysfs_mrp_port_write,
+#endif
+
+#ifdef CONFIG_KSZ_HSR
+	.sysfs_hsr_read		= sysfs_hsr_read,
+	.sysfs_hsr_write	= sysfs_hsr_write,
+#endif
+
+	.sysfs_acl_read		= sysfs_acl_read,
+	.sysfs_acl_write	= sysfs_acl_write,
+
+	.cfg_mac		= sw_cfg_mac,
+	.cfg_vlan		= sw_cfg_vlan,
+	.alloc_mac		= sw_alloc_mac,
+	.free_mac		= sw_free_mac,
+	.alloc_vlan		= sw_alloc_vlan,
+	.free_vlan		= sw_free_vlan,
+	.alloc_fid		= sw_alloc_fid,
+	.free_fid		= sw_free_fid,
+
+	.get_br_id		= sw_get_br_id,
+	.from_backup		= sw_from_backup,
+	.to_backup		= sw_to_backup,
+	.from_designated	= sw_from_designated,
+	.to_designated		= sw_to_designated,
+	.tc_detected		= sw_tc_detected,
+	.get_tcDetected		= sw_get_tcDetected,
+
+	.cfg_src_filter		= sw_cfg_src_filter,
+	.flush_table		= sw_flush_dyn_mac_table,
+	.fwd_unk_mcast		= sw_fwd_unk_mcast,
+	.fwd_unk_ucast		= sw_fwd_unk_ucast,
+	.fwd_unk_vid		= sw_fwd_unk_vid,
+
+	.port_freeze_mib	= port_freeze_mib,
+	.freeze_mib		= sw_freeze_mib,
+};
+
+static int sw_proc_intr(struct ksz_sw *sw)
+{
+	u32 intr_mask;
+	u32 status;
+	u16 port_intr_mask;
+	u8 lue_status;
+	u8 port_status;
+	uint port;
+#ifdef CONFIG_1588_PTP
+	struct ptp_info *ptp = &sw->ptp_hw;
+#endif
+	u32 intr_status[2];
+	int cnt = 0;
+	static int dbg_intr_status = 5;
+	static int dbg_intr_cnt = 0;
+
+	intr_mask = sw->intr_mask;
+	status = sw->reg->r32(sw, REG_SW_INT_STATUS__4);
+	if (status)
+		cnt++;
+	intr_status[0] = status;
+	status &= sw->intr_mask;
+#ifdef CONFIG_1588_PTP
+	if (status & TRIG_TS_INT) {
+		if (ptp->started)
+			ptp->ops->proc_intr(ptp);
+		else
+			sw->intr_mask &= ~TRIG_TS_INT;
+	}
+#endif
+	if (status & APB_TIMEOUT_INT) {
+dbg_msg(" apb: %08x"NL, sw->reg->r32(sw, REG_SW_APB_TIMEOUT_ADDR__4));
+		sw->reg->w32(sw, REG_SW_APB_TIMEOUT_ADDR__4,
+			APB_TIMEOUT_ACKNOWLEDGE);
+		status = sw->reg->r32(sw, REG_SW_INT_STATUS__4);
+		if (status & APB_TIMEOUT_INT)
+			sw->intr_mask &= ~APB_TIMEOUT_INT;
+	}
+	if (intr_mask != sw->intr_mask)
+		sw->reg->w32(sw, REG_SW_INT_MASK__4,
+			~sw->intr_mask & SWITCH_INT_MASK);
+
+	intr_mask = sw->lue_intr_mask;
+	if (sw->lue_intr_mask & LUE_INT_MASK) {
+		lue_status = sw->reg->r8(sw, REG_SW_LUE_INT_STATUS__1);
+		lue_status &= sw->lue_intr_mask;
+		if (lue_status & LEARN_FAIL_INT) {
+dbg_msg(" learn fail!"NL);
+		}
+		if (lue_status & WRITE_FAIL_INT) {
+dbg_msg(" write fail!"NL);
+		}
+		if (lue_status) {
+			sw->reg->w8(sw, REG_SW_LUE_INT_STATUS__1, lue_status);
+			lue_status = sw->reg->r8(sw, REG_SW_LUE_INT_STATUS__1);
+#if 0
+			if (lue_status & LEARN_FAIL_INT)
+#endif
+				sw->lue_intr_mask &= ~LEARN_FAIL_INT;
+			if (lue_status & WRITE_FAIL_INT)
+				sw->lue_intr_mask &= ~WRITE_FAIL_INT;
+		}
+	}
+	if (intr_mask != sw->lue_intr_mask)
+		sw->reg->w8(sw, REG_SW_LUE_INT_MASK__1,
+			    ~sw->lue_intr_mask & LUE_INT_MASK);
+
+	intr_mask = sw->port_intr_mask;
+	status = sw->reg->r32(sw, REG_SW_PORT_INT_STATUS__4);
+	if (status)
+		cnt++;
+	intr_status[1] = status;
+#if 0
+dbg_msg("port irq: %08x; %08x"NL, status, sw->port_intr_mask);
+#endif
+	status &= sw->port_intr_mask;
+	for (port = 0; port < sw->port_cnt; port++) {
+		if (status & 1) {
+			struct ksz_port_cfg *cfg = get_port_cfg(sw, port);
+			int stop_phy_irq = false;
+
+			if (!(sw->features & NEW_CAP))
+				stop_phy_irq = true;
+			port_intr_mask = cfg->intr_mask;
+			port_r(sw, port, REG_PORT_INT_STATUS, &port_status);
+			port_status &= cfg->intr_mask;
+			if (port_status & PORT_SGMII_INT) {
+				u16 data = 0;
+
+				port_sgmii_w(sw, port, SR_MII,
+					MMD_SR_MII_AUTO_NEG_STATUS, &data, 1);
+				sw->phy_intr |= (1 << port);
+			}
+			if (port_status & PORT_PHY_INT) {
+				u8 val;
+
+				/* The status is cleared after read. */
+				port_r8(sw, port, REG_PORT_PHY_INT_STATUS,
+					&val);
+				if (val & (LINK_DOWN_INT | LINK_UP_INT))
+					sw->phy_intr |= (1 << port);
+			}
+#ifdef CONFIG_1588_PTP
+			if (port_status & PORT_PTP_INT) {
+				if (ptp->started)
+					proc_ptp_tx_intr(ptp, port);
+				else
+					cfg->intr_mask &= ~PORT_PTP_INT;
+			}
+#endif
+			if (port_status & PORT_ACL_INT) {
+				if (sw->features & NEW_CAP)
+					port_w(sw, port, REG_PORT_INT_STATUS,
+						PORT_ACL_INT);
+				else {
+					port_w(sw, port, REG_PORT_INT_MASK,
+						(~cfg->intr_mask &
+						PORT_INT_MASK) |
+						PORT_ACL_INT);
+					port_w(sw, port, REG_PORT_INT_MASK,
+						~cfg->intr_mask &
+						PORT_INT_MASK);
+				}
+#ifdef CONFIG_KSZ_DLR
+				if (sw->features & DLR_HW)
+					dlr_timeout(&sw->info->dlr, port);
+#endif
+				if (sw->overrides & ACL_INTR_MONITOR)
+					printk(KERN_INFO "  acl: %d %lx"NL,
+						port, jiffies);
+			}
+			if (port_intr_mask != cfg->intr_mask)
+				port_w(sw, port, REG_PORT_INT_MASK,
+					~cfg->intr_mask & PORT_INT_MASK);
+		}
+		status >>= 1;
+	}
+
+	/* Do it once when interrupt happens. */
+	if (cnt && sw->phy_intr)
+		schedule_delayed_work(sw->link_read, 0);
+	if (intr_mask != sw->port_intr_mask)
+		sw->reg->w32(sw, REG_SW_PORT_INT_MASK__4,
+			~sw->port_intr_mask & sw->PORT_MASK);
+	if (sw->intr_cnt && cnt) {
+		dbg_intr_cnt = cnt;
+#if 0
+		dbg_msg("intr: %08x %08x"NL, intr_status[0], intr_status[1]);
+#endif
+	}
+	if (!sw->intr_cnt && cnt)
+		dbg_intr_cnt = 0;
+	sw->intr_cnt += cnt;
+	if (!sw->intr_cnt && !dbg_intr_cnt && dbg_intr_status) {
+		dbg_msg("no intr status"NL);
+		dbg_intr_status--;
+	}
+	return cnt;
+}  /* sw_proc_intr */
+
+/* -------------------------------------------------------------------------- */
+
+#ifndef CONFIG_KSZ_IBA_ONLY
+/* debugfs code */
+static int state_show(struct seq_file *seq, void *v)
+{
+	int i;
+	int j;
+	SW_D data[16 / SW_SIZE];
+	struct sw_priv *priv = seq->private;
+	struct ksz_sw *sw = &priv->sw;
+
+	for (i = 0; i < 0x100; i += 16) {
+		seq_printf(seq, SW_SIZE_STR":\t", i);
+		mutex_lock(&priv->lock);
+		for (j = 0; j < 16 / SW_SIZE; j++)
+			data[j] = sw->reg->r8(sw, i + j * SW_SIZE);
+		mutex_unlock(&priv->lock);
+		for (j = 0; j < 16 / SW_SIZE; j++)
+			seq_printf(seq, SW_SIZE_STR" ", data[j]);
+		seq_printf(seq, NL);
+	}
+	return 0;
+}
+
+static int state_open(struct inode *inode, struct file *file)
+{
+	return single_open(file, state_show, inode->i_private);
+}
+
+static const struct file_operations state_fops = {
+	.owner	= THIS_MODULE,
+	.open	= state_open,
+	.read	= seq_read,
+	.llseek	= seq_lseek,
+	.release = single_release,
+};
+
+/**
+ * create_debugfs - create debugfs directory and files
+ * @priv:	The switch device structure.
+ *
+ * Create the debugfs entries for the specific device.
+ */
+static void create_debugfs(struct sw_priv *priv)
+{
+	struct dentry *root;
+	char root_name[32];
+
+	snprintf(root_name, sizeof(root_name), "%s",
+		 dev_name(priv->dev));
+
+	root = debugfs_create_dir(root_name, NULL);
+	if (IS_ERR(root)) {
+		pr_err("cannot create debugfs root"NL);
+		return;
+	}
+
+	priv->debug_root = root;
+	priv->debug_file = debugfs_create_file("state", 0444, root,
+		priv, &state_fops);
+	if (IS_ERR(priv->debug_file))
+		pr_err("cannot create debugfs state file"NL);
+}
+
+static void delete_debugfs(struct sw_priv *priv)
+{
+	debugfs_remove(priv->debug_file);
+	debugfs_remove(priv->debug_root);
+}
+#endif
+
+/* -------------------------------------------------------------------------- */
+
+#define USE_SPEED_LINK
+#define USE_MIB
+#include "ksz_sw_sysfs_9897.c"
+#ifdef CONFIG_1588_PTP
+#include "ksz_ptp_sysfs.c"
+#endif
+#ifdef CONFIG_KSZ_DLR
+#include "ksz_dlr_sysfs.c"
+#endif
+
+static irqreturn_t sw_interrupt(int irq, void *phy_dat)
+{
+	struct sw_priv *ks = phy_dat;
+
+	ks->sw.intr_using += 1;
+	ks->irq_work.func(&ks->irq_work);
+
+	return IRQ_HANDLED;
+}  /* sw_interrupt */
+
+static void sw_change(struct work_struct *work)
+{
+	struct sw_priv *ks =
+		container_of(work, struct sw_priv, irq_work);
+	struct ksz_sw *sw = &ks->sw;
+	int intr;
+
+	/* Fake interrupt can be triggered once. */
+	if (ks->intr_working & 0x80000000) {
+		int p = 0;
+
+		/* First physical port can be disabled. */
+		if (sw->netport[0])
+			p = get_phy_port(sw, sw->netport[0]->first_port);
+		ks->intr_working |= 1;
+		if (ks->sw.info->port_cfg[p].intr_mask & PORT_PHY_INT)
+			ks->intr_working |= 2;
+	}
+	ks->intr_working |= 0x80000000;
+	mutex_lock(&sw->lock);
+	mutex_lock(&ks->hwlock);
+	mutex_lock(&ks->lock);
+	sw->intr_using++;
+	sw->intr_cnt = 0;
+	do {
+		intr = sw_proc_intr(sw);
+	} while (intr);
+	sw->intr_using--;
+	mutex_unlock(&ks->lock);
+	mutex_unlock(&ks->hwlock);
+	mutex_unlock(&sw->lock);
+	sw->intr_using = 0;
+}  /* sw_change */
+
+static int sw_start_interrupt(struct sw_priv *ks, const char *name)
+{
+	int err = 0;
+
+	INIT_WORK(&ks->irq_work, sw_change);
+
+	err = request_threaded_irq(ks->irq, NULL, sw_interrupt,
+		ks->intr_mode, name, ks);
+	if (err < 0) {
+		printk(KERN_WARNING "%s: Can't get IRQ %d (PHY)"NL,
+			name,
+			ks->irq);
+		ks->irq = 0;
+	}
+
+	return err;
+}  /* sw_start_interrupt */
+
+static void sw_stop_interrupt(struct sw_priv *ks)
+{
+	free_irq(ks->irq, ks);
+	cancel_work_sync(&ks->irq_work);
+}  /* sw_stop_interrupt */
+
+static void sw_init_phy_priv(struct sw_priv *ks)
+{
+	struct ksz_sw *sw = &ks->sw;
+	struct ksz_port_info *info;
+	struct phy_priv *phydata;
+	struct ksz_port *port;
+	uint n;
+	uint p;
+
+	for (n = 0; n <= sw->port_cnt; n++) {
+		phydata = &sw->phydata[n];
+		port = &ks->ports[n];
+		phydata->port = port;
+		port->sw = sw;
+		port->phydev = &sw->phy_map[n];
+		port->flow_ctrl = PHY_FLOW_CTRL;
+		port->port_cnt = 1;
+		port->mib_port_cnt = 1;
+		p = n;
+		if (!n) {
+			port->port_cnt = sw->mib_port_cnt;
+			port->mib_port_cnt = sw->mib_port_cnt;
+			p = 1;
+		}
+		port->first_port = p;
+		p = get_phy_port(sw, p);
+		info = get_port_info(sw, p);
+		port->linked = info;
+		port->phydev->interface = info->interface;
+#if 0
+dbg_msg(" %s %d=p:%d; f:%d c:%d i:%d"NL, __func__, n, p,
+port->first_port, port->port_cnt, port->linked->phy_id);
+#endif
+		INIT_DELAYED_WORK(&port->link_update, link_update_work);
+		sw->phy_map[n].priv = phydata;
+	}
+}  /* sw_init_phy_priv */
+
+static void sw_init_phydev(struct ksz_sw *sw, struct phy_device *phydev)
+{
+	struct ksz_port_info *info = get_port_info(sw, sw->HOST_PORT);
+
+	phydev->interface = sw->interface;
+	phydev->speed = info->tx_rate / TX_RATE_UNIT;
+	phydev->duplex = (info->duplex == 2);
+	phydev->pause = 1;
+	phydev->asym_pause = 1;
+}  /* sw_init_phydev */
+
+static int sw_device_present;
+
+static int ksz_platform_init(struct sw_priv *ks)
+{
+	struct platform_device *pdev;
+	struct ksz_sw *sw = &ks->sw;
+	struct device *dev;
+	char buf[80];
+	int i;
+
+	/* Check the switch id is not already used. */
+	i = sw->id;
+	snprintf(buf, sizeof(buf), "Switch MII bus.%u", i);
+	dev = bus_find_device_by_name(&platform_bus_type, NULL, buf);
+	while (dev) {
+		i++;
+		if (i >= MAX_SW_DEVICES)
+			return -ENODEV;
+		snprintf(buf, sizeof(buf), "Switch MII bus.%u", i);
+		dev = bus_find_device_by_name(&platform_bus_type, NULL, buf);
+	}
+	if (i != ks->sw.id) {
+		sw_device_present = i;
+		sw->id = i;
+		sw->change_id = true;
+	}
+
+	pdev = platform_device_register_simple("Switch MII bus", sw->id,
+		NULL, 0);
+	if (!pdev)
+		return -ENOMEM;
+	ks->pdev = pdev;
+	return 0;
+}  /* ksz_platform_init */
+
+static void ksz_platform_exit(struct sw_priv *ks)
+{
+	if (ks->pdev)
+		platform_device_unregister(ks->pdev);
+	ks->pdev = NULL;
+}  /* ksz_platform_exit */
+
+#define KSZ989X_SW_ID		0x9897
+#define KSZ889X_SW_ID		0x8897
+#define PHY_ID_KSZ989X_SW	((KSZ9897_ID_HI << 16) | KSZ989X_SW_ID)
+#define PHY_ID_KSZ889X_SW	((KSZ9897_ID_HI << 16) | KSZ889X_SW_ID)
+
+static char *kszsw_phy_driver_names[LAST_SW_CHIP] = {
+	"Microchip KSZ9897 Switch",
+	"Microchip KSZ9567 Switch",
+	"Microchip KSZ9477 Switch",
+	"Microchip KSZ9896 Switch",
+	"Microchip KSZ9566 Switch",
+	"Microchip KSZ8567 Switch",
+	"Microchip KSZ8565 Switch",
+	"Microchip KSZ9893 Switch",
+	"Microchip KSZ9563 Switch",
+	"Microchip KSZ8563 Switch",
+	"Microchip LAN9646 Switch",
+};
+
+static int kszphy_probe(struct phy_device *phydev)
+{
+	struct mii_bus *bus = phydev->mdio.bus;
+	struct sw_priv *sw_priv = bus->priv;
+	struct ksz_sw *sw = &sw_priv->sw;
+	struct ksz_port_info *info;
+	uint p;
+
+	p = phydev->mdio.addr;
+	phydev->priv = &sw->phydata[p];
+	if (p)
+		--p;
+	else
+		p = sw->HOST_PORT;
+	info = get_port_info(sw, p);
+	phydev->interface = info->interface;
+	return 0;
+}
+
+static int kszphy_get_features(struct phy_device *phydev)
+{
+	struct phy_priv *priv = phydev->priv;
+	struct ksz_port *port = priv->port;
+	struct ksz_sw *sw = port->sw;
+	struct ksz_port_info *info;
+	int ret;
+	uint p;
+
+	ret = genphy_read_abilities(phydev);
+	if (ret < 0)
+		return ret;
+
+	set_phy_support(port, phydev);
+
+	if (phydev->mdio.addr) {
+		p = phydev->mdio.addr - 1;
+	} else {
+		p = sw->HOST_PORT;
+	}
+	info = get_port_info(sw, p);
+
+	/* Do not support half-duplex in gigabit. */
+	if (sw->features & GIGABIT_SUPPORT)
+		linkmode_set_bit(ETHTOOL_LINK_MODE_1000baseT_Full_BIT,
+				 phydev->supported);
+	if (PHY_INTERFACE_MODE_SGMII == info->interface) {
+		linkmode_set_bit(ETHTOOL_LINK_MODE_1000baseT_Full_BIT,
+				 phydev->supported);
+		linkmode_set_bit(ETHTOOL_LINK_MODE_1000baseT_Half_BIT,
+				 phydev->supported);
+	}
+
+	/* Special for first PHY connected to MAC. */
+	if (phydev->mdio.addr == 0) {
+		int speed = info->tx_rate / TX_RATE_UNIT;
+
+		if (speed == 1000)
+			linkmode_set_bit(ETHTOOL_LINK_MODE_1000baseT_Full_BIT,
+					 phydev->supported);
+		linkmode_clear_bit(ETHTOOL_LINK_MODE_Autoneg_BIT,
+				   phydev->supported);
+	}
+	return 0;
+}
+
+static int kszphy_config_init(struct phy_device *phydev)
+{
+	return 0;
+}
+
+static struct phy_driver kszsw_phy_driver[] = {
+{
+	.phy_id		= PHY_ID_KSZ989X_SW,
+	.phy_id_mask	= 0x00ffffff,
+	.name		= "Microchip KSZ989X Switch",
+	.probe		= kszphy_probe,
+	.get_features	= kszphy_get_features,
+	.config_init	= kszphy_config_init,
+	.config_aneg	= genphy_config_aneg,
+	.read_status	= genphy_read_status,
+}
+};
+
+/**
+ * sw_r_phy - read data from PHY register
+ * @sw:		The switch instance.
+ * @phy:	PHY address to read.
+ * @reg:	PHY register to read.
+ * @val:	Buffer to store the read data.
+ *
+ * This routine reads data from the PHY register.
+ */
+static void sw_r_phy(struct ksz_sw *sw, u16 phy, u16 reg, u16 *val)
+{
+	u16 ret = 0;
+	uint p;
+
+	if (phy)
+		p = phy - 1;
+	else
+		p = sw->HOST_PORT;
+	if (p < sw->phy_port_cnt) {
+		u16 data;
+
+		port_r16(sw, p, P_PHY_CTRL + reg * 2, &data);
+		ret = data;
+		if (1 == reg && !(ret & PORT_LINK_STATUS)) {
+			port_r16(sw, p, P_SPEED_STATUS, &data);
+			if ((ret & PORT_AUTO_NEG_ACKNOWLEDGE) &&
+			    (data & (PORT_STAT_SPEED_1000MBIT |
+			    PORT_STAT_SPEED_100MBIT |
+			    PORT_STAT_SPEED_10MBIT)))
+				ret |= PORT_LINK_STATUS;
+		}
+	} else {
+		switch (reg) {
+		case MII_BMCR:
+			ret = 0x1140;
+			break;
+		case MII_BMSR:
+			ret = 0x796d;
+			break;
+		case MII_ADVERTISE:
+			ret = 0x05e1;
+			break;
+		case MII_LPA:
+			ret = 0xc5e1;
+			break;
+		case MII_CTRL1000:
+			ret = 0x0700;
+			break;
+		case MII_STAT1000:
+			if (sw->port_info[p].tx_rate >= 1000 * TX_RATE_UNIT)
+				ret = 0x7800;
+			else
+				ret = 0;
+			break;
+		case MII_ESTATUS:
+			ret = 0x2000;
+			break;
+		}
+	}
+
+	/* Use unique switch id to differentiate from regular PHY. */
+	if (MII_PHYSID2 == reg) {
+		ret = KSZ989X_SW_ID;
+	} else if (MII_PHYSID1 == reg) {
+		ret = KSZ9897_ID_LO;
+	}
+	*val = ret;
+}  /* sw_r_phy */
+
+static int ksz_mii_read(struct mii_bus *bus, int phy_id, int regnum)
+{
+	struct sw_priv *ks = bus->priv;
+	struct ksz_sw *sw = &ks->sw;
+	int ret = 0xffff;
+
+	if (phy_id > sw->port_cnt)
+		return 0xffff;
+	if (phy_id && get_log_port(sw, phy_id - 1) > sw->mib_port_cnt)
+		return 0xffff;
+
+	sw->ops->acquire(sw);
+	ret = 0;
+	if (regnum < 16) {
+		u16 data;
+
+		sw_r_phy(sw, phy_id, regnum, &data);
+		ret = data;
+	}
+	sw->ops->release(sw);
+	return ret;
+}  /* ksz_mii_read */
+
+static int ksz_mii_write(struct mii_bus *bus, int phy_id, int regnum, u16 val)
+{
+	struct sw_priv *ks = bus->priv;
+	struct ksz_sw *sw = &ks->sw;
+
+	if (phy_id > sw->port_cnt)
+		return -EINVAL;
+
+	/* Zero is used for the whole switch. */
+	if ((sw->multi_dev & 1) && phy_id == 0)
+		return 0;
+
+	sw->ops->acquire(sw);
+	if (regnum < 16) {
+		uint i;
+		uint p;
+		int first;
+		int last;
+
+		if (0 == phy_id) {
+			first = 1;
+			last = sw->mib_port_cnt;
+		} else {
+			bool found;
+			int n;
+			int f;
+			int l;
+			struct ksz_dev_map *map;
+
+			first = phy_id;
+			last = phy_id;
+			found = false;
+			for (n = 0; n < sw->eth_cnt; n++) {
+				map = &sw->eth_maps[n];
+				f = map->first;
+				l = f + map->cnt - 1;
+				for (i = f; i <= l; i++) {
+					p = get_phy_port(sw, i);
+					if (phy_id == p + 1) {
+						found = true;
+						break;
+					}
+				}
+				if (found) {
+					first = map->first;
+					last = first + map->cnt - 1;
+					break;
+				}
+			}
+dbg_msg(" %d f:%d l:%d"NL, phy_id, first, last);
+		}
+
+		/* PHY device driver resets or powers down the PHY. */
+		if (0 == regnum &&
+		    (val & (PORT_PHY_RESET | PORT_POWER_DOWN)))
+			goto done;
+		for (i = first; i <= last; i++) {
+			p = get_phy_port(sw, i);
+			if (p >= sw->phy_port_cnt)
+				break;
+			if (p == sw->HOST_PORT)
+				continue;
+			port_w16(sw, p, P_PHY_CTRL + regnum * 2, val);
+		}
+		if (0 == regnum &&
+		    !(val & PORT_AUTO_NEG_ENABLE))
+			schedule_delayed_work(&ks->link_read, 1);
+	}
+done:
+	sw->ops->release(sw);
+	return 0;
+}  /* ksz_mii_write */
+
+static int driver_installed;
+
+static int ksz_mii_init(struct sw_priv *ks)
+{
+	struct platform_device *pdev;
+	struct phy_device *phydev;
+	struct mii_bus *bus;
+	struct device *dev;
+	char buf[80];
+	int err;
+	int i;
+
+	/* Check the MDIO bus is not already created. */
+	i = ks->sw.id;
+	snprintf(buf, sizeof(buf), PHY_ID_FMT, "sw.%u", i);
+	dev = bus_find_device_by_name(&mdio_bus_type, NULL, buf);
+	if (dev)
+		return -ENODEV;
+	if (ks->sw.change_id)
+		driver_installed = true;
+
+	pdev = ks->pdev;
+
+	bus = mdiobus_alloc();
+	if (bus == NULL) {
+		err = -ENOMEM;
+		goto mii_init_reg;
+	}
+
+	if (!driver_installed) {
+		kszsw_phy_driver[0].name =
+			kszsw_phy_driver_names[ks->sw.chip_id];
+		err = phy_drivers_register(kszsw_phy_driver,
+					   ARRAY_SIZE(kszsw_phy_driver),
+					   THIS_MODULE);
+		if (err)
+			goto mii_init_free_mii_bus;
+		driver_installed = true;
+	}
+
+	bus->name = "Switch MII bus",
+	bus->read = ksz_mii_read;
+	bus->write = ksz_mii_write;
+	snprintf(bus->id, MII_BUS_ID_SIZE, "sw.%d", ks->sw.id);
+	bus->parent = &pdev->dev;
+	bus->phy_mask = ~((1 << (ks->sw.port_cnt + 1)) - 1);
+	bus->priv = ks;
+
+	for (i = 0; i < PHY_MAX_ADDR; i++)
+		bus->irq[i] = -1;
+
+	err = mdiobus_register(bus);
+	if (err < 0)
+		goto mii_init_free_mii_bus;
+
+	for (i = 0; i < PHY_MAX_ADDR; i++) {
+		phydev = mdiobus_get_phy(bus, i);
+		if (phydev) {
+			struct phy_priv *priv = phydev->priv;
+
+			priv->state = phydev->state;
+		}
+	}
+
+	ks->bus = bus;
+	phydev = mdiobus_get_phy(bus, 0);
+	ks->phydev = phydev;
+	sw_init_phydev(&ks->sw, phydev);
+
+	return 0;
+
+mii_init_free_mii_bus:
+	if (driver_installed && sw_device_present == 0) {
+		phy_drivers_unregister(kszsw_phy_driver,
+				       ARRAY_SIZE(kszsw_phy_driver));
+		driver_installed = false;
+	}
+	mdiobus_free(bus);
+
+mii_init_reg:
+
+	return err;
+}  /* ksz_mii_init */
+
+static void ksz_mii_exit(struct sw_priv *ks)
+{
+	int i;
+	struct phy_device *phydev;
+	struct mii_bus *bus = ks->bus;
+
+	for (i = 0; i < PHY_MAX_ADDR; i++) {
+		phydev = mdiobus_get_phy(bus, i);
+		if (phydev) {
+			struct ksz_port *port;
+
+			port = &ks->ports[i];
+			cancel_delayed_work_sync(&port->link_update);
+		}
+	}
+	mdiobus_unregister(bus);
+	if (driver_installed && sw_device_present == 1) {
+		phy_drivers_unregister(kszsw_phy_driver,
+				       ARRAY_SIZE(kszsw_phy_driver));
+		driver_installed = false;
+	}
+	mdiobus_free(bus);
+}  /* ksz_mii_exit */
+
+/* driver bus management functions */
+
+static void determine_rate(struct ksz_sw *sw, struct ksz_port_mib *mib)
+{
+	int j;
+
+	for (j = 0; j < 2; j++) {
+		if (mib->rate[j].last) {
+			unsigned long diff = jiffies - mib->rate[j].last;
+			u64 cnt = mib->counter[MIB_RX_TOTAL + j] -
+				mib->rate[j].last_cnt;
+
+			if (cnt > 1000000 && diff >= HZ) {
+				u64 rate = cnt;
+
+				rate *= 8;
+				diff *= 1000 * 100 / HZ;
+				rate = div_u64_u32(rate, diff);
+				mib->rate[j].last = jiffies;
+				mib->rate[j].last_cnt =
+					mib->counter[MIB_RX_TOTAL + j];
+				if (mib->rate[j].peak < (u32) rate)
+					mib->rate[j].peak = (u32) rate;
+			}
+			diff = jiffies - mib->rate[j].check;
+			cnt = mib->counter[MIB_RX_TOTAL + j] -
+				mib->rate[j].prev_cnt;
+			mib->rate[j].prev_cnt =
+				mib->counter[MIB_RX_TOTAL + j];
+			if (cnt) {
+				mib->rate[j].check = jiffies;
+				mib->rate[j].no_change = false;
+			} else if (diff >= HZ / 2) {
+				mib->rate[j].no_change = true;
+			}
+		} else {
+			mib->rate[j].last = jiffies;
+			mib->rate[j].check = jiffies;
+			mib->rate[j].no_change = false;
+		}
+	}
+}  /* determine_rate */
+
+static void ksz9897_mib_read_work(struct work_struct *work)
+{
+	struct sw_priv *hw_priv =
+		container_of(work, struct sw_priv, mib_read);
+	struct ksz_sw *sw = &hw_priv->sw;
+	struct ksz_port_mib *mib;
+	unsigned long interval;
+	uint n;
+	uint p;
+	int cnt = 0;
+
+#ifdef CONFIG_KSZ_IBA
+	if (iba_stopped(sw))
+		return;
+#endif
+
+	/* Find out how many ports are connected. */
+	for (n = 1; n <= sw->mib_port_cnt; n++) {
+		p = get_phy_port(sw, n);
+		if (media_connected == sw->port_state[p].state)
+			++cnt;
+	}
+	cnt++;
+	interval = MIB_READ_INTERVAL * 2 / cnt;
+	if (time_before(sw->next_jiffies, jiffies)) {
+		sw->next_jiffies = jiffies;
+		sw->next_jiffies += interval;
+	}
+
+	/* Restart auto-negotiation for those ports used in port_setup_eee. */
+	do {
+		struct ksz_port_cfg *cfg;
+		u32 period = hw_priv->mib_timer_info.period * 1000 / HZ;
+
+		for (n = 0; n <= sw->mib_port_cnt; n++) {
+			p = get_phy_port(sw, n);
+			cfg = get_port_cfg(sw, p);
+			if (cfg->setup_time) {
+				if (cfg->setup_time > period)
+					cfg->setup_time -= period;
+				else {
+					sw->ops->acquire(sw);
+					port_w16(sw, p, REG_PORT_PHY_CTRL,
+						0x1140);
+					sw->ops->release(sw);
+					cfg->setup_time = 0;
+				}
+			}
+		}
+	} while (0);
+	for (n = 0; n <= sw->mib_port_cnt; n++) {
+		p = get_phy_port(sw, n);
+		mib = get_port_mib(sw, p);
+
+		if (mib->cnt_ptr || 1 == hw_priv->counter[p].read) {
+		} else if (time_after_eq(jiffies, hw_priv->counter[p].time)) {
+			hw_priv->counter[p].time = sw->next_jiffies;
+			/* Only read MIB counters when the port is connected. */
+			if (media_connected == sw->port_state[p].state) {
+				hw_priv->counter[p].read = 1;
+				sw->next_jiffies += interval;
+			}
+
+		/* Port is just disconnected. */
+		} else if (sw->port_state[p].link_down) {
+			sw->port_state[p].link_down = 0;
+			mib->rate[0].no_change = false;
+
+			/* Read counters one last time after link is lost. */
+			hw_priv->counter[p].read = 1;
+		}
+
+		/* Reading MIB counters or requested to read. */
+		if (mib->cnt_ptr || 1 == hw_priv->counter[p].read) {
+
+			/* Need to process interrupt. */
+			if (port_r_cnt(sw, p))
+				return;
+			hw_priv->counter[p].read = 0;
+
+			/* Finish reading counters. */
+			if (0 == mib->cnt_ptr) {
+				struct ksz_port_info *info;
+
+				hw_priv->counter[p].read = 2;
+				wake_up_interruptible(
+					&hw_priv->counter[p].counter);
+				determine_rate(sw, mib);
+				info = get_port_info(sw, p);
+
+				/* No interrupt when cable is removed. */
+				if (info->fiber &&
+				    info->state == media_connected &&
+				    mib->rate[0].no_change &&
+				    mib->rate[1].no_change) {
+					sw->phy_intr |= (1 << p);
+					schedule_delayed_work(&hw_priv->link_read, 0);
+				}
+			}
+		}
+	}
+#ifdef CONFIG_KSZ_MRP
+	if (sw->features & MRP_SUPPORT)
+		mrp_chk_blocked_addr(&sw->mrp);
+#endif
+}  /* ksz9897_mib_read_work */
+
+static void copy_port_status(struct ksz_port *src, struct ksz_port *dst)
+{
+	dst->duplex = src->duplex;
+	dst->speed = src->speed;
+	dst->force_link = src->force_link;
+	dst->linked = src->linked;
+}
+
+static void link_read_work(struct work_struct *work)
+{
+	struct delayed_work *dwork = to_delayed_work(work);
+	struct sw_priv *hw_priv =
+		container_of(dwork, struct sw_priv, link_read);
+	struct ksz_sw *sw = &hw_priv->sw;
+	struct ksz_port *sw_port = NULL;
+	struct ksz_port *port = NULL;
+	int dev_cnt = sw->dev_count + sw->dev_offset;
+	int s = 1;
+	int i;
+
+#ifdef CONFIG_KSZ_IBA
+	if (iba_stopped(sw))
+		return;
+#endif
+
+	if (1 == sw->dev_count || 1 == sw->dev_offset)
+		s = 0;
+	/* Check main device when child devices are used. */
+	if (sw->dev_offset) {
+		sw_port = sw->netport[0];
+		if (sw_port)
+			sw_port->live_ports = 0;
+	}
+
+	/*
+	 * Only check port which has interrupts triggered.
+	 * If no interrupt poll all the ports with PHY.
+	 */
+	if (!sw->phy_intr) {
+		sw->phy_intr = (1 << sw->phy_port_cnt) - 1;
+		for (i = sw->phy_port_cnt; i <= sw->mib_port_cnt; i++)
+			if (sw->port_info[i].phy)
+				sw->phy_intr |= (1 << i);
+	}
+	sw->ops->acquire(sw);
+	for (i = sw->dev_offset; i < dev_cnt; i++) {
+		struct phy_priv *phydata;
+		int n = i + s;
+
+		port = sw->netport[i];
+
+		/* Port can be non-contiguous when using multiple devices. */
+		if (dev_cnt > 1 && port && port->linked)
+			n = port->linked->phy_id;
+		phydata = &sw->phydata[n];
+		if (!port)
+			port = phydata->port;
+		port_get_link_speed(port);
+		if (sw_port)
+			sw_port->live_ports |= port->live_ports;
+
+		/* Copy all port information for user access. */
+		if (port != phydata->port)
+			copy_port_status(port, phydata->port);
+	}
+	if (sw->phy_intr && sw->port_info[sw->HOST_PORT].phy) {
+		port = sw->phydata[sw->mib_port_cnt + 1].port;
+		port_get_link_speed(port);
+	}
+	sw->phy_intr = 0;
+	sw->ops->release(sw);
+
+	/* Need to invoke link_update_work before sw_port->linked is updated
+	 * as link_update_work can be called before link_read_work is
+	 * finished if the delay is not long enough.
+	 */
+	for (i = sw->dev_offset; i < dev_cnt; i++) {
+		struct phy_priv *phydata;
+
+		port = sw->netport[i];
+		if (!port) {
+			phydata = &sw->phydata[i];
+			port = phydata->port;
+		}
+		if (port->link_ports)
+			schedule_delayed_work(&port->link_update, 0);
+	}
+	port = sw->phydata[sw->mib_port_cnt + 1].port;
+	if (port->link_ports)
+		schedule_delayed_work(&port->link_update, 0);
+}  /* link_read_work */
+
+/*
+ * Hardware monitoring
+ */
+
+static void ksz9897_mib_monitor(struct timer_list *t)
+{
+	struct ksz_timer_info *info = from_timer(info, t, timer);
+	struct sw_priv *hw_priv = info->dev;
+
+	schedule_work(&hw_priv->mib_read);
+
+	ksz_update_timer(&hw_priv->mib_timer_info);
+}  /* ksz9897_mib_monitor */
+
+static void ksz9897_dev_monitor(struct timer_list *t)
+{
+	struct ksz_timer_info *info = from_timer(info, t, timer);
+	struct sw_priv *hw_priv = info->dev;
+	struct phy_device *phydev;
+	struct phy_priv *priv;
+	int i;
+
+	for (i = 0; i <= TOTAL_PORT_NUM; i++) {
+		phydev = mdiobus_get_phy(hw_priv->bus, i);
+		if (!phydev)
+			continue;
+		priv = phydev->priv;
+		if (priv->state != phydev->state) {
+			priv->state = phydev->state;
+		}
+	}
+	if (!(hw_priv->intr_working & 2))
+		schedule_delayed_work(&hw_priv->link_read, 0);
+
+	ksz_update_timer(&hw_priv->monitor_timer_info);
+}  /* ksz9897_dev_monitor */
+
+static int intr_mode;
+static int sw_host_port;
+static int sysfs_sw;
+static int ports;
+
+
+static int ksz_probe_prep(struct sw_priv *ks, void *dev)
+{
+	struct ksz_sw *sw;
+
+#if defined(CONFIG_SPI_PEGASUS) || defined(CONFIG_SPI_PEGASUS_MODULE)
+	intr_mode = 1;
+#endif
+
+	if (sw_device_present >= MAX_SW_DEVICES)
+		return -ENODEV;
+
+	if (1 == intr_mode)
+		ks->intr_mode = IRQF_TRIGGER_LOW | IRQF_ONESHOT;
+	else if (2 == intr_mode)
+		ks->intr_mode = IRQF_TRIGGER_FALLING;
+	ks->intr_mode |= IRQF_ONESHOT;
+
+	dev_set_drvdata(ks->dev, ks);
+
+	mutex_init(&ks->hwlock);
+	mutex_init(&ks->lock);
+
+	sw = &ks->sw;
+	mutex_init(&sw->lock);
+	mutex_init(&sw->acllock);
+	mutex_init(&sw->alulock);
+	mutex_init(&sw->vlanlock);
+	mutex_init(&sw->hsrlock);
+	sw->hwlock = &ks->hwlock;
+	sw->reglock = &ks->lock;
+	sw->dev = ks;
+
+	sw->info = kzalloc(sizeof(struct ksz_sw_info), GFP_KERNEL);
+	if (!sw->info)
+		return -ENOMEM;
+
+	/* Save original register access. */
+	sw->old = sw->reg;
+	sw->net_ops = &sw_net_ops;
+	sw->ops = &sw_ops;
+	init_waitqueue_head(&sw->queue);
+
+#ifdef CONFIG_KSZ_IBA_ONLY
+	if (dev) {
+		sw->netdev[0] = dev;
+		sw->features |= IBA_SUPPORT;
+		sw->info->iba.use_iba = IBA_USE_CODE_PREPARE;
+	}
+#endif
+
+#ifdef CONFIG_KSZ_IBA
+	ksz_iba_init(&sw->info->iba, sw);
+	INIT_DELAYED_WORK(&sw->set_ops, sw_set_ops);
+#endif
+
+	return 0;
+}  /* ksz_probe_prep */
+
+struct ksz_port_mapping {
+	u8 id;
+	u8 cnt;
+	u8 p7_intf;
+	u8 map[8];
+};
+
+enum {
+	KSZ9897_SKU,
+	KSZ9477_SKU,
+	KSZ9896_SKU,
+	KSZ9893_SKU,
+	KSZ8565_SKU,
+	LAN9646_SKU,
+
+	KSZ9477_5_SKU,
+	KSZ9477_3_SKU,
+	KSZ9897_T_SKU,
+	KSZ9897_U_SKU,
+};
+
+static struct ksz_port_mapping port_mappings[] = {
+	{ KSZ9897_SKU,   7, INTF_RGMII, { 1, 2, 3, 4, 5, 6, 7, 0 }},
+	{ KSZ9477_SKU,   7, INTF_SGMII, { 1, 2, 3, 4, 5, 6, 7, 0 }},
+	{ KSZ9896_SKU,   6, INTF_RGMII, { 1, 2, 3, 4, 5, 6, 0, 0 }},
+	{ KSZ9893_SKU,   3, INTF_RGMII, { 1, 2, 3, 0, 0, 0, 0, 0 }},
+	{ KSZ8565_SKU,   5, INTF_RGMII, { 1, 2, 3, 4, 7, 0, 0, 0 }},
+	{ LAN9646_SKU,   6, INTF_SGMII, { 1, 2, 3, 4, 6, 7, 0, 0 }},
+
+	{ KSZ9477_5_SKU, 5, INTF_SGMII, { 1, 2, 3, 4, 6, 0, 0, 0 }},
+	{ KSZ9477_3_SKU, 3, INTF_SGMII, { 4, 7, 6, 0, 0, 0, 0, 0 }},
+	{ KSZ9897_T_SKU, 6, INTF_RGMII, { 4, 3, 2, 1, 5, 6, 0, 0 }},
+	{ KSZ9897_U_SKU, 6, INTF_RGMII, { 6, 4, 3, 2, 1, 5, 0, 0 }},
+};
+
+static u8 port_map[8];
+
+static int ksz_setup_port_mappings(struct ksz_sw *sw, u8 id)
+{
+	struct ksz_port_mapping *map;
+	struct ksz_port_info *info;
+	uint i;
+	uint n;
+	int cnt = 0;
+
+	for (i = 0; i < ARRAY_SIZE(port_mappings); i++) {
+		map = &port_mappings[i];
+		if (id == map->id) {
+			cnt = map->cnt;
+			memset(port_map, 0, 8 * sizeof(u8));
+			memcpy(port_map, map->map, cnt * sizeof(u8));
+			for (n = 0; n < sw->phy_port_cnt; n++) {
+				info = &sw->port_info[n];
+				info->intf = INTF_TX_PHY;
+			}
+			for (; n < cnt; n++) {
+				info = &sw->port_info[n];
+				info->intf = INTF_RGMII;
+				if (n > 5)
+					info->intf = map->p7_intf;
+			}
+			break;
+		}
+	}
+	return cnt;
+}  /* ksz_setup_port_mappings */
+
+static void ksz_update_port_mappings(struct ksz_sw *sw, u8 id, int *host_port)
+{
+	struct ksz_port_mapping *map;
+	uint i;
+	uint n;
+	uint p;
+	int cnt = 0;
+	bool not_end = false;
+
+	for (i = 0; i < ARRAY_SIZE(port_mappings); i++) {
+		map = &port_mappings[i];
+		if (id == map->id) {
+			cnt = map->cnt;
+			break;
+		}
+	}
+	for (n = 0; n < cnt - 1; n++) {
+		p = map->map[n];
+		if (!p)
+			break;
+		if (p == *host_port)
+			not_end = true;
+		if (not_end)
+			map->map[n] = map->map[n + 1];
+	}
+	if (*host_port)
+		map->map[n] = *host_port;
+	else
+		*host_port = map->map[n];
+}  /* ksz_update_port_mappings */
+
+static void ksz_setup_logical_ports(struct ksz_sw *sw, u8 id, uint ports)
+{
+	struct ksz_port_mapping *map;
+	struct ksz_port_info *info;
+	struct ksz_port_info *pinfo;
+	uint i;
+	uint l;
+	uint n;
+	uint p;
+
+	for (i = 0; i < ARRAY_SIZE(port_mappings); i++) {
+		map = &port_mappings[i];
+		if (id == map->id) {
+			break;
+		}
+	}
+	n = (1 << sw->port_cnt) - 1;
+	ports &= n;
+	for (i = 0, n = 0; n <= sw->port_cnt; n++) {
+		if (n > 0) {
+			if (!(ports & BIT(n - 1)))
+				continue;
+			p = n - 1;
+			p = map->map[p] - 1;
+			if (p == sw->HOST_PORT)
+				continue;
+			l = i;
+			++i;
+		} else {
+			p = sw->HOST_PORT;
+			l = 0;
+		}
+		info = &sw->port_info[i];
+		info->phy_p = p;
+		info->phy_m = BIT(p);
+		info = &sw->port_info[p];
+		info->log_p = i;
+		info->log_m = BIT(l);
+		info->phy_id = p + 1;
+	}
+	info = &sw->port_info[sw->HOST_PORT];
+	info->log_m = BIT(i);
+
+	ports = 0;
+	for (n = 0; n <= i; n++) {
+		info = &sw->port_info[n];
+		ports |= info->phy_m;
+	}
+dbg_msg("ports: %x"NL, ports);
+
+	for (n = 0; n <= i; n++) {
+		info = &sw->port_info[n];
+		pinfo = &sw->port_info[info->phy_p];
+dbg_msg("%d= %d:%02x %d:%02x %d; %d:%02x %d"NL, n,
+info->phy_p, info->phy_m, info->log_p, info->log_m, info->phy_id,
+pinfo->log_p, pinfo->log_m, pinfo->intf);
+	}
+#if 1
+	if (i + 1 < sw->port_cnt) {
+		for (n = 0; n < sw->port_cnt; n++) {
+			info = &sw->port_info[n];
+			if (!info->log_m)
+dbg_msg(" %d= %d %d"NL, n, info->log_p, info->intf);
+			if (!info->log_m)
+				info->log_p = sw->port_cnt;
+		}
+	}
+#endif
+	sw->PORT_MASK = ports;
+	sw->PORT_INTR_MASK = ports;
+	sw->mib_port_cnt = i;
+
+	if (sw->mib_port_cnt + 1 < sw->port_cnt)
+		sw->features |= USE_FEWER_PORTS;
+	for (i = 0; i < sw->eth_cnt; i++) {
+dbg_msg(" eth_maps: %d=%x"NL, i, sw->eth_maps[i].mask);
+		sw->eth_maps[i].mask = 0;
+		for (l = 0, n = sw->eth_maps[i].first;
+		     l < sw->eth_maps[i].cnt; l++, n++) {
+			p = get_phy_port(sw, n);
+			sw->eth_maps[i].mask |= BIT(p);
+		}
+dbg_msg("   eth_maps: %d=%x"NL, i, sw->eth_maps[i].mask);
+	}
+#ifdef CONFIG_KSZ_DLR
+	if (sw->features & DLR_HW) {
+		struct ksz_dlr_info *dlr = &sw->info->dlr;
+
+		dlr->ports[0] = port_map[dlr->ports[0]] - 1;
+		dlr->ports[1] = port_map[dlr->ports[1]] - 1;
+		dlr->member = BIT(dlr->ports[0]) | BIT(dlr->ports[1]);
+dbg_msg(" dlr member: %d:%d %x"NL, dlr->ports[0], dlr->ports[1], dlr->member);
+	}
+#endif
+#ifdef CONFIG_KSZ_HSR
+	if (sw->features & HSR_HW) {
+		struct ksz_hsr_info *hsr = &sw->info->hsr;
+
+dbg_msg("port_map %d %d %d %d %d %d %d:%d"NL,
+port_map[0], port_map[1], port_map[2], port_map[3], port_map[4], port_map[5],
+hsr->ports[0], hsr->ports[1]);
+		hsr->ports[0] = port_map[hsr->ports[0]] - 1;
+		hsr->ports[1] = port_map[hsr->ports[1]] - 1;
+		hsr->member = BIT(hsr->ports[0]) | BIT(hsr->ports[1]);
+dbg_msg(" hsr member: %d:%d %x"NL, hsr->ports[0], hsr->ports[1], hsr->member);
+	}
+#endif
+}  /* ksz_setup_logical_ports */
+
+static int ksz_probe_next(struct sw_priv *ks)
+{
+	struct ksz_sw *sw = &ks->sw;
+	struct ksz_port_info *info;
+	u32 id;
+	u32 id1;
+	u32 id2;
+	u8 sku;
+	int i;
+	uint p;
+	uint mib_port_count;
+	uint phy_port_count;
+	uint pi;
+	uint port_count;
+	int reset = true;
+	int ret = -ENODEV;
+
+#ifdef CONFIG_KSZ_IBA
+	if (sw->info->iba.use_iba)
+		reset = false;
+#endif
+
+	sw->ops->acquire(sw);
+
+#ifndef CONFIG_KSZ_IBA_ONLY
+	if (ks->spi_mode) {
+		/* Turn off SPI auto edge detection. */
+		sw->reg->w8(sw, REG_SW_GLOBAL_SERIAL_CTRL_0, 0x40);
+
+#ifdef CONFIG_SPI_ATMEL
+		/* Switch may not be accessible the very first time when SPI
+		 * mode is not 0 in newer kernels where Atmel SPI was changed
+		 * to use standard SPI transfer function.
+		 */
+		if (ks->spi_mode & 2)
+			sw->reg->w8(sw, REG_SW_GLOBAL_SERIAL_CTRL_0, 0x40);
+#endif
+	}
+#endif
+
+	/* simple check for a valid chip being connected to the bus */
+	id = sw->reg->r32(sw, REG_CHIP_ID0__1);
+	sw->ops->release(sw);
+	id1 = id;
+	id1 >>= 8;
+	id1 &= 0xffff;
+	id2 = id1 & 0xff;
+	id1 >>= 8;
+dbg_msg("%02x %02x"NL, id1, id2);
+	if (id1 != FAMILY_ID_95 && id1 != FAMILY_ID_98 &&
+	    id1 != FAMILY_ID_94 && id1 != FAMILY_ID_85 && id1 != 0x64) {
+		dev_err(ks->dev, "failed to read device ID(0x%x)"NL, id);
+		ret = -ENODEV;
+		goto err_platform;
+	}
+	dev_info(ks->dev, "chip id 0x%08x"NL, id);
+
+	phy_port_count = 1;
+
+#ifdef CONFIG_1588_PTP
+	if ((FAMILY_ID_95 & 0x0f) == (id1 & 0x0f)) {
+		sw->features |= PTP_HW;
+	}
+	if (0x64 == id1) {
+		sw->features |= PTP_HW;
+#if 1
+		sw->features |= SETUP_PHY;
+#endif
+	}
+#endif
+
+	sku = KSZ9897_SKU;
+	port_count = 7;
+	mib_port_count = 7;
+	if ((CHIP_ID_67 & 0x0f) == (id2 & 0x0f)) {
+		phy_port_count = 5;
+		sw->TAIL_TAG_SHIFT = 7;
+		sw->TAIL_TAG_LOOKUP = (1 << (7 + 3));
+		sw->TAIL_TAG_OVERRIDE = (1 << (7 + 2));
+		sku = KSZ9897_SKU;
+	} else if ((CHIP_ID_66 & 0x0f) == (id2 & 0x0f)) {
+		port_count = 6;
+		mib_port_count = 6;
+		phy_port_count = 5;
+		sw->TAIL_TAG_SHIFT = 7;
+		sw->TAIL_TAG_LOOKUP = (1 << (7 + 3));
+		sw->TAIL_TAG_OVERRIDE = (1 << (7 + 2));
+		sku = KSZ9896_SKU;
+	} else if ((CHIP_ID_63 & 0x0f) == (id2 & 0x0f)) {
+		port_count = 3;
+		mib_port_count = 3;
+		phy_port_count = 2;
+		sw->TAIL_TAG_SHIFT = 3;
+		sw->TAIL_TAG_LOOKUP = (1 << (3 + 3));
+		sw->TAIL_TAG_OVERRIDE = (1 << (3 + 2));
+		sw->features |= IS_9893;
+		sku = KSZ9893_SKU;
+	}
+
+	/* Check for S2 revision. */
+	sw->ops->acquire(sw);
+	id = sw->reg->r8(sw, REG_GLOBAL_OPTIONS);
+	sw->ops->release(sw);
+	if (id) {
+		u16 val;
+
+		sw->revision = 1;
+
+		sw->ops->acquire(sw);
+		port_r16(sw, 0, REG_PORT_INT_STATUS & ~1, &val);
+		sw->ops->release(sw);
+		if (!(val & PORT_PHY_INT)) {
+			sw->revision = 2;
+		}
+
+		sw->features &= ~SETUP_PHY;
+		sw->features |= NEW_CAP;
+
+		/* Only new KSZ9897 changes XMII definitions. */
+		if ((CHIP_ID_63 & 0x0f) != (id2 & 0x0f)) {
+			sw->features |= NEW_XMII;
+			if (id & SW_REDUNDANCY_ABLE) {
+				sw->features |= REDUNDANCY_SUPPORT;
+			}
+		} else {
+			if (!(id & SW_QW_ABLE))
+				sw->features |= GIGABIT_SUPPORT;
+		}
+		if (id & SW_AVB_ABLE) {
+			sw->features |= AVB_SUPPORT;
+#ifdef CONFIG_1588_PTP
+			sw->features |= PTP_HW;
+#endif
+		}
+
+		switch (id & 0x0f) {
+		case SW_9477_SL_5_2:
+			/* Last port is SGMII. */
+			if (!sw_host_port)
+				sw_host_port = 6;
+			sku = KSZ9477_SKU;
+			break;
+		}
+		if (id & SW_GIGABIT_ABLE)
+			sw->features |= GIGABIT_SUPPORT;
+
+		if (sw->features & IS_9893)
+dbg_msg("avb=%d  qw=%d  giga=%d"NL,
+!!(id & SW_AVB_ABLE), !!(id & SW_QW_ABLE), !!(id & SW_GIGABIT_ABLE));
+		else
+dbg_msg("avb=%d  rr=%d  giga=%d"NL,
+!!(id & SW_AVB_ABLE), !!(id & SW_REDUNDANCY_ABLE), !!(id & SW_GIGABIT_ABLE));
+	} else if ((FAMILY_ID_95 & 0x0f) == (id1 & 0x0f))
+		sw->features |= AVB_SUPPORT;
+	if ((sw->features & REDUNDANCY_SUPPORT) && port_count > 3)
+		sw->overrides |= HAVE_MORE_THAN_2_PORTS;
+	if (sw->features & IS_9893) {
+		sw->chip_id = KSZ9893_SW_CHIP;
+		if (sw->features & AVB_SUPPORT) {
+			if (sw->features & GIGABIT_SUPPORT)
+				sw->chip_id = KSZ9563_SW_CHIP;
+			else
+				sw->chip_id = KSZ8563_SW_CHIP;
+		}
+	} else {
+		sw->chip_id = KSZ9897_SW_CHIP;
+		if (port_count == 6)
+			sw->chip_id = KSZ9896_SW_CHIP;
+		if (sw->features & REDUNDANCY_SUPPORT)
+			sw->chip_id = KSZ9477_SW_CHIP;
+		else if (sw->features & AVB_SUPPORT) {
+			if (sw->features & GIGABIT_SUPPORT) {
+				sw->chip_id = KSZ9567_SW_CHIP;
+				if (port_count == 6)
+					sw->chip_id = KSZ9566_SW_CHIP;
+			} else {
+				sw->chip_id = KSZ8567_SW_CHIP;
+			}
+		}
+	}
+
+#ifdef CONFIG_OF
+	if (ks->of_dev && ks->of_dev->of_node) {
+		int score;
+		char name[80];
+
+		if (!of_modalias_node(ks->of_dev->of_node, name, sizeof(name)))
+			dbg_msg(" compatible: %s"NL, name);
+		do {
+			score = of_device_is_compatible(ks->of_dev->of_node,
+							"microchip,ksz8565");
+			if (score > 0 && sw->chip_id == KSZ8567_SW_CHIP) {
+				sku = KSZ8565_SKU;
+				sw->chip_id = KSZ8565_SW_CHIP;
+				break;
+			}
+			score = of_device_is_compatible(ks->of_dev->of_node,
+							"microchip,lan9646");
+			if (score > 0 && sw->chip_id == KSZ9477_SW_CHIP) {
+				sku = LAN9646_SKU;
+				sw->chip_id = LAN9646_SW_CHIP;
+				sw->features &=
+					~(PTP_HW | AVB_SUPPORT |
+					  REDUNDANCY_SUPPORT);
+				break;
+			}
+		} while (0);
+	}
+#endif
+
+	/* This specifies the maximum physical port count of the switch. */
+	sw->port_cnt = port_count;
+
+	/* PORT_MASK may change if fewer ports are used. */
+	sw->PORT_INTR_MASK = (1 << port_count) - 1;
+	sw->PORT_MASK = (1 << port_count) - 1;
+
+	/* Find out which port is host port. */
+	/* Ignore user specified host port if not correct. */
+	if (sw_host_port < 0 || sw_host_port > port_count)
+		sw_host_port = 0;
+
+	/* Find out how many ports are available. */
+	port_count = ksz_setup_port_mappings(sw, sku);
+	if (!port_count) {
+		dev_err(ks->dev, "device not known"NL);
+		ret = -ENODEV;
+		goto err_platform;
+	}
+
+	sw->id = sw_device_present;
+
+	ksz_update_port_mappings(sw, sku, &sw_host_port);
+	sw->HOST_PORT = sw_host_port - 1;
+
+	/* No specific ports are specified. */
+	if (!ports)
+		ports = (1 << port_count) - 1;
+dbg_msg("init ports: %x"NL, ports);
+
+	sw->dev_count = 1;
+
+	sw->mib_cnt = TOTAL_SWITCH_COUNTER_NUM;
+	sw->mib_port_cnt = port_count - 1;
+	sw->phy_port_cnt = phy_port_count;
+	sw->HOST_MASK = (1 << sw->HOST_PORT);
+
+#ifdef DEBUG
+	sw->verbose = 1;
+#endif
+	if (authen)
+		sw->overrides |= USE_802_1X_AUTH;
+	if (!(sw->features & AVB_SUPPORT) || !(sw->features & PTP_HW)) {
+		avb = 0;
+	} else if (avb < 0) {
+		avb = 0;
+		if (!(sw->features & IS_9893))
+			avb = 1;
+	}
+
+	ports = sw_setup_zone(sw, ports);
+
+	if (multi_dev < 0 && (avb || sw->features & (PTP_HW) ||
+	    sw->overrides & (USE_802_1X_AUTH))) {
+		multi_dev = 3;
+		if (!(sw->overrides & (USE_802_1X_AUTH)) && stp < 0)
+			stp = 1;
+	}
+	if (avb == 1 && stp < 0)
+		stp = 1;
+	if (avb == 2 && stp > 0)
+		stp = 0;
+	if (multi_dev < 0)
+		multi_dev = 0;
+	if (stp < 0)
+		stp = 0;
+
+	ksz_setup_logical_ports(sw, sku, ports);
+
+	sw->PORT_MASK |= sw->HOST_MASK;
+dbg_msg("mask: %x %x; %x %x"NL, sw->HOST_MASK, sw->PORT_MASK,
+sw->TAIL_TAG_LOOKUP, sw->TAIL_TAG_OVERRIDE);
+dbg_msg("port: %x %x %x"NL, sw->port_cnt, sw->mib_port_cnt, sw->phy_port_cnt);
+
+	INIT_DELAYED_WORK(&ks->link_read, link_read_work);
+
+	/* No SGMII setting. */
+	if (sgmii < 0) {
+
+		/* Direct connect. */
+		if (sw->HOST_PORT == 6)
+			sgmii = 0;
+		else
+			sgmii = 1;
+	}
+	sw->sgmii_mode = sgmii;
+	sw->interface = PHY_INTERFACE_MODE_MII;
+	if (setup_device_node(sw)) {
+		ret = -ENODEV;
+		goto err_platform;
+	}
+
+#ifdef DEBUG_MSG
+	flush_work(&db.dbg_print);
+#endif
+
+	for (pi = 0; pi < phy_port_count; pi++) {
+		/*
+		 * Initialize to invalid value so that link detection
+		 * is done.
+		 */
+		info = get_port_info(sw, pi);
+		info->tx_rate = 10 * TX_RATE_UNIT;
+		info->link = 0xFF;
+		info->state = media_disconnected;
+		info->get_link_speed = phy_port_get_speed;
+		info->set_link_speed = phy_port_set_speed;
+		info->force_link_speed = phy_port_force_speed;
+		info->phy = true;
+		info->interface = PHY_INTERFACE_MODE_RGMII;
+	}
+	sw->ops->acquire(sw);
+	for (; pi < mib_port_count; pi++) {
+		u16 data;
+		u16 orig;
+		u8 *data_lo;
+		u8 *data_hi;
+		int speed;
+		phy_interface_t phy;
+		int gbit;
+		int mode;
+
+		if (sw->TAIL_TAG_SHIFT != 7)
+			mode = 2;
+		else
+			mode = 5;
+		info = get_port_info(sw, pi);
+		if (pi < mode) {
+#ifdef CONFIG_KSZ_IBA_ONLY
+			if (pi == sw->HOST_PORT) {
+				info->state = media_connected;
+				sw->live_ports |= (1 << pi);
+				info->tx_rate = 1000 * TX_RATE_UNIT;
+				info->duplex = 2;
+			}
+#endif
+			break;
+		}
+		port_r16(sw, pi, REG_PORT_XMII_CTRL_0, &data);
+		orig = data;
+		data_hi = (u8 *) &data;
+		data_lo = data_hi + 1;
+
+		if (!(sw->features & NEW_CAP))
+			*data_hi &= ~(PORT_RGMII_ID_IG_ENABLE |
+				PORT_RGMII_ID_EG_ENABLE);
+#ifdef USE_10_MBIT_MODE
+		*data_lo &= ~PORT_MII_100MBIT;
+#endif
+#ifdef USE_HALF_DUPLEX
+		*data_lo &= ~PORT_MII_FULL_DUPLEX;
+#endif
+#ifdef USE_RGMII_MODE
+		sw_set_gbit(sw, true, data_hi);
+		sw_set_xmii(sw, 3, data_hi);
+#endif
+#ifdef USE_MII_MODE
+		sw_set_gbit(sw, false, data_hi);
+		sw_set_xmii(sw, 0, data_hi);
+#endif
+#ifdef USE_GMII_MODE
+		sw_set_gbit(sw, true, data_hi);
+		sw_set_xmii(sw, 2, data_hi);
+#endif
+#ifdef USE_GMII_100_MODE
+		sw_set_gbit(sw, false, data_hi);
+#endif
+#ifdef USE_RMII_MODE
+		sw_set_gbit(sw, false, data_hi);
+		sw_set_xmii(sw, 1, data_hi);
+#endif
+/* Strap options may not valid after reset. */
+#if 1
+if (PORT_RMII_SEL == (*data_hi & PORT_MII_SEL_M)) {
+dbg_msg("?%02x"NL, *data_hi);
+		sw_set_gbit(sw, true, data_hi);
+		sw_set_xmii(sw, 3, data_hi);
+}
+#endif
+		gbit = sw_get_gbit(sw, *data_hi);
+		mode = sw_get_xmii(sw, *data_hi);
+		switch (mode) {
+		case 2:
+			phy = PHY_INTERFACE_MODE_GMII;
+			speed = 1000;
+			if (gbit)
+				break;
+			fallthrough;
+
+		case 0:
+			phy = PHY_INTERFACE_MODE_MII;
+			speed = 100;
+			break;
+		case 1:
+			phy = PHY_INTERFACE_MODE_RMII;
+			speed = 100;
+			break;
+		default:
+			phy = PHY_INTERFACE_MODE_RGMII;
+			if (*data_hi & PORT_RGMII_ID_IG_ENABLE)
+				phy = PHY_INTERFACE_MODE_RGMII_RXID;
+			if (*data_hi & PORT_RGMII_ID_EG_ENABLE) {
+				if (PHY_INTERFACE_MODE_RGMII_RXID == phy)
+					phy = PHY_INTERFACE_MODE_RGMII_ID;
+				else
+					phy = PHY_INTERFACE_MODE_RGMII_TXID;
+			}
+			speed = 100;
+			if (gbit)
+				speed = 1000;
+			break;
+		}
+		if (*data_lo & PORT_SGMII_SEL) {
+			phy = PHY_INTERFACE_MODE_SGMII;
+			info->link = 0xFF;
+			info->state = media_disconnected;
+			info->partner = 0x2800cde1;
+			info->get_link_speed = sgmii_port_get_speed;
+			info->set_link_speed = sgmii_port_set_speed;
+			info->phy = port_sgmii_detect(sw, pi);
+		}
+		info->interface = phy;
+
+		/* Switch interface is not set through device tree. */
+		if (sw->HOST_PORT == pi &&
+		    sw->interface == PHY_INTERFACE_MODE_MII)
+			sw->interface = phy;
+		if (sw->HOST_PORT == pi)
+dbg_msg("host: %d %d"NL, sw->HOST_PORT, sw->interface);
+		if (info->phy)
+			info->state = media_disconnected;
+		else if (info->intf == INTF_RGMII ||
+			 info->intf == INTF_SGMII) {
+			info->state = media_connected;
+			info->partner = 0x2800cde1;
+			sw->live_ports |= (1 << pi);
+		}
+		if (!(*data_lo & PORT_MII_100MBIT))
+			info->tx_rate = 10 * TX_RATE_UNIT;
+		else
+			info->tx_rate = speed * TX_RATE_UNIT;
+		if (*data_lo & PORT_MII_FULL_DUPLEX)
+			info->duplex = 2;
+		else
+			info->duplex = 1;
+		info->flow_ctrl = 0x33;
+		sw->cached.xmii[pi - sw->phy_port_cnt] = (*data_lo << 8) |
+			*data_hi;
+dbg_msg("xmii: %04x %02x %02x; %u %u"NL, orig, *data_lo, *data_hi,
+info->tx_rate / TX_RATE_UNIT, info->duplex);
+	}
+	sw->ops->release(sw);
+
+	ret = ksz_platform_init(ks);
+	if (ret)
+		goto err_platform;
+
+	sw_init_phy_priv(ks);
+
+	ret = ksz_mii_init(ks);
+	if (ret)
+		goto err_mii;
+
+	if (ks->bus) {
+		for (i = 0; i <= sw->port_cnt; i++)
+			sw->phy[i] = mdiobus_get_phy(ks->bus, i);
+	} else {
+		for (i = 0; i <= sw->port_cnt; i++)
+			sw->phy[i] = &sw->phy_map[i];
+	}
+
+	sw->multi_dev |= multi_dev;
+	sw->stp |= stp;
+	sw->fast_aging |= fast_aging;
+#ifdef CONFIG_KSZ_STP
+	if (sw->stp) {
+		sw->features |= STP_SUPPORT;
+
+		/* Delay link notification to avoid having receive drops in
+		 * the host port because the outgoing port is not sending.
+		 */
+		sw->overrides |= DELAY_UPDATE_LINK;
+	}
+#endif
+	if (sw->fast_aging)
+		sw->overrides |= FAST_AGING;
+
+#ifdef CONFIG_KSZ_IBA
+	if (iba)
+		sw->features |= IBA_SUPPORT;
+#endif
+
+#ifdef CONFIG_KSZ_MRP
+	if (sw->features & AVB_SUPPORT) {
+		if (avb == 1)
+			mrp = 2;
+		else if (avb == 2)
+			mrp = 0;
+	} else if (mrp > 1)
+		mrp = 1;
+	if (mrp > 0) {
+#ifdef CONFIG_KSZ_MSRP
+		struct mrp_info *mrp_info = &sw->mrp;
+
+		if (mrp == 2)
+			mrp_info->status.msrpEnabledStatus = true;
+#endif
+		sw->features |= MRP_SUPPORT;
+	}
+#endif
+
+	sw->counter = ks->counter;
+	sw->mib_timer_info = &ks->mib_timer_info;
+	sw->monitor_timer_info = &ks->monitor_timer_info;
+	sw->link_read = &ks->link_read;
+
+	sw_setup_mib(sw);
+	sw_init_mib(sw);
+
+	for (i = 0; i < TOTAL_PORT_NUM; i++)
+		init_waitqueue_head(&ks->counter[i].counter);
+
+#ifndef CONFIG_KSZ_IBA_ONLY
+	create_debugfs(ks);
+#endif
+
+#ifdef CONFIG_KSZ_STP
+	ksz_stp_init(&sw->info->rstp, sw);
+#endif
+#ifdef CONFIG_KSZ_DLR
+	if (sw->features & DLR_HW)
+		ksz_dlr_init(&sw->info->dlr, sw);
+#endif
+#ifdef CONFIG_KSZ_HSR
+	if (sw->features & HSR_HW)
+		ksz_hsr_init(&sw->info->hsr, sw);
+#endif
+
+#ifdef DEBUG_MSG
+	flush_work(&db.dbg_print);
+#endif
+
+	sw->ops->acquire(sw);
+
+	/* Turn off PTP in case the feature is not enabled. */
+	if (reset)
+		sw->reg->w16(sw, REG_PTP_MSG_CONF1, 0);
+
+	if (reset)
+		sw_reset_hw(sw);
+	sw_init(sw);
+	sw_setup(sw);
+	sw_enable(sw);
+	sw->ops->release(sw);
+	sw->ops->init(sw);
+
+	if (sysfs_sw)
+		sw->overrides |= SYSFS_1_BASE;
+
+#if !defined(CONFIG_KSZ9897_EMBEDDED)
+	init_sw_sysfs(sw, &ks->sysfs, ks->dev);
+#ifdef CONFIG_KSZ_DLR
+	if (sw->features & DLR_HW)
+		init_dlr_sysfs(ks->dev);
+#endif
+#endif
+#ifdef KSZSW_REGS_SIZE
+	ret = sysfs_create_bin_file(&ks->dev->kobj,
+		&kszsw_registers_attr);
+	if (ret < 0)
+		goto err_drv;
+#endif
+	sema_init(&ks->proc_sem, 1);
+
+	INIT_WORK(&sw->set_addr, sw_delayed_set_addr);
+	INIT_WORK(&sw->tx_fwd, sw_tx_fwd);
+	skb_queue_head_init(&sw->txq);
+
+	INIT_WORK(&ks->mib_read, ksz9897_mib_read_work);
+
+	/* 500 ms timeout */
+	ksz_init_timer(&ks->mib_timer_info, 500 * HZ / 1000,
+		ksz9897_mib_monitor, ks);
+	ksz_init_timer(&ks->monitor_timer_info, 100 * HZ / 1000,
+		ksz9897_dev_monitor, ks);
+
+	ksz_start_timer(&ks->mib_timer_info, ks->mib_timer_info.period);
+	if (!(sw->multi_dev & 1) && !sw->stp)
+		ksz_start_timer(&ks->monitor_timer_info,
+			ks->monitor_timer_info.period * 10);
+
+	sw_device_present++;
+
+#ifdef CONFIG_1588_PTP
+	sw->ptp_hw.parent = sw;
+	if (sw->features & PTP_HW) {
+		struct ptp_info *ptp = &sw->ptp_hw;
+
+		ptp->ports = sw->mib_port_cnt;
+#if 1
+		if (1 == sw->multi_dev && (sw->features & SW_VLAN_DEV))
+			ptp->ports = sw->eth_maps[0].cnt + 1;
+#endif
+		ptp->reg = &ptp_reg_ops;
+		ptp->ops = &ptp_ops;
+		ptp->dev_parent = ks->dev;
+		ptp->ops->init(ptp, sw->info->mac_addr);
+#if !defined(CONFIG_KSZ9897_EMBEDDED)
+		init_ptp_sysfs(&ks->ptp_sysfs, ks->dev);
+#endif
+	}
+#endif
+#if defined(CONFIG_KSZ_AVB) || defined(CONFIG_KSZ_MRP)
+	sw->mrp.parent = sw;
+	if (sw->features & (AVB_SUPPORT | MRP_SUPPORT)) {
+		struct mrp_info *mrp = &sw->mrp;
+
+#ifdef CONFIG_KSZ_MRP
+		INIT_DELAYED_WORK(&sw->set_mrp, sw_set_mrp);
+#endif
+		mrp->ops = &mrp_ops;
+		mrp->ops->init(mrp);
+	}
+#endif
+
+#ifdef DEBUG_MSG
+	flush_work(&db.dbg_print);
+#endif
+
+#ifdef CONFIG_KSZ_IBA_ONLY
+	if (IBA_USE_CODE_PREPARE == sw->info->iba.use_iba)
+		sw->info->iba.ready = true;
+#endif
+
+	if (ks->irq <= 0)
+		return 0;
+
+	/* Default is enable interrupts. */
+	sw->ops->acquire(sw);
+	sw_dis_intr(sw);
+	sw->reg->w32(sw, REG_PTP_INT_STATUS__4, 0xffffffff);
+	sw->reg->w8(sw, REG_SW_LUE_INT_MASK__1, LUE_INT_MASK);
+	for (p = 0; p < sw->port_cnt; p++) {
+		port_w(sw, p, REG_PORT_INT_MASK, 0xff);
+		port_w16(sw, p, REG_PTP_PORT_TX_INT_STATUS__2, 0xffff);
+
+		/* PHY interrupt may trigger before driver is ready. */
+		if (p < sw->phy_port_cnt) {
+			u8 val;
+
+			port_r8(sw, p, REG_PORT_PHY_INT_STATUS, &val);
+		}
+	}
+	sw->ops->release(sw);
+	ret = sw_start_interrupt(ks, dev_name(ks->dev));
+	if (ret < 0) {
+		printk(KERN_WARNING "No switch interrupt"NL);
+	} else {
+		sw->ops->acquire(sw);
+		sw_ena_intr(sw);
+		sw->ops->release(sw);
+	}
+
+	return 0;
+
+err_drv:
+	ksz_mii_exit(ks);
+
+err_mii:
+	ksz_platform_exit(ks);
+
+err_platform:
+
+#ifdef CONFIG_KSZ_IBA
+	ksz_iba_exit(&sw->info->iba);
+#endif
+	kfree(sw->info);
+
+#ifndef CONFIG_KSZ_IBA_ONLY
+	kfree(ks->hw_dev);
+#endif
+	kfree(ks);
+
+	return ret;
+}  /* ksz_probe_next */
+
+#ifndef CONFIG_KSZ_IBA_ONLY
+static int ksz_probe(struct sw_priv *ks)
+{
+	int ret = -ENODEV;
+
+	ret = ksz_probe_prep(ks, NULL);
+	if (ret)
+		return ret;
+
+	return ksz_probe_next(ks);
+}
+#endif
+
+static int ksz_remove(struct sw_priv *ks)
+{
+	struct ksz_sw *sw = &ks->sw;
+
+#ifdef CONFIG_KSZ_MRP
+	if (sw->features & MRP_SUPPORT) {
+		struct mrp_info *mrp = &sw->mrp;
+
+		cancel_delayed_work_sync(&sw->set_mrp);
+		mrp->ops->exit(mrp);
+	}
+#endif
+#ifdef CONFIG_1588_PTP
+	if (sw->features & PTP_HW) {
+		struct ptp_info *ptp = &sw->ptp_hw;
+
+#if !defined(CONFIG_KSZ9897_EMBEDDED)
+		exit_ptp_sysfs(&ks->ptp_sysfs, ks->dev);
+#endif
+		ptp->ops->exit(ptp);
+	}
+#endif
+
+	if (ks->irq > 0) {
+#ifndef CONFIG_KSZ_IBA_ONLY
+		sw->ops->acquire(sw);
+		sw->reg->w32(sw, REG_SW_INT_MASK__4, 0xffffffff);
+		sw->reg->w32(sw, REG_SW_PORT_INT_MASK__4, 0xffffffff);
+		sw->ops->release(sw);
+#endif
+		sw_stop_interrupt(ks);
+	}
+
+	ksz_stop_timer(&ks->monitor_timer_info);
+	ksz_stop_timer(&ks->mib_timer_info);
+	flush_work(&ks->mib_read);
+	cancel_delayed_work_sync(&ks->link_read);
+	ksz_mii_exit(ks);
+	ksz_platform_exit(ks);
+
+#ifdef KSZSW_REGS_SIZE
+	sysfs_remove_bin_file(&ks->dev->kobj, &kszsw_registers_attr);
+#endif
+
+#if !defined(CONFIG_KSZ9897_EMBEDDED)
+#ifdef CONFIG_KSZ_DLR
+	if (sw->features & DLR_HW)
+		exit_dlr_sysfs(ks->dev);
+#endif
+	exit_sw_sysfs(sw, &ks->sysfs, ks->dev);
+#endif
+	sw->ops->exit(sw);
+
+#ifdef CONFIG_KSZ_STP
+	ksz_stp_exit(&sw->info->rstp);
+#endif
+#ifdef CONFIG_KSZ_DLR
+	if (sw->features & DLR_HW)
+		ksz_dlr_exit(&sw->info->dlr);
+#endif
+#ifdef CONFIG_KSZ_HSR
+	if (sw->features & HSR_HW)
+		ksz_hsr_exit(&sw->info->hsr);
+#endif
+#ifdef CONFIG_KSZ_IBA
+	ksz_iba_exit(&sw->info->iba);
+#endif
+#ifndef CONFIG_KSZ_IBA_ONLY
+	delete_debugfs(ks);
+#endif
+
+	kfree(sw->info);
+#ifndef CONFIG_KSZ_IBA_ONLY
+	kfree(ks->hw_dev);
+#endif
+	kfree(ks);
+
+	if (sw_device_present)
+		sw_device_present--;
+	return 0;
+}  /* ksz_remove */
+
+module_param(fast_aging, int, 0);
+module_param(multi_dev, int, 0);
+module_param(stp, int, 0);
+module_param(avb, int, 0);
+module_param(authen, int, 0);
+module_param(sgmii, int, 0);
+MODULE_PARM_DESC(fast_aging, "Fast aging");
+MODULE_PARM_DESC(multi_dev, "Multiple device interfaces");
+MODULE_PARM_DESC(stp, "STP support");
+MODULE_PARM_DESC(avb, "AVB support");
+MODULE_PARM_DESC(authen, "802.1X Authentication");
+MODULE_PARM_DESC(sgmii, "SGMII mode");
+
+#ifdef CONFIG_KSZ_MRP
+module_param(mrp, int, 0);
+MODULE_PARM_DESC(mrp, "MRP support");
+#endif
+
+#ifdef CONFIG_1588_PTP
+module_param(pps_gpo, int, 0);
+module_param(mhz_gpo, int, 0);
+MODULE_PARM_DESC(pps_gpo, "PPS GPIO");
+MODULE_PARM_DESC(mhz_gpo, "10MHz GPIO");
+#endif
+
+#ifdef CONFIG_KSZ_IBA
+module_param(iba, int, 0);
+MODULE_PARM_DESC(iba, "IBA support");
+#endif
+
+module_param(intr_mode, int, 0);
+MODULE_PARM_DESC(intr_mode,
+	"Configure which interrupt mode to use(1=level low, 2=falling)");
+
+module_param(sw_host_port, int, 0);
+MODULE_PARM_DESC(sw_host_port,
+	"Configure switch host port");
+
+module_param(sysfs_sw, int, 0);
+MODULE_PARM_DESC(sysfs_sw,
+	"Configure sysfs port base");
+
+module_param(ports, int, 0);
+MODULE_PARM_DESC(ports,
+	"Configure number of ports");
+
+module_param(eth1_ports, int, 0);
+module_param(eth2_ports, int, 0);
+module_param(eth3_ports, int, 0);
+module_param(eth4_ports, int, 0);
+module_param(eth5_ports, int, 0);
+module_param(eth6_ports, int, 0);
+MODULE_PARM_DESC(eth1_ports, "Ports to use on device 1.");
+MODULE_PARM_DESC(eth2_ports, "Ports to use on device 2.");
+MODULE_PARM_DESC(eth3_ports, "Ports to use on device 3.");
+MODULE_PARM_DESC(eth4_ports, "Ports to use on device 4.");
+MODULE_PARM_DESC(eth5_ports, "Ports to use on device 5.");
+MODULE_PARM_DESC(eth6_ports, "Ports to use on device 6.");
+
+module_param(eth1_vlan, int, 0);
+module_param(eth2_vlan, int, 0);
+module_param(eth3_vlan, int, 0);
+module_param(eth4_vlan, int, 0);
+module_param(eth5_vlan, int, 0);
+module_param(eth6_vlan, int, 0);
+MODULE_PARM_DESC(eth1_vlan, "VLAN to use on device 1.");
+MODULE_PARM_DESC(eth2_vlan, "VLAN to use on device 2.");
+MODULE_PARM_DESC(eth3_vlan, "VLAN to use on device 3.");
+MODULE_PARM_DESC(eth4_vlan, "VLAN to use on device 4.");
+MODULE_PARM_DESC(eth5_vlan, "VLAN to use on device 5.");
+MODULE_PARM_DESC(eth6_vlan, "VLAN to use on device 6.");
+
+module_param(eth1_proto, charp, 0);
+module_param(eth2_proto, charp, 0);
+module_param(eth3_proto, charp, 0);
+module_param(eth4_proto, charp, 0);
+module_param(eth5_proto, charp, 0);
+module_param(eth6_proto, charp, 0);
+MODULE_PARM_DESC(eth1_proto, "Protocol to use on device 1.");
+MODULE_PARM_DESC(eth2_proto, "Protocol to use on device 2.");
+MODULE_PARM_DESC(eth3_proto, "Protocol to use on device 3.");
+MODULE_PARM_DESC(eth4_proto, "Protocol to use on device 4.");
+MODULE_PARM_DESC(eth5_proto, "Protocol to use on device 5.");
+MODULE_PARM_DESC(eth6_proto, "Protocol to use on device 6.");
+
diff -Napur linux-linux4microchip-2022.10-orig/drivers/net/ethernet/micrel/ksz_sw_9897.h linux-linux4microchip-2022.10/drivers/net/ethernet/micrel/ksz_sw_9897.h
--- linux-linux4microchip-2022.10-orig/drivers/net/ethernet/micrel/ksz_sw_9897.h	1969-12-31 16:00:00.000000000 -0800
+++ linux-linux4microchip-2022.10/drivers/net/ethernet/micrel/ksz_sw_9897.h	2023-11-16 19:02:12.061572448 -0800
@@ -0,0 +1,1434 @@
+/**
+ * Microchip KSZ9897 switch common header
+ *
+ * Copyright (c) 2015-2023 Microchip Technology Inc.
+ *	Tristram Ha <Tristram.Ha@microchip.com>
+ *
+ * Copyright (c) 2013-2015 Micrel, Inc.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ */
+
+#ifndef KSZ_SW_9897_H
+#define KSZ_SW_9897_H
+
+
+#if defined(CONFIG_PHYLINK) || defined(CONFIG_PHYLINK_MODULE)
+#include <linux/phylink.h>
+#endif
+
+/* These definitions should be defined before this header file. */
+#ifndef PRIO_QUEUES
+#define PRIO_QUEUES			4
+#endif
+#define PRIO_QUEUES_M			(PRIO_QUEUES - 1)
+
+#ifndef KS_PRIO_IN_REG
+#define KS_PRIO_IN_REG			2
+#endif
+
+#ifndef TOTAL_PORT_NUM
+#define TOTAL_PORT_NUM			7
+#endif
+
+#ifndef SWITCH_COUNTER_NUM
+#define SWITCH_COUNTER_NUM		0x20
+#endif
+
+#ifndef SW_D
+#error "SW_D and other data bus parameters need to be defined."
+#endif
+
+/* Host port can be any one of them. */
+#define SWITCH_PORT_NUM			(TOTAL_PORT_NUM)
+
+#define MAX_SW_DEVICES			2
+
+
+struct ksz_vlan_table;
+
+#define NUM_OF_VID			4094
+#define NUM_OF_MSTI			8
+
+#include "ksz_sw_api.h"
+#ifdef CONFIG_KSZ_MSTP
+#include "ksz_mstp.h"
+#elif defined(CONFIG_KSZ_STP)
+#include "ksz_stp.h"
+#endif
+#ifdef CONFIG_KSZ_IBA
+#include "ksz_iba.h"
+#endif
+#ifdef CONFIG_1588_PTP
+#include "ksz_ptp_9897.h"
+#endif
+#if defined(CONFIG_KSZ_AVB) || defined(CONFIG_KSZ_MRP)
+#include "ksz_mrp.h"
+#endif
+#ifdef CONFIG_KSZ_DLR
+#include "ksz_dlr.h"
+#endif
+#ifdef CONFIG_KSZ_HSR
+#include "ksz_hsr.h"
+#endif
+
+
+#define LEARNED_MAC_TABLE_ENTRIES	1024
+#define STATIC_MAC_TABLE_ENTRIES	16
+#define RESERVED_MCAST_TABLE_ENTRIES	0x30
+#define ACTUAL_MCAST_TABLE_ENTRIES	8
+#define SWITCH_MAC_TABLE_ENTRIES	16
+#define MULTI_MAC_TABLE_ENTRIES		80
+
+/**
+ * struct ksz_mac_table - Static MAC table data structure
+ * @mac_addr:	MAC address to filter.
+ * @vid:	VID value.
+ * @fid:	FID value.
+ * @ports:	Port membership.
+ * @override:	Override setting.
+ * @use_fid:	FID use setting.
+ * @valid:	Valid setting indicating the entry is being used.
+ */
+struct ksz_mac_table {
+	u8 addr[ETH_ALEN];
+	u32 ports;
+	u16 fid;
+	u8 mstp;
+	u8 prio;
+	u8 src:1;
+	u8 dst:1;
+	u8 override:1;
+	u8 use_fid:1;
+	u8 valid:1;
+	u8 dirty:1;
+};
+
+#define FWD_HOST_OVERRIDE		BIT(0)
+#define FWD_HOST			BIT(1)
+#define FWD_STP_DEV			BIT(2)
+#define FWD_MAIN_DEV			BIT(3)
+#define FWD_VLAN_DEV			BIT(4)
+#define FWD_MCAST			BIT(5)
+#define FWD_UCAST			BIT(6)
+#define FWD_KNOWN			BIT(7)
+
+struct ksz_alu_table {
+	u16 index;
+	u16 owner;
+	u8 forward;
+	u8 type;
+	u8 valid:1;
+};
+
+#define VLAN_TABLE_ENTRIES		4096
+#define VID_IN_DATA			32
+#define FID_ENTRIES			128
+#define FID_IN_DATA			32
+
+/**
+ * struct ksz_vlan_table - VLAN table data structure
+ * @vid:	VID value.
+ * @fid:	FID value.
+ * @ports:	Port membership.
+ * @untag:	Untag membership.
+ * @mstp:	MSTP number.
+ * @prio:	Priority
+ * @fo:		Forward option.
+ * @valid:	Valid setting indicating the entry is being used.
+ */
+struct ksz_vlan_table {
+	u16 vid;
+	u16 fid;
+	u32 ports;
+	u32 untag;
+	u8 mstp;
+	u8 prio;
+	u8 option:1;
+	u8 valid:1;
+	u8 dirty:1;
+};
+
+#ifdef CONFIG_KSZ_HSR
+/**
+ * struct ksz_hsr_table - HSR table data structure
+ * @dst:	Destination MAC address.
+ * @src:	Source MAC address.
+ * @age_cnt:	Age count.
+ * @path_id:	Path ID.
+ * @start_seq:	Starting sequence number.
+ * @exp_seq:	Expected sequence number.
+ * @seq_cnt:	Out of sequence number count.
+ * @valid:	Valid setting indicating the entry is being used.
+ * @dirty:	Flag indicating structure has been changed.
+ */
+struct ksz_hsr_table {
+	u8 dst_mac[ETH_ALEN];
+	u8 src_mac[ETH_ALEN];
+	u8 age_cnt;
+	u8 path_id;
+	u16 start_seq[2];
+	u16 exp_seq[2];
+	u16 seq_cnt[2];
+	u8 valid:1;
+	u8 dirty:1;
+};
+#endif
+
+#define PRIO_802_1P_ENTRIES		8
+
+#define DIFFSERV_ENTRIES		64
+
+#define ACL_TABLE_ENTRIES		16
+
+struct ksz_acl_table {
+	u16 first_rule;
+	u16 ruleset;
+	u8 mac[ETH_ALEN];
+	u16 eth_type;
+	u8 protocol;
+	u8 ip4_addr[4];
+	u8 ip4_mask[4];
+	u32 seqnum;
+	u16 max_port;
+	u16 min_port;
+	u8 prio;
+	u8 vlan_prio;
+	u16 ports;
+	u16 cnt;
+	u8 tcp_flag_mask;
+	u8 tcp_flag;
+	u32 mode:2;
+	u32 enable:2;
+	u32 src:1;
+	u32 equal:1;
+	u32 port_mode:2;
+	u32 tcp_flag_enable:1;
+	u32 msec:1;
+	u32 intr_mode:1;
+	u32 prio_mode:2;
+	u32 vlan_prio_replace:1;
+	u32 map_mode:2;
+	u32 changed:1;
+	u32 action_changed:1;
+	u32 ruleset_changed:1;
+	u32 action_selected:1;
+
+	u8 data[ACL_TABLE_LEN];
+};
+
+/**
+ * struct ksz_port_mib - Port MIB data structure
+ * @cnt_ptr:	Current pointer to MIB counter index.
+ * @mib_start:	The starting counter index.  Some ports do not start at 0.
+ * @counter:	64-bit MIB counter value.
+ * @dropped:	Temporary buffer to remember last read packet dropped values.
+ * @read_cnt:	Used to signal when to read the MIB counter.
+ * @read_max:	Used to indicate how often to read the MIB counter.
+ *
+ * MIB counters needs to be read periodically so that counters do not get
+ * overflowed and give incorrect values.  A right balance is needed to
+ * satisfy this condition and not waste too much CPU time.
+ */
+struct ksz_port_mib {
+	u8 cnt_ptr;
+	u8 mib_start;
+	u8 interval;
+	u8 reserved[1];
+
+	u64 counter[TOTAL_SWITCH_COUNTER_NUM];
+	struct {
+		unsigned long last;
+		u64 last_cnt;
+		u32 peak;
+		unsigned long check;
+		u64 prev_cnt;
+		bool no_change;
+	} rate[2];
+	unsigned long last_drop;
+	u64 drop;
+	u64 first_drop;
+};
+
+enum {
+	STP_STATE_DISABLED = 0,
+	STP_STATE_LISTENING,
+	STP_STATE_LEARNING,
+	STP_STATE_FORWARDING,
+	STP_STATE_BLOCKED,
+	STP_STATE_SIMPLE
+};
+
+/**
+ * struct ksz_port_cfg - Port configuration data structure
+ * @vid:	VID value.
+ * @member:	Port membership.
+ * @port_prio:	Port priority.
+ * @rate_ctrl:	Priority rate control.
+ * @rx_rate:	Receive priority rate.
+ * @tx_rate:	Transmit priority rate.
+ * @rate_limit: Priority rate limit value.
+ * @vid_member:	VLAN membership.
+ * @index:	Net device pointer.
+ * @stp_state:	Current Spanning Tree Protocol state.
+ */
+struct ksz_port_cfg {
+	u16 vid;
+	u16 member;
+	u8 rate_ctrl[PRIO_QUEUES];
+	uint rx_packet[RX_PRIO_QUEUES];
+	uint rx_rate[RX_PRIO_QUEUES];
+	uint tx_packet[PRIO_QUEUES];
+	uint tx_rate[PRIO_QUEUES];
+	u32 color_map[DIFFSERV_ENTRIES / 16];
+	u32 tc_map[PRIO_802_1P_ENTRIES / 8];
+	u32 untagged[VLAN_TABLE_ENTRIES / VID_IN_DATA];
+	u8 p_index;
+	u8 q_index;
+	u8 port_prio;
+	u8 rate_limit;
+	bool gigabit;
+	bool packet_based;
+	u16 intr_mask;
+	u16 vid_member;
+	int index;
+	int stp_state[NUM_OF_MSTI];
+
+	struct ksz_acl_table acl_info[ACL_TABLE_ENTRIES];
+	u16 acl_index;
+	u16 acl_act_index;
+	u16 acl_rule_index;
+
+	u16 mmd_id;
+	u16 mmd_reg;
+
+	u32 enabled:1;
+	u32 ptp_enabled:1;
+	u32 asCapable:1;
+	u32 asCapable_set:1;
+	u32 avb_a:1;
+	u32 avb_b:1;
+	u32 restricted:1;
+	u32 freeze:1;
+
+	u16 phy_ctrl;
+	u16 phy_adv;
+	u16 phy_adv_g;
+	u8 phy_intr;
+
+	u8 mstp;
+
+	u32 setup_time;
+};
+
+/**
+ * struct ksz_sw_info - KSZ9897 switch information data structure
+ * @mac_table:	MAC table entries information.
+ * @alu_table:	ALU table entries information.
+ * @multi_net:	Network multicast addresses used.
+ * @multi_sys:	System multicast addresses used.
+ * @port_cfg:	Port configuration information.
+ * @rstp:	RSTP information.
+ * @iba:	IBA information.
+ * @dlr:	DLR information.
+ * @hsr:	HSR information.
+ * @hsr_entry:	HSR table entry information.
+ * @mac_entry:	MAC table entry information.
+ * @vlan_entry:	VLAN table entry information.
+ * @diffserv:	DiffServ priority settings.  Possible values from 6-bit of ToS
+ *		(bit7 ~ bit2) field.
+ * @p_802_1p:	802.1P priority settings.  Possible values from 3-bit of 802.1p
+ *		Tag priority field.
+ * @br_addr:	Bridge address.  Used for STP.
+ * @mac_addr:	Switch MAC address.
+ * @broad_per:	Broadcast storm percentage.
+ * @member:	Current port membership.  Used for STP.
+ * @phy_addr:	PHY address used by first port.
+ */
+struct ksz_sw_info {
+	struct ksz_mac_table reserved_table[ACTUAL_MCAST_TABLE_ENTRIES];
+	struct ksz_mac_table mac_table[MULTI_MAC_TABLE_ENTRIES];
+	struct ksz_alu_table alu_table[MULTI_MAC_TABLE_ENTRIES];
+	int forward;
+	int multi_net;
+	int multi_sys;
+	struct ksz_port_cfg port_cfg[TOTAL_PORT_NUM];
+#ifdef CONFIG_KSZ_STP
+	struct ksz_stp_info rstp;
+#endif
+#ifdef CONFIG_KSZ_IBA
+	struct ksz_iba_info iba;
+#endif
+#ifdef CONFIG_KSZ_DLR
+	struct ksz_dlr_info dlr;
+#endif
+#ifdef CONFIG_KSZ_HSR
+	struct ksz_hsr_info hsr;
+	struct ksz_hsr_table hsr_entry;
+#endif
+	struct ksz_mac_table mac_entry;
+	struct ksz_vlan_table vlan_entry;
+
+	SW_D diffserv[DIFFSERV_ENTRIES / KS_PRIO_IN_REG];
+	SW_D p_802_1p[PRIO_802_1P_ENTRIES / KS_PRIO_IN_REG];
+
+	u8 br_addr[ETH_ALEN];
+	u8 mac_addr[ETH_ALEN];
+
+	u8 vid2fid[VLAN_TABLE_ENTRIES];
+	u8 fid2mstid[FID_ENTRIES];
+	u32 vid[VLAN_TABLE_ENTRIES / VID_IN_DATA];
+	u32 fid[FID_ENTRIES / FID_IN_DATA];
+	u16 fid_cnt;
+
+	u8 broad_per;
+	u8 member[NUM_OF_MSTI];
+	u8 phy_addr;
+	u8 fid_updated;
+};
+
+struct ksz_sw;
+struct ksz_port;
+
+/**
+ * struct ksz_port_state - Port state information data structure
+ * @state:	Connection status of the port.
+ * @link_down:	Indication the link has just gone down.
+ *
+ * It is pointless to read MIB counters when the port is disconnected.  The
+ * @state provides the connection status so that MIB counters are read only
+ * when the port is connected.  The @link_down indicates the port is just
+ * disconnected so that all MIB counters are read one last time to update the
+ * information.
+ */
+struct ksz_port_state {
+	uint state;
+	uint tx_rate;
+	u8 link_down;
+};
+
+enum {
+	INTF_RGMII,
+	INTF_SGMII,
+	INTF_TX_PHY,
+	INTF_T1_PHY,
+};
+
+#define TX_RATE_UNIT			10000
+
+/**
+ * struct ksz_port_info - Port information data structure
+ * @interface:	PHY interface.
+ * @state:	Connection status of the port.
+ * @tx_rate:	Transmit rate divided by 10000 to get Mbit.
+ * @duplex:	Duplex mode.
+ * @flow_ctrl:	Flow control.
+ * @link:	Link status.  Used to determine link.
+ * @advertised:	Advertised auto-negotiation setting.  Used to determine link.
+ * @partner:	Auto-negotiation partner setting.  Used to determine link.
+ * @status:	LinkMD status values.
+ * @length:	LinkMD length values.
+ * @sqi:	Signal Quality Indicator.
+ * @mac_addr:	MAC address of the port.
+ * @phy_id:	PHY id used by the port.
+ */
+struct ksz_port_info {
+	phy_interface_t interface;
+	uint state;
+	uint tx_rate;
+	u8 duplex;
+	u8 flow_ctrl;
+	u8 link_down;
+	u16 link;
+	u32 advertised;
+	u32 partner;
+	u32 status[5];
+	u32 length[5];
+	u16 sqi;
+	u8 mac_addr[ETH_ALEN];
+	u8 own_flow_ctrl;
+	u8 own_duplex;
+	u16 own_speed;
+	u8 phy_id;
+	u32 phy:1;
+	u32 fiber:1;
+
+	u8 phy_p;
+	u8 log_p;
+	u16 phy_m;
+	u16 log_m;
+	u8 intf;
+
+	int (*get_link_speed)(struct ksz_sw *sw, uint p, bool force_link);
+	void (*set_link_speed)(struct ksz_sw *sw, uint p, int speed,
+			       int duplex, int flow_ctrl);
+	void (*force_link_speed)(struct ksz_sw *sw, uint p, int speed,
+				 int duplex, int flow_ctrl);
+};
+
+struct ksz_sw_reg_ops {
+	void (*lock)(struct ksz_sw *sw);
+	void (*unlock)(struct ksz_sw *sw);
+
+	u8 (*r8)(struct ksz_sw *sw, unsigned reg);
+	u16 (*r16)(struct ksz_sw *sw, unsigned reg);
+	u32 (*r24)(struct ksz_sw *sw, unsigned reg);
+	u32 (*r32)(struct ksz_sw *sw, unsigned reg);
+	void (*w8)(struct ksz_sw *sw, unsigned reg, unsigned val);
+	void (*w16)(struct ksz_sw *sw, unsigned reg, unsigned val);
+	void (*w24)(struct ksz_sw *sw, unsigned reg, unsigned val);
+	void (*w32)(struct ksz_sw *sw, unsigned reg, unsigned val);
+
+	void (*r)(struct ksz_sw *sw, unsigned reg, void *buf, size_t cnt);
+	void (*w)(struct ksz_sw *sw, unsigned reg, void *buf, size_t cnt);
+
+	int (*get)(struct ksz_sw *sw, u32 reg, size_t count, void *buf);
+	int (*set)(struct ksz_sw *sw, u32 reg, size_t count, void *buf);
+
+	int (*r_dyn_mac_hw)(struct ksz_sw *sw, u16 addr, u8 *src_addr,
+		u16 src_fid, struct ksz_mac_table *mac, u16 *entry);
+	int (*w_dyn_mac_hw)(struct ksz_sw *sw, u16 addr, u8 *src_addr,
+		u16 src_fid, struct ksz_mac_table *mac);
+	int (*start_dyn_mac_hw)(struct ksz_sw *sw);
+	int (*g_dyn_mac_hw)(struct ksz_sw *sw, struct ksz_mac_table *mac);
+	u32 (*stop_dyn_mac_hw)(struct ksz_sw *sw);
+	int (*r_sta_mac_hw)(struct ksz_sw *sw, u32 ctrl[], int num,
+		struct ksz_mac_table *mac);
+	int (*w_sta_mac_hw)(struct ksz_sw *sw, u32 ctrl[], int num,
+		struct ksz_mac_table *mac);
+	int (*r_vlan_hw)(struct ksz_sw *sw, u32 data[], int num);
+	int (*w_vlan_hw)(struct ksz_sw *sw, u32 data[], int num);
+	int (*r_mib_cnt_hw)(struct ksz_sw *sw, uint port, u32 addr[], int num,
+			    u32 data[]);
+	int (*r_acl_hw)(struct ksz_sw *sw, uint port, u16 addr, u8 data[]);
+	int (*w_acl_hw)(struct ksz_sw *sw, uint port, u16 addr, u8 data[]);
+
+#ifdef CONFIG_KSZ_HSR
+	int (*r_hsr_hw)(struct ksz_sw *sw, u16 addr,
+		struct ksz_hsr_table *hsr);
+	int (*w_hsr_hw)(struct ksz_sw *sw, u16 addr,
+		struct ksz_hsr_table *hsr);
+	int (*start_hsr_hw)(struct ksz_sw *sw);
+	int (*g_hsr_hw)(struct ksz_sw *sw, struct ksz_hsr_table *hsr);
+	u32 (*stop_hsr_hw)(struct ksz_sw *sw);
+#endif
+};
+
+struct ksz_sw_net_ops {
+	void (*setup_special)(struct ksz_sw *sw, int *port_cnt,
+		int *mib_port_cnt, int *dev_cnt,
+		const void *ops);
+	void (*setup_mdiobus)(struct ksz_sw *sw, void *bus);
+	int (*setup_dev)(struct ksz_sw *sw, struct net_device *dev,
+		char *dev_name, struct ksz_port *port, int i, uint port_cnt,
+		uint mib_port_cnt);
+	void (*leave_dev)(struct ksz_sw *sw);
+	int (*get_ready)(struct net_device *dev);
+
+	void (*start)(struct ksz_sw *sw, u8 *addr);
+	int (*stop)(struct ksz_sw *sw, int complete);
+	int (*open_dev)(struct ksz_sw *sw, struct net_device *dev,
+		struct ksz_port *port, u8 *addr);
+	void (*open_port)(struct ksz_sw *sw, struct net_device *dev,
+		struct ksz_port *port);
+	void (*close_port)(struct ksz_sw *sw, struct net_device *dev,
+		struct ksz_port *port);
+	void (*open)(struct ksz_sw *sw);
+	void (*close)(struct ksz_sw *sw);
+
+	void (*netdev_start_queue)(struct ksz_sw *sw, struct net_device *dev);
+	void (*netdev_stop_queue)(struct ksz_sw *sw, struct net_device *dev);
+	void (*netdev_wake_queue)(struct ksz_sw *sw, struct net_device *dev);
+	void (*netdev_open_port)(struct ksz_sw *sw, struct net_device *dev);
+
+	u8 (*set_mac_addr)(struct ksz_sw *sw, struct net_device *dev,
+		u8 promiscuous, uint port);
+
+	int (*get_mtu)(struct ksz_sw *sw);
+	int (*get_tx_len)(struct ksz_sw *sw, struct sk_buff *skb, uint port,
+		int *header);
+	void (*add_tail_tag)(struct ksz_sw *sw, struct sk_buff *skb, uint dst);
+	int (*get_tail_tag)(struct ksz_sw *sw, u8 *trailer, uint *port);
+	void (*add_vid)(struct ksz_sw *sw, u16 vid);
+	void (*kill_vid)(struct ksz_sw *sw, u16 vid);
+	struct sk_buff *(*check_tx)(struct ksz_sw *sw, struct net_device *dev,
+		struct sk_buff *skb, struct ksz_port *priv);
+	struct net_device *(*rx_dev)(struct ksz_sw *sw, u8 *data, u32 *len,
+		int *tag, int *port);
+	int (*match_pkt)(struct ksz_sw *sw, struct net_device **dev,
+		void **priv, int (*get_promiscuous)(void *ptr),
+		int (*get_multi)(void *ptr), struct sk_buff *skb,
+		u8 h_promiscuous);
+	struct net_device *(*parent_rx)(struct ksz_sw *sw,
+		struct net_device *dev, int *forward);
+	int (*port_vlan_rx)(struct sk_buff *skb, int forward, int tag);
+	struct sk_buff *(*final_skb)(struct ksz_sw *sw, struct sk_buff *skb,
+		struct net_device *dev, struct ksz_port *port);
+	int (*drv_rx)(struct ksz_sw *sw, struct sk_buff *skb, uint port);
+	void (*set_multi)(struct ksz_sw *sw, struct net_device *dev,
+		struct ksz_port *priv);
+};
+
+struct ksz_sw_ops {
+	void (*init)(struct ksz_sw *sw);
+	void (*exit)(struct ksz_sw *sw);
+	int (*dev_req)(struct ksz_sw *sw, char *arg,
+		struct file_dev_info *info);
+
+	uint (*get_phy_port)(struct ksz_sw *sw, uint n);
+	uint (*get_log_port)(struct ksz_sw *sw, uint p);
+	uint (*get_phy_mask_from_log)(struct ksz_sw *sw, uint log_m);
+
+	void (*chk_regs)(struct ksz_sw *sw, u32 addr, u8 *val, size_t txl);
+
+	void (*acquire)(struct ksz_sw *sw);
+	void (*release)(struct ksz_sw *sw);
+
+	void (*p_r8)(struct ksz_sw *sw, uint port, uint offset, u8 *data);
+	void (*p_w8)(struct ksz_sw *sw, uint port, uint offset, u8 data);
+	void (*p_r16)(struct ksz_sw *sw, uint port, uint offset, u16 *data);
+	void (*p_w16)(struct ksz_sw *sw, uint port, uint offset, u16 data);
+	void (*p_r32)(struct ksz_sw *sw, uint port, uint offset, u32 *data);
+	void (*p_w32)(struct ksz_sw *sw, uint port, uint offset, u32 data);
+
+	int (*chk)(struct ksz_sw *sw, u32 addr, SW_D bits);
+	void (*cfg)(struct ksz_sw *sw, u32 addr, SW_D bits, bool set);
+
+	int (*port_get_link_speed)(struct ksz_port *port);
+	void (*port_set_link_speed)(struct ksz_port *port);
+	void (*port_force_link_speed)(struct ksz_port *port);
+
+	int (*port_r_cnt)(struct ksz_sw *sw, uint port);
+	void (*get_mib_counters)(struct ksz_sw *sw, int first, int cnt,
+		u64 *counter);
+
+	ssize_t (*sysfs_read)(struct ksz_sw *sw, int proc_num,
+		struct ksz_port *port, ssize_t len, char *buf);
+	ssize_t (*sysfs_read_hw)(struct ksz_sw *sw, int proc_num, ssize_t len,
+		char *buf);
+	int (*sysfs_write)(struct ksz_sw *sw, int proc_num,
+		struct ksz_port *port, int num, const char *buf);
+	ssize_t (*sysfs_port_read)(struct ksz_sw *sw, int proc_num, uint port,
+		ssize_t len, char *buf);
+	ssize_t (*sysfs_port_read_hw)(struct ksz_sw *sw, int proc_num,
+		uint port, ssize_t len, char *buf);
+	int (*sysfs_port_write)(struct ksz_sw *sw, int proc_num, uint port,
+		int num, const char *buf);
+	ssize_t (*sysfs_mac_read)(struct ksz_sw *sw, int proc_num, ssize_t len,
+		char *buf);
+	int (*sysfs_mac_write)(struct ksz_sw *sw, int proc_num, int num,
+		const char *buf);
+	ssize_t (*sysfs_vlan_read)(struct ksz_sw *sw, int proc_num,
+		ssize_t len, char *buf);
+	int (*sysfs_vlan_write)(struct ksz_sw *sw, int proc_num, int num);
+
+#ifdef CONFIG_KSZ_STP
+	ssize_t (*sysfs_stp_read)(struct ksz_sw *sw, int proc_num, ssize_t len,
+		char *buf);
+	int (*sysfs_stp_write)(struct ksz_sw *sw, int proc_num, int num,
+		const char *buf);
+	ssize_t (*sysfs_stp_port_read)(struct ksz_sw *sw, int proc_num,
+		uint port, ssize_t len, char *buf);
+	int (*sysfs_stp_port_write)(struct ksz_sw *sw, int proc_num, uint port,
+		int num, const char *buf);
+#endif
+
+#if defined(CONFIG_KSZ_AVB) || defined(CONFIG_KSZ_MRP)
+	ssize_t (*sysfs_mrp_read)(struct ksz_sw *sw, int proc_num, ssize_t len,
+		char *buf);
+	int (*sysfs_mrp_write)(struct ksz_sw *sw, int proc_num, int num,
+		const char *buf);
+	ssize_t (*sysfs_mrp_port_read)(struct ksz_sw *sw, int proc_num,
+		uint port, ssize_t len, char *buf);
+	int (*sysfs_mrp_port_write)(struct ksz_sw *sw, int proc_num, uint port,
+		int num, const char *buf);
+#endif
+
+#ifdef CONFIG_KSZ_HSR
+	ssize_t (*sysfs_hsr_read)(struct ksz_sw *sw, int proc_num, ssize_t len,
+		char *buf);
+	int (*sysfs_hsr_write)(struct ksz_sw *sw, int proc_num, int num,
+		const char *buf);
+#endif
+
+	ssize_t (*sysfs_acl_read)(struct ksz_sw *sw, int proc_num, uint port,
+		ssize_t len, char *buf);
+	int (*sysfs_acl_write)(struct ksz_sw *sw, int proc_num, uint port,
+		int num, const char *buf);
+
+	void (*cfg_mac)(struct ksz_sw *sw, u8 index, const u8 *dest, u32 ports,
+		int override, int use_fid, u16 fid);
+	void (*cfg_vlan)(struct ksz_sw *sw, u8 index, u16 vid, u16 fid,
+		u32 ports);
+	u8 (*alloc_mac)(struct ksz_sw *sw);
+	void (*free_mac)(struct ksz_sw *sw, u8 index);
+	u8 (*alloc_vlan)(struct ksz_sw *sw);
+	void (*free_vlan)(struct ksz_sw *sw, u8 index);
+	u16 (*alloc_fid)(struct ksz_sw *sw, u16 vid);
+	void (*free_fid)(struct ksz_sw *sw, u16 fid);
+
+	const u8 *(*get_br_id)(struct ksz_sw *sw);
+	void (*from_backup)(struct ksz_sw *sw, uint p);
+	void (*to_backup)(struct ksz_sw *sw, uint p);
+	void (*from_designated)(struct ksz_sw *sw, uint p, bool alt);
+	void (*to_designated)(struct ksz_sw *sw, uint p);
+	void (*tc_detected)(struct ksz_sw *sw, uint p);
+	int (*get_tcDetected)(struct ksz_sw *sw, uint p);
+
+	int (*get_id)(struct ksz_sw *sw, u8 *id1, u8 *id2, char *name);
+	void (*cfg_tail_tag)(struct ksz_sw *sw, bool enable);
+	void (*cfg_each_port)(struct ksz_sw *sw, uint p, bool cpu);
+	int (*port_to_phy_addr)(struct ksz_sw *sw, uint p);
+	void (*set_port_addr)(struct ksz_sw *sw, uint p, u8 *addr);
+
+	void (*cfg_src_filter)(struct ksz_sw *sw, bool set);
+	void (*flush_table)(struct ksz_sw *sw, uint port);
+	void (*fwd_unk_mcast)(struct ksz_sw *sw, bool set);
+	void (*fwd_unk_ucast)(struct ksz_sw *sw);
+	void (*fwd_unk_vid)(struct ksz_sw *sw);
+
+	void (*port_freeze_mib)(struct ksz_sw *sw, uint port, bool freeze);
+	void (*freeze_mib)(struct ksz_sw *sw, bool freeze);
+};
+
+struct ksz_sw_tx_tag {
+	u32 timestamp;
+	u16 ports;
+};
+
+struct ksz_sw_cached_regs {
+	u32 ptp_unit_index;
+	u16 ptp_clk_ctrl;
+	u16 xmii[2];
+};
+
+struct ksz_dev_map {
+	u16 cnt;
+	u16 mask;
+	u16 first;
+	u16 phy_id;
+	u16 vlan;
+	uint proto;
+};
+
+struct phy_priv {
+	struct ksz_port *port;
+	enum phy_state state;
+};
+
+/* Switch features and bug fixes. */
+#define STP_SUPPORT			BIT(0)
+#define VLAN_PORT			BIT(1)
+#define VLAN_PORT_REMOVE_TAG		BIT(2)
+#define VLAN_PORT_TAGGING		BIT(3)
+#define VLAN_PORT_START			200
+#define SW_VLAN_DEV			BIT(4)
+#define MRP_SUPPORT			BIT(5)
+
+#define ACL_CORRUPT_BUG			BIT(8)
+#define NO_GLOBAL_RESET			BIT(9)
+#define PHY_INTR_BUG			BIT(10)
+#define IS_9893				BIT(15)
+#define SETUP_PHY			BIT(16)
+#define NEW_XMII			BIT(17)
+#define USE_FEWER_PORTS			BIT(18)
+#define GIGABIT_SUPPORT			BIT(19)
+#define IBA_SUPPORT			BIT(20)
+#define NEW_CAP				BIT(21)
+#define AVB_SUPPORT			BIT(22)
+#define REDUNDANCY_SUPPORT		BIT(23)
+#define DLR_HW				BIT(24)
+#define HSR_HW				BIT(25)
+#define HSR_REDBOX			BIT(26)
+#define DSA_SUPPORT			BIT(28)
+#define DIFF_MAC_ADDR			BIT(29)
+#define PTP_HW				BIT(31)
+
+/* Software overrides. */
+#define PAUSE_FLOW_CTRL			BIT(0)
+#define FAST_AGING			BIT(1)
+#define MCAST_FILTER			BIT(2)
+#define HAVE_MORE_THAN_2_PORTS		BIT(3)
+#define HSR_FORWARD			BIT(4)
+#define UNK_MCAST_BLOCK			BIT(5)
+#define UPDATE_CSUM			BIT(6)
+#define DELAY_UPDATE_LINK		BIT(7)
+
+#define IBA_TEST			BIT(16)
+#define ACL_INTR_MONITOR		BIT(17)
+#define SYSFS_PHY_PORT			BIT(18)
+#define SYSFS_1_BASE			BIT(19)
+
+#define TAIL_PRP_0			(1 << 24)
+#define TAIL_PRP_1			(1 << 25)
+
+#define USE_802_1X_AUTH			BIT(27)
+#define VLAN_SET			BIT(28)
+#define PTP_TAG				BIT(29)
+#define TAG_REMOVE			BIT(30)
+#define TAIL_TAGGING			BIT(31)
+
+#define TAIL_TAG_SET_OVERRIDE		BIT(31)
+#define TAIL_TAG_SET_QUEUE		BIT(30)
+
+/**
+ * struct ksz_sw - Virtual switch data structure
+ * @dev:		Pointer to hardware device.
+ * @phydev:		Pointer to PHY device interface.
+ * @interface:		The R/G/MII interface used.
+ * @msg_enable:		The message flags controlling driver output.
+ * @hwlock:		Pointer to hardware lock.
+ * @reglock:		Pointer to register lock.
+ * @acllock:		ACL table lock.
+ * @alulock:		ALU table lock.
+ * @vlanlock:		VLAN table lock.
+ * @hsrlock:		HSR table lock.
+ * @lock		Software lock to switch structure.
+ * @locked:		locked status.
+ * @info:		Pointer to switch information structure.
+ * @port_info:		Port information.
+ * @netdev:		Pointer to OS dependent network devices.
+ * @phy:		Pointer to OS dependent PHY devices.
+ * @dev_offset:		Indication of a switch associated network device.
+ * @phy_offset:		Indication of a port associated PHY device.
+ * @port_state:		Port state information.
+ * @port_mib:		Port MIB information.
+ * @mib_cnt:		Number of MIB counters this switch has.
+ * @mib_port_cnt:	Number of ports with MIB counters.
+ * @phy_port_cnt:	Number of ports with actual PHY.
+ * @port_cnt:		Number of ports to support.
+ * @monitor_timer_info:	Timer information for monitoring ports.
+ * @counter:		Pointer to OS dependent MIB counter information.
+ * @link_read:		Workqueue for link monitoring.
+ * @ops:		Switch function access.
+ * @reg:		Switch register access.
+ * @net_ops:		Network related switch function access.
+ * @HOST_PORT:		A predefined value indicating the host port.
+ * @HOST_MASK:		A predefined value indicating the host port mask.
+ * @PORT_MASK:		A predefined value indicating the port mask.
+ * @PORT_INTR_MASK:	A predefined value indicating the port interrupt mask.
+ * @TAIL_TAG_LOOKUP:	A predefined value indicating tx tail tag lookup.
+ * @TAIL_TAG_OVERRIDE:	A predefined value indicating tx tail tag override.
+ * @live_ports:		Bitmap of ports with link enabled.
+ * @on_ports:		Bitmap of ports with 802.1X enabled.
+ * @rx_ports:		Bitmap of ports with receive enabled.
+ * @tx_ports:		Bitmap of ports with transmit enabled.
+ * @dev_count:		Number of network devices this switch supports.
+ * @id:			Hardware ID.  Used for display only.
+ * @vlan_id		Used for the VLAN port forwarding feature.
+ * @vid:		Used for the VLAN port forwarding feature.
+ * @revision:		Hardware revision number.
+ * @features:		Switch features to enable.
+ * @overrides:		Switch features to override.
+ * @multi_dev:		Used to specify multiple devices mode.
+ * @stp:		Used to enable STP.
+ * @fast_aging:		Used to enable fast aging.
+ */
+struct ksz_sw {
+	void *dev;
+	phy_interface_t interface;
+	u32 msg_enable;
+	wait_queue_head_t queue;
+	struct sk_buff_head txq;
+	struct mutex *hwlock;
+	struct mutex *reglock;
+	struct mutex acllock;
+	struct mutex alulock;
+	struct mutex vlanlock;
+	struct mutex hsrlock;
+	struct mutex lock;
+	int intr_cnt;
+	int intr_using;
+
+	struct ksz_sw_info *info;
+	struct ksz_port_info port_info[TOTAL_PORT_NUM];
+	struct net_device *main_dev;
+	struct ksz_port *main_port;
+	struct net_device *netdev[TOTAL_PORT_NUM];
+	struct ksz_port *netport[TOTAL_PORT_NUM];
+	struct device_node *devnode[TOTAL_PORT_NUM];
+	struct phy_device phy_map[TOTAL_PORT_NUM + 1];
+	struct phy_device *phy[TOTAL_PORT_NUM + 1];
+	struct phy_priv phydata[TOTAL_PORT_NUM + 1];
+	int dev_offset;
+	int phy_offset;
+	struct ksz_port_state port_state[TOTAL_PORT_NUM];
+	struct ksz_port_mib port_mib[TOTAL_PORT_NUM];
+	u8 mib_interval_start[4];
+	unsigned long next_jiffies;
+	int mib_cnt;
+	int mib_port_cnt;
+	int phy_port_cnt;
+	int dsa_port_cnt;
+	int port_cnt;
+	struct ksz_timer_info *mib_timer_info;
+	struct ksz_timer_info *monitor_timer_info;
+	struct ksz_counter_info *counter;
+	struct delayed_work *link_read;
+
+#if defined(CONFIG_PHYLINK) || defined(CONFIG_PHYLINK_MODULE)
+	const struct phylink_mac_ops *phylink_ops;
+#endif
+
+	const struct ksz_sw_ops *ops;
+	const struct ksz_sw_reg_ops *reg;
+	const struct ksz_sw_reg_ops *cur;
+	const struct ksz_sw_reg_ops *old;
+	struct ksz_sw_net_ops *net_ops;
+	struct delayed_work set_ops;
+	struct work_struct set_addr;
+	struct work_struct tx_fwd;
+
+	int HOST_PORT;
+	u16 HOST_MASK;
+	u16 PORT_MASK;
+	u16 PORT_INTR_MASK;
+	u16 TAIL_TAG_LOOKUP;
+	u16 TAIL_TAG_OVERRIDE;
+	u16 TAIL_TAG_SHIFT;
+	u32 intr_mask;
+	u8 lue_intr_mask;
+	u32 port_intr_mask;
+	u32 phy_intr;
+	u16 dev_ports;
+	u16 live_ports;
+	u16 on_ports;
+	u16 open_ports;
+	u16 rx_ports[NUM_OF_MSTI];
+	u16 tx_ports[NUM_OF_MSTI];
+	u8 ctrl_queue;
+	u8 tx_pad[60];
+	int mtu;
+	int tx_start;
+	int priv_port;
+	struct ksz_sw_tx_tag tag;
+	struct ksz_sw_cached_regs cached;
+
+	int dev_major;
+	u8 *msg_buf;
+	struct file_dev_info *dev_list[2];
+	uint notifications;
+	char dev_name[20];
+
+	int chip_id;
+	int dev_count;
+	int id;
+	bool change_id;
+	u32 vlan_id;
+	u16 vid;
+	u16 alu_index;
+	u8 alu_type;
+	u8 alu_dirty;
+	u16 vlan_index;
+	u16 hsr_index;
+	u8 hsr_dirty;
+	u8 vlan_dirty;
+	u8 verbose;
+	u8 running;
+
+	int revision;
+	uint features;
+	uint overrides;
+
+	int multi_dev;
+	int stp;
+	int fast_aging;
+	int sgmii_mode;
+	struct ksz_dev_map eth_maps[SWITCH_PORT_NUM];
+	int eth_cnt;
+
+#ifdef CONFIG_KSZ_MRP
+	struct delayed_work set_mrp;
+#endif
+#if defined(CONFIG_KSZ_AVB) || defined(CONFIG_KSZ_MRP)
+	struct mrp_info mrp;
+#endif
+
+#ifdef CONFIG_1588_PTP
+	/* PTP structure size can be variable. */
+	struct ptp_info ptp_hw;
+#endif
+};
+
+struct ksz_sw_sysfs {
+	struct ksz_dev_attr *ksz_port_attrs[TOTAL_PORT_NUM];
+	struct attribute **port_attrs[TOTAL_PORT_NUM];
+};
+
+/**
+ * struct ksz_port - Virtual port data structure
+ * @first_port:		Index of first port this port supports.
+ * @mib_port_cnt:	Number of ports with MIB counters.
+ * @port_cnt:		Number of ports this port supports.
+ * @flow_ctrl:		Flow control setting.  PHY_NO_FLOW_CTRL for no flow
+ *			control, and PHY_FLOW_CTRL for flow control.
+ *			PHY_TX_ONLY and PHY_RX_ONLY are not supported for 100
+ *			Mbit PHY.
+ * @duplex:		Duplex mode setting.  1 for half duplex, 2 for full
+ *			duplex, and 0 for auto, which normally results in full
+ *			duplex.
+ * @speed:		Speed setting.  10 for 10 Mbit, 100 for 100 Mbit, and
+ *			0 for auto, which normally results in 100 Mbit.
+ * @force_link:		Force link setting.  0 for auto-negotiation, and 1 for
+ *			force.
+ * @linked:		Pointer to port information linked to this port.
+ * @sw:			Pointer to virtual switch structure.
+ */
+struct ksz_port {
+	int first_port;
+	int mib_port_cnt;
+	int port_cnt;
+
+	u8 flow_ctrl;
+	u8 duplex;
+	u16 speed;
+	u8 force_link;
+	u8 state;
+	uint iba_ready:1;
+	uint need_mac:1;
+	uint opened:1;
+	uint ready:1;
+	uint report:1;
+	u16 link_ports;
+	u16 live_ports;
+
+	struct ksz_port_info *linked;
+
+	struct ksz_sw *sw;
+
+	struct delayed_work link_update;
+	struct net_device *netdev;
+	struct phy_device *phydev;
+	struct device_node *dn;
+#if defined(CONFIG_PHYLINK) || defined(CONFIG_PHYLINK_MODULE)
+	struct phylink *pl;
+	struct phylink_config pl_config;
+	struct phylink_link_state pl_state;
+#endif
+};
+
+static inline void sw_update_csum(struct ksz_sw *sw)
+{
+	sw->overrides |= UPDATE_CSUM;
+}
+
+static inline uint get_rx_tag_ports(struct ksz_sw_tx_tag *tag)
+{
+	return (tag->ports & TAIL_TAG_RX_PORTS_M);
+}
+
+static inline bool get_rx_tag_ptp(struct ksz_sw_tx_tag *tag)
+{
+	return (tag->ports & TAIL_TAG_PTP);
+}
+
+static inline uint get_tx_tag_ports(struct ksz_sw *sw,
+				    struct ksz_sw_tx_tag *tag)
+{
+	return (tag->ports & sw->PORT_MASK);
+}
+
+static inline void set_tx_tag_ports(struct ksz_sw_tx_tag *tag, uint ports)
+{
+	tag->ports = ports;
+}
+
+static inline void set_tx_tag_queue(struct ksz_sw *sw,
+				    struct ksz_sw_tx_tag *tag, u8 q)
+{
+	q &= 3;
+	tag->ports &= ~(3 << sw->TAIL_TAG_SHIFT);
+	tag->ports |= (q << sw->TAIL_TAG_SHIFT);
+}
+
+static inline bool using_hsr(struct ksz_sw *sw)
+{
+	return (sw->features & HSR_HW);
+}
+
+static inline bool using_tail_tag(struct ksz_sw *sw)
+{
+	return (sw->overrides & TAIL_TAGGING);
+}
+
+static inline bool iba_stopped(void *ptr)
+{
+	struct ksz_sw *sw = ptr;
+
+	if ((sw->info->iba.use_iba & IBA_USE_CODE_MASK) >=
+	    IBA_USE_CODE_HARD_RESET)
+		return true;
+	return false;
+}
+
+struct lan_attributes {
+	int info;
+	int version;
+	int duplex;
+	int speed;
+	int force;
+	int flow_ctrl;
+	int features;
+	int overrides;
+	int mib;
+	int reg;
+	int vid;
+	int dynamic_table;
+	int static_table;
+	int vlan_table;
+	int hsr_table;
+	int aging;
+	int fast_aging;
+	int link_aging;
+	int bcast_per;
+	int mcast_storm;
+	int tx_queue_based;
+	int diffserv_map;
+	int p_802_1p_map;
+	int vlan;
+	int null_vid;
+	int drop_inv_vid;
+	int macaddr;
+	int mirror_mode;
+	int igmp_snoop;
+	int ipv6_mld_snoop;
+	int ipv6_mld_option;
+	int aggr_backoff;
+	int no_exc_drop;
+	int jumbo_packet;
+	int legal_packet;
+	int length_check;
+	int back_pressure;
+	int sw_flow_ctrl;
+	int sw_half_duplex;
+	int sw_10_mbit;
+	int fair_flow_ctrl;
+	int vlan_bound;
+	int double_tag;
+	int isp;
+	int hsr;
+	int hsr_redbox_id;
+	int hsr_net_id;
+	int mtu;
+	int unk_ucast_fwd;
+	int unk_ucast_ports;
+	int unk_mcast_fwd;
+	int unk_mcast_ports;
+	int unk_vid_fwd;
+	int unk_vid_ports;
+	int pass_pause;
+	int pme;
+	int pme_polarity;
+
+	int host_port;
+	int ports;
+	int dev_start;
+	int port_start;
+	int vlan_start;
+	int avb;
+	int stp;
+	int two_dev;
+	int authen;
+
+	int alu_fid;
+	int alu_use_fid;
+	int alu_override;
+	int alu_valid;
+	int alu_mstp;
+	int alu_prio;
+	int alu_src;
+	int alu_dst;
+	int alu_ports;
+	int alu_addr;
+	int alu_type;
+	int alu_index;
+	int alu_info;
+
+	int vlan_valid;
+	int vlan_ports;
+	int vlan_untag;
+	int vlan_fid;
+	int vlan_mstp;
+	int vlan_prio;
+	int vlan_option;
+	int vlan_index;
+	int vlan_info;
+	int vid2fid;
+	int fid2mstid;
+
+#ifdef CONFIG_KSZ_STP
+	int stp_br_info;
+	int stp_br_on;
+	int stp_br_prio;
+	int stp_br_fwd_delay;
+	int stp_br_max_age;
+	int stp_br_hello_time;
+	int stp_br_tx_hold;
+	int stp_version;
+#ifdef CONFIG_KSZ_MSTP
+	int stp_br_max_hops;
+	int stp_msti;
+	int stp_msti_vid;
+	int stp_mstp_cfg;
+	int stp_mstp_name;
+#endif
+#endif
+
+#ifdef CONFIG_KSZ_MSRP
+	int msrp_info;
+	int msrpEnabled;
+#endif
+
+#ifdef CONFIG_KSZ_AVB
+	int msrp_sr_a;
+#endif
+
+#ifdef CONFIG_KSZ_HSR
+	int hsr_valid;
+	int hsr_age_cnt;
+	int hsr_path_id;
+	int hsr_addr;
+	int hsr_index;
+	int hsr_info;
+	int hsr_state;
+#endif
+
+	int no_color;
+	int color_red;
+	int color_yellow;
+	int color_green;
+
+	int vlan_filter_dynamic;
+	int vlan_filter_static;
+};
+
+struct sw_attributes {
+	int mib;
+	int vid;
+	int member;
+	int bcast_storm;
+	int diffserv;
+	int p_802_1p;
+	int prio_vlan;
+	int prio_mac;
+	int prio_acl;
+	int prio_highest;
+	int prio_or;
+	int port_prio;
+	int non_vid;
+	int ingress;
+	int drop_non_vlan;
+	int drop_tagged;
+	int replace_vid;
+	int replace_prio;
+	int mac_802_1x;
+	int src_addr_filter;
+	int vlan_lookup_0;
+	int mstp;
+	int rx;
+	int tx;
+	int learn;
+	int power;
+	int prio_queue;
+	int rx_prio_rate;
+	int tx_prio_rate;
+	int limit;
+	int limit_port_based;
+	int limit_packet_based;
+	int limit_flow_ctrl;
+	int limit_cnt_ifg;
+	int limit_cnt_pre;
+	int rx_p0_rate;
+	int rx_p1_rate;
+	int rx_p2_rate;
+	int rx_p3_rate;
+	int rx_p4_rate;
+	int rx_p5_rate;
+	int rx_p6_rate;
+	int rx_p7_rate;
+	int tx_q0_rate;
+	int tx_q1_rate;
+	int tx_q2_rate;
+	int tx_q3_rate;
+	int color_map;
+	int tc_map;
+	int mirror_port;
+	int mirror_rx;
+	int mirror_tx;
+	int back_pressure;
+	int force_flow_ctrl;
+	int pass_all;
+	int tail_tag;
+
+	int cust_vid;
+	int sr_1_vid;
+	int sr_2_vid;
+	int sr_1_type;
+	int sr_2_type;
+
+	int pme_ctrl;
+	int pme_status;
+
+	int authen_mode;
+	int acl;
+	int acl_first_rule;
+	int acl_ruleset;
+	int acl_mode;
+	int acl_enable;
+	int acl_src;
+	int acl_equal;
+	int acl_addr;
+	int acl_type;
+	int acl_cnt;
+	int acl_msec;
+	int acl_intr_mode;
+	int acl_ip_addr;
+	int acl_ip_mask;
+	int acl_protocol;
+	int acl_seqnum;
+	int acl_port_mode;
+	int acl_max_port;
+	int acl_min_port;
+	int acl_tcp_flag_enable;
+	int acl_tcp_flag;
+	int acl_tcp_flag_mask;
+	int acl_prio_mode;
+	int acl_prio;
+	int acl_vlan_prio_replace;
+	int acl_vlan_prio;
+	int acl_map_mode;
+	int acl_ports;
+	int acl_index;
+	int acl_act_index;
+	int acl_act;
+	int acl_rule_index;
+	int acl_info;
+	int acl_table;
+
+	int p_index;
+	int q_index;
+	int police_type;
+	int non_dscp_color;
+	int police_drop_all;
+	int police_port_based;
+	int color_mark;
+	int color_remap;
+	int drop_srp;
+	int color_aware;
+	int police;
+
+	int q_cir;
+	int q_pir;
+	int q_cbs;
+	int q_pbs;
+
+	int wred_max;
+	int wred_min;
+	int wred_multiplier;
+	int wred_avg_size;
+	int wred_q_max;
+	int wred_q_min;
+	int wred_q_multiplier;
+	int wred_q_avg_size;
+	int wred_random_drop;
+	int wred_drop_gyr;
+	int wred_drop_yr;
+	int wred_drop_r;
+	int wred_drop_all;
+	int wred_q_pmon;
+
+	int q_scheduling;
+	int q_shaping;
+#ifdef MTI_PREEMPT_ENABLE
+	int preempt;
+#endif
+	int q_tx_ratio;
+	int q_credit_hi;
+	int q_credit_lo;
+	int q_credit_incr;
+	int srp;
+
+	int qm_drop;
+	int qm_burst;
+	int qm_resv_space;
+	int qm_hi;
+	int qm_lo;
+	int qm_tx_used;
+	int qm_tx_avail;
+	int qm_tx_calc;
+
+	int mmd_id;
+	int mmd_reg;
+	int mmd_val;
+
+	int rx_flow_ctrl;
+	int tx_flow_ctrl;
+
+	int duplex;
+	int speed;
+	int mac_oper;
+	int vlan_restricted;
+	int vlan_untagged;
+
+#ifdef CONFIG_KSZ_STP
+	int stp_info;
+	int stp_on;
+	int stp_prio;
+	int stp_admin_path_cost;
+	int stp_path_cost;
+	int stp_admin_edge;
+	int stp_auto_edge;
+	int stp_mcheck;
+	int stp_admin_p2p;
+	int stp_auto_isolate;
+#endif
+
+#ifdef CONFIG_KSZ_MRP
+	int mmrpEnabled;
+	int mmrp_mac;
+	int mmrp_svc;
+	int mmrp_reg;
+	int mvrpEnabled;
+	int mvrp_vid;
+	int mvrp_reg;
+#endif
+
+#ifdef CONFIG_KSZ_MSRP
+	int msrpEnabled;
+#endif
+
+#ifdef CONFIG_KSZ_AVB
+	int asCapable;
+	int q_delta;
+	int q_admin_mbps;
+	int q_admin_slope;
+	int q_oper_slope;
+	int q_alg;
+	int sr_a_rx_prio;
+	int sr_a_tx_prio;
+	int sr_a_boundary;
+	int sr_a_latency;
+	int sr_b_rx_prio;
+	int sr_b_tx_prio;
+	int sr_b_boundary;
+	int sr_b_latency;
+	int max_frame_size;
+	int max_int_frames;
+	int class_prio;
+#endif
+
+	int linkmd;
+	int sqi;
+	int mac_loopback;
+	int phy_loopback;
+	int remote_loopback;
+};
+
+#endif
diff -Napur linux-linux4microchip-2022.10-orig/drivers/net/ethernet/micrel/ksz_sw_api.h linux-linux4microchip-2022.10/drivers/net/ethernet/micrel/ksz_sw_api.h
--- linux-linux4microchip-2022.10-orig/drivers/net/ethernet/micrel/ksz_sw_api.h	1969-12-31 16:00:00.000000000 -0800
+++ linux-linux4microchip-2022.10/drivers/net/ethernet/micrel/ksz_sw_api.h	2023-04-25 16:13:55.392163494 -0700
@@ -0,0 +1,74 @@
+/**
+ * Microchip switch driver API header
+ *
+ * Copyright (c) 2016 Microchip Technology Inc.
+ *	Tristram Ha <Tristram.Ha@microchip.com>
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ */
+
+
+#ifndef KSZ_SW_API_H
+#define KSZ_SW_API_H
+
+
+enum {
+	DEV_INFO_SW_LINK = DEV_INFO_LAST,
+	DEV_INFO_SW_ACL,
+};
+
+enum {
+	DEV_SW_CFG,
+};
+
+
+#define SW_INFO_LINK_CHANGE		(1 << 0)
+#define SW_INFO_ACL_INTR		(1 << 1)
+
+
+#define SP_RX				(1 << 0)
+#define SP_TX				(1 << 1)
+#define SP_LEARN			(1 << 2)
+#define SP_MIRROR_RX			(1 << 3)
+#define SP_MIRROR_TX			(1 << 4)
+#define SP_MIRROR_SNIFFER		(1 << 5)
+#define SP_PHY_POWER			(1 << 6)
+
+#define SP_BCAST_STORM			(1 << 16)
+#define SP_DIFFSERV			(1 << 17)
+#define SP_802_1P			(1 << 18)
+
+struct ksz_info_cfg {
+	uint set;
+	uint on_off;
+} __packed;
+
+#ifndef TX_RATE_UNIT
+#define TX_RATE_UNIT			10000
+#endif
+
+struct ksz_info_speed {
+	uint tx_rate;
+	u8 duplex;
+	u8 flow_ctrl;
+} __packed;
+
+union ksz_info_data {
+	struct ksz_info_cfg cfg;
+	struct ksz_info_speed speed;
+} __packed;
+
+struct ksz_info_opt {
+	u8 num;
+	u8 port;
+	union ksz_info_data data;
+} __packed;
+
+#endif
diff -Napur linux-linux4microchip-2022.10-orig/drivers/net/ethernet/micrel/ksz_sw.c linux-linux4microchip-2022.10/drivers/net/ethernet/micrel/ksz_sw.c
--- linux-linux4microchip-2022.10-orig/drivers/net/ethernet/micrel/ksz_sw.c	1969-12-31 16:00:00.000000000 -0800
+++ linux-linux4microchip-2022.10/drivers/net/ethernet/micrel/ksz_sw.c	2024-04-26 16:31:45.219629739 -0700
@@ -0,0 +1,10231 @@
+/**
+ * Microchip switch common code
+ *
+ * Copyright (c) 2015-2024 Microchip Technology Inc.
+ *	Tristram Ha <Tristram.Ha@microchip.com>
+ *
+ * Copyright (c) 2010-2015 Micrel, Inc.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ */
+
+
+/* -------------------------------------------------------------------------- */
+
+#define MAX_SYSFS_BUF_SIZE		(4080 - 80)
+
+enum {
+	PROC_SW_INFO,
+	PROC_SW_VERSION,
+
+	PROC_SET_SW_DUPLEX,
+	PROC_SET_SW_SPEED,
+	PROC_SET_SW_FORCE,
+	PROC_SET_SW_FLOW_CTRL,
+
+	PROC_SET_SW_FEATURES,
+	PROC_SET_SW_OVERRIDES,
+	PROC_SET_SW_MIB,
+
+	PROC_SET_SW_REG,
+	PROC_SET_SW_VID,
+
+	PROC_DYNAMIC,
+	PROC_STATIC,
+	PROC_VLAN,
+
+	PROC_SET_AGING,
+	PROC_SET_FAST_AGING,
+	PROC_SET_LINK_AGING,
+
+	PROC_SET_BROADCAST_STORM,
+	PROC_SET_MULTICAST_STORM,
+	PROC_SET_DIFFSERV,
+	PROC_SET_802_1P,
+
+	PROC_ENABLE_VLAN,
+	PROC_SET_REPLACE_NULL_VID,
+	PROC_SET_MAC_ADDR,
+	PROC_SET_MIRROR_MODE,
+	PROC_SET_TAIL_TAG,
+
+	PROC_SET_IGMP_SNOOP,
+	PROC_SET_IPV6_MLD_SNOOP,
+	PROC_SET_IPV6_MLD_OPTION,
+
+	PROC_SET_AGGR_BACKOFF,
+	PROC_SET_NO_EXC_DROP,
+	PROC_SET_BUFFER_RESERVE,
+
+	PROC_SET_HUGE_PACKET,
+	PROC_SET_LEGAL_PACKET,
+	PROC_SET_LENGTH_CHECK,
+
+	PROC_SET_BACK_PRESSURE_MODE,
+	PROC_SET_SWITCH_FLOW_CTRL,
+	PROC_SET_SWITCH_HALF_DUPLEX,
+	PROC_SET_SWITCH_10_MBIT,
+
+	PROC_SET_RX_FLOW_CTRL,
+	PROC_SET_TX_FLOW_CTRL,
+	PROC_SET_FAIR_FLOW_CTRL,
+	PROC_SET_VLAN_BOUNDARY,
+
+	PROC_SET_FORWARD_UNKNOWN_DEST,
+	PROC_SET_INS_TAG_0_1,
+	PROC_SET_INS_TAG_0_2,
+	PROC_SET_INS_TAG_1_0,
+	PROC_SET_INS_TAG_1_2,
+	PROC_SET_INS_TAG_2_0,
+	PROC_SET_INS_TAG_2_1,
+
+	PROC_SET_PASS_ALL,
+	PROC_SET_PASS_PAUSE,
+
+	PROC_SET_PHY_ADDR,
+
+	PROC_GET_PORTS,
+	PROC_GET_DEV_START,
+	PROC_GET_VLAN_START,
+	PROC_GET_STP,
+
+#ifdef CONFIG_KSZ_STP
+	PROC_GET_STP_BR_INFO,
+	PROC_SET_STP_BR_ON,
+	PROC_SET_STP_BR_PRIO,
+	PROC_SET_STP_BR_FWD_DELAY,
+	PROC_SET_STP_BR_MAX_AGE,
+	PROC_SET_STP_BR_HELLO_TIME,
+	PROC_SET_STP_BR_TX_HOLD,
+	PROC_SET_STP_VERSION,
+#endif
+};
+
+enum {
+	PROC_SET_PORT_MIB,
+
+	PROC_SET_DEF_VID,
+	PROC_SET_MEMBER,
+
+	PROC_ENABLE_BROADCAST_STORM,
+	PROC_ENABLE_DIFFSERV,
+	PROC_ENABLE_802_1P,
+
+	PROC_SET_PORT_BASED,
+
+	PROC_SET_DIS_NON_VID,
+	PROC_SET_INGRESS,
+	PROC_SET_INSERT_TAG,
+	PROC_SET_REMOVE_TAG,
+	PROC_SET_DOUBLE_TAG,
+	PROC_SET_DROP_TAG,
+	PROC_SET_REPLACE_PRIO,
+
+	PROC_SET_RX,
+	PROC_SET_TX,
+	PROC_SET_LEARN,
+
+	PROC_ENABLE_PRIO_QUEUE,
+	PROC_SET_TX_P0_CTRL,
+	PROC_SET_TX_P1_CTRL,
+	PROC_SET_TX_P2_CTRL,
+	PROC_SET_TX_P3_CTRL,
+	PROC_SET_TX_P0_RATIO,
+	PROC_SET_TX_P1_RATIO,
+	PROC_SET_TX_P2_RATIO,
+	PROC_SET_TX_P3_RATIO,
+
+	PROC_ENABLE_RX_PRIO_RATE,
+	PROC_ENABLE_TX_PRIO_RATE,
+	PROC_SET_RX_LIMIT,
+	PROC_SET_CNT_IFG,
+	PROC_SET_CNT_PRE,
+	PROC_SET_RX_P0_RATE,
+	PROC_SET_RX_P1_RATE,
+	PROC_SET_RX_P2_RATE,
+	PROC_SET_RX_P3_RATE,
+	PROC_SET_TX_P0_RATE,
+	PROC_SET_TX_P1_RATE,
+	PROC_SET_TX_P2_RATE,
+	PROC_SET_TX_P3_RATE,
+
+	PROC_SET_MIRROR_PORT,
+	PROC_SET_MIRROR_RX,
+	PROC_SET_MIRROR_TX,
+
+	PROC_SET_BACK_PRESSURE,
+	PROC_SET_FORCE_FLOW_CTRL,
+
+	PROC_SET_UNKNOWN_DEF_PORT,
+	PROC_SET_FORWARD_INVALID_VID,
+
+	PROC_SET_PORT_DUPLEX,
+	PROC_SET_PORT_SPEED,
+	PROC_SET_LINK_MD,
+
+	PROC_SET_PORT_MAC_ADDR,
+	PROC_SET_SRC_FILTER_0,
+	PROC_SET_SRC_FILTER_1,
+
+#ifdef CONFIG_KSZ_STP
+	PROC_GET_STP_INFO,
+	PROC_SET_STP_ON,
+	PROC_SET_STP_PRIO,
+	PROC_SET_STP_ADMIN_PATH_COST,
+	PROC_SET_STP_PATH_COST,
+	PROC_SET_STP_ADMIN_EDGE,
+	PROC_SET_STP_AUTO_EDGE,
+	PROC_SET_STP_MCHECK,
+	PROC_SET_STP_ADMIN_P2P,
+#endif
+};
+
+enum {
+	PROC_SET_STATIC_FID,
+	PROC_SET_STATIC_USE_FID,
+	PROC_SET_STATIC_OVERRIDE,
+	PROC_SET_STATIC_VALID,
+	PROC_SET_STATIC_PORTS,
+	PROC_SET_STATIC_MAC_ADDR,
+};
+
+enum {
+	PROC_SET_VLAN_VALID,
+	PROC_SET_VLAN_MEMBER,
+	PROC_SET_VLAN_FID,
+	PROC_SET_VLAN_VID,
+};
+
+/* -------------------------------------------------------------------------- */
+
+static uint get_phy_port(struct ksz_sw *sw, uint n)
+{
+if (n > sw->mib_port_cnt + 1)
+dbg_msg("  !!! %s %d\n", __func__, n);
+	if (n >= sw->mib_port_cnt + 1)
+		n = 0;
+	return sw->port_info[n].phy_p;
+}
+
+static uint get_log_port(struct ksz_sw *sw, uint p)
+{
+if (p >= sw->port_cnt)
+dbg_msg("  !!! %s %d\n", __func__, p);
+if (!sw->port_info[p].log_m)
+dbg_msg("  ??? %s %d\n", __func__, p);
+	return sw->port_info[p].log_p;
+}
+
+#if 0
+static uint get_log_port_zero(struct ksz_sw *sw, uint p)
+{
+	uint n;
+
+	n = get_log_port(sw, p);
+	if (n)
+		n--;
+	else
+		n = sw->mib_port_cnt;
+	return n;
+}
+#endif
+
+#if 0
+static uint get_phy_mask_from_log(struct ksz_sw *sw, uint log_m)
+{
+	struct ksz_port_info *info;
+	uint n;
+	uint p;
+	uint phy_m = 0;
+
+	for (n = 0; n <= sw->mib_port_cnt; n++) {
+		info = &sw->port_info[n];
+		p = info->phy_p;
+		if (log_m & sw->port_info[p].log_m)
+			phy_m |= info->phy_m;
+	}
+	return phy_m;
+}
+
+static uint get_log_mask_from_phy(struct ksz_sw *sw, uint phy_m)
+{
+	struct ksz_port_info *info;
+	uint n;
+	uint p;
+	uint log_m = 0;
+
+	for (n = 0; n <= sw->mib_port_cnt; n++) {
+		info = &sw->port_info[n];
+		p = info->phy_p;
+		if (phy_m & info->phy_m)
+			log_m |= sw->port_info[p].log_m;
+	}
+	return log_m;
+}
+#endif
+
+static uint get_sysfs_port(struct ksz_sw *sw, uint n)
+{
+	return n;
+}
+
+static inline struct ksz_port_cfg *get_port_cfg(struct ksz_sw *sw, uint p)
+{
+	return &sw->info->port_cfg[p];
+}
+
+static inline struct ksz_port_info *get_port_info(struct ksz_sw *sw, uint p)
+{
+	return &sw->port_info[p];
+}
+
+static inline struct ksz_port_mib *get_port_mib(struct ksz_sw *sw, uint p)
+{
+	return &sw->port_mib[p];
+}
+
+static void sw_acquire(struct ksz_sw *sw)
+{
+	mutex_lock(&sw->lock);
+	mutex_lock(sw->reglock);
+}  /* sw_acquire */
+
+static void sw_release(struct ksz_sw *sw)
+{
+	mutex_unlock(sw->reglock);
+	mutex_unlock(&sw->lock);
+}  /* sw_release */
+
+/* -------------------------------------------------------------------------- */
+
+/*
+#define STATIC_MAC_TABLE_ADDR		00-0000FFFF-FFFFFFFF
+#define STATIC_MAC_TABLE_FWD_PORTS	00-00070000-00000000
+#define STATIC_MAC_TABLE_VALID		00-00080000-00000000
+#define STATIC_MAC_TABLE_OVERRIDE	00-00100000-00000000
+#define STATIC_MAC_TABLE_USE_FID	00-00200000-00000000
+#define STATIC_MAC_TABLE_FID		00-03C00000-00000000
+*/
+
+#define STATIC_MAC_TABLE_ADDR		0x0000FFFF
+#define STATIC_MAC_TABLE_FWD_PORTS	0x00070000
+#define STATIC_MAC_TABLE_VALID		0x00080000
+#define STATIC_MAC_TABLE_OVERRIDE	0x00100000
+#define STATIC_MAC_TABLE_USE_FID	0x00200000
+#define STATIC_MAC_TABLE_FID		0x03C00000
+
+#define STATIC_MAC_FWD_PORTS_SHIFT	16
+#define STATIC_MAC_FID_SHIFT		22
+
+/*
+#define VLAN_TABLE_VID			00-00000000-00000FFF
+#define VLAN_TABLE_FID			00-00000000-0000F000
+#define VLAN_TABLE_MEMBERSHIP		00-00000000-00070000
+#define VLAN_TABLE_VALID		00-00000000-00080000
+*/
+
+#define VLAN_TABLE_VID			0x00000FFF
+#define VLAN_TABLE_FID			0x0000F000
+#define VLAN_TABLE_MEMBERSHIP		0x00070000
+#define VLAN_TABLE_VALID		0x00080000
+
+#define VLAN_TABLE_FID_SHIFT		12
+#define VLAN_TABLE_MEMBERSHIP_SHIFT	16
+
+/*
+#define DYNAMIC_MAC_TABLE_ADDR		00-0000FFFF-FFFFFFFF
+#define DYNAMIC_MAC_TABLE_FID		00-000F0000-00000000
+#define DYNAMIC_MAC_TABLE_SRC_PORT	00-00300000-00000000
+#define DYNAMIC_MAC_TABLE_TIMESTAMP	00-00C00000-00000000
+#define DYNAMIC_MAC_TABLE_ENTRIES	03-FF000000-00000000
+#define DYNAMIC_MAC_TABLE_MAC_EMPTY	04-00000000-00000000
+#define DYNAMIC_MAC_TABLE_RESERVED	78-00000000-00000000
+#define DYNAMIC_MAC_TABLE_NOT_READY	80-00000000-00000000
+*/
+
+#define DYNAMIC_MAC_TABLE_ADDR		0x0000FFFF
+#define DYNAMIC_MAC_TABLE_FID		0x000F0000
+#define DYNAMIC_MAC_TABLE_SRC_PORT	0x00300000
+#define DYNAMIC_MAC_TABLE_TIMESTAMP	0x00C00000
+#define DYNAMIC_MAC_TABLE_ENTRIES	0xFF000000
+
+#define DYNAMIC_MAC_TABLE_ENTRIES_H	0x03
+#define DYNAMIC_MAC_TABLE_MAC_EMPTY	0x04
+#define DYNAMIC_MAC_TABLE_RESERVED	0x78
+#define DYNAMIC_MAC_TABLE_NOT_READY	0x80
+
+#define DYNAMIC_MAC_FID_SHIFT		16
+#define DYNAMIC_MAC_SRC_PORT_SHIFT	20
+#define DYNAMIC_MAC_TIMESTAMP_SHIFT	22
+#define DYNAMIC_MAC_ENTRIES_SHIFT	24
+#define DYNAMIC_MAC_ENTRIES_H_SHIFT	8
+
+/*
+#define MIB_COUNTER_VALUE		00-00000000-3FFFFFFF
+#define MIB_COUNTER_VALID		00-00000000-40000000
+#define MIB_COUNTER_OVERFLOW		00-00000000-80000000
+*/
+
+#ifndef MIB_COUNTER_OVERFLOW
+#define MIB_COUNTER_OVERFLOW		(1 << 31)
+#define MIB_COUNTER_VALID		(1 << 30)
+#define MIB_COUNTER_VALUE		0x3FFFFFFF
+#endif
+
+#define KS_MIB_PACKET_DROPPED_TX_0	0x100
+#define KS_MIB_PACKET_DROPPED_TX_1	0x101
+#define KS_MIB_PACKET_DROPPED_TX	0x102
+#define KS_MIB_PACKET_DROPPED_RX_0	0x103
+#define KS_MIB_PACKET_DROPPED_RX_1	0x104
+#define KS_MIB_PACKET_DROPPED_RX	0x105
+
+#define MIB_PACKET_DROPPED		0x0000FFFF
+
+#ifdef CONFIG_1588_PTP
+#include "ksz_ptp.c"
+#endif
+
+/* -------------------------------------------------------------------------- */
+
+/* Switch functions */
+
+#define HW_DELAY(sw, reg)			\
+	do {					\
+		u16 dummy;			\
+		dummy = SW_R(sw, reg);		\
+	} while (0)
+
+/**
+ * sw_r_table - read 4 bytes of data from switch table
+ * @sw:		The switch instance.
+ * @table:	The table selector.
+ * @addr:	The address of the table entry.
+ * @data:	Buffer to store the read data.
+ *
+ * This routine reads 4 bytes of data from the table of the switch.
+ * Hardware is locked to minimize corruption of read data.
+ */
+static void sw_r_table(struct ksz_sw *sw, int table, u16 addr, u32 *data)
+{
+	u16 ctrl_addr;
+
+	ctrl_addr = IND_ACC_TABLE(table | TABLE_READ) | addr;
+
+	mutex_lock(sw->reglock);
+	sw->reg->w16(sw, REG_IND_CTRL_0, ctrl_addr);
+	HW_DELAY(sw, REG_IND_CTRL_0);
+	*data = sw->reg->r32(sw, REG_IND_DATA_LO);
+	mutex_unlock(sw->reglock);
+}
+
+/**
+ * sw_w_table - write 4 bytes of data to the switch table
+ * @sw:		The switch instance.
+ * @table:	The table selector.
+ * @addr:	The address of the table entry.
+ * @data:	Data to be written.
+ *
+ * This routine writes 4 bytes of data to the table of the switch.
+ * Hardware is locked to minimize corruption of written data.
+ */
+static void sw_w_table(struct ksz_sw *sw, int table, u16 addr, u32 data)
+{
+	u16 ctrl_addr;
+
+	ctrl_addr = IND_ACC_TABLE(table) | addr;
+
+	mutex_lock(sw->reglock);
+	sw->reg->w32(sw, REG_IND_DATA_LO, data);
+	sw->reg->w16(sw, REG_IND_CTRL_0, ctrl_addr);
+	HW_DELAY(sw, REG_IND_CTRL_0);
+	mutex_unlock(sw->reglock);
+}
+
+/**
+ * sw_r_table_64 - read 8 bytes of data from the switch table
+ * @sw:		The switch instance.
+ * @table:	The table selector.
+ * @addr:	The address of the table entry.
+ * @data_hi:	Buffer to store the high part of read data (bit63 ~ bit32).
+ * @data_lo:	Buffer to store the low part of read data (bit31 ~ bit0).
+ *
+ * This routine reads 8 bytes of data from the table of the switch.
+ * Hardware is locked to minimize corruption of read data.
+ */
+static void sw_r_table_64(struct ksz_sw *sw, int table, u16 addr, u32 *data_hi,
+	u32 *data_lo)
+{
+	u16 ctrl_addr;
+
+	ctrl_addr = IND_ACC_TABLE(table | TABLE_READ) | addr;
+
+	mutex_lock(sw->reglock);
+	sw->reg->w16(sw, REG_IND_CTRL_0, ctrl_addr);
+	HW_DELAY(sw, REG_IND_CTRL_0);
+	*data_hi = sw->reg->r32(sw, REG_IND_DATA_HI);
+	*data_lo = sw->reg->r32(sw, REG_IND_DATA_LO);
+	mutex_unlock(sw->reglock);
+}
+
+/**
+ * sw_w_table_64 - write 8 bytes of data to the switch table
+ * @sw:		The switch instance.
+ * @table:	The table selector.
+ * @addr:	The address of the table entry.
+ * @data_hi:	The high part of data to be written (bit63 ~ bit32).
+ * @data_lo:	The low part of data to be written (bit31 ~ bit0).
+ *
+ * This routine writes 8 bytes of data to the table of the switch.
+ * Hardware is locked to minimize corruption of written data.
+ */
+static void sw_w_table_64(struct ksz_sw *sw, int table, u16 addr, u32 data_hi,
+	u32 data_lo)
+{
+	u16 ctrl_addr;
+
+	ctrl_addr = IND_ACC_TABLE(table) | addr;
+
+	mutex_lock(sw->reglock);
+	sw->reg->w32(sw, REG_IND_DATA_HI, data_hi);
+	sw->reg->w32(sw, REG_IND_DATA_LO, data_lo);
+	sw->reg->w16(sw, REG_IND_CTRL_0, ctrl_addr);
+	HW_DELAY(sw, REG_IND_CTRL_0);
+	mutex_unlock(sw->reglock);
+}
+
+static inline int valid_dyn_entry(struct ksz_sw *sw, u8 *data)
+{
+	int timeout = 100;
+
+	do {
+		*data = sw->reg->r8(sw, REG_IND_DATA_CHECK);
+		timeout--;
+	} while ((*data & DYNAMIC_MAC_TABLE_NOT_READY) && timeout);
+
+	/* Entry is not ready for accessing. */
+	if (*data & DYNAMIC_MAC_TABLE_NOT_READY)
+		return 1;
+
+	/* Entry is ready for accessing. */
+	else {
+		/* There is no valid entry in the table. */
+		if (*data & DYNAMIC_MAC_TABLE_MAC_EMPTY)
+			return 2;
+	}
+	return 0;
+}
+
+/**
+ * sw_r_dyn_mac_table - read from dynamic MAC table
+ * @sw:		The switch instance.
+ * @addr:	The address of the table entry.
+ * @mac_addr:	Buffer to store the MAC address.
+ * @fid:	Buffer to store the FID.
+ * @src_port:	Buffer to store the source port number.
+ * @timestamp:	Buffer to store the timestamp.
+ * @entries:	Buffer to store the number of entries.  If this is zero, the
+ *		table is empty and so this function should not be called again
+ *		until later.
+ *
+ * This function reads an entry of the dynamic MAC table of the switch.
+ * Hardware is locked to minimize corruption of read data.
+ *
+ * Return 0 if the entry is successfully read; otherwise -1.
+ */
+static int sw_r_dyn_mac_table(struct ksz_sw *sw, u16 addr, u8 *mac_addr,
+	u8 *fid, u8 *src_port, u8 *timestamp, u16 *entries)
+{
+	u32 data_hi;
+	u32 data_lo;
+	u16 ctrl_addr;
+	int rc;
+	u8 data;
+
+	ctrl_addr = IND_ACC_TABLE(TABLE_DYNAMIC_MAC | TABLE_READ) | addr;
+
+	mutex_lock(sw->reglock);
+	sw->reg->w16(sw, REG_IND_CTRL_0, ctrl_addr);
+	HW_DELAY(sw, REG_IND_CTRL_0);
+
+	rc = valid_dyn_entry(sw, &data);
+	if (1 == rc) {
+		if (0 == addr)
+			*entries = 0;
+	} else if (2 == rc)
+		*entries = 0;
+	/* At least one valid entry in the table. */
+	else {
+		data_hi = sw->reg->r32(sw, REG_IND_DATA_HI);
+
+		/* Check out how many valid entry in the table. */
+		*entries = (u16)(((((u16)
+			data & DYNAMIC_MAC_TABLE_ENTRIES_H) <<
+			DYNAMIC_MAC_ENTRIES_H_SHIFT) |
+			(((data_hi & DYNAMIC_MAC_TABLE_ENTRIES) >>
+			DYNAMIC_MAC_ENTRIES_SHIFT))) + 1);
+
+		*fid = (u8)((data_hi & DYNAMIC_MAC_TABLE_FID) >>
+			DYNAMIC_MAC_FID_SHIFT);
+		*src_port = (u8)((data_hi & DYNAMIC_MAC_TABLE_SRC_PORT) >>
+			DYNAMIC_MAC_SRC_PORT_SHIFT);
+		*timestamp = (u8)((
+			data_hi & DYNAMIC_MAC_TABLE_TIMESTAMP) >>
+			DYNAMIC_MAC_TIMESTAMP_SHIFT);
+
+		data_lo = sw->reg->r32(sw, REG_IND_DATA_LO);
+
+		mac_addr[5] = (u8) data_lo;
+		mac_addr[4] = (u8)(data_lo >> 8);
+		mac_addr[3] = (u8)(data_lo >> 16);
+		mac_addr[2] = (u8)(data_lo >> 24);
+
+		mac_addr[1] = (u8) data_hi;
+		mac_addr[0] = (u8)(data_hi >> 8);
+		rc = 0;
+	}
+	mutex_unlock(sw->reglock);
+
+	return rc;
+}
+
+static ssize_t sw_d_dyn_mac_table(struct ksz_sw *sw, char *buf, ssize_t len)
+{
+	u16 entries = 0;
+	u16 i;
+	u8 mac_addr[ETH_ALEN];
+	char str[80];
+	u8 port = 0;
+	u8 timestamp = 0;
+	u8 fid = 0;
+	int first_break = true;
+
+	memset(mac_addr, 0, ETH_ALEN);
+	i = 0;
+	do {
+		if (!sw_r_dyn_mac_table(sw, i, mac_addr, &fid, &port,
+				&timestamp, &entries)) {
+			if (len >= MAX_SYSFS_BUF_SIZE && first_break) {
+				first_break = false;
+				len += sprintf(buf + len, "...\n");
+			}
+			sprintf(str,
+				"%02X:%02X:%02X:%02X:%02X:%02X"
+				"  f:%x  p:%x  t:%x  %03x\n",
+				mac_addr[0], mac_addr[1], mac_addr[2],
+				mac_addr[3], mac_addr[4], mac_addr[5],
+				fid, port, timestamp, entries);
+			if (len < MAX_SYSFS_BUF_SIZE)
+				len += sprintf(buf + len, "%s", str);
+			else
+				printk(KERN_INFO "%s", str);
+		}
+		i++;
+	} while (i < entries);
+	return len;
+}
+
+/**
+ * sw_r_sta_mac_table - read from static MAC table
+ * @sw:		The switch instance.
+ * @addr:	The address of the table entry.
+ * @mac_addr:	Buffer to store the MAC address.
+ * @ports:	Buffer to store the port members.
+ * @override:	Buffer to store the override flag.
+ * @use_fid:	Buffer to store the use FID flag which indicates the FID is
+ *		valid.
+ * @fid:	Buffer to store the FID.
+ *
+ * This function reads an entry of the static MAC table of the switch.  It
+ * calls sw_r_table_64() to get the data.
+ *
+ * Return 0 if the entry is valid; otherwise -1.
+ */
+static int sw_r_sta_mac_table(struct ksz_sw *sw, u16 addr, u8 *mac_addr,
+	u8 *ports, int *override, int *use_fid, u8 *fid)
+{
+	u32 data_hi;
+	u32 data_lo;
+
+	sw_r_table_64(sw, TABLE_STATIC_MAC, addr, &data_hi, &data_lo);
+	if (data_hi & (STATIC_MAC_TABLE_VALID | STATIC_MAC_TABLE_OVERRIDE)) {
+		mac_addr[5] = (u8) data_lo;
+		mac_addr[4] = (u8)(data_lo >> 8);
+		mac_addr[3] = (u8)(data_lo >> 16);
+		mac_addr[2] = (u8)(data_lo >> 24);
+		mac_addr[1] = (u8) data_hi;
+		mac_addr[0] = (u8)(data_hi >> 8);
+		*ports = (u8)((data_hi & STATIC_MAC_TABLE_FWD_PORTS) >>
+			STATIC_MAC_FWD_PORTS_SHIFT);
+		*override = (data_hi & STATIC_MAC_TABLE_OVERRIDE) ? 1 : 0;
+		*use_fid = (data_hi & STATIC_MAC_TABLE_USE_FID) ? 1 : 0;
+		*fid = (u8)((data_hi & STATIC_MAC_TABLE_FID) >>
+			STATIC_MAC_FID_SHIFT);
+		return 0;
+	}
+	return -1;
+}
+
+/**
+ * sw_w_sta_mac_table - write to the static MAC table
+ * @sw:		The switch instance.
+ * @addr:	The address of the table entry.
+ * @mac_addr:	The MAC address.
+ * @ports:	The port members.
+ * @override:	The flag to override the port receive/transmit settings.
+ * @valid:	The flag to indicate entry is valid.
+ * @use_fid:	The flag to indicate the FID is valid.
+ * @fid:	The FID value.
+ *
+ * This routine writes an entry of the static MAC table of the switch.  It
+ * calls sw_w_table_64() to write the data.
+ */
+static void sw_w_sta_mac_table(struct ksz_sw *sw, u16 addr, u8 *mac_addr,
+	u8 ports, int override, int valid, int use_fid, u8 fid)
+{
+	u32 data_hi;
+	u32 data_lo;
+
+	data_lo = ((u32) mac_addr[2] << 24) |
+		((u32) mac_addr[3] << 16) |
+		((u32) mac_addr[4] << 8) | mac_addr[5];
+	data_hi = ((u32) mac_addr[0] << 8) | mac_addr[1];
+	data_hi |= (u32) ports << STATIC_MAC_FWD_PORTS_SHIFT;
+
+	if (override)
+		data_hi |= STATIC_MAC_TABLE_OVERRIDE;
+	if (use_fid) {
+		data_hi |= STATIC_MAC_TABLE_USE_FID;
+		data_hi |= (u32) fid << STATIC_MAC_FID_SHIFT;
+	}
+	if (valid)
+		data_hi |= STATIC_MAC_TABLE_VALID;
+	else
+		data_hi &= ~STATIC_MAC_TABLE_OVERRIDE;
+
+	sw_w_table_64(sw, TABLE_STATIC_MAC, addr, data_hi, data_lo);
+}
+
+static ssize_t sw_d_sta_mac_table(struct ksz_sw *sw, char *buf, ssize_t len)
+{
+	u16 i;
+	u8 mac_addr[ETH_ALEN];
+	u8 ports;
+	int override;
+	int use_fid;
+	u8 fid;
+
+	i = 0;
+	do {
+		if (!sw_r_sta_mac_table(sw, i, mac_addr, &ports, &override,
+				&use_fid, &fid)) {
+			len += sprintf(buf + len,
+				"%d: %02X:%02X:%02X:%02X:%02X:%02X "
+				"%x %u %u:%x\n",
+				i, mac_addr[0], mac_addr[1], mac_addr[2],
+				mac_addr[3], mac_addr[4], mac_addr[5],
+				ports, override, use_fid, fid);
+		}
+		i++;
+	} while (i < STATIC_MAC_TABLE_ENTRIES);
+	return len;
+}
+
+static ssize_t sw_d_mac_table(struct ksz_sw *sw, char *buf, ssize_t len)
+{
+	struct ksz_mac_table *entry;
+	struct ksz_alu_table *alu;
+	int i;
+	int first_static = true;
+
+	i = STATIC_MAC_TABLE_ENTRIES;
+	do {
+		entry = &sw->info->mac_table[i];
+		if (entry->valid) {
+			if (first_static) {
+				first_static = false;
+				len += sprintf(buf + len, "\n");
+			}
+			alu = &sw->info->alu_table[i];
+			len += sprintf(buf + len,
+				"%2x: %02X:%02X:%02X:%02X:%02X:%02X "
+				"%x %u %u:%x  %02x:%x\n",
+				i, entry->mac_addr[0], entry->mac_addr[1],
+				entry->mac_addr[2], entry->mac_addr[3],
+				entry->mac_addr[4], entry->mac_addr[5],
+				entry->ports, entry->override, entry->use_fid,
+				entry->fid,
+				alu->forward, alu->owner);
+		}
+		i++;
+		if (SWITCH_MAC_TABLE_ENTRIES == i)
+			first_static = true;
+	} while (i < MULTI_MAC_TABLE_ENTRIES);
+	return len;
+}
+
+/* -------------------------------------------------------------------------- */
+
+/**
+ * sw_r_vlan_table - read from the VLAN table
+ * @sw:		The switch instance.
+ * @addr:	The address of the table entry.
+ * @vid:	Buffer to store the VID.
+ * @fid:	Buffer to store the VID.
+ * @member:	Buffer to store the port membership.
+ *
+ * This function reads an entry of the VLAN table of the switch.  It calls
+ * sw_r_table() to get the data.
+ *
+ * Return 0 if the entry is valid; otherwise -1.
+ */
+static int sw_r_vlan_table(struct ksz_sw *sw, u16 addr, u16 *vid, u8 *fid,
+	u8 *member)
+{
+	u32 data;
+
+	sw_r_table(sw, TABLE_VLAN, addr, &data);
+	if (data & VLAN_TABLE_VALID) {
+		*vid = (u16)(data & VLAN_TABLE_VID);
+		*fid = (u8)((data & VLAN_TABLE_FID) >> VLAN_TABLE_FID_SHIFT);
+		*member = (u8)((data & VLAN_TABLE_MEMBERSHIP) >>
+			VLAN_TABLE_MEMBERSHIP_SHIFT);
+		return 0;
+	}
+	return -1;
+}
+
+/**
+ * sw_w_vlan_table - write to the VLAN table
+ * @sw:		The switch instance.
+ * @addr:	The address of the table entry.
+ * @vid:	The VID value.
+ * @fid:	The FID value.
+ * @member:	The port membership.
+ * @valid:	The flag to indicate entry is valid.
+ *
+ * This routine writes an entry of the VLAN table of the switch.  It calls
+ * sw_w_table() to write the data.
+ */
+static void sw_w_vlan_table(struct ksz_sw *sw, u16 addr, u16 vid, u8 fid,
+	u8 member, int valid)
+{
+	u32 data;
+	int entry;
+	struct ksz_sw_info *info = sw->info;
+
+	data = vid;
+	data |= (u32) fid << VLAN_TABLE_FID_SHIFT;
+	data |= (u32) member << VLAN_TABLE_MEMBERSHIP_SHIFT;
+	if (valid)
+		data |= VLAN_TABLE_VALID;
+
+	sw_w_table(sw, TABLE_VLAN, addr, data);
+
+	entry = addr;
+	if (entry >= VLAN_TABLE_ENTRIES)
+		return;
+	info->vlan_table[entry].vid = vid;
+	info->vlan_table[entry].fid = fid;
+	info->vlan_table[entry].member = member;
+	info->vlan_table[entry].valid = valid;
+}
+
+static ssize_t sw_d_vlan_table(struct ksz_sw *sw, char *buf, ssize_t len)
+{
+	u16 i;
+	u16 vid;
+	u8 fid;
+	u8 member;
+	struct ksz_sw_info *info = sw->info;
+
+	i = 0;
+	do {
+		if (!sw_r_vlan_table(sw, i, &vid, &fid, &member)) {
+			info->vlan_table[i].vid = vid;
+			info->vlan_table[i].fid = fid;
+			info->vlan_table[i].member = member;
+			info->vlan_table[i].valid = 1;
+			len += sprintf(buf + len,
+				"%x: 0x%03x %x %x\n", i, vid, fid, member);
+		} else
+			info->vlan_table[i].valid = 0;
+		i++;
+	} while (i < VLAN_TABLE_ENTRIES);
+	return len;
+}
+
+#ifdef CONFIG_MRP
+#include "ksz_mrp.c"
+#endif
+
+/* -------------------------------------------------------------------------- */
+
+/*
+ * Some counters do not need to be read too often because they are less likely
+ * to increase much.
+ */
+static u8 mib_read_max[SWITCH_COUNTER_NUM] = {
+	1,
+	1,
+	4,
+	4,
+	4,
+	4,
+	4,
+	4,
+	4,
+	4,
+	1,
+	1,
+	1,
+	1,
+	2,
+	2,
+	2,
+	2,
+	2,
+	2,
+
+	1,
+	1,
+	4,
+	1,
+	1,
+	1,
+	1,
+	4,
+	4,
+	4,
+	4,
+	4,
+};
+
+/**
+ * port_r_mib_cnt - read MIB counter
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ * @addr:	The address of the counter.
+ * @cnt:	Buffer to store the counter.
+ *
+ * This routine reads a MIB counter of the port.
+ * Hardware is locked to minimize corruption of read data.
+ */
+static void port_r_mib_cnt(struct ksz_sw *sw, uint port, u16 addr, u64 *cnt)
+{
+	u32 data;
+	u16 ctrl_addr;
+	int timeout;
+
+	ctrl_addr = addr + SWITCH_COUNTER_NUM * port;
+
+	mutex_lock(sw->reglock);
+
+	ctrl_addr |= IND_ACC_TABLE(TABLE_MIB | TABLE_READ);
+	sw->reg->w16(sw, REG_IND_CTRL_0, ctrl_addr);
+	HW_DELAY(sw, REG_IND_CTRL_0);
+
+	for (timeout = 2; timeout > 0; timeout--) {
+		data = sw->reg->r32(sw, REG_IND_DATA_LO);
+
+if (data & 0x3f000000)
+dbg_msg(" mib: %d=%04x %08x\n", port, ctrl_addr, data);
+		if (data & MIB_COUNTER_VALID) {
+			if (data & MIB_COUNTER_OVERFLOW)
+dbg_msg(" overflow: %d %x\n", addr, ctrl_addr);
+			if (data & MIB_COUNTER_OVERFLOW)
+				*cnt += MIB_COUNTER_VALUE + 1;
+			*cnt += data & MIB_COUNTER_VALUE;
+			break;
+		}
+	}
+
+	mutex_unlock(sw->reglock);
+}  /* port_r_mib_cnt */
+
+/**
+ * port_r_mib_pkt - read dropped packet counts
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ * @cnt:	Buffer to store the receive and transmit dropped packet counts.
+ *
+ * This routine reads the dropped packet counts of the port.
+ * Hardware is locked to minimize corruption of read data.
+ */
+static void port_r_mib_pkt(struct ksz_sw *sw, uint port, u32 *last, u64 *cnt)
+{
+	u32 cur;
+	u32 data;
+	u16 ctrl_addr;
+	int index;
+
+	index = KS_MIB_PACKET_DROPPED_RX_0 + port;
+	do {
+		ctrl_addr = index;
+
+		mutex_lock(sw->reglock);
+
+		ctrl_addr |= IND_ACC_TABLE(TABLE_MIB | TABLE_READ);
+		sw->reg->w16(sw, REG_IND_CTRL_0, ctrl_addr);
+		HW_DELAY(sw, REG_IND_CTRL_0);
+		data = sw->reg->r32(sw, REG_IND_DATA_LO);
+
+		mutex_unlock(sw->reglock);
+
+		data &= MIB_PACKET_DROPPED;
+		cur = *last;
+		if (data != cur) {
+			*last = data;
+			if (data < cur)
+				data += MIB_PACKET_DROPPED + 1;
+			data -= cur;
+			*cnt += data;
+		}
+		++last;
+		++cnt;
+		index -= KS_MIB_PACKET_DROPPED_TX -
+			KS_MIB_PACKET_DROPPED_TX_0 + 1;
+	} while (index >= KS_MIB_PACKET_DROPPED_TX_0 + port);
+}  /* port_r_mib_pkt */
+
+static int exit_mib_read(struct ksz_sw *sw)
+{
+	if (sw->intr_using)
+		return true;
+	return false;
+}  /* exit_mib_read */
+
+/**
+ * port_r_cnt - read MIB counters periodically
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ *
+ * This routine is used to read the counters of the port periodically to avoid
+ * counter overflow.  The hardware should be acquired first before calling this
+ * routine.
+ *
+ * Return non-zero when not all counters not read.
+ */
+static int port_r_cnt(struct ksz_sw *sw, uint port)
+{
+	struct ksz_port_mib *mib = get_port_mib(sw, port);
+
+	if (mib->mib_start < SWITCH_COUNTER_NUM)
+		while (mib->cnt_ptr < SWITCH_COUNTER_NUM) {
+			if (exit_mib_read(sw))
+				return mib->cnt_ptr;
+			++mib->read_cnt[mib->cnt_ptr];
+			if (mib->read_cnt[mib->cnt_ptr] >=
+					mib->read_max[mib->cnt_ptr]) {
+				mib->read_cnt[mib->cnt_ptr] = 0;
+				port_r_mib_cnt(sw, port, mib->cnt_ptr,
+					&mib->counter[mib->cnt_ptr]);
+			}
+			++mib->cnt_ptr;
+		}
+	if (sw->mib_cnt > SWITCH_COUNTER_NUM)
+		port_r_mib_pkt(sw, port, mib->dropped,
+			&mib->counter[SWITCH_COUNTER_NUM]);
+	mib->cnt_ptr = 0;
+	return 0;
+}  /* port_r_cnt */
+
+/**
+ * port_init_cnt - initialize MIB counter values
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ *
+ * This routine is used to initialize all counters to zero if the hardware
+ * cannot do it after reset.
+ */
+static inline void port_init_cnt(struct ksz_sw *sw, uint port)
+{
+	struct ksz_port_mib *mib = get_port_mib(sw, port);
+
+	mutex_lock(&sw->lock);
+	mib->cnt_ptr = 0;
+	if (mib->mib_start < SWITCH_COUNTER_NUM)
+		do {
+			mib->read_cnt[mib->cnt_ptr] = 0;
+			mib->read_max[mib->cnt_ptr] =
+				mib_read_max[mib->cnt_ptr];
+			port_r_mib_cnt(sw, port, mib->cnt_ptr,
+				&mib->counter[mib->cnt_ptr]);
+			++mib->cnt_ptr;
+		} while (mib->cnt_ptr < SWITCH_COUNTER_NUM);
+	if (sw->mib_cnt > SWITCH_COUNTER_NUM)
+		port_r_mib_pkt(sw, port, mib->dropped,
+			&mib->counter[SWITCH_COUNTER_NUM]);
+	memset((void *) mib->counter, 0, sizeof(u64) *
+		TOTAL_SWITCH_COUNTER_NUM);
+	mib->cnt_ptr = 0;
+	mib->rate[0].last = mib->rate[1].last = 0;
+	mib->rate[0].last_cnt = mib->rate[1].last_cnt = 0;
+	mib->rate[0].peak = mib->rate[1].peak = 0;
+	mutex_unlock(&sw->lock);
+}  /* port_init_cnt */
+
+/* -------------------------------------------------------------------------- */
+
+/*
+ * Port functions
+ */
+
+/**
+ * port_chk - check port register bits
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ * @offset:	The offset of the port register.
+ * @bits:	The data bits to check.
+ *
+ * This function checks whether the specified bits of the port register are set
+ * or not.
+ *
+ * Return 0 if the bits are not set.
+ */
+static int port_chk(struct ksz_sw *sw, uint port, uint offset, SW_D bits)
+{
+	u32 addr;
+	SW_D data;
+
+	PORT_CTRL_ADDR(port, addr);
+	addr += offset;
+	data = SW_R(sw, addr);
+	return (data & bits) == bits;
+}
+
+/**
+ * port_cfg - set port register bits
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ * @offset:	The offset of the port register.
+ * @bits:	The data bits to set.
+ * @set:	The flag indicating whether the bits are to be set or not.
+ *
+ * This routine sets or resets the specified bits of the port register.
+ */
+static void port_cfg(struct ksz_sw *sw, uint port, uint offset, SW_D bits,
+	bool set)
+{
+	u32 addr;
+	SW_D data;
+
+	PORT_CTRL_ADDR(port, addr);
+	addr += offset;
+	data = SW_R(sw, addr);
+	if (set)
+		data |= bits;
+	else
+		data &= ~bits;
+	SW_W(sw, addr, data);
+}
+
+/**
+ * port_chk_shift - check port bit
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ * @offset:	The offset of the register.
+ * @shift:	Number of bits to shift.
+ *
+ * This function checks whether the specified port is set in the register or
+ * not.
+ *
+ * Return 0 if the port is not set.
+ */
+static int port_chk_shift(struct ksz_sw *sw, uint port, u32 addr, int shift)
+{
+	SW_D data;
+	SW_D bit = 1 << port;
+
+	data = SW_R(sw, addr);
+	data >>= shift;
+	return (data & bit) == bit;
+}
+
+/**
+ * port_cfg_shift - set port bit
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ * @offset:	The offset of the register.
+ * @shift:	Number of bits to shift.
+ * @set:	The flag indicating whether the port is to be set or not.
+ *
+ * This routine sets or resets the specified port in the register.
+ */
+static void port_cfg_shift(struct ksz_sw *sw, uint port, u32 addr, int shift,
+	bool set)
+{
+	SW_D data;
+	SW_D bits = 1 << port;
+
+	data = SW_R(sw, addr);
+	bits <<= shift;
+	if (set)
+		data |= bits;
+	else
+		data &= ~bits;
+	SW_W(sw, addr, data);
+}
+
+/**
+ * port_r8 - read byte from port register
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ * @offset:	The offset of the port register.
+ * @data:	Buffer to store the data.
+ *
+ * This routine reads a byte from the port register.
+ */
+static void port_r8(struct ksz_sw *sw, uint port, uint offset, u8 *data)
+{
+	u32 addr;
+
+	PORT_CTRL_ADDR(port, addr);
+	addr += offset;
+	*data = sw->reg->r8(sw, addr);
+}
+
+/**
+ * port_w8 - write byte to port register
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ * @offset:	The offset of the port register.
+ * @data:	Data to write.
+ *
+ * This routine writes a byte to the port register.
+ */
+static void port_w8(struct ksz_sw *sw, uint port, uint offset, u8 data)
+{
+	u32 addr;
+
+	PORT_CTRL_ADDR(port, addr);
+	addr += offset;
+	sw->reg->w8(sw, addr, data);
+}
+
+/**
+ * port_r16 - read word from port register.
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ * @offset:	The offset of the port register.
+ * @data:	Buffer to store the data.
+ *
+ * This routine reads a word from the port register.
+ */
+static void port_r16(struct ksz_sw *sw, uint port, uint offset, u16 *data)
+{
+	u32 addr;
+
+	PORT_CTRL_ADDR(port, addr);
+	addr += offset;
+	*data = sw->reg->r16(sw, addr);
+}
+
+/**
+ * port_w16 - write word to port register.
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ * @offset:	The offset of the port register.
+ * @data:	Data to write.
+ *
+ * This routine writes a word to the port register.
+ */
+static void port_w16(struct ksz_sw *sw, uint port, uint offset, u16 data)
+{
+	u32 addr;
+
+	PORT_CTRL_ADDR(port, addr);
+	addr += offset;
+	sw->reg->w16(sw, addr, data);
+}
+
+/**
+ * sw_chk - check switch register bits
+ * @sw:		The switch instance.
+ * @addr:	The address of the switch register.
+ * @bits:	The data bits to check.
+ *
+ * This function checks whether the specified bits of the switch register are
+ * set or not.
+ *
+ * Return 0 if the bits are not set.
+ */
+static int sw_chk(struct ksz_sw *sw, u32 addr, SW_D bits)
+{
+	SW_D data;
+
+	data = SW_R(sw, addr);
+	return (data & bits) == bits;
+}
+
+/**
+ * sw_cfg - set switch register bits
+ * @sw:		The switch instance.
+ * @addr:	The address of the switch register.
+ * @bits:	The data bits to set.
+ * @set:	The flag indicating whether the bits are to be set or not.
+ *
+ * This function sets or resets the specified bits of the switch register.
+ */
+static void sw_cfg(struct ksz_sw *sw, u32 addr, SW_D bits, bool set)
+{
+	SW_D data;
+
+	data = SW_R(sw, addr);
+	if (set)
+		data |= bits;
+	else
+		data &= ~bits;
+	SW_W(sw, addr, data);
+}
+
+#ifdef PORT_OUT_RATE_ADDR
+/**
+ * port_out_rate_r8 - read byte from port register
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ * @offset:	The offset of the port register.
+ * @data:	Buffer to store the data.
+ *
+ * This routine reads a byte from the port register.
+ */
+static void port_out_rate_r8(struct ksz_sw *sw, uint port, uint offset,
+	u8 *data)
+{
+	u32 addr;
+
+	PORT_OUT_RATE_ADDR(port, addr);
+	addr += offset;
+	*data = sw->reg->r8(sw, addr);
+}
+
+/**
+ * port_out_rate_w8 - write byte to port register
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ * @offset:	The offset of the port register.
+ * @data:	Data to write.
+ *
+ * This routine writes a byte to the port register.
+ */
+static void port_out_rate_w8(struct ksz_sw *sw, uint port, uint offset,
+	u8 data)
+{
+	u32 addr;
+
+	PORT_OUT_RATE_ADDR(port, addr);
+	addr += offset;
+	sw->reg->w8(sw, addr, data);
+}
+#endif
+
+/* -------------------------------------------------------------------------- */
+
+/* Bandwidth */
+
+static inline void port_cfg_broad_storm(struct ksz_sw *sw, uint p, bool set)
+{
+	port_cfg(sw, p,
+		P_BCAST_STORM_CTRL, PORT_BROADCAST_STORM, set);
+}
+
+static inline int port_chk_broad_storm(struct ksz_sw *sw, uint p)
+{
+	return port_chk(sw, p,
+		P_BCAST_STORM_CTRL, PORT_BROADCAST_STORM);
+}
+
+/* Driver set switch broadcast storm protection at 10% rate. */
+#define BROADCAST_STORM_PROTECTION_RATE	10
+
+/* 148,800 frames * 67 ms / 100 */
+#define BROADCAST_STORM_VALUE		9969
+
+/**
+ * sw_cfg_broad_storm - configure broadcast storm threshold
+ * @sw:		The switch instance.
+ * @percent:	Broadcast storm threshold in percent of transmit rate.
+ *
+ * This routine configures the broadcast storm threshold of the switch.
+ */
+static void sw_cfg_broad_storm(struct ksz_sw *sw, u8 percent)
+{
+	u16 data;
+	u32 value = ((u32) BROADCAST_STORM_VALUE * (u32) percent / 100);
+
+	if (value > BROADCAST_STORM_RATE)
+		value = BROADCAST_STORM_RATE;
+
+	data = sw->reg->r16(sw, S_REPLACE_VID_CTRL);
+#if (SW_SIZE == (2))
+	data = ntohs(data);
+#endif
+	data &= ~BROADCAST_STORM_RATE;
+	data |= value;
+#if (SW_SIZE == (2))
+	data = ntohs(data);
+#endif
+	sw->reg->w16(sw, S_REPLACE_VID_CTRL, data);
+}
+
+/**
+ * sw_get_board_storm - get broadcast storm threshold
+ * @sw:		The switch instance.
+ * @percent:	Buffer to store the broadcast storm threshold percentage.
+ *
+ * This routine retrieves the broadcast storm threshold of the switch.
+ */
+static void sw_get_broad_storm(struct ksz_sw *sw, u8 *percent)
+{
+	int num;
+	u16 data;
+
+	data = sw->reg->r16(sw, S_REPLACE_VID_CTRL);
+#if (SW_SIZE == (2))
+	data = ntohs(data);
+#endif
+	num = (data & BROADCAST_STORM_RATE);
+	num = (num * 100 + BROADCAST_STORM_VALUE / 2) / BROADCAST_STORM_VALUE;
+	*percent = (u8) num;
+}
+
+/**
+ * sw_dis_broad_storm - disable broadcast storm protection
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ *
+ * This routine disables the broadcast storm limit function of the switch.
+ */
+static void sw_dis_broad_storm(struct ksz_sw *sw, uint port)
+{
+	port_cfg_broad_storm(sw, port, 0);
+}
+
+/**
+ * sw_ena_broad_storm - enable broadcast storm protection
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ *
+ * This routine enables the broadcast storm limit function of the switch.
+ */
+static void sw_ena_broad_storm(struct ksz_sw *sw, uint port)
+{
+	sw_cfg_broad_storm(sw, sw->info->broad_per);
+	port_cfg_broad_storm(sw, port, 1);
+}
+
+/**
+ * sw_init_broad_storm - initialize broadcast storm
+ * @sw:		The switch instance.
+ *
+ * This routine initializes the broadcast storm limit function of the switch.
+ */
+static void sw_init_broad_storm(struct ksz_sw *sw)
+{
+	u8 percent;
+
+	sw_get_broad_storm(sw, &percent);
+	sw->info->broad_per = percent;
+}
+
+/**
+ * hw_cfg_broad_storm - configure broadcast storm
+ * @sw:		The switch instance.
+ * @percent:	Broadcast storm threshold in percent of transmit rate.
+ *
+ * This routine configures the broadcast storm threshold of the switch.
+ * It is called by user functions.  The hardware should be acquired first.
+ */
+static void hw_cfg_broad_storm(struct ksz_sw *sw, u8 percent)
+{
+	if (percent > 100)
+		percent = 100;
+
+	sw_cfg_broad_storm(sw, percent);
+	sw_init_broad_storm(sw);
+}
+
+/**
+ * sw_setup_broad_storm - setup broadcast storm
+ * @sw:		The switch instance.
+ *
+ * This routine setup the broadcast storm limit function of the switch.
+ */
+static void sw_setup_broad_storm(struct ksz_sw *sw)
+{
+	uint port;
+
+	/* Enable switch broadcast storm protection at 10% percent rate. */
+	hw_cfg_broad_storm(sw, BROADCAST_STORM_PROTECTION_RATE);
+	for (port = 0; port < SWITCH_PORT_NUM; port++)
+		sw_ena_broad_storm(sw, port);
+	sw_cfg(sw, REG_SWITCH_CTRL_2, MULTICAST_STORM_DISABLE, 1);
+}
+
+/* -------------------------------------------------------------------------- */
+
+/* Rate Control */
+
+static inline int get_rate_ctrl_offset(int port, int prio)
+{
+	int offset;
+
+#if (SW_SIZE == (2))
+	offset = REG_PORT1_TXQ_RATE_CTRL1 + port * 4;
+	offset += ((3 - prio) / 2) * 2;
+	offset++;
+	offset -= (prio & 1);
+#else
+	offset = REG_PORT1_TXQ3_RATE_CTRL + port * 4;
+	offset += (3 - prio);
+#endif
+	return offset;
+}
+
+/**
+ * hw_cfg_rate_ctrl - configure port rate control
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ * @prio:	The priority index to configure.
+ * @ctrl:	The flag indicating whether the rate control bit is set or not.
+ *
+ * This routine configures the priority rate control of the port.
+ */
+static void hw_cfg_rate_ctrl(struct ksz_sw *sw, uint port, int prio, int ctrl)
+{
+	int offset;
+	u8 data;
+	u8 saved;
+
+	offset = get_rate_ctrl_offset(port, prio);
+
+	data = sw->reg->r8(sw, offset);
+	saved = data;
+	data &= ~RATE_CTRL_ENABLE;
+	if (ctrl)
+		data |= RATE_CTRL_ENABLE;
+	if (data != saved)
+		sw->reg->w8(sw, offset, data);
+	sw->info->port_cfg[port].rate_ctrl[prio] = data;
+}
+
+#ifdef RATE_RATIO_MASK
+/**
+ * hw_cfg_rate_ratio - configure port rate ratio
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ * @prio:	The priority index to configure.
+ * @ratio:	The rate ratio.
+ *
+ * This routine configures the priority rate ratio of the port.
+ */
+static void hw_cfg_rate_ratio(struct ksz_sw *sw, uint port, int prio, u8 ratio)
+{
+	int offset;
+	u8 data;
+	u8 saved;
+
+	if (ratio >= RATE_CTRL_ENABLE)
+		return;
+
+	offset = get_rate_ctrl_offset(port, prio);
+
+	data = sw->reg->r8(sw, offset);
+	saved = data;
+	data &= RATE_CTRL_ENABLE;
+	data |= ratio;
+	if (data != saved)
+		sw->reg->w8(sw, offset, data);
+	sw->info->port_cfg[port].rate_ctrl[prio] = data;
+}
+#endif
+
+/**
+ * hw_get_rate_ctrl - get port rate control
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ * @prio:	The priority index to retrieve.
+ *
+ * This routine retrieves the priority rate control of the port.
+ */
+static void hw_get_rate_ctrl(struct ksz_sw *sw, uint port, int prio)
+{
+	int offset;
+	u8 data;
+
+	offset = get_rate_ctrl_offset(port, prio);
+
+	data = sw->reg->r8(sw, offset);
+	sw->info->port_cfg[port].rate_ctrl[prio] = data;
+}
+
+/* -------------------------------------------------------------------------- */
+
+/* Rate Limit */
+
+/**
+ * hw_cfg_rate_limit - configure port rate limit modes
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ * @mask:	The mask value.
+ * @shift:	The shift position.
+ * @mode:	The rate limit mode.
+ *
+ * This helper routine configures the rate limit modes of the port.
+ */
+static void hw_cfg_rate_limit(struct ksz_sw *sw, uint port, u8 mask, u8 shift,
+	u8 mode)
+{
+	u8 data;
+	u8 saved;
+
+	port_r8(sw, port, P_RATE_LIMIT_CTRL, &data);
+	saved = data;
+	data &= ~(mask << shift);
+	data |= mode << shift;
+	if (data != saved)
+		port_w8(sw, port, P_RATE_LIMIT_CTRL, data);
+	sw->info->port_cfg[port].rate_limit = data;
+}
+
+/**
+ * hw_cfg_cnt_ifg - configure port rate limit count IFG control
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ * @set:	The flag indicating whether the count control is set or not.
+ *
+ * This routine configures the rate limit count IFG control of the port.
+ */
+static void hw_cfg_cnt_ifg(struct ksz_sw *sw, uint port, bool set)
+{
+	hw_cfg_rate_limit(sw, port, 1, 1, set);
+}
+
+/**
+ * hw_cfg_cnt_pre - configure port rate limit count preamble control
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ * @set:	The flag indicating whether the count control is set or not.
+ *
+ * This routine configures the rate limit count preamble control of the port.
+ */
+static void hw_cfg_cnt_pre(struct ksz_sw *sw, uint port, bool set)
+{
+	hw_cfg_rate_limit(sw, port, 1, 0, set);
+}
+
+/**
+ * hw_cfg_rx_limit - configure port rate limit mode
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ * @mode:	The rate limit mode.
+ *
+ * This routine configures the rate limit mode of the port.
+ */
+static void hw_cfg_rx_limit(struct ksz_sw *sw, uint port, u8 mode)
+{
+	if (mode >= 4)
+		return;
+
+	hw_cfg_rate_limit(sw, port, 3, 2, mode);
+}
+
+/**
+ * hw_get_rate_limit - get port rate limit control
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ *
+ * This routine retrieves the rate limit of the port.
+ */
+static void hw_get_rate_limit(struct ksz_sw *sw, uint port)
+{
+	u8 data;
+
+	port_r8(sw, port, P_RATE_LIMIT_CTRL, &data);
+	sw->info->port_cfg[port].rate_limit = data;
+}
+
+/* -------------------------------------------------------------------------- */
+
+static uint get_rate_from_val(u8 val)
+{
+	uint i;
+
+	if (0 == val)
+		i = 0;
+	else if (val <= 0x64)
+		i = 1000 * val;
+	else
+		i = 64 * (val - 0x64);
+	return i;
+}
+
+static int get_rate_to_val(uint rate)
+{
+	int i;
+
+	if (rate >= 1000) {
+		i = (rate + 500) / 1000;
+		if (i > 0x64)
+			i = 0x64;
+	} else if (0 == rate) {
+		i = 0;
+	} else {
+		i = (rate + 32) / 64;
+		if (0 == i)
+			i = 1;
+		else if (i > 15)
+			i = 15;
+		i += 0x64;
+	}
+	return i;
+}
+
+/**
+ * port_cfg_rate - configure port priority rate
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ * @prio:	The priority index to configure.
+ * @offset:	The receive or transmit rate offset.
+ * @shift:	The shift position to set the value.
+ * @rate:	The rate limit in number of Kbps.
+ *
+ * This helper routine configures the priority rate of the port.
+ */
+static void port_cfg_rate(struct ksz_sw *sw, uint port, uint prio, uint offset,
+	uint rate)
+{
+	u8 data;
+	u8 factor;
+
+	factor = (u8) get_rate_to_val(rate);
+
+#ifndef PORT_OUT_RATE_ADDR
+	offset += (prio / 2) * 2;
+	offset += (prio & 1);
+
+	/* First rate limit register may have special function bit. */
+	if (0 == prio) {
+		port_r8(sw, port, offset, &data);
+		data &= 0x80;
+		factor |= data;
+	}
+	port_w8(sw, port, offset, factor);
+#else
+
+	/* First rate limit register may have special function bit. */
+	if (0 == prio) {
+		if (REG_PORT_1_OUT_RATE_0 == offset)
+			port_out_rate_r8(sw, port, REG_PORT_OUT_RATE_0 + prio,
+				&data);
+		else
+			port_r8(sw, port, REG_PORT_IN_RATE_0 + prio, &data);
+		data &= 0x80;
+		factor |= data;
+	}
+	if (REG_PORT_1_OUT_RATE_0 == offset)
+		port_out_rate_w8(sw, port, REG_PORT_OUT_RATE_0 + prio, factor);
+	else
+		port_w8(sw, port, REG_PORT_IN_RATE_0 + prio, factor);
+#endif
+}
+
+/**
+ * port_get_rate - get port priority rate
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ * @prio:	The priority index to configure.
+ * @offset:	The receive or transmit rate offset.
+ * @shift:	The shift position to get the value.
+ * @rate:	Buffer to store the data rate in number of Kbps.
+ *
+ * This helper routine retrieves the priority rate of the port.
+ */
+static void port_get_rate(struct ksz_sw *sw, uint port, uint prio, uint offset,
+	uint *rate)
+{
+	u8 data;
+
+#ifndef PORT_OUT_RATE_ADDR
+	offset += (prio / 2) * 2;
+	offset += (prio & 1);
+	port_r8(sw, port, offset, &data);
+#else
+	if (REG_PORT_1_OUT_RATE_0 == offset)
+		port_out_rate_r8(sw, port, REG_PORT_OUT_RATE_0 + prio, &data);
+	else
+		port_r8(sw, port, REG_PORT_IN_RATE_0 + prio, &data);
+#endif
+	data &= 0x7F;
+	*rate = get_rate_from_val(data);
+}
+
+/**
+ * hw_cfg_prio_rate - configure port priority
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ * @prio:	The priority index to configure.
+ * @rate:	The rate limit in number of Kbps.
+ * @offset:	The receive or transmit rate offset.
+ * @result:	Buffer to store the data rate in number of Kbps.
+ *
+ * This helper routine configures the priority rate of the port and retrieves
+ * the actual rate number.
+ */
+static void hw_cfg_prio_rate(struct ksz_sw *sw, uint port, int prio, uint rate,
+	int offset, uint *result)
+{
+	port_cfg_rate(sw, port, prio, offset, rate);
+	port_get_rate(sw, port, prio, offset, result);
+}
+
+/**
+ * hw_cfg_rx_prio_rate - configure port receive priority
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ * @prio:	The priority index to configure.
+ * @rate:	The rate limit in number of Kbps.
+ *
+ * This routine configures the receive priority rate of the port.
+ * It is called by user functions.  The hardware should be acquired first.
+ */
+static void hw_cfg_rx_prio_rate(struct ksz_sw *sw, uint port, int prio,
+	uint rate)
+{
+	hw_cfg_prio_rate(sw, port, prio, rate,
+#ifdef PORT_OUT_RATE_ADDR
+		REG_PORT_1_IN_RATE_0,
+#else
+		REG_PORT_IN_RATE_0,
+#endif
+		&sw->info->port_cfg[port].rx_rate[prio]);
+}
+
+/**
+ * hw_cfg_tx_prio_rate - configure port transmit priority
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ * @prio:	The priority index to configure.
+ * @rate:	The rate limit in number of Kbps.
+ *
+ * This routine configures the transmit priority rate of the port.
+ * It is called by user functions.  The hardware should be acquired first.
+ */
+static void hw_cfg_tx_prio_rate(struct ksz_sw *sw, uint port, int prio,
+	uint rate)
+{
+	hw_cfg_prio_rate(sw, port, prio, rate,
+#ifdef PORT_OUT_RATE_ADDR
+		REG_PORT_1_OUT_RATE_0,
+#else
+		REG_PORT_OUT_RATE_0,
+#endif
+		&sw->info->port_cfg[port].tx_rate[prio]);
+}
+
+/**
+ * sw_chk_rx_prio_rate - check switch rx priority rate
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ *
+ * This function checks whether the rx priority rate function of the switch is
+ * enabled.
+ *
+ * Return 0 if not enabled.
+ */
+static int sw_chk_rx_prio_rate(struct ksz_sw *sw, uint port)
+{
+	u32 rate_addr;
+	u32 in_rate;
+
+	PORT_CTRL_ADDR(port, rate_addr);
+	rate_addr += REG_PORT_IN_RATE_0;
+	in_rate = sw->reg->r32(sw, rate_addr);
+	return (in_rate) != 0;
+}  /* sw_chk_rx_prio_rate */
+
+/**
+ * sw_chk_prio_rate - check switch tx priority rate
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ *
+ * This function checks whether the tx priority rate function of the switch is
+ * enabled.
+ *
+ * Return 0 if not enabled.
+ */
+static int sw_chk_tx_prio_rate(struct ksz_sw *sw, uint port)
+{
+#ifdef PORT_OUT_RATE_ADDR
+	u32 addr;
+	u32 rate_addr;
+	u32 out_rate;
+
+	PORT_OUT_RATE_ADDR(port, addr);
+	rate_addr = addr + REG_PORT_OUT_RATE_0;
+	out_rate = sw->reg->r32(sw, rate_addr);
+	return (out_rate) != 0;
+#else
+	return port_chk(sw, port, REG_PORT_OUT_RATE_0, 0x80);
+#endif
+}  /* sw_chk_tx_prio_rate */
+
+/**
+ * sw_dis_rx_prio_rate - disable switch rx priority rate
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ *
+ * This routine disables the rx priority rate function of the switch.
+ */
+static void sw_dis_rx_prio_rate(struct ksz_sw *sw, uint port)
+{
+	u32 rate_addr;
+
+	PORT_CTRL_ADDR(port, rate_addr);
+	rate_addr += REG_PORT_IN_RATE_0;
+	sw->reg->w32(sw, rate_addr, 0);
+}  /* sw_dis_rx_prio_rate */
+
+/**
+ * sw_dis_tx_prio_rate - disable tx switch priority rate
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ *
+ * This routine disables the tx priority rate function of the switch.
+ */
+static void sw_dis_tx_prio_rate(struct ksz_sw *sw, uint port)
+{
+#ifdef PORT_OUT_RATE_ADDR
+	u32 addr;
+	u32 rate_addr;
+
+	PORT_OUT_RATE_ADDR(port, addr);
+	rate_addr = addr + REG_PORT_OUT_RATE_0;
+	sw->reg->w32(sw, rate_addr, 0);
+#else
+	u8 data;
+
+	port_r8(sw, port, REG_PORT_OUT_RATE_0, &data);
+	data &= ~0x80;
+	port_w8(sw, port, REG_PORT_OUT_RATE_0, data);
+#endif
+}  /* sw_dis_tx_prio_rate */
+
+/**
+ * sw_ena_rx_prio_rate - enable switch rx priority rate
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ *
+ * This routine enables the rx priority rate function of the switch.
+ */
+static void sw_ena_rx_prio_rate(struct ksz_sw *sw, uint port)
+{
+	int prio;
+
+	for (prio = 0; prio < PRIO_QUEUES; prio++)
+		hw_cfg_rx_prio_rate(sw, port, prio,
+			sw->info->port_cfg[port].rx_rate[prio]);
+}  /* sw_ena_rx_prio_rate */
+
+/**
+ * sw_ena_tx_prio_rate - enable switch tx priority rate
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ *
+ * This routine enables the tx priority rate function of the switch.
+ */
+static void sw_ena_tx_prio_rate(struct ksz_sw *sw, uint port)
+{
+	int prio;
+
+	for (prio = 0; prio < PRIO_QUEUES; prio++)
+		hw_cfg_tx_prio_rate(sw, port, prio,
+			sw->info->port_cfg[port].tx_rate[prio]);
+#ifndef PORT_OUT_RATE_ADDR
+	do {
+		u8 data;
+
+		port_r8(sw, port, REG_PORT_OUT_RATE_0, &data);
+		data |= 0x80;
+		port_w8(sw, port, REG_PORT_OUT_RATE_0, data);
+	} while (0);
+#endif
+}  /* sw_ena_tx_prio_rate */
+
+/**
+ * sw_init_prio_rate - initialize switch prioirty rate
+ * @sw:		The switch instance.
+ *
+ * This routine initializes the priority rate function of the switch.
+ */
+static void sw_init_prio_rate(struct ksz_sw *sw)
+{
+	int offset;
+	uint port;
+	int prio;
+
+	for (port = 0; port < TOTAL_PORT_NUM; port++) {
+		hw_get_rate_limit(sw, port);
+		for (prio = 0; prio < PRIO_QUEUES; prio++) {
+			hw_get_rate_ctrl(sw, port, prio);
+#ifdef PORT_OUT_RATE_ADDR
+			offset = REG_PORT_1_IN_RATE_0;
+#else
+			offset = REG_PORT_IN_RATE_0;
+#endif
+			port_get_rate(sw, port, prio, offset,
+				&sw->info->port_cfg[port].rx_rate[prio]);
+#ifdef PORT_OUT_RATE_ADDR
+			offset = REG_PORT_1_OUT_RATE_0;
+#else
+			offset = REG_PORT_OUT_RATE_0;
+#endif
+			port_get_rate(sw, port, prio, offset,
+				&sw->info->port_cfg[port].tx_rate[prio]);
+		}
+	}
+}  /* sw_init_prio_rate */
+
+/* -------------------------------------------------------------------------- */
+
+/* Communication */
+
+static inline void port_cfg_back_pressure(struct ksz_sw *sw, uint p, bool set)
+{
+	port_cfg(sw, p,
+		P_STP_CTRL, PORT_BACK_PRESSURE, set);
+}
+
+static inline void port_cfg_force_flow_ctrl(struct ksz_sw *sw, uint p, bool set)
+{
+	port_cfg(sw, p,
+		P_STP_CTRL, PORT_FORCE_FLOW_CTRL, set);
+}
+
+static inline int port_chk_back_pressure(struct ksz_sw *sw, uint p)
+{
+	return port_chk(sw, p,
+		P_STP_CTRL, PORT_BACK_PRESSURE);
+}
+
+static inline int port_chk_force_flow_ctrl(struct ksz_sw *sw, uint p)
+{
+	return port_chk(sw, p,
+		P_STP_CTRL, PORT_FORCE_FLOW_CTRL);
+}
+
+/* -------------------------------------------------------------------------- */
+
+/* Spanning Tree */
+
+static inline void port_cfg_dis_learn(struct ksz_sw *sw, uint p, bool set)
+{
+	port_cfg(sw, p,
+		P_STP_CTRL, PORT_LEARN_DISABLE, set);
+}
+
+static inline void port_cfg_rx(struct ksz_sw *sw, uint p, bool set)
+{
+	port_cfg(sw, p,
+		P_STP_CTRL, PORT_RX_ENABLE, set);
+	if (set)
+		sw->rx_ports |= (1 << p);
+	else
+		sw->rx_ports &= ~(1 << p);
+}
+
+static inline void port_cfg_tx(struct ksz_sw *sw, uint p, bool set)
+{
+	port_cfg(sw, p,
+		P_STP_CTRL, PORT_TX_ENABLE, set);
+	if (set)
+		sw->tx_ports |= (1 << p);
+	else
+		sw->tx_ports &= ~(1 << p);
+}
+
+static inline int port_chk_dis_learn(struct ksz_sw *sw, uint p)
+{
+	return port_chk(sw, p,
+		P_STP_CTRL, PORT_LEARN_DISABLE);
+}
+
+static inline int port_chk_rx(struct ksz_sw *sw, uint p)
+{
+	return port_chk(sw, p,
+		P_STP_CTRL, PORT_RX_ENABLE);
+}
+
+static inline int port_chk_tx(struct ksz_sw *sw, uint p)
+{
+	return port_chk(sw, p,
+		P_STP_CTRL, PORT_TX_ENABLE);
+}
+
+static inline void sw_cfg_fast_aging(struct ksz_sw *sw, bool set)
+{
+	sw_cfg(sw, REG_SWITCH_CTRL_1, SWITCH_FAST_AGING, set);
+}
+
+static void sw_flush_dyn_mac_table(struct ksz_sw *sw, uint port)
+{
+	uint cnt;
+	uint first;
+	uint index;
+	int learn_disable[TOTAL_PORT_NUM];
+
+	if ((uint) port < TOTAL_PORT_NUM) {
+		first = port;
+		cnt = first + 1;
+	} else {
+		first = 0;
+		cnt = TOTAL_PORT_NUM;
+	}
+	for (index = first; index < cnt; index++) {
+		port = get_phy_port(sw, index);
+		learn_disable[port] = port_chk_dis_learn(sw, port);
+		if (!learn_disable[port])
+			port_cfg_dis_learn(sw, port, 1);
+	}
+	sw_cfg(sw, S_FLUSH_TABLE_CTRL, SWITCH_FLUSH_DYN_MAC_TABLE, 1);
+	for (index = first; index < cnt; index++) {
+		port = get_phy_port(sw, index);
+		if (!learn_disable[port])
+			port_cfg_dis_learn(sw, port, 0);
+	}
+}
+
+/* -------------------------------------------------------------------------- */
+
+/* VLAN */
+
+static inline void port_cfg_ins_tag(struct ksz_sw *sw, uint p, bool insert)
+{
+	port_cfg(sw, p,
+		P_TAG_CTRL, PORT_INSERT_TAG, insert);
+}
+
+static inline void port_cfg_rmv_tag(struct ksz_sw *sw, uint p, bool remove)
+{
+	port_cfg(sw, p,
+		P_TAG_CTRL, PORT_REMOVE_TAG, remove);
+}
+
+static inline int port_chk_ins_tag(struct ksz_sw *sw, uint p)
+{
+	return port_chk(sw, p,
+		P_TAG_CTRL, PORT_INSERT_TAG);
+}
+
+static inline int port_chk_rmv_tag(struct ksz_sw *sw, uint p)
+{
+	return port_chk(sw, p,
+		P_TAG_CTRL, PORT_REMOVE_TAG);
+}
+
+#ifdef PORT_DOUBLE_TAG
+static inline void port_cfg_double_tag(struct ksz_sw *sw, uint p, int remove)
+{
+	port_cfg(sw, p,
+		P_MIRROR_CTRL, PORT_DOUBLE_TAG, remove);
+}
+
+static inline int port_chk_double_tag(struct ksz_sw *sw, uint p)
+{
+	return port_chk(sw, p,
+		P_MIRROR_CTRL, PORT_DOUBLE_TAG);
+}
+#endif
+
+static inline void port_cfg_dis_non_vid(struct ksz_sw *sw, uint p, bool set)
+{
+	port_cfg(sw, p,
+		P_STP_CTRL, PORT_DISCARD_NON_VID, set);
+}
+
+static inline void port_cfg_drop_tag(struct ksz_sw *sw, uint p, bool set)
+{
+	port_cfg(sw, p,
+		P_SA_MAC_CTRL, PORT_DROP_TAG, set);
+}
+
+static inline void port_cfg_in_filter(struct ksz_sw *sw, uint p, bool set)
+{
+	port_cfg(sw, p,
+		P_STP_CTRL, PORT_INGRESS_FILTER, set);
+}
+
+static inline int port_chk_dis_non_vid(struct ksz_sw *sw, uint p)
+{
+	return port_chk(sw, p,
+		P_STP_CTRL, PORT_DISCARD_NON_VID);
+}
+
+static inline int port_chk_drop_tag(struct ksz_sw *sw, uint p)
+{
+	return port_chk(sw, p,
+		P_SA_MAC_CTRL, PORT_DROP_TAG);
+}
+
+static inline int port_chk_in_filter(struct ksz_sw *sw, uint p)
+{
+	return port_chk(sw, p,
+		P_STP_CTRL, PORT_INGRESS_FILTER);
+}
+
+/* -------------------------------------------------------------------------- */
+
+/* Mirroring */
+
+static inline void port_cfg_mirror_sniffer(struct ksz_sw *sw, uint p, bool set)
+{
+	port_cfg(sw, p,
+		P_MIRROR_CTRL, PORT_MIRROR_SNIFFER, set);
+}
+
+static inline void port_cfg_mirror_rx(struct ksz_sw *sw, uint p, bool set)
+{
+	port_cfg(sw, p,
+		P_MIRROR_CTRL, PORT_MIRROR_RX, set);
+}
+
+static inline void port_cfg_mirror_tx(struct ksz_sw *sw, uint p, bool set)
+{
+	port_cfg(sw, p,
+		P_MIRROR_CTRL, PORT_MIRROR_TX, set);
+}
+
+static inline void sw_cfg_mirror_rx_tx(struct ksz_sw *sw, bool set)
+{
+	sw_cfg(sw, S_MIRROR_CTRL, SWITCH_MIRROR_RX_TX, set);
+}
+
+static inline int port_chk_mirror_sniffer(struct ksz_sw *sw, uint p)
+{
+	return port_chk(sw, p,
+		P_MIRROR_CTRL, PORT_MIRROR_SNIFFER);
+}
+
+static inline int port_chk_mirror_rx(struct ksz_sw *sw, uint p)
+{
+	return port_chk(sw, p,
+		P_MIRROR_CTRL, PORT_MIRROR_RX);
+}
+
+static inline int port_chk_mirror_tx(struct ksz_sw *sw, uint p)
+{
+	return port_chk(sw, p,
+		P_MIRROR_CTRL, PORT_MIRROR_TX);
+}
+
+static inline int sw_chk_mirror_rx_tx(struct ksz_sw *sw)
+{
+	return sw_chk(sw, S_MIRROR_CTRL, SWITCH_MIRROR_RX_TX);
+}
+
+static void sw_setup_mirror(struct ksz_sw *sw)
+{
+	uint port;
+
+	for (port = 0; port < TOTAL_PORT_NUM; port++) {
+		port_cfg_mirror_sniffer(sw, port, 0);
+		port_cfg_mirror_rx(sw, port, 0);
+		port_cfg_mirror_tx(sw, port, 0);
+	}
+	sw_cfg_mirror_rx_tx(sw, 0);
+}
+
+/* -------------------------------------------------------------------------- */
+
+static void sw_cfg_unk_dest(struct ksz_sw *sw, bool set)
+{
+	u8 data;
+
+	data = sw->reg->r8(sw, S_UNKNOWN_DA_CTRL);
+	if (set)
+		data |= SWITCH_UNKNOWN_DA_ENABLE;
+	else
+		data &= ~SWITCH_UNKNOWN_DA_ENABLE;
+	sw->reg->w8(sw, S_UNKNOWN_DA_CTRL, data);
+}
+
+static int sw_chk_unk_dest(struct ksz_sw *sw)
+{
+	u8 data;
+
+	data = sw->reg->r8(sw, S_UNKNOWN_DA_CTRL);
+	return (data & SWITCH_UNKNOWN_DA_ENABLE) == SWITCH_UNKNOWN_DA_ENABLE;
+}
+
+static void sw_cfg_unk_def_port(struct ksz_sw *sw, uint port, bool set)
+{
+	u8 data;
+	u8 bits = 1 << port;
+
+	data = sw->reg->r8(sw, S_UNKNOWN_DA_CTRL);
+	if (set)
+		data |= bits;
+	else
+		data &= ~bits;
+	sw->reg->w8(sw, S_UNKNOWN_DA_CTRL, data);
+}
+
+static int sw_chk_unk_def_port(struct ksz_sw *sw, uint port)
+{
+	u8 data;
+	u8 bit = 1 << port;
+
+	data = sw->reg->r8(sw, S_UNKNOWN_DA_CTRL);
+	return (data & bit) == bit;
+}
+
+static inline void sw_cfg_for_inv_vid(struct ksz_sw *sw, uint port, bool set)
+{
+	port_cfg_shift(sw, port, S_FORWARD_INVALID_VID_CTRL,
+		FORWARD_INVALID_PORT_SHIFT, set);
+}
+
+static inline int sw_chk_for_inv_vid(struct ksz_sw *sw, uint port)
+{
+	return port_chk_shift(sw, port, S_FORWARD_INVALID_VID_CTRL,
+		FORWARD_INVALID_PORT_SHIFT);
+}
+
+/* -------------------------------------------------------------------------- */
+
+/* Priority */
+
+static inline void port_cfg_diffserv(struct ksz_sw *sw, uint p, bool set)
+{
+	port_cfg(sw, p,
+		P_PRIO_CTRL, PORT_DIFFSERV_ENABLE, set);
+}
+
+static inline void port_cfg_802_1p(struct ksz_sw *sw, uint p, bool set)
+{
+	port_cfg(sw, p,
+		P_PRIO_CTRL, PORT_802_1P_ENABLE, set);
+}
+
+static inline void port_cfg_replace_prio(struct ksz_sw *sw, uint p, bool set)
+{
+	port_cfg(sw, p,
+		P_MIRROR_CTRL, PORT_802_1P_REMAPPING, set);
+}
+
+static inline void port_cfg_2_queue(struct ksz_sw *sw, uint p, bool set)
+{
+	port_cfg(sw, p,
+		P_2_QUEUE_CTRL, PORT_2_QUEUES_ENABLE, set);
+}
+
+static inline void port_cfg_4_queue(struct ksz_sw *sw, uint p, bool set)
+{
+	port_cfg(sw, p,
+		P_4_QUEUE_CTRL, PORT_4_QUEUES_ENABLE, set);
+}
+
+static inline int port_chk_diffserv(struct ksz_sw *sw, uint p)
+{
+	return port_chk(sw, p,
+		P_PRIO_CTRL, PORT_DIFFSERV_ENABLE);
+}
+
+static inline int port_chk_802_1p(struct ksz_sw *sw, uint p)
+{
+	return port_chk(sw, p,
+		P_PRIO_CTRL, PORT_802_1P_ENABLE);
+}
+
+static inline int port_chk_replace_prio(struct ksz_sw *sw, uint p)
+{
+	return port_chk(sw, p,
+		P_MIRROR_CTRL, PORT_802_1P_REMAPPING);
+}
+
+static inline int port_chk_2_queue(struct ksz_sw *sw, uint p)
+{
+	return port_chk(sw, p,
+		P_2_QUEUE_CTRL, PORT_2_QUEUES_ENABLE);
+}
+
+static inline int port_chk_4_queue(struct ksz_sw *sw, uint p)
+{
+	return port_chk(sw, p,
+		P_4_QUEUE_CTRL, PORT_4_QUEUES_ENABLE);
+}
+
+/* -------------------------------------------------------------------------- */
+
+static inline void port_cfg_src_filter_0(struct ksz_sw *sw, uint p, bool set)
+{
+	port_cfg(sw, p,
+		P_SA_MAC_CTRL, PORT_SA_MAC1, set);
+}
+
+static inline void port_cfg_src_filter_1(struct ksz_sw *sw, uint p, bool set)
+{
+	port_cfg(sw, p,
+		P_SA_MAC_CTRL, PORT_SA_MAC2, set);
+}
+
+static inline int port_chk_src_filter_0(struct ksz_sw *sw, uint p)
+{
+	return port_chk(sw, p,
+		P_SA_MAC_CTRL, PORT_SA_MAC1);
+}
+
+static inline int port_chk_src_filter_1(struct ksz_sw *sw, uint p)
+{
+	return port_chk(sw, p,
+		P_SA_MAC_CTRL, PORT_SA_MAC2);
+}
+
+/**
+ * port_get_addr - get the port MAC address.
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ * @mac_addr:	Buffer to store the MAC address.
+ *
+ * This function retrieves the MAC address of the port.
+ */
+static inline void port_get_addr(struct ksz_sw *sw, uint port, u8 *mac_addr)
+{
+	int i;
+	SW_D reg;
+#if (SW_SIZE == (2))
+	u16 *addr = (u16 *) mac_addr;
+#endif
+
+	if (0 == port)
+		reg = REG_PORT_0_MAC_ADDR_0;
+	else
+		reg = REG_PORT_1_MAC_ADDR_0;
+#if (SW_SIZE == (2))
+	for (i = 0; i < 6; i += 2) {
+		*addr = sw->reg->r16(sw, reg - i);
+		*addr = ntohs(*addr);
+		addr++;
+	}
+#else
+	for (i = 0; i < 6; i++)
+		mac_addr[i] = sw->reg->r8(sw, reg + i);
+#endif
+	memcpy(sw->port_info[port].mac_addr, mac_addr, 6);
+}
+
+/**
+ * port_set_addr - configure port MAC address
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ * @mac_addr:	The MAC address.
+ *
+ * This function configures the MAC address of the port.
+ */
+static void port_set_addr(struct ksz_sw *sw, uint port, u8 *mac_addr)
+{
+	int i;
+	SW_D reg;
+#if (SW_SIZE == (2))
+	u16 *addr = (u16 *) mac_addr;
+#endif
+
+	if (0 == port)
+		reg = REG_PORT_0_MAC_ADDR_0;
+	else
+		reg = REG_PORT_1_MAC_ADDR_0;
+#if (SW_SIZE == (2))
+	for (i = 0; i < 6; i += 2) {
+		sw->reg->w16(sw, reg - i, htons(*addr));
+		addr++;
+	}
+#else
+	for (i = 0; i < 6; i++)
+		sw->reg->w8(sw, reg + i, mac_addr[i]);
+#endif
+	memcpy(sw->port_info[port].mac_addr, mac_addr, 6);
+}
+
+static inline void sw_setup_src_filter(struct ksz_sw *sw, u8 *mac_addr)
+{
+	uint port;
+
+	for (port = 0; port < SWITCH_PORT_NUM; port++) {
+		port_set_addr(sw, port, mac_addr);
+		port_cfg_src_filter_0(sw, port, 1);
+		port_cfg_src_filter_1(sw, port, 1);
+	}
+}
+
+static void sw_cfg_src_filter(struct ksz_sw *sw, bool set)
+{
+	uint port;
+
+	for (port = 0; port < SWITCH_PORT_NUM; port++) {
+		port_cfg_src_filter_0(sw, port, set);
+		port_cfg_src_filter_1(sw, port, set);
+	}
+}  /* sw_cfg_src_filter */
+
+/* -------------------------------------------------------------------------- */
+
+/**
+ * sw_set_tos_prio - program switch TOS priority
+ * @sw:		The switch instance.
+ * @tos:	ToS value from 6-bit (bit7 ~ bit2) of ToS field, ranging from 0
+ *		to 63.
+ * @prio:	Priority to be assigned.
+ *
+ * This routine programs the TOS priority into the switch registers.
+ */
+static void sw_set_tos_prio(struct ksz_sw *sw, u8 tos, SW_D prio)
+{
+	SW_W(sw, S_TOS_PRIO_CTRL + tos * SW_SIZE, prio);
+}
+
+/**
+ * sw_dis_diffserv - disable switch DiffServ priority
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ *
+ * This routine disables the DiffServ priority function of the switch.
+ */
+static void sw_dis_diffserv(struct ksz_sw *sw, uint port)
+{
+	port_cfg_diffserv(sw, port, 0);
+}
+
+/**
+ * sw_ena_diffserv - enable switch DiffServ priority
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ *
+ * This routine enables the DiffServ priority function of the switch.
+ */
+static void sw_ena_diffserv(struct ksz_sw *sw, uint port)
+{
+	port_cfg_diffserv(sw, port, 1);
+}
+
+/**
+ * hw_cfg_tos_prio - configure TOS priority
+ * @sw:		The switch instance.
+ * @tos:	ToS value from 6-bit (bit7 ~ bit2) of ToS field, ranging from 0
+ *		to 63.
+ * @prio:	Priority to be assigned.
+ *
+ * This routine configures the TOS priority in the hardware.
+ * DiffServ Value 0 ~ 63 is mapped to Priority Queue Number 0 ~ 3.
+ * It is called by user functions.  The hardware should be acquired first.
+ */
+static void hw_cfg_tos_prio(struct ksz_sw *sw, u8 tos, SW_D prio)
+{
+	int shift;
+	SW_D data = prio;
+	SW_D mask = KS_PRIO_M;
+
+	if (tos >= DIFFSERV_ENTRIES)
+		return;
+
+#if (SW_SIZE == (2))
+	if (prio >= 0x100)
+		mask = 0xffff;
+	else
+#endif
+	if (prio >= 0x10)
+		mask = 0xff;
+	else if (prio > KS_PRIO_M)
+		mask = 0xf;
+	shift = (tos & (KS_PRIO_IN_REG - 1)) * KS_PRIO_S;
+	prio = prio << shift;
+	if (prio >> shift != data)
+		return;
+	mask <<= shift;
+	tos /= KS_PRIO_IN_REG;
+
+	sw->info->diffserv[tos] &= ~mask;
+	sw->info->diffserv[tos] |= prio;
+
+	sw_set_tos_prio(sw, tos, sw->info->diffserv[tos]);
+}
+
+/* -------------------------------------------------------------------------- */
+
+/**
+ * sw_set_802_1p_prio - program switch 802.1p priority
+ * @sw:		The switch instance.
+ * @prio:	Priority to be assigned.
+ *
+ * This routine programs the 802.1p priority into the switch register.
+ */
+static void sw_set_802_1p_prio(struct ksz_sw *sw, u8 tag, SW_D prio)
+{
+	SW_W(sw, S_802_1P_PRIO_CTRL + tag / SW_SIZE, prio);
+}
+
+/**
+ * sw_dis_802_1p - disable switch 802.1p priority
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ *
+ * This routine disables the 802.1p priority function of the switch.
+ */
+static void sw_dis_802_1p(struct ksz_sw *sw, uint port)
+{
+	port_cfg_802_1p(sw, port, 0);
+}
+
+/**
+ * sw_ena_802_1p - enable switch 802.1p priority
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ *
+ * This routine enables the 802.1p priority function of the switch.
+ */
+static void sw_ena_802_1p(struct ksz_sw *sw, uint port)
+{
+	port_cfg_802_1p(sw, port, 1);
+}
+
+/**
+ * hw_cfg_802_1p_prio - configure 802.1p priority
+ * @sw:		The switch instance.
+ * @tag:	The 802.1p tag priority value, ranging from 0 to 7.
+ * @prio:	Priority to be assigned.
+ *
+ * This routine configures the 802.1p priority in the hardware.
+ * 802.1p Tag priority value 0 ~ 7 is mapped to Priority Queue Number 0 ~ 3.
+ * It is called by user functions.  The hardware should be acquired first.
+ */
+static void hw_cfg_802_1p_prio(struct ksz_sw *sw, u8 tag, SW_D prio)
+{
+	int shift;
+	SW_D data = prio;
+	SW_D mask = KS_PRIO_M;
+
+	if (tag >= PRIO_802_1P_ENTRIES)
+		return;
+
+#if (SW_SIZE == (2))
+	if (prio >= 0x100)
+		mask = 0xffff;
+	else
+#endif
+	if (prio >= 0x10)
+		mask = 0xff;
+	else if (prio > KS_PRIO_M)
+		mask = 0xf;
+	shift = (tag & (KS_PRIO_IN_REG - 1)) * KS_PRIO_S;
+	prio = prio << shift;
+	if (prio >> shift != data)
+		return;
+	mask <<= shift;
+	tag /= KS_PRIO_IN_REG;
+
+	sw->info->p_802_1p[tag] &= ~mask;
+	sw->info->p_802_1p[tag] |= prio;
+
+	sw_set_802_1p_prio(sw, tag, sw->info->p_802_1p[tag]);
+}
+
+/**
+ * sw_cfg_replace_null_vid -
+ * @sw:		The switch instance.
+ * @set:	The flag to disable or enable.
+ *
+ */
+static void sw_cfg_replace_null_vid(struct ksz_sw *sw, bool set)
+{
+	sw_cfg(sw, S_REPLACE_VID_CTRL, SWITCH_REPLACE_VID, set);
+}
+
+/**
+ * sw_cfg_replace_prio - enable switch 802.10 priority re-mapping
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ * @set:	The flag to disable or enable.
+ *
+ * This routine enables the 802.1p priority re-mapping function of the switch.
+ * That allows 802.1p priority field to be replaced with the port's default
+ * tag's priority value if the ingress packet's 802.1p priority has a higher
+ * priority than port's default tag's priority.
+ */
+static void sw_cfg_replace_prio(struct ksz_sw *sw, uint port, bool set)
+{
+	port_cfg_replace_prio(sw, port, set);
+}
+
+/* -------------------------------------------------------------------------- */
+
+/**
+ * sw_cfg_port_based - configure switch port based priority
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ * @prio:	The priority to set.
+ *
+ * This routine configures the port based priority of the switch.
+ */
+static void sw_cfg_port_based(struct ksz_sw *sw, uint port, u8 prio)
+{
+	SW_D data;
+
+	if (prio > PORT_BASED_PRIORITY_BASE)
+		prio = PORT_BASED_PRIORITY_BASE;
+
+	port_r(sw, port, P_PRIO_CTRL, &data);
+	data &= ~PORT_BASED_PRIORITY_MASK;
+	data |= prio << PORT_BASED_PRIORITY_SHIFT;
+	port_w(sw, port, P_PRIO_CTRL, data);
+
+	sw->info->port_cfg[port].port_prio = prio;
+}
+
+/* -------------------------------------------------------------------------- */
+
+/**
+ * sw_set_multi_queue - enable transmit multiple queues
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ * @queue:	The number of queues.
+ *
+ * This routine enables the transmit multiple queues selection of the switch
+ * port.  The port transmit queue is split into two or four priority queues.
+ */
+static void sw_set_multi_queue(struct ksz_sw *sw, uint port, int queue)
+{
+	switch (queue) {
+	case 4:
+	case 3:
+		port_cfg_2_queue(sw, port, 0);
+		port_cfg_4_queue(sw, port, 1);
+		break;
+	case 2:
+		port_cfg_2_queue(sw, port, 1);
+		port_cfg_4_queue(sw, port, 0);
+		break;
+	default:
+		port_cfg_2_queue(sw, port, 0);
+		port_cfg_4_queue(sw, port, 0);
+	}
+}  /* sw_set_multi_queue */
+
+/**
+ * sw_init_prio - initialize switch priority
+ * @sw:		The switch instance.
+ *
+ * This routine initializes the switch QoS priority functions.
+ */
+static void sw_init_prio(struct ksz_sw *sw)
+{
+	uint port;
+	int tos;
+	SW_D data;
+
+	for (tos = 0; tos < PRIO_802_1P_ENTRIES / KS_PRIO_IN_REG; tos++)
+		sw->info->p_802_1p[tos] =
+			SW_R(sw, S_802_1P_PRIO_CTRL + tos * SW_SIZE);
+
+	for (tos = 0; tos < DIFFSERV_ENTRIES / KS_PRIO_IN_REG; tos++)
+		sw->info->diffserv[tos] =
+			SW_R(sw, S_TOS_PRIO_CTRL + tos * SW_SIZE);
+
+	for (port = 0; port < TOTAL_PORT_NUM; port++) {
+		port_r(sw, port, P_PRIO_CTRL, &data);
+		data &= PORT_BASED_PRIORITY_MASK;
+		data >>= PORT_BASED_PRIORITY_SHIFT;
+		sw->info->port_cfg[port].port_prio = data;
+	}
+
+#ifdef USE_DIFF_PORT_PRIORITY
+/*
+ * THa  2012/02/01
+ * Port 3 sometimes cannot send frames out through the port where 100%
+ * wire-rate 64 bytes traffic also goes through.  Solution is to assign
+ * different port priorities.  It does not matter port 3 has higher priority,
+ * just different from the port where heavy traffic comes in.
+ */
+	for (port = 0; port < TOTAL_PORT_NUM; port++)
+		sw->info->port_cfg[port].port_prio = port;
+#endif
+#ifdef USE_DIFF_PORT_PRIORITY
+	for (port = 0; port < SWITCH_PORT_NUM; port++) {
+		int prio;
+
+		for (prio = 0; prio < PRIO_QUEUES; prio++) {
+			sw->info->port_cfg[port].rx_rate[prio] = 95000;
+			sw->info->port_cfg[port].tx_rate[prio] = 0;
+		}
+		sw_ena_rx_prio_rate(sw, port);
+	}
+#endif
+}
+
+/**
+ * sw_setup_prio - setup switch priority
+ * @sw:		The switch instance.
+ *
+ * This routine setup the switch QoS priority functions.
+ */
+static void sw_setup_prio(struct ksz_sw *sw)
+{
+	uint port;
+
+	/* All QoS functions disabled. */
+	for (port = 0; port < TOTAL_PORT_NUM; port++) {
+		sw_set_multi_queue(sw, port, 4);
+		sw_dis_diffserv(sw, port);
+		sw_cfg_replace_prio(sw, port, 0);
+		sw_cfg_port_based(sw, port, sw->info->port_cfg[port].port_prio);
+
+		sw_ena_802_1p(sw, port);
+	}
+	sw_cfg_replace_null_vid(sw, 0);
+}
+
+/* -------------------------------------------------------------------------- */
+
+/**
+ * port_cfg_def_vid - configure port default VID
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ * @vid:	The VID value.
+ *
+ * This routine configures the default VID of the port.
+ */
+static void port_cfg_def_vid(struct ksz_sw *sw, uint port, u16 vid)
+{
+	port_w16(sw, port, REG_PORT_CTRL_VID, vid);
+}
+
+/**
+ * port_get_def_vid - get port default VID.
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ * @vid:	Buffer to store the VID.
+ *
+ * This routine retrieves the default VID of the port.
+ */
+static void port_get_def_vid(struct ksz_sw *sw, uint port, u16 *vid)
+{
+	port_r16(sw, port, REG_PORT_CTRL_VID, vid);
+}
+
+/**
+ * sw_cfg_def_vid - configure switch port default VID
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ * @vid:	The VID value.
+ *
+ * This routine configures the default VID of the port.
+ */
+static void sw_cfg_def_vid(struct ksz_sw *sw, uint port, u16 vid)
+{
+	sw->info->port_cfg[port].vid = vid;
+	port_cfg_def_vid(sw, port, vid);
+}
+
+/**
+ * sw_cfg_port_base_vlan - configure port-based VLAN membership
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ * @member:	The port-based VLAN membership.
+ *
+ * This routine configures the port-based VLAN membership of the port.
+ */
+static void sw_cfg_port_base_vlan(struct ksz_sw *sw, uint port, u8 member)
+{
+	SW_D data;
+
+	port_r(sw, port, P_MIRROR_CTRL, &data);
+	data &= ~PORT_VLAN_MEMBERSHIP;
+	data |= (member & sw->PORT_MASK);
+	port_w(sw, port, P_MIRROR_CTRL, data);
+
+	sw->info->port_cfg[port].member = member;
+}
+
+/**
+ * sw_vlan_cfg_dis_non_vid - configure discard non VID
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ * @set:	The flag to disable or enable.
+ *
+ * This routine configures Discard Non VID packets of the switch port.
+ * If enabled, the device will discard packets whose VLAN id does not match
+ * ingress port-based default VLAN id.
+ */
+static void sw_vlan_cfg_dis_non_vid(struct ksz_sw *sw, uint port, bool set)
+{
+	port_cfg_dis_non_vid(sw, port, set);
+}
+
+/**
+ * sw_vlan_cfg_drop_tag - configure 802.1q tagged packet drop
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ * @set:	The flag to disable or enable.
+ *
+ */
+static void sw_vlan_cfg_drop_tag(struct ksz_sw *sw, uint port, bool set)
+{
+	port_cfg_drop_tag(sw, port, set);
+}
+
+/**
+ * sw_vlan_cfg_in_filter - configure ingress VLAN filtering
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ * @set:	The flag to disable or enable.
+ *
+ * This routine configures Ingress VLAN filtering of the switch port.
+ * If enabled, the device will discard packets whose VLAN id membership	in the
+ * VLAN table bits [18:16] does not include the ingress port that received this
+ * packet.
+ */
+static void sw_vlan_cfg_in_filter(struct ksz_sw *sw, uint port, bool set)
+{
+	port_cfg_in_filter(sw, port, set);
+}
+
+/**
+ * sw_vlan_cfg_ins_tag - configure 802.1q tag insertion
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ * @set:	The flag to disable or enable.
+ *
+ * This routine configures 802.1q Tag insertion to the switch port.
+ * If enabled, the device will insert 802.1q tag to the transmit packet on this
+ * port if received packet is an untagged packet.  The device will not insert
+ * 802.1q tag if received packet is tagged packet.
+ */
+static void sw_vlan_cfg_ins_tag(struct ksz_sw *sw, uint port, bool set)
+{
+	port_cfg_ins_tag(sw, port, set);
+}
+
+/**
+ * sw_vlan_cfg_rmv_tag - configure 802.1q tag removal
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ * @set:	The flag to disable or enable.
+ *
+ * This routine configures 802.1q Tag removal to the switch port.
+ * If enabled, the device will removed 802.1q tag to the transmit packet on
+ * this port if received packet is a tagged packet.  The device will not remove
+ * 802.1q tag if received packet is untagged packet.
+ */
+static void sw_vlan_cfg_rmv_tag(struct ksz_sw *sw, uint port, bool set)
+{
+	port_cfg_rmv_tag(sw, port, set);
+}
+
+#ifdef PORT_DOUBLE_TAG
+/**
+ * sw_vlan_cfg_double_tag - configure 802.1q double tag
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ * @set:	The flag to disable or enable.
+ *
+ */
+static void sw_vlan_cfg_double_tag(struct ksz_sw *sw, uint port, bool set)
+{
+	port_cfg_double_tag(sw, port, set);
+}
+#endif
+
+/**
+ * sw_dis_vlan - disable switch VLAN
+ * @sw:		The switch instance.
+ *
+ * This routine disables the VLAN function of the switch.
+ */
+static void sw_dis_vlan(struct ksz_sw *sw)
+{
+	sw_cfg(sw, S_MIRROR_CTRL, SWITCH_VLAN_ENABLE, 0);
+}
+
+/**
+ * sw_ena_vlan - enable switch VLAN
+ * @sw:		The switch instance.
+ *
+ * This routine enables the VLAN function of the switch.
+ */
+static void sw_ena_vlan(struct ksz_sw *sw)
+{
+	int entry;
+	struct ksz_sw_info *info = sw->info;
+
+	/* Create 16 VLAN entries in the VLAN table. */
+	sw->ops->release(sw);
+	for (entry = 0; entry < VLAN_TABLE_ENTRIES; entry++) {
+		sw_w_vlan_table(sw, entry,
+			info->vlan_table[entry].vid,
+			info->vlan_table[entry].fid,
+			info->vlan_table[entry].member,
+			info->vlan_table[entry].valid);
+	}
+	sw->ops->acquire(sw);
+
+	/* Enable 802.1q VLAN mode. */
+	sw_cfg(sw, REG_SWITCH_CTRL_2, UNICAST_VLAN_BOUNDARY, 1);
+	sw_cfg(sw, S_MIRROR_CTRL, SWITCH_VLAN_ENABLE, 1);
+}
+
+/**
+ * sw_init_vlan - initialize switch VLAN
+ * @sw:		The switch instance.
+ *
+ * This routine initializes the VLAN function of the switch.
+ */
+static void sw_init_vlan(struct ksz_sw *sw)
+{
+	uint port;
+	int entry;
+	SW_D data;
+	struct ksz_sw_info *info = sw->info;
+
+	/* Read 16 VLAN entries from device's VLAN table. */
+	sw->ops->release(sw);
+	for (entry = 0; entry < VLAN_TABLE_ENTRIES; entry++) {
+		if (!sw_r_vlan_table(sw, entry,
+				&info->vlan_table[entry].vid,
+				&info->vlan_table[entry].fid,
+				&info->vlan_table[entry].member))
+			info->vlan_table[entry].valid = 1;
+		else
+			info->vlan_table[entry].valid = 0;
+	}
+	sw->ops->acquire(sw);
+
+	for (port = 0; port < TOTAL_PORT_NUM; port++) {
+		port_get_def_vid(sw, port, &info->port_cfg[port].vid);
+		port_r(sw, port, P_MIRROR_CTRL, &data);
+		data &= PORT_VLAN_MEMBERSHIP;
+		info->port_cfg[port].member = data;
+		info->port_cfg[port].vid_member = data;
+	}
+	sw_cfg(sw, S_INS_SRC_PVID_CTRL,
+		(SWITCH_INS_TAG_1_PORT_2 | SWITCH_INS_TAG_1_PORT_3 |
+		SWITCH_INS_TAG_2_PORT_1 | SWITCH_INS_TAG_2_PORT_3 |
+		SWITCH_INS_TAG_3_PORT_1 | SWITCH_INS_TAG_3_PORT_2),
+		true);
+	info->vlan_table_used = 1;
+	info->fid_used = 1;
+}
+
+/**
+ * sw_get_addr - get the switch MAC address.
+ * @sw:		The switch instance.
+ * @mac_addr:	Buffer to store the MAC address.
+ *
+ * This function retrieves the MAC address of the switch.
+ */
+static inline void sw_get_addr(struct ksz_sw *sw, u8 *mac_addr)
+{
+	int i;
+#if (SW_SIZE == (2))
+	u16 *addr = (u16 *) mac_addr;
+
+	for (i = 0; i < 6; i += 2) {
+		*addr = sw->reg->r16(sw, REG_SWITCH_MAC_ADDR_0 + i);
+		*addr = ntohs(*addr);
+		addr++;
+	}
+#else
+
+	for (i = 0; i < 6; i++)
+		mac_addr[i] = sw->reg->r8(sw, REG_SWITCH_MAC_ADDR_0 + i);
+#endif
+	memcpy(sw->info->mac_addr, mac_addr, 6);
+}
+
+/**
+ * sw_set_addr - configure switch MAC address
+ * @sw:		The switch instance.
+ * @mac_addr:	The MAC address.
+ *
+ * This function configures the MAC address of the switch.
+ */
+static void sw_set_addr(struct ksz_sw *sw, u8 *mac_addr)
+{
+	int i;
+#if (SW_SIZE == (2))
+	u16 *addr = (u16 *) mac_addr;
+
+	for (i = 0; i < 6; i += 2) {
+		sw->reg->w16(sw, REG_SWITCH_MAC_ADDR_0 + i, htons(*addr));
+		addr++;
+	}
+#else
+
+	for (i = 0; i < 6; i++)
+		sw->reg->w8(sw, REG_SWITCH_MAC_ADDR_0 + i, mac_addr[i]);
+#endif
+	memcpy(sw->info->mac_addr, mac_addr, 6);
+}
+
+#ifdef SWITCH_PORT_PHY_ADDR_MASK
+/**
+ * sw_init_phy_addr - initialize switch PHY address
+ * @sw:		The switch instance.
+ *
+ * This function initializes the PHY address of the switch.
+ */
+static void sw_init_phy_addr(struct ksz_sw *sw)
+{
+	u8 addr;
+
+	addr = sw->reg->r8(sw, REG_SWITCH_CTRL_13);
+	addr >>= SWITCH_PORT_PHY_ADDR_SHIFT;
+	addr &= SWITCH_PORT_PHY_ADDR_MASK;
+	sw->info->phy_addr = addr;
+}
+
+/**
+ * sw_set_phy_addr - configure switch PHY address
+ * @sw:		The switch instance.
+ * @addr:	The PHY address.
+ *
+ * This function configures the PHY address of the switch.
+ */
+static void sw_set_phy_addr(struct ksz_sw *sw, u8 addr)
+{
+	sw->info->phy_addr = addr;
+	addr &= SWITCH_PORT_PHY_ADDR_MASK;
+	addr <<= SWITCH_PORT_PHY_ADDR_SHIFT;
+	sw->reg->w8(sw, REG_SWITCH_CTRL_13, addr);
+}
+#endif
+
+#define STP_ENTRY			0
+#define BROADCAST_ENTRY			1
+#define BRIDGE_ADDR_ENTRY		2
+#define IPV6_ADDR_ENTRY			3
+
+/**
+ * sw_set_global_ctrl - set switch global control
+ * @sw:		The switch instance.
+ *
+ * This routine sets the global control of the switch function.
+ */
+static void sw_set_global_ctrl(struct ksz_sw *sw)
+{
+	SW_D data;
+
+	/* Enable switch MII flow control. */
+	data = SW_R(sw, S_REPLACE_VID_CTRL);
+	data |= SWITCH_FLOW_CTRL;
+	SW_W(sw, S_REPLACE_VID_CTRL, data);
+
+	data = SW_R(sw, S_LINK_AGING_CTRL);
+	data |= SWITCH_LINK_AUTO_AGING;
+	SW_W(sw, S_LINK_AGING_CTRL, data);
+
+	data = SW_R(sw, REG_SWITCH_CTRL_1);
+
+	/* Enable aggressive back off algorithm in half duplex mode. */
+	data |= SWITCH_AGGR_BACKOFF;
+
+	/* Enable automatic fast aging when link changed detected. */
+	data |= SWITCH_AGING_ENABLE;
+
+	if (sw->overrides & FAST_AGING)
+		data |= SWITCH_FAST_AGING;
+	else
+		data &= ~SWITCH_FAST_AGING;
+
+	SW_W(sw, REG_SWITCH_CTRL_1, data);
+
+	data = SW_R(sw, REG_SWITCH_CTRL_2);
+
+	/* Make sure unicast VLAN boundary is set as default. */
+	if (sw->dev_count > 1)
+		data |= UNICAST_VLAN_BOUNDARY;
+
+	/* Enable no excessive collision drop. */
+	data |= NO_EXC_COLLISION_DROP;
+	SW_W(sw, REG_SWITCH_CTRL_2, data);
+
+#ifdef REG_ANA_CTRL_3
+	/* Enable LinkMD function. */
+	sw_w16(sw, REG_ANA_CTRL_3, 0x8008);
+#endif
+
+#ifdef CONFIG_KSZ_HSR
+	if (sw->features & HSR_HW) {
+		int n;
+
+		for (n = 0; n < sw->eth_cnt; n++) {
+			if (sw->eth_maps[n].proto & HSR_HW) {
+				if (sw->netdev[n]) {
+					sw->ops->release(sw);
+					sw->ops->cfg_mac(sw, BRIDGE_ADDR_ENTRY,
+						sw->netdev[n]->dev_addr,
+						sw->HOST_MASK, false, false, 0);
+					sw->ops->acquire(sw);
+				}
+				break;
+			}
+		}
+	}
+#endif
+}  /* sw_set_global_ctrl */
+
+/* -------------------------------------------------------------------------- */
+
+/**
+ * port_set_stp_state - configure port spanning tree state
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ * @state:	The spanning tree state.
+ *
+ * This routine configures the spanning tree state of the port.
+ */
+static void port_set_stp_state(struct ksz_sw *sw, uint port, int state)
+{
+	SW_D data;
+	struct ksz_port_cfg *port_cfg;
+	int member = -1;
+
+	port_cfg = get_port_cfg(sw, port);
+	port_r(sw, port, P_STP_CTRL, &data);
+	switch (state) {
+	case STP_STATE_DISABLED:
+		data &= ~(PORT_TX_ENABLE | PORT_RX_ENABLE);
+		data |= PORT_LEARN_DISABLE;
+		if (port < SWITCH_PORT_NUM)
+			member = 0;
+		break;
+	case STP_STATE_LISTENING:
+/*
+ * No need to turn on transmit because of port direct mode.
+ * Turning on receive is required if static MAC table is not setup.
+ */
+		data &= ~PORT_TX_ENABLE;
+		data |= PORT_RX_ENABLE;
+		data |= PORT_LEARN_DISABLE;
+		if (port < SWITCH_PORT_NUM &&
+		    STP_STATE_DISABLED == port_cfg->stp_state)
+			member = sw->HOST_MASK | port_cfg->vid_member;
+		break;
+	case STP_STATE_LEARNING:
+		data &= ~PORT_TX_ENABLE;
+		data |= PORT_RX_ENABLE;
+		data &= ~PORT_LEARN_DISABLE;
+		break;
+	case STP_STATE_FORWARDING:
+		data |= (PORT_TX_ENABLE | PORT_RX_ENABLE);
+		data &= ~PORT_LEARN_DISABLE;
+
+#ifdef CONFIG_KSZ_STP
+		/* Actual port membership setting is done in another RSTP
+		 * processing routine.
+		 */
+		if (sw->stp == 1 || (sw->stp && (sw->stp & (1 << port)))) {
+			struct ksz_stp_info *info = &sw->info->rstp;
+
+			if (info->br.bridgeEnabled)
+				break;
+		}
+#endif
+		if (((sw->features & SW_VLAN_DEV) || sw->dev_offset) &&
+		    port != sw->HOST_PORT)
+			/* Set port-base vlan membership with host port. */
+			member = sw->HOST_MASK | port_cfg->vid_member;
+		break;
+	case STP_STATE_BLOCKED:
+/*
+ * Need to setup static MAC table with override to keep receiving BPDU
+ * messages.  See sw_setup_stp routine.
+ */
+		data &= ~(PORT_TX_ENABLE | PORT_RX_ENABLE);
+		data |= PORT_LEARN_DISABLE;
+		if (port < SWITCH_PORT_NUM &&
+		    STP_STATE_DISABLED == port_cfg->stp_state)
+			member = sw->HOST_MASK | port_cfg->vid_member;
+		break;
+	case STP_STATE_SIMPLE:
+		data |= (PORT_TX_ENABLE | PORT_RX_ENABLE);
+		data |= PORT_LEARN_DISABLE;
+		if (port < SWITCH_PORT_NUM)
+			/* Set port-base vlan membership with host port. */
+			member = sw->HOST_MASK | port_cfg->vid_member;
+		break;
+	}
+	port_w(sw, port, P_STP_CTRL, data);
+	port_cfg->stp_state = state;
+	if (data & PORT_RX_ENABLE)
+		sw->rx_ports |= (1 << port);
+	else
+		sw->rx_ports &= ~(1 << port);
+	if (data & PORT_TX_ENABLE)
+		sw->tx_ports |= (1 << port);
+	else
+		sw->tx_ports &= ~(1 << port);
+
+	/* Port membership may share register with STP state. */
+	if (member >= 0)
+		sw_cfg_port_base_vlan(sw, port, (u8) member);
+}
+
+/**
+ * sw_clr_sta_mac_table - clear static MAC table
+ * @sw:		The switch instance.
+ *
+ * This routine clears the static MAC table.
+ */
+static void sw_clr_sta_mac_table(struct ksz_sw *sw)
+{
+	struct ksz_mac_table *entry;
+	int i;
+
+	for (i = 0; i < STATIC_MAC_TABLE_ENTRIES; i++) {
+		entry = &sw->info->mac_table[i];
+		sw_w_sta_mac_table(sw, i,
+			entry->mac_addr, entry->ports,
+			0, 0,
+			entry->use_fid, entry->fid);
+	}
+}
+
+/**
+ * sw_setup_stp - setup switch spanning tree support
+ * @sw:		The switch instance.
+ *
+ * This routine initializes the spanning tree support of the switch.
+ */
+static void sw_setup_stp(struct ksz_sw *sw)
+{
+	struct ksz_mac_table *entry;
+	struct ksz_alu_table *alu;
+	struct ksz_sw_info *info = sw->info;
+
+	entry = &info->mac_table[STP_ENTRY];
+	entry->mac_addr[0] = 0x01;
+	entry->mac_addr[1] = 0x80;
+	entry->mac_addr[2] = 0xC2;
+	entry->mac_addr[3] = 0x00;
+	entry->mac_addr[4] = 0x00;
+	entry->mac_addr[5] = 0x00;
+	entry->ports = sw->HOST_MASK;
+	entry->override = 1;
+	entry->valid = 1;
+	alu = &info->alu_table[STP_ENTRY];
+	alu->forward = FWD_STP_DEV | FWD_HOST | FWD_HOST_OVERRIDE;
+	alu->owner = sw->PORT_MASK;
+	alu->valid = 1;
+	sw_w_sta_mac_table(sw, STP_ENTRY,
+		entry->mac_addr, entry->ports,
+		entry->override, entry->valid,
+		entry->use_fid, entry->fid);
+
+	entry = &info->mac_table[BROADCAST_ENTRY];
+	memset(entry->mac_addr, 0xFF, ETH_ALEN);
+	entry->ports = sw->HOST_MASK;
+	entry->override = 0;
+	entry->valid = 1;
+	alu = &sw->info->alu_table[BROADCAST_ENTRY];
+	alu->forward = FWD_MAIN_DEV | FWD_STP_DEV | FWD_HOST;
+	alu->owner = sw->PORT_MASK;
+	alu->valid = 1;
+
+	entry = &info->mac_table[BRIDGE_ADDR_ENTRY];
+	memcpy(entry->mac_addr, info->mac_addr, ETH_ALEN);
+	entry->ports = sw->HOST_MASK;
+	entry->override = 0;
+	entry->valid = 1;
+	alu = &sw->info->alu_table[BRIDGE_ADDR_ENTRY];
+	alu->forward = FWD_MAIN_DEV | FWD_HOST;
+	alu->owner = sw->PORT_MASK;
+	alu->valid = 1;
+
+	entry = &info->mac_table[IPV6_ADDR_ENTRY];
+	memcpy(entry->mac_addr, info->mac_addr, ETH_ALEN);
+	entry->mac_addr[0] = 0x33;
+	entry->mac_addr[1] = 0x33;
+	entry->mac_addr[2] = 0xFF;
+	entry->ports = sw->HOST_MASK;
+	entry->override = 0;
+	entry->valid = 1;
+	alu = &sw->info->alu_table[IPV6_ADDR_ENTRY];
+	alu->forward = FWD_MAIN_DEV | FWD_STP_DEV | FWD_HOST;
+	alu->owner = sw->PORT_MASK;
+	alu->valid = 1;
+}
+
+#ifdef CONFIG_KSZ_STP
+static void bridge_change(struct ksz_sw *sw)
+{
+	uint port;
+	u8 member;
+	struct ksz_sw_info *info = sw->info;
+
+	for (port = 0; port < SWITCH_PORT_NUM; port++) {
+		if (STP_STATE_FORWARDING == info->port_cfg[port].stp_state)
+			member = sw->HOST_MASK | info->member[0];
+		else if (STP_STATE_DISABLED == info->port_cfg[port].stp_state)
+			member = 0;
+		else
+			member = sw->HOST_MASK | (1 << port);
+		if (member != info->port_cfg[port].member)
+			sw_cfg_port_base_vlan(sw, port, member);
+	}
+}  /* bridge_change */
+#endif
+
+#define MAX_SW_LEN			1500
+
+#ifdef CONFIG_KSZ_STP
+#include "ksz_stp.c"
+#endif
+#ifdef CONFIG_KSZ_DLR
+#include "ksz_dlr.c"
+#endif
+#ifdef CONFIG_KSZ_HSR
+#include "ksz_hsr.c"
+#endif
+
+/*
+ * Link detection routines
+ */
+
+static inline void dbp_link(struct ksz_port *port, struct ksz_sw *sw,
+	int change)
+{
+	struct ksz_port_info *info;
+	uint i;
+	uint n;
+	uint p;
+
+	for (i = 0, n = port->first_port; i < port->port_cnt; i++, n++) {
+		p = get_phy_port(sw, n);
+		info = get_port_info(sw, p);
+		if (media_connected == info->state) {
+			if (change & (1 << i)) {
+				printk(KERN_INFO "link %d-%d: %d, %d\n",
+					sw->id, i + port->first_port,
+					info->tx_rate / TX_RATE_UNIT,
+					info->duplex);
+			}
+		} else {
+			if (change & (1 << i))
+				printk(KERN_INFO "link %d-%d disconnected\n",
+					sw->id, i + port->first_port);
+		}
+	}
+}
+
+static SW_D port_advertised_flow_ctrl(struct ksz_port *port, SW_D ctrl)
+{
+	ctrl &= ~PORT_AUTO_NEG_SYM_PAUSE;
+	switch (port->flow_ctrl) {
+	case PHY_FLOW_CTRL:
+		ctrl |= PORT_AUTO_NEG_SYM_PAUSE;
+		break;
+	/* Not supported. */
+	case PHY_TX_ONLY:
+	case PHY_RX_ONLY:
+	default:
+		break;
+	}
+	return ctrl;
+}
+
+static u8 sw_determine_flow_ctrl(struct ksz_sw *sw, struct ksz_port *port,
+	u8 local, u8 remote)
+{
+	int rx;
+	int tx;
+	u8 flow = 0;
+
+	if (sw->overrides & PAUSE_FLOW_CTRL)
+		return flow;
+
+	rx = tx = 0;
+	if (port->force_link)
+		rx = tx = 1;
+	if (remote & PORT_REMOTE_SYM_PAUSE) {
+		if (local & PORT_AUTO_NEG_SYM_PAUSE)
+			rx = tx = 1;
+	}
+	if (rx)
+		flow |= 0x01;
+	if (tx)
+		flow |= 0x02;
+#ifdef DEBUG
+	printk(KERN_INFO "pause: %d, %d; %02x %02x\n",
+		rx, tx, local, remote);
+#endif
+	return flow;
+}
+
+static void sw_notify_link_change(struct ksz_sw *sw, uint ports)
+{
+	static u8 link_buf[sizeof(struct ksz_info_opt) +
+		sizeof(struct ksz_info_speed) * TOTAL_PORT_NUM +
+		sizeof(struct ksz_resp_msg)];
+
+	if ((sw->notifications & SW_INFO_LINK_CHANGE)) {
+		struct ksz_resp_msg *msg = (struct ksz_resp_msg *) link_buf;
+		struct ksz_info_opt *opt = (struct ksz_info_opt *)
+			&msg->resp.data;
+		struct ksz_port_info *info;
+		struct ksz_info_speed *speed;
+		struct file_dev_info *dev_info;
+		int c;
+		int n;
+		int p;
+		int q;
+
+		/* Check whether only 1 port has change. */
+		c = 0;
+		q = 0;
+		for (n = 1; n <= sw->mib_port_cnt; n++) {
+			p = get_phy_port(sw, n);
+			if (ports & (1 << p)) {
+				q = n;
+				c++;
+			}
+		}
+		if (c > 1) {
+			c = sw->mib_port_cnt;
+			q = 1;
+		}
+		msg->module = DEV_MOD_BASE;
+		msg->cmd = DEV_INFO_SW_LINK;
+		opt->num = (u8) c;
+		opt->port = (u8) q;
+		speed = &opt->data.speed;
+		for (n = 1; n <= opt->num; n++, q++) {
+			p = get_phy_port(sw, q);
+			info = get_port_info(sw, p);
+			if (info->state == media_connected) {
+				speed->tx_rate = info->tx_rate;
+				speed->duplex = info->duplex;
+				speed->flow_ctrl = info->flow_ctrl;
+			} else {
+				speed->tx_rate = 0;
+				speed->duplex = 0;
+				speed->flow_ctrl = 0;
+			}
+			++speed;
+		}
+		n = opt->num * sizeof(struct ksz_info_speed);
+		n += 2;
+		n += sizeof(struct ksz_resp_msg);
+		n -= 4;
+		dev_info = sw->dev_list[0];
+		while (dev_info) {
+			if ((dev_info->notifications[DEV_MOD_BASE] &
+			    SW_INFO_LINK_CHANGE))
+				file_dev_setup_msg(dev_info, msg, n, NULL,
+						   NULL);
+			dev_info = dev_info->next;
+		}
+	}
+}  /* sw_notify_link_change */
+
+#ifndef NO_PHYDEV
+#ifdef CONFIG_HAVE_KSZ8463
+#define KSZ8463_ID_HI		0x0022
+
+#define KSZ8463_SW_ID		0x8463
+#define PHY_ID_KSZ_SW		((KSZ8463_ID_HI << 16) | KSZ8463_SW_ID)
+
+static char *kszsw_phy_driver_names[] = {
+	"Microchip KSZ8463 Switch",
+};
+#endif
+
+#ifdef CONFIG_HAVE_KSZ8863
+#define KSZ8863_SW_ID		0x8863
+#define PHY_ID_KSZ_SW		((KSZ8863_ID_HI << 16) | KSZ8863_SW_ID)
+
+static char *kszsw_phy_driver_names[] = {
+	"Microchip KSZ8863 Switch",
+	"Microchip KSZ8873 Switch",
+};
+#endif
+
+#ifdef CONFIG_HAVE_KSZ8463
+/*
+ * Tha  2011/03/11
+ * The hardware register reads low word first of PHY id instead of high word.
+ */
+static inline int actual_reg(int regnum)
+{
+	if (2 == regnum)
+		regnum = 3;
+	else if (3 == regnum)
+		regnum = 2;
+	return regnum;
+}
+
+/**
+ * sw_r_phy - read data from PHY register
+ * @sw:		The switch instance.
+ * @phy:	PHY address to read.
+ * @reg:	PHY register to read.
+ * @val:	Buffer to store the read data.
+ *
+ * This routine reads data from the PHY register.
+ */
+static void sw_r_phy(struct ksz_sw *sw, u16 phy, u16 reg, u16 *val)
+{
+	u16 base;
+	u16 data = 0;
+
+	reg = actual_reg(reg);
+	if (2 == phy)
+		base = PHY2_REG_CTRL;
+	else
+		base = PHY1_REG_CTRL;
+	data = sw->reg->r16(sw, base + reg * 2);
+	if (2 == reg)
+		data = KSZ8463_SW_ID;
+	*val = data;
+}  /* sw_r_phy */
+
+/**
+ * sw_w_phy - write data to PHY register
+ * @hw:		The switch instance.
+ * @phy:	PHY address to write.
+ * @reg:	PHY register to write.
+ * @val:	Word data to write.
+ *
+ * This routine writes data to the PHY register.
+ */
+static void sw_w_phy(struct ksz_sw *sw, u16 phy, u16 reg, u16 val)
+{
+	u16 base;
+
+	if (2 == phy)
+		base = PHY2_REG_CTRL;
+	else
+		base = PHY1_REG_CTRL;
+	sw->reg->w16(sw, base + reg * 2, val);
+}  /* sw_w_phy */
+#endif
+
+#ifdef CONFIG_HAVE_KSZ8863
+/**
+ * sw_r_phy - read data from PHY register
+ * @sw:		The switch instance.
+ * @phy:	PHY address to read.
+ * @reg:	PHY register to read.
+ * @val:	Buffer to store the read data.
+ *
+ * This routine reads data from the PHY register.
+ */
+static void sw_r_phy(struct ksz_sw *sw, u16 phy, u16 reg, u16 *val)
+{
+	u8 ctrl;
+	u8 restart;
+	u8 link;
+	u8 speed;
+	u8 p;
+	u16 data = 0;
+
+	if (phy) {
+		p = phy - 1;
+	} else {
+		switch (reg) {
+		case PHY_REG_CTRL:
+			data = 0x1140;
+			break;
+		case PHY_REG_STATUS:
+			data = 0x7808;
+			break;
+		case PHY_REG_ID_1:
+			data = KSZ8863_ID_HI;
+			break;
+		case PHY_REG_ID_2:
+			data = KSZ8863_SW_ID;
+			break;
+		case PHY_REG_AUTO_NEGOTIATION:
+			data = 0x05e1;
+			break;
+		case PHY_REG_REMOTE_CAPABILITY:
+			data = 0xc5e1;
+			break;
+		}
+		*val = data;
+		return;
+	}
+	switch (reg) {
+	case PHY_REG_CTRL:
+		port_r(sw, p, P_PHY_CTRL, &ctrl);
+		port_r(sw, p, P_NEG_RESTART_CTRL, &restart);
+		port_r(sw, p, P_SPEED_STATUS, &speed);
+		if (restart & PORT_LOOPBACK)
+			data |= PHY_LOOPBACK;
+		if (ctrl & PORT_FORCE_100_MBIT)
+			data |= PHY_SPEED_100MBIT;
+		if (ctrl & PORT_AUTO_NEG_ENABLE)
+			data |= PHY_AUTO_NEG_ENABLE;
+		if (restart & PORT_POWER_DOWN)
+			data |= PHY_POWER_DOWN;
+		if (restart & PORT_AUTO_NEG_RESTART)
+			data |= PHY_AUTO_NEG_RESTART;
+		if (ctrl & PORT_FORCE_FULL_DUPLEX)
+			data |= PHY_FULL_DUPLEX;
+		if (speed & PORT_HP_MDIX)
+			data |= PHY_HP_MDIX;
+		if (restart & PORT_FORCE_MDIX)
+			data |= PHY_FORCE_MDIX;
+		if (restart & PORT_AUTO_MDIX_DISABLE)
+			data |= PHY_AUTO_MDIX_DISABLE;
+		if (restart & PORT_REMOTE_FAULT_DISABLE)
+			data |= PHY_REMOTE_FAULT_DISABLE;
+		if (restart & PORT_TX_DISABLE)
+			data |= PHY_TRANSMIT_DISABLE;
+		if (restart & PORT_LED_OFF)
+			data |= PHY_LED_DISABLE;
+		break;
+	case PHY_REG_STATUS:
+		port_r(sw, p, P_LINK_STATUS, &link);
+		port_r(sw, p, P_SPEED_STATUS, &speed);
+		data = PHY_100BTX_FD_CAPABLE |
+			PHY_100BTX_CAPABLE |
+			PHY_10BT_FD_CAPABLE |
+			PHY_10BT_CAPABLE |
+			PHY_AUTO_NEG_CAPABLE;
+		if (link & PORT_AUTO_NEG_COMPLETE)
+			data |= PHY_AUTO_NEG_ACKNOWLEDGE;
+		if (link & PORT_STATUS_LINK_GOOD)
+			data |= PHY_LINK_STATUS;
+		if (speed & PORT_REMOTE_FAULT)
+			data |= PHY_REMOTE_FAULT;
+		break;
+	case PHY_REG_ID_1:
+		data = 0x0022;
+		break;
+	case PHY_REG_ID_2:
+		/* Use unique switch id to differentiate from regular PHY. */
+		data = KSZ8863_SW_ID;
+		break;
+	case PHY_REG_AUTO_NEGOTIATION:
+		port_r(sw, p, P_PHY_CTRL, &ctrl);
+		data = PHY_AUTO_NEG_802_3;
+		if (ctrl & PORT_AUTO_NEG_SYM_PAUSE)
+			data |= PHY_AUTO_NEG_SYM_PAUSE;
+		if (ctrl & PORT_AUTO_NEG_100BTX_FD)
+			data |= PHY_AUTO_NEG_100BTX_FD;
+		if (ctrl & PORT_AUTO_NEG_100BTX)
+			data |= PHY_AUTO_NEG_100BTX;
+		if (ctrl & PORT_AUTO_NEG_10BT_FD)
+			data |= PHY_AUTO_NEG_10BT_FD;
+		if (ctrl & PORT_AUTO_NEG_10BT)
+			data |= PHY_AUTO_NEG_10BT;
+		break;
+	case PHY_REG_REMOTE_CAPABILITY:
+		port_r(sw, p, P_LINK_STATUS, &link);
+		data = PHY_AUTO_NEG_802_3;
+		if (link & PORT_REMOTE_SYM_PAUSE)
+			data |= PHY_AUTO_NEG_SYM_PAUSE;
+		if (link & PORT_REMOTE_100BTX_FD)
+			data |= PHY_AUTO_NEG_100BTX_FD;
+		if (link & PORT_REMOTE_100BTX)
+			data |= PHY_AUTO_NEG_100BTX;
+		if (link & PORT_REMOTE_10BT_FD)
+			data |= PHY_AUTO_NEG_10BT_FD;
+		if (link & PORT_REMOTE_10BT)
+			data |= PHY_AUTO_NEG_10BT;
+		break;
+	default:
+		break;
+	}
+	*val = data;
+}  /* sw_r_phy */
+
+/**
+ * sw_w_phy - write data to PHY register
+ * @hw:		The switch instance.
+ * @phy:	PHY address to write.
+ * @reg:	PHY register to write.
+ * @val:	Word data to write.
+ *
+ * This routine writes data to the PHY register.
+ */
+static void sw_w_phy(struct ksz_sw *sw, u16 phy, u16 reg, u16 val)
+{
+	u8 ctrl;
+	u8 restart;
+	u8 speed;
+	u8 data;
+	u8 p;
+
+	if (phy)
+		p = phy - 1;
+	else
+		return;
+	switch (reg) {
+	case PHY_REG_CTRL:
+		port_r(sw, p, P_SPEED_STATUS, &speed);
+		data = speed;
+		if (val & PHY_HP_MDIX)
+			data |= PORT_HP_MDIX;
+		else
+			data &= ~PORT_HP_MDIX;
+		if (data != speed)
+			port_w(sw, p, P_SPEED_STATUS, data);
+		port_r(sw, p, P_PHY_CTRL, &ctrl);
+		data = ctrl;
+		if (val & PHY_AUTO_NEG_ENABLE)
+			data |= PORT_AUTO_NEG_ENABLE;
+		else
+			data &= ~PORT_AUTO_NEG_ENABLE;
+		if (val & PHY_SPEED_100MBIT)
+			data |= PORT_FORCE_100_MBIT;
+		else
+			data &= ~PORT_FORCE_100_MBIT;
+		if (val & PHY_FULL_DUPLEX)
+			data |= PORT_FORCE_FULL_DUPLEX;
+		else
+			data &= ~PORT_FORCE_FULL_DUPLEX;
+		if (data != ctrl)
+			port_w(sw, p, P_PHY_CTRL, data);
+		port_r(sw, p, P_NEG_RESTART_CTRL, &restart);
+		data = restart;
+		if (val & PHY_LED_DISABLE)
+			data |= PORT_LED_OFF;
+		else
+			data &= ~PORT_LED_OFF;
+		if (val & PHY_TRANSMIT_DISABLE)
+			data |= PORT_TX_DISABLE;
+		else
+			data &= ~PORT_TX_DISABLE;
+		if (val & PHY_AUTO_NEG_RESTART)
+			data |= PORT_AUTO_NEG_RESTART;
+		else
+			data &= ~(PORT_AUTO_NEG_RESTART);
+		if (val & PHY_REMOTE_FAULT_DISABLE)
+			data |= PORT_REMOTE_FAULT_DISABLE;
+		else
+			data &= ~PORT_REMOTE_FAULT_DISABLE;
+		if (val & PHY_POWER_DOWN)
+			data |= PORT_POWER_DOWN;
+		else
+			data &= ~PORT_POWER_DOWN;
+		if (val & PHY_AUTO_MDIX_DISABLE)
+			data |= PORT_AUTO_MDIX_DISABLE;
+		else
+			data &= ~PORT_AUTO_MDIX_DISABLE;
+		if (val & PHY_FORCE_MDIX)
+			data |= PORT_FORCE_MDIX;
+		else
+			data &= ~PORT_FORCE_MDIX;
+		if (val & PHY_LOOPBACK)
+			data |= PORT_LOOPBACK;
+		else
+			data &= ~PORT_LOOPBACK;
+		if (data != restart)
+			port_w(sw, p, P_NEG_RESTART_CTRL, data);
+		break;
+	case PHY_REG_AUTO_NEGOTIATION:
+		port_r(sw, p, P_PHY_CTRL, &ctrl);
+		data = ctrl;
+		data &= ~(PORT_AUTO_NEG_SYM_PAUSE |
+			PORT_AUTO_NEG_100BTX_FD |
+			PORT_AUTO_NEG_100BTX |
+			PORT_AUTO_NEG_10BT_FD |
+			PORT_AUTO_NEG_10BT);
+		if (val & PHY_AUTO_NEG_SYM_PAUSE)
+			data |= PORT_AUTO_NEG_SYM_PAUSE;
+		if (val & PHY_AUTO_NEG_100BTX_FD)
+			data |= PORT_AUTO_NEG_100BTX_FD;
+		if (val & PHY_AUTO_NEG_100BTX)
+			data |= PORT_AUTO_NEG_100BTX;
+		if (val & PHY_AUTO_NEG_10BT_FD)
+			data |= PORT_AUTO_NEG_10BT_FD;
+		if (val & PHY_AUTO_NEG_10BT)
+			data |= PORT_AUTO_NEG_10BT;
+		if (data != ctrl)
+			port_w(sw, p, P_PHY_CTRL, data);
+		break;
+	default:
+		break;
+	}
+}  /* sw_w_phy */
+#endif
+#endif
+
+static int port_chk_force_link(struct ksz_sw *sw, uint p, SW_D data,
+	SW_D status)
+{
+#define PORT_REMOTE_STATUS				\
+	(PORT_REMOTE_100BTX_FD | PORT_REMOTE_100BTX |	\
+	PORT_REMOTE_10BT_FD | PORT_REMOTE_10BT)
+
+	static SW_D saved_ctrl;
+	static SW_D saved_status;
+	static int test_stage;
+
+	if (!(data & PORT_AUTO_NEG_ENABLE))
+		return 0;
+	if (!(status & PORT_REMOTE_SYM_PAUSE) &&
+	    (status & PORT_REMOTE_STATUS) != PORT_REMOTE_STATUS) {
+		if (saved_ctrl) {
+			if ((status & PORT_STAT_FULL_DUPLEX) !=
+			    (saved_status & PORT_STAT_FULL_DUPLEX)) {
+				printk(KERN_INFO
+					"%d-%d: duplex is defaulted to %s\n",
+					sw->id, p,
+					(saved_ctrl & PORT_FORCE_FULL_DUPLEX) ?
+					"full" : "half");
+			}
+			if (data != saved_ctrl) {
+				saved_ctrl |= PORT_AUTO_NEG_RESTART;
+				port_w(sw, p, P_PHY_CTRL, saved_ctrl);
+				test_stage = 2;
+			} else
+				test_stage = 0;
+			saved_ctrl = 0;
+			if (test_stage)
+				return 1;
+		} else if (!test_stage) {
+			saved_ctrl = data;
+			saved_status = status;
+			if (status & PORT_STAT_FULL_DUPLEX)
+				data &= ~PORT_FORCE_FULL_DUPLEX;
+			else
+				data |= PORT_FORCE_FULL_DUPLEX;
+			data |= PORT_AUTO_NEG_RESTART;
+			port_w(sw, p, P_PHY_CTRL, data);
+			test_stage = 1;
+			return 1;
+		}
+		test_stage = 0;
+	}
+	return 0;
+}
+
+/**
+ * port_get_link_speed - get current link status
+ * @port:	The port instance.
+ *
+ * This routine reads PHY registers to determine the current link status of the
+ * switch ports.
+ */
+static int port_get_link_speed(struct ksz_port *port)
+{
+	struct ksz_port_info *info;
+	struct ksz_port_info *linked = NULL;
+	struct ksz_port_state *state;
+	struct ksz_sw *sw = port->sw;
+	SW_D data;
+	SW_D status;
+	u8 local;
+	u8 remote;
+	uint i;
+	uint n;
+	uint p;
+	int change = 0;
+
+	for (i = 0, n = port->first_port; i < port->port_cnt; i++, n++) {
+		p = get_phy_port(sw, n);
+		info = get_port_info(sw, p);
+		state = &sw->port_state[p];
+		port_r(sw, p, P_PHY_CTRL, &data);
+		port_r(sw, p, P_SPEED_STATUS, &status);
+
+#if (SW_SIZE == (1))
+		port_r(sw, p, P_LINK_STATUS, &remote);
+#else
+		remote = status;
+#endif
+		remote &= ~PORT_MDIX_STATUS;
+		local = (u8) data;
+
+		if (remote & PORT_STATUS_LINK_GOOD) {
+
+			/* Remember the first linked port. */
+			if (!linked)
+				linked = info;
+		}
+
+		/* No change to status. */
+		if (local == info->advertised && remote == info->partner)
+			continue;
+
+#ifdef DEBUG
+		printk(KERN_INFO "advertised: %02X-%02X; partner: %02X-%02X\n",
+			local, info->advertised, remote, info->partner);
+#endif
+		if (remote & PORT_STATUS_LINK_GOOD) {
+			if (port_chk_force_link(sw, p, data, status)) {
+				if (linked == info)
+					linked = NULL;
+				continue;
+			}
+			info->tx_rate = 10 * TX_RATE_UNIT;
+			if (status & PORT_STAT_SPEED_100MBIT)
+				info->tx_rate = 100 * TX_RATE_UNIT;
+
+			info->duplex = 1;
+			if (status & PORT_STAT_FULL_DUPLEX)
+				info->duplex = 2;
+
+#ifdef DEBUG
+			printk(KERN_INFO "flow_ctrl: "SW_SIZE_STR"\n", status &
+				(PORT_RX_FLOW_CTRL | PORT_TX_FLOW_CTRL));
+#endif
+			if (media_connected != info->state) {
+				info->flow_ctrl = sw_determine_flow_ctrl(sw,
+					port, local, remote);
+				if (status & PORT_RX_FLOW_CTRL)
+					info->flow_ctrl |= 0x10;
+				if (status & PORT_TX_FLOW_CTRL)
+					info->flow_ctrl |= 0x20;
+				if (sw->info)
+					port_cfg_back_pressure(sw, p,
+						(1 == info->duplex));
+				change |= 1 << i;
+			}
+			info->state = media_connected;
+#ifndef NO_PHYDEV
+			sw_r_phy(sw, p + 1, PHY_REG_REMOTE_CAPABILITY,
+				 &info->lpa);
+#endif
+		} else {
+			if (media_disconnected != info->state) {
+				change |= 1 << i;
+
+				/* Indicate the link just goes down. */
+				state->link_down = 1;
+			}
+			info->state = media_disconnected;
+		}
+		info->advertised = local;
+		info->partner = remote;
+		state->state = info->state;
+	}
+
+	if (linked && media_disconnected == port->linked->state)
+		port->linked = linked;
+
+#ifdef DEBUG
+	if (change)
+		dbp_link(port, sw, change);
+#endif
+	if (change) {
+		port->report = true;
+		port->link_ports |= change;
+	}
+	return change;
+}  /* port_get_link_speed */
+
+/**
+ * port_set_link_speed - set port speed
+ * @port:	The port instance.
+ *
+ * This routine sets the link speed of the switch ports.
+ */
+static void port_set_link_speed(struct ksz_port *port)
+{
+	struct ksz_sw *sw = port->sw;
+	struct ksz_port_info *info;
+	SW_D data;
+	SW_D cfg;
+	SW_D status;
+	uint i;
+	uint n;
+	uint p;
+
+	for (i = 0, n = port->first_port; i < port->port_cnt; i++, n++) {
+		p = get_phy_port(sw, n);
+		info = get_port_info(sw, p);
+		if (info->fiber)
+			continue;
+
+		info->own_flow_ctrl = port->flow_ctrl;
+		info->own_duplex = port->duplex;
+		info->own_speed = port->speed;
+
+		port_r(sw, p, P_PHY_CTRL, &data);
+		port_r(sw, p, P_LINK_STATUS, &status);
+
+		cfg = 0;
+		if (status & PORT_STATUS_LINK_GOOD)
+			cfg = data;
+
+		data |= PORT_AUTO_NEG_ENABLE;
+		data = port_advertised_flow_ctrl(port, data);
+
+		data |= PORT_AUTO_NEG_100BTX_FD | PORT_AUTO_NEG_100BTX |
+			PORT_AUTO_NEG_10BT_FD | PORT_AUTO_NEG_10BT;
+
+		/* Check if manual configuration is specified by the user. */
+		if (port->speed || port->duplex) {
+			if (10 == port->speed)
+				data &= ~(PORT_AUTO_NEG_100BTX_FD |
+					PORT_AUTO_NEG_100BTX);
+			else if (100 == port->speed)
+				data &= ~(PORT_AUTO_NEG_10BT_FD |
+					PORT_AUTO_NEG_10BT);
+			if (1 == port->duplex)
+				data &= ~(PORT_AUTO_NEG_100BTX_FD |
+					PORT_AUTO_NEG_10BT_FD);
+			else if (2 == port->duplex)
+				data &= ~(PORT_AUTO_NEG_100BTX |
+					PORT_AUTO_NEG_10BT);
+		}
+		if (data != cfg) {
+#if (SW_SIZE == (1))
+			port_w(sw, p, P_PHY_CTRL, data);
+			port_r(sw, p, P_NEG_RESTART_CTRL, &data);
+#endif
+			data |= PORT_AUTO_NEG_RESTART;
+			port_w(sw, p, P_NEG_RESTART_CTRL, data);
+
+			/* Link is going down. */
+			sw->port_state[p].state = media_disconnected;
+		}
+	}
+}
+
+/**
+ * port_force_link_speed - force port speed
+ * @port:	The port instance.
+ *
+ * This routine forces the link speed of the switch ports.
+ */
+static void port_force_link_speed(struct ksz_port *port)
+{
+	struct ksz_sw *sw = port->sw;
+	struct ksz_port_info *info;
+	SW_D data;
+	uint i;
+	uint n;
+	uint p;
+
+	for (i = 0, n = port->first_port; i < port->port_cnt; i++, n++) {
+		p = get_phy_port(sw, n);
+		info = get_port_info(sw, p);
+		info->own_flow_ctrl = port->flow_ctrl;
+		info->own_duplex = port->duplex;
+		info->own_speed = port->speed;
+
+		port_r(sw, p, P_PHY_CTRL, &data);
+		data &= ~PORT_AUTO_NEG_ENABLE;
+		if (10 == port->speed)
+			data &= ~PORT_FORCE_100_MBIT;
+		else if (100 == port->speed)
+			data |= PORT_FORCE_100_MBIT;
+		if (1 == port->duplex)
+			data &= ~PORT_FORCE_FULL_DUPLEX;
+		else if (2 == port->duplex)
+			data |= PORT_FORCE_FULL_DUPLEX;
+		port_w(sw, p, P_PHY_CTRL, data);
+	}
+}
+
+/**
+ * sw_enable - enable the switch
+ * @sw:		The switch instance.
+ *
+ * This routine enables the switch with specific configuration.
+ */
+static void sw_enable(struct ksz_sw *sw)
+{
+	uint n;
+	uint port;
+	int state = STP_STATE_FORWARDING;
+
+	if ((sw->dev_count > 1 && !sw->dev_offset) ||
+	    (sw->features & STP_SUPPORT)) {
+		u8 member;
+
+		for (n = 1; n <= SWITCH_PORT_NUM; n++) {
+			port = get_phy_port(sw, n);
+			member = (1 << port);
+			if (sw->features & SW_VLAN_DEV) {
+				struct ksz_dev_map *map;
+				int q;
+
+				for (q = 0; q < sw->eth_cnt; q++) {
+					map = &sw->eth_maps[q];
+					if (map->first <= n &&
+					    n <= map->first + map->cnt - 1) {
+						member = map->mask;
+						break;
+					}
+				}
+			}
+			sw->info->port_cfg[port].vid_member = member;
+		}
+	} else if (1 == sw->eth_cnt) {
+		u8 member;
+
+		for (n = 1; n <= sw->mib_port_cnt; n++) {
+			port = get_phy_port(sw, n);
+			member = 0;
+			if (sw->eth_maps[0].mask & (1 << port))
+				member = sw->eth_maps[0].mask;
+			sw->info->port_cfg[port].vid_member = member;
+		}
+	}
+	for (port = 0; port < SWITCH_PORT_NUM; port++) {
+		if (sw->dev_count > 1 ||
+		    (sw->eth_maps[0].mask &&
+		    !(sw->eth_maps[0].mask & (1 << port))))
+			port_set_stp_state(sw, port, STP_STATE_DISABLED);
+		else
+			port_set_stp_state(sw, port, state);
+	}
+	if (sw->dev_count > 1 && !sw->dev_offset && sw->eth_cnt < 2)
+		port_set_stp_state(sw, SWITCH_PORT_NUM, STP_STATE_SIMPLE);
+	else
+		port_set_stp_state(sw, SWITCH_PORT_NUM, state);
+
+	/*
+	 * There may be some entries in the dynamic MAC table before the
+	 * the learning is turned off.  Once the entries are in the table the
+	 * switch may keep updating them even learning is off.
+	 */
+	if (sw->dev_count > 1)
+		sw_flush_dyn_mac_table(sw, TOTAL_PORT_NUM);
+}  /* sw_enable */
+
+/**
+ * sw_init - initialize the switch
+ * @sw:		The switch instance.
+ *
+ * This routine initializes the hardware switch engine for default operation.
+ */
+static void sw_init(struct ksz_sw *sw)
+{
+	memset(sw->tx_pad, 0, 60);
+	sw->tx_start = 0;
+
+#ifdef REG_DSP_CTRL_6
+	do {
+		SW_D data;
+		int i;
+		int fiber = 0;
+
+		for (i = 0; i < SWITCH_PORT_NUM; i++) {
+			if (sw->port_info[i].fiber)
+				fiber |= (1 << i);
+		}
+		if (fiber) {
+			data = SW_R(sw, REG_CFG_CTRL);
+			data &= ~(fiber << PORT_COPPER_MODE_S);
+			SW_W(sw, REG_CFG_CTRL, data);
+			data = SW_R(sw, REG_DSP_CTRL_6);
+			data &= ~COPPER_RECEIVE_ADJUSTMENT;
+			SW_W(sw, REG_DSP_CTRL_6, data);
+		}
+	} while (0);
+#endif
+
+#ifdef SWITCH_PORT_PHY_ADDR_MASK
+	sw_init_phy_addr(sw);
+#endif
+
+	sw_init_broad_storm(sw);
+
+	sw_init_prio(sw);
+
+	sw_init_prio_rate(sw);
+
+	sw_init_vlan(sw);
+
+	if (!sw_chk(sw, REG_SWITCH_CTRL_1,
+			SWITCH_TX_FLOW_CTRL | SWITCH_RX_FLOW_CTRL))
+		sw->overrides |= PAUSE_FLOW_CTRL;
+}  /* sw_init */
+
+/**
+ * sw_setup - setup the switch
+ * @sw:		The switch instance.
+ *
+ * This routine setup the hardware switch engine for default operation.
+ */
+static void sw_setup(struct ksz_sw *sw)
+{
+	uint port;
+
+	sw_set_global_ctrl(sw);
+	for (port = 0; port < SWITCH_PORT_NUM; port++) {
+		SW_D data;
+
+		port_cfg_back_pressure(sw, port, 1);
+
+		/*
+		 * Switch actually cannot do auto-negotiation with old 10Mbit
+		 * hub.
+		 */
+		port_r(sw, port, P_FORCE_CTRL, &data);
+		if (sw->port_info[port].fiber) {
+			port_cfg_force_flow_ctrl(sw, port, 1);
+			data &= ~PORT_AUTO_NEG_ENABLE;
+			data |= PORT_FORCE_FULL_DUPLEX;
+		} else {
+			port_cfg_force_flow_ctrl(sw, port, 0);
+			data &= ~PORT_FORCE_FULL_DUPLEX;
+		}
+		port_w(sw, port, P_FORCE_CTRL, data);
+	}
+
+	sw_setup_broad_storm(sw);
+
+	sw_setup_prio(sw);
+
+	sw_setup_mirror(sw);
+
+	sw->info->multi_sys = MULTI_MAC_TABLE_ENTRIES;
+	sw->info->multi_net = SWITCH_MAC_TABLE_ENTRIES;
+	if (sw->features & STP_SUPPORT) {
+		sw->ops->release(sw);
+		sw_setup_stp(sw);
+		sw->ops->acquire(sw);
+	}
+#ifdef CONFIG_KSZ_DLR
+	if (sw->features & DLR_HW)
+		sw_setup_dlr(sw);
+#endif
+}  /* sw_setup */
+
+static inline void sw_reset(struct ksz_sw *sw)
+{
+	u8 data;
+
+	data = sw->reg->r8(sw, REG_SWITCH_RESET);
+	data |= GLOBAL_SOFTWARE_RESET;
+	sw->reg->w8(sw, REG_SWITCH_RESET, data);
+	udelay(1);
+	data &= ~GLOBAL_SOFTWARE_RESET;
+	sw->reg->w8(sw, REG_SWITCH_RESET, data);
+}  /* sw_reset */
+
+/* -------------------------------------------------------------------------- */
+
+static struct sw_regs {
+	int start;
+	int end;
+} sw_regs_range[] = {
+
+#ifdef CONFIG_HAVE_KSZ8863
+	{ 0x000, 0x100 },
+	{ 0, 0 }
+};
+
+#define KSZSW_REGS_SIZE			0x100
+#else
+	/* Used by KSZ8462/KSZ8461 and KSZ8463. */
+	{ 0x000, 0x1BA },
+	{ 0x200, 0x398 },
+	{ 0x400, 0x690 },
+	{ 0x734, 0x736 },
+	{ 0x748, 0x74E },
+	{ 0, 0 }
+};
+
+#define KSZSW_REGS_SIZE			0x800
+#endif
+
+static int check_sw_reg_range(unsigned addr)
+{
+	struct sw_regs *range = sw_regs_range;
+
+	while (range->end > range->start) {
+		if (range->start <= addr && addr < range->end)
+			return true;
+		range++;
+	}
+	return false;
+}
+
+/* -------------------------------------------------------------------------- */
+
+#ifdef KSZSW_REGS_SIZE
+static struct ksz_sw *get_sw_data(struct device *d)
+{
+	struct sw_priv *hw_priv = dev_get_drvdata(d);
+
+	return &hw_priv->sw;
+}
+
+static ssize_t kszsw_registers_read(struct file *filp, struct kobject *kobj,
+	struct bin_attribute *bin_attr, char *buf, loff_t off, size_t count)
+{
+	size_t i;
+	SW_D reg;
+	struct device *dev;
+	struct ksz_sw *sw;
+
+	if (unlikely(off > KSZSW_REGS_SIZE))
+		return 0;
+
+	if ((off + count) > KSZSW_REGS_SIZE)
+		count = KSZSW_REGS_SIZE - off;
+
+	if (unlikely(!count))
+		return count;
+
+	dev = container_of(kobj, struct device, kobj);
+	sw = get_sw_data(dev);
+
+	reg = off;
+	sw->ops->acquire(sw);
+	i = sw->reg->get(sw, reg, count, buf);
+	sw->ops->release(sw);
+
+	/* Linux 5.10 return -1 to application if sizes do not match. */
+	if (i > count)
+		i = count;
+	return i;
+}
+
+static ssize_t kszsw_registers_write(struct file *filp, struct kobject *kobj,
+	struct bin_attribute *bin_attr, char *buf, loff_t off, size_t count)
+{
+	size_t i;
+	SW_D reg;
+	struct device *dev;
+	struct ksz_sw *sw;
+
+	if (unlikely(off >= KSZSW_REGS_SIZE))
+		return -EFBIG;
+
+	if ((off + count) > KSZSW_REGS_SIZE)
+		count = KSZSW_REGS_SIZE - off;
+
+	if (unlikely(!count))
+		return count;
+
+	dev = container_of(kobj, struct device, kobj);
+	sw = get_sw_data(dev);
+
+	reg = off;
+	sw->ops->acquire(sw);
+	i = sw->reg->set(sw, reg, count, buf);
+	sw->ops->release(sw);
+	return i;
+}
+
+static struct bin_attribute kszsw_registers_attr = {
+	.attr = {
+		.name	= "registers",
+		.mode	= S_IRUSR | S_IWUSR,
+	},
+	.size	= KSZSW_REGS_SIZE,
+	.read	= kszsw_registers_read,
+	.write	= kszsw_registers_write,
+};
+#endif
+
+static int sw_reg_get(struct ksz_sw *sw, u32 reg, size_t count, char *buf)
+{
+	size_t i;
+	SW_D *addr;
+
+	addr = (SW_D *) buf;
+	for (i = 0; i < count; i += SW_SIZE, reg += SW_SIZE, addr++) {
+		*addr = 0;
+		if (check_sw_reg_range(reg))
+			*addr = SW_R(sw, reg);
+	}
+	return i;
+}
+
+static int sw_reg_set(struct ksz_sw *sw, u32 reg, size_t count, char *buf)
+{
+	size_t i;
+	SW_D *addr;
+
+	addr = (SW_D *) buf;
+	for (i = 0; i < count; i += SW_SIZE, reg += SW_SIZE, addr++) {
+		if (check_sw_reg_range(reg))
+			SW_W(sw, reg, *addr);
+	}
+	return i;
+}
+
+/* -------------------------------------------------------------------------- */
+
+/*
+ * Microchip LinkMD routines
+ */
+
+enum {
+	CABLE_UNKNOWN,
+	CABLE_GOOD,
+	CABLE_CROSSED,
+	CABLE_REVERSED,
+	CABLE_CROSSED_REVERSED,
+	CABLE_OPEN,
+	CABLE_SHORT
+};
+
+#define STATUS_FULL_DUPLEX		0x01
+#define STATUS_CROSSOVER		0x02
+#define STATUS_REVERSED			0x04
+
+#define LINK_10MBPS_FULL		0x00000001
+#define LINK_10MBPS_HALF		0x00000002
+#define LINK_100MBPS_FULL		0x00000004
+#define LINK_100MBPS_HALF		0x00000008
+#define LINK_1GBPS_FULL			0x00000010
+#define LINK_1GBPS_HALF			0x00000020
+#define LINK_10GBPS_FULL		0x00000040
+#define LINK_10GBPS_HALF		0x00000080
+#define LINK_SYM_PAUSE			0x00000100
+#define LINK_ASYM_PAUSE			0x00000200
+
+#define LINK_AUTO_MDIX			0x00010000
+#define LINK_MDIX			0x00020000
+#define LINK_AUTO_POLARITY		0x00040000
+
+#define CABLE_LEN_MAXIMUM		15000
+#define CABLE_LEN_MULTIPLIER		41
+
+#define PHY_RESET_TIMEOUT		10
+
+/**
+ * sw_get_link_md - get LinkMD status
+ * @sw:		The switch instance.
+ * @port:	The port index.
+ *
+ * This routine is used to get the LinkMD status.
+ */
+static void sw_get_link_md(struct ksz_sw *sw, uint port)
+{
+	SW_D crossover;
+	SW_D ctrl;
+	SW_D data;
+	SW_D link;
+	u16 len;
+	int i;
+	int timeout;
+	struct ksz_port_info *port_info = &sw->port_info[port];
+
+	if (port_info->fiber)
+		return;
+	port_r(sw, port, P_SPEED_STATUS, &data);
+#if (SW_SIZE == (1))
+	port_r(sw, port, P_LINK_STATUS, &link);
+#else
+	link = data;
+#endif
+	port_info->status[0] = CABLE_UNKNOWN;
+	if (link & PORT_STATUS_LINK_GOOD) {
+		int stat = 0;
+
+		port_info->status[0] = CABLE_GOOD;
+		port_info->length[0] = 1;
+		port_info->status[1] = CABLE_GOOD;
+		port_info->length[1] = 1;
+		port_info->status[2] = CABLE_GOOD;
+		port_info->length[2] = 1;
+
+		if (link & PORT_MDIX_STATUS)
+			stat |= STATUS_CROSSOVER;
+		if (data & PORT_REVERSED_POLARITY)
+			stat |= STATUS_REVERSED;
+		if ((stat & (STATUS_CROSSOVER | STATUS_REVERSED)) ==
+				(STATUS_CROSSOVER | STATUS_REVERSED))
+			port_info->status[0] = CABLE_CROSSED_REVERSED;
+		else if ((stat & STATUS_CROSSOVER) == STATUS_CROSSOVER)
+			port_info->status[0] = CABLE_CROSSED;
+		else if ((stat & STATUS_REVERSED) == STATUS_REVERSED)
+			port_info->status[0] = CABLE_REVERSED;
+		return;
+	}
+
+	/* Put in 10 Mbps mode. */
+	port_r(sw, port, P_PHY_CTRL, &ctrl);
+	data = ctrl;
+	data &= ~(PORT_AUTO_NEG_ENABLE | PORT_FORCE_100_MBIT |
+		PORT_FORCE_FULL_DUPLEX);
+#if (SW_SIZE == (1))
+	port_w(sw, port, P_PHY_CTRL, data);
+
+	port_r(sw, port, P_NEG_RESTART_CTRL, &data);
+#endif
+	crossover = data;
+
+	for (i = 1; i <= 2; i++) {
+		data = crossover;
+
+		/* Disable auto MDIX. */
+		data |= PORT_AUTO_MDIX_DISABLE;
+		if (0 == i)
+			data &= ~PORT_FORCE_MDIX;
+		else
+			data |= PORT_FORCE_MDIX;
+
+		/* Disable transmitter. */
+		data |= PORT_TX_DISABLE;
+		port_w(sw, port, P_NEG_RESTART_CTRL, data);
+
+		/* Wait at most 1 second.*/
+		delay_milli(100);
+
+		/* Enable transmitter. */
+		data &= ~PORT_TX_DISABLE;
+		port_w(sw, port, P_NEG_RESTART_CTRL, data);
+
+		/* Start cable diagnostic test. */
+		port_r(sw, port, REG_PORT_LINK_MD_CTRL, &data);
+		data |= PORT_START_CABLE_DIAG;
+		port_w(sw, port, REG_PORT_LINK_MD_CTRL, data);
+		timeout = PHY_RESET_TIMEOUT;
+		do {
+			if (!--timeout)
+				break;
+			delay_milli(10);
+			port_r(sw, port, REG_PORT_LINK_MD_CTRL, &data);
+		} while ((data & PORT_START_CABLE_DIAG));
+
+		port_info->length[i] = 0;
+		port_info->status[i] = CABLE_UNKNOWN;
+
+		if (!(data & PORT_START_CABLE_DIAG)) {
+#if (SW_SIZE == (1))
+			port_r8(sw, port, REG_PORT_LINK_MD_RESULT, &link);
+			len = data & PORT_CABLE_FAULT_COUNTER_H;
+			len <<= 16;
+			len |= link;
+#else
+			len = data & PORT_CABLE_FAULT_COUNTER;
+#endif
+			port_info->length[i] = len *
+				CABLE_LEN_MULTIPLIER;
+			if (data & PORT_CABLE_10M_SHORT)
+				port_info->length[i] = 1;
+			data &= PORT_CABLE_DIAG_RESULT;
+			switch (data) {
+			case PORT_CABLE_STAT_NORMAL:
+				port_info->status[i] = CABLE_GOOD;
+				port_info->length[i] = 1;
+				break;
+			case PORT_CABLE_STAT_OPEN:
+				port_info->status[i] = CABLE_OPEN;
+				break;
+			case PORT_CABLE_STAT_SHORT:
+				port_info->status[i] = CABLE_SHORT;
+				break;
+			}
+		}
+	}
+
+	port_w(sw, port, P_PHY_CTRL, ctrl);
+	if (ctrl & PORT_AUTO_NEG_ENABLE) {
+		crossover |= PORT_AUTO_NEG_RESTART;
+		port_w(sw, port, P_NEG_RESTART_CTRL, crossover);
+	}
+
+	port_info->length[0] = port_info->length[1];
+	port_info->status[0] = port_info->status[1];
+	for (i = 2; i < 3; i++) {
+		if (CABLE_GOOD == port_info->status[0]) {
+			if (port_info->status[i] != CABLE_GOOD) {
+				port_info->status[0] = port_info->status[i];
+				port_info->length[0] = port_info->length[i];
+				break;
+			}
+		}
+	}
+}
+
+/* -------------------------------------------------------------------------- */
+
+static void get_sw_mib_counters(struct ksz_sw *sw, int first, int cnt,
+	u64 *counter)
+{
+	int i;
+	int mib;
+	uint n;
+	uint p;
+	struct ksz_port_mib *port_mib;
+
+	memset(counter, 0, sizeof(u64) * TOTAL_SWITCH_COUNTER_NUM);
+	for (i = 0, n = first; i < cnt; i++, n++) {
+		p = get_phy_port(sw, n);
+		port_mib = get_port_mib(sw, p);
+		for (mib = port_mib->mib_start; mib < sw->mib_cnt; mib++)
+			counter[mib] += port_mib->counter[mib];
+	}
+}
+
+#define MIB_RX_LO_PRIO			0x00
+#define MIB_RX_HI_PRIO			0x01
+#define MIB_RX_UNDERSIZE		0x02
+#define MIB_RX_FRAGMENT			0x03
+#define MIB_RX_OVERSIZE			0x04
+#define MIB_RX_JABBER			0x05
+#define MIB_RX_SYMBOL_ERR		0x06
+#define MIB_RX_CRC_ERR			0x07
+#define MIB_RX_ALIGNMENT_ERR		0x08
+#define MIB_RX_CTRL_8808		0x09
+#define MIB_RX_PAUSE			0x0A
+#define MIB_RX_BROADCAST		0x0B
+#define MIB_RX_MULTICAST		0x0C
+#define MIB_RX_UNICAST			0x0D
+#define MIB_RX_OCTET_64			0x0E
+#define MIB_RX_OCTET_65_127		0x0F
+#define MIB_RX_OCTET_128_255		0x10
+#define MIB_RX_OCTET_256_511		0x11
+#define MIB_RX_OCTET_512_1023		0x12
+#define MIB_RX_OCTET_1024_1522		0x13
+#define MIB_TX_LO_PRIO			0x14
+#define MIB_TX_HI_PRIO			0x15
+#define MIB_TX_LATE_COLLISION		0x16
+#define MIB_TX_PAUSE			0x17
+#define MIB_TX_BROADCAST		0x18
+#define MIB_TX_MULTICAST		0x19
+#define MIB_TX_UNICAST			0x1A
+#define MIB_TX_DEFERRED			0x1B
+#define MIB_TX_TOTAL_COLLISION		0x1C
+#define MIB_TX_EXCESS_COLLISION		0x1D
+#define MIB_TX_SINGLE_COLLISION		0x1E
+#define MIB_TX_MULTI_COLLISION		0x1F
+
+#define MIB_RX_DROPS			0x20
+#define MIB_TX_DROPS			0x21
+
+static struct {
+	char string[20];
+} mib_names[TOTAL_SWITCH_COUNTER_NUM] = {
+	{ "rx           " },
+	{ "rx_hi        " },
+	{ "rx_undersize" },
+	{ "rx_fragments" },
+	{ "rx_oversize" },
+	{ "rx_jabbers" },
+	{ "rx_symbol_err" },
+	{ "rx_crc_err" },
+	{ "rx_align_err" },
+	{ "rx_mac_ctrl" },
+	{ "rx_pause" },
+	{ "rx_bcast" },
+	{ "rx_mcast" },
+	{ "rx_ucast" },
+	{ "rx_64_or_less" },
+	{ "rx_65_127" },
+	{ "rx_128_255" },
+	{ "rx_256_511" },
+	{ "rx_512_1023" },
+	{ "rx_1024_1522" },
+
+	{ "tx           " },
+	{ "tx_hi        " },
+	{ "tx_late_col" },
+	{ "tx_pause" },
+	{ "tx_bcast" },
+	{ "tx_mcast" },
+	{ "tx_ucast" },
+	{ "tx_deferred" },
+	{ "tx_total_col" },
+	{ "tx_exc_col" },
+	{ "tx_single_col" },
+	{ "tx_mult_col" },
+
+	{ "rx_discards" },
+	{ "tx_discards" },
+};
+
+static struct {
+	int rx;
+	int tx;
+} mib_display[TOTAL_SWITCH_COUNTER_NUM / 2] = {
+	{ MIB_RX_LO_PRIO, MIB_TX_LO_PRIO },
+	{ MIB_RX_HI_PRIO, MIB_TX_HI_PRIO },
+	{ MIB_RX_PAUSE, MIB_TX_PAUSE },
+	{ MIB_RX_BROADCAST, MIB_TX_BROADCAST },
+	{ MIB_RX_MULTICAST, MIB_TX_MULTICAST },
+	{ MIB_RX_UNICAST, MIB_TX_UNICAST },
+	{ MIB_RX_OCTET_64, MIB_RX_OCTET_65_127 },
+	{ MIB_RX_OCTET_128_255, MIB_RX_OCTET_256_511 },
+	{ MIB_RX_OCTET_512_1023, MIB_RX_OCTET_1024_1522 },
+	{ MIB_RX_UNDERSIZE, MIB_RX_OVERSIZE },
+	{ MIB_RX_FRAGMENT, MIB_RX_JABBER },
+	{ MIB_RX_SYMBOL_ERR, MIB_RX_CRC_ERR },
+	{ MIB_RX_ALIGNMENT_ERR, MIB_RX_CTRL_8808 },
+	{ MIB_TX_LATE_COLLISION, MIB_TX_DEFERRED },
+	{ MIB_TX_TOTAL_COLLISION, MIB_TX_EXCESS_COLLISION },
+	{ MIB_TX_SINGLE_COLLISION, MIB_TX_MULTI_COLLISION },
+	{ MIB_RX_DROPS, MIB_TX_DROPS },
+};
+
+static int display_sw_mib_counters(struct ksz_sw *sw, int first, int cnt,
+	char *buf)
+{
+	int mib;
+	int n;
+	int len = 0;
+	u64 counter[TOTAL_SWITCH_COUNTER_NUM];
+
+	get_sw_mib_counters(sw, first, cnt, counter);
+	for (mib = 0; mib < TOTAL_SWITCH_COUNTER_NUM / 2; mib++) {
+		int rx = mib_display[mib].rx;
+		int tx = mib_display[mib].tx;
+		if (buf)
+			len += sprintf(buf + len,
+				"%s\t= %-20llu\t%s\t= %-20llu\n",
+				mib_names[rx].string,
+				counter[rx],
+				mib_names[tx].string,
+				counter[tx]);
+		else
+			printk(KERN_INFO "%s\t= %-20llu\t%s\t= %-20llu\n",
+				mib_names[rx].string,
+				counter[rx],
+				mib_names[tx].string,
+				counter[tx]);
+	}
+	for (n = 0, mib = first; n < cnt; n++, mib++) {
+		int j;
+		uint p = get_phy_port(sw, mib);
+		struct ksz_port_mib *port_mib = get_port_mib(sw, p);
+
+		for (j = 0; j < 2; j++) {
+			if (port_mib->rate[j].peak) {
+				u32 num;
+				u32 frac;
+
+				num = port_mib->rate[j].peak / 10;
+				frac = port_mib->rate[j].peak % 10;
+				if (buf)
+					len += sprintf(buf + len,
+						"%d:%d=%u.%u\n", mib, j,
+						num, frac);
+				else
+					printk(KERN_INFO
+						"%d:%d=%u.%u\n", mib, j,
+						num, frac);
+				port_mib->rate[j].peak = 0;
+			}
+		}
+	}
+	return len;
+}  /* display_sw_mib_counters */
+
+/* -------------------------------------------------------------------------- */
+
+static ssize_t display_sw_info(int cnt, char *buf, ssize_t len)
+{
+	len += sprintf(buf + len, "duplex:\t\t");
+	len += sprintf(buf + len, "0 for auto; ");
+	len += sprintf(buf + len,
+		"set to 1 for half-duplex; 2, full-duplex\n");
+	len += sprintf(buf + len, "speed:\t\t");
+	len += sprintf(buf + len,
+		"0 for auto; set to 10 or 100\n");
+	len += sprintf(buf + len, "force:\t\t");
+	len += sprintf(buf + len,
+		"set to 1 to force link to specific speed setting\n");
+	len += sprintf(buf + len, "flow_ctrl:\t");
+	len += sprintf(buf + len,
+		"set to 0 to disable flow control\n");
+	len += sprintf(buf + len, "mib:\t\t");
+	len += sprintf(buf + len,
+		"display the MIB table\n");
+	printk(KERN_INFO "%s", buf);
+	len = 0;
+
+	if (TOTAL_PORT_NUM != cnt)
+		return len;
+
+	len += sprintf(buf + len, "\ndynamic_table:\t");
+	len += sprintf(buf + len,
+		"display the switch's dynamic MAC table\n");
+	len += sprintf(buf + len, "static_table:\t");
+	len += sprintf(buf + len,
+		"display the switch's static MAC table\n");
+	len += sprintf(buf + len, "vlan_table:\t");
+	len += sprintf(buf + len,
+		"display the switch's VLAN table\n");
+
+	len += sprintf(buf + len, "\naging:\t\t");
+	len += sprintf(buf + len,
+		"disable/enable aging\n");
+	len += sprintf(buf + len, "fast_aging:\t");
+	len += sprintf(buf + len,
+		"disable/enable fast aging\n");
+	len += sprintf(buf + len, "link_aging:\t");
+	len += sprintf(buf + len,
+		"disable/enable link change auto aging\n");
+
+	len += sprintf(buf + len, "\nbcast_per:\t");
+	len += sprintf(buf + len,
+		"set broadcast storm percentage\n");
+	len += sprintf(buf + len, "mcast_storm:\t");
+	len += sprintf(buf + len,
+		"disable multicast storm protection\n");
+	len += sprintf(buf + len, "diffserv_map:\t");
+	len += sprintf(buf + len,
+		"set DiffServ value.  Use \"decimal=hexadecimal\" format\n");
+	len += sprintf(buf + len, "p_802_1p_map:\t");
+	len += sprintf(buf + len,
+		"set 802.1p value.  Use \"decimal=hexadecimal\" format\n");
+	printk(KERN_INFO "%s", buf);
+	len = 0;
+
+	len += sprintf(buf + len, "\nvlan:\t\t");
+	len += sprintf(buf + len,
+		"disable/enable 802.1Q VLAN\n");
+	len += sprintf(buf + len, "null_vid:\t");
+	len += sprintf(buf + len,
+		"set to 1 to replace null vid\n");
+	len += sprintf(buf + len, "macaddr:\t");
+	len += sprintf(buf + len,
+		"set switch MAC address\n");
+	len += sprintf(buf + len, "mirror_mode:\t");
+	len += sprintf(buf + len,
+		"set to 1 to use mirror rx AND tx mode\n");
+	len += sprintf(buf + len, "tail_tag:\t");
+	len += sprintf(buf + len,
+		"disable/enable tail tagging\n");
+
+	len += sprintf(buf + len, "\nigmp_snoop:\t");
+	len += sprintf(buf + len,
+		"disable/enable IGMP snooping\n");
+	len += sprintf(buf + len, "ipv6_mld_snoop:\t");
+	len += sprintf(buf + len,
+		"disable/enable IPv6 MLD snooping\n");
+	len += sprintf(buf + len, "ipv6_mld_option:");
+	len += sprintf(buf + len,
+		"disable/enable IPv6 MLD option snooping\n");
+
+	len += sprintf(buf + len, "\naggr_backoff:\t");
+	len += sprintf(buf + len,
+		"disable/enable aggressive backoff in half-duplex mode\n");
+	len += sprintf(buf + len, "no_exc_drop:\t");
+	len += sprintf(buf + len,
+		"disable/enable no excessive collision drop\n");
+	len += sprintf(buf + len, "buf_reserve:\t");
+	len += sprintf(buf + len,
+		"disable/enable buffer reserve\n");
+	printk(KERN_INFO "%s", buf);
+	len = 0;
+
+	len += sprintf(buf + len, "\nhuge_packet:\t");
+	len += sprintf(buf + len,
+		"disable/enable huge packet support\n");
+	len += sprintf(buf + len, "legal_packet:\t");
+	len += sprintf(buf + len,
+		"disable/enable legal packet\n");
+	len += sprintf(buf + len, "length_check:\t");
+	len += sprintf(buf + len,
+		"disable/enable legal packet length check\n");
+
+	len += sprintf(buf + len, "\nback_pressure:\t");
+	len += sprintf(buf + len,
+		"set back pressure mode\n");
+	len += sprintf(buf + len, "sw_flow_ctrl:\t");
+	len += sprintf(buf + len,
+		"disable/enable switch host port flow control\n");
+	len += sprintf(buf + len, "sw_half_duplex:\t");
+	len += sprintf(buf + len,
+		"disable/enable switch host port half-duplex mode\n");
+#ifdef SWITCH_10_MBIT
+	len += sprintf(buf + len, "sw_10_mbit:\t");
+	len += sprintf(buf + len,
+		"disable/enable switch host port 10Mbit mode\n");
+#endif
+	len += sprintf(buf + len, "rx_flow_ctrl:\t");
+	len += sprintf(buf + len,
+		"disable/enable receive flow control\n");
+	len += sprintf(buf + len, "tx_flow_ctrl:\t");
+	len += sprintf(buf + len,
+		"disable/enable transmit flow control\n");
+	len += sprintf(buf + len, "fair_flow_ctrl:\t");
+	len += sprintf(buf + len,
+		"disable/enable fair flow control mode\n");
+	printk(KERN_INFO "%s", buf);
+	len = 0;
+
+	len += sprintf(buf + len, "vlan_bound:\t");
+	len += sprintf(buf + len,
+		"disable/enable unicast VLAN boundary\n");
+	len += sprintf(buf + len, "fw_unk_dest:\t");
+	len += sprintf(buf + len,
+		"disable/enable unknown destination address forwarding\n");
+
+	len += sprintf(buf + len, "\nins_tag_0_1:\t");
+	len += sprintf(buf + len,
+		"set to 1 to insert tag from port 1 to 2\n");
+	len += sprintf(buf + len, "ins_tag_0_2:\t");
+	len += sprintf(buf + len,
+		"set to 1 to insert tag from port 1 to 3\n");
+	len += sprintf(buf + len, "ins_tag_1_0:\t");
+	len += sprintf(buf + len,
+		"set to 1 to insert tag from port 2 to 1\n");
+	len += sprintf(buf + len, "ins_tag_1_2:\t");
+	len += sprintf(buf + len,
+		"set to 1 to insert tag from port 2 to 3\n");
+	len += sprintf(buf + len, "ins_tag_2_0:\t");
+	len += sprintf(buf + len,
+		"set to 1 to insert tag from port 3 to 1\n");
+	len += sprintf(buf + len, "ins_tag_2_1:\t");
+	len += sprintf(buf + len,
+		"set to 1 to insert tag from port 3 to 2\n");
+
+	len += sprintf(buf + len, "\npass_all:\t");
+	len += sprintf(buf + len,
+		"set to 1 to pass all frames for debugging\n");
+	len += sprintf(buf + len, "pass_pause:\t");
+	len += sprintf(buf + len,
+		"set to 1 to pass PAUSE frames for debugging\n");
+	printk(KERN_INFO "%s", buf);
+	len = 0;
+
+	len += sprintf(buf + len, "\nswitch port settings:\n");
+	len += sprintf(buf + len, "duplex:\t\t");
+	len += sprintf(buf + len,
+		"display the port's duplex setting\n");
+	len += sprintf(buf + len, "speed:\t\t");
+	len += sprintf(buf + len,
+		"display the port's link speed\n");
+	len += sprintf(buf + len, "linkmd:\t\t");
+	len += sprintf(buf + len,
+		"write to start LinkMD test.  read for result\n");
+	len += sprintf(buf + len, "mib:\t\t");
+	len += sprintf(buf + len,
+		"display the port's MIB table\n");
+	printk(KERN_INFO "%s", buf);
+	len = 0;
+
+	len += sprintf(buf + len, "vid:\t\t");
+	len += sprintf(buf + len,
+		"set default VID value\n");
+	len += sprintf(buf + len, "member:\t\t");
+	len += sprintf(buf + len,
+		"set VLAN membership\n");
+
+	len += sprintf(buf + len, "bcast_storm:\t");
+	len += sprintf(buf + len,
+		"disable/enable broadcast storm protection\n");
+	len += sprintf(buf + len, "diffserv:\t");
+	len += sprintf(buf + len,
+		"disable/enable DiffServ priority\n");
+	len += sprintf(buf + len, "p_802_1p:\t");
+	len += sprintf(buf + len,
+		"disable/enable 802.1p priority\n");
+
+	len += sprintf(buf + len, "port_based:\t");
+	len += sprintf(buf + len,
+		"disable/enable port-based priority\n");
+	printk(KERN_INFO "%s", buf);
+	len = 0;
+
+	len += sprintf(buf + len, "prio_queue:\t");
+	len += sprintf(buf + len,
+		"disable/enable priority queue\n");
+	len += sprintf(buf + len, "tx_p0_ctrl:\t");
+	len += sprintf(buf + len,
+		"set priority queue 0 control\n");
+	len += sprintf(buf + len, "tx_p1_ctrl:\t");
+	len += sprintf(buf + len,
+		"set priority queue 1 control\n");
+	len += sprintf(buf + len, "tx_p2_ctrl:\t");
+	len += sprintf(buf + len,
+		"set priority queue 2 control\n");
+	len += sprintf(buf + len, "tx_p3_ctrl:\t");
+	len += sprintf(buf + len,
+		"set priority queue 3 control\n");
+	len += sprintf(buf + len, "tx_p0_ratio:\t");
+	len += sprintf(buf + len,
+		"set priority queue 0 ratio\n");
+	len += sprintf(buf + len, "tx_p1_ratio:\t");
+	len += sprintf(buf + len,
+		"set priority queue 1 ratio\n");
+	len += sprintf(buf + len, "tx_p2_ratio:\t");
+	len += sprintf(buf + len,
+		"set priority queue 2 ratio\n");
+	len += sprintf(buf + len, "tx_p3_ratio:\t");
+	len += sprintf(buf + len,
+		"set priority queue 3 ratio\n");
+	len += sprintf(buf + len, "prio_rate:\t");
+	len += sprintf(buf + len,
+		"disable/enable priority queue rate limiting\n");
+	len += sprintf(buf + len, "rx_limit:\t");
+	len += sprintf(buf + len,
+		"set rx rate limiting mode\n");
+	len += sprintf(buf + len, "cnt_ifg:\t");
+	len += sprintf(buf + len,
+		"set to 1 to count IPG\n");
+	len += sprintf(buf + len, "cnt_pre:\t");
+	len += sprintf(buf + len,
+		"set to 1 to count preamble\n");
+	len += sprintf(buf + len, "rx_p0_rate:\t");
+	len += sprintf(buf + len,
+		"set rx priority queue 0 rate in 64Kbps unit\n");
+	len += sprintf(buf + len, "rx_p1_rate:\t");
+	len += sprintf(buf + len,
+		"set rx priority queue 1 rate in 64Kbps unit\n");
+	len += sprintf(buf + len, "rx_p2_rate:\t");
+	len += sprintf(buf + len,
+		"set rx priority queue 2 rate in 64Kbps unit\n");
+	len += sprintf(buf + len, "rx_p3_rate:\t");
+	len += sprintf(buf + len,
+		"set rx priority queue 3 rate in 64Kbps unit\n");
+	len += sprintf(buf + len, "tx_p0_rate:\t");
+	len += sprintf(buf + len,
+		"set tx priority queue 0 rate in 64Kbps unit\n");
+	len += sprintf(buf + len, "tx_p1_rate:\t");
+	len += sprintf(buf + len,
+		"set tx priority queue 1 rate in 64Kbps unit\n");
+	len += sprintf(buf + len, "tx_p2_rate:\t");
+	len += sprintf(buf + len,
+		"set tx priority queue 2 rate in 64Kbps unit\n");
+	len += sprintf(buf + len, "tx_p3_rate:\t");
+	len += sprintf(buf + len,
+		"set tx priority queue 3 rate in 64Kbps unit\n");
+	printk(KERN_INFO "%s", buf);
+	len = 0;
+
+	len += sprintf(buf + len, "rx:\t\t");
+	len += sprintf(buf + len,
+		"disable/enable rx\n");
+	len += sprintf(buf + len, "tx:\t\t");
+	len += sprintf(buf + len,
+		"disable/enable tx\n");
+	len += sprintf(buf + len, "learn:\t\t");
+	len += sprintf(buf + len,
+		"disable/enable learning\n");
+
+	len += sprintf(buf + len, "mirror_port:\t");
+	len += sprintf(buf + len,
+		"disable/enable mirror port\n");
+	len += sprintf(buf + len, "mirror_rx:\t");
+	len += sprintf(buf + len,
+		"disable/enable mirror receive\n");
+	len += sprintf(buf + len, "mirror_tx:\t");
+	len += sprintf(buf + len,
+		"disable/enable mirror transmit\n");
+	printk(KERN_INFO "%s", buf);
+	len = 0;
+
+	len += sprintf(buf + len, "\nnon_vid:\t");
+	len += sprintf(buf + len,
+		"set to 1 to discard non-VID packets\n");
+	len += sprintf(buf + len, "ingress:\t");
+	len += sprintf(buf + len,
+		"disable/enable ingress VLAN filtering\n");
+	len += sprintf(buf + len, "ins_tag:\t");
+	len += sprintf(buf + len,
+		"disable/enable insert VLAN tag feature\n");
+	len += sprintf(buf + len, "rmv_tag:\t");
+	len += sprintf(buf + len,
+		"disable/enable remove VLAN tag feature\n");
+	len += sprintf(buf + len, "drop_tagged:\t");
+	len += sprintf(buf + len,
+		"disable/enable drop tagged packet feature\n");
+	len += sprintf(buf + len, "replace_prio:\t");
+	len += sprintf(buf + len,
+		"disable/enable replace 802.1p priority feature\n");
+	len += sprintf(buf + len, "back_pressure:\t");
+	len += sprintf(buf + len,
+		"disable/enable back pressure in half-duplex mode\n");
+	len += sprintf(buf + len, "force_flow_ctrl:");
+	len += sprintf(buf + len,
+		"set to 1 to force flow control\n");
+	len += sprintf(buf + len, "fw_unk_dest:\t");
+	len += sprintf(buf + len,
+		"set to 1 to forward unknown destination address packets\n");
+	len += sprintf(buf + len, "fw_inv_vid:\t");
+	len += sprintf(buf + len,
+		"set to 1 to forward invalid VID packets\n");
+	len += sprintf(buf + len, "\nmacaddr:\t");
+	len += sprintf(buf + len,
+		"set port MAC address\n");
+	len += sprintf(buf + len, "src_filter_0:\t");
+	len += sprintf(buf + len,
+		"disable/enable source filtering port 1 MAC address\n");
+	len += sprintf(buf + len, "src_filter_1:\t");
+	len += sprintf(buf + len,
+		"disable/enable source filtering port 2 MAC address\n");
+	printk(KERN_INFO "%s", buf);
+	len = 0;
+
+	len += sprintf(buf + len, "\nstatic MAC table:\n");
+	len += sprintf(buf + len, "addr:\t\t");
+	len += sprintf(buf + len,
+		"set MAC address\n");
+	len += sprintf(buf + len, "ports:\t\t");
+	len += sprintf(buf + len,
+		"set destination ports\n");
+	len += sprintf(buf + len, "override:\t");
+	len += sprintf(buf + len,
+		"set override bit\n");
+	len += sprintf(buf + len, "use_fid:\t");
+	len += sprintf(buf + len,
+		"set use FID bit\n");
+	len += sprintf(buf + len, "fid:\t\t");
+	len += sprintf(buf + len,
+		"set FID\n");
+	len += sprintf(buf + len, "valid:\t\t");
+	len += sprintf(buf + len,
+		"set valid bit and write to table\n");
+	len += sprintf(buf + len, "\nVLAN table:\n");
+	len += sprintf(buf + len, "vid:\t\t");
+	len += sprintf(buf + len,
+		"set VID\n");
+	len += sprintf(buf + len, "fid:\t\t");
+	len += sprintf(buf + len,
+		"set FID\n");
+	len += sprintf(buf + len, "member:\t\t");
+	len += sprintf(buf + len,
+		"set membership\n");
+	len += sprintf(buf + len, "valid:\t\t");
+	len += sprintf(buf + len,
+		"set valid bit and write to table\n");
+	printk(KERN_INFO "%s", buf);
+	len = 0;
+
+	return len;
+}
+
+static ssize_t sysfs_sw_read(struct ksz_sw *sw, int proc_num,
+	struct ksz_port *port, ssize_t len, char *buf)
+{
+	int i;
+	int j;
+	u16 map;
+	struct ksz_sw_info *info = sw->info;
+
+	switch (proc_num) {
+	case PROC_SW_INFO:
+		len = display_sw_info(TOTAL_PORT_NUM, buf, len);
+		break;
+	case PROC_SW_VERSION:
+		len += sprintf(buf + len, "%s  %s\n",
+			SW_DRV_VERSION, SW_DRV_RELDATE);
+		break;
+	case PROC_SET_SW_DUPLEX:
+		if (!port)
+			break;
+		len += sprintf(buf + len, "%u; ", port->duplex);
+		if (media_connected == port->linked->state) {
+			if (1 == port->linked->duplex)
+				len += sprintf(buf + len, "half-duplex\n");
+			else if (2 == port->linked->duplex)
+				len += sprintf(buf + len, "full-duplex\n");
+		} else
+			len += sprintf(buf + len, "unlinked\n");
+		break;
+	case PROC_SET_SW_SPEED:
+		if (!port)
+			break;
+		len += sprintf(buf + len, "%u; ", port->speed);
+		if (media_connected == port->linked->state)
+			len += sprintf(buf + len, "%u\n",
+				port->linked->tx_rate / TX_RATE_UNIT);
+		else
+			len += sprintf(buf + len, "unlinked\n");
+		break;
+	case PROC_SET_SW_FORCE:
+		if (!port)
+			break;
+		len += sprintf(buf + len, "%u\n", port->force_link);
+		break;
+	case PROC_SET_SW_FLOW_CTRL:
+		if (!port)
+			break;
+		len += sprintf(buf + len, "%u; ", port->flow_ctrl);
+		switch (port->flow_ctrl) {
+		case PHY_FLOW_CTRL:
+			len += sprintf(buf + len, "flow control\n");
+			break;
+		case PHY_TX_ONLY:
+			len += sprintf(buf + len, "tx only\n");
+			break;
+		case PHY_RX_ONLY:
+			len += sprintf(buf + len, "rx only\n");
+			break;
+		default:
+			len += sprintf(buf + len, "no flow control\n");
+			break;
+		}
+		break;
+	case PROC_SET_SW_MIB:
+		if (!port)
+			break;
+		len += display_sw_mib_counters(sw, port->first_port,
+			port->mib_port_cnt, buf + len);
+		break;
+	case PROC_SET_BROADCAST_STORM:
+		len += sprintf(buf + len, "%u%%\n", info->broad_per);
+		break;
+	case PROC_SET_DIFFSERV:
+		for (i = 0; i < DIFFSERV_ENTRIES / KS_PRIO_IN_REG; i++) {
+			len += sprintf(buf + len, "%2u=",
+				i * KS_PRIO_IN_REG);
+			map = info->diffserv[i];
+			for (j = 0; j < KS_PRIO_IN_REG; j++) {
+				len += sprintf(buf + len, "%u ",
+					map & KS_PRIO_M);
+				map >>= KS_PRIO_S;
+			}
+			len += sprintf(buf + len, "\t"SW_SIZE_STR"\n",
+				info->diffserv[i]);
+		}
+		break;
+	case PROC_SET_802_1P:
+		for (i = 0; i < PRIO_802_1P_ENTRIES / KS_PRIO_IN_REG; i++) {
+			len += sprintf(buf + len, "%u=",
+				i * KS_PRIO_IN_REG);
+			map = info->p_802_1p[i];
+			for (j = 0; j < KS_PRIO_IN_REG; j++) {
+				len += sprintf(buf + len, "%u ",
+					map & KS_PRIO_M);
+				map >>= KS_PRIO_S;
+			}
+			len += sprintf(buf + len, "\t"SW_SIZE_STR"\n",
+				info->p_802_1p[i]);
+		}
+		break;
+#ifdef SWITCH_PORT_PHY_ADDR_MASK
+	case PROC_SET_PHY_ADDR:
+		len += sprintf(buf + len, "%u\n",
+			info->phy_addr);
+		break;
+#endif
+	case PROC_SET_SW_VID:
+		len += sprintf(buf + len, "0x%04x\n", sw->vid);
+		break;
+	case PROC_GET_PORTS:
+	{
+		uint ports = sw->mib_port_cnt;
+
+		if (sw->eth_cnt)
+			ports = sw->eth_maps[0].cnt;
+		len += sprintf(buf + len, "%u\n", ports);
+		break;
+	}
+	case PROC_GET_DEV_START:
+	{
+		int start = 0;
+
+		if (sw->dev_offset)
+			start = 100;
+		len += sprintf(buf + len, "%u\n", start);
+		break;
+	}
+	case PROC_GET_VLAN_START:
+	{
+		int start = 0;
+
+		if (sw->features & VLAN_PORT)
+			start = VLAN_PORT_START;
+		len += sprintf(buf + len, "%u\n", start);
+		break;
+	}
+	case PROC_GET_STP:
+		len += sprintf(buf + len, "0\n");
+		break;
+	case PROC_SET_SW_FEATURES:
+		len += sprintf(buf + len, "%08x:\n", sw->features);
+		len += sprintf(buf + len, "\t%08x = STP support\n",
+			STP_SUPPORT);
+		len += sprintf(buf + len, "\t%08x = VLAN port forwarding\n",
+			VLAN_PORT);
+		len += sprintf(buf + len, "\t%08x = VLAN port remove tag\n",
+			VLAN_PORT_REMOVE_TAG);
+		len += sprintf(buf + len, "\t%08x = VLAN port tag tailing\n",
+			VLAN_PORT_TAGGING);
+#ifdef CONFIG_KSZ_DLR
+		len += sprintf(buf + len, "\t%08x = DLR support\n",
+			DLR_HW);
+#endif
+#ifdef CONFIG_KSZ_HSR
+		len += sprintf(buf + len, "\t%08x = HSR support\n",
+			HSR_HW);
+#endif
+		len += sprintf(buf + len, "\t%08x = different MAC addresses\n",
+			DIFF_MAC_ADDR);
+#ifdef CONFIG_1588_PTP
+		len += sprintf(buf + len, "\t%08x = 1588 PTP\n",
+			PTP_HW);
+#endif
+		break;
+	case PROC_SET_SW_OVERRIDES:
+		len += sprintf(buf + len, "%08x:\n", sw->overrides);
+		len += sprintf(buf + len, "\t%08x = flow control\n",
+			PAUSE_FLOW_CTRL);
+		len += sprintf(buf + len, "\t%08x = fast aging\n",
+			FAST_AGING);
+		len += sprintf(buf + len, "\t%08x = tag is removed\n",
+			TAG_REMOVE);
+		len += sprintf(buf + len, "\t%08x = tail tagging\n",
+			TAIL_TAGGING);
+		break;
+	case PROC_DYNAMIC:
+		len = sw_d_dyn_mac_table(sw, buf, len);
+		break;
+	case PROC_STATIC:
+		len = sw_d_sta_mac_table(sw, buf, len);
+		len = sw_d_mac_table(sw, buf, len);
+		break;
+	case PROC_VLAN:
+		len = sw_d_vlan_table(sw, buf, len);
+		break;
+	}
+	return len;
+}
+
+static ssize_t sysfs_sw_read_hw(struct ksz_sw *sw, int proc_num, ssize_t len,
+	char *buf)
+{
+	u8 data[8];
+	int chk = 0;
+	int type = SHOW_HELP_ON_OFF;
+	char note[40];
+
+	note[0] = '\0';
+	switch (proc_num) {
+	case PROC_SET_AGING:
+		chk = sw_chk(sw, REG_SWITCH_CTRL_1, SWITCH_AGING_ENABLE);
+		break;
+	case PROC_SET_FAST_AGING:
+		chk = sw_chk(sw, REG_SWITCH_CTRL_1, SWITCH_FAST_AGING);
+		break;
+	case PROC_SET_LINK_AGING:
+		chk = sw_chk(sw, S_LINK_AGING_CTRL, SWITCH_LINK_AUTO_AGING);
+		break;
+	case PROC_SET_MULTICAST_STORM:
+		chk = !sw_chk(sw, REG_SWITCH_CTRL_2, MULTICAST_STORM_DISABLE);
+		break;
+	case PROC_ENABLE_VLAN:
+		chk = sw_chk(sw, S_MIRROR_CTRL, SWITCH_VLAN_ENABLE);
+		break;
+	case PROC_SET_REPLACE_NULL_VID:
+		chk = sw_chk(sw, S_REPLACE_VID_CTRL, SWITCH_REPLACE_VID);
+		break;
+	case PROC_SET_MAC_ADDR:
+		sw_get_addr(sw, data);
+		len += sprintf(buf + len, "%02X:%02X:%02X:%02X:%02X:%02X\n",
+			data[0], data[1], data[2], data[3], data[4], data[5]);
+		type = SHOW_HELP_NONE;
+		break;
+	case PROC_SET_MIRROR_MODE:
+		chk = sw_chk_mirror_rx_tx(sw);
+		if (sw->verbose) {
+			if (chk)
+				strcpy(note, " (rx and tx)");
+			else
+				strcpy(note, " (rx or tx)");
+		}
+		type = SHOW_HELP_SPECIAL;
+		break;
+	case PROC_SET_IGMP_SNOOP:
+		chk = sw_chk(sw, S_MIRROR_CTRL, SWITCH_IGMP_SNOOP);
+		break;
+#ifdef SWITCH_IPV6_MLD_SNOOP
+	case PROC_SET_IPV6_MLD_SNOOP:
+		chk = sw_chk(sw, S_MIRROR_CTRL, SWITCH_IPV6_MLD_SNOOP);
+		break;
+	case PROC_SET_IPV6_MLD_OPTION:
+		chk = sw_chk(sw, S_MIRROR_CTRL, SWITCH_IPV6_MLD_OPTION);
+		break;
+#endif
+	case PROC_SET_TAIL_TAG:
+		chk = sw_chk(sw, S_TAIL_TAG_CTRL, SWITCH_TAIL_TAG_ENABLE);
+		break;
+	case PROC_SET_AGGR_BACKOFF:
+		chk = sw_chk(sw, REG_SWITCH_CTRL_1, SWITCH_AGGR_BACKOFF);
+		break;
+	case PROC_SET_NO_EXC_DROP:
+		chk = sw_chk(sw, REG_SWITCH_CTRL_2, NO_EXC_COLLISION_DROP);
+		break;
+#ifdef SWITCH_BUF_RESERVE
+	case PROC_SET_BUFFER_RESERVE:
+		chk = sw_chk(sw, REG_SWITCH_CTRL_2, SWITCH_BUF_RESERVE);
+		break;
+#endif
+	case PROC_SET_VLAN_BOUNDARY:
+		chk = sw_chk(sw, REG_SWITCH_CTRL_2, UNICAST_VLAN_BOUNDARY);
+		break;
+	case PROC_SET_HUGE_PACKET:
+		chk = sw_chk(sw, REG_SWITCH_CTRL_2, SWITCH_HUGE_PACKET);
+		break;
+	case PROC_SET_LEGAL_PACKET:
+		chk = sw_chk(sw, REG_SWITCH_CTRL_2, SWITCH_LEGAL_PACKET);
+		break;
+	case PROC_SET_LENGTH_CHECK:
+		chk = sw_chk(sw, REG_SWITCH_CTRL_1, SWITCH_CHECK_LENGTH);
+		break;
+	case PROC_SET_BACK_PRESSURE_MODE:
+		chk = sw_chk(sw, REG_SWITCH_CTRL_2, SWITCH_BACK_PRESSURE);
+		break;
+	case PROC_SET_SWITCH_FLOW_CTRL:
+		chk = sw_chk(sw, S_REPLACE_VID_CTRL, SWITCH_FLOW_CTRL);
+		break;
+	case PROC_SET_SWITCH_HALF_DUPLEX:
+		chk = sw_chk(sw, S_REPLACE_VID_CTRL, SWITCH_HALF_DUPLEX);
+		break;
+#ifdef SWITCH_10_MBIT
+	case PROC_SET_SWITCH_10_MBIT:
+		chk = sw_chk(sw, S_REPLACE_VID_CTRL, SWITCH_10_MBIT);
+		break;
+#endif
+	case PROC_SET_RX_FLOW_CTRL:
+		chk = sw_chk(sw, REG_SWITCH_CTRL_1, SWITCH_RX_FLOW_CTRL);
+		break;
+	case PROC_SET_TX_FLOW_CTRL:
+		chk = sw_chk(sw, REG_SWITCH_CTRL_1, SWITCH_TX_FLOW_CTRL);
+		break;
+	case PROC_SET_FAIR_FLOW_CTRL:
+		chk = sw_chk(sw, REG_SWITCH_CTRL_2, FAIR_FLOW_CTRL);
+		break;
+	case PROC_SET_FORWARD_UNKNOWN_DEST:
+		chk = sw_chk_unk_dest(sw);
+		break;
+	case PROC_SET_INS_TAG_0_1:
+		chk = sw_chk(sw, S_INS_SRC_PVID_CTRL, SWITCH_INS_TAG_1_PORT_2);
+		break;
+	case PROC_SET_INS_TAG_0_2:
+		chk = sw_chk(sw, S_INS_SRC_PVID_CTRL, SWITCH_INS_TAG_1_PORT_3);
+		break;
+	case PROC_SET_INS_TAG_1_0:
+		chk = sw_chk(sw, S_INS_SRC_PVID_CTRL, SWITCH_INS_TAG_2_PORT_1);
+		break;
+	case PROC_SET_INS_TAG_1_2:
+		chk = sw_chk(sw, S_INS_SRC_PVID_CTRL, SWITCH_INS_TAG_2_PORT_3);
+		break;
+	case PROC_SET_INS_TAG_2_0:
+		chk = sw_chk(sw, S_INS_SRC_PVID_CTRL, SWITCH_INS_TAG_3_PORT_1);
+		break;
+	case PROC_SET_INS_TAG_2_1:
+		chk = sw_chk(sw, S_INS_SRC_PVID_CTRL, SWITCH_INS_TAG_3_PORT_2);
+		break;
+	case PROC_SET_PASS_ALL:
+		chk = sw_chk(sw, REG_SWITCH_CTRL_1, SWITCH_PASS_ALL);
+		break;
+	case PROC_SET_PASS_PAUSE:
+		chk = sw_chk(sw, S_LINK_AGING_CTRL, SWITCH_PASS_PAUSE);
+		break;
+	default:
+		type = SHOW_HELP_NONE;
+		break;
+	}
+	return sysfs_show(len, buf, type, chk, note, sw->verbose);
+}
+
+static int sysfs_sw_write(struct ksz_sw *sw, int proc_num,
+	struct ksz_port *port, int num, const char *buf)
+{
+	int changes;
+	int count;
+	unsigned int val;
+	u8 data[8];
+	int processed = true;
+
+	switch (proc_num) {
+	case PROC_SW_INFO:
+		sw_init(sw);
+		sw->verbose = !!num;
+		break;
+	case PROC_SET_SW_DUPLEX:
+		if (!port)
+			break;
+		if (num <= 2)
+			port->duplex = (u8) num;
+		break;
+	case PROC_SET_SW_SPEED:
+		if (!port)
+			break;
+		if (0 == num || 10 == num || 100 == num)
+			port->speed = (u8) num;
+		break;
+	case PROC_SET_SW_FORCE:
+		if (!port)
+			break;
+		port->force_link = (u8) num;
+		if (port->force_link)
+			port_force_link_speed(port);
+		else
+			port_set_link_speed(port);
+		break;
+	case PROC_SET_SW_FLOW_CTRL:
+		if (!port)
+			break;
+		if (num <= PHY_FLOW_CTRL)
+			port->flow_ctrl = (u8) num;
+		break;
+	case PROC_SET_SW_MIB:
+		for (count = 0; count < sw->port_cnt; count++) {
+			struct ksz_port_mib *mib = get_port_mib(sw, count);
+
+			memset((void *) mib->counter, 0, sizeof(u64) *
+				TOTAL_SWITCH_COUNTER_NUM);
+		}
+		break;
+	case PROC_SET_SW_REG:
+		count = sscanf(buf, "%x=%x", (unsigned int *) &num, &val);
+		if (1 == count)
+			printk(KERN_INFO SW_SIZE_STR"\n",
+				SW_R(sw, num));
+		else if (2 == count)
+			SW_W(sw, num, val);
+		break;
+	case PROC_SET_SW_VID:
+		sw->vid = num;
+		break;
+	case PROC_SET_SW_FEATURES:
+		if ('0' != buf[0] || 'x' != buf[1])
+			sscanf(buf, "%x", &num);
+		changes = sw->features ^ num;
+		sw->features = num;
+		if (changes & VLAN_PORT_REMOVE_TAG) {
+			int i;
+			bool enabled;
+
+			if (num & VLAN_PORT_REMOVE_TAG) {
+				enabled = true;
+				sw->overrides |= TAG_REMOVE;
+			} else {
+				enabled = false;
+				sw->overrides &= ~TAG_REMOVE;
+			}
+			for (i = 0; i < SWITCH_PORT_NUM; i++)
+				port_cfg_rmv_tag(sw, i, enabled);
+		}
+		break;
+	case PROC_SET_SW_OVERRIDES:
+		if ('0' != buf[0] || 'x' != buf[1])
+			sscanf(buf, "%x", &num);
+		sw->overrides = num;
+		break;
+	case PROC_DYNAMIC:
+		sw_flush_dyn_mac_table(sw, TOTAL_PORT_NUM);
+		break;
+	case PROC_STATIC:
+		sw->ops->release(sw);
+		sw_clr_sta_mac_table(sw);
+		sw->ops->acquire(sw);
+		break;
+	case PROC_SET_AGING:
+		sw_cfg(sw, REG_SWITCH_CTRL_1, SWITCH_AGING_ENABLE, num);
+		break;
+	case PROC_SET_FAST_AGING:
+		sw_cfg(sw, REG_SWITCH_CTRL_1, SWITCH_FAST_AGING, num);
+		break;
+	case PROC_SET_LINK_AGING:
+		sw_cfg(sw, S_LINK_AGING_CTRL, SWITCH_LINK_AUTO_AGING, num);
+		break;
+	case PROC_SET_BROADCAST_STORM:
+		hw_cfg_broad_storm(sw, num);
+		break;
+	case PROC_SET_MULTICAST_STORM:
+		sw_cfg(sw, REG_SWITCH_CTRL_2, MULTICAST_STORM_DISABLE, !num);
+		break;
+	case PROC_SET_DIFFSERV:
+		count = sscanf(buf, "%d=%x", (unsigned int *) &num, &val);
+		if (2 == count)
+			hw_cfg_tos_prio(sw, (u8) num, (u16) val);
+		break;
+	case PROC_SET_802_1P:
+		count = sscanf(buf, "%d=%x", (unsigned int *) &num, &val);
+		if (2 == count)
+			hw_cfg_802_1p_prio(sw, (u8) num, (u16) val);
+		break;
+	case PROC_ENABLE_VLAN:
+		if (!num)
+			sw_dis_vlan(sw);
+		else
+			sw_ena_vlan(sw);
+		break;
+	case PROC_SET_REPLACE_NULL_VID:
+		sw_cfg_replace_null_vid(sw, num);
+		break;
+	case PROC_SET_MAC_ADDR:
+	{
+		int i;
+		int n[6];
+
+		i = sscanf(buf, "%x:%x:%x:%x:%x:%x",
+			&n[0], &n[1], &n[2], &n[3], &n[4], &n[5]);
+		if (6 == i) {
+			for (i = 0; i < 6; i++)
+				data[i] = (u8) n[i];
+			sw_set_addr(sw, data);
+		}
+		break;
+	}
+	case PROC_SET_MIRROR_MODE:
+		sw_cfg_mirror_rx_tx(sw, num);
+		break;
+	case PROC_SET_IGMP_SNOOP:
+		sw_cfg(sw, S_MIRROR_CTRL, SWITCH_IGMP_SNOOP, num);
+		break;
+#ifdef SWITCH_IPV6_MLD_SNOOP
+	case PROC_SET_IPV6_MLD_SNOOP:
+		sw_cfg(sw, S_MIRROR_CTRL, SWITCH_IPV6_MLD_SNOOP, num);
+		break;
+	case PROC_SET_IPV6_MLD_OPTION:
+		sw_cfg(sw, S_MIRROR_CTRL, SWITCH_IPV6_MLD_OPTION, num);
+		break;
+#endif
+	case PROC_SET_TAIL_TAG:
+		sw_cfg(sw, S_TAIL_TAG_CTRL, SWITCH_TAIL_TAG_ENABLE, num);
+		if (num)
+			sw->overrides |= TAIL_TAGGING;
+		else
+			sw->overrides &= ~TAIL_TAGGING;
+		break;
+	case PROC_SET_AGGR_BACKOFF:
+		sw_cfg(sw, REG_SWITCH_CTRL_1, SWITCH_AGGR_BACKOFF, num);
+		break;
+	case PROC_SET_NO_EXC_DROP:
+		sw_cfg(sw, REG_SWITCH_CTRL_2, NO_EXC_COLLISION_DROP, num);
+		break;
+#ifdef SWITCH_BUF_RESERVE
+	case PROC_SET_BUFFER_RESERVE:
+		sw_cfg(sw, REG_SWITCH_CTRL_2, SWITCH_BUF_RESERVE, num);
+		break;
+#endif
+	case PROC_SET_VLAN_BOUNDARY:
+		sw_cfg(sw, REG_SWITCH_CTRL_2, UNICAST_VLAN_BOUNDARY, num);
+		break;
+	case PROC_SET_HUGE_PACKET:
+		sw_cfg(sw, REG_SWITCH_CTRL_2, SWITCH_HUGE_PACKET, num);
+		break;
+	case PROC_SET_LEGAL_PACKET:
+		sw_cfg(sw, REG_SWITCH_CTRL_2, SWITCH_LEGAL_PACKET, num);
+		break;
+	case PROC_SET_LENGTH_CHECK:
+		sw_cfg(sw, REG_SWITCH_CTRL_1, SWITCH_CHECK_LENGTH, num);
+		break;
+	case PROC_SET_BACK_PRESSURE_MODE:
+		sw_cfg(sw, REG_SWITCH_CTRL_2, SWITCH_BACK_PRESSURE, num);
+		break;
+	case PROC_SET_SWITCH_FLOW_CTRL:
+		sw_cfg(sw, S_REPLACE_VID_CTRL, SWITCH_FLOW_CTRL, num);
+		break;
+	case PROC_SET_SWITCH_HALF_DUPLEX:
+		sw_cfg(sw, S_REPLACE_VID_CTRL, SWITCH_HALF_DUPLEX, num);
+		break;
+#ifdef SWITCH_10_MBIT
+	case PROC_SET_SWITCH_10_MBIT:
+		sw_cfg(sw, S_REPLACE_VID_CTRL, SWITCH_10_MBIT, num);
+		break;
+#endif
+	case PROC_SET_RX_FLOW_CTRL:
+		sw_cfg(sw, REG_SWITCH_CTRL_1, SWITCH_RX_FLOW_CTRL, num);
+		break;
+	case PROC_SET_TX_FLOW_CTRL:
+		sw_cfg(sw, REG_SWITCH_CTRL_1, SWITCH_TX_FLOW_CTRL, num);
+		break;
+	case PROC_SET_FAIR_FLOW_CTRL:
+		sw_cfg(sw, REG_SWITCH_CTRL_2, FAIR_FLOW_CTRL, num);
+		break;
+	case PROC_SET_FORWARD_UNKNOWN_DEST:
+		sw_cfg_unk_dest(sw, num);
+		break;
+	case PROC_SET_INS_TAG_0_1:
+		sw_cfg(sw, S_INS_SRC_PVID_CTRL, SWITCH_INS_TAG_1_PORT_2, num);
+		break;
+	case PROC_SET_INS_TAG_0_2:
+		sw_cfg(sw, S_INS_SRC_PVID_CTRL, SWITCH_INS_TAG_1_PORT_3, num);
+		break;
+	case PROC_SET_INS_TAG_1_0:
+		sw_cfg(sw, S_INS_SRC_PVID_CTRL, SWITCH_INS_TAG_2_PORT_1, num);
+		break;
+	case PROC_SET_INS_TAG_1_2:
+		sw_cfg(sw, S_INS_SRC_PVID_CTRL, SWITCH_INS_TAG_2_PORT_3, num);
+		break;
+	case PROC_SET_INS_TAG_2_0:
+		sw_cfg(sw, S_INS_SRC_PVID_CTRL, SWITCH_INS_TAG_3_PORT_1, num);
+		break;
+	case PROC_SET_INS_TAG_2_1:
+		sw_cfg(sw, S_INS_SRC_PVID_CTRL, SWITCH_INS_TAG_3_PORT_2, num);
+		break;
+	case PROC_SET_PASS_ALL:
+		sw_cfg(sw, REG_SWITCH_CTRL_1, SWITCH_PASS_ALL, num);
+		break;
+	case PROC_SET_PASS_PAUSE:
+		sw_cfg(sw, S_LINK_AGING_CTRL, SWITCH_PASS_PAUSE, num);
+		break;
+#ifdef SWITCH_PORT_PHY_ADDR_MASK
+	case PROC_SET_PHY_ADDR:
+		sw_set_phy_addr(sw, num);
+		break;
+#endif
+	default:
+		processed = false;
+		break;
+	}
+	return processed;
+}
+
+static ssize_t sysfs_port_read(struct ksz_sw *sw, int proc_num, uint port,
+	ssize_t len, char *buf)
+{
+	struct ksz_port_cfg *port_cfg;
+	struct ksz_port_info *port_info;
+	int chk = 0;
+	int type = SHOW_HELP_NONE;
+	char note[40];
+
+	note[0] = '\0';
+	port = get_sysfs_port(sw, port);
+	port_cfg = get_port_cfg(sw, port);
+	port_info = get_port_info(sw, port);
+	switch (proc_num) {
+	case PROC_SET_PORT_DUPLEX:
+		if (media_connected == port_info->state) {
+			if (1 == port_info->duplex)
+				len += sprintf(buf + len, "half-duplex\n");
+			else if (2 == port_info->duplex)
+				len += sprintf(buf + len, "full-duplex\n");
+		} else
+			len += sprintf(buf + len, "unlinked\n");
+		break;
+	case PROC_SET_PORT_SPEED:
+		if (media_connected == port_info->state)
+			len += sprintf(buf + len, "%u\n",
+				port_info->tx_rate / TX_RATE_UNIT);
+		else
+			len += sprintf(buf + len, "unlinked\n");
+		break;
+	case PROC_SET_PORT_MIB:
+		port = get_log_port(sw, port);
+		len += display_sw_mib_counters(sw, port, 1, buf + len);
+		break;
+	case PROC_SET_LINK_MD:
+		len += sprintf(buf + len, "%u:%u %u:%u %u:%u\n",
+			port_info->length[0], port_info->status[0],
+			port_info->length[1], port_info->status[1],
+			port_info->length[2], port_info->status[2]);
+		if (sw->verbose)
+			len += sprintf(buf + len,
+				"(%d=unknown; %d=normal; %d=open; %d=short)\n",
+				CABLE_UNKNOWN, CABLE_GOOD, CABLE_OPEN,
+				CABLE_SHORT);
+		break;
+	case PROC_SET_PORT_BASED:
+		chk = port_cfg->port_prio;
+		type = SHOW_HELP_NUM;
+		break;
+	case PROC_SET_DEF_VID:
+		chk = port_cfg->vid;
+		type = SHOW_HELP_HEX_4;
+		break;
+	case PROC_SET_MEMBER:
+		chk = port_cfg->member;
+		type = SHOW_HELP_HEX_2;
+		break;
+	case PROC_SET_TX_P0_CTRL:
+		chk = (port_cfg->rate_ctrl[0] & RATE_CTRL_ENABLE) != 0;
+		type = SHOW_HELP_ON_OFF;
+		break;
+	case PROC_SET_TX_P1_CTRL:
+		chk = (port_cfg->rate_ctrl[1] & RATE_CTRL_ENABLE) != 0;
+		type = SHOW_HELP_ON_OFF;
+		break;
+	case PROC_SET_TX_P2_CTRL:
+		chk = (port_cfg->rate_ctrl[2] & RATE_CTRL_ENABLE) != 0;
+		type = SHOW_HELP_ON_OFF;
+		break;
+	case PROC_SET_TX_P3_CTRL:
+		chk = (port_cfg->rate_ctrl[3] & RATE_CTRL_ENABLE) != 0;
+		type = SHOW_HELP_ON_OFF;
+		break;
+#ifdef RATE_RATIO_MASK
+	case PROC_SET_TX_P0_RATIO:
+		chk = port_cfg->rate_ctrl[0] & RATE_RATIO_MASK;
+		type = SHOW_HELP_NUM;
+		break;
+	case PROC_SET_TX_P1_RATIO:
+		chk = port_cfg->rate_ctrl[1] & RATE_RATIO_MASK;
+		type = SHOW_HELP_NUM;
+		break;
+	case PROC_SET_TX_P2_RATIO:
+		chk = port_cfg->rate_ctrl[2] & RATE_RATIO_MASK;
+		type = SHOW_HELP_NUM;
+		break;
+	case PROC_SET_TX_P3_RATIO:
+		chk = port_cfg->rate_ctrl[3] & RATE_RATIO_MASK;
+		type = SHOW_HELP_NUM;
+		break;
+#endif
+	case PROC_SET_RX_LIMIT:
+		chk = (port_cfg->rate_limit >> 2) & 3;
+		if (sw->verbose) {
+			switch (chk) {
+			case 1:
+				strcpy(note, " (flooded unicast)");
+				break;
+			case 2:
+				strcpy(note, " (multicast)");
+				break;
+			case 3:
+				strcpy(note, " (broadcast)");
+				break;
+			default:
+				strcpy(note, " (all)");
+			}
+		}
+		type = SHOW_HELP_SPECIAL;
+		break;
+	case PROC_SET_CNT_IFG:
+		chk = (port_cfg->rate_limit & 2) != 0;
+		type = SHOW_HELP_ON_OFF;
+		break;
+	case PROC_SET_CNT_PRE:
+		chk = (port_cfg->rate_limit & 1) != 0;
+		type = SHOW_HELP_ON_OFF;
+		break;
+	case PROC_SET_RX_P0_RATE:
+		chk = port_cfg->rx_rate[0];
+		type = SHOW_HELP_NUM;
+		break;
+	case PROC_SET_RX_P1_RATE:
+		chk = port_cfg->rx_rate[1];
+		type = SHOW_HELP_NUM;
+		break;
+	case PROC_SET_RX_P2_RATE:
+		chk = port_cfg->rx_rate[2];
+		type = SHOW_HELP_NUM;
+		break;
+	case PROC_SET_RX_P3_RATE:
+		chk = port_cfg->rx_rate[3];
+		type = SHOW_HELP_NUM;
+		break;
+	case PROC_SET_TX_P0_RATE:
+		chk = port_cfg->tx_rate[0];
+		type = SHOW_HELP_NUM;
+		break;
+	case PROC_SET_TX_P1_RATE:
+		chk = port_cfg->tx_rate[1];
+		type = SHOW_HELP_NUM;
+		break;
+	case PROC_SET_TX_P2_RATE:
+		chk = port_cfg->tx_rate[2];
+		type = SHOW_HELP_NUM;
+		break;
+	case PROC_SET_TX_P3_RATE:
+		chk = port_cfg->tx_rate[3];
+		type = SHOW_HELP_NUM;
+		break;
+	}
+	return sysfs_show(len, buf, type, chk, note, sw->verbose);
+}
+
+static ssize_t sysfs_port_read_hw(struct ksz_sw *sw, int proc_num, uint port,
+	ssize_t len, char *buf)
+{
+	u8 data[8];
+	int chk = 0;
+	int type = SHOW_HELP_ON_OFF;
+	char note[40];
+
+	note[0] = '\0';
+	port = get_sysfs_port(sw, port);
+	switch (proc_num) {
+	case PROC_ENABLE_BROADCAST_STORM:
+		chk = port_chk_broad_storm(sw, port);
+		break;
+	case PROC_ENABLE_DIFFSERV:
+		chk = port_chk_diffserv(sw, port);
+		break;
+	case PROC_ENABLE_802_1P:
+		chk = port_chk_802_1p(sw, port);
+		break;
+	case PROC_ENABLE_PRIO_QUEUE:
+		chk = port_chk_4_queue(sw, port);
+		chk <<= 1;
+		chk |= port_chk_2_queue(sw, port);
+		chk = (1 << chk);
+		type = SHOW_HELP_NUM;
+		break;
+	case PROC_SET_MIRROR_PORT:
+		chk = port_chk_mirror_sniffer(sw, port);
+		break;
+	case PROC_SET_MIRROR_RX:
+		chk = port_chk_mirror_rx(sw, port);
+		break;
+	case PROC_SET_MIRROR_TX:
+		chk = port_chk_mirror_tx(sw, port);
+		break;
+	case PROC_SET_LEARN:
+		chk = !port_chk_dis_learn(sw, port);
+		break;
+	case PROC_SET_RX:
+		chk = port_chk_rx(sw, port);
+		break;
+	case PROC_SET_TX:
+		chk = port_chk_tx(sw, port);
+		break;
+	case PROC_SET_INSERT_TAG:
+		chk = port_chk_ins_tag(sw, port);
+		break;
+	case PROC_SET_REMOVE_TAG:
+		chk = port_chk_rmv_tag(sw, port);
+		break;
+#ifdef PORT_DOUBLE_TAG
+	case PROC_SET_DOUBLE_TAG:
+		chk = port_chk_double_tag(sw, port);
+		break;
+#endif
+	case PROC_SET_DROP_TAG:
+		chk = port_chk_drop_tag(sw, port);
+		break;
+	case PROC_SET_REPLACE_PRIO:
+		chk = port_chk_replace_prio(sw, port);
+		break;
+	case PROC_ENABLE_RX_PRIO_RATE:
+		chk = sw_chk_rx_prio_rate(sw, port);
+		break;
+	case PROC_ENABLE_TX_PRIO_RATE:
+		chk = sw_chk_tx_prio_rate(sw, port);
+		break;
+	case PROC_SET_DIS_NON_VID:
+		chk = port_chk_dis_non_vid(sw, port);
+		break;
+	case PROC_SET_INGRESS:
+		chk = port_chk_in_filter(sw, port);
+		break;
+	case PROC_SET_BACK_PRESSURE:
+		chk = port_chk_back_pressure(sw, port);
+		break;
+	case PROC_SET_FORCE_FLOW_CTRL:
+		chk = port_chk_force_flow_ctrl(sw, port);
+		break;
+	case PROC_SET_UNKNOWN_DEF_PORT:
+		chk = sw_chk_unk_def_port(sw, port);
+		break;
+	case PROC_SET_FORWARD_INVALID_VID:
+		chk = sw_chk_for_inv_vid(sw, port);
+		break;
+	case PROC_SET_PORT_MAC_ADDR:
+		port_get_addr(sw, port, data);
+		len += sprintf(buf + len, "%02X:%02X:%02X:%02X:%02X:%02X\n",
+			data[0], data[1], data[2], data[3], data[4], data[5]);
+		type = SHOW_HELP_NONE;
+		break;
+	case PROC_SET_SRC_FILTER_0:
+		chk = port_chk_src_filter_0(sw, port);
+		break;
+	case PROC_SET_SRC_FILTER_1:
+		chk = port_chk_src_filter_1(sw, port);
+		break;
+	default:
+		type = SHOW_HELP_NONE;
+		break;
+	}
+	return sysfs_show(len, buf, type, chk, note, sw->verbose);
+}
+
+static int sysfs_port_write(struct ksz_sw *sw, int proc_num, uint port,
+	int num, const char *buf)
+{
+	u8 data[8];
+	int processed = true;
+
+	port = get_sysfs_port(sw, port);
+	switch (proc_num) {
+	case PROC_SET_PORT_DUPLEX:
+	case PROC_SET_PORT_SPEED:
+	{
+		struct ksz_port phy_port;
+		struct ksz_port_info *port_info = get_port_info(sw, port);
+
+		if ((PROC_SET_PORT_DUPLEX == proc_num && num > 2) ||
+		    (PROC_SET_PORT_SPEED == proc_num &&
+		    num != 0 && num != 10 && num != 100))
+			break;
+
+		phy_port.sw = sw;
+		phy_port.port_cnt = 1;
+		phy_port.first_port = get_log_port(sw, port);
+		phy_port.flow_ctrl = port_info->own_flow_ctrl;
+		phy_port.duplex = port_info->own_duplex;
+		phy_port.speed = port_info->own_speed;
+		if (PROC_SET_PORT_DUPLEX == proc_num)
+			phy_port.duplex = (u8) num;
+		else
+			phy_port.speed = (u16) num;
+		port_set_link_speed(&phy_port);
+		break;
+	}
+	case PROC_SET_PORT_MIB:
+	{
+		struct ksz_port_mib *mib = get_port_mib(sw, port);
+
+		memset((void *) mib->counter, 0, sizeof(u64) *
+			TOTAL_SWITCH_COUNTER_NUM);
+		break;
+	}
+	case PROC_ENABLE_BROADCAST_STORM:
+		if (!num)
+			sw_dis_broad_storm(sw, port);
+		else
+			sw_ena_broad_storm(sw, port);
+		break;
+	case PROC_ENABLE_DIFFSERV:
+		if (!num)
+			sw_dis_diffserv(sw, port);
+		else
+			sw_ena_diffserv(sw, port);
+		break;
+	case PROC_ENABLE_802_1P:
+		if (!num)
+			sw_dis_802_1p(sw, port);
+		else
+			sw_ena_802_1p(sw, port);
+		break;
+	case PROC_SET_PORT_BASED:
+		sw_cfg_port_based(sw, port, num);
+		break;
+	case PROC_SET_DEF_VID:
+		sw_cfg_def_vid(sw, port, num);
+		break;
+	case PROC_SET_MEMBER:
+		sw_cfg_port_base_vlan(sw, port, (u8) num);
+		break;
+	case PROC_ENABLE_PRIO_QUEUE:
+		if (0 <= num && num <= 4)
+			sw_set_multi_queue(sw, port, num);
+		break;
+	case PROC_SET_TX_P0_CTRL:
+		hw_cfg_rate_ctrl(sw, port, 0, num);
+		break;
+	case PROC_SET_TX_P1_CTRL:
+		hw_cfg_rate_ctrl(sw, port, 1, num);
+		break;
+	case PROC_SET_TX_P2_CTRL:
+		hw_cfg_rate_ctrl(sw, port, 2, num);
+		break;
+	case PROC_SET_TX_P3_CTRL:
+		hw_cfg_rate_ctrl(sw, port, 3, num);
+		break;
+#ifdef RATE_RATIO_MASK
+	case PROC_SET_TX_P0_RATIO:
+		hw_cfg_rate_ratio(sw, port, 0, (u8) num);
+		break;
+	case PROC_SET_TX_P1_RATIO:
+		hw_cfg_rate_ratio(sw, port, 1, (u8) num);
+		break;
+	case PROC_SET_TX_P2_RATIO:
+		hw_cfg_rate_ratio(sw, port, 2, (u8) num);
+		break;
+	case PROC_SET_TX_P3_RATIO:
+		hw_cfg_rate_ratio(sw, port, 3, (u8) num);
+		break;
+#endif
+	case PROC_SET_RX_LIMIT:
+		hw_cfg_rx_limit(sw, port, (u8) num);
+		break;
+	case PROC_SET_CNT_IFG:
+		hw_cfg_cnt_ifg(sw, port, num);
+		break;
+	case PROC_SET_CNT_PRE:
+		hw_cfg_cnt_pre(sw, port, num);
+		break;
+	case PROC_SET_RX_P0_RATE:
+		hw_cfg_rx_prio_rate(sw, port, 0, num);
+		break;
+	case PROC_SET_RX_P1_RATE:
+		hw_cfg_rx_prio_rate(sw, port, 1, num);
+		break;
+	case PROC_SET_RX_P2_RATE:
+		hw_cfg_rx_prio_rate(sw, port, 2, num);
+		break;
+	case PROC_SET_RX_P3_RATE:
+		hw_cfg_rx_prio_rate(sw, port, 3, num);
+		break;
+	case PROC_SET_TX_P0_RATE:
+		hw_cfg_tx_prio_rate(sw, port, 0, num);
+		break;
+	case PROC_SET_TX_P1_RATE:
+		hw_cfg_tx_prio_rate(sw, port, 1, num);
+		break;
+	case PROC_SET_TX_P2_RATE:
+		hw_cfg_tx_prio_rate(sw, port, 2, num);
+		break;
+	case PROC_SET_TX_P3_RATE:
+		hw_cfg_tx_prio_rate(sw, port, 3, num);
+		break;
+	case PROC_SET_MIRROR_PORT:
+		port_cfg_mirror_sniffer(sw, port, num);
+		break;
+	case PROC_SET_MIRROR_RX:
+		port_cfg_mirror_rx(sw, port, num);
+		break;
+	case PROC_SET_MIRROR_TX:
+		port_cfg_mirror_tx(sw, port, num);
+		break;
+	case PROC_SET_LEARN:
+		port_cfg_dis_learn(sw, port, !num);
+		if (!num)
+			sw_cfg(sw, S_FLUSH_TABLE_CTRL,
+				SWITCH_FLUSH_DYN_MAC_TABLE, 1);
+		break;
+	case PROC_SET_RX:
+		port_cfg_rx(sw, port, num);
+		break;
+	case PROC_SET_TX:
+		port_cfg_tx(sw, port, num);
+		break;
+	case PROC_SET_INSERT_TAG:
+		sw_vlan_cfg_ins_tag(sw, port, num);
+		break;
+	case PROC_SET_REMOVE_TAG:
+		sw_vlan_cfg_rmv_tag(sw, port, num);
+		break;
+#ifdef PORT_DOUBLE_TAG
+	case PROC_SET_DOUBLE_TAG:
+		sw_vlan_cfg_double_tag(sw, port, num);
+		break;
+#endif
+	case PROC_SET_DROP_TAG:
+		sw_vlan_cfg_drop_tag(sw, port, num);
+		break;
+	case PROC_SET_REPLACE_PRIO:
+		sw_cfg_replace_prio(sw, port, num);
+		break;
+	case PROC_ENABLE_RX_PRIO_RATE:
+		if (!num)
+			sw_dis_rx_prio_rate(sw, port);
+		else
+			sw_ena_rx_prio_rate(sw, port);
+		break;
+	case PROC_ENABLE_TX_PRIO_RATE:
+		if (!num)
+			sw_dis_tx_prio_rate(sw, port);
+		else
+			sw_ena_tx_prio_rate(sw, port);
+		break;
+	case PROC_SET_DIS_NON_VID:
+		sw_vlan_cfg_dis_non_vid(sw, port, num);
+		break;
+	case PROC_SET_INGRESS:
+		sw_vlan_cfg_in_filter(sw, port, num);
+		break;
+	case PROC_SET_BACK_PRESSURE:
+		port_cfg_back_pressure(sw, port, num);
+		break;
+	case PROC_SET_FORCE_FLOW_CTRL:
+		port_cfg_force_flow_ctrl(sw, port, num);
+		break;
+	case PROC_SET_UNKNOWN_DEF_PORT:
+		sw_cfg_unk_def_port(sw, port, num);
+		break;
+	case PROC_SET_FORWARD_INVALID_VID:
+		sw_cfg_for_inv_vid(sw, port, num);
+		break;
+	case PROC_SET_LINK_MD:
+		sw_get_link_md(sw, port);
+		break;
+	case PROC_SET_PORT_MAC_ADDR:
+	{
+		int i;
+		int n[6];
+
+		i = sscanf(buf, "%x:%x:%x:%x:%x:%x",
+			&n[0], &n[1], &n[2], &n[3], &n[4], &n[5]);
+		if (6 == i) {
+			for (i = 0; i < 6; i++)
+				data[i] = (u8) n[i];
+			port_set_addr(sw, port, data);
+		}
+		break;
+	}
+	case PROC_SET_SRC_FILTER_0:
+		port_cfg_src_filter_0(sw, port, num);
+		break;
+	case PROC_SET_SRC_FILTER_1:
+		port_cfg_src_filter_1(sw, port, num);
+		break;
+	default:
+		processed = false;
+		break;
+	}
+	return processed;
+}
+
+static ssize_t sysfs_mac_read(struct ksz_sw *sw, int proc_num, int index,
+	ssize_t len, char *buf)
+{
+	struct ksz_mac_table *entry;
+
+	entry = &sw->info->mac_table[index];
+	switch (proc_num) {
+	case PROC_SET_STATIC_FID:
+		len += sprintf(buf + len, "%u\n", entry->fid);
+		break;
+	case PROC_SET_STATIC_USE_FID:
+		len += sprintf(buf + len, "%u\n", entry->use_fid);
+		break;
+	case PROC_SET_STATIC_OVERRIDE:
+		len += sprintf(buf + len, "%u\n", entry->override);
+		break;
+	case PROC_SET_STATIC_VALID:
+		len += sprintf(buf + len, "%u\n", entry->valid);
+		break;
+	case PROC_SET_STATIC_PORTS:
+		len += sprintf(buf + len, "%u\n", entry->ports);
+		break;
+	case PROC_SET_STATIC_MAC_ADDR:
+		len += sprintf(buf + len, "%02x:%02x:%02x:%02x:%02x:%02x\n",
+			entry->mac_addr[0], entry->mac_addr[1],
+			entry->mac_addr[2], entry->mac_addr[3],
+			entry->mac_addr[4], entry->mac_addr[5]);
+		break;
+	}
+	return len;
+}
+
+static int sysfs_mac_write(struct ksz_sw *sw, int proc_num, int index,
+	int num, const char *buf)
+{
+	struct ksz_mac_table *entry;
+	int processed = true;
+
+	entry = &sw->info->mac_table[index];
+	switch (proc_num) {
+	case PROC_SET_STATIC_FID:
+		if (0 <= num && num < 16)
+			entry->fid = num;
+		break;
+	case PROC_SET_STATIC_USE_FID:
+		if (num)
+			entry->use_fid = 1;
+		else
+			entry->use_fid = 0;
+		break;
+	case PROC_SET_STATIC_OVERRIDE:
+		if (num)
+			entry->override = 1;
+		else
+			entry->override = 0;
+		break;
+	case PROC_SET_STATIC_VALID:
+		if (num)
+			entry->valid = 1;
+		else
+			entry->valid = 0;
+
+		/* Not hardware entry. */
+		if (index >= STATIC_MAC_TABLE_ENTRIES)
+			break;
+		sw_w_sta_mac_table(sw, index,
+			entry->mac_addr, entry->ports,
+			entry->override, entry->valid,
+			entry->use_fid, entry->fid);
+		break;
+	case PROC_SET_STATIC_PORTS:
+		if (0 <= num && num <= sw->PORT_MASK)
+			entry->ports = num;
+		break;
+	case PROC_SET_STATIC_MAC_ADDR:
+	{
+		int i;
+		int n[6];
+
+		i = sscanf(buf, "%x:%x:%x:%x:%x:%x",
+			&n[0], &n[1], &n[2], &n[3], &n[4], &n[5]);
+		if (6 == i) {
+			for (i = 0; i < 6; i++)
+				entry->mac_addr[i] = (u8) n[i];
+		}
+		break;
+	}
+	default:
+		processed = false;
+		break;
+	}
+	return processed;
+}
+
+static ssize_t sysfs_vlan_read(struct ksz_sw *sw, int proc_num, int index,
+	ssize_t len, char *buf)
+{
+	struct ksz_vlan_table *entry;
+
+	entry = &sw->info->vlan_table[index];
+	switch (proc_num) {
+	case PROC_SET_VLAN_VALID:
+		len += sprintf(buf + len, "%u\n", entry->valid);
+		break;
+	case PROC_SET_VLAN_MEMBER:
+		len += sprintf(buf + len, "%u\n", entry->member);
+		break;
+	case PROC_SET_VLAN_FID:
+		len += sprintf(buf + len, "%u\n", entry->fid);
+		break;
+	case PROC_SET_VLAN_VID:
+		len += sprintf(buf + len, "0x%04x\n", entry->vid);
+		break;
+	}
+	return len;
+}
+
+static int sysfs_vlan_write(struct ksz_sw *sw, int proc_num, int index,
+	int num)
+{
+	struct ksz_vlan_table *entry;
+	int processed = true;
+
+	entry = &sw->info->vlan_table[index];
+	switch (proc_num) {
+	case PROC_SET_VLAN_VALID:
+		if (num)
+			entry->valid = 1;
+		else
+			entry->valid = 0;
+		sw_w_vlan_table(sw, index, entry->vid, entry->fid,
+			entry->member, entry->valid);
+		break;
+	case PROC_SET_VLAN_MEMBER:
+		if (0 <= num && num <= sw->PORT_MASK)
+			entry->member = num;
+		break;
+	case PROC_SET_VLAN_FID:
+		if (0 <= num && num < 16)
+			entry->fid = num;
+		break;
+	case PROC_SET_VLAN_VID:
+		if (0 <= num && num < 0x1000)
+			entry->vid = num;
+		break;
+	default:
+		processed = false;
+		break;
+	}
+	return processed;
+}
+
+/* -------------------------------------------------------------------------- */
+
+static void sw_cfg_mac(struct ksz_sw *sw, u8 index, u8 *dest, u32 ports,
+	int override, int use_fid, u16 fid)
+{
+	struct ksz_mac_table *mac;
+
+	if (index >= STATIC_MAC_TABLE_ENTRIES)
+		return;
+	mac = &sw->info->mac_table[index];
+	memset(mac, 0, sizeof(struct ksz_mac_table));
+	memcpy(mac->mac_addr, dest, ETH_ALEN);
+	mac->ports = (u8) ports;
+	mac->override = override;
+	mac->use_fid = use_fid;
+	mac->fid = (u8) fid;
+	mac->valid = mac->ports != 0;
+	if (!mac->valid && mac->override) {
+		mac->override = 0;
+		mac->valid = 1;
+	}
+	sw_w_sta_mac_table(sw, index, mac->mac_addr, mac->ports, mac->override,
+		mac->valid, mac->use_fid, mac->fid);
+}  /* sw_cfg_mac */
+
+static void sw_cfg_vlan(struct ksz_sw *sw, u8 index, u16 vid, u16 fid,
+	u32 ports)
+{
+	struct ksz_vlan_table vlan;
+
+	memset(&vlan, 0, sizeof(struct ksz_vlan_table));
+	vlan.vid = vid;
+	vlan.fid = (u8) fid;
+	vlan.member = (u8) ports;
+	vlan.valid = vlan.member != 0;
+	sw_w_vlan_table(sw, index, vlan.vid, vlan.fid, vlan.member, vlan.valid);
+}  /* sw_cfg_vlan */
+
+static u8 sw_alloc_mac(struct ksz_sw *sw)
+{
+	int i;
+
+	for (i = 1; i < STATIC_MAC_TABLE_ENTRIES; i++) {
+		if (!(sw->info->mac_table_used & (1 << i))) {
+			sw->info->mac_table_used |= (1 << i);
+			return i;
+		}
+	}
+	return 0;
+}  /* sw_alloc_mac */
+
+static void sw_free_mac(struct ksz_sw *sw, u8 index)
+{
+	sw->info->mac_table_used &= ~(1 << index);
+}  /* sw_free_mac */
+
+static u8 sw_alloc_vlan(struct ksz_sw *sw)
+{
+	int i;
+
+	/* First one is reserved. */
+	for (i = 1; i < VLAN_TABLE_ENTRIES - 1; i++) {
+		if (!(sw->info->vlan_table_used & (1 << i))) {
+			sw->info->vlan_table_used |= (1 << i);
+			return i;
+		}
+	}
+
+	/* Reject request. */
+	return 0;
+}  /* sw_alloc_vlan */
+
+static void sw_free_vlan(struct ksz_sw *sw, u8 index)
+{
+	if (index)
+		sw->info->vlan_table_used &= ~(1 << index);
+}  /* sw_free_vlan */
+
+static u16 sw_alloc_fid(struct ksz_sw *sw, u16 vid)
+{
+	int i;
+
+	/* First one is reserved. */
+	for (i = 1; i < VLAN_TABLE_ENTRIES; i++) {
+		if (!(sw->info->fid_used & (1 << i))) {
+			sw->info->fid_used |= (1 << i);
+			return i;
+		}
+	}
+	return 0;
+}  /* sw_alloc_fid */
+
+static void sw_free_fid(struct ksz_sw *sw, u16 fid)
+{
+	if (fid)
+		sw->info->fid_used &= ~(1 << fid);
+}  /* sw_free_fid */
+
+static const u8 *sw_get_br_id(struct ksz_sw *sw)
+{
+	static u8 id[8];
+	const u8* ret = id;
+
+	memcpy(&id[2], sw->info->mac_addr, ETH_ALEN);
+	id[0] = 0x80;
+	id[1] = 0x00;
+
+#ifdef CONFIG_KSZ_STP
+	ret = stp_br_id(&sw->info->rstp);
+#endif
+	return ret;
+}  /* sw_get_br_id */
+
+static void sw_from_backup(struct ksz_sw *sw, uint p)
+{
+#ifdef CONFIG_KSZ_MRP
+	if (sw->features & MRP_SUPPORT) {
+		struct mrp_info *mrp = &sw->mrp;
+
+		mrp->ops->from_backup(mrp, p);
+	}
+#endif
+}  /* sw_from_backup */
+
+static void sw_to_backup(struct ksz_sw *sw, uint p)
+{
+#ifdef CONFIG_KSZ_MRP
+	if (sw->features & MRP_SUPPORT) {
+		struct mrp_info *mrp = &sw->mrp;
+
+		mrp->ops->to_backup(mrp, p);
+	}
+#endif
+}  /* sw_to_backup */
+
+static void sw_from_designated(struct ksz_sw *sw, uint p, bool alt)
+{
+#ifdef CONFIG_KSZ_MRP
+	if (sw->features & MRP_SUPPORT) {
+		struct mrp_info *mrp = &sw->mrp;
+
+		mrp->ops->from_designated(mrp, p, alt);
+	}
+#endif
+}  /* sw_from_designated */
+
+static void sw_to_designated(struct ksz_sw *sw, uint p)
+{
+#ifdef CONFIG_KSZ_MRP
+	if (sw->features & MRP_SUPPORT) {
+		struct mrp_info *mrp = &sw->mrp;
+
+		mrp->ops->to_designated(mrp, p);
+	}
+#endif
+}  /* sw_to_designated */
+
+static void sw_tc_detected(struct ksz_sw *sw, uint p)
+{
+#ifdef CONFIG_KSZ_MRP
+	if (sw->features & MRP_SUPPORT) {
+		struct mrp_info *mrp = &sw->mrp;
+
+		mrp->ops->tc_detected(mrp, p);
+	}
+#endif
+}  /* sw_tc_detected */
+
+static int sw_get_tcDetected(struct ksz_sw *sw, uint p)
+{
+	int ret = false;
+
+#ifdef CONFIG_KSZ_STP
+	if (sw->features & STP_SUPPORT) {
+		struct ksz_stp_info *info = &sw->info->rstp;
+
+		ret = info->ops->get_tcDetected(info, p);
+	}
+#endif
+	return ret;
+}  /* sw_get_tcDetected */
+
+#ifdef CONFIG_HAVE_KSZ8463
+enum {
+	KSZ8463_SW_CHIP,
+};
+
+#ifndef NO_PHYDEV
+static void sw_dis_intr(struct ksz_sw *sw)
+{
+	sw->reg->w16(sw, TS_INT_ENABLE, 0);
+	sw->reg->w16(sw, TRIG_INT_ENABLE, 0);
+	sw->reg->w16(sw, REG_INT_MASK, 0);
+}  /* sw_dis_intr */
+
+static void sw_ena_intr(struct ksz_sw *sw)
+{
+	struct sw_priv *ks = container_of(sw, struct sw_priv, sw);
+
+	sw->reg->w16(sw, REG_INT_MASK, ks->intr_mask);
+}  /* sw_ena_intr */
+
+static SW_D sw_proc_intr(struct ksz_sw *sw)
+{
+	SW_D status;
+	struct sw_priv *ks = container_of(sw, struct sw_priv, sw);
+
+	status = sw->reg->r16(sw, REG_INT_STATUS);
+	status &= ks->intr_mask;
+	if (status & INT_PHY) {
+		sw->reg->w16(sw, REG_INT_STATUS, INT_PHY);
+		status &= ~INT_PHY;
+		schedule_delayed_work(&ks->link_read, 0);
+	}
+#ifdef CONFIG_1588_PTP
+	do {
+		struct ptp_info *ptp = &sw->ptp_hw;
+
+		if (ptp->ops->proc_intr) {
+			ptp->ops->proc_intr(ptp);
+			status = 0;
+		}
+	} while (0);
+#endif
+	return status;
+}  /* sw_proc_intr */
+
+static void sw_setup_intr(struct ksz_sw *sw)
+{
+	struct sw_priv *ks = container_of(sw, struct sw_priv, sw);
+
+	ks->intr_mask = INT_PHY | INT_TIMESTAMP | INT_TRIG_OUTPUT;
+	sw->reg->w16(sw, TS_INT_ENABLE, 0);
+	sw->reg->w16(sw, TS_INT_STATUS, 0xffff);
+}  /* sw_setup_intr */
+
+static int sw_chk_id(struct ksz_sw *sw, u16 *id)
+{
+	*id = sw->reg->r16(sw, REG_SWITCH_SIDER);
+	if ((*id & CIDER_ID_MASK) != CIDER_ID_8463 &&
+	    (*id & CIDER_ID_MASK) != CIDER_ID_8463_RLI)
+		return -ENODEV;
+	sw->chip_id = KSZ8463_SW_CHIP;
+	return 2;
+}
+#endif
+#endif
+
+#ifdef CONFIG_HAVE_KSZ8863
+enum {
+	KSZ8863_SW_CHIP,
+	KSZ8873_SW_CHIP,
+};
+
+static void sw_dis_intr(struct ksz_sw *sw)
+{
+	sw->reg->w8(sw, REG_INT_ENABLE, 0);
+}  /* sw_dis_intr */
+
+static void sw_ena_intr(struct ksz_sw *sw)
+{
+	struct sw_priv *ks = container_of(sw, struct sw_priv, sw);
+
+	sw->reg->w8(sw, REG_INT_ENABLE, ks->intr_mask);
+}  /* sw_ena_intr */
+
+static SW_D sw_proc_intr(struct ksz_sw *sw)
+{
+	SW_D status;
+	struct sw_priv *ks = container_of(sw, struct sw_priv, sw);
+
+	status = sw->reg->r8(sw, REG_INT_STATUS);
+	status &= ks->intr_mask;
+	if (status & ks->intr_mask) {
+		sw->reg->w8(sw, REG_INT_STATUS, status);
+		status &= ~ks->intr_mask;
+		schedule_delayed_work(&ks->link_read, 0);
+	}
+	return status;
+}  /* sw_proc_intr */
+
+static void sw_setup_intr(struct ksz_sw *sw)
+{
+	struct sw_priv *ks = container_of(sw, struct sw_priv, sw);
+
+	ks->intr_mask = INT_PORT_1_LINK_CHANGE | INT_PORT_2_LINK_CHANGE |
+		INT_PORT_3_LINK_CHANGE | INT_PORT_1_2_LINK_CHANGE;
+	sw->reg->w8(sw, REG_INT_ENABLE, 0);
+	sw->reg->w8(sw, REG_INT_STATUS, ks->intr_mask);
+}  /* sw_setup_intr */
+
+static int sw_chk_id(struct ksz_sw *sw, u16 *id)
+{
+	u8 mode;
+
+	*id = sw->reg->r16(sw, REG_CHIP_ID0);
+	if ((*id & SWITCH_CHIP_ID_MASK) != CHIP_ID_63)
+		return -ENODEV;
+	sw->chip_id = KSZ8863_SW_CHIP;
+	mode = sw->reg->r8(sw, REG_MODE_INDICATOR);
+	if (!(mode & (PORT_1_COPPER | PORT_2_COPPER)) ||
+	    !(mode & MODE_2_PHY))
+		sw->chip_id = KSZ8873_SW_CHIP;
+	return 2;
+}
+#endif
+
+static void sw_cfg_tail_tag(struct ksz_sw *sw, bool enable)
+{
+	sw_cfg(sw, S_TAIL_TAG_CTRL, SWITCH_TAIL_TAG_ENABLE, enable);
+}
+
+static void sw_set_multi(struct ksz_sw *sw, struct net_device *dev,
+	struct ksz_port *priv)
+{
+	struct ksz_mac_table *entry;
+	struct ksz_alu_table *alu;
+	struct netdev_hw_addr *ha;
+	int i;
+	int found;
+	int owner;
+	int port = 0;
+	struct ksz_sw_info *info = sw->info;
+
+	/* This device is associated with a switch port. */
+	if (1 == priv->port_cnt)
+		port = priv->first_port;
+	owner = 1 << port;
+
+	/* Remove old multicast entries. */
+	for (i = SWITCH_MAC_TABLE_ENTRIES; i < info->multi_net; i++) {
+		entry = &info->mac_table[i];
+		alu = &info->alu_table[i];
+		if (alu->valid && (alu->owner & owner)) {
+			/* Remove device ownership. */
+			alu->owner &= ~owner;
+			if (!port)
+				alu->forward &= ~FWD_MAIN_DEV;
+			else if (alu->owner <= 1)
+				alu->forward &= ~FWD_STP_DEV;
+			if (!alu->owner) {
+				alu->valid = 0;
+				entry->ports = 0;
+				entry->valid = 0;
+			}
+		}
+	}
+	netdev_for_each_mc_addr(ha, dev) {
+		if (!(*ha->addr & 1))
+			continue;
+		if (info->multi_net == info->multi_sys)
+			break;
+		found = 0;
+		for (i = 0; i < MULTI_MAC_TABLE_ENTRIES; i++) {
+			entry = &info->mac_table[i];
+			alu = &info->alu_table[i];
+			if (alu->valid &&
+			    !memcmp(entry->mac_addr, ha->addr, ETH_ALEN)) {
+				found = i + 1;
+				break;
+			}
+			if (!alu->valid && !found &&
+			    i >= SWITCH_MAC_TABLE_ENTRIES &&
+			    i < info->multi_net)
+				found = i + 1;
+		}
+		if (!found) {
+			info->multi_net++;
+			found = info->multi_net;
+		}
+		found--;
+		if (found >= SWITCH_MAC_TABLE_ENTRIES &&
+		    found < info->multi_net) {
+			entry = &info->mac_table[found];
+			alu = &info->alu_table[found];
+			if (port)
+				alu->forward |= FWD_STP_DEV;
+			else
+				alu->forward |= FWD_MAIN_DEV;
+			alu->owner |= owner;
+			alu->valid = 1;
+			memcpy(entry->mac_addr, ha->addr, ETH_ALEN);
+			entry->ports = sw->PORT_MASK;
+			entry->valid = 1;
+		}
+	}
+}  /* sw_set_multi */
+
+static void sw_forward(struct ksz_sw *sw, u8 *addr, u8 *self, u16 proto,
+	int tag)
+{
+	int forward = 0;
+
+	/* Already set for PTP message. */
+	if (sw->info->forward)
+		return;
+
+	/* Check for multicast addresses that are not forwarding. */
+	if (addr[0] & 0x01) {
+		int i;
+		struct ksz_mac_table *entry;
+		struct ksz_alu_table *alu;
+
+		for (i = MULTI_MAC_TABLE_ENTRIES - 1; i >= 0; i--) {
+			alu = &sw->info->alu_table[i];
+			entry = &sw->info->mac_table[i];
+			if (alu->valid &&
+			    !memcmp(addr, entry->mac_addr, ETH_ALEN)) {
+				forward = alu->forward;
+				if (!(forward & FWD_VLAN_DEV)) {
+					if (proto == 0x888E)
+						forward = FWD_STP_DEV |
+							  FWD_VLAN_DEV;
+				}
+				break;
+			}
+		}
+		if (!forward)
+			forward = FWD_MAIN_DEV/* | FWD_MCAST*/;
+
+	/* Check unicast address to host. */
+	} else if (!memcmp(addr, self, ETH_ALEN))
+		forward = FWD_HOST;
+	else
+		forward = FWD_MAIN_DEV/* | FWD_UCAST*/;
+	if (!tag)
+		forward &= ~FWD_VLAN_DEV;
+	sw->info->forward = forward;
+#if 0
+dbg_msg(" forward: %d %x\n", tag, forward);
+#endif
+}  /* sw_forward */
+
+static struct net_device *sw_rx_dev(struct ksz_sw *sw, u8 *data, u32 *len,
+	int *tag, int *port)
+{
+	u16 proto;
+	u16* proto_loc;
+	struct net_device *dev;
+	struct vlan_ethhdr *vlan = (struct vlan_ethhdr *) data;
+	int index = -1;
+	int vid = 0;
+
+	proto_loc = &vlan->h_vlan_proto;
+	proto = htons(*proto_loc);
+	sw->info->forward = 0;
+
+	/* Get received port number. */
+	if (sw->overrides & TAIL_TAGGING) {
+		(*len)--;
+		*tag = data[*len];
+
+		/* In case tagging is not working right. */
+		if (*tag >= SWITCH_PORT_NUM)
+			*tag = 0;
+
+		/* Save receiving port. */
+		*port = *tag;
+		index = sw->info->port_cfg[*tag].index;
+	}
+
+	/* Determine network device from VLAN id. */
+	if (index < 0) {
+		index = 0;
+		if (vlan->h_vlan_proto == htons(ETH_P_8021Q)) {
+			u16 vlan_tci = ntohs(vlan->h_vlan_TCI);
+
+			vid = vlan_tci & VLAN_VID_MASK;
+			proto_loc = &vlan->h_vlan_encapsulated_proto;
+			proto = htons(*proto_loc);
+		}
+		if (vid && (sw->features & SW_VLAN_DEV)) {
+			struct ksz_dev_map *map;
+			int p;
+
+			for (p = 0; p < sw->eth_cnt; p++) {
+				map = &sw->eth_maps[p];
+				if (vid == map->vlan) {
+					*port = map->first;
+					p = get_phy_port(sw, *port);
+					*port = p;
+					index = sw->info->port_cfg[p].index;
+					break;
+				}
+			}
+		}
+	}
+	if (index >= sw->dev_count + sw->dev_offset) {
+		printk(KERN_INFO "  [%s] netdev not correct\n", __func__);
+		BUG();
+	}
+	dev = sw->netdev[index];
+#ifdef CONFIG_KSZ_DLR
+	if (proto == DLR_TAG_TYPE)
+		return dev;
+#endif
+	if (!(sw->features & VLAN_PORT_TAGGING) ||
+	    !(sw->vlan_id & (1 << *tag))) {
+		*tag = 0;
+	}
+	sw_forward(sw, data, dev->dev_addr, proto, *tag);
+	return dev;
+}  /* sw_rx_dev */
+
+static int pkt_matched(struct sk_buff *skb, struct net_device *dev, void *ptr,
+	int (*match_multi)(void *ptr, u8 *addr), u8 h_promiscuous)
+{
+	int drop = false;
+	u8 bcast_addr[] = { 0xff, 0xff, 0xff, 0xff, 0xff, 0xff };
+
+	if (skb->data[0] & 0x01) {
+		if (memcmp(skb->data, bcast_addr, ETH_ALEN))
+			drop = match_multi(ptr, skb->data);
+	} else if (h_promiscuous && memcmp(skb->data, dev->dev_addr, ETH_ALEN))
+		drop = true;
+	if (drop)
+		return 0;
+	return skb->len;
+}  /* pkt_matched */
+
+static int sw_match_pkt(struct ksz_sw *sw, struct net_device **dev,
+	void **priv, int (*get_promiscuous)(void *ptr),
+	int (*match_multi)(void *ptr, u8 *data), struct sk_buff *skb,
+	u8 h_promiscuous)
+{
+	int s_promiscuous;
+
+	if (sw->dev_count <= 1)
+		return true;
+	s_promiscuous = get_promiscuous(*priv);
+	if (!s_promiscuous && !pkt_matched(skb, *dev, *priv, match_multi,
+			h_promiscuous)) {
+		int matched = false;
+
+		/* There is a parent network device. */
+		if (sw->dev_offset) {
+			matched = true;
+			*dev = sw->netdev[0];
+			*priv = netdev_priv(*dev);
+			s_promiscuous = get_promiscuous(*priv);
+			if (!s_promiscuous && !pkt_matched(skb, *dev, *priv,
+					match_multi, h_promiscuous))
+				matched = false;
+		}
+		return matched;
+	}
+	return true;
+}  /* sw_match_pkt */
+
+static struct net_device *sw_parent_rx(struct ksz_sw *sw,
+				       struct net_device *dev, int *forward)
+{
+	if (sw->dev_offset && dev != sw->netdev[0]) {
+		if (!*forward)
+			*forward = FWD_MAIN_DEV;
+		if (!(*forward & FWD_STP_DEV))
+			dev = sw->netdev[0];
+		else
+			*forward &= ~FWD_VLAN_DEV;
+	}
+	return dev;
+}  /* sw_parent_rx */
+
+static int sw_port_vlan_rx(struct sk_buff *skb, int forward, int tag)
+{
+	/* Add VLAN tag manually. */
+	if (!(forward & FWD_VLAN_DEV) || !tag)
+		return false;
+
+	tag += VLAN_PORT_START;
+
+	/* Only forward to one network device. */
+	__vlan_hwaccel_put_tag(skb, htons(ETH_P_8021Q), tag);
+	return true;
+}  /* sw_port_vlan_rx */
+
+static int sw_drv_rx(struct ksz_sw *sw, struct sk_buff *skb, uint port)
+{
+	int ret = 1;
+
+#ifdef CONFIG_KSZ_STP
+	if (sw->features & STP_SUPPORT) {
+		ret = stp_rcv(&sw->info->rstp, skb, port);
+		if (!ret)
+			return ret;
+	}
+#endif
+#ifdef CONFIG_KSZ_DLR
+	if (sw->features & DLR_HW) {
+		ret = dlr_rcv(&sw->info->dlr, skb, port);
+		if (!ret)
+			return ret;
+	}
+#endif
+#ifdef CONFIG_KSZ_HSR
+	if (sw->features & HSR_HW) {
+		ret = hsr_rcv(&sw->info->hsr, skb, port);
+
+		/* It is an HSR frame or consumed. */
+		if (ret < 2)
+			return ret;
+	}
+#endif
+
+	/* Need to remove VLAN tag if not using tail tag. */
+	if (sw->dev_count > 1 && (sw->features & SW_VLAN_DEV) &&
+	    !(sw->overrides & TAIL_TAGGING)) {
+		struct vlan_ethhdr *vlan = (struct vlan_ethhdr *) skb->data;
+
+		if (vlan->h_vlan_proto == htons(ETH_P_8021Q)) {
+			int p;
+			int vid;
+			struct ethhdr *eth;
+			u16 vlan_tci = ntohs(vlan->h_vlan_TCI);
+
+			vid = vlan_tci & VLAN_VID_MASK;
+			for (p = 0; p < sw->eth_cnt; p++) {
+				if (vid == sw->eth_maps[p].vlan) {
+					eth = (struct ethhdr *)
+						skb_pull(skb, VLAN_HLEN);
+					memmove(eth, vlan, 12);
+					break;
+				}
+			}
+		}
+	}
+	return ret;
+}  /* sw_drv_rx */
+
+static int sw_get_mtu(struct ksz_sw *sw)
+{
+	int need_tail_tag = false;
+	int header = 0;
+	int mtu = 0;
+
+#ifdef CONFIG_KSZ_DLR
+	if (sw->features & DLR_HW)
+		need_tail_tag = true;
+#endif
+#ifdef CONFIG_KSZ_HSR
+	if (sw->features & HSR_HW)
+		need_tail_tag = true;
+#endif
+	if (sw->dev_count > 1 && !(sw->features & SW_VLAN_DEV))
+		need_tail_tag = true;
+	if (sw->features & VLAN_PORT_TAGGING)
+		need_tail_tag = true;
+	if (sw->features & STP_SUPPORT)
+		need_tail_tag = true;
+	if (need_tail_tag)
+		mtu += 1;
+#ifdef CONFIG_KSZ_HSR
+	if (sw->features & HSR_HW)
+		header = HSR_HLEN;
+#endif
+	if (sw->features & SW_VLAN_DEV)
+		if (header < VLAN_HLEN)
+			header = VLAN_HLEN;
+	mtu += header;
+dbg_msg("mtu: %d\n", mtu);
+	return mtu;
+}  /* sw_get_mtu */
+
+static int sw_get_tx_len(struct ksz_sw *sw, struct sk_buff *skb, uint port,
+	int *header)
+{
+	int len = skb->len;
+	int hlen = 0;
+
+	if (sw->features & SW_VLAN_DEV)
+		hlen = VLAN_HLEN;
+#ifdef CONFIG_KSZ_HSR
+	do {
+		int i;
+
+		i = sw->info->port_cfg[port].index;
+		if (sw->eth_cnt && (sw->eth_maps[i].proto & HSR_HW))
+			hlen = HSR_HLEN;
+	} while (0);
+#endif
+	*header += hlen;
+	if (!(sw->overrides & TAIL_TAGGING))
+		return len;
+	if (len < 60)
+		len = 60;
+	len += 1;
+	return len;
+}  /* sw_get_tx_len */
+
+static void sw_add_tail_tag(struct ksz_sw *sw, struct sk_buff *skb, uint ports)
+{
+	u8 *trailer;
+	int len = 1;
+
+	trailer = skb_put(skb, len);
+	trailer[0] = (u8) ports & sw->PORT_MASK;
+}  /* sw_add_tail_tag */
+
+static int sw_get_tail_tag(u8 *trailer, int *port)
+{
+	int len = 1;
+
+	*port = *trailer;
+	return len;
+}  /* sw_get_tail_tag */
+
+static void sw_add_vid(struct ksz_sw *sw, u16 vid)
+{
+	if ((sw->features & VLAN_PORT) && vid >= VLAN_PORT_START) {
+		vid -= VLAN_PORT_START;
+		if (vid <= SWITCH_PORT_NUM)
+			sw->vlan_id |= (1 << vid);
+	}
+}  /* sw_add_vid */
+
+static void sw_kill_vid(struct ksz_sw *sw, u16 vid)
+{
+	if ((sw->features & VLAN_PORT) && vid >= VLAN_PORT_START) {
+		vid -= VLAN_PORT_START;
+		if (vid <= SWITCH_PORT_NUM)
+			sw->vlan_id &= ~(1 << vid);
+	}
+}  /* sw_kill_vid */
+
+static struct sk_buff *sw_ins_vlan(struct ksz_sw *sw, uint port,
+	struct sk_buff *skb)
+{
+	port = get_phy_port(sw, port);
+
+#ifdef CONFIG_KSZ_HSR
+	do {
+		int i = sw->info->port_cfg[port].index;
+
+		if (sw->eth_cnt && (sw->eth_maps[i].proto & HSR_HW))
+			return skb;
+	} while (0);
+#endif
+
+	/* Need to insert VLAN tag. */
+	if (sw->dev_count > 1 && (sw->features & SW_VLAN_DEV)) {
+		u16 vid;
+		struct vlan_ethhdr *vlan;
+		struct ethhdr *eth = (struct ethhdr *) skb->data;
+
+		vid = sw->info->port_cfg[port].vid;
+		vlan = (struct vlan_ethhdr *) skb_push(skb, VLAN_HLEN);
+		memmove(vlan, eth, 12);
+		vlan->h_vlan_TCI = htons(vid);
+		vlan->h_vlan_proto = htons(ETH_P_8021Q);
+	}
+	return skb;
+}  /* sw_ins_vlan */
+
+#ifdef CONFIG_KSZ_HSR
+static struct sk_buff *sw_ins_hsr(struct ksz_sw *sw, uint port,
+	struct sk_buff *skb, u8 *ports)
+{
+	int i;
+	uint p;
+
+	p = get_phy_port(sw, n);
+	i = sw->info->port_cfg[port].index;
+	if (sw->eth_cnt && (sw->eth_maps[i].proto & HSR_HW)) {
+		struct ksz_hsr_info *info = &sw->info->hsr;
+		struct hsr_port *from =
+			hsr_port_get_hsr(&info->hsr, HSR_PT_MASTER);
+
+		if (!hsr_forward_skb(skb, from))
+			return NULL;
+		memcpy(&skb->data[6], info->src_addr, ETH_ALEN);
+		*ports = info->member;
+	}
+	return skb;
+}  /* sw_ins_hsr */
+#endif
+
+static struct sk_buff *sw_check_skb(struct ksz_sw *sw, struct sk_buff *skb,
+	struct ksz_port *priv, void *ptr,
+	int (*update_msg)(u8 *data, u32 port, u32 overrides))
+{
+	bool need_new_copy = false;
+	int len;
+	int padlen = 0;
+	uint port;
+	u8 dest;
+	struct sk_buff *org_skb;
+	int update_dst = (sw->overrides & TAIL_TAGGING);
+	int headlen = 0;
+
+#ifdef CONFIG_1588_PTP
+	struct ptp_info *ptp = ptr;
+
+	if (ptp->overrides & (PTP_PORT_FORWARD | PTP_PORT_TX_FORWARD))
+		update_dst |= 2;
+#endif
+	if (!update_dst)
+		return sw_ins_vlan(sw, priv->first_port, skb);
+
+#ifdef CONFIG_KSZ_STP
+	if (skb->protocol == htons(STP_TAG_TYPE))
+		return skb;
+#endif
+#ifdef CONFIG_KSZ_DLR
+	if (skb->protocol == htons(DLR_TAG_TYPE))
+		return skb;
+#endif
+
+	if (sw->features & SW_VLAN_DEV)
+		headlen = VLAN_HLEN;
+	org_skb = skb;
+	port = 0;
+
+	/* This device is associated with a switch port. */
+	if (1 == priv->port_cnt)
+		port = priv->first_port;
+
+#if 0
+	do {
+		u16 prio;
+		u16 vid;
+		uint i;
+		uint p;
+		u32 features = sw->features;
+
+		p = get_phy_port(sw, priv->first_port);
+		i = sw->info->port_cfg[p].index;
+		if (sw->features & SW_VLAN_DEV)
+			features = sw->eth_maps[i].proto;
+		if (!(features & VLAN_PORT) || port || vlan_get_tag(skb, &vid))
+			break;
+		prio = vid & VLAN_PRIO_MASK;
+		vid &= VLAN_VID_MASK;
+		if (vid < VLAN_PORT_START)
+			break;
+		vid -= VLAN_PORT_START;
+		if (!vid || vid > SWITCH_PORT_NUM)
+			break;
+		port = vid;
+
+		if (sw->vid || prio) {
+			struct vlan_ethhdr *vlan =
+				(struct vlan_ethhdr *) skb->data;
+			u16 vlan_tci = ntohs(vlan->h_vlan_TCI);
+
+			vlan_tci &= ~VLAN_VID_MASK;
+			vlan_tci |= sw->vid;
+			vlan->h_vlan_TCI = htons(vlan_tci);
+
+		/* Need to remove VLAN tag manually. */
+		} else if (!(sw->overrides & TAG_REMOVE)) {
+			u8 *data;
+
+			len = VLAN_ETH_HLEN - 2;
+			data = &skb->data[len];
+			memmove(data - VLAN_HLEN, data, skb->len - len);
+			skb->len -= VLAN_HLEN;
+		}
+	} while (0);
+#endif
+
+#ifdef CONFIG_1588_PTP
+	if (ptp) {
+		int blocked;
+		u32 dst = port;
+		u32 overrides = ptp->overrides;
+
+		if (!dst && ptp->version < 1)
+			dst = 3;
+		if (ptp->features & PTP_PDELAY_HACK) {
+			dst |= (u32) sw->tx_ports << 16;
+			overrides |= PTP_UPDATE_DST_PORT;
+		}
+		blocked = update_msg(skb->data, dst, overrides);
+		if (blocked) {
+			dev_kfree_skb_irq(skb);
+			return NULL;
+		}
+	}
+#endif
+	if (!(sw->overrides & TAIL_TAGGING))
+		return skb;
+
+	dest = 0;
+	if (port) {
+		port = get_phy_port(sw, port);
+		dest = 1 << port;
+	}
+
+	/* Check the socket buffer length is enough to hold the tail tag. */
+	if (skb->len < ETH_ZLEN)
+		padlen = ETH_ZLEN - skb->len;
+	len = skb_tailroom(skb);
+	if (len < 1 + padlen) {
+		need_new_copy = true;
+		len = (skb->len + padlen + 4) & ~3;
+	}
+	if (skb_headroom(skb) < headlen) {
+		need_new_copy = true;
+	}
+	if (need_new_copy) {
+		int headerlen = skb_headroom(skb);
+
+		if (headerlen < headlen)
+			headerlen = headlen;
+		skb = skb_copy_expand(org_skb, headerlen, len, GFP_ATOMIC);
+		if (!skb)
+			return NULL;
+		consume_skb(org_skb);
+	}
+
+	/* skb_put requires tail pointer set first. */
+	skb_set_tail_pointer(skb, skb->len);
+	if (padlen) {
+		if (__skb_put_padto(skb, skb->len + padlen, false))
+			return NULL;
+	}
+	len = skb->len;
+
+	if (!dest) {
+#ifdef CONFIG_KSZ_HSR
+		skb = sw_ins_hsr(sw, priv->first_port, skb, &dest);
+		if (!skb)
+			return NULL;
+		len = skb->len;
+#endif
+	}	
+	if (!dest) {
+		/* Use VLAN for port forwarding if not specified directly. */
+		skb = sw_ins_vlan(sw, priv->first_port, skb);
+		if (len != skb->len)
+			len = skb->len;
+	}
+	skb->data[len] = dest;
+	skb_put(skb, 1);
+
+	/* Need to compensate checksum for some devices. */
+	if (skb->ip_summed != CHECKSUM_PARTIAL)
+		dest = 0;
+	if (dest && (sw->overrides & UPDATE_CSUM)) {
+		__sum16 *csum_loc = (__sum16 *)
+			(skb->head + skb->csum_start + skb->csum_offset);
+
+		/* Checksum is cleared by driver to be filled by hardware. */
+		if (!*csum_loc) {
+			__sum16 new_csum;
+
+			new_csum = dest << 8;
+			*csum_loc = ~htons(new_csum);
+		}
+	}
+	return skb;
+}  /* sw_check_skb */
+
+static struct sk_buff *sw_check_tx(struct ksz_sw *sw, struct net_device *dev,
+	struct sk_buff *skb, struct ksz_port *priv)
+{
+	void *ptr = NULL;
+	int (*update_msg)(u8 *data, u32 port, u32 overrides) = NULL;
+
+#ifdef CONFIG_1588_PTP
+	if (sw->features & PTP_HW) {
+		struct ptp_info *ptp = &sw->ptp_hw;
+
+		ptr = ptp;
+		update_msg = ptp->ops->update_msg;
+	}
+#endif
+
+	return sw_check_skb(sw, skb, priv, ptr, update_msg);
+}  /* sw_check_tx */
+
+static struct sk_buff *sw_final_skb(struct ksz_sw *sw, struct sk_buff *skb,
+	struct net_device *dev, struct ksz_port *port)
+{
+	skb = sw->net_ops->check_tx(sw, dev, skb, port);
+	if (!skb)
+		return NULL;
+
+#ifdef CONFIG_1588_PTP
+	if (sw->features & PTP_HW) {
+		struct ptp_info *ptp = &sw->ptp_hw;
+
+		if (skb_shinfo(skb)->tx_flags & SKBTX_HW_TSTAMP)
+			ptp->ops->get_tx_tstamp(ptp, skb);
+	}
+#endif
+	return skb;
+}  /* sw_final_skb */
+
+static void sw_start(struct ksz_sw *sw, u8 *addr)
+{
+	int need_tail_tag = false;
+	int need_vlan = false;
+
+	sw->ops->acquire(sw);
+	sw_setup(sw);
+	sw_enable(sw);
+	sw_set_addr(sw, addr);
+	if (1 == sw->dev_count)
+		sw_setup_src_filter(sw, addr);
+#ifdef CONFIG_KSZ_DLR
+	if (sw->features & DLR_HW)
+		need_tail_tag = true;
+#endif
+#ifdef CONFIG_KSZ_HSR
+	if (sw->features & HSR_HW)
+		need_tail_tag = true;
+#endif
+	if (sw->dev_count > 1 && !(sw->features & SW_VLAN_DEV))
+		need_tail_tag = true;
+	if (sw->features & VLAN_PORT) {
+		if (sw->features & VLAN_PORT_REMOVE_TAG) {
+			uint n;
+			uint p;
+
+			for (n = 0; n < SWITCH_PORT_NUM; n++) {
+				p = n;
+				port_cfg_rmv_tag(sw, p, true);
+			}
+			sw->overrides |= TAG_REMOVE;
+		}
+		if (sw->features & VLAN_PORT_TAGGING)
+			need_tail_tag = true;
+	} else if (sw->features & SW_VLAN_DEV) {
+		struct ksz_dev_map *map;
+		int i;
+		int p;
+		uint port;
+		uint q;
+
+		for (p = 0; p < sw->eth_cnt; p++) {
+
+			/* Not really using VLAN. */
+			if (1 == sw->eth_maps[p].vlan)
+				continue;
+
+			map = &sw->eth_maps[p];
+
+			/*
+			 * Setting FID allows same MAC address in different
+			 * VLANs.
+			 */
+			sw->ops->release(sw);
+			sw_w_vlan_table(sw, p + 1,
+				map->vlan,
+				map->vlan & (FID_ENTRIES - 1),
+				sw->HOST_MASK | map->mask, true);
+			sw->ops->acquire(sw);
+			for (i = 0, q = map->first;
+			     i < map->cnt; i++, q++) {
+				port = get_phy_port(sw, q);
+				sw_cfg_def_vid(sw, port, map->vlan);
+				port_cfg_rmv_tag(sw, port, true);
+			}
+		}
+
+		/* Use tail tag to determine the network device. */
+		if (need_tail_tag)
+			port_cfg_rmv_tag(sw, sw->HOST_PORT, true);
+		else
+			port_cfg_ins_tag(sw, sw->HOST_PORT, true);
+		need_vlan = true;
+	}
+	if (sw->features & STP_SUPPORT)
+		need_tail_tag = true;
+	if (need_tail_tag) {
+		sw_cfg_tail_tag(sw, true);
+		sw->overrides |= TAIL_TAGGING;
+	}
+	if (need_vlan) {
+		sw_ena_vlan(sw);
+	}
+	sw_ena_intr(sw);
+	sw->ops->release(sw);
+
+#ifdef CONFIG_KSZ_STP
+	if (sw->features & STP_SUPPORT) {
+		struct ksz_stp_info *stp = &sw->info->rstp;
+
+		if (stp->br.bridgeEnabled)
+			stp_start(stp);
+	} else
+		stp_set_addr(&sw->info->rstp, sw->info->mac_addr);
+#endif
+
+#ifdef CONFIG_1588_PTP
+	if (sw->features & PTP_HW) {
+		struct ptp_info *ptp = &sw->ptp_hw;
+
+		ptp->reg->start(ptp, true);
+	}
+#endif
+}  /* sw_start */
+
+static int sw_stop(struct ksz_sw *sw, int complete)
+{
+	int reset = false;
+
+#ifdef CONFIG_KSZ_STP
+	if (sw->features & STP_SUPPORT) {
+		struct ksz_stp_info *stp = &sw->info->rstp;
+
+		if (stp->br.bridgeEnabled)
+			stp_stop(stp, true);
+	}
+#endif
+
+#ifdef CONFIG_1588_PTP
+	if (sw->features & PTP_HW) {
+		struct ptp_info *ptp = &sw->ptp_hw;
+
+		reset = ptp->ops->stop(ptp, true);
+	}
+#endif
+	sw->ops->acquire(sw);
+	if (!reset)
+		sw_reset(sw);
+	reset = true;
+	sw_init(sw);
+	sw->ops->release(sw);
+
+	/* Clean out static MAC table when the switch shutdown. */
+	if ((sw->features & STP_SUPPORT) && complete)
+		sw_clr_sta_mac_table(sw);
+	return reset;
+}  /* sw_stop */
+
+static void sw_init_mib(struct ksz_sw *sw)
+{
+	unsigned long interval;
+	uint n;
+	uint p;
+
+	interval = MIB_READ_INTERVAL * 2 / (sw->mib_port_cnt + 1);
+	for (n = 0; n <= sw->mib_port_cnt; n++) {
+		p = get_phy_port(sw, n);
+		sw->port_mib[p].mib_start = 0;
+		if (sw->next_jiffies < jiffies)
+			sw->next_jiffies = jiffies + HZ * 2;
+		else
+			sw->next_jiffies += interval;
+		sw->counter[p].time = sw->next_jiffies;
+		sw->port_state[p].state = media_disconnected;
+		port_init_cnt(sw, p);
+	}
+	sw->port_state[sw->HOST_PORT].state = media_connected;
+}  /* sw_init_mib */
+
+#ifdef CONFIG_PHYLINK
+static void sw_set_phylink_support(struct ksz_sw *sw, struct ksz_port *port,
+				   unsigned long *supported,
+				   struct phylink_link_state *state)
+{
+	__ETHTOOL_DECLARE_LINK_MODE_MASK(mask) = { 0, };
+
+	phylink_set(mask, TP);
+	phylink_set(mask, MII);
+
+	phylink_set(mask, Autoneg);
+
+	phylink_set(mask, 10baseT_Half);
+	phylink_set(mask, 10baseT_Full);
+	phylink_set(mask, 100baseT_Half);
+	phylink_set(mask, 100baseT_Full);
+
+	switch (port->flow_ctrl) {
+	case PHY_NO_FLOW_CTRL:
+		phylink_clear(mask, Pause);
+		phylink_clear(mask, Asym_Pause);
+		break;
+	case PHY_RX_ONLY:
+		/* Depend on link partner to only advertise Asym_Pause. */
+		phylink_set(mask, Pause);
+		phylink_set(mask, Asym_Pause);
+		break;
+	case PHY_TX_ONLY:
+		phylink_set(mask, Asym_Pause);
+		phylink_clear(mask, Pause);
+		break;
+	default:
+		phylink_set(mask, Pause);
+		phylink_clear(mask, Asym_Pause);
+	}
+
+	bitmap_and(supported, supported, mask, __ETHTOOL_LINK_MODE_MASK_NBITS);
+	bitmap_and(state->advertising, state->advertising, mask,
+		   __ETHTOOL_LINK_MODE_MASK_NBITS);
+	linkmode_copy(port->phydev->supported, supported);
+	linkmode_copy(port->phydev->advertising, state->advertising);
+}  /* sw_set_phylink_support */
+
+static void sw_port_phylink_get_fixed_state(struct phylink_config *config,
+					    struct phylink_link_state *s)
+{
+	struct ksz_port *p = container_of(config, struct ksz_port, pl_config);
+	struct ksz_sw *sw = p->sw;
+	struct ksz_port_info *info = get_port_info(sw, sw->HOST_PORT);
+
+	s->interface = sw->interface;
+	s->speed = info->tx_rate / TX_RATE_UNIT;
+	s->duplex = 1;
+	s->pause = 3;
+	s->link = 1;
+	s->an_enabled = 0;
+	s->an_complete = 0;
+}  /* sw_port_phylink_get_fixed_state */
+
+static void sw_port_phylink_validate(struct phylink_config *config,
+				     unsigned long *supported,
+				     struct phylink_link_state *state)
+{
+	struct ksz_port *p = container_of(config, struct ksz_port, pl_config);
+	struct ksz_sw *sw = p->sw;
+
+dbg_msg(" validate: %d\n", state->interface);
+	if ((sw->dev_offset && p->port_cnt > 1) ||
+	    (!sw->dev_offset && !sw->phy_offset)) {
+		if (sw->phylink_ops && sw->phylink_ops->validate)
+			sw->phylink_ops->validate(config, supported, state);
+	} else {
+		sw_set_phylink_support(sw, p, supported, state);
+	}
+}
+
+static int sw_port_phylink_mac_prepare(struct phylink_config *config,
+				       unsigned int mode,
+				       phy_interface_t interface)
+{
+	struct ksz_port *p = container_of(config, struct ksz_port, pl_config);
+	struct ksz_sw *sw = p->sw;
+
+	if (sw->phylink_ops && sw->phylink_ops->mac_prepare)
+		return sw->phylink_ops->mac_prepare(config, mode, interface);
+	return 0;
+}
+
+static void sw_port_phylink_mac_config(struct phylink_config *config,
+				       unsigned int mode,
+				       const struct phylink_link_state *state)
+{
+	/* The switch is always connected to the MAC. */
+}
+
+static void sw_port_phylink_mac_link_down(struct phylink_config *config,
+					  unsigned int mode,
+					  phy_interface_t interface)
+{
+	struct ksz_port *p = container_of(config, struct ksz_port, pl_config);
+	struct ksz_sw *sw = p->sw;
+
+	/* Tell MAC driver to turn off transmit queues. */
+	interface = PHY_INTERFACE_MODE_INTERNAL;
+	if (sw->phylink_ops && sw->phylink_ops->mac_link_down)
+		sw->phylink_ops->mac_link_down(config, mode, interface);
+}
+
+static void sw_port_phylink_mac_link_up(struct phylink_config *config,
+					struct phy_device *phydev,
+					unsigned int mode,
+					phy_interface_t interface,
+					int speed, int duplex,
+					bool tx_pause, bool rx_pause)
+{
+	struct ksz_port *p = container_of(config, struct ksz_port, pl_config);
+	struct ksz_sw *sw = p->sw;
+
+	/* Tell MAC driver to turn on transmit queues. */
+	interface = PHY_INTERFACE_MODE_INTERNAL;
+	if (sw->phylink_ops && sw->phylink_ops->mac_link_up)
+		sw->phylink_ops->mac_link_up(config, phydev, mode,
+					     interface, speed, duplex,
+					     tx_pause, rx_pause);
+}
+
+static const struct phylink_mac_ops sw_port_phylink_mac_ops = {
+	.validate = sw_port_phylink_validate,
+	.mac_prepare = sw_port_phylink_mac_prepare,
+	.mac_config = sw_port_phylink_mac_config,
+	.mac_link_down = sw_port_phylink_mac_link_down,
+	.mac_link_up = sw_port_phylink_mac_link_up,
+};
+
+static int setup_phylink(struct ksz_port *port)
+{
+	struct device_node *dn = port->dn;
+	phy_interface_t mode;
+	int ret;
+
+	ret = of_get_phy_mode(dn, &mode);
+	if (ret)
+		mode = PHY_INTERFACE_MODE_NA;
+
+	port->pl_config.dev = &port->netdev->dev;
+
+	/* netif_carrier_on is called automatically for netdevice. */
+	port->pl_config.type = PHYLINK_NETDEV;
+
+#if 0
+	/* netif_carrier_on is not called for base device. */
+	port->pl_config.type = PHYLINK_DEV;
+#endif
+	port->pl_config.get_fixed_state = sw_port_phylink_get_fixed_state;
+
+	port->pl = phylink_create(&port->pl_config, of_fwnode_handle(dn), mode,
+				  &sw_port_phylink_mac_ops);
+	if (IS_ERR(port->pl)) {
+		netdev_err(port->netdev,
+			   "error creating PHYLINK: %ld\n", PTR_ERR(port->pl));
+		return PTR_ERR(port->pl);
+	}
+
+	return ret;
+}
+
+static void sw_exit_phylink(struct ksz_sw *sw, struct ksz_port *port)
+{
+	const struct phylink_mac_ops *ops = sw->phylink_ops;
+
+	if (ops && port) {
+		unsigned int mode = MLO_AN_PHY;
+		struct phylink_config *config;
+
+		config = &port->pl_config;
+		ops->mac_link_down(config, mode, sw->interface);
+	}
+}  /* sw_exit_phylink */
+
+static void sw_init_phylink(struct ksz_sw *sw, struct ksz_port *port)
+{
+	const struct phylink_mac_ops *ops = sw->phylink_ops;
+
+	if (ops && port) {
+		struct phylink_link_state *state;
+		unsigned int mode = MLO_AN_PHY;
+		struct phylink_config *config;
+		bool rx_pause = true;
+		bool tx_pause = true;
+
+		config = &port->pl_config;
+		state = &port->pl_state;
+		sw_port_phylink_get_fixed_state(config, state);
+		ops->mac_config(config, mode, state);
+		ops->mac_link_up(config, port->phydev, mode,
+				 sw->interface,
+				 state->speed, state->duplex,
+				 tx_pause, rx_pause);
+	}
+}  /* sw_init_phylink */
+#endif
+
+static void setup_device_node(struct ksz_sw *sw)
+{
+	struct sw_priv *ks = sw->dev;
+	struct device_node *np;
+	struct device_node *ports, *port;
+	struct device_node *ethernet;
+	const char *name;
+	int err;
+	u32 reg;
+
+	if (!ks->of_dev)
+		return;
+	np = ks->of_dev->of_node;
+	if (np) {
+		ports = of_get_child_by_name(np, "ports");
+		if (ports) {
+			for_each_available_child_of_node(ports, port) {
+				err = of_property_read_u32(port, "reg", &reg);
+				if (err)
+					break;
+dbg_msg(" reg: %d\n", reg);
+				ethernet = of_parse_phandle(port, "ethernet", 0);
+				if (ethernet)
+dbg_msg(" found eth\n");
+				name = of_get_property(port, "label", NULL);
+				if (name)
+dbg_msg(" name: %s\n", name);
+				/* Save the device node. */
+				sw->devnode[reg] = port;
+			}
+		}
+	}
+}
+
+static int sw_open_dev(struct ksz_sw *sw, struct net_device *dev,
+	struct ksz_port *port, u8 *addr)
+{
+	int mode = 0;
+
+	sw_init_mib(sw);
+
+	sw->main_dev = dev;
+	sw->main_port = port;
+	sw->net_ops->start(sw, addr);
+	if (sw->dev_count > 1)
+		mode |= 1;
+	if (sw->features & DIFF_MAC_ADDR)
+		mode |= 2;
+	return mode;
+}  /* sw_open_dev */
+
+static void sw_open_port(struct ksz_sw *sw, struct net_device *dev,
+	struct ksz_port *port)
+{
+	uint i;
+	uint n;
+	uint p;
+	struct ksz_port_info *info;
+	struct ksz_port_info *host;
+
+	host = get_port_info(sw, sw->HOST_PORT);
+	for (i = 0, n = port->first_port; i < port->port_cnt; i++, n++) {
+		p = get_phy_port(sw, n);
+		info = get_port_info(sw, p);
+		/*
+		 * Initialize to invalid value so that link detection
+		 * is done.
+		 */
+		info->partner = 0xFF;
+		info->state = media_unknown;
+		info->tx_rate = host->tx_rate;
+		info->duplex = host->duplex;
+		if (port->port_cnt == 1) {
+			if (sw->netdev[0]) {
+				struct ksz_port *sw_port = sw->netport[0];
+
+				port->speed = sw_port->speed;
+				port->duplex = sw_port->duplex;
+				port->flow_ctrl = sw_port->flow_ctrl;
+			}
+			if (info->own_speed != port->speed ||
+			    info->own_duplex != port->duplex) {
+				if (info->own_speed)
+					port->speed = info->own_speed;
+				if (info->own_duplex)
+					port->duplex = info->own_duplex;
+			}
+		}
+	}
+	port->opened = true;
+	port->report = true;
+
+	sw->ops->acquire(sw);
+
+	/* Need to open the port in multiple device interfaces mode. */
+	if (sw->dev_count > 1 && (!sw->dev_offset || dev != sw->netdev[0])) {
+		port->state = STP_STATE_SIMPLE;
+		if (sw->dev_offset && !(sw->features & STP_SUPPORT)) {
+			port->state = STP_STATE_FORWARDING;
+		}
+		if (sw->features & SW_VLAN_DEV) {
+			p = get_phy_port(sw, port->first_port);
+			i = sw->info->port_cfg[p].index;
+			if (!(sw->eth_maps[i].proto & HSR_HW))
+				port->state = STP_STATE_FORWARDING;
+		}
+		for (i = 0, n = port->first_port; i < port->port_cnt;
+		     i++, n++) {
+			p = get_phy_port(sw, n);
+			sw->dev_ports |= (1 << p);
+#ifdef CONFIG_KSZ_STP
+			if (sw->features & STP_SUPPORT) {
+				stp_enable_port(&sw->info->rstp, p,
+						&port->state);
+			}
+#endif
+			port_set_stp_state(sw, p, port->state);
+		}
+		port_set_addr(sw, port->first_port, dev->dev_addr);
+		port_cfg_src_filter_0(sw, port->first_port, 1);
+		port_cfg_src_filter_1(sw, port->first_port, 1);
+	} else if (sw->dev_count == 1) {
+		sw->dev_ports = sw->PORT_MASK;
+	}
+
+	if (port->force_link)
+		port_force_link_speed(port);
+	else
+		port_set_link_speed(port);
+	port_get_link_speed(port);
+	if (port->link_ports)
+		schedule_delayed_work(&port->link_update, 0);
+	sw->ops->release(sw);
+
+#ifdef CONFIG_KSZ_DLR
+	if (sw->features & DLR_HW) {
+		struct ksz_dlr_info *info = &sw->info->dlr;
+
+		p = get_phy_port(sw, port->first_port);
+		if (info->ports[0] == p)
+			prep_dlr(info, sw->main_dev, sw->main_dev->dev_addr);
+	}
+#endif
+#ifdef CONFIG_KSZ_HSR
+	if (sw->features & HSR_HW) {
+		struct ksz_hsr_info *info = &sw->info->hsr;
+
+		p = get_phy_port(sw, port->first_port);
+		if (info->ports[0] == p)
+			prep_hsr(info, sw->main_dev, sw->main_dev->dev_addr);
+	}
+#endif
+}  /* sw_open_port */
+
+static void sw_close_port(struct ksz_sw *sw, struct net_device *dev,
+	struct ksz_port *port)
+{
+	int i;
+	uint p;
+
+	port->opened = false;
+
+	/* Need to shut the port manually in multiple device interfaces mode. */
+	if (sw->dev_count > 1 && (!sw->dev_offset || dev != sw->netdev[0])) {
+		uint n;
+
+		sw->ops->acquire(sw);
+		for (i = 0, n = port->first_port; i < port->port_cnt;
+		     i++, n++) {
+			p = get_phy_port(sw, n);
+#ifdef CONFIG_KSZ_STP
+			if (sw->features & STP_SUPPORT)
+				stp_disable_port(&sw->info->rstp, p);
+#endif
+#ifdef CONFIG_KSZ_MRP
+			if (sw->features & MRP_SUPPORT) {
+				mrp_close_port(mrp, p);
+			}
+#endif
+			sw->dev_ports &= ~(1 << p);
+			port_set_stp_state(sw, p, STP_STATE_DISABLED);
+		}
+		sw->ops->release(sw);
+	} else if (sw->dev_count == 1) {
+#ifdef CONFIG_KSZ_MRP
+		uint n;
+
+		if (sw->features & MRP_SUPPORT) {
+			for (n = 0; n <= sw->mib_port_cnt; n++) {
+				p = get_phy_port(sw, n);
+				mrp_close_port(mrp, p);
+			}
+		}
+#endif
+		sw->dev_ports = 0;
+	}
+#ifdef CONFIG_KSZ_HSR
+	if (sw->features & HSR_HW) {
+		struct ksz_hsr_info *info = &sw->info->hsr;
+
+		p = get_phy_port(sw, port->first_port);
+		if (info->ports[0] == p)
+			stop_hsr(info);
+	}
+#endif
+}  /* sw_close_port */
+
+static void sw_open(struct ksz_sw *sw)
+{
+#ifdef CONFIG_PHYLINK
+	sw_init_phylink(sw, sw->main_port);
+#endif
+#ifdef CONFIG_KSZ_DLR
+	prep_dlr(&sw->info->dlr, sw->main_dev, sw->main_dev->dev_addr);
+#endif
+
+	/* Timer may already be started by the SPI device. */
+	if (!sw->monitor_timer_info->max)
+		ksz_start_timer(sw->monitor_timer_info,
+			sw->monitor_timer_info->period);
+}  /* sw_open */
+
+static void sw_close(struct ksz_sw *sw)
+{
+#ifdef CONFIG_PHYLINK
+	sw_exit_phylink(sw, sw->main_port);
+#endif
+	ksz_stop_timer(sw->monitor_timer_info);
+	cancel_delayed_work_sync(sw->link_read);
+}  /* sw_close */
+
+static u8 sw_set_mac_addr(struct ksz_sw *sw, struct net_device *dev,
+	u8 promiscuous, uint port)
+{
+	sw->ops->acquire(sw);
+	if (sw->dev_count > 1 && (!sw->dev_offset || dev != sw->netdev[0])) {
+		port_set_addr(sw, port, dev->dev_addr);
+		if (sw->features & DIFF_MAC_ADDR) {
+			sw->features &= ~DIFF_MAC_ADDR;
+			--promiscuous;
+		}
+		for (port = 0; port < SWITCH_PORT_NUM; port++)
+			if (memcmp(sw->port_info[port].mac_addr,
+					dev->dev_addr, ETH_ALEN)) {
+				sw->features |= DIFF_MAC_ADDR;
+				++promiscuous;
+				break;
+			}
+	} else {
+		int i;
+
+		sw_setup_src_filter(sw, dev->dev_addr);
+
+		/* Make MAC address the same in all the ports. */
+		if (sw->dev_count > 1) {
+			for (i = 0; i < SWITCH_PORT_NUM; i++)
+				memcpy(sw->netdev[i + 1]->dev_addr,
+					dev->dev_addr, ETH_ALEN);
+			if (sw->features & DIFF_MAC_ADDR) {
+				sw->features &= ~DIFF_MAC_ADDR;
+				--promiscuous;
+			}
+		}
+		if (dev == sw->netdev[0])
+			sw_set_addr(sw, dev->dev_addr);
+	}
+	sw->ops->release(sw);
+#ifdef CONFIG_KSZ_DLR
+	dlr_change_addr(&sw->info->dlr, dev->dev_addr);
+#endif
+	return promiscuous;
+}  /* sw_set_mac_addr */
+
+static struct ksz_dev_major sw_majors[MAX_SW_DEVICES];
+
+static struct file_dev_info *alloc_sw_dev_info(struct ksz_sw *sw, uint minor)
+{
+	struct file_dev_info *info;
+
+	info = kzalloc(sizeof(struct file_dev_info), GFP_KERNEL);
+	if (info) {
+		info->dev = sw;
+		sema_init(&info->sem, 1);
+		mutex_init(&info->lock);
+		init_waitqueue_head(&info->wait_msg);
+		info->read_max = 60000;
+		info->read_tmp = MAX_SW_LEN;
+		info->read_buf = kzalloc(info->read_max + info->read_tmp,
+			GFP_KERNEL);
+		info->read_in = &info->read_buf[info->read_max];
+		info->write_len = 1000;
+		info->write_buf = kzalloc(info->write_len, GFP_KERNEL);
+
+		info->minor = minor;
+		info->next = sw->dev_list[minor];
+		sw->dev_list[minor] = info;
+	}
+	return info;
+}  /* alloc_sw_dev_info */
+
+static void free_sw_dev_info(struct file_dev_info *info)
+{
+	if (info) {
+		struct ksz_sw *sw = info->dev;
+		uint minor = info->minor;
+
+		file_dev_clear_notify(sw->dev_list[minor], info, DEV_MOD_BASE,
+				      &sw->notifications);
+		file_gen_dev_release(info, &sw->dev_list[minor]);
+	}
+}  /* free_sw_dev_info */
+
+static int sw_dev_open(struct inode *inode, struct file *filp)
+{
+	struct file_dev_info *info = (struct file_dev_info *)
+		filp->private_data;
+	uint minor = MINOR(inode->i_rdev);
+	uint major = MAJOR(inode->i_rdev);
+	struct ksz_sw *sw = NULL;
+	int i;
+
+	if (minor > 1)
+		return -ENODEV;
+	for (i = 0; i < MAX_SW_DEVICES; i++) {
+		if (sw_majors[i].major == major) {
+			sw = sw_majors[i].dev;
+			break;
+		}
+	}
+	if (!sw)
+		return -ENODEV;
+	if (!info) {
+		info = alloc_sw_dev_info(sw, minor);
+		if (info)
+			filp->private_data = info;
+		else
+			return -ENOMEM;
+	}
+	return 0;
+}  /* sw_dev_open */
+
+static int sw_dev_release(struct inode *inode, struct file *filp)
+{
+	struct file_dev_info *info = (struct file_dev_info *)
+		filp->private_data;
+
+	free_sw_dev_info(info);
+	filp->private_data = NULL;
+	return 0;
+}  /* sw_dev_release */
+
+static int sw_get_attrib(struct ksz_sw *sw, int subcmd, int size,
+	int *req_size, size_t *len, u8 *data, int *output)
+{
+	struct ksz_info_opt *opt = (struct ksz_info_opt *) data;
+	struct ksz_info_cfg *cfg = &opt->data.cfg;
+	uint i;
+	uint n;
+	uint p;
+
+	*len = 0;
+	*output = 0;
+	switch (subcmd) {
+	case DEV_SW_CFG:
+		n = opt->num;
+		p = opt->port;
+		if (!n)
+			n = 1;
+		*len = 2 + n * sizeof(struct ksz_info_cfg);
+		if (n > sw->mib_port_cnt ||
+		    p > sw->mib_port_cnt)
+			return DEV_IOC_INVALID_CMD;
+		break;
+	}
+	if (!*len)
+		return DEV_IOC_INVALID_CMD;
+	if (size < *len) {
+		*req_size = *len + SIZEOF_ksz_request;
+		return DEV_IOC_INVALID_LEN;
+	}
+	switch (subcmd) {
+	case DEV_SW_CFG:
+		n = opt->port;
+		sw->ops->acquire(sw);
+		for (i = 0; i < opt->num; i++, n++) {
+			p = get_phy_port(sw, n);
+			cfg->on_off = 0;
+			if (cfg->set & SP_LEARN) {
+				if (!port_chk_dis_learn(sw, p))
+					cfg->on_off |= SP_LEARN;
+			}
+			if (cfg->set & SP_RX) {
+				if (port_chk_rx(sw, p))
+					cfg->on_off |= SP_RX;
+			}
+			if (cfg->set & SP_TX) {
+				if (port_chk_tx(sw, p))
+					cfg->on_off |= SP_TX;
+			}
+			if (p == sw->HOST_PORT)
+				continue;
+#if 0
+			if (cfg->set & SP_PHY_POWER) {
+				if (port_chk_power(sw, p))
+					cfg->on_off |= SP_PHY_POWER;
+			}
+#endif
+			cfg++;
+		}
+		sw->ops->release(sw);
+		break;
+	}
+	return DEV_IOC_OK;
+}  /* sw_get_attrib */
+
+static int sw_set_attrib(struct ksz_sw *sw, int subcmd, int size,
+	int *req_size, u8 *data, int *output)
+{
+	struct ksz_info_opt *opt = (struct ksz_info_opt *) data;
+	struct ksz_info_cfg *cfg = &opt->data.cfg;
+	int len;
+	uint i;
+	uint n;
+	uint p;
+
+	*output = 0;
+	switch (subcmd) {
+	case DEV_SW_CFG:
+		n = opt->num;
+		p = opt->port;
+		if (!n)
+			n = 1;
+		len = 2 + n * sizeof(struct ksz_info_cfg);
+		if (size < len)
+			goto not_enough;
+		if (n > sw->mib_port_cnt ||
+		    p > sw->mib_port_cnt)
+			return DEV_IOC_INVALID_CMD;
+		n = opt->port;
+		sw->ops->acquire(sw);
+		for (i = 0; i < opt->num; i++, n++) {
+			p = get_phy_port(sw, n);
+			if (cfg->set & SP_LEARN)
+				port_cfg_dis_learn(sw, p,
+					!(cfg->on_off & SP_LEARN));
+			if (cfg->set & SP_RX)
+				port_cfg_rx(sw, p,
+					!!(cfg->on_off & SP_RX));
+			if (cfg->set & SP_TX)
+				port_cfg_tx(sw, p,
+					!!(cfg->on_off & SP_TX));
+			if (p == sw->HOST_PORT)
+				continue;
+#if 0
+			if (cfg->set & SP_PHY_POWER)
+				port_cfg_power(sw, p,
+					!!(cfg->on_off & SP_PHY_POWER));
+#endif
+			cfg++;
+		}
+		sw->ops->release(sw);
+		break;
+	default:
+		return DEV_IOC_INVALID_CMD;
+	}
+	return DEV_IOC_OK;
+
+not_enough:
+	*req_size = len + SIZEOF_ksz_request;
+	return DEV_IOC_INVALID_LEN;
+}  /* sw_set_attrib */
+
+static int sw_get_info(struct ksz_sw *sw, int subcmd, int size,
+	int *req_size, size_t *len, u8 *data)
+{
+	struct ksz_info_opt *opt = (struct ksz_info_opt *) data;
+	struct ksz_info_speed *speed = &opt->data.speed;
+	struct ksz_port_info *info;
+	uint i;
+	uint n;
+	uint p;
+
+	*len = 0;
+	switch (subcmd) {
+	case DEV_INFO_SW_LINK:
+		n = opt->num;
+		p = opt->port;
+		if (!n)
+			n = 1;
+		*len = 2 + n * sizeof(struct ksz_info_speed);
+		if (n > sw->mib_port_cnt ||
+		    p > sw->mib_port_cnt)
+			return DEV_IOC_INVALID_CMD;
+		break;
+	}
+	if (!*len)
+		return DEV_IOC_INVALID_CMD;
+	if (size < *len) {
+		*req_size = *len + SIZEOF_ksz_request;
+		return DEV_IOC_INVALID_LEN;
+	}
+	switch (subcmd) {
+	case DEV_INFO_SW_LINK:
+		n = p;
+		for (i = 0; i < opt->num; i++, n++) {
+			p = get_phy_port(sw, n);
+			info = get_port_info(sw, p);
+			if (info->state == media_connected) {
+				speed->tx_rate = info->tx_rate;
+				speed->duplex = info->duplex;
+				speed->flow_ctrl = info->flow_ctrl;
+			} else {
+				memset(speed, 0, sizeof(struct ksz_info_speed));
+			}
+			++speed;
+		}
+		break;
+	}
+	return DEV_IOC_OK;
+}  /* sw_get_info */
+
+static int base_dev_req(struct ksz_sw *sw, char *arg, void *info)
+{
+	struct ksz_request *req = (struct ksz_request *) arg;
+	int len;
+	int maincmd;
+	int req_size;
+	int subcmd;
+	int output;
+	size_t param_size;
+	u8 data[PARAM_DATA_SIZE];
+	struct ksz_resp_msg *msg = (struct ksz_resp_msg *) data;
+	int err = 0;
+	int result = 0;
+
+	get_user_data(&req_size, &req->size, info);
+	get_user_data(&maincmd, &req->cmd, info);
+	get_user_data(&subcmd, &req->subcmd, info);
+	get_user_data(&output, &req->output, info);
+	len = req_size - SIZEOF_ksz_request;
+
+	maincmd &= 0xffff;
+	switch (maincmd) {
+	case DEV_CMD_INFO:
+		switch (subcmd) {
+		case DEV_INFO_INIT:
+			req_size = SIZEOF_ksz_request + 4;
+			if (len >= 4) {
+				data[0] = 'M';
+				data[1] = 'i';
+				data[2] = 'c';
+				data[3] = 'r';
+				data[4] = 1;
+				data[5] = sw->mib_port_cnt;
+				err = write_user_data(data, req->param.data,
+					6, info);
+				if (err)
+					goto dev_ioctl_done;
+			} else
+				result = DEV_IOC_INVALID_LEN;
+			break;
+		case DEV_INFO_EXIT:
+			fallthrough;
+
+		case DEV_INFO_QUIT:
+
+			/* Not called through char device. */
+			if (!info)
+				break;
+			file_dev_clear_notify(sw->dev_list[0], info,
+					      DEV_MOD_BASE,
+					      &sw->notifications);
+			msg->module = DEV_MOD_BASE;
+			msg->cmd = DEV_INFO_QUIT;
+			msg->resp.data[0] = 0;
+			file_dev_setup_msg(info, msg, 8, NULL, NULL);
+			sw->notifications = 0;
+			break;
+		case DEV_INFO_NOTIFY:
+			if (len >= 4) {
+				struct file_dev_info *dev_info = info;
+				uint *notify = (uint *) data;
+
+				_chk_ioctl_size(len, 4, 0, &req_size, &result,
+					&req->param, data, info);
+				dev_info->notifications[DEV_MOD_BASE] =
+					*notify;
+				sw->notifications |= *notify;
+			}
+			break;
+		case DEV_INFO_SW_LINK:
+			if (_chk_ioctl_size(len, len, 0, &req_size, &result,
+			    &req->param, data, info))
+				goto dev_ioctl_resp;
+			result = sw_get_info(sw, subcmd, len, &req_size,
+					     &param_size, data);
+			if (result)
+				goto dev_ioctl_resp;
+			err = write_user_data(data, req->param.data,
+					      param_size, info);
+			if (err)
+				goto dev_ioctl_done;
+			req_size = param_size + SIZEOF_ksz_request;
+			break;
+		default:
+			result = DEV_IOC_INVALID_CMD;
+			break;
+		}
+		break;
+	case DEV_CMD_PUT:
+		if (_chk_ioctl_size(len, len, 0, &req_size, &result,
+		    &req->param, data, info))
+			goto dev_ioctl_resp;
+		result = sw_set_attrib(sw, subcmd, len, &req_size, data,
+			&output);
+		if (result)
+			goto dev_ioctl_resp;
+		put_user_data(&output, &req->output, info);
+		break;
+	case DEV_CMD_GET:
+		if (_chk_ioctl_size(len, len, 0, &req_size, &result,
+		    &req->param, data, info))
+			goto dev_ioctl_resp;
+		result = sw_get_attrib(sw, subcmd, len, &req_size,
+			&param_size, data, &output);
+		if (result)
+			goto dev_ioctl_resp;
+		err = write_user_data(data, req->param.data, param_size, info);
+		if (err)
+			goto dev_ioctl_done;
+		req_size = param_size + SIZEOF_ksz_request;
+		put_user_data(&output, &req->output, info);
+		break;
+	default:
+		result = DEV_IOC_INVALID_CMD;
+		break;
+	}
+
+dev_ioctl_resp:
+	put_user_data(&req_size, &req->size, info);
+	put_user_data(&result, &req->result, info);
+
+	/* Return ERESTARTSYS so that the system call is called again. */
+	if (result < 0)
+		err = result;
+
+dev_ioctl_done:
+	return err;
+}  /* base_dev_req */
+
+static int sw_dev_req(struct ksz_sw *sw, char *arg,
+	struct file_dev_info *info)
+{
+	struct ksz_request *req = (struct ksz_request *) arg;
+	int maincmd;
+	int req_size;
+	int err = 0;
+	int result = DEV_IOC_OK;
+
+	/* Check request size. */
+	get_user_data(&req_size, &req->size, info);
+	if (chk_ioctl_size(req_size, SIZEOF_ksz_request, 0, &req_size,
+	    &result, NULL, NULL))
+		goto dev_ioctl_resp;
+
+	result = -EOPNOTSUPP;
+	get_user_data(&maincmd, &req->cmd, info);
+	maincmd >>= 16;
+	switch (maincmd) {
+	case DEV_MOD_BASE:
+		err = base_dev_req(sw, arg, info);
+		result = 0;
+		break;
+#ifdef CONFIG_KSZ_DLR
+	case DEV_MOD_DLR:
+		if (sw->features & DLR_HW) {
+			struct ksz_dlr_info *dlr = &sw->info->dlr;
+
+			err = dlr->ops->dev_req(dlr, arg, info);
+			result = 0;
+		}
+		break;
+#endif
+	default:
+		break;
+	}
+
+	/* Processed by specific module. */
+	if (!result)
+		return err;
+	if (result < 0)
+		goto dev_ioctl_done;
+
+dev_ioctl_resp:
+	put_user_data(&req_size, &req->size, info);
+	put_user_data(&result, &req->result, info);
+
+dev_ioctl_done:
+
+	/* Return ERESTARTSYS so that the system call is called again. */
+	if (result < 0)
+		err = result;
+
+	return err;
+}  /* sw_dev_req */
+
+static ssize_t sw_dev_read(struct file *filp, char *buf, size_t count,
+	loff_t *offp)
+{
+	struct file_dev_info *info = (struct file_dev_info *)
+		filp->private_data;
+	ssize_t result = 0;
+	int rc;
+
+	if (!info->read_len) {
+		*offp = 0;
+		rc = wait_event_interruptible(info->wait_msg,
+			0 != info->read_len);
+
+		/* Cannot continue if ERESTARTSYS. */
+		if (rc < 0)
+			return 0;
+	}
+
+	if (down_interruptible(&info->sem))
+		return -ERESTARTSYS;
+
+	mutex_lock(&info->lock);
+	if (*offp >= info->read_len) {
+		info->read_len = 0;
+		count = 0;
+		*offp = 0;
+		goto dev_read_done;
+	}
+
+	if (*offp + count > info->read_len) {
+		count = info->read_len - *offp;
+		info->read_len = 0;
+	}
+
+	if (copy_to_user(buf, &info->read_buf[*offp], count)) {
+		result = -EFAULT;
+		goto dev_read_done;
+	}
+	if (info->read_len)
+		*offp += count;
+	else
+		*offp = 0;
+	result = count;
+
+dev_read_done:
+	mutex_unlock(&info->lock);
+	up(&info->sem);
+	return result;
+}  /* sw_dev_read */
+
+static long sw_dev_ioctl(struct file *filp, unsigned int cmd,
+	unsigned long arg)
+{
+	struct file_dev_info *info = (struct file_dev_info *)
+		filp->private_data;
+	struct ksz_sw *sw = info->dev;
+	int err = 0;
+
+	if (_IOC_TYPE(cmd) != DEV_IOC_MAGIC)
+		return -ENOTTY;
+	if (_IOC_NR(cmd) > DEV_IOC_MAX)
+		return -ENOTTY;
+	if (_IOC_DIR(cmd) & _IOC_READ)
+		err = !access_ok((void *) arg, _IOC_SIZE(cmd));
+	else if (_IOC_DIR(cmd) & _IOC_WRITE)
+		err = !access_ok((void *) arg, _IOC_SIZE(cmd));
+	if (err) {
+		printk(KERN_ALERT "err fault\n");
+		return -EFAULT;
+	}
+	if (down_interruptible(&info->sem))
+		return -ERESTARTSYS;
+
+	err = sw_dev_req(sw, (char *) arg, info);
+	up(&info->sem);
+	return err;
+}  /* sw_dev_ioctl */
+
+static ssize_t sw_dev_write(struct file *filp, const char *buf, size_t count,
+	loff_t *offp)
+{
+	struct file_dev_info *info = (struct file_dev_info *)
+		filp->private_data;
+	ssize_t result = 0;
+	size_t size;
+	int rc;
+
+	if (!count)
+		return result;
+
+	if (down_interruptible(&info->sem))
+		return -ERESTARTSYS;
+
+	if (*offp >= info->write_len) {
+		result = -ENOSPC;
+		goto dev_write_done;
+	}
+	if (*offp + count > info->write_len)
+		count = info->write_len - *offp;
+	if (copy_from_user(info->write_buf, buf, count)) {
+		result = -EFAULT;
+		goto dev_write_done;
+	}
+	size = 0;
+	result = size;
+	rc = 0;
+	if (rc)
+		result = rc;
+
+dev_write_done:
+	up(&info->sem);
+	return result;
+}  /* sw_dev_write */
+
+static const struct file_operations sw_dev_fops = {
+	.read		= sw_dev_read,
+	.write		= sw_dev_write,
+	.unlocked_ioctl	= sw_dev_ioctl,
+	.open		= sw_dev_open,
+	.release	= sw_dev_release,
+};
+
+static struct class *sw_class[MAX_SW_DEVICES];
+
+static int init_sw_dev(int id, int dev_major, char *dev_name)
+{
+	int result;
+
+	result = register_chrdev(dev_major, dev_name, &sw_dev_fops);
+	if (result < 0) {
+		printk(KERN_WARNING "%s: can't get major %d\n", dev_name,
+			dev_major);
+		return result;
+	}
+	if (0 == dev_major)
+		dev_major = result;
+	sw_class[id] = class_create(THIS_MODULE, dev_name);
+	if (IS_ERR(sw_class[id])) {
+		unregister_chrdev(dev_major, dev_name);
+		return -ENODEV;
+	}
+	device_create(sw_class[id], NULL, MKDEV(dev_major, 0), NULL, dev_name);
+	return dev_major;
+}  /* init_sw_dev */
+
+static void exit_sw_dev(int id, int dev_major, char *dev_name)
+{
+	device_destroy(sw_class[id], MKDEV(dev_major, 0));
+	class_destroy(sw_class[id]);
+	unregister_chrdev(dev_major, dev_name);
+}  /* exit_sw_dev */
+
+static void sw_init_dev(struct ksz_sw *sw)
+{
+	sprintf(sw->dev_name, "sw_dev");
+	if (sw->id)
+		sprintf(sw->dev_name, "sw_dev_%u", sw->id);
+	sw->dev_major = init_sw_dev(sw->id, 0, sw->dev_name);
+	sw->msg_buf = kzalloc(MAX_SW_LEN, GFP_KERNEL);
+	sw_majors[sw->id].dev = sw;
+	sw_majors[sw->id].major = sw->dev_major;
+}  /* sw_init_dev */
+
+static void sw_exit_dev(struct ksz_sw *sw)
+{
+	kfree(sw->msg_buf);
+	if (sw->dev_major >= 0)
+		exit_sw_dev(sw->id, sw->dev_major, sw->dev_name);
+}  /* sw_exit_dev */
+
+static void sw_report_link(struct ksz_sw *sw, struct ksz_port *port,
+			   struct ksz_port_info *info)
+{
+	struct ksz_port_info *linked = port->linked;
+	struct phy_device *phydev = port->phydev;
+	struct net_device *dev = port->netdev;
+	int lpa = info->lpa;
+	int phy_link = 0;
+	int link;
+
+	phydev->link = (info->state == media_connected);
+	phydev->speed = info->tx_rate / TX_RATE_UNIT;
+	if (!phydev->speed)
+		phydev->speed = 10;
+	phydev->duplex = (info->duplex == 2);
+
+#ifdef CONFIG_PHYLINK
+	/* Not started yet. */
+	if (sw->phylink_ops && port != sw->main_port &&
+	    phydev->state == PHY_READY)
+		return;
+#endif
+	if (phydev->link)
+		phy_link = (linked->state == media_connected);
+	link = netif_carrier_ok(dev);
+	if (port->report) {
+		port->report = false;
+		link = !phy_link;
+	}
+	if (phy_link == link)
+		return;
+
+	/* Update link partner capabilities. */
+	if (lpa) {
+		mii_lpa_mod_linkmode_lpa_t(phydev->lp_advertising, lpa);
+#if 0
+dbg_msg(" %*pb\n", __ETHTOOL_LINK_MODE_MASK_NBITS, phydev->advertising);
+dbg_msg(" %*pb\n", __ETHTOOL_LINK_MODE_MASK_NBITS, phydev->lp_advertising);
+#endif
+	}
+	if (netif_msg_link(sw))
+		pr_info("%s link %s"NL,
+			dev->name,
+			phy_link ? "on" : "off");
+	if (phydev->phy_link_change) {
+		phydev->phy_link_change(phydev, phy_link);
+	} else if (phy_link != link) {
+		if (phy_link)
+			netif_carrier_on(dev);
+		else
+			netif_carrier_off(dev);
+	}
+}  /* sw_report_link */
+
+static void link_update_work(struct work_struct *work)
+{
+	struct delayed_work *dwork = to_delayed_work(work);
+	struct ksz_port *port =
+		container_of(dwork, struct ksz_port, link_update);
+	struct ksz_sw *sw = port->sw;
+	struct ksz_port_info *info;
+
+	/* Netdevice associated with port was closed. */
+	if (!port->opened)
+		goto do_main;
+
+	if (sw->dev_offset && sw->netport[0]) {
+		int dev_cnt = sw->dev_count + sw->dev_offset;
+		struct ksz_port *sw_port = sw->netport[0];
+		struct ksz_port *dev_port;
+		int i;
+
+		for (i = sw->dev_offset; i < dev_cnt; i++) {
+			struct phy_priv *phydata;
+
+			dev_port = sw->netport[i];
+			if (!dev_port) {
+				phydata = &sw->phydata[i];
+				dev_port = phydata->port;
+			}
+			if (media_connected == dev_port->linked->state) {
+				sw_port->linked = dev_port->linked;
+				break;
+			}
+		}
+	}
+
+	sw_notify_link_change(sw, port->link_ports);
+
+	if ((!sw->dev_offset || port != sw->netport[0]) && port->netdev)
+		sw_report_link(sw, port, port->linked);
+
+#ifdef CONFIG_KSZ_STP
+	if (sw->features & STP_SUPPORT) {
+		struct ksz_stp_info *stp = &sw->info->rstp;
+
+		stp->ops->link_change(stp, true);
+	}
+#endif
+
+#ifdef CONFIG_KSZ_HSR
+	if (sw->features & HSR_HW) {
+		struct ksz_hsr_info *hsr = &sw->info->hsr;
+		uint p;
+
+		p = get_phy_port(sw, port->first_port);
+		if (hsr->ports[0] <= port->first_port &&
+		    port->first_port <= hsr->ports[1])
+			hsr->ops->check_announce(hsr);
+	}
+#endif
+
+do_main:
+	port->link_ports = 0;
+
+	/* There is an extra network device for the main device. */
+	/* The switch is always linked; speed and duplex are also fixed. */
+	if (sw->dev_offset) {
+		port = sw->netport[0];
+		if (port && port->opened && netif_running(port->netdev)) {
+			info = get_port_info(sw, sw->HOST_PORT);
+			sw_report_link(sw, port, info);
+		}
+	}
+}  /* link_update_work */
+
+#ifndef NO_PHYDEV
+static void set_phy_support(struct ksz_port *port, struct phy_device *phydev)
+{
+	switch (port->flow_ctrl) {
+	case PHY_NO_FLOW_CTRL:
+		linkmode_clear_bit(ETHTOOL_LINK_MODE_Pause_BIT,
+				   phydev->supported);
+		linkmode_clear_bit(ETHTOOL_LINK_MODE_Asym_Pause_BIT,
+				   phydev->supported);
+		break;
+	case PHY_RX_ONLY:
+		/* Depend on link partner to only advertise Asym_Pause. */
+		linkmode_set_bit(ETHTOOL_LINK_MODE_Pause_BIT,
+				 phydev->supported);
+		linkmode_set_bit(ETHTOOL_LINK_MODE_Asym_Pause_BIT,
+				 phydev->supported);
+		break;
+	case PHY_TX_ONLY:
+		linkmode_clear_bit(ETHTOOL_LINK_MODE_Pause_BIT,
+				   phydev->supported);
+		linkmode_set_bit(ETHTOOL_LINK_MODE_Asym_Pause_BIT,
+				 phydev->supported);
+		break;
+	default:
+		linkmode_set_bit(ETHTOOL_LINK_MODE_Pause_BIT,
+				 phydev->supported);
+		linkmode_clear_bit(ETHTOOL_LINK_MODE_Asym_Pause_BIT,
+				   phydev->supported);
+		break;
+	}
+}  /* set_phy_support */
+#endif
+
+/*
+ * This enables multiple network device mode for the switch, which contains at
+ * least two physical ports.  Some users like to take control of the ports for
+ * running Spanning Tree Protocol.  The driver will create an additional eth?
+ * device for each port depending on the mode.
+ *
+ * Some limitations are the network devices cannot have different MTU and
+ * multicast hash tables.
+ */
+static int multi_dev;
+
+static int stp;
+
+/*
+ * This enables fast aging in the switch.  Not sure what situation requires
+ * that.  However, fast aging is used to flush the dynamic MAC table when STP
+ * support is enabled.
+ */
+static int fast_aging;
+
+static void sw_setup_zone(struct ksz_sw *sw)
+{
+#ifdef CONFIG_KSZ_DLR
+	sw->eth_maps[0].cnt = 2;
+	sw->eth_maps[0].mask = 3;
+	sw->eth_maps[0].port = 0;
+	sw->eth_maps[0].phy_id = 1;
+	sw->eth_maps[0].vlan = 1;
+	sw->eth_maps[0].proto = DLR_HW;
+	sw->eth_cnt = 1;
+	sw->features |= DLR_HW;
+#endif
+#ifdef CONFIG_KSZ_HSR
+	sw->eth_maps[0].cnt = 2;
+	sw->eth_maps[0].mask = 3;
+	sw->eth_maps[0].port = 0;
+	sw->eth_maps[0].phy_id = 1;
+	sw->eth_maps[0].vlan = 1;
+	sw->eth_maps[0].proto = HSR_HW;
+	sw->eth_cnt = 1;
+	sw->features |= HSR_HW;
+#endif
+}  /* sw_setup_zone */
+
+static void sw_setup_logical_ports(struct ksz_sw *sw)
+{
+	struct ksz_port_info *info;
+	struct ksz_port_info *pinfo;
+	uint l;
+	uint m;
+	uint n;
+	uint p;
+
+	for (n = 0; n < TOTAL_PORT_NUM; n++) {
+		info = &sw->port_info[n];
+		if (n) {
+			p = n - 1;
+			l = n;
+			m = BIT(p);
+		} else {
+			p = sw->HOST_PORT;
+			l = 0;
+			m = BIT(p);
+		}
+		info->phy_p = p;
+		info->phy_m = BIT(p);
+		pinfo = &sw->port_info[p];
+		pinfo->log_p = l;
+		pinfo->log_m = m;
+		pinfo->phy_id = l;
+	}
+	for (n = 0; n <= SWITCH_PORT_NUM; n++) {
+		info = &sw->port_info[n];
+		pinfo = &sw->port_info[info->phy_p];
+dbg_msg("%d= %d:%02x %d:%02x %d:%02x %d %d\n", n,
+info->phy_p, info->phy_m, info->log_p, info->log_m,
+pinfo->log_p, pinfo->log_m, pinfo->phy_id, pinfo->fiber);
+	}
+}  /* sw_setup_logical_ports */
+
+static int phy_offset;
+
+static void sw_setup_special(struct ksz_sw *sw, int *port_cnt,
+	int *mib_port_cnt, int *dev_cnt,
+	const void *phylink_ops)
+{
+#ifdef CONFIG_PHYLINK
+	sw->phylink_ops = phylink_ops;
+#endif
+	phy_offset = 0;
+	sw->dev_offset = 0;
+	sw->phy_offset = 0;
+
+	/* Multiple network device interfaces are required. */
+	if (1 == sw->multi_dev) {
+		sw->dev_count = SWITCH_PORT_NUM;
+		sw->phy_offset = 1;
+	} else if (2 == sw->multi_dev)
+		sw->features |= VLAN_PORT | VLAN_PORT_TAGGING;
+	else if (3 == sw->multi_dev) {
+		sw->dev_count = SWITCH_PORT_NUM;
+		sw->dev_offset = 1;
+	} else if (4 == sw->multi_dev)
+		sw->features |= VLAN_PORT;
+	else if (5 == sw->multi_dev) {
+		sw->dev_count = SWITCH_PORT_NUM;
+		sw->dev_offset = 1;
+		sw->features |= VLAN_PORT | VLAN_PORT_TAGGING;
+	}
+
+	/* Single network device has multiple ports. */
+	if (1 == sw->dev_count) {
+		*port_cnt = SWITCH_PORT_NUM;
+		*mib_port_cnt = SWITCH_PORT_NUM;
+	}
+	if (1 == sw->multi_dev && sw->eth_cnt)
+		sw->dev_count = sw->eth_cnt;
+	*dev_cnt = sw->dev_count;
+	if (3 == sw->multi_dev || 5 == sw->multi_dev)
+		(*dev_cnt)++;
+#ifdef CONFIG_1588_PTP
+	if (sw->features & VLAN_PORT) {
+		struct ptp_info *ptp = &sw->ptp_hw;
+
+		ptp->overrides |= PTP_PORT_FORWARD;
+	}
+#endif
+}  /* sw_setup_special */
+
+static void sw_leave_dev(struct ksz_sw *sw)
+{
+	int dev_count = sw->dev_count + sw->dev_offset;
+	struct sw_priv *ks = sw->dev;
+	struct phy_priv *phydata;
+#ifdef CONFIG_PHYLINK
+	struct ksz_port *port;
+#endif
+	int i;
+
+#ifdef CONFIG_KSZ_STP
+	if (sw->features & STP_SUPPORT)
+		leave_stp(&sw->info->rstp);
+#endif
+	for (i = 0; i < dev_count; i++) {
+#ifdef CONFIG_PHYLINK
+		port = sw->netport[i];
+		if (port && port->pl) {
+		       phylink_destroy(port->pl);
+		       port->pl = NULL;
+		}
+#endif
+		sw->netdev[i] = NULL;
+		sw->netport[i] = NULL;
+	}
+
+	/* Reset port pointer as it is pointed to one from device. */
+	for (i = 0; i <= sw->port_cnt; i++) {
+		phydata = &sw->phydata[i];
+		phydata->port = &ks->ports[i];
+	}
+	sw->eth_cnt = 0;
+	sw->dev_count = 1;
+	sw->dev_offset = 0;
+	sw->phy_offset = 0;
+}  /* sw_leave_dev */
+
+static int sw_setup_dev(struct ksz_sw *sw, struct net_device *dev,
+	char *dev_name, struct ksz_port *port, int i, uint port_cnt,
+	uint mib_port_cnt)
+{
+	struct ksz_port_info *info;
+	int cnt;
+	uint n;
+	uint p;
+	uint pi;
+	int phy_id;
+	struct ksz_dev_map *map;
+
+	if (!phy_offset)
+		phy_offset = sw->phy_offset;
+
+	/* dev_offset is ether 0 or 1. */
+	p = i;
+	if (p)
+		p -= sw->dev_offset;
+
+	if (sw->dev_offset) {
+		/*
+		 * First device associated with switch has been
+		 * created.
+		 */
+		if (i)
+			snprintf(dev->name, IFNAMSIZ, "%s.10%%d", dev_name);
+		else {
+			port_cnt = SWITCH_PORT_NUM;
+			mib_port_cnt = SWITCH_PORT_NUM;
+		}
+	}
+
+	map = &sw->eth_maps[i];
+	if (1 == sw->multi_dev && sw->eth_cnt) {
+		port_cnt = map->cnt;
+		p = map->first - 1;
+		mib_port_cnt = port_cnt;
+	}
+
+#ifdef CONFIG_KSZ_HSR
+	if (sw->eth_cnt && (sw->eth_maps[i].proto & HSR_HW)) {
+		port_cnt = sw->eth_maps[i].cnt;
+		p = sw->eth_maps[i].port;
+		mib_port_cnt = port_cnt;
+		setup_hsr(&sw->info->hsr, dev);
+		dev->hard_header_len += HSR_HLEN;
+	}
+#endif
+
+	port->port_cnt = port_cnt;
+	port->mib_port_cnt = mib_port_cnt;
+	port->first_port = p + 1;
+	port->flow_ctrl = PHY_FLOW_CTRL;
+
+#ifdef CONFIG_KSZ_STP
+	if (!i && sw->features & STP_SUPPORT)
+		prep_stp_mcast(dev);
+#endif
+
+	p = get_phy_port(sw, port->first_port);
+	port->sw = sw;
+	port->linked = get_port_info(sw, p);
+
+	/* Point to port under netdev. */
+	if (phy_offset)
+		phy_id = port->linked->phy_id;
+	else
+		phy_id = 0;
+
+#ifndef NO_PHYDEV
+	/* Replace virtual port with one from network device. */
+	do {
+		struct phy_device *phydev;
+		struct phy_priv *priv;
+		struct sw_priv *hw_priv = container_of(sw, struct sw_priv, sw);
+
+		phydev = mdiobus_get_phy(hw_priv->bus, phy_id);
+		priv = phydev->priv;
+		priv->port = port;
+		set_phy_support(port, phydev);
+	} while (0);
+#endif
+
+	if (!phy_offset)
+		phy_offset = 1;
+
+	for (cnt = 0, n = port->first_port; cnt < port_cnt; cnt++, n++) {
+		pi = get_phy_port(sw, n);
+		info = get_port_info(sw, pi);
+		info->port_id = pi;
+		info->state = media_disconnected;
+		sw->info->port_cfg[pi].index = i;
+	}
+	sw->netdev[i] = dev;
+	sw->netport[i] = port;
+	port->netdev = dev;
+	port->phydev = sw->phy[phy_id];
+	if (phy_id)
+		port->dn = sw->devnode[phy_id - 1];
+#ifdef CONFIG_PHYLINK
+	setup_phylink(port);
+#endif
+
+	INIT_DELAYED_WORK(&port->link_update, link_update_work);
+
+	if (sw->features & VLAN_PORT)
+		dev->features |= NETIF_F_HW_VLAN_CTAG_FILTER;
+
+	/* Needed for inserting VLAN tag. */
+	if (sw->features & SW_VLAN_DEV)
+		dev->hard_header_len += VLAN_HLEN;
+dbg_msg("%s %d:%d phy:%d\n", __func__, port->first_port, port->port_cnt, phy_id);
+
+	return phy_id;
+}  /* sw_setup_dev */
+
+static int netdev_chk_running(struct net_device *dev)
+{
+	return netif_running(dev);
+}
+
+static int netdev_chk_stopped(struct net_device *dev)
+{
+	return netif_running(dev) && netif_queue_stopped(dev);
+}
+
+static void netdev_start_queue(struct net_device *dev)
+{
+	netif_start_queue(dev);
+}
+
+static void netdev_stop_queue(struct net_device *dev)
+{
+	netif_stop_queue(dev);
+}
+
+static void netdev_wake_queue(struct net_device *dev)
+{
+	netif_wake_queue(dev);
+}
+
+static void sw_netdev_oper(struct ksz_sw *sw, struct net_device *dev,
+	int (*netdev_chk)(struct net_device *dev),
+	void (*netdev_oper)(struct net_device *dev))
+{
+	uint port;
+	int dev_count = 1;
+
+	dev_count = sw->dev_count + sw->dev_offset;
+	if (dev_count <= 1) {
+		netdev_oper(dev);
+		return;
+	}
+	for (port = 0; port < dev_count; port++) {
+		dev = sw->netdev[port];
+		if (!dev)
+			continue;
+		if (!netdev_chk || netdev_chk(dev))
+			netdev_oper(dev);
+	}
+}  /* sw_netdev_oper */
+
+static void sw_netdev_open_port(struct ksz_sw *sw, struct net_device *dev)
+{
+	struct ksz_port *port;
+	int p;
+	int dev_count = 1;
+
+	dev_count = sw->dev_count + sw->dev_offset;
+	if (dev_count <= 1) {
+		port = sw->netport[0];
+		sw->net_ops->open_port(sw, dev, port);
+		return;
+	}
+	for (p = 0; p < dev_count; p++) {
+		dev = sw->netdev[p];
+		if (!dev)
+			continue;
+		port = sw->netport[p];
+		sw->net_ops->open_port(sw, dev, port);
+	}
+}  /* sw_netdev_open_port */
+
+static void sw_netdev_start_queue(struct ksz_sw *sw, struct net_device *dev)
+{
+	sw_netdev_oper(sw, dev, netdev_chk_running, netdev_start_queue);
+}
+
+static void sw_netdev_stop_queue(struct ksz_sw *sw, struct net_device *dev)
+{
+	sw_netdev_oper(sw, dev, netdev_chk_running, netdev_stop_queue);
+}
+
+static void sw_netdev_wake_queue(struct ksz_sw *sw, struct net_device *dev)
+{
+	sw_netdev_oper(sw, dev, netdev_chk_stopped, netdev_wake_queue);
+}
+
+static struct ksz_sw_net_ops sw_net_ops = {
+	.setup_special		= sw_setup_special,
+	.setup_dev		= sw_setup_dev,
+	.leave_dev		= sw_leave_dev,
+
+	.start			= sw_start,
+	.stop			= sw_stop,
+	.open_dev		= sw_open_dev,
+	.open_port		= sw_open_port,
+	.close_port		= sw_close_port,
+	.open			= sw_open,
+	.close			= sw_close,
+
+	.netdev_start_queue	= sw_netdev_start_queue,
+	.netdev_stop_queue	= sw_netdev_stop_queue,
+	.netdev_wake_queue	= sw_netdev_wake_queue,
+	.netdev_open_port	= sw_netdev_open_port,
+
+	.set_mac_addr		= sw_set_mac_addr,
+
+	.get_mtu		= sw_get_mtu,
+	.get_tx_len		= sw_get_tx_len,
+	.add_tail_tag		= sw_add_tail_tag,
+	.get_tail_tag		= sw_get_tail_tag,
+	.add_vid		= sw_add_vid,
+	.kill_vid		= sw_kill_vid,
+	.check_tx		= sw_check_tx,
+	.rx_dev			= sw_rx_dev,
+	.match_pkt		= sw_match_pkt,
+	.parent_rx		= sw_parent_rx,
+	.port_vlan_rx		= sw_port_vlan_rx,
+	.final_skb		= sw_final_skb,
+	.drv_rx			= sw_drv_rx,
+	.set_multi		= sw_set_multi,
+
+};
+
+static struct ksz_sw_ops sw_ops = {
+	.init			= sw_init_dev,
+	.exit			= sw_exit_dev,
+	.dev_req		= sw_dev_req,
+
+	.get_phy_port		= get_phy_port,
+	.get_log_port		= get_log_port,
+
+	.acquire		= sw_acquire,
+	.release		= sw_release,
+
+	.chk			= sw_chk,
+	.cfg			= sw_cfg,
+
+	.port_get_link_speed	= port_get_link_speed,
+	.port_set_link_speed	= port_set_link_speed,
+	.port_force_link_speed	= port_force_link_speed,
+
+	.port_r_cnt		= port_r_cnt,
+	.get_mib_counters	= get_sw_mib_counters,
+
+	.sysfs_read		= sysfs_sw_read,
+	.sysfs_read_hw		= sysfs_sw_read_hw,
+	.sysfs_write		= sysfs_sw_write,
+	.sysfs_port_read	= sysfs_port_read,
+	.sysfs_port_read_hw	= sysfs_port_read_hw,
+	.sysfs_port_write	= sysfs_port_write,
+	.sysfs_mac_read		= sysfs_mac_read,
+	.sysfs_mac_write	= sysfs_mac_write,
+	.sysfs_vlan_read	= sysfs_vlan_read,
+	.sysfs_vlan_write	= sysfs_vlan_write,
+
+#ifdef CONFIG_KSZ_STP
+	.sysfs_stp_read		= sysfs_stp_read,
+	.sysfs_stp_write	= sysfs_stp_write,
+	.sysfs_stp_port_read	= sysfs_stp_port_read,
+	.sysfs_stp_port_write	= sysfs_stp_port_write,
+#endif
+
+	.cfg_mac		= sw_cfg_mac,
+	.cfg_vlan		= sw_cfg_vlan,
+	.alloc_mac		= sw_alloc_mac,
+	.free_mac		= sw_free_mac,
+	.alloc_vlan		= sw_alloc_vlan,
+	.free_vlan		= sw_free_vlan,
+	.alloc_fid		= sw_alloc_fid,
+	.free_fid		= sw_free_fid,
+
+	.get_br_id		= sw_get_br_id,
+	.from_backup		= sw_from_backup,
+	.to_backup		= sw_to_backup,
+	.from_designated	= sw_from_designated,
+	.to_designated		= sw_to_designated,
+	.tc_detected		= sw_tc_detected,
+	.get_tcDetected		= sw_get_tcDetected,
+
+	.cfg_src_filter		= sw_cfg_src_filter,
+	.flush_table		= sw_flush_dyn_mac_table,
+
+};
+/* -------------------------------------------------------------------------- */
+
+/* debugfs code */
+static int state_show(struct seq_file *seq, void *v)
+{
+	int i;
+	int j;
+	SW_D data[16 / SW_SIZE];
+	struct sw_priv *priv = seq->private;
+	struct ksz_sw *sw = &priv->sw;
+
+	for (i = 0; i < 0x100; i += 16) {
+		seq_printf(seq, SW_SIZE_STR":\t", i);
+		mutex_lock(&priv->lock);
+		for (j = 0; j < 16 / SW_SIZE; j++)
+			data[j] = sw->reg->r(sw, i + j * SW_SIZE);
+		mutex_unlock(&priv->lock);
+		for (j = 0; j < 16 / SW_SIZE; j++)
+			seq_printf(seq, SW_SIZE_STR" ", data[j]);
+		seq_printf(seq, "\n");
+	}
+	return 0;
+}
+
+static int state_open(struct inode *inode, struct file *file)
+{
+	return single_open(file, state_show, inode->i_private);
+}
+
+static const struct file_operations state_fops = {
+	.owner	= THIS_MODULE,
+	.open	= state_open,
+	.read	= seq_read,
+	.llseek	= seq_lseek,
+	.release = single_release,
+};
+
+/**
+ * create_debugfs - create debugfs directory and files
+ * @priv:	The switch device structure.
+ *
+ * Create the debugfs entries for the specific device.
+ */
+static void create_debugfs(struct sw_priv *priv)
+{
+	struct dentry *root;
+	char root_name[32];
+
+	snprintf(root_name, sizeof(root_name), "%s",
+		 dev_name(priv->dev));
+
+	root = debugfs_create_dir(root_name, NULL);
+	if (IS_ERR(root)) {
+		pr_err("cannot create debugfs root\n");
+		return;
+	}
+
+	priv->debug_root = root;
+	priv->debug_file = debugfs_create_file("state", 0444, root,
+		priv, &state_fops);
+	if (IS_ERR(priv->debug_file))
+		pr_err("cannot create debugfs state file\n");
+}
+
+static void delete_debugfs(struct sw_priv *priv)
+{
+	debugfs_remove(priv->debug_file);
+	debugfs_remove(priv->debug_root);
+}
+
+/* -------------------------------------------------------------------------- */
+
+#define USE_SPEED_LINK
+#define USE_MIB
+#include "ksz_sw_sysfs.c"
+
+#ifdef CONFIG_1588_PTP
+#include "ksz_ptp_sysfs.c"
+#endif
+
+#ifndef NO_PHYDEV
+static irqreturn_t sw_interrupt(int irq, void *phy_dat)
+{
+	struct sw_priv *ks = phy_dat;
+
+	ks->sw.intr_using += 1;
+	ks->irq_work.func(&ks->irq_work);
+
+	return IRQ_HANDLED;
+}  /* sw_interrupt */
+
+static void sw_change(struct work_struct *work)
+{
+	struct sw_priv *ks =
+		container_of(work, struct sw_priv, irq_work);
+	struct ksz_sw *sw = &ks->sw;
+	SW_D status;
+
+	ks->intr_working = true;
+	mutex_lock(&ks->hwlock);
+	mutex_lock(&ks->lock);
+	sw->intr_using++;
+	status = sw_proc_intr(sw);
+	sw->intr_using--;
+	mutex_unlock(&ks->lock);
+	if (status) {
+		mutex_lock(&ks->lock);
+		sw->reg->w(sw, REG_INT_STATUS, status);
+		mutex_unlock(&ks->lock);
+	}
+	mutex_unlock(&ks->hwlock);
+	sw->intr_using = 0;
+}  /* sw_change */
+
+static int sw_start_interrupt(struct sw_priv *ks, const char *name)
+{
+	int err = 0;
+
+	INIT_WORK(&ks->irq_work, sw_change);
+
+	err = request_threaded_irq(ks->irq, NULL, sw_interrupt,
+		ks->intr_mode, name, ks);
+	if (err < 0) {
+		printk(KERN_WARNING "%s: Can't get IRQ %d (PHY)\n",
+			name,
+			ks->irq);
+		ks->irq = 0;
+	}
+
+	return err;
+}  /* sw_start_interrupt */
+
+static void sw_stop_interrupt(struct sw_priv *ks)
+{
+	free_irq(ks->irq, ks);
+	cancel_work_sync(&ks->irq_work);
+}  /* sw_stop_interrupt */
+#endif
+
+/* -------------------------------------------------------------------------- */
+
+static void sw_init_phy_priv(struct sw_priv *ks)
+{
+	struct phy_priv *phydata;
+	struct ksz_port *port;
+	struct ksz_sw *sw = &ks->sw;
+	uint n;
+	uint p;
+
+	for (n = 0; n <= sw->mib_port_cnt + 1; n++) {
+		phydata = &sw->phydata[n];
+		port = &ks->ports[n];
+		phydata->port = port;
+		port->sw = sw;
+		port->phydev = &sw->phy_map[n];
+		port->flow_ctrl = PHY_FLOW_CTRL;
+		port->port_cnt = 1;
+		port->mib_port_cnt = 1;
+		p = n;
+		if (!n) {
+			port->port_cnt = sw->mib_port_cnt;
+			port->mib_port_cnt = sw->mib_port_cnt;
+			p = 1;
+		}
+		port->first_port = p;
+		p = get_phy_port(sw, p);
+		port->linked = get_port_info(sw, p);
+dbg_msg(" %s %d=p:%d; f:%d c:%d i:%d\n", __func__, n, p,
+port->first_port, port->port_cnt, port->linked->phy_id);
+		INIT_DELAYED_WORK(&port->link_update, link_update_work);
+		sw->phy_map[n].priv = phydata;
+	}
+}  /* sw_init_phy_priv */
+
+#ifndef NO_PHYDEV
+static void sw_init_phydev(struct ksz_sw *sw, struct phy_device *phydev)
+{
+	struct ksz_port_info *info = get_port_info(sw, sw->HOST_PORT);
+
+	phydev->interface = sw->interface;
+	phydev->link = 1;
+	phydev->speed = info->tx_rate / TX_RATE_UNIT;
+	phydev->duplex = (info->duplex == 2);
+	phydev->pause = 1;
+}  /* sw_init_phydev */
+
+static int kszphy_probe(struct phy_device *phydev)
+{
+	struct mii_bus *bus = phydev->mdio.bus;
+	struct sw_priv *sw_priv = bus->priv;
+	struct ksz_sw *sw = &sw_priv->sw;
+	uint p;
+
+	p = phydev->mdio.addr;
+	phydev->priv = &sw->phydata[p];
+	phydev->interface = sw->interface;
+	return 0;
+}
+
+static int kszphy_get_features(struct phy_device *phydev)
+{
+	struct phy_priv *priv = phydev->priv;
+	struct ksz_port *port = priv->port;
+	int ret;
+
+	ret = genphy_read_abilities(phydev);
+	if (ret < 0)
+		return ret;
+
+	set_phy_support(port, phydev);
+
+	/* Special for first PHY connected to MAC. */
+	if (phydev->mdio.addr == 0) {
+		linkmode_clear_bit(ETHTOOL_LINK_MODE_Autoneg_BIT,
+				   phydev->supported);
+	}
+	return 0;
+}
+
+static int kszphy_config_init(struct phy_device *phydev)
+{
+	return 0;
+}
+
+static struct phy_driver kszsw_phy_driver = {
+	.phy_id		= PHY_ID_KSZ_SW,
+	.phy_id_mask	= 0x00ffffff,
+	.name		= "Microchip KSZ8463 Switch",
+	.probe		= kszphy_probe,
+	.get_features	= kszphy_get_features,
+	.config_init	= kszphy_config_init,
+	.config_aneg	= genphy_config_aneg,
+	.read_status	= genphy_read_status,
+};
+
+static int ksz_mii_read(struct mii_bus *bus, int phy_id, int regnum)
+{
+	struct sw_priv *ks = bus->priv;
+	int ret = 0;
+
+	if (phy_id > SWITCH_PORT_NUM)
+		return 0xffff;
+
+	mutex_lock(&ks->lock);
+	if (regnum < 6) {
+		u16 data;
+
+		sw_r_phy(&ks->sw, phy_id, regnum, &data);
+		ret = data;
+	}
+	mutex_unlock(&ks->lock);
+	return ret;
+}  /* ksz_mii_read */
+
+static int ksz_mii_write(struct mii_bus *bus, int phy_id, int regnum, u16 val)
+{
+	struct sw_priv *ks = bus->priv;
+
+	if (phy_id > SWITCH_PORT_NUM)
+		return -EINVAL;
+
+	mutex_lock(&ks->lock);
+	if (regnum < 6) {
+		uint i;
+		uint p;
+		int first;
+		int last;
+		struct ksz_sw *sw = &ks->sw;
+
+		if (0 == phy_id) {
+			first = 1;
+			last = SWITCH_PORT_NUM;
+		} else {
+			int n;
+			int f;
+			int l;
+			struct ksz_dev_map *map;
+
+			first = phy_id;
+			last = phy_id;
+			for (n = 0; n < sw->eth_cnt; n++) {
+				map = &sw->eth_maps[n];
+				f = map->first;
+				l = f + map->cnt - 1;
+				if (f <= phy_id && phy_id < l) {
+					first = map->first;
+					last = first + map->cnt + 1;
+					break;
+				}
+			}
+dbg_msg(" %d f:%d l:%d\n", phy_id, first, last);
+		}
+
+		/* PHY device driver resets or powers down the PHY. */
+		if (0 == regnum &&
+#ifdef CONFIG_HAVE_KSZ8463
+		    (val & (PHY_RESET | PHY_POWER_DOWN)))
+#else
+		    (val & (PHY_RESET_NOT | PHY_POWER_DOWN)))
+#endif
+			goto done;
+		for (i = first; i <= last; i++) {
+			p = i;
+			sw_w_phy(sw, p, regnum, val);
+		}
+		if (PHY_REG_CTRL == regnum &&
+		    !(val & PHY_AUTO_NEG_ENABLE))
+			schedule_delayed_work(&ks->link_read, 1);
+	}
+done:
+	mutex_unlock(&ks->lock);
+	return 0;
+}  /* ksz_mii_write */
+
+static int driver_installed;
+
+static int ksz_mii_init(struct sw_priv *ks)
+{
+	struct platform_device *pdev;
+	struct mii_bus *bus;
+	struct phy_device *phydev;
+	int err;
+	int i;
+
+	pdev = platform_device_register_simple("Switch MII bus", ks->sw.id,
+		NULL, 0);
+	if (!pdev)
+		return -ENOMEM;
+
+	bus = mdiobus_alloc();
+	if (bus == NULL) {
+		err = -ENOMEM;
+		goto mii_init_reg;
+	}
+
+	if (!driver_installed) {
+		kszsw_phy_driver.name =
+			kszsw_phy_driver_names[ks->sw.chip_id];
+		err = phy_driver_register(&kszsw_phy_driver, THIS_MODULE);
+		if (err)
+			goto mii_init_free_mii_bus;
+		driver_installed = true;
+	}
+
+	bus->name = "Switch MII bus",
+	bus->read = ksz_mii_read;
+	bus->write = ksz_mii_write;
+	snprintf(bus->id, MII_BUS_ID_SIZE, "sw.%d", ks->sw.id);
+	bus->parent = &pdev->dev;
+	bus->phy_mask = ~((1 << (ks->sw.mib_port_cnt + 2)) - 1);
+	bus->priv = ks;
+
+	for (i = 0; i < PHY_MAX_ADDR; i++)
+		bus->irq[i] = -1;
+
+	err = mdiobus_register(bus);
+	if (err < 0)
+		goto mii_init_free_mii_bus;
+
+	for (i = 0; i < PHY_MAX_ADDR; i++) {
+		phydev = mdiobus_get_phy(bus, i);
+		if (phydev) {
+			struct phy_priv *priv = phydev->priv;
+
+			priv->state = phydev->state;
+		}
+	}
+
+	ks->bus = bus;
+	ks->pdev = pdev;
+	phydev = mdiobus_get_phy(bus, 0);
+	ks->phydev = phydev;
+	sw_init_phydev(&ks->sw, phydev);
+
+	return 0;
+
+mii_init_free_mii_bus:
+	if (driver_installed) {
+		phy_driver_unregister(&kszsw_phy_driver);
+		driver_installed = false;
+	}
+	mdiobus_free(bus);
+
+mii_init_reg:
+	platform_device_unregister(pdev);
+
+	return err;
+}  /* ksz_mii_init */
+
+static void ksz_mii_exit(struct sw_priv *ks)
+{
+	int i;
+	struct phy_device *phydev;
+	struct platform_device *pdev = ks->pdev;
+	struct mii_bus *bus = ks->bus;
+
+	if (ks->irq > 0) {
+		struct ksz_sw *sw = &ks->sw;
+
+		mutex_lock(&ks->lock);
+		sw_dis_intr(sw);
+		mutex_unlock(&ks->lock);
+		sw_stop_interrupt(ks);
+	}
+	for (i = 0; i < PHY_MAX_ADDR; i++) {
+		phydev = mdiobus_get_phy(bus, i);
+		if (phydev) {
+			struct ksz_port *port;
+
+			port = &ks->ports[i];
+			cancel_delayed_work_sync(&port->link_update);
+		}
+	}
+	mdiobus_unregister(bus);
+	if (driver_installed) {
+		phy_driver_unregister(&kszsw_phy_driver);
+		driver_installed = false;
+	}
+	mdiobus_free(bus);
+	platform_device_unregister(pdev);
+}  /* ksz_mii_exit */
+#endif
+
+/* driver bus management functions */
+
+static void determine_rate(struct ksz_sw *sw, struct ksz_port_mib *mib)
+{
+	int j;
+
+	for (j = 0; j < 2; j++) {
+		if (mib->rate[j].last) {
+			int offset;
+			u64 cnt;
+			u64 last_cnt;
+			unsigned long diff = jiffies - mib->rate[j].last;
+
+			if (0 == j)
+				offset = MIB_RX_LO_PRIO;
+			else
+				offset = MIB_TX_LO_PRIO;
+			cnt = mib->counter[offset] + mib->counter[offset + 1];
+			last_cnt = cnt;
+			cnt -= mib->rate[j].last_cnt;
+			if (cnt > 1000000 && diff >= HZ) {
+				u64 rate = cnt;
+
+				rate *= 8;
+				diff *= 1000 * 100 / HZ;
+				rate = div_u64_u32(rate, diff);
+				mib->rate[j].last = jiffies;
+				mib->rate[j].last_cnt = last_cnt;
+				if (mib->rate[j].peak < (u32) rate)
+					mib->rate[j].peak = (u32) rate;
+			}
+		} else
+			mib->rate[j].last = jiffies;
+	}
+}  /* determine_rate */
+
+static void ksz_mib_read_work(struct work_struct *work)
+{
+	struct sw_priv *hw_priv =
+		container_of(work, struct sw_priv, mib_read);
+	struct ksz_sw *sw = &hw_priv->sw;
+	struct ksz_port_mib *mib;
+	unsigned long interval;
+	uint n;
+	uint p;
+	int cnt = 0;
+
+	/* Find out how many ports are connected. */
+	for (n = 0; n <= sw->mib_port_cnt; n++) {
+		p = get_phy_port(sw, n);
+		if (media_connected == sw->port_state[p].state)
+			++cnt;
+	}
+	if (!cnt)
+		cnt++;
+	interval = MIB_READ_INTERVAL * 2 / cnt;
+	if (time_before(sw->next_jiffies, jiffies)) {
+		sw->next_jiffies = jiffies;
+		sw->next_jiffies += interval;
+	}
+	for (n = 0; n <= sw->mib_port_cnt; n++) {
+		p = get_phy_port(sw, n);
+		mib = get_port_mib(sw, p);
+
+		/* Reading MIB counters or requested to read. */
+		if (mib->cnt_ptr || 1 == hw_priv->counter[p].read) {
+
+			/* Need to process interrupt. */
+			if (port_r_cnt(sw, p))
+				return;
+			hw_priv->counter[p].read = 0;
+
+			/* Finish reading counters. */
+			if (0 == mib->cnt_ptr) {
+				hw_priv->counter[p].read = 2;
+				wake_up_interruptible(
+					&hw_priv->counter[p].counter);
+				if (p != sw->HOST_PORT)
+					determine_rate(sw, mib);
+			}
+		} else if (time_after_eq(jiffies, hw_priv->counter[p].time)) {
+			hw_priv->counter[p].time = sw->next_jiffies;
+			/* Only read MIB counters when the port is connected. */
+			if (media_connected == sw->port_state[p].state)
+				hw_priv->counter[p].read = 1;
+
+			/* Read dropped counters. */
+			else
+				mib->cnt_ptr = SWITCH_COUNTER_NUM;
+			sw->next_jiffies += interval;
+
+		/* Port is just disconnected. */
+		} else if (sw->port_state[p].link_down) {
+			int j;
+
+			for (j = 0; j < SWITCH_COUNTER_NUM; j++)
+				mib->read_cnt[j] += mib->read_max[j];
+			sw->port_state[p].link_down = 0;
+
+			/* Read counters one last time after link is lost. */
+			hw_priv->counter[p].read = 1;
+		}
+	}
+}  /* ksz_mib_read_work */
+
+static void copy_port_status(struct ksz_port *src, struct ksz_port *dst)
+{
+	dst->duplex = src->duplex;
+	dst->speed = src->speed;
+	dst->force_link = src->force_link;
+	dst->linked = src->linked;
+}
+
+static void link_read_work(struct work_struct *work)
+{
+	struct delayed_work *dwork = to_delayed_work(work);
+	struct sw_priv *hw_priv =
+		container_of(dwork, struct sw_priv, link_read);
+	struct ksz_sw *sw = &hw_priv->sw;
+	struct ksz_port *port = NULL;
+	struct ksz_port *sw_port = NULL;
+	int i;
+	int s = 1;
+	int dev_cnt = sw->dev_count + sw->dev_offset;
+
+	if (1 == sw->dev_count || 1 == sw->dev_offset)
+		s = 0;
+
+	/* Check main device when child devices are used. */
+	if (sw->dev_offset)
+		sw_port = sw->netport[0];
+	sw->ops->acquire(sw);
+	for (i = sw->dev_offset; i < dev_cnt; i++) {
+		struct phy_priv *phydata;
+		int n = i + s;
+
+		port = sw->netport[i];
+		phydata = &sw->phydata[n];
+		if (!port)
+			port = phydata->port;
+		port_get_link_speed(port);
+
+		/* Copy all port information for user access. */
+		if (port != phydata->port)
+			copy_port_status(port, phydata->port);
+	}
+	sw->ops->release(sw);
+
+	/* Need to invoke link_update_work before sw_port->linked is updated
+	 * as link_update_work can be called before link_read_work is
+	 * finished if the delay is not long enough.
+	 */
+	for (i = sw->dev_offset; i < dev_cnt; i++) {
+		struct phy_priv *phydata;
+
+		port = sw->netport[i];
+		if (!port) {
+			phydata = &sw->phydata[i];
+			port = phydata->port;
+		}
+		if (port->link_ports)
+			schedule_delayed_work(&port->link_update, 0);
+	}
+}  /* link_read_work */
+
+/*
+ * Hardware monitoring
+ */
+
+static void ksz_mib_monitor(struct timer_list *t)
+{
+	struct ksz_timer_info *info = from_timer(info, t, timer);
+	struct sw_priv *hw_priv = info->dev;
+
+	schedule_work(&hw_priv->mib_read);
+
+	ksz_update_timer(&hw_priv->mib_timer_info);
+}  /* ksz_mib_monitor */
+
+static void ksz_dev_monitor(struct timer_list *t)
+{
+	struct ksz_timer_info *info = from_timer(info, t, timer);
+	struct sw_priv *hw_priv = info->dev;
+
+#ifndef NO_PHYDEV
+	struct phy_device *phydev;
+	struct phy_priv *priv;
+	int i;
+
+	for (i = 0; i < TOTAL_PORT_NUM; i++) {
+		phydev = mdiobus_get_phy(hw_priv->bus, i);
+		if (!phydev)
+			continue;
+		priv = phydev->priv;
+		if (priv->state != phydev->state) {
+			priv->state = phydev->state;
+			if (PHY_UP == phydev->state)
+				schedule_delayed_work(&priv->port->link_update, 
+						      0);
+		}
+	}
+	if (!hw_priv->intr_working)
+		schedule_delayed_work(&hw_priv->link_read, 0);
+#endif
+
+	ksz_update_timer(&hw_priv->monitor_timer_info);
+}  /* ksz_dev_monitor */
+
+static int fiber;
+static int intr_mode;
+
+static int sw_device_present;
+
+static void ksz_probe_last(struct sw_priv *ks)
+{
+	struct ksz_sw *sw = &ks->sw;
+	int ret;
+
+	dev_set_drvdata(ks->dev, ks);
+
+#ifndef CONFIG_KSZ_SWITCH_EMBEDDED
+	init_sw_sysfs(sw, &ks->sysfs, ks->dev);
+#endif
+#ifdef KSZSW_REGS_SIZE
+	ret = sysfs_create_bin_file(&ks->dev->kobj,
+		&kszsw_registers_attr);
+#endif
+	sema_init(&ks->proc_sem, 1);
+
+	INIT_WORK(&ks->mib_read, ksz_mib_read_work);
+
+	/* 500 ms timeout */
+	ksz_init_timer(&ks->mib_timer_info, 500 * HZ / 1000,
+		ksz_mib_monitor, ks);
+	ksz_init_timer(&ks->monitor_timer_info, 100 * HZ / 1000,
+		ksz_dev_monitor, ks);
+
+	ksz_start_timer(&ks->mib_timer_info, ks->mib_timer_info.period);
+	if (!(sw->multi_dev & 1) && !sw->stp)
+		ksz_start_timer(&ks->monitor_timer_info,
+			ks->monitor_timer_info.period * 10);
+
+	sw_device_present++;
+
+#ifdef CONFIG_1588_PTP
+	if (sw->features & PTP_HW) {
+		struct ptp_info *ptp = &sw->ptp_hw;
+
+		ptp->ops->init(ptp);
+		init_ptp_sysfs(&ks->ptp_sysfs, ks->dev);
+	}
+#endif
+
+#ifndef NO_PHYDEV
+	if (ks->irq <= 0)
+		return;
+	mutex_lock(&ks->lock);
+	sw_setup_intr(sw);
+	mutex_unlock(&ks->lock);
+	ret = sw_start_interrupt(ks, dev_name(ks->dev));
+	if (ret < 0)
+		printk(KERN_WARNING "No switch interrupt\n");
+	else {
+		mutex_lock(&ks->lock);
+		sw_ena_intr(sw);
+		mutex_unlock(&ks->lock);
+	}
+#endif
+}
+
+static int ksz_probe(struct sw_priv *ks)
+{
+	struct ksz_sw *sw;
+	struct ksz_port_info *info;
+	bool rmii = false;
+	u16 id;
+	int i;
+	uint mib_port_count;
+	uint pi;
+	uint port_count;
+	int ret;
+
+	if (sw_device_present >= MAX_SW_DEVICES)
+		return -ENODEV;
+
+	ks->intr_mode = intr_mode ? IRQF_TRIGGER_FALLING :
+		IRQF_TRIGGER_LOW;
+	ks->intr_mode |= IRQF_ONESHOT;
+
+	mutex_init(&ks->hwlock);
+	mutex_init(&ks->lock);
+
+	sw = &ks->sw;
+	mutex_init(&sw->lock);
+	sw->hwlock = &ks->hwlock;
+	sw->reglock = &ks->lock;
+	sw->dev = ks;
+
+	sw->net_ops = &sw_net_ops;
+	sw->ops = &sw_ops;
+
+	/* simple check for a valid chip being connected to the bus */
+	mutex_lock(&ks->lock);
+#ifdef CONFIG_SPI_ATMEL
+	/* Switch may not be accessible the very first time when SPI mode is
+	 * not 0 in newer kernels where Atmel SPI was changed to use standard
+	 * SPI transfer function.
+	 */
+	if (ks->spi_mode & 2)
+		sw->reg->r8(sw, 0);
+#endif
+	ret = sw_chk_id(sw, &id);
+#ifdef CONFIG_HAVE_KSZ8863
+	if (ret > 0) {
+		u8 mode = sw->reg->r8(sw, REG_MODE_INDICATOR);
+
+		if (mode & PORT_3_RMII)
+			rmii = true;
+	}
+#endif
+	mutex_unlock(&ks->lock);
+	if (ret < 0) {
+		dev_err(ks->dev, "failed to read device ID(0x%x)\n", id);
+		goto err_sw;
+	}
+	dev_info(ks->dev, "chip id 0x%04x\n", id);
+
+	if (ret > 1) {
+		sw->info = kzalloc(sizeof(struct ksz_sw_info), GFP_KERNEL);
+		if (!sw->info) {
+			ret = -ENOMEM;
+			goto err_sw;
+		}
+
+		port_count = TOTAL_PORT_NUM;
+		mib_port_count = SWITCH_PORT_NUM;
+	} else {
+		port_count = 1;
+		mib_port_count = 1;
+		multi_dev = stp = 0;
+	}
+
+	sw->PORT_MASK = (1 << mib_port_count) - 1;
+
+	sw->HOST_PORT = port_count - 1;
+	sw->HOST_MASK = (1 << sw->HOST_PORT);
+
+	sw->mib_cnt = TOTAL_SWITCH_COUNTER_NUM;
+	sw->mib_port_cnt = mib_port_count;
+	sw->port_cnt = port_count;
+
+	sw->id = sw_device_present;
+
+	sw->dev_count = 1;
+
+#ifdef DEBUG
+	sw->verbose = 1;
+#endif
+	if (multi_dev < 0)
+		multi_dev = 0;
+	if (stp < 0)
+		stp = 0;
+
+	sw_setup_zone(sw);
+
+	sw_setup_logical_ports(sw);
+
+	sw->PORT_MASK |= sw->HOST_MASK;
+dbg_msg("mask: %x %x\n", sw->HOST_MASK, sw->PORT_MASK);
+
+#ifndef NO_PHYDEV
+	dbg_msg("%s\n", kszsw_phy_driver_names[ks->sw.chip_id]);
+#endif
+
+#ifdef CONFIG_1588_PTP
+	sw->features |= PTP_HW;
+	if (sw->features & PTP_HW) {
+		struct ptp_info *ptp = &sw->ptp_hw;
+
+		ptp->ports = ret;
+		ptp->reg = &ptp_reg_ops;
+		ptp->ops = &ptp_ops;
+		ptp->dev_parent = ks->dev;
+	}
+#endif
+
+	INIT_DELAYED_WORK(&ks->link_read, link_read_work);
+
+	for (pi = 0; pi < SWITCH_PORT_NUM; pi++) {
+		/*
+		 * Initialize to invalid value so that link detection
+		 * is done.
+		 */
+		info = get_port_info(sw, pi);
+		info->partner = 0xFF;
+		info->state = media_disconnected;
+	}
+	sw->interface = PHY_INTERFACE_MODE_MII;
+	if (rmii)
+		sw->interface = PHY_INTERFACE_MODE_RMII;
+	info = get_port_info(sw, pi);
+	info->state = media_connected;
+	info->tx_rate = 100 * TX_RATE_UNIT;
+	info->duplex = 2;
+	info->lpa = 0x05e1;
+
+	sw_init_phy_priv(ks);
+	setup_device_node(sw);
+
+#ifndef NO_PHYDEV
+	ret = ksz_mii_init(ks);
+	if (ret)
+		goto err_mii;
+#endif
+
+	if (ks->bus) {
+		for (i = 0; i <= sw->port_cnt; i++)
+			sw->phy[i] = mdiobus_get_phy(ks->bus, i);
+	} else {
+		for (i = 0; i <= sw->port_cnt; i++)
+			sw->phy[i] = &sw->phy_map[i];
+	}
+
+	sw->multi_dev |= multi_dev;
+	sw->stp |= stp;
+	sw->fast_aging |= fast_aging;
+	if (sw->stp)
+		sw->features |= STP_SUPPORT;
+	if (sw->fast_aging)
+		sw->overrides |= FAST_AGING;
+
+	sw->counter = ks->counter;
+	sw->monitor_timer_info = &ks->monitor_timer_info;
+	sw->link_read = &ks->link_read;
+
+	sw_init_mib(sw);
+
+	init_waitqueue_head(&sw->queue);
+	for (i = 0; i < TOTAL_PORT_NUM; i++)
+		init_waitqueue_head(&ks->counter[i].counter);
+
+	create_debugfs(ks);
+
+#ifdef CONFIG_HAVE_KSZ8863
+	sw->ops->acquire(sw);
+	id = sw->reg->r8(sw, REG_MODE_INDICATOR);
+	fiber = 0;
+	if (!(id & PORT_1_COPPER))
+		fiber |= 1;
+	if (!(id & PORT_2_COPPER))
+		fiber |= 2;
+	sw->ops->release(sw);
+#endif
+	for (i = 0; i < SWITCH_PORT_NUM; i++) {
+		if (fiber & (1 << i))
+			sw->port_info[i].fiber = true;
+	}
+	if (sw->info) {
+#ifdef CONFIG_KSZ_STP
+		ksz_stp_init(&sw->info->rstp, sw);
+#endif
+#ifdef CONFIG_KSZ_HSR
+		if (sw->features & HSR_HW)
+			ksz_hsr_init(&sw->info->hsr, sw);
+#endif
+		sw->ops->acquire(sw);
+		sw_init(sw);
+		sw_setup(sw);
+		sw_enable(sw);
+		sw->ops->release(sw);
+		sw->ops->init(sw);
+	}
+
+#ifndef NO_PHYDEV
+	ksz_probe_last(ks);
+#endif
+
+	return 0;
+
+#ifndef NO_PHYDEV
+err_mii:
+	kfree(sw->info);
+#endif
+
+err_sw:
+	kfree(ks->hw_dev);
+	kfree(ks);
+
+	return ret;
+}
+
+static void ksz_remove_first(struct sw_priv *ks)
+{
+	struct ksz_sw *sw = &ks->sw;
+
+#ifdef CONFIG_1588_PTP
+	if (sw->features & PTP_HW) {
+		struct ptp_info *ptp = &sw->ptp_hw;
+
+		exit_ptp_sysfs(&ks->ptp_sysfs, ks->dev);
+		ptp->ops->exit(ptp);
+	}
+#endif
+	ksz_stop_timer(&ks->monitor_timer_info);
+	ksz_stop_timer(&ks->mib_timer_info);
+	flush_work(&ks->mib_read);
+	cancel_delayed_work_sync(&ks->link_read);
+#ifndef NO_PHYDEV
+	ksz_mii_exit(ks);
+#endif
+
+#ifdef KSZSW_REGS_SIZE
+	sysfs_remove_bin_file(&ks->dev->kobj, &kszsw_registers_attr);
+#endif
+
+	if (sw->info) {
+		sw->ops->exit(sw);
+
+#ifndef CONFIG_KSZ_SWITCH_EMBEDDED
+		exit_sw_sysfs(sw, &ks->sysfs, ks->dev);
+#endif
+
+#ifdef CONFIG_KSZ_STP
+		ksz_stp_exit(&sw->info->rstp);
+#endif
+	}
+
+	delete_debugfs(ks);
+
+	/* Indicate this function has been called before. */
+	dev_set_drvdata(ks->dev, NULL);
+}
+
+static int ksz_remove(struct sw_priv *ks)
+{
+	struct sw_priv *hw_priv = dev_get_drvdata(ks->dev);
+	struct ksz_sw *sw = &ks->sw;
+
+	if (hw_priv)
+		ksz_remove_first(hw_priv);
+	kfree(sw->info);
+	kfree(ks->hw_dev);
+	kfree(ks);
+
+	return 0;
+}
+
+module_param(fiber, int, 0);
+MODULE_PARM_DESC(fiber, "Use fiber in ports");
+
+module_param(intr_mode, int, 0);
+MODULE_PARM_DESC(intr_mode,
+	"Configure which interrupt mode to use(0=level low, 1=falling)");
+
+module_param(fast_aging, int, 0);
+module_param(multi_dev, int, 0);
+module_param(stp, int, 0);
+MODULE_PARM_DESC(fast_aging, "Fast aging");
+MODULE_PARM_DESC(multi_dev, "Multiple device interfaces");
+MODULE_PARM_DESC(stp, "STP support");
+
diff -Napur linux-linux4microchip-2022.10-orig/drivers/net/ethernet/micrel/ksz_sw.h linux-linux4microchip-2022.10/drivers/net/ethernet/micrel/ksz_sw.h
--- linux-linux4microchip-2022.10-orig/drivers/net/ethernet/micrel/ksz_sw.h	1969-12-31 16:00:00.000000000 -0800
+++ linux-linux4microchip-2022.10/drivers/net/ethernet/micrel/ksz_sw.h	2023-12-05 18:41:12.000000000 -0800
@@ -0,0 +1,867 @@
+/**
+ * Microchip switch common header
+ *
+ * Copyright (c) 2015-2023 Microchip Technology Inc.
+ *	Tristram Ha <Tristram.Ha@microchip.com>
+ *
+ * Copyright (c) 2010-2015 Micrel, Inc.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ */
+
+#ifndef KSZ_SW_H
+#define KSZ_SW_H
+
+
+#ifdef CONFIG_PHYLINK
+#include <linux/phylink.h>
+#endif
+
+/* These definitions should be defined before this header file. */
+#ifndef PRIO_QUEUES
+#define PRIO_QUEUES			4
+#endif
+
+#ifndef KS_PRIO_IN_REG
+#define KS_PRIO_IN_REG			4
+#endif
+
+#ifndef TOTAL_PORT_NUM
+#define TOTAL_PORT_NUM			3
+#endif
+
+#ifndef SWITCH_COUNTER_NUM
+#define SWITCH_COUNTER_NUM		0x20
+#endif
+#ifndef TOTAL_SWITCH_COUNTER_NUM
+#define TOTAL_SWITCH_COUNTER_NUM	(SWITCH_COUNTER_NUM + 2)
+#endif
+
+#ifndef SW_D
+#error "SW_D and other data bus parameters need to be defined."
+#endif
+
+/* Host port can only be last of them. */
+#define SWITCH_PORT_NUM			(TOTAL_PORT_NUM - 1)
+
+#define MAX_SW_DEVICES			2
+
+
+#include "ksz_sw_api.h"
+#ifdef CONFIG_KSZ_STP
+#include "ksz_stp.h"
+#endif
+#ifdef CONFIG_1588_PTP
+#include "ksz_ptp.h"
+#endif
+#ifdef CONFIG_KSZ_MRP
+#include "ksz_mrp.h"
+#endif
+#ifdef CONFIG_KSZ_DLR
+#include "ksz_dlr.h"
+#endif
+#ifdef CONFIG_KSZ_HSR
+#include "ksz_hsr.h"
+#endif
+
+
+#define KS_PRIO_M			SWITCH_802_1P_MAP_MASK
+#define KS_PRIO_S			SWITCH_802_1P_MAP_SHIFT
+
+#define LEARNED_MAC_TABLE_ENTRIES	1024
+#define STATIC_MAC_TABLE_ENTRIES	8
+#define SWITCH_MAC_TABLE_ENTRIES	16
+#define MULTI_MAC_TABLE_ENTRIES		40
+
+
+/**
+ * struct ksz_mac_table - Static MAC table data structure
+ * @mac_addr:	MAC address to filter.
+ * @vid:	VID value.
+ * @fid:	FID value.
+ * @ports:	Port membership.
+ * @override:	Override setting.
+ * @use_fid:	FID use setting.
+ * @valid:	Valid setting indicating the entry is being used.
+ */
+struct ksz_mac_table {
+	u8 mac_addr[ETH_ALEN];
+	u16 vid;
+	u8 fid;
+	u8 ports;
+	u8 override:1;
+	u8 use_fid:1;
+	u8 valid:1;
+};
+
+#define FWD_HOST_OVERRIDE		(1 << 0)
+#define FWD_HOST			(1 << 1)
+#define FWD_STP_DEV			(1 << 2)
+#define FWD_MAIN_DEV			(1 << 3)
+#define FWD_VLAN_DEV			(1 << 4)
+
+struct ksz_alu_table {
+	u8 owner;
+	u8 forward;
+	u8 valid:1;
+};
+
+#define VLAN_TABLE_ENTRIES		16
+#define FID_ENTRIES			16
+
+/**
+ * struct ksz_vlan_table - VLAN table data structure
+ * @vid:	VID value.
+ * @fid:	FID value.
+ * @member:	Port membership.
+ * @valid:	Valid setting indicating the entry is being used.
+ */
+struct ksz_vlan_table {
+	u16 vid;
+	u8 fid;
+	u8 member;
+	u8 valid:1;
+};
+
+#define PRIO_802_1P_ENTRIES		8
+
+#define DIFFSERV_ENTRIES		64
+
+/**
+ * struct ksz_port_mib - Port MIB data structure
+ * @cnt_ptr:	Current pointer to MIB counter index.
+ * @mib_start:	The starting counter index.  Some ports do not start at 0.
+ * @counter:	64-bit MIB counter value.
+ * @dropped:	Temporary buffer to remember last read packet dropped values.
+ * @read_cnt:	Used to signal when to read the MIB counter.
+ * @read_max:	Used to indicate how often to read the MIB counter.
+ *
+ * MIB counters needs to be read periodically so that counters do not get
+ * overflowed and give incorrect values.  A right balance is needed to
+ * satisfy this condition and not waste too much CPU time.
+ */
+struct ksz_port_mib {
+	u8 cnt_ptr;
+	u8 mib_start;
+	u8 reserved[2];
+
+	u64 counter[TOTAL_SWITCH_COUNTER_NUM];
+	u32 dropped[2];
+	u8 read_cnt[SWITCH_COUNTER_NUM];
+	u8 read_max[SWITCH_COUNTER_NUM];
+	struct {
+		unsigned long last;
+		u64 last_cnt;
+		u32 peak;
+	} rate[2];
+};
+
+enum {
+	STP_STATE_DISABLED = 0,
+	STP_STATE_LISTENING,
+	STP_STATE_LEARNING,
+	STP_STATE_FORWARDING,
+	STP_STATE_BLOCKED,
+	STP_STATE_SIMPLE
+};
+
+/**
+ * struct ksz_port_cfg - Port configuration data structure
+ * @vid:	VID value.
+ * @member:	Port membership.
+ * @port_prio:	Port priority.
+ * @rate_ctrl:	Priority rate control.
+ * @rx_rate:	Receive priority rate.
+ * @tx_rate:	Transmit priority rate.
+ * @rate_limit: Priority rate limit value.
+ * @vid_member:	VLAN membership.
+ * @index:	Net device pointer.
+ * @stp_state:	Current Spanning Tree Protocol state.
+ */
+struct ksz_port_cfg {
+	u16 vid;
+	u8 member;
+	u8 port_prio;
+	u8 rate_ctrl[PRIO_QUEUES];
+	u32 rx_rate[PRIO_QUEUES];
+	u32 tx_rate[PRIO_QUEUES];
+	u8 rate_limit;
+	u8 vid_member;
+	int index;
+	int stp_state;
+};
+
+/**
+ * struct ksz_sw_info - KSZ8463 switch information data structure
+ * @mac_table:	MAC table entries information.
+ * @multi_net:	Network multicast addresses used.
+ * @multi_sys:	System multicast addresses used.
+ * @blocked_rx:	Blocked receive addresses.
+ * @blocked_rx_cnt: Blocked receive addresses count.
+ * @vlan_table:	VLAN table entries information.
+ * @port_cfg:	Port configuration information.
+ * @rx_table:	Receive frame information.
+ * @tx_table:	Transmit frame information.
+ * @diffserv:	DiffServ priority settings.  Possible values from 6-bit of ToS
+ *		(bit7 ~ bit2) field.
+ * @p_802_1p:	802.1P priority settings.  Possible values from 3-bit of 802.1p
+ *		Tag priority field.
+ * @br_addr:	Bridge address.  Used for STP.
+ * @mac_addr:	Switch MAC address.
+ * @broad_per:	Broadcast storm percentage.
+ * @member:	Current port membership.  Used for STP.
+ * @phy_addr:	PHY address used by first port.
+ */
+struct ksz_sw_info {
+	struct ksz_mac_table mac_table[MULTI_MAC_TABLE_ENTRIES];
+	struct ksz_alu_table alu_table[MULTI_MAC_TABLE_ENTRIES];
+	u32 mac_table_used;
+	int forward;
+	int multi_net;
+	int multi_sys;
+	struct ksz_vlan_table vlan_table[VLAN_TABLE_ENTRIES];
+	u16 vlan_table_used;
+	u16 fid_used;
+	struct ksz_port_cfg port_cfg[TOTAL_PORT_NUM];
+#ifdef CONFIG_KSZ_STP
+	struct ksz_stp_info rstp;
+#endif
+#ifdef CONFIG_KSZ_DLR
+	struct ksz_dlr_info dlr;
+#endif
+#ifdef CONFIG_KSZ_HSR
+	struct ksz_hsr_info hsr;
+#endif
+
+	SW_D diffserv[DIFFSERV_ENTRIES / KS_PRIO_IN_REG];
+	SW_D p_802_1p[PRIO_802_1P_ENTRIES / KS_PRIO_IN_REG];
+
+	u8 br_addr[ETH_ALEN];
+	u8 mac_addr[ETH_ALEN];
+
+	u8 broad_per;
+	u8 member[1];
+	u8 phy_addr;
+};
+
+/**
+ * struct ksz_port_state - Port state information data structure
+ * @state:	Connection status of the port.
+ * @link_down:	Indication the link has just gone down.
+ *
+ * It is pointless to read MIB counters when the port is disconnected.  The
+ * @state provides the connection status so that MIB counters are read only
+ * when the port is connected.  The @link_down indicates the port is just
+ * disconnected so that all MIB counters are read one last time to update the
+ * information.
+ */
+struct ksz_port_state {
+	uint state;
+	u8 link_down;
+};
+
+#define TX_RATE_UNIT			10000
+
+/**
+ * struct ksz_port_info - Port information data structure
+ * @state:	Connection status of the port.
+ * @tx_rate:	Transmit rate divided by 10000 to get Mbit.
+ * @duplex:	Duplex mode.
+ * @flow_ctrl:	Flow control.
+ * @advertised:	Advertised auto-negotiation setting.  Used to determine link.
+ * @partner:	Auto-negotiation partner setting.  Used to determine link.
+ * @port_id:	Port index to access actual hardware register.
+ * @status:	LinkMD status values.
+ * @length:	LinkMD length values.
+ * @mac_addr:	MAC address of the port.
+ * @phy_id:	PHY id used by the port.
+ * @fiber:	Using fiber instead of copper.
+ */
+struct ksz_port_info {
+	uint state;
+	uint tx_rate;
+	u8 duplex;
+	u8 flow_ctrl;
+	u8 advertised;
+	u8 partner;
+	u8 port_id;
+	u16 lpa;
+	u32 status[3];
+	u32 length[3];
+	u8 mac_addr[ETH_ALEN];
+	u8 own_flow_ctrl;
+	u8 own_duplex;
+	u16 own_speed;
+	u8 phy_id;
+	u32 phy:1;
+	u32 fiber:1;
+
+	u8 phy_p;
+	u8 log_p;
+	u16 phy_m;
+	u16 log_m;
+};
+
+struct ksz_sw;
+struct ksz_port;
+
+struct ksz_sw_reg_ops {
+	u8 (*r8)(struct ksz_sw *sw, unsigned reg);
+	u16 (*r16)(struct ksz_sw *sw, unsigned reg);
+	u32 (*r32)(struct ksz_sw *sw, unsigned reg);
+	void (*w8)(struct ksz_sw *sw, unsigned reg, unsigned val);
+	void (*w16)(struct ksz_sw *sw, unsigned reg, unsigned val);
+	void (*w32)(struct ksz_sw *sw, unsigned reg, unsigned val);
+
+	SW_D (*r)(struct ksz_sw *sw, unsigned reg);
+	void (*w)(struct ksz_sw *sw, unsigned reg, unsigned val);
+
+	int (*get)(struct ksz_sw *sw, u32 reg, size_t count, char *buf);
+	int (*set)(struct ksz_sw *sw, u32 reg, size_t count, char *buf);
+};
+
+struct ksz_sw_net_ops {
+	void (*setup_special)(struct ksz_sw *sw, int *port_cnt,
+		int *mib_port_cnt, int *dev_cnt,
+		const void *ops);
+	void (*setup_mdiobus)(struct ksz_sw *sw, void *bus);
+	int (*setup_dev)(struct ksz_sw *sw, struct net_device *dev,
+		char *dev_name, struct ksz_port *port, int i, uint port_cnt,
+		uint mib_port_cnt);
+	void (*leave_dev)(struct ksz_sw *sw);
+
+	void (*start)(struct ksz_sw *sw, u8 *addr);
+	int (*stop)(struct ksz_sw *sw, int complete);
+	int (*open_dev)(struct ksz_sw *sw, struct net_device *dev,
+		struct ksz_port *port, u8 *addr);
+	void (*open_port)(struct ksz_sw *sw, struct net_device *dev,
+		struct ksz_port *port);
+	void (*close_port)(struct ksz_sw *sw, struct net_device *dev,
+		struct ksz_port *port);
+	void (*open)(struct ksz_sw *sw);
+	void (*close)(struct ksz_sw *sw);
+
+	void (*netdev_start_queue)(struct ksz_sw *sw, struct net_device *dev);
+	void (*netdev_stop_queue)(struct ksz_sw *sw, struct net_device *dev);
+	void (*netdev_wake_queue)(struct ksz_sw *sw, struct net_device *dev);
+	void (*netdev_open_port)(struct ksz_sw *sw, struct net_device *dev);
+
+	u8 (*set_mac_addr)(struct ksz_sw *sw, struct net_device *dev,
+		u8 promiscuous, uint port);
+
+	int (*get_mtu)(struct ksz_sw *sw);
+	int (*get_tx_len)(struct ksz_sw *sw, struct sk_buff *skb, uint port,
+		int *header);
+	void (*add_tail_tag)(struct ksz_sw *sw, struct sk_buff *skb, uint dst);
+	int (*get_tail_tag)(u8 *trailer, int *port);
+	void (*add_vid)(struct ksz_sw *sw, u16 vid);
+	void (*kill_vid)(struct ksz_sw *sw, u16 vid);
+	struct sk_buff *(*check_tx)(struct ksz_sw *sw, struct net_device *dev,
+		struct sk_buff *skb, struct ksz_port *priv);
+	struct net_device *(*rx_dev)(struct ksz_sw *sw, u8 *data, u32 *len,
+		int *tag, int *port);
+	int (*match_pkt)(struct ksz_sw *sw, struct net_device **dev,
+		void **priv, int (*get_promiscuous)(void *ptr),
+		int (*match_multi)(void *ptr, u8 *data),
+		struct sk_buff *skb, u8 h_promiscuous);
+	struct net_device *(*parent_rx)(struct ksz_sw *sw,
+		struct net_device *dev, int *forward);
+	int (*port_vlan_rx)(struct sk_buff *skb, int forward, int tag);
+	struct sk_buff *(*final_skb)(struct ksz_sw *sw, struct sk_buff *skb,
+		struct net_device *dev, struct ksz_port *port);
+	int (*drv_rx)(struct ksz_sw *sw, struct sk_buff *skb, uint port);
+	void (*set_multi)(struct ksz_sw *sw, struct net_device *dev,
+		struct ksz_port *priv);
+};
+
+struct ksz_sw_ops {
+	void (*init)(struct ksz_sw *sw);
+	void (*exit)(struct ksz_sw *sw);
+	int (*dev_req)(struct ksz_sw *sw, char *arg,
+		struct file_dev_info *info);
+
+	uint (*get_phy_port)(struct ksz_sw *sw, uint n);
+	uint (*get_log_port)(struct ksz_sw *sw, uint p);
+
+	void (*acquire)(struct ksz_sw *sw);
+	void (*release)(struct ksz_sw *sw);
+
+	int (*chk)(struct ksz_sw *sw, u32 addr, SW_D bits);
+	void (*cfg)(struct ksz_sw *sw, u32 addr, SW_D bits, bool set);
+
+	int (*port_get_link_speed)(struct ksz_port *port);
+	void (*port_set_link_speed)(struct ksz_port *port);
+	void (*port_force_link_speed)(struct ksz_port *port);
+
+	int (*port_r_cnt)(struct ksz_sw *sw, uint port);
+	void (*get_mib_counters)(struct ksz_sw *sw, int first, int cnt,
+		u64 *counter);
+
+	ssize_t (*sysfs_read)(struct ksz_sw *sw, int proc_num,
+		struct ksz_port *port, ssize_t len, char *buf);
+	ssize_t (*sysfs_read_hw)(struct ksz_sw *sw, int proc_num, ssize_t len,
+		char *buf);
+	int (*sysfs_write)(struct ksz_sw *sw, int proc_num,
+		struct ksz_port *port, int num, const char *buf);
+	ssize_t (*sysfs_port_read)(struct ksz_sw *sw, int proc_num, uint port,
+		ssize_t len, char *buf);
+	ssize_t (*sysfs_port_read_hw)(struct ksz_sw *sw, int proc_num,
+		uint port, ssize_t len, char *buf);
+	int (*sysfs_port_write)(struct ksz_sw *sw, int proc_num, uint port,
+		int num, const char *buf);
+	ssize_t (*sysfs_mac_read)(struct ksz_sw *sw, int proc_num, int index,
+		ssize_t len, char *buf);
+	int (*sysfs_mac_write)(struct ksz_sw *sw, int proc_num, int index,
+		int num, const char *buf);
+	ssize_t (*sysfs_vlan_read)(struct ksz_sw *sw, int proc_num, int index,
+		ssize_t len, char *buf);
+	int (*sysfs_vlan_write)(struct ksz_sw *sw, int proc_num, int index,
+		int num);
+
+#ifdef CONFIG_KSZ_STP
+	ssize_t (*sysfs_stp_read)(struct ksz_sw *sw, int proc_num, ssize_t len,
+		char *buf);
+	int (*sysfs_stp_write)(struct ksz_sw *sw, int proc_num, int num,
+		const char *buf);
+	ssize_t (*sysfs_stp_port_read)(struct ksz_sw *sw, int proc_num,
+		uint port, ssize_t len, char *buf);
+	int (*sysfs_stp_port_write)(struct ksz_sw *sw, int proc_num, uint port,
+		int num, const char *buf);
+#endif
+
+	void (*cfg_mac)(struct ksz_sw *sw, u8 index, u8 *dest, u32 ports,
+		int override, int use_fid, u16 fid);
+	void (*cfg_vlan)(struct ksz_sw *sw, u8 index, u16 vid, u16 fid,
+		u32 ports);
+	u8 (*alloc_mac)(struct ksz_sw *sw);
+	void (*free_mac)(struct ksz_sw *sw, u8 index);
+	u8 (*alloc_vlan)(struct ksz_sw *sw);
+	void (*free_vlan)(struct ksz_sw *sw, u8 index);
+	u16 (*alloc_fid)(struct ksz_sw *sw, u16 vid);
+	void (*free_fid)(struct ksz_sw *sw, u16 fid);
+
+	const u8 *(*get_br_id)(struct ksz_sw *sw);
+	void (*from_backup)(struct ksz_sw *sw, uint p);
+	void (*to_backup)(struct ksz_sw *sw, uint p);
+	void (*from_designated)(struct ksz_sw *sw, uint p, bool alt);
+	void (*to_designated)(struct ksz_sw *sw, uint p);
+	void (*tc_detected)(struct ksz_sw *sw, uint p);
+	int (*get_tcDetected)(struct ksz_sw *sw, uint p);
+
+	void (*cfg_src_filter)(struct ksz_sw *sw, bool set);
+	void (*flush_table)(struct ksz_sw *sw, uint port);
+};
+
+struct ksz_dev_map {
+	u8 cnt;
+	u8 mask;
+	u8 first;
+	u8 phy_id;
+	u16 vlan;
+	uint proto;
+};
+
+struct phy_priv {
+	struct ksz_port *port;
+	enum phy_state state;
+};
+
+struct ksz_sw_cached_regs {
+	u16 ptp_clk_ctrl;
+};
+
+/* Switch features and bug fixes. */
+#define STP_SUPPORT			(1 << 0)
+#define VLAN_PORT			(1 << 1)
+#define VLAN_PORT_REMOVE_TAG		(1 << 2)
+#define VLAN_PORT_TAGGING		(1 << 3)
+#define VLAN_PORT_START			200
+#define SW_VLAN_DEV			(1 << 4)
+#define MRP_SUPPORT			(1 << 5)
+
+#define DLR_HW				(1 << 24)
+#define HSR_HW				(1 << 25)
+
+#define DSA_SUPPORT			(1 << 28)
+#define DIFF_MAC_ADDR			(1 << 30)
+#ifdef CONFIG_1588_PTP
+#define PTP_HW				(1 << 31)
+#endif
+
+/* Software overrides. */
+#define PAUSE_FLOW_CTRL			(1 << 0)
+#define FAST_AGING			(1 << 1)
+#define UPDATE_CSUM			(1 << 2)
+
+#define TAIL_PRP_0			(1 << 24)
+#define TAIL_PRP_1			(1 << 25)
+
+#define TAG_REMOVE			(1 << 30)
+#define TAIL_TAGGING			(1 << 31)
+
+#define TAIL_TAG_SET_OVERRIDE		BIT(31)
+#define TAIL_TAG_SET_QUEUE		BIT(30)
+
+/**
+ * struct ksz_sw - Virtual switch data structure
+ * @dev:		Pointer to hardware device.
+ * @phydev:		Pointer to PHY device interface.
+ * @interface:		The R/G/MII interface used.
+ * @msg_enable:		The message flags controlling driver output.
+ * @hwlock:		Pointer to hardware lock.
+ * @reglock:		Pointer to register lock.
+ * @lock		Software lock to switch structure.
+ * @locked:		locked status.
+ * @info:		Pointer to switch information structure.
+ * @port_info:		Port information.
+ * @netdev:		Pointer to OS dependent network devices.
+ * @phy:		Pointer to OS dependent PHY devices.
+ * @dev_offset:		Indication of a switch associated network device.
+ * @phy_offset:		Indication of a port associated PHY device.
+ * @port_state:		Port state information.
+ * @port_mib:		Port MIB information.
+ * @mib_cnt:		Number of MIB counters this switch has.
+ * @mib_port_cnt:	Number of ports with MIB counters.
+ * @port_cnt:		Number of ports to support.
+ * @monitor_timer_info:	Timer information for monitoring ports.
+ * @counter:		Pointer to OS dependent MIB counter information.
+ * @link_read:		Workqueue for link monitoring.
+ * @ops:		Switch function access.
+ * @reg:		Switch register access.
+ * @net_ops:		Network related switch function access.
+ * @HOST_PORT:		A predefined value indicating the host port.
+ * @HOST_MASK:		A predefined value indicating the host port mask.
+ * @PORT_MASK:		A predefined value indicating the port mask.
+ * @rx_ports:		Bitmap of ports with receive enabled.
+ * @tx_ports:		Bitmap of ports with transmit enabled.
+ * @dev_count:		Number of network devices this switch supports.
+ * @id:			Hardware ID.  Used for display only.
+ * @vlan_id		Used for the VLAN port forwarding feature.
+ * @vid:		Used for the VLAN port forwarding feature.
+ * @features:		Switch features to enable.
+ * @overrides:		Switch features to override.
+ * @multi_dev:		Used to specify multiple devices mode.
+ * @stp:		Used to enable STP.
+ * @fast_aging:		Used to enable fast aging.
+ */
+struct ksz_sw {
+	void *dev;
+	phy_interface_t interface;
+	u32 msg_enable;
+	wait_queue_head_t queue;
+	struct mutex *hwlock;
+	struct mutex *reglock;
+	struct mutex lock;
+	int intr_using;
+
+	struct ksz_sw_info *info;
+	struct ksz_port_info port_info[TOTAL_PORT_NUM];
+	struct net_device *main_dev;
+	struct ksz_port *main_port;
+	struct net_device *netdev[TOTAL_PORT_NUM];
+	struct ksz_port *netport[TOTAL_PORT_NUM];
+	struct device_node *devnode[TOTAL_PORT_NUM];
+	struct phy_device phy_map[TOTAL_PORT_NUM + 1];
+	struct phy_device *phy[TOTAL_PORT_NUM + 1];
+	struct phy_priv phydata[TOTAL_PORT_NUM + 1];
+	int dev_offset;
+	int phy_offset;
+	struct ksz_port_state port_state[TOTAL_PORT_NUM];
+	struct ksz_port_mib port_mib[TOTAL_PORT_NUM];
+	unsigned long next_jiffies;
+	int mib_cnt;
+	int mib_port_cnt;
+	int dsa_port_cnt;
+	int port_cnt;
+	struct ksz_timer_info *monitor_timer_info;
+	struct ksz_counter_info *counter;
+	struct delayed_work *link_read;
+
+#ifdef CONFIG_PHYLINK
+	const struct phylink_mac_ops *phylink_ops;
+#endif
+
+	const struct ksz_sw_ops *ops;
+	const struct ksz_sw_reg_ops *reg;
+	struct ksz_sw_net_ops *net_ops;
+
+	int HOST_PORT;
+	u16 HOST_MASK;
+	u16 PORT_MASK;
+	u16 dev_ports;
+	u16 rx_ports;
+	u16 tx_ports;
+	u8 tx_pad[60];
+	int tx_start;
+	struct ksz_sw_cached_regs cached;
+
+	int dev_major;
+	u8 *msg_buf;
+	struct file_dev_info *dev_list[2];
+	uint notifications;
+	char dev_name[20];
+
+	int chip_id;
+	int dev_count;
+	int id;
+	u32 vlan_id;
+	u16 vid;
+	u8 verbose;
+
+	uint features;
+	uint overrides;
+
+	int multi_dev;
+	int stp;
+	int fast_aging;
+	struct ksz_dev_map eth_maps[SWITCH_PORT_NUM];
+	int eth_cnt;
+
+#ifdef CONFIG_MRP
+	struct mrp_info mrp;
+#endif
+
+#ifdef CONFIG_1588_PTP
+	/* PTP structure size can be variable. */
+	struct ptp_info ptp_hw;
+#endif
+};
+
+struct ksz_sw_sysfs {
+	struct ksz_dev_attr *ksz_port_attrs[TOTAL_PORT_NUM];
+	struct attribute **port_attrs[TOTAL_PORT_NUM];
+	struct ksz_dev_attr *ksz_mac_attrs[SWITCH_MAC_TABLE_ENTRIES];
+	struct attribute **mac_attrs[SWITCH_MAC_TABLE_ENTRIES];
+	struct ksz_dev_attr *ksz_vlan_attrs[VLAN_TABLE_ENTRIES];
+	struct attribute **vlan_attrs[VLAN_TABLE_ENTRIES];
+};
+
+/**
+ * struct ksz_port - Virtual port data structure
+ * @first_port:		Index of first port this port supports.
+ * @mib_port_cnt:	Number of ports with MIB counters.
+ * @port_cnt:		Number of ports this port supports.
+ * @flow_ctrl:		Flow control setting.  PHY_NO_FLOW_CTRL for no flow
+ *			control, and PHY_FLOW_CTRL for flow control.
+ *			PHY_TX_ONLY and PHY_RX_ONLY are not supported for 100
+ *			Mbit PHY.
+ * @duplex:		Duplex mode setting.  1 for half duplex, 2 for full
+ *			duplex, and 0 for auto, which normally results in full
+ *			duplex.
+ * @speed:		Speed setting.  10 for 10 Mbit, 100 for 100 Mbit, and
+ *			0 for auto, which normally results in 100 Mbit.
+ * @force_link:		Force link setting.  0 for auto-negotiation, and 1 for
+ *			force.
+ * @linked:		Pointer to port information linked to this port.
+ * @sw:			Pointer to virtual switch structure.
+ */
+struct ksz_port {
+	int first_port;
+	int mib_port_cnt;
+	int port_cnt;
+
+	u8 flow_ctrl;
+	u8 duplex;
+	u8 speed;
+	u8 force_link;
+	u8 state;
+	uint opened:1;
+	uint ready:1;
+	uint report:1;
+	u16 link_ports;
+
+	struct ksz_port_info *linked;
+
+	struct ksz_sw *sw;
+
+	struct delayed_work link_update;
+	struct net_device *netdev;
+	struct phy_device *phydev;
+	struct device_node *dn;
+#ifdef CONFIG_PHYLINK
+	struct phylink *pl;
+	struct phylink_config pl_config;
+	struct phylink_link_state pl_state;
+#endif
+};
+
+static inline void sw_update_csum(struct ksz_sw *sw)
+{
+	sw->overrides |= UPDATE_CSUM;
+}
+
+#ifdef CONFIG_KSZ_HSR
+static inline bool using_hsr(struct ksz_sw *sw)
+{
+	return (sw->features & HSR_HW);
+}
+#endif
+
+static inline bool using_tail_tag(struct ksz_sw *sw)
+{
+	return (sw->overrides & TAIL_TAGGING);
+}
+
+struct lan_attributes {
+	int info;
+	int version;
+	int duplex;
+	int speed;
+	int force;
+	int flow_ctrl;
+	int features;
+	int overrides;
+	int mib;
+	int reg;
+	int vid;
+	int dynamic_table;
+	int static_table;
+	int vlan_table;
+	int aging;
+	int fast_aging;
+	int link_aging;
+	int bcast_per;
+	int mcast_storm;
+	int diffserv_map;
+	int p_802_1p_map;
+	int vlan;
+	int null_vid;
+	int macaddr;
+	int mirror_mode;
+	int tail_tag;
+	int igmp_snoop;
+	int ipv6_mld_snoop;
+	int ipv6_mld_option;
+	int aggr_backoff;
+	int no_exc_drop;
+	int buf_reserve;
+	int huge_packet;
+	int legal_packet;
+	int length_check;
+	int back_pressure;
+	int sw_flow_ctrl;
+	int sw_half_duplex;
+	int sw_10_mbit;
+	int rx_flow_ctrl;
+	int tx_flow_ctrl;
+	int fair_flow_ctrl;
+	int vlan_bound;
+	int fw_unk_dest;
+	int ins_tag_0_1;
+	int ins_tag_0_2;
+	int ins_tag_1_0;
+	int ins_tag_1_2;
+	int ins_tag_2_0;
+	int ins_tag_2_1;
+	int pass_all;
+	int pass_pause;
+	int phy_addr;
+	int ports;
+	int dev_start;
+	int vlan_start;
+	int stp;
+
+#ifdef CONFIG_KSZ_STP
+	int stp_br_info;
+	int stp_br_on;
+	int stp_br_prio;
+	int stp_br_fwd_delay;
+	int stp_br_max_age;
+	int stp_br_hello_time;
+	int stp_br_tx_hold;
+	int stp_version;
+#endif
+};
+
+struct sw_attributes {
+	int mib;
+	int vid;
+	int member;
+	int bcast_storm;
+	int diffserv;
+	int p_802_1p;
+	int port_based;
+	int non_vid;
+	int ingress;
+	int ins_tag;
+	int rmv_tag;
+	int double_tag;
+	int drop_tagged;
+	int replace_prio;
+	int rx;
+	int tx;
+	int learn;
+	int prio_queue;
+	int tx_p0_ctrl;
+	int tx_p1_ctrl;
+	int tx_p2_ctrl;
+	int tx_p3_ctrl;
+	int tx_p0_ratio;
+	int tx_p1_ratio;
+	int tx_p2_ratio;
+	int tx_p3_ratio;
+	int rx_prio_rate;
+	int tx_prio_rate;
+	int rx_limit;
+	int cnt_ifg;
+	int cnt_pre;
+	int rx_p0_rate;
+	int rx_p1_rate;
+	int rx_p2_rate;
+	int rx_p3_rate;
+	int tx_p0_rate;
+	int tx_p1_rate;
+	int tx_p2_rate;
+	int tx_p3_rate;
+	int mirror_port;
+	int mirror_rx;
+	int mirror_tx;
+	int back_pressure;
+	int force_flow_ctrl;
+	int fw_unk_dest;
+	int fw_inv_vid;
+
+	int duplex;
+	int speed;
+	int linkmd;
+	int macaddr;
+	int src_filter_0;
+	int src_filter_1;
+
+#ifdef CONFIG_KSZ_STP
+	int stp_info;
+	int stp_on;
+	int stp_prio;
+	int stp_admin_path_cost;
+	int stp_path_cost;
+	int stp_admin_edge;
+	int stp_auto_edge;
+	int stp_mcheck;
+	int stp_admin_p2p;
+#endif
+};
+
+struct static_mac_attributes {
+	int fid;
+	int use_fid;
+	int override;
+	int valid;
+	int ports;
+	int addr;
+};
+
+struct vlan_attributes {
+	int valid;
+	int member;
+	int fid;
+	int vid;
+};
+
+#endif
diff -Napur linux-linux4microchip-2022.10-orig/drivers/net/ethernet/micrel/ksz_sw_sysfs_8795.c linux-linux4microchip-2022.10/drivers/net/ethernet/micrel/ksz_sw_sysfs_8795.c
--- linux-linux4microchip-2022.10-orig/drivers/net/ethernet/micrel/ksz_sw_sysfs_8795.c	1969-12-31 16:00:00.000000000 -0800
+++ linux-linux4microchip-2022.10/drivers/net/ethernet/micrel/ksz_sw_sysfs_8795.c	2023-04-25 16:13:55.392163494 -0700
@@ -0,0 +1,750 @@
+/**
+ * Microchip KSZ8795 switch common sysfs code
+ *
+ * Copyright (c) 2015-2018 Microchip Technology Inc.
+ *	Tristram Ha <Tristram.Ha@microchip.com>
+ *
+ * Copyright (c) 2011-2015 Micrel, Inc.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ */
+
+
+#include "ksz_sysfs.h"
+
+
+static char *sw_name[TOTAL_PORT_NUM] = {
+	"sw0",
+	"sw1",
+	"sw2",
+	"sw3",
+	"sw4",
+};
+
+static ssize_t netlan_show(struct device *d, struct device_attribute *attr,
+	char *buf, unsigned long offset)
+{
+	struct ksz_port *port;
+	struct ksz_sw *sw;
+	struct semaphore *proc_sem;
+	ssize_t len = -EINVAL;
+	int proc_num;
+
+	get_private_data(d, &proc_sem, &sw, &port);
+	if (down_interruptible(proc_sem))
+		return -ERESTARTSYS;
+
+	len = 0;
+	proc_num = offset / sizeof(int);
+	len = sw->ops->sysfs_read(sw, proc_num, port, len, buf);
+	if (len)
+		goto netlan_show_done;
+
+	len = sw->ops->sysfs_mac_read(sw, proc_num, len, buf);
+	if (len)
+		goto netlan_show_done;
+
+	len = sw->ops->sysfs_vlan_read(sw, proc_num, len, buf);
+	if (len)
+		goto netlan_show_done;
+
+#ifdef CONFIG_KSZ_STP
+	len = sw->ops->sysfs_stp_read(sw, proc_num, len, buf);
+	if (len)
+		goto netlan_show_done;
+#endif
+
+	/* Require hardware to be acquired first. */
+	sw->ops->acquire(sw);
+	len = sw->ops->sysfs_read_hw(sw, proc_num, len, buf);
+	sw->ops->release(sw);
+
+netlan_show_done:
+	up(proc_sem);
+	return len;
+}
+
+static ssize_t netlan_store(struct device *d, struct device_attribute *attr,
+	const char *buf, size_t count, unsigned long offset)
+{
+	struct ksz_port *port;
+	struct ksz_sw *sw;
+	struct semaphore *proc_sem;
+	ssize_t ret = -EINVAL;
+	int num;
+	int proc_num;
+
+	num = get_num_val(buf);
+	get_private_data(d, &proc_sem, &sw, &port);
+	if (down_interruptible(proc_sem))
+		return -ERESTARTSYS;
+
+	proc_num = offset / sizeof(int);
+	ret = count;
+
+	if (sw->ops->sysfs_mac_write(sw, proc_num, num, buf))
+		goto netlan_store_done;
+
+	if (sw->ops->sysfs_vlan_write(sw, proc_num, num))
+		goto netlan_store_done;
+
+#ifdef CONFIG_KSZ_STP
+	if (sw->ops->sysfs_stp_write(sw, proc_num, num, buf))
+		goto netlan_store_done;
+#endif
+
+	sw->ops->acquire(sw);
+	sw->ops->sysfs_write(sw, proc_num, port, num, buf);
+	sw->ops->release(sw);
+
+netlan_store_done:
+	up(proc_sem);
+	return ret;
+}
+
+static ssize_t netsw_show(struct device *d, struct device_attribute *attr,
+	char *buf, unsigned long offset)
+{
+	struct ksz_sw *sw;
+	struct semaphore *proc_sem;
+	ssize_t len = -EINVAL;
+	int num;
+	uint port;
+
+	if (attr->attr.name[1] != '_')
+		return len;
+	port = attr->attr.name[0] - '0';
+	if (port >= TOTAL_PORT_NUM)
+		return len;
+
+	get_private_data(d, &proc_sem, &sw, NULL);
+	if (down_interruptible(proc_sem))
+		return -ERESTARTSYS;
+
+	len = 0;
+	num = offset / sizeof(int);
+	len = sw->ops->sysfs_port_read(sw, num, port, len, buf);
+	if (len)
+		goto netsw_show_done;
+
+	len = sw->ops->sysfs_acl_read(sw, num, port, len, buf);
+	if (len)
+		goto netsw_show_done;
+
+#ifdef CONFIG_KSZ_STP
+	len = sw->ops->sysfs_stp_port_read(sw, num, port, len, buf);
+	if (len)
+		goto netsw_show_done;
+#endif
+
+	/* Require hardware to be acquired first. */
+	sw->ops->acquire(sw);
+	len = sw->ops->sysfs_port_read_hw(sw, num, port, len, buf);
+	sw->ops->release(sw);
+
+netsw_show_done:
+	up(proc_sem);
+	return len;
+}
+
+static ssize_t netsw_store(struct device *d, struct device_attribute *attr,
+	const char *buf, size_t count, unsigned long offset)
+{
+	struct ksz_sw *sw;
+	struct semaphore *proc_sem;
+	ssize_t ret = -EINVAL;
+	int num;
+	uint port;
+	int proc_num;
+
+	if (attr->attr.name[1] != '_')
+		return ret;
+	port = attr->attr.name[0] - '0';
+	if (port >= TOTAL_PORT_NUM)
+		return ret;
+	num = get_num_val(buf);
+	get_private_data(d, &proc_sem, &sw, NULL);
+	if (down_interruptible(proc_sem))
+		return -ERESTARTSYS;
+
+	proc_num = offset / sizeof(int);
+	ret = count;
+
+	if (sw->ops->sysfs_acl_write(sw, proc_num, port, num, buf))
+		goto netsw_store_done;
+
+#ifdef CONFIG_KSZ_STP
+	if (sw->ops->sysfs_stp_port_write(sw, proc_num, port, num, buf))
+		goto netsw_store_done;
+#endif
+
+	sw->ops->acquire(sw);
+	sw->ops->sysfs_port_write(sw, proc_num, port, num, buf);
+	sw->ops->release(sw);
+
+netsw_store_done:
+	up(proc_sem);
+	return ret;
+}
+
+#define LAN_ATTR(_name, _mode, _show, _store) \
+struct device_attribute lan_attr_##_name = \
+	__ATTR(_name, _mode, _show, _store)
+
+/* generate a read-only attribute */
+#define NETLAN_RD_ENTRY(name)						\
+static ssize_t show_lan_##name(struct device *d,			\
+	struct device_attribute *attr, char *buf)			\
+{									\
+	return netlan_show(d, attr, buf,				\
+		offsetof(struct lan_attributes, name));			\
+}									\
+static LAN_ATTR(name, S_IRUGO, show_lan_##name, NULL)
+
+/* generate a write-able attribute */
+#define NETLAN_WR_ENTRY(name)						\
+static ssize_t show_lan_##name(struct device *d,			\
+	struct device_attribute *attr, char *buf)			\
+{									\
+	return netlan_show(d, attr, buf,				\
+		offsetof(struct lan_attributes, name));			\
+}									\
+static ssize_t store_lan_##name(struct device *d,			\
+	struct device_attribute *attr, const char *buf, size_t count)	\
+{									\
+	return netlan_store(d, attr, buf, count,			\
+		offsetof(struct lan_attributes, name));			\
+}									\
+static LAN_ATTR(name, S_IRUGO | S_IWUSR, show_lan_##name, store_lan_##name)
+
+#define SW_ATTR(_name, _mode, _show, _store) \
+struct device_attribute sw_attr_##_name = \
+	__ATTR(0_##_name, _mode, _show, _store)
+
+/* generate a read-only attribute */
+#define NETSW_RD_ENTRY(name)						\
+static ssize_t show_sw_##name(struct device *d,				\
+	struct device_attribute *attr, char *buf)			\
+{									\
+	return netsw_show(d, attr, buf,					\
+		offsetof(struct sw_attributes, name));			\
+}									\
+static SW_ATTR(name, S_IRUGO, show_sw_##name, NULL)
+
+/* generate a write-able attribute */
+#define NETSW_WR_ENTRY(name)						\
+static ssize_t show_sw_##name(struct device *d,				\
+	struct device_attribute *attr, char *buf)			\
+{									\
+	return netsw_show(d, attr, buf,					\
+		offsetof(struct sw_attributes, name));			\
+}									\
+static ssize_t store_sw_##name(struct device *d,			\
+	struct device_attribute *attr, const char *buf, size_t count)	\
+{									\
+	return netsw_store(d, attr, buf, count,				\
+		offsetof(struct sw_attributes, name));			\
+}									\
+static SW_ATTR(name, S_IRUGO | S_IWUSR, show_sw_##name, store_sw_##name)
+
+NETLAN_WR_ENTRY(info);
+NETLAN_RD_ENTRY(version);
+NETLAN_WR_ENTRY(duplex);
+NETLAN_WR_ENTRY(speed);
+NETLAN_WR_ENTRY(force);
+NETLAN_WR_ENTRY(flow_ctrl);
+NETLAN_WR_ENTRY(mib);
+NETLAN_WR_ENTRY(reg);
+NETLAN_WR_ENTRY(vid);
+NETLAN_WR_ENTRY(features);
+NETLAN_WR_ENTRY(overrides);
+
+NETLAN_WR_ENTRY(dynamic_table);
+NETLAN_WR_ENTRY(static_table);
+NETLAN_RD_ENTRY(vlan_table);
+NETLAN_WR_ENTRY(aging);
+NETLAN_WR_ENTRY(fast_aging);
+NETLAN_WR_ENTRY(link_aging);
+NETLAN_WR_ENTRY(bcast_per);
+NETLAN_WR_ENTRY(mcast_storm);
+NETLAN_WR_ENTRY(tx_queue_based);
+NETLAN_WR_ENTRY(diffserv_map);
+NETLAN_WR_ENTRY(p_802_1p_map);
+NETLAN_WR_ENTRY(vlan);
+NETLAN_WR_ENTRY(null_vid);
+NETLAN_WR_ENTRY(macaddr);
+NETLAN_WR_ENTRY(mirror_mode);
+NETLAN_WR_ENTRY(tail_tag);
+NETLAN_WR_ENTRY(igmp_snoop);
+NETLAN_WR_ENTRY(ipv6_mld_snoop);
+NETLAN_WR_ENTRY(ipv6_mld_option);
+NETLAN_WR_ENTRY(aggr_backoff);
+NETLAN_WR_ENTRY(no_exc_drop);
+NETLAN_WR_ENTRY(huge_packet);
+NETLAN_WR_ENTRY(legal_packet);
+NETLAN_WR_ENTRY(length_check);
+NETLAN_WR_ENTRY(back_pressure);
+NETLAN_WR_ENTRY(sw_flow_ctrl);
+NETLAN_WR_ENTRY(sw_half_duplex);
+#ifdef SWITCH_10_MBIT
+NETLAN_WR_ENTRY(sw_10_mbit);
+#endif
+NETLAN_WR_ENTRY(rx_flow_ctrl);
+NETLAN_WR_ENTRY(tx_flow_ctrl);
+NETLAN_WR_ENTRY(fair_flow_ctrl);
+NETLAN_WR_ENTRY(vlan_bound);
+NETLAN_WR_ENTRY(fw_unk_ucast_dest);
+NETLAN_WR_ENTRY(fw_unk_ucast_ports);
+NETLAN_WR_ENTRY(fw_unk_mcast_dest);
+NETLAN_WR_ENTRY(fw_unk_mcast_ports);
+NETLAN_WR_ENTRY(fw_inv_vid);
+NETLAN_WR_ENTRY(fw_inv_vid_ports);
+NETLAN_WR_ENTRY(fw_unk_ip_mcast_dest);
+NETLAN_WR_ENTRY(fw_unk_ip_mcast_ports);
+NETLAN_WR_ENTRY(self_filter);
+NETLAN_WR_ENTRY(ins_tag);
+NETLAN_WR_ENTRY(pme);
+NETLAN_WR_ENTRY(pass_pause);
+NETLAN_WR_ENTRY(hi_prio_queues);
+NETLAN_RD_ENTRY(ports);
+NETLAN_RD_ENTRY(dev_start);
+NETLAN_RD_ENTRY(vlan_start);
+NETLAN_RD_ENTRY(stp);
+
+#ifdef CONFIG_KSZ_STP
+NETLAN_RD_ENTRY(stp_br_info);
+NETLAN_WR_ENTRY(stp_br_on);
+NETLAN_WR_ENTRY(stp_br_prio);
+NETLAN_WR_ENTRY(stp_br_fwd_delay);
+NETLAN_WR_ENTRY(stp_br_hello_time);
+NETLAN_WR_ENTRY(stp_br_max_age);
+NETLAN_WR_ENTRY(stp_br_tx_hold);
+NETLAN_WR_ENTRY(stp_version);
+#endif
+
+NETLAN_WR_ENTRY(mac_fid);
+NETLAN_WR_ENTRY(mac_use_fid);
+NETLAN_WR_ENTRY(mac_override);
+NETLAN_WR_ENTRY(mac_valid);
+NETLAN_WR_ENTRY(mac_ports);
+NETLAN_WR_ENTRY(mac_addr);
+NETLAN_WR_ENTRY(mac_index);
+NETLAN_RD_ENTRY(mac_info);
+
+NETLAN_WR_ENTRY(vlan_valid);
+NETLAN_WR_ENTRY(vlan_ports);
+NETLAN_WR_ENTRY(vlan_fid);
+NETLAN_WR_ENTRY(vlan_index);
+NETLAN_RD_ENTRY(vlan_info);
+
+NETSW_WR_ENTRY(mib);
+NETSW_WR_ENTRY(vid);
+NETSW_WR_ENTRY(member);
+NETSW_WR_ENTRY(bcast_storm);
+NETSW_WR_ENTRY(rx);
+NETSW_WR_ENTRY(tx);
+NETSW_WR_ENTRY(learn);
+NETSW_WR_ENTRY(mirror_port);
+NETSW_WR_ENTRY(mirror_rx);
+NETSW_WR_ENTRY(mirror_tx);
+NETSW_WR_ENTRY(diffserv);
+NETSW_WR_ENTRY(p_802_1p);
+NETSW_WR_ENTRY(port_based);
+NETSW_WR_ENTRY(non_vid);
+NETSW_WR_ENTRY(drop_tagged);
+NETSW_WR_ENTRY(ingress);
+NETSW_WR_ENTRY(ins_tag);
+NETSW_WR_ENTRY(rmv_tag);
+NETSW_WR_ENTRY(ins_tag_0);
+NETSW_WR_ENTRY(ins_tag_1);
+NETSW_WR_ENTRY(ins_tag_2);
+NETSW_WR_ENTRY(ins_tag_3);
+NETSW_WR_ENTRY(ins_tag_4);
+NETSW_WR_ENTRY(pass_all);
+NETSW_WR_ENTRY(replace_prio);
+NETSW_WR_ENTRY(prio_queue);
+NETSW_WR_ENTRY(tx_q0_ctrl);
+NETSW_WR_ENTRY(tx_q1_ctrl);
+NETSW_WR_ENTRY(tx_q2_ctrl);
+NETSW_WR_ENTRY(tx_q3_ctrl);
+NETSW_WR_ENTRY(tx_q0_ratio);
+NETSW_WR_ENTRY(tx_q1_ratio);
+NETSW_WR_ENTRY(tx_q2_ratio);
+NETSW_WR_ENTRY(tx_q3_ratio);
+NETSW_WR_ENTRY(rx_prio_rate);
+NETSW_WR_ENTRY(tx_prio_rate);
+NETSW_WR_ENTRY(rx_limit);
+NETSW_WR_ENTRY(rx_limit_port_based);
+NETSW_WR_ENTRY(limit_packet_based);
+NETSW_WR_ENTRY(rx_limit_flow_ctrl);
+NETSW_WR_ENTRY(cnt_ifg);
+NETSW_WR_ENTRY(cnt_pre);
+NETSW_WR_ENTRY(rx_p0_rate);
+NETSW_WR_ENTRY(rx_p1_rate);
+NETSW_WR_ENTRY(rx_p2_rate);
+NETSW_WR_ENTRY(rx_p3_rate);
+NETSW_WR_ENTRY(tx_q0_rate);
+NETSW_WR_ENTRY(tx_q1_rate);
+NETSW_WR_ENTRY(tx_q2_rate);
+NETSW_WR_ENTRY(tx_q3_rate);
+NETSW_WR_ENTRY(back_pressure);
+NETSW_WR_ENTRY(force_flow_ctrl);
+NETSW_WR_ENTRY(fw_unk_ucast_dest);
+NETSW_WR_ENTRY(fw_unk_mcast_dest);
+NETSW_WR_ENTRY(fw_inv_vid);
+NETSW_WR_ENTRY(fw_unk_ip_mcast_dest);
+NETSW_WR_ENTRY(pme_ctrl);
+NETSW_WR_ENTRY(pme_status);
+
+NETSW_RD_ENTRY(duplex);
+NETSW_RD_ENTRY(speed);
+NETSW_WR_ENTRY(linkmd);
+
+#ifdef CONFIG_KSZ_STP
+NETSW_RD_ENTRY(stp_info);
+NETSW_WR_ENTRY(stp_on);
+NETSW_WR_ENTRY(stp_prio);
+NETSW_WR_ENTRY(stp_admin_path_cost);
+NETSW_WR_ENTRY(stp_path_cost);
+NETSW_WR_ENTRY(stp_admin_edge);
+NETSW_WR_ENTRY(stp_auto_edge);
+NETSW_WR_ENTRY(stp_mcheck);
+NETSW_WR_ENTRY(stp_admin_p2p);
+#endif
+
+NETSW_WR_ENTRY(authen_mode);
+NETSW_WR_ENTRY(acl);
+NETSW_WR_ENTRY(acl_first_rule);
+NETSW_WR_ENTRY(acl_ruleset);
+NETSW_WR_ENTRY(acl_mode);
+NETSW_WR_ENTRY(acl_enable);
+NETSW_WR_ENTRY(acl_src);
+NETSW_WR_ENTRY(acl_equal);
+NETSW_WR_ENTRY(acl_addr);
+NETSW_WR_ENTRY(acl_type);
+NETSW_WR_ENTRY(acl_cnt);
+NETSW_WR_ENTRY(acl_msec);
+NETSW_WR_ENTRY(acl_intr_mode);
+NETSW_WR_ENTRY(acl_ip_addr);
+NETSW_WR_ENTRY(acl_ip_mask);
+NETSW_WR_ENTRY(acl_protocol);
+NETSW_WR_ENTRY(acl_seqnum);
+NETSW_WR_ENTRY(acl_port_mode);
+NETSW_WR_ENTRY(acl_max_port);
+NETSW_WR_ENTRY(acl_min_port);
+NETSW_WR_ENTRY(acl_tcp_flag_enable);
+NETSW_WR_ENTRY(acl_tcp_flag);
+NETSW_WR_ENTRY(acl_tcp_flag_mask);
+NETSW_WR_ENTRY(acl_prio_mode);
+NETSW_WR_ENTRY(acl_prio);
+NETSW_WR_ENTRY(acl_vlan_prio_replace);
+NETSW_WR_ENTRY(acl_vlan_prio);
+NETSW_WR_ENTRY(acl_map_mode);
+NETSW_WR_ENTRY(acl_ports);
+NETSW_WR_ENTRY(acl_index);
+NETSW_WR_ENTRY(acl_act_index);
+NETSW_WR_ENTRY(acl_act);
+NETSW_WR_ENTRY(acl_rule_index);
+NETSW_WR_ENTRY(acl_info);
+NETSW_RD_ENTRY(acl_table);
+
+static struct attribute *lan_attrs[] = {
+	&lan_attr_info.attr,
+	&lan_attr_version.attr,
+#ifdef USE_SPEED_LINK
+	&lan_attr_duplex.attr,
+	&lan_attr_speed.attr,
+	&lan_attr_force.attr,
+	&lan_attr_flow_ctrl.attr,
+#endif
+#ifdef USE_MIB
+	&lan_attr_mib.attr,
+#endif
+	&lan_attr_reg.attr,
+	&lan_attr_vid.attr,
+	&lan_attr_features.attr,
+	&lan_attr_overrides.attr,
+
+	&lan_attr_dynamic_table.attr,
+	&lan_attr_static_table.attr,
+	&lan_attr_vlan_table.attr,
+	&lan_attr_aging.attr,
+	&lan_attr_fast_aging.attr,
+	&lan_attr_link_aging.attr,
+	&lan_attr_bcast_per.attr,
+	&lan_attr_mcast_storm.attr,
+	&lan_attr_tx_queue_based.attr,
+	&lan_attr_diffserv_map.attr,
+	&lan_attr_p_802_1p_map.attr,
+	&lan_attr_vlan.attr,
+	&lan_attr_null_vid.attr,
+	&lan_attr_macaddr.attr,
+	&lan_attr_mirror_mode.attr,
+	&lan_attr_tail_tag.attr,
+	&lan_attr_igmp_snoop.attr,
+	&lan_attr_ipv6_mld_snoop.attr,
+	&lan_attr_ipv6_mld_option.attr,
+	&lan_attr_aggr_backoff.attr,
+	&lan_attr_no_exc_drop.attr,
+	&lan_attr_huge_packet.attr,
+	&lan_attr_legal_packet.attr,
+	&lan_attr_length_check.attr,
+	&lan_attr_back_pressure.attr,
+	&lan_attr_sw_flow_ctrl.attr,
+	&lan_attr_sw_half_duplex.attr,
+#ifdef SWITCH_10_MBIT
+	&lan_attr_sw_10_mbit.attr,
+#endif
+	&lan_attr_rx_flow_ctrl.attr,
+	&lan_attr_tx_flow_ctrl.attr,
+	&lan_attr_fair_flow_ctrl.attr,
+	&lan_attr_vlan_bound.attr,
+	&lan_attr_fw_unk_ucast_dest.attr,
+	&lan_attr_fw_unk_ucast_ports.attr,
+	&lan_attr_fw_unk_mcast_dest.attr,
+	&lan_attr_fw_unk_mcast_ports.attr,
+	&lan_attr_fw_inv_vid.attr,
+	&lan_attr_fw_inv_vid_ports.attr,
+	&lan_attr_fw_unk_ip_mcast_dest.attr,
+	&lan_attr_fw_unk_ip_mcast_ports.attr,
+	&lan_attr_self_filter.attr,
+	&lan_attr_ins_tag.attr,
+	&lan_attr_pme.attr,
+	&lan_attr_pass_pause.attr,
+	&lan_attr_hi_prio_queues.attr,
+	&lan_attr_ports.attr,
+	&lan_attr_dev_start.attr,
+	&lan_attr_vlan_start.attr,
+	&lan_attr_stp.attr,
+
+	&lan_attr_mac_fid.attr,
+	&lan_attr_mac_use_fid.attr,
+	&lan_attr_mac_override.attr,
+	&lan_attr_mac_valid.attr,
+	&lan_attr_mac_ports.attr,
+	&lan_attr_mac_addr.attr,
+	&lan_attr_mac_index.attr,
+	&lan_attr_mac_info.attr,
+
+	&lan_attr_vlan_valid.attr,
+	&lan_attr_vlan_ports.attr,
+	&lan_attr_vlan_fid.attr,
+	&lan_attr_vlan_index.attr,
+	&lan_attr_vlan_info.attr,
+
+#ifdef CONFIG_KSZ_STP
+	&lan_attr_stp_br_info.attr,
+	&lan_attr_stp_br_on.attr,
+	&lan_attr_stp_br_prio.attr,
+	&lan_attr_stp_br_fwd_delay.attr,
+	&lan_attr_stp_br_hello_time.attr,
+	&lan_attr_stp_br_max_age.attr,
+	&lan_attr_stp_br_tx_hold.attr,
+	&lan_attr_stp_version.attr,
+#endif
+
+	NULL
+};
+
+static struct attribute *sw_attrs[] = {
+	&sw_attr_vid.attr,
+	&sw_attr_member.attr,
+	&sw_attr_bcast_storm.attr,
+	&sw_attr_rx.attr,
+	&sw_attr_tx.attr,
+	&sw_attr_learn.attr,
+	&sw_attr_mirror_port.attr,
+	&sw_attr_mirror_rx.attr,
+	&sw_attr_mirror_tx.attr,
+	&sw_attr_diffserv.attr,
+	&sw_attr_p_802_1p.attr,
+	&sw_attr_port_based.attr,
+	&sw_attr_non_vid.attr,
+	&sw_attr_drop_tagged.attr,
+	&sw_attr_ingress.attr,
+	&sw_attr_ins_tag.attr,
+	&sw_attr_rmv_tag.attr,
+	&sw_attr_ins_tag_0.attr,
+	&sw_attr_ins_tag_1.attr,
+	&sw_attr_ins_tag_2.attr,
+	&sw_attr_ins_tag_3.attr,
+	&sw_attr_ins_tag_4.attr,
+	&sw_attr_pass_all.attr,
+	&sw_attr_replace_prio.attr,
+	&sw_attr_prio_queue.attr,
+	&sw_attr_tx_q0_ctrl.attr,
+	&sw_attr_tx_q1_ctrl.attr,
+	&sw_attr_tx_q2_ctrl.attr,
+	&sw_attr_tx_q3_ctrl.attr,
+	&sw_attr_tx_q0_ratio.attr,
+	&sw_attr_tx_q1_ratio.attr,
+	&sw_attr_tx_q2_ratio.attr,
+	&sw_attr_tx_q3_ratio.attr,
+	&sw_attr_rx_prio_rate.attr,
+	&sw_attr_tx_prio_rate.attr,
+	&sw_attr_rx_limit.attr,
+	&sw_attr_rx_limit_port_based.attr,
+	&sw_attr_limit_packet_based.attr,
+	&sw_attr_rx_limit_flow_ctrl.attr,
+	&sw_attr_cnt_ifg.attr,
+	&sw_attr_cnt_pre.attr,
+	&sw_attr_rx_p0_rate.attr,
+	&sw_attr_rx_p1_rate.attr,
+	&sw_attr_rx_p2_rate.attr,
+	&sw_attr_rx_p3_rate.attr,
+	&sw_attr_tx_q0_rate.attr,
+	&sw_attr_tx_q1_rate.attr,
+	&sw_attr_tx_q2_rate.attr,
+	&sw_attr_tx_q3_rate.attr,
+	&sw_attr_back_pressure.attr,
+	&sw_attr_force_flow_ctrl.attr,
+	&sw_attr_fw_unk_ucast_dest.attr,
+	&sw_attr_fw_unk_mcast_dest.attr,
+	&sw_attr_fw_inv_vid.attr,
+	&sw_attr_fw_unk_ip_mcast_dest.attr,
+	&sw_attr_mib.attr,
+	&sw_attr_pme_ctrl.attr,
+	&sw_attr_pme_status.attr,
+
+	&sw_attr_authen_mode.attr,
+	&sw_attr_acl.attr,
+	&sw_attr_acl_first_rule.attr,
+	&sw_attr_acl_ruleset.attr,
+	&sw_attr_acl_mode.attr,
+	&sw_attr_acl_enable.attr,
+	&sw_attr_acl_src.attr,
+	&sw_attr_acl_equal.attr,
+	&sw_attr_acl_addr.attr,
+	&sw_attr_acl_type.attr,
+	&sw_attr_acl_cnt.attr,
+	&sw_attr_acl_msec.attr,
+	&sw_attr_acl_intr_mode.attr,
+	&sw_attr_acl_ip_addr.attr,
+	&sw_attr_acl_ip_mask.attr,
+	&sw_attr_acl_protocol.attr,
+	&sw_attr_acl_seqnum.attr,
+	&sw_attr_acl_port_mode.attr,
+	&sw_attr_acl_max_port.attr,
+	&sw_attr_acl_min_port.attr,
+	&sw_attr_acl_tcp_flag_enable.attr,
+	&sw_attr_acl_tcp_flag.attr,
+	&sw_attr_acl_tcp_flag_mask.attr,
+	&sw_attr_acl_prio_mode.attr,
+	&sw_attr_acl_prio.attr,
+	&sw_attr_acl_vlan_prio_replace.attr,
+	&sw_attr_acl_vlan_prio.attr,
+	&sw_attr_acl_map_mode.attr,
+	&sw_attr_acl_ports.attr,
+	&sw_attr_acl_index.attr,
+	&sw_attr_acl_act_index.attr,
+	&sw_attr_acl_act.attr,
+	&sw_attr_acl_rule_index.attr,
+	&sw_attr_acl_info.attr,
+	&sw_attr_acl_table.attr,
+
+	&sw_attr_duplex.attr,
+	&sw_attr_speed.attr,
+	&sw_attr_linkmd.attr,
+
+#ifdef CONFIG_KSZ_STP
+	&sw_attr_stp_info.attr,
+	&sw_attr_stp_on.attr,
+	&sw_attr_stp_prio.attr,
+	&sw_attr_stp_admin_path_cost.attr,
+	&sw_attr_stp_path_cost.attr,
+	&sw_attr_stp_admin_edge.attr,
+	&sw_attr_stp_auto_edge.attr,
+	&sw_attr_stp_mcheck.attr,
+	&sw_attr_stp_admin_p2p.attr,
+#endif
+
+	NULL
+};
+
+static struct attribute_group lan_group = {
+	.name  = "sw",
+	.attrs  = lan_attrs,
+};
+
+static struct attribute_group sw_group = {
+	.name  = "sw0",
+	.attrs  = sw_attrs,
+};
+
+/* Kernel checking requires the attributes are in data segment. */
+#define SW_ATTRS_SIZE		(sizeof(sw_attrs) / sizeof(void *) - 1)
+
+#define MAX_SWITCHES		2
+
+static struct ksz_dev_attr ksz_sw_dev_attrs[(
+	SW_ATTRS_SIZE * TOTAL_PORT_NUM) * MAX_SWITCHES];
+static struct ksz_dev_attr *ksz_sw_dev_attrs_ptr = ksz_sw_dev_attrs;
+
+static void exit_sw_sysfs(struct ksz_sw *sw, struct ksz_sw_sysfs *info,
+	struct device *dev)
+{
+	int i;
+	uint n;
+
+	if (sw->overrides & SYSFS_PHY_PORT)
+		n = sw->port_cnt;
+	else
+		n = sw->mib_port_cnt + 1;
+	for (i = 0; i < n; i++) {
+		sw_group.name = sw_name[i];
+		sw_group.attrs = info->port_attrs[i];
+		sysfs_remove_group(&dev->kobj, &sw_group);
+		kfree(info->port_attrs[i]);
+		info->port_attrs[i] = NULL;
+		info->ksz_port_attrs[i] = NULL;
+	}
+	ksz_sw_dev_attrs_ptr = ksz_sw_dev_attrs;
+
+	sysfs_remove_group(&dev->kobj, &lan_group);
+}
+
+static int init_sw_sysfs(struct ksz_sw *sw, struct ksz_sw_sysfs *info,
+	struct device *dev)
+{
+	int err;
+	int i;
+	uint n;
+	uint p;
+	char *file;
+
+	err = sysfs_create_group(&dev->kobj, &lan_group);
+	if (err)
+		return err;
+	if (sw->overrides & SYSFS_PHY_PORT)
+		n = sw->port_cnt;
+	else
+		n = sw->mib_port_cnt + 1;
+	for (i = 0; i < n; i++) {
+		p = i;
+		if (!(sw->overrides & SYSFS_PHY_PORT))
+			p = sw->ops->get_phy_port(sw, i + 1);
+		file = NULL;
+		if (p == sw->HOST_PORT)
+			file = "0_linkmd";
+		err = alloc_dev_attr(sw_attrs,
+			sizeof(sw_attrs) / sizeof(void *), i,
+			&info->ksz_port_attrs[i], &info->port_attrs[i],
+			file, &ksz_sw_dev_attrs_ptr);
+		if (err)
+			return err;
+		sw_group.name = sw_name[i];
+		sw_group.attrs = info->port_attrs[i];
+		err = sysfs_create_group(&dev->kobj, &sw_group);
+		if (err)
+			return err;
+	}
+	return err;
+}
+
diff -Napur linux-linux4microchip-2022.10-orig/drivers/net/ethernet/micrel/ksz_sw_sysfs_8895.c linux-linux4microchip-2022.10/drivers/net/ethernet/micrel/ksz_sw_sysfs_8895.c
--- linux-linux4microchip-2022.10-orig/drivers/net/ethernet/micrel/ksz_sw_sysfs_8895.c	1969-12-31 16:00:00.000000000 -0800
+++ linux-linux4microchip-2022.10/drivers/net/ethernet/micrel/ksz_sw_sysfs_8895.c	2023-04-25 16:13:55.392163494 -0700
@@ -0,0 +1,659 @@
+/**
+ * Microchip KSZ8895 switch common sysfs code
+ *
+ * Copyright (c) 2015-2018 Microchip Technology Inc.
+ *	Tristram Ha <Tristram.Ha@microchip.com>
+ *
+ * Copyright (c) 2011-2015 Micrel, Inc.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ */
+
+
+#include "ksz_sysfs.h"
+
+
+static char *sw_name[TOTAL_PORT_NUM] = {
+	"sw0",
+	"sw1",
+	"sw2",
+	"sw3",
+	"sw4",
+};
+
+static ssize_t netlan_show(struct device *d, struct device_attribute *attr,
+	char *buf, unsigned long offset)
+{
+	struct ksz_port *port;
+	struct ksz_sw *sw;
+	struct semaphore *proc_sem;
+	ssize_t len = -EINVAL;
+	int proc_num;
+
+	get_private_data(d, &proc_sem, &sw, &port);
+	if (down_interruptible(proc_sem))
+		return -ERESTARTSYS;
+
+	len = 0;
+	proc_num = offset / sizeof(int);
+	len = sw->ops->sysfs_read(sw, proc_num, port, len, buf);
+	if (len)
+		goto netlan_show_done;
+
+	len = sw->ops->sysfs_mac_read(sw, proc_num, len, buf);
+	if (len)
+		goto netlan_show_done;
+
+	len = sw->ops->sysfs_vlan_read(sw, proc_num, len, buf);
+	if (len)
+		goto netlan_show_done;
+
+#ifdef CONFIG_KSZ_STP
+	len = sw->ops->sysfs_stp_read(sw, proc_num, len, buf);
+	if (len)
+		goto netlan_show_done;
+#endif
+
+	/* Require hardware to be acquired first. */
+	sw->ops->acquire(sw);
+	len = sw->ops->sysfs_read_hw(sw, proc_num, len, buf);
+	sw->ops->release(sw);
+
+netlan_show_done:
+	up(proc_sem);
+	return len;
+}
+
+static ssize_t netlan_store(struct device *d, struct device_attribute *attr,
+	const char *buf, size_t count, unsigned long offset)
+{
+	struct ksz_port *port;
+	struct ksz_sw *sw;
+	struct semaphore *proc_sem;
+	ssize_t ret = -EINVAL;
+	int num;
+	int proc_num;
+
+	num = get_num_val(buf);
+	get_private_data(d, &proc_sem, &sw, &port);
+	if (down_interruptible(proc_sem))
+		return -ERESTARTSYS;
+
+	proc_num = offset / sizeof(int);
+	ret = count;
+
+	if (sw->ops->sysfs_mac_write(sw, proc_num, num, buf))
+		goto netlan_store_done;
+
+	if (sw->ops->sysfs_vlan_write(sw, proc_num, num))
+		goto netlan_store_done;
+
+#ifdef CONFIG_KSZ_STP
+	if (sw->ops->sysfs_stp_write(sw, proc_num, num, buf))
+		goto netlan_store_done;
+#endif
+
+	sw->ops->acquire(sw);
+	sw->ops->sysfs_write(sw, proc_num, port, num, buf);
+	sw->ops->release(sw);
+
+netlan_store_done:
+	up(proc_sem);
+	return ret;
+}
+
+static ssize_t netsw_show(struct device *d, struct device_attribute *attr,
+	char *buf, unsigned long offset)
+{
+	struct ksz_sw *sw;
+	struct semaphore *proc_sem;
+	ssize_t len = -EINVAL;
+	int num;
+	uint port;
+
+	if (attr->attr.name[1] != '_')
+		return len;
+	port = attr->attr.name[0] - '0';
+	if (port >= TOTAL_PORT_NUM)
+		return len;
+
+	get_private_data(d, &proc_sem, &sw, NULL);
+	if (down_interruptible(proc_sem))
+		return -ERESTARTSYS;
+
+	len = 0;
+	num = offset / sizeof(int);
+	len = sw->ops->sysfs_port_read(sw, num, port, len, buf);
+	if (len)
+		goto netsw_show_done;
+
+#ifdef CONFIG_KSZ_STP
+	len = sw->ops->sysfs_stp_port_read(sw, num, port, len, buf);
+	if (len)
+		goto netsw_show_done;
+#endif
+
+	/* Require hardware to be acquired first. */
+	sw->ops->acquire(sw);
+	len = sw->ops->sysfs_port_read_hw(sw, num, port, len, buf);
+	sw->ops->release(sw);
+
+netsw_show_done:
+	up(proc_sem);
+	return len;
+}
+
+static ssize_t netsw_store(struct device *d, struct device_attribute *attr,
+	const char *buf, size_t count, unsigned long offset)
+{
+	struct ksz_sw *sw;
+	struct semaphore *proc_sem;
+	ssize_t ret = -EINVAL;
+	int num;
+	uint port;
+	int proc_num;
+
+	if (attr->attr.name[1] != '_')
+		return ret;
+	port = attr->attr.name[0] - '0';
+	if (port >= TOTAL_PORT_NUM)
+		return ret;
+	num = get_num_val(buf);
+	get_private_data(d, &proc_sem, &sw, NULL);
+	if (down_interruptible(proc_sem))
+		return -ERESTARTSYS;
+
+	proc_num = offset / sizeof(int);
+	ret = count;
+
+#ifdef CONFIG_KSZ_STP
+	if (sw->ops->sysfs_stp_port_write(sw, proc_num, port, num, buf))
+		goto netsw_store_done;
+#endif
+
+	sw->ops->acquire(sw);
+	sw->ops->sysfs_port_write(sw, proc_num, port, num, buf);
+	sw->ops->release(sw);
+
+#ifdef CONFIG_KSZ_STP
+netsw_store_done:
+#endif
+	up(proc_sem);
+	return ret;
+}
+
+#define LAN_ATTR(_name, _mode, _show, _store) \
+struct device_attribute lan_attr_##_name = \
+	__ATTR(_name, _mode, _show, _store)
+
+/* generate a read-only attribute */
+#define NETLAN_RD_ENTRY(name)						\
+static ssize_t show_lan_##name(struct device *d,			\
+	struct device_attribute *attr, char *buf)			\
+{									\
+	return netlan_show(d, attr, buf,				\
+		offsetof(struct lan_attributes, name));			\
+}									\
+static LAN_ATTR(name, S_IRUGO, show_lan_##name, NULL)
+
+/* generate a write-able attribute */
+#define NETLAN_WR_ENTRY(name)						\
+static ssize_t show_lan_##name(struct device *d,			\
+	struct device_attribute *attr, char *buf)			\
+{									\
+	return netlan_show(d, attr, buf,				\
+		offsetof(struct lan_attributes, name));			\
+}									\
+static ssize_t store_lan_##name(struct device *d,			\
+	struct device_attribute *attr, const char *buf, size_t count)	\
+{									\
+	return netlan_store(d, attr, buf, count,			\
+		offsetof(struct lan_attributes, name));			\
+}									\
+static LAN_ATTR(name, S_IRUGO | S_IWUSR, show_lan_##name, store_lan_##name)
+
+#define SW_ATTR(_name, _mode, _show, _store) \
+struct device_attribute sw_attr_##_name = \
+	__ATTR(0_##_name, _mode, _show, _store)
+
+/* generate a read-only attribute */
+#define NETSW_RD_ENTRY(name)						\
+static ssize_t show_sw_##name(struct device *d,				\
+	struct device_attribute *attr, char *buf)			\
+{									\
+	return netsw_show(d, attr, buf,					\
+		offsetof(struct sw_attributes, name));			\
+}									\
+static SW_ATTR(name, S_IRUGO, show_sw_##name, NULL)
+
+/* generate a write-able attribute */
+#define NETSW_WR_ENTRY(name)						\
+static ssize_t show_sw_##name(struct device *d,				\
+	struct device_attribute *attr, char *buf)			\
+{									\
+	return netsw_show(d, attr, buf,					\
+		offsetof(struct sw_attributes, name));			\
+}									\
+static ssize_t store_sw_##name(struct device *d,			\
+	struct device_attribute *attr, const char *buf, size_t count)	\
+{									\
+	return netsw_store(d, attr, buf, count,				\
+		offsetof(struct sw_attributes, name));			\
+}									\
+static SW_ATTR(name, S_IRUGO | S_IWUSR, show_sw_##name, store_sw_##name)
+
+NETLAN_WR_ENTRY(info);
+NETLAN_RD_ENTRY(version);
+NETLAN_WR_ENTRY(duplex);
+NETLAN_WR_ENTRY(speed);
+NETLAN_WR_ENTRY(force);
+NETLAN_WR_ENTRY(flow_ctrl);
+NETLAN_WR_ENTRY(mib);
+NETLAN_WR_ENTRY(reg);
+NETLAN_WR_ENTRY(vid);
+NETLAN_WR_ENTRY(features);
+NETLAN_WR_ENTRY(overrides);
+
+NETLAN_WR_ENTRY(dynamic_table);
+NETLAN_WR_ENTRY(static_table);
+NETLAN_RD_ENTRY(vlan_table);
+NETLAN_WR_ENTRY(aging);
+NETLAN_WR_ENTRY(fast_aging);
+NETLAN_WR_ENTRY(link_aging);
+NETLAN_WR_ENTRY(bcast_per);
+NETLAN_WR_ENTRY(mcast_storm);
+NETLAN_WR_ENTRY(tx_queue_based);
+NETLAN_WR_ENTRY(diffserv_map);
+NETLAN_WR_ENTRY(p_802_1p_map);
+NETLAN_WR_ENTRY(vlan);
+NETLAN_WR_ENTRY(null_vid);
+NETLAN_WR_ENTRY(macaddr);
+NETLAN_WR_ENTRY(mirror_mode);
+NETLAN_WR_ENTRY(tail_tag);
+NETLAN_WR_ENTRY(igmp_snoop);
+NETLAN_WR_ENTRY(aggr_backoff);
+NETLAN_WR_ENTRY(no_exc_drop);
+NETLAN_WR_ENTRY(huge_packet);
+NETLAN_WR_ENTRY(legal_packet);
+NETLAN_WR_ENTRY(length_check);
+NETLAN_WR_ENTRY(back_pressure);
+NETLAN_WR_ENTRY(sw_flow_ctrl);
+NETLAN_WR_ENTRY(sw_half_duplex);
+#ifdef SWITCH_10_MBIT
+NETLAN_WR_ENTRY(sw_10_mbit);
+#endif
+NETLAN_WR_ENTRY(rx_flow_ctrl);
+NETLAN_WR_ENTRY(tx_flow_ctrl);
+NETLAN_WR_ENTRY(fair_flow_ctrl);
+NETLAN_WR_ENTRY(vlan_bound);
+NETLAN_WR_ENTRY(fw_unk_ucast_dest);
+NETLAN_WR_ENTRY(fw_unk_ucast_ports);
+NETLAN_WR_ENTRY(fw_unk_mcast_dest);
+NETLAN_WR_ENTRY(fw_unk_mcast_ports);
+NETLAN_WR_ENTRY(fw_inv_vid);
+NETLAN_WR_ENTRY(fw_inv_vid_ports);
+NETLAN_WR_ENTRY(fw_unk_ip_mcast_dest);
+NETLAN_WR_ENTRY(fw_unk_ip_mcast_ports);
+NETLAN_WR_ENTRY(self_filter);
+NETLAN_WR_ENTRY(ins_tag);
+NETLAN_WR_ENTRY(pass_all);
+NETLAN_WR_ENTRY(pass_pause);
+NETLAN_WR_ENTRY(hi_prio_queues);
+NETLAN_RD_ENTRY(ports);
+NETLAN_RD_ENTRY(dev_start);
+NETLAN_RD_ENTRY(vlan_start);
+NETLAN_RD_ENTRY(stp);
+
+#ifdef CONFIG_KSZ_STP
+NETLAN_RD_ENTRY(stp_br_info);
+NETLAN_WR_ENTRY(stp_br_on);
+NETLAN_WR_ENTRY(stp_br_prio);
+NETLAN_WR_ENTRY(stp_br_fwd_delay);
+NETLAN_WR_ENTRY(stp_br_hello_time);
+NETLAN_WR_ENTRY(stp_br_max_age);
+NETLAN_WR_ENTRY(stp_br_tx_hold);
+NETLAN_WR_ENTRY(stp_version);
+#endif
+
+NETLAN_WR_ENTRY(mac_fid);
+NETLAN_WR_ENTRY(mac_use_fid);
+NETLAN_WR_ENTRY(mac_override);
+NETLAN_WR_ENTRY(mac_valid);
+NETLAN_WR_ENTRY(mac_ports);
+NETLAN_WR_ENTRY(mac_addr);
+NETLAN_WR_ENTRY(mac_index);
+NETLAN_RD_ENTRY(mac_info);
+
+NETLAN_WR_ENTRY(vlan_valid);
+NETLAN_WR_ENTRY(vlan_ports);
+NETLAN_WR_ENTRY(vlan_fid);
+NETLAN_WR_ENTRY(vlan_index);
+NETLAN_RD_ENTRY(vlan_info);
+
+NETSW_WR_ENTRY(mib);
+NETSW_WR_ENTRY(vid);
+NETSW_WR_ENTRY(member);
+NETSW_WR_ENTRY(bcast_storm);
+NETSW_WR_ENTRY(rx);
+NETSW_WR_ENTRY(tx);
+NETSW_WR_ENTRY(learn);
+NETSW_WR_ENTRY(mirror_port);
+NETSW_WR_ENTRY(mirror_rx);
+NETSW_WR_ENTRY(mirror_tx);
+NETSW_WR_ENTRY(diffserv);
+NETSW_WR_ENTRY(p_802_1p);
+NETSW_WR_ENTRY(port_based);
+NETSW_WR_ENTRY(non_vid);
+NETSW_WR_ENTRY(drop_tagged);
+NETSW_WR_ENTRY(ingress);
+NETSW_WR_ENTRY(ins_tag);
+NETSW_WR_ENTRY(rmv_tag);
+NETSW_WR_ENTRY(ins_tag_0);
+NETSW_WR_ENTRY(ins_tag_1);
+NETSW_WR_ENTRY(ins_tag_2);
+NETSW_WR_ENTRY(ins_tag_3);
+NETSW_WR_ENTRY(ins_tag_4);
+NETSW_WR_ENTRY(replace_prio);
+NETSW_WR_ENTRY(prio_queue);
+NETSW_WR_ENTRY(tx_q0_ctrl);
+NETSW_WR_ENTRY(tx_q1_ctrl);
+NETSW_WR_ENTRY(tx_q2_ctrl);
+NETSW_WR_ENTRY(tx_q3_ctrl);
+NETSW_WR_ENTRY(tx_q0_ratio);
+NETSW_WR_ENTRY(tx_q1_ratio);
+NETSW_WR_ENTRY(tx_q2_ratio);
+NETSW_WR_ENTRY(tx_q3_ratio);
+NETSW_WR_ENTRY(rx_prio_rate);
+NETSW_WR_ENTRY(tx_prio_rate);
+NETSW_WR_ENTRY(rx_limit);
+NETSW_WR_ENTRY(rx_limit_flow_ctrl);
+NETSW_WR_ENTRY(cnt_ifg);
+NETSW_WR_ENTRY(cnt_pre);
+NETSW_WR_ENTRY(rx_p0_rate);
+NETSW_WR_ENTRY(rx_p1_rate);
+NETSW_WR_ENTRY(rx_p2_rate);
+NETSW_WR_ENTRY(rx_p3_rate);
+NETSW_WR_ENTRY(tx_q0_rate);
+NETSW_WR_ENTRY(tx_q1_rate);
+NETSW_WR_ENTRY(tx_q2_rate);
+NETSW_WR_ENTRY(tx_q3_rate);
+NETSW_WR_ENTRY(back_pressure);
+NETSW_WR_ENTRY(force_flow_ctrl);
+NETSW_WR_ENTRY(fw_unk_ucast_dest);
+NETSW_WR_ENTRY(fw_unk_mcast_dest);
+NETSW_WR_ENTRY(fw_inv_vid);
+NETSW_WR_ENTRY(fw_unk_ip_mcast_dest);
+
+NETSW_RD_ENTRY(duplex);
+NETSW_RD_ENTRY(speed);
+NETSW_WR_ENTRY(linkmd);
+
+#ifdef CONFIG_KSZ_STP
+NETSW_RD_ENTRY(stp_info);
+NETSW_WR_ENTRY(stp_on);
+NETSW_WR_ENTRY(stp_prio);
+NETSW_WR_ENTRY(stp_admin_path_cost);
+NETSW_WR_ENTRY(stp_path_cost);
+NETSW_WR_ENTRY(stp_admin_edge);
+NETSW_WR_ENTRY(stp_auto_edge);
+NETSW_WR_ENTRY(stp_mcheck);
+NETSW_WR_ENTRY(stp_admin_p2p);
+#endif
+
+static struct attribute *lan_attrs[] = {
+	&lan_attr_info.attr,
+	&lan_attr_version.attr,
+#ifdef USE_SPEED_LINK
+	&lan_attr_duplex.attr,
+	&lan_attr_speed.attr,
+	&lan_attr_force.attr,
+	&lan_attr_flow_ctrl.attr,
+#endif
+#ifdef USE_MIB
+	&lan_attr_mib.attr,
+#endif
+	&lan_attr_reg.attr,
+	&lan_attr_vid.attr,
+	&lan_attr_features.attr,
+	&lan_attr_overrides.attr,
+
+	&lan_attr_dynamic_table.attr,
+	&lan_attr_static_table.attr,
+	&lan_attr_vlan_table.attr,
+	&lan_attr_aging.attr,
+	&lan_attr_fast_aging.attr,
+	&lan_attr_link_aging.attr,
+	&lan_attr_bcast_per.attr,
+	&lan_attr_mcast_storm.attr,
+	&lan_attr_tx_queue_based.attr,
+	&lan_attr_diffserv_map.attr,
+	&lan_attr_p_802_1p_map.attr,
+	&lan_attr_vlan.attr,
+	&lan_attr_null_vid.attr,
+	&lan_attr_macaddr.attr,
+	&lan_attr_mirror_mode.attr,
+	&lan_attr_tail_tag.attr,
+	&lan_attr_igmp_snoop.attr,
+	&lan_attr_aggr_backoff.attr,
+	&lan_attr_no_exc_drop.attr,
+	&lan_attr_huge_packet.attr,
+	&lan_attr_legal_packet.attr,
+	&lan_attr_length_check.attr,
+	&lan_attr_back_pressure.attr,
+	&lan_attr_sw_flow_ctrl.attr,
+	&lan_attr_sw_half_duplex.attr,
+#ifdef SWITCH_10_MBIT
+	&lan_attr_sw_10_mbit.attr,
+#endif
+	&lan_attr_rx_flow_ctrl.attr,
+	&lan_attr_tx_flow_ctrl.attr,
+	&lan_attr_fair_flow_ctrl.attr,
+	&lan_attr_vlan_bound.attr,
+	&lan_attr_fw_unk_ucast_dest.attr,
+	&lan_attr_fw_unk_ucast_ports.attr,
+	&lan_attr_fw_unk_mcast_dest.attr,
+	&lan_attr_fw_unk_mcast_ports.attr,
+	&lan_attr_fw_inv_vid.attr,
+	&lan_attr_fw_inv_vid_ports.attr,
+	&lan_attr_fw_unk_ip_mcast_dest.attr,
+	&lan_attr_fw_unk_ip_mcast_ports.attr,
+	&lan_attr_self_filter.attr,
+	&lan_attr_ins_tag.attr,
+	&lan_attr_pass_all.attr,
+	&lan_attr_pass_pause.attr,
+	&lan_attr_hi_prio_queues.attr,
+	&lan_attr_ports.attr,
+	&lan_attr_dev_start.attr,
+	&lan_attr_vlan_start.attr,
+	&lan_attr_stp.attr,
+
+	&lan_attr_mac_fid.attr,
+	&lan_attr_mac_use_fid.attr,
+	&lan_attr_mac_override.attr,
+	&lan_attr_mac_valid.attr,
+	&lan_attr_mac_ports.attr,
+	&lan_attr_mac_addr.attr,
+	&lan_attr_mac_index.attr,
+	&lan_attr_mac_info.attr,
+
+	&lan_attr_vlan_valid.attr,
+	&lan_attr_vlan_ports.attr,
+	&lan_attr_vlan_fid.attr,
+	&lan_attr_vlan_index.attr,
+	&lan_attr_vlan_info.attr,
+
+#ifdef CONFIG_KSZ_STP
+	&lan_attr_stp_br_info.attr,
+	&lan_attr_stp_br_on.attr,
+	&lan_attr_stp_br_prio.attr,
+	&lan_attr_stp_br_fwd_delay.attr,
+	&lan_attr_stp_br_hello_time.attr,
+	&lan_attr_stp_br_max_age.attr,
+	&lan_attr_stp_br_tx_hold.attr,
+	&lan_attr_stp_version.attr,
+#endif
+
+	NULL
+};
+
+static struct attribute *sw_attrs[] = {
+	&sw_attr_vid.attr,
+	&sw_attr_member.attr,
+	&sw_attr_bcast_storm.attr,
+	&sw_attr_rx.attr,
+	&sw_attr_tx.attr,
+	&sw_attr_learn.attr,
+	&sw_attr_mirror_port.attr,
+	&sw_attr_mirror_rx.attr,
+	&sw_attr_mirror_tx.attr,
+	&sw_attr_diffserv.attr,
+	&sw_attr_p_802_1p.attr,
+	&sw_attr_port_based.attr,
+	&sw_attr_non_vid.attr,
+	&sw_attr_drop_tagged.attr,
+	&sw_attr_ingress.attr,
+	&sw_attr_ins_tag.attr,
+	&sw_attr_rmv_tag.attr,
+	&sw_attr_ins_tag_0.attr,
+	&sw_attr_ins_tag_1.attr,
+	&sw_attr_ins_tag_2.attr,
+	&sw_attr_ins_tag_3.attr,
+	&sw_attr_ins_tag_4.attr,
+	&sw_attr_replace_prio.attr,
+	&sw_attr_prio_queue.attr,
+	&sw_attr_tx_q0_ctrl.attr,
+	&sw_attr_tx_q1_ctrl.attr,
+	&sw_attr_tx_q2_ctrl.attr,
+	&sw_attr_tx_q3_ctrl.attr,
+	&sw_attr_tx_q0_ratio.attr,
+	&sw_attr_tx_q1_ratio.attr,
+	&sw_attr_tx_q2_ratio.attr,
+	&sw_attr_tx_q3_ratio.attr,
+	&sw_attr_rx_prio_rate.attr,
+	&sw_attr_tx_prio_rate.attr,
+	&sw_attr_rx_limit.attr,
+	&sw_attr_rx_limit_flow_ctrl.attr,
+	&sw_attr_cnt_ifg.attr,
+	&sw_attr_cnt_pre.attr,
+	&sw_attr_rx_p0_rate.attr,
+	&sw_attr_rx_p1_rate.attr,
+	&sw_attr_rx_p2_rate.attr,
+	&sw_attr_rx_p3_rate.attr,
+	&sw_attr_tx_q0_rate.attr,
+	&sw_attr_tx_q1_rate.attr,
+	&sw_attr_tx_q2_rate.attr,
+	&sw_attr_tx_q3_rate.attr,
+	&sw_attr_back_pressure.attr,
+	&sw_attr_force_flow_ctrl.attr,
+	&sw_attr_fw_unk_ucast_dest.attr,
+	&sw_attr_fw_unk_mcast_dest.attr,
+	&sw_attr_fw_inv_vid.attr,
+	&sw_attr_fw_unk_ip_mcast_dest.attr,
+	&sw_attr_mib.attr,
+
+	&sw_attr_duplex.attr,
+	&sw_attr_speed.attr,
+	&sw_attr_linkmd.attr,
+
+#ifdef CONFIG_KSZ_STP
+	&sw_attr_stp_info.attr,
+	&sw_attr_stp_on.attr,
+	&sw_attr_stp_prio.attr,
+	&sw_attr_stp_admin_path_cost.attr,
+	&sw_attr_stp_path_cost.attr,
+	&sw_attr_stp_admin_edge.attr,
+	&sw_attr_stp_auto_edge.attr,
+	&sw_attr_stp_mcheck.attr,
+	&sw_attr_stp_admin_p2p.attr,
+#endif
+
+	NULL
+};
+
+static struct attribute_group lan_group = {
+	.name  = "sw",
+	.attrs  = lan_attrs,
+};
+
+static struct attribute_group sw_group = {
+	.name  = "sw0",
+	.attrs  = sw_attrs,
+};
+
+/* Kernel checking requires the attributes are in data segment. */
+#define SW_ATTRS_SIZE		(sizeof(sw_attrs) / sizeof(void *) - 1)
+
+#define MAX_SWITCHES		2
+
+static struct ksz_dev_attr ksz_sw_dev_attrs[(
+	SW_ATTRS_SIZE * TOTAL_PORT_NUM) * MAX_SWITCHES];
+static struct ksz_dev_attr *ksz_sw_dev_attrs_ptr = ksz_sw_dev_attrs;
+
+static void exit_sw_sysfs(struct ksz_sw *sw, struct ksz_sw_sysfs *info,
+	struct device *dev)
+{
+	int i;
+	uint n;
+
+	if (sw->overrides & SYSFS_PHY_PORT)
+		n = sw->port_cnt;
+	else
+		n = sw->mib_port_cnt + 1;
+	for (i = 0; i < n; i++) {
+		sw_group.name = sw_name[i];
+		sw_group.attrs = info->port_attrs[i];
+		sysfs_remove_group(&dev->kobj, &sw_group);
+		kfree(info->port_attrs[i]);
+		info->port_attrs[i] = NULL;
+		info->ksz_port_attrs[i] = NULL;
+	}
+	ksz_sw_dev_attrs_ptr = ksz_sw_dev_attrs;
+
+	sysfs_remove_group(&dev->kobj, &lan_group);
+}
+
+static int init_sw_sysfs(struct ksz_sw *sw, struct ksz_sw_sysfs *info,
+	struct device *dev)
+{
+	int err;
+	int i;
+	uint n;
+	uint p;
+	char *file;
+
+	err = sysfs_create_group(&dev->kobj, &lan_group);
+	if (err)
+		return err;
+	if (sw->overrides & SYSFS_PHY_PORT)
+		n = sw->port_cnt;
+	else
+		n = sw->mib_port_cnt + 1;
+	for (i = 0; i < n; i++) {
+		p = i;
+		if (!(sw->overrides & SYSFS_PHY_PORT))
+			p = sw->ops->get_phy_port(sw, i + 1);
+		file = NULL;
+		if (p == sw->HOST_PORT)
+			file = "0_linkmd";
+		err = alloc_dev_attr(sw_attrs,
+			sizeof(sw_attrs) / sizeof(void *), i,
+			&info->ksz_port_attrs[i], &info->port_attrs[i],
+			file, &ksz_sw_dev_attrs_ptr);
+		if (err)
+			return err;
+		sw_group.name = sw_name[i];
+		sw_group.attrs = info->port_attrs[i];
+		err = sysfs_create_group(&dev->kobj, &sw_group);
+		if (err)
+			return err;
+	}
+	return err;
+}
+
diff -Napur linux-linux4microchip-2022.10-orig/drivers/net/ethernet/micrel/ksz_sw_sysfs_9897.c linux-linux4microchip-2022.10/drivers/net/ethernet/micrel/ksz_sw_sysfs_9897.c
--- linux-linux4microchip-2022.10-orig/drivers/net/ethernet/micrel/ksz_sw_sysfs_9897.c	1969-12-31 16:00:00.000000000 -0800
+++ linux-linux4microchip-2022.10/drivers/net/ethernet/micrel/ksz_sw_sysfs_9897.c	2023-04-25 16:13:55.392163494 -0700
@@ -0,0 +1,1126 @@
+/**
+ * Microchip gigabit switch common sysfs code
+ *
+ * Copyright (c) 2015-2019 Microchip Technology Inc.
+ *	Tristram Ha <Tristram.Ha@microchip.com>
+ *
+ * Copyright (c) 2011-2014 Micrel, Inc.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ */
+
+
+#include "ksz_sysfs.h"
+
+
+static char *sw_name[] = {
+	"sw0",
+	"sw1",
+	"sw2",
+	"sw3",
+	"sw4",
+	"sw5",
+	"sw6",
+	"sw7",
+};
+
+static ssize_t netlan_show(struct device *d, struct device_attribute *attr,
+	char *buf, unsigned long offset)
+{
+	struct ksz_port *port;
+	struct ksz_sw *sw;
+	struct semaphore *proc_sem;
+	ssize_t len = -EINVAL;
+	int proc_num;
+
+	get_private_data(d, &proc_sem, &sw, &port);
+	if (down_interruptible(proc_sem))
+		return -ERESTARTSYS;
+
+	len = 0;
+	proc_num = offset / sizeof(int);
+	len = sw->ops->sysfs_read(sw, proc_num, port, len, buf);
+	if (len)
+		goto netlan_show_done;
+
+	len = sw->ops->sysfs_mac_read(sw, proc_num, len, buf);
+	if (len)
+		goto netlan_show_done;
+
+	len = sw->ops->sysfs_vlan_read(sw, proc_num, len, buf);
+	if (len)
+		goto netlan_show_done;
+
+#ifdef CONFIG_KSZ_STP
+	len = sw->ops->sysfs_stp_read(sw, proc_num, len, buf);
+	if (len)
+		goto netlan_show_done;
+#endif
+
+#ifdef CONFIG_KSZ_AVB
+	len = sw->ops->sysfs_mrp_read(sw, proc_num, len, buf);
+	if (len)
+		goto netlan_show_done;
+#endif
+
+#ifdef CONFIG_KSZ_HSR
+	len = sw->ops->sysfs_hsr_read(sw, proc_num, len, buf);
+	if (len)
+		goto netlan_show_done;
+#endif
+
+	/* Require hardware to be acquired first. */
+	sw->ops->acquire(sw);
+	len = sw->ops->sysfs_read_hw(sw, proc_num, len, buf);
+	sw->ops->release(sw);
+
+netlan_show_done:
+	up(proc_sem);
+	return len;
+}
+
+static ssize_t netlan_store(struct device *d, struct device_attribute *attr,
+	const char *buf, size_t count, unsigned long offset)
+{
+	struct ksz_port *port;
+	struct ksz_sw *sw;
+	struct semaphore *proc_sem;
+	ssize_t ret = -EINVAL;
+	int num;
+	int proc_num;
+
+	num = get_num_val(buf);
+	get_private_data(d, &proc_sem, &sw, &port);
+	if (down_interruptible(proc_sem))
+		return -ERESTARTSYS;
+
+	proc_num = offset / sizeof(int);
+	ret = count;
+
+	if (sw->ops->sysfs_mac_write(sw, proc_num, num, buf))
+		goto netlan_store_done;
+
+	if (sw->ops->sysfs_vlan_write(sw, proc_num, num))
+		goto netlan_store_done;
+
+#ifdef CONFIG_KSZ_STP
+	if (sw->ops->sysfs_stp_write(sw, proc_num, num, buf))
+		goto netlan_store_done;
+#endif
+
+#ifdef CONFIG_KSZ_AVB
+	if (sw->ops->sysfs_mrp_write(sw, proc_num, num, buf))
+		goto netlan_store_done;
+#endif
+
+#ifdef CONFIG_KSZ_HSR
+	if (sw->ops->sysfs_hsr_write(sw, proc_num, num, buf))
+		goto netlan_store_done;
+#endif
+
+	sw->ops->acquire(sw);
+	sw->ops->sysfs_write(sw, proc_num, port, num, buf);
+	sw->ops->release(sw);
+
+netlan_store_done:
+	up(proc_sem);
+	return ret;
+}
+
+static ssize_t netsw_show(struct device *d, struct device_attribute *attr,
+	char *buf, unsigned long offset)
+{
+	struct ksz_sw *sw;
+	struct semaphore *proc_sem;
+	ssize_t len = -EINVAL;
+	int cnt;
+	int num;
+	uint port;
+
+	if (attr->attr.name[1] != '_')
+		return len;
+	port = attr->attr.name[0] - '0';
+
+	get_private_data(d, &proc_sem, &sw, NULL);
+	cnt = TOTAL_PORT_NUM;
+	if (sw->overrides & SYSFS_1_BASE)
+		cnt++;
+	if (port >= cnt)
+		return len;
+	if (down_interruptible(proc_sem))
+		return -ERESTARTSYS;
+
+	len = 0;
+	num = offset / sizeof(int);
+	len = sw->ops->sysfs_port_read(sw, num, port, len, buf);
+	if (len)
+		goto netsw_show_done;
+
+	len = sw->ops->sysfs_acl_read(sw, num, port, len, buf);
+	if (len)
+		goto netsw_show_done;
+
+#ifdef CONFIG_KSZ_STP
+	len = sw->ops->sysfs_stp_port_read(sw, num, port, len, buf);
+	if (len)
+		goto netsw_show_done;
+#endif
+
+#ifdef CONFIG_KSZ_AVB
+	len = sw->ops->sysfs_mrp_port_read(sw, num, port, len, buf);
+	if (len)
+		goto netsw_show_done;
+#endif
+
+	/* Require hardware to be acquired first. */
+	sw->ops->acquire(sw);
+	len = sw->ops->sysfs_port_read_hw(sw, num, port, len, buf);
+	sw->ops->release(sw);
+
+netsw_show_done:
+	up(proc_sem);
+	return len;
+}
+
+static ssize_t netsw_store(struct device *d, struct device_attribute *attr,
+	const char *buf, size_t count, unsigned long offset)
+{
+	struct ksz_sw *sw;
+	struct semaphore *proc_sem;
+	ssize_t ret = -EINVAL;
+	int cnt;
+	int num;
+	uint port;
+	int proc_num;
+
+	if (attr->attr.name[1] != '_')
+		return ret;
+	port = attr->attr.name[0] - '0';
+	num = get_num_val(buf);
+	get_private_data(d, &proc_sem, &sw, NULL);
+	cnt = TOTAL_PORT_NUM;
+	if (sw->overrides & SYSFS_1_BASE)
+		cnt++;
+	if (port >= cnt)
+		return ret;
+	if (down_interruptible(proc_sem))
+		return -ERESTARTSYS;
+
+	proc_num = offset / sizeof(int);
+	ret = count;
+
+	if (sw->ops->sysfs_acl_write(sw, proc_num, port, num, buf))
+		goto netsw_store_done;
+
+#ifdef CONFIG_KSZ_STP
+	if (sw->ops->sysfs_stp_port_write(sw, proc_num, port, num, buf))
+		goto netsw_store_done;
+#endif
+
+#ifdef CONFIG_KSZ_AVB
+	if (sw->ops->sysfs_mrp_port_write(sw, proc_num, port, num, buf))
+		goto netsw_store_done;
+#endif
+
+	sw->ops->acquire(sw);
+	sw->ops->sysfs_port_write(sw, proc_num, port, num, buf);
+	sw->ops->release(sw);
+
+netsw_store_done:
+	up(proc_sem);
+	return ret;
+}
+
+#define LAN_ATTR(_name, _mode, _show, _store) \
+struct device_attribute lan_attr_##_name = \
+	__ATTR(_name, _mode, _show, _store)
+
+/* generate a read-only attribute */
+#define NETLAN_RD_ENTRY(name)						\
+static ssize_t show_lan_##name(struct device *d,			\
+	struct device_attribute *attr, char *buf)			\
+{									\
+	return netlan_show(d, attr, buf,				\
+		offsetof(struct lan_attributes, name));			\
+}									\
+static LAN_ATTR(name, S_IRUGO, show_lan_##name, NULL)
+
+/* generate a write-able attribute */
+#define NETLAN_WR_ENTRY(name)						\
+static ssize_t show_lan_##name(struct device *d,			\
+	struct device_attribute *attr, char *buf)			\
+{									\
+	return netlan_show(d, attr, buf,				\
+		offsetof(struct lan_attributes, name));			\
+}									\
+static ssize_t store_lan_##name(struct device *d,			\
+	struct device_attribute *attr, const char *buf, size_t count)	\
+{									\
+	return netlan_store(d, attr, buf, count,			\
+		offsetof(struct lan_attributes, name));			\
+}									\
+static LAN_ATTR(name, S_IRUGO | S_IWUSR, show_lan_##name, store_lan_##name)
+
+#define SW_ATTR(_name, _mode, _show, _store) \
+struct device_attribute sw_attr_##_name = \
+	__ATTR(0_##_name, _mode, _show, _store)
+
+/* generate a read-only attribute */
+#define NETSW_RD_ENTRY(name)						\
+static ssize_t show_sw_##name(struct device *d,				\
+	struct device_attribute *attr, char *buf)			\
+{									\
+	return netsw_show(d, attr, buf,					\
+		offsetof(struct sw_attributes, name));			\
+}									\
+static SW_ATTR(name, S_IRUGO, show_sw_##name, NULL)
+
+/* generate a write-able attribute */
+#define NETSW_WR_ENTRY(name)						\
+static ssize_t show_sw_##name(struct device *d,				\
+	struct device_attribute *attr, char *buf)			\
+{									\
+	return netsw_show(d, attr, buf,					\
+		offsetof(struct sw_attributes, name));			\
+}									\
+static ssize_t store_sw_##name(struct device *d,			\
+	struct device_attribute *attr, const char *buf, size_t count)	\
+{									\
+	return netsw_store(d, attr, buf, count,				\
+		offsetof(struct sw_attributes, name));			\
+}									\
+static SW_ATTR(name, S_IRUGO | S_IWUSR, show_sw_##name, store_sw_##name)
+
+NETLAN_WR_ENTRY(info);
+NETLAN_RD_ENTRY(version);
+NETLAN_WR_ENTRY(duplex);
+NETLAN_WR_ENTRY(speed);
+NETLAN_WR_ENTRY(force);
+NETLAN_WR_ENTRY(flow_ctrl);
+NETLAN_WR_ENTRY(mib);
+NETLAN_WR_ENTRY(reg);
+NETLAN_WR_ENTRY(vid);
+NETLAN_WR_ENTRY(features);
+NETLAN_WR_ENTRY(overrides);
+
+NETLAN_WR_ENTRY(dynamic_table);
+NETLAN_WR_ENTRY(static_table);
+NETLAN_RD_ENTRY(vlan_table);
+NETLAN_RD_ENTRY(hsr_table);
+NETLAN_WR_ENTRY(aging);
+NETLAN_WR_ENTRY(fast_aging);
+NETLAN_WR_ENTRY(link_aging);
+NETLAN_WR_ENTRY(bcast_per);
+NETLAN_WR_ENTRY(mcast_storm);
+NETLAN_WR_ENTRY(tx_queue_based);
+NETLAN_WR_ENTRY(diffserv_map);
+NETLAN_WR_ENTRY(p_802_1p_map);
+NETLAN_WR_ENTRY(vlan);
+NETLAN_WR_ENTRY(null_vid);
+NETLAN_WR_ENTRY(drop_inv_vid);
+NETLAN_WR_ENTRY(macaddr);
+NETLAN_WR_ENTRY(mirror_mode);
+NETLAN_WR_ENTRY(igmp_snoop);
+NETLAN_WR_ENTRY(ipv6_mld_snoop);
+NETLAN_WR_ENTRY(ipv6_mld_option);
+NETLAN_WR_ENTRY(aggr_backoff);
+NETLAN_WR_ENTRY(no_exc_drop);
+NETLAN_WR_ENTRY(jumbo_packet);
+NETLAN_WR_ENTRY(legal_packet);
+NETLAN_WR_ENTRY(length_check);
+NETLAN_WR_ENTRY(back_pressure);
+NETLAN_WR_ENTRY(sw_flow_ctrl);
+NETLAN_WR_ENTRY(sw_half_duplex);
+#ifdef SWITCH_10_MBIT
+NETLAN_WR_ENTRY(sw_10_mbit);
+#endif
+NETLAN_WR_ENTRY(fair_flow_ctrl);
+NETLAN_WR_ENTRY(vlan_bound);
+NETLAN_WR_ENTRY(double_tag);
+NETLAN_WR_ENTRY(isp);
+NETLAN_WR_ENTRY(hsr);
+NETLAN_WR_ENTRY(hsr_redbox_id);
+NETLAN_WR_ENTRY(hsr_net_id);
+NETLAN_WR_ENTRY(mtu);
+NETLAN_WR_ENTRY(unk_ucast_fwd);
+NETLAN_WR_ENTRY(unk_ucast_ports);
+NETLAN_WR_ENTRY(unk_mcast_fwd);
+NETLAN_WR_ENTRY(unk_mcast_ports);
+NETLAN_WR_ENTRY(unk_vid_fwd);
+NETLAN_WR_ENTRY(unk_vid_ports);
+NETLAN_WR_ENTRY(pass_pause);
+NETLAN_WR_ENTRY(pme);
+NETLAN_WR_ENTRY(pme_polarity);
+
+NETLAN_RD_ENTRY(host_port);
+NETLAN_RD_ENTRY(ports);
+NETLAN_RD_ENTRY(dev_start);
+NETLAN_RD_ENTRY(port_start);
+NETLAN_RD_ENTRY(vlan_start);
+NETLAN_RD_ENTRY(avb);
+NETLAN_RD_ENTRY(stp);
+NETLAN_RD_ENTRY(two_dev);
+NETLAN_WR_ENTRY(authen);
+
+NETLAN_WR_ENTRY(alu_fid);
+NETLAN_WR_ENTRY(alu_use_fid);
+NETLAN_WR_ENTRY(alu_override);
+NETLAN_WR_ENTRY(alu_valid);
+NETLAN_WR_ENTRY(alu_mstp);
+NETLAN_WR_ENTRY(alu_prio);
+NETLAN_WR_ENTRY(alu_src);
+NETLAN_WR_ENTRY(alu_dst);
+NETLAN_WR_ENTRY(alu_ports);
+NETLAN_WR_ENTRY(alu_addr);
+NETLAN_WR_ENTRY(alu_type);
+NETLAN_WR_ENTRY(alu_index);
+NETLAN_WR_ENTRY(alu_info);
+
+NETLAN_WR_ENTRY(vlan_valid);
+NETLAN_WR_ENTRY(vlan_ports);
+NETLAN_WR_ENTRY(vlan_untag);
+NETLAN_WR_ENTRY(vlan_fid);
+NETLAN_WR_ENTRY(vlan_mstp);
+NETLAN_WR_ENTRY(vlan_prio);
+NETLAN_WR_ENTRY(vlan_option);
+NETLAN_WR_ENTRY(vlan_index);
+NETLAN_WR_ENTRY(vlan_info);
+NETLAN_WR_ENTRY(vid2fid);
+NETLAN_WR_ENTRY(fid2mstid);
+
+#ifdef CONFIG_KSZ_STP
+NETLAN_RD_ENTRY(stp_br_info);
+NETLAN_WR_ENTRY(stp_br_on);
+NETLAN_WR_ENTRY(stp_br_prio);
+NETLAN_WR_ENTRY(stp_br_fwd_delay);
+NETLAN_WR_ENTRY(stp_br_hello_time);
+NETLAN_WR_ENTRY(stp_br_max_age);
+NETLAN_WR_ENTRY(stp_br_tx_hold);
+NETLAN_WR_ENTRY(stp_version);
+#ifdef CONFIG_KSZ_MSTP
+NETLAN_WR_ENTRY(stp_br_max_hops);
+NETLAN_WR_ENTRY(stp_msti);
+NETLAN_RD_ENTRY(stp_msti_vid);
+NETLAN_RD_ENTRY(stp_mstp_cfg);
+NETLAN_WR_ENTRY(stp_mstp_name);
+#endif
+#endif
+
+#ifdef CONFIG_KSZ_MSRP
+NETLAN_WR_ENTRY(msrp_info);
+NETLAN_WR_ENTRY(msrpEnabled);
+#endif
+
+#ifdef CONFIG_KSZ_AVB
+NETLAN_WR_ENTRY(msrp_sr_a);
+#endif
+
+#ifdef CONFIG_KSZ_HSR
+NETLAN_WR_ENTRY(hsr_valid);
+NETLAN_WR_ENTRY(hsr_age_cnt);
+NETLAN_WR_ENTRY(hsr_path_id);
+NETLAN_WR_ENTRY(hsr_addr);
+NETLAN_WR_ENTRY(hsr_index);
+NETLAN_WR_ENTRY(hsr_info);
+NETLAN_RD_ENTRY(hsr_state);
+#endif
+
+NETLAN_WR_ENTRY(no_color);
+NETLAN_WR_ENTRY(color_red);
+NETLAN_WR_ENTRY(color_yellow);
+NETLAN_WR_ENTRY(color_green);
+NETLAN_WR_ENTRY(vlan_filter_dynamic);
+NETLAN_WR_ENTRY(vlan_filter_static);
+
+NETSW_WR_ENTRY(mib);
+NETSW_WR_ENTRY(vid);
+NETSW_WR_ENTRY(member);
+NETSW_WR_ENTRY(bcast_storm);
+NETSW_WR_ENTRY(mstp);
+NETSW_WR_ENTRY(rx);
+NETSW_WR_ENTRY(tx);
+NETSW_WR_ENTRY(learn);
+NETSW_WR_ENTRY(power);
+NETSW_WR_ENTRY(mirror_port);
+NETSW_WR_ENTRY(mirror_rx);
+NETSW_WR_ENTRY(mirror_tx);
+NETSW_WR_ENTRY(diffserv);
+NETSW_WR_ENTRY(p_802_1p);
+NETSW_WR_ENTRY(prio_vlan);
+NETSW_WR_ENTRY(prio_mac);
+NETSW_WR_ENTRY(prio_acl);
+NETSW_WR_ENTRY(prio_highest);
+NETSW_WR_ENTRY(prio_or);
+NETSW_WR_ENTRY(port_prio);
+NETSW_WR_ENTRY(non_vid);
+NETSW_WR_ENTRY(drop_non_vlan);
+NETSW_WR_ENTRY(drop_tagged);
+NETSW_WR_ENTRY(ingress);
+NETSW_WR_ENTRY(replace_vid);
+NETSW_WR_ENTRY(replace_prio);
+NETSW_WR_ENTRY(mac_802_1x);
+NETSW_WR_ENTRY(src_addr_filter);
+NETSW_WR_ENTRY(vlan_lookup_0);
+NETSW_WR_ENTRY(prio_queue);
+NETSW_WR_ENTRY(rx_prio_rate);
+NETSW_WR_ENTRY(tx_prio_rate);
+NETSW_WR_ENTRY(limit);
+NETSW_WR_ENTRY(limit_port_based);
+NETSW_WR_ENTRY(limit_packet_based);
+NETSW_WR_ENTRY(limit_flow_ctrl);
+NETSW_WR_ENTRY(limit_cnt_ifg);
+NETSW_WR_ENTRY(limit_cnt_pre);
+NETSW_WR_ENTRY(rx_p0_rate);
+NETSW_WR_ENTRY(rx_p1_rate);
+NETSW_WR_ENTRY(rx_p2_rate);
+NETSW_WR_ENTRY(rx_p3_rate);
+NETSW_WR_ENTRY(rx_p4_rate);
+NETSW_WR_ENTRY(rx_p5_rate);
+NETSW_WR_ENTRY(rx_p6_rate);
+NETSW_WR_ENTRY(rx_p7_rate);
+NETSW_WR_ENTRY(tx_q0_rate);
+NETSW_WR_ENTRY(tx_q1_rate);
+NETSW_WR_ENTRY(tx_q2_rate);
+NETSW_WR_ENTRY(tx_q3_rate);
+NETSW_WR_ENTRY(color_map);
+NETSW_WR_ENTRY(tc_map);
+NETSW_WR_ENTRY(back_pressure);
+NETSW_WR_ENTRY(force_flow_ctrl);
+NETSW_WR_ENTRY(pass_all);
+NETSW_WR_ENTRY(tail_tag);
+
+NETSW_WR_ENTRY(cust_vid);
+NETSW_WR_ENTRY(sr_1_vid);
+NETSW_WR_ENTRY(sr_2_vid);
+NETSW_WR_ENTRY(sr_1_type);
+NETSW_WR_ENTRY(sr_2_type);
+
+NETSW_WR_ENTRY(pme_ctrl);
+NETSW_WR_ENTRY(pme_status);
+
+NETSW_WR_ENTRY(authen_mode);
+NETSW_WR_ENTRY(acl);
+NETSW_WR_ENTRY(acl_first_rule);
+NETSW_WR_ENTRY(acl_ruleset);
+NETSW_WR_ENTRY(acl_mode);
+NETSW_WR_ENTRY(acl_enable);
+NETSW_WR_ENTRY(acl_src);
+NETSW_WR_ENTRY(acl_equal);
+NETSW_WR_ENTRY(acl_addr);
+NETSW_WR_ENTRY(acl_type);
+NETSW_WR_ENTRY(acl_cnt);
+NETSW_WR_ENTRY(acl_msec);
+NETSW_WR_ENTRY(acl_intr_mode);
+NETSW_WR_ENTRY(acl_ip_addr);
+NETSW_WR_ENTRY(acl_ip_mask);
+NETSW_WR_ENTRY(acl_protocol);
+NETSW_WR_ENTRY(acl_seqnum);
+NETSW_WR_ENTRY(acl_port_mode);
+NETSW_WR_ENTRY(acl_max_port);
+NETSW_WR_ENTRY(acl_min_port);
+NETSW_WR_ENTRY(acl_tcp_flag_enable);
+NETSW_WR_ENTRY(acl_tcp_flag);
+NETSW_WR_ENTRY(acl_tcp_flag_mask);
+NETSW_WR_ENTRY(acl_prio_mode);
+NETSW_WR_ENTRY(acl_prio);
+NETSW_WR_ENTRY(acl_vlan_prio_replace);
+NETSW_WR_ENTRY(acl_vlan_prio);
+NETSW_WR_ENTRY(acl_map_mode);
+NETSW_WR_ENTRY(acl_ports);
+NETSW_WR_ENTRY(acl_index);
+NETSW_WR_ENTRY(acl_act_index);
+NETSW_WR_ENTRY(acl_act);
+NETSW_WR_ENTRY(acl_rule_index);
+NETSW_WR_ENTRY(acl_info);
+NETSW_RD_ENTRY(acl_table);
+
+NETSW_WR_ENTRY(p_index);
+NETSW_WR_ENTRY(q_index);
+NETSW_WR_ENTRY(police_type);
+NETSW_WR_ENTRY(non_dscp_color);
+NETSW_WR_ENTRY(police_drop_all);
+NETSW_WR_ENTRY(police_port_based);
+NETSW_WR_ENTRY(color_mark);
+NETSW_WR_ENTRY(color_remap);
+NETSW_WR_ENTRY(drop_srp);
+NETSW_WR_ENTRY(color_aware);
+NETSW_WR_ENTRY(police);
+NETSW_WR_ENTRY(q_cir);
+NETSW_WR_ENTRY(q_pir);
+NETSW_WR_ENTRY(q_cbs);
+NETSW_WR_ENTRY(q_pbs);
+NETSW_WR_ENTRY(wred_max);
+NETSW_WR_ENTRY(wred_min);
+NETSW_WR_ENTRY(wred_multiplier);
+NETSW_RD_ENTRY(wred_avg_size);
+NETSW_WR_ENTRY(wred_q_max);
+NETSW_WR_ENTRY(wred_q_min);
+NETSW_WR_ENTRY(wred_q_multiplier);
+NETSW_RD_ENTRY(wred_q_avg_size);
+NETSW_WR_ENTRY(wred_random_drop);
+NETSW_WR_ENTRY(wred_drop_gyr);
+NETSW_WR_ENTRY(wred_drop_yr);
+NETSW_WR_ENTRY(wred_drop_r);
+NETSW_WR_ENTRY(wred_drop_all);
+NETSW_RD_ENTRY(wred_q_pmon);
+
+NETSW_WR_ENTRY(q_scheduling);
+NETSW_WR_ENTRY(q_shaping);
+#ifdef MTI_PREEMPT_ENABLE
+NETSW_WR_ENTRY(preempt);
+#endif
+NETSW_WR_ENTRY(q_tx_ratio);
+NETSW_WR_ENTRY(q_credit_hi);
+NETSW_WR_ENTRY(q_credit_lo);
+NETSW_WR_ENTRY(q_credit_incr);
+NETSW_WR_ENTRY(srp);
+
+NETSW_WR_ENTRY(qm_drop);
+NETSW_WR_ENTRY(qm_burst);
+NETSW_WR_ENTRY(qm_resv_space);
+NETSW_WR_ENTRY(qm_hi);
+NETSW_WR_ENTRY(qm_lo);
+NETSW_RD_ENTRY(qm_tx_used);
+NETSW_RD_ENTRY(qm_tx_avail);
+NETSW_RD_ENTRY(qm_tx_calc);
+
+NETSW_WR_ENTRY(mmd_id);
+NETSW_WR_ENTRY(mmd_reg);
+NETSW_WR_ENTRY(mmd_val);
+
+NETSW_RD_ENTRY(rx_flow_ctrl);
+NETSW_RD_ENTRY(tx_flow_ctrl);
+
+NETSW_WR_ENTRY(duplex);
+NETSW_WR_ENTRY(speed);
+NETSW_WR_ENTRY(mac_oper);
+NETSW_WR_ENTRY(vlan_restricted);
+NETSW_WR_ENTRY(vlan_untagged);
+
+#ifdef CONFIG_KSZ_STP
+NETSW_RD_ENTRY(stp_info);
+NETSW_WR_ENTRY(stp_on);
+NETSW_WR_ENTRY(stp_prio);
+NETSW_WR_ENTRY(stp_admin_path_cost);
+NETSW_WR_ENTRY(stp_path_cost);
+NETSW_WR_ENTRY(stp_admin_edge);
+NETSW_WR_ENTRY(stp_auto_edge);
+NETSW_WR_ENTRY(stp_mcheck);
+NETSW_WR_ENTRY(stp_admin_p2p);
+#ifdef CONFIG_KSZ_MSTP
+NETSW_WR_ENTRY(stp_auto_isolate);
+#endif
+#endif
+
+#ifdef CONFIG_KSZ_MRP
+NETSW_WR_ENTRY(mmrpEnabled);
+NETSW_WR_ENTRY(mmrp_mac);
+NETSW_WR_ENTRY(mmrp_svc);
+NETSW_WR_ENTRY(mmrp_reg);
+NETSW_WR_ENTRY(mvrpEnabled);
+NETSW_WR_ENTRY(mvrp_vid);
+NETSW_WR_ENTRY(mvrp_reg);
+#endif
+
+#ifdef CONFIG_KSZ_MSRP
+NETSW_WR_ENTRY(msrpEnabled);
+#endif
+
+#ifdef CONFIG_KSZ_AVB
+NETSW_WR_ENTRY(asCapable);
+NETSW_WR_ENTRY(q_delta);
+NETSW_WR_ENTRY(q_admin_mbps);
+NETSW_WR_ENTRY(q_admin_slope);
+NETSW_RD_ENTRY(q_oper_slope);
+NETSW_WR_ENTRY(q_alg);
+NETSW_RD_ENTRY(sr_a_rx_prio);
+NETSW_WR_ENTRY(sr_a_tx_prio);
+NETSW_RD_ENTRY(sr_a_boundary);
+NETSW_WR_ENTRY(sr_a_latency);
+NETSW_RD_ENTRY(sr_b_rx_prio);
+NETSW_WR_ENTRY(sr_b_tx_prio);
+NETSW_RD_ENTRY(sr_b_boundary);
+NETSW_WR_ENTRY(sr_b_latency);
+NETSW_WR_ENTRY(max_frame_size);
+NETSW_WR_ENTRY(max_int_frames);
+NETSW_WR_ENTRY(class_prio);
+#endif
+
+NETSW_WR_ENTRY(linkmd);
+NETSW_WR_ENTRY(sqi);
+NETSW_WR_ENTRY(mac_loopback);
+NETSW_WR_ENTRY(phy_loopback);
+NETSW_WR_ENTRY(remote_loopback);
+
+static struct attribute *lan_attrs[] = {
+	&lan_attr_info.attr,
+	&lan_attr_version.attr,
+#ifdef USE_SPEED_LINK
+	&lan_attr_duplex.attr,
+	&lan_attr_speed.attr,
+	&lan_attr_force.attr,
+	&lan_attr_flow_ctrl.attr,
+#endif
+#ifdef USE_MIB
+	&lan_attr_mib.attr,
+#endif
+	&lan_attr_reg.attr,
+	&lan_attr_vid.attr,
+	&lan_attr_features.attr,
+	&lan_attr_overrides.attr,
+
+	&lan_attr_dynamic_table.attr,
+	&lan_attr_static_table.attr,
+	&lan_attr_vlan_table.attr,
+	&lan_attr_hsr_table.attr,
+	&lan_attr_aging.attr,
+	&lan_attr_fast_aging.attr,
+	&lan_attr_link_aging.attr,
+	&lan_attr_bcast_per.attr,
+	&lan_attr_mcast_storm.attr,
+	&lan_attr_tx_queue_based.attr,
+	&lan_attr_diffserv_map.attr,
+	&lan_attr_p_802_1p_map.attr,
+	&lan_attr_vlan.attr,
+	&lan_attr_null_vid.attr,
+	&lan_attr_drop_inv_vid.attr,
+	&lan_attr_macaddr.attr,
+	&lan_attr_mirror_mode.attr,
+	&lan_attr_igmp_snoop.attr,
+	&lan_attr_ipv6_mld_snoop.attr,
+	&lan_attr_ipv6_mld_option.attr,
+	&lan_attr_aggr_backoff.attr,
+	&lan_attr_no_exc_drop.attr,
+	&lan_attr_jumbo_packet.attr,
+	&lan_attr_legal_packet.attr,
+	&lan_attr_length_check.attr,
+	&lan_attr_back_pressure.attr,
+	&lan_attr_sw_flow_ctrl.attr,
+	&lan_attr_sw_half_duplex.attr,
+#ifdef SWITCH_10_MBIT
+	&lan_attr_sw_10_mbit.attr,
+#endif
+	&lan_attr_fair_flow_ctrl.attr,
+	&lan_attr_vlan_bound.attr,
+	&lan_attr_double_tag.attr,
+	&lan_attr_isp.attr,
+	&lan_attr_hsr.attr,
+	&lan_attr_hsr_redbox_id.attr,
+	&lan_attr_hsr_net_id.attr,
+	&lan_attr_mtu.attr,
+	&lan_attr_unk_ucast_fwd.attr,
+	&lan_attr_unk_ucast_ports.attr,
+	&lan_attr_unk_mcast_fwd.attr,
+	&lan_attr_unk_mcast_ports.attr,
+	&lan_attr_unk_vid_fwd.attr,
+	&lan_attr_unk_vid_ports.attr,
+	&lan_attr_pass_pause.attr,
+	&lan_attr_pme.attr,
+	&lan_attr_pme_polarity.attr,
+
+	&lan_attr_host_port.attr,
+	&lan_attr_ports.attr,
+	&lan_attr_dev_start.attr,
+	&lan_attr_port_start.attr,
+	&lan_attr_vlan_start.attr,
+	&lan_attr_avb.attr,
+	&lan_attr_stp.attr,
+	&lan_attr_two_dev.attr,
+	&lan_attr_authen.attr,
+
+	&lan_attr_alu_fid.attr,
+	&lan_attr_alu_use_fid.attr,
+	&lan_attr_alu_override.attr,
+	&lan_attr_alu_valid.attr,
+	&lan_attr_alu_mstp.attr,
+	&lan_attr_alu_prio.attr,
+	&lan_attr_alu_src.attr,
+	&lan_attr_alu_dst.attr,
+	&lan_attr_alu_ports.attr,
+	&lan_attr_alu_addr.attr,
+	&lan_attr_alu_type.attr,
+	&lan_attr_alu_index.attr,
+	&lan_attr_alu_info.attr,
+
+	&lan_attr_vlan_valid.attr,
+	&lan_attr_vlan_ports.attr,
+	&lan_attr_vlan_untag.attr,
+	&lan_attr_vlan_fid.attr,
+	&lan_attr_vlan_mstp.attr,
+	&lan_attr_vlan_prio.attr,
+	&lan_attr_vlan_option.attr,
+	&lan_attr_vlan_index.attr,
+	&lan_attr_vlan_info.attr,
+	&lan_attr_vid2fid.attr,
+	&lan_attr_fid2mstid.attr,
+
+#ifdef CONFIG_KSZ_STP
+	&lan_attr_stp_br_info.attr,
+	&lan_attr_stp_br_on.attr,
+	&lan_attr_stp_br_prio.attr,
+	&lan_attr_stp_br_fwd_delay.attr,
+	&lan_attr_stp_br_hello_time.attr,
+	&lan_attr_stp_br_max_age.attr,
+	&lan_attr_stp_br_tx_hold.attr,
+	&lan_attr_stp_version.attr,
+#ifdef CONFIG_KSZ_MSTP
+	&lan_attr_stp_br_max_hops.attr,
+	&lan_attr_stp_msti.attr,
+	&lan_attr_stp_msti_vid.attr,
+	&lan_attr_stp_mstp_cfg.attr,
+	&lan_attr_stp_mstp_name.attr,
+#endif
+#endif
+
+#ifdef CONFIG_KSZ_MSRP
+	&lan_attr_msrp_info.attr,
+	&lan_attr_msrpEnabled.attr,
+#endif
+
+#ifdef CONFIG_KSZ_AVB
+	&lan_attr_msrp_sr_a.attr,
+#endif
+
+#ifdef CONFIG_KSZ_HSR
+	&lan_attr_hsr_valid.attr,
+	&lan_attr_hsr_age_cnt.attr,
+	&lan_attr_hsr_path_id.attr,
+	&lan_attr_hsr_addr.attr,
+	&lan_attr_hsr_index.attr,
+	&lan_attr_hsr_info.attr,
+	&lan_attr_hsr_state.attr,
+#endif
+
+	&lan_attr_no_color.attr,
+	&lan_attr_color_red.attr,
+	&lan_attr_color_yellow.attr,
+	&lan_attr_color_green.attr,
+
+	&lan_attr_vlan_filter_dynamic.attr,
+	&lan_attr_vlan_filter_static.attr,
+
+	NULL
+};
+
+static struct attribute *sw_attrs[] = {
+	&sw_attr_mib.attr,
+
+	&sw_attr_vid.attr,
+	&sw_attr_member.attr,
+	&sw_attr_bcast_storm.attr,
+	&sw_attr_mstp.attr,
+	&sw_attr_rx.attr,
+	&sw_attr_tx.attr,
+	&sw_attr_learn.attr,
+	&sw_attr_power.attr,
+	&sw_attr_mirror_port.attr,
+	&sw_attr_mirror_rx.attr,
+	&sw_attr_mirror_tx.attr,
+	&sw_attr_diffserv.attr,
+	&sw_attr_p_802_1p.attr,
+	&sw_attr_prio_vlan.attr,
+	&sw_attr_prio_mac.attr,
+	&sw_attr_prio_acl.attr,
+	&sw_attr_prio_highest.attr,
+	&sw_attr_prio_or.attr,
+	&sw_attr_port_prio.attr,
+	&sw_attr_non_vid.attr,
+	&sw_attr_drop_non_vlan.attr,
+	&sw_attr_drop_tagged.attr,
+	&sw_attr_ingress.attr,
+	&sw_attr_replace_vid.attr,
+	&sw_attr_replace_prio.attr,
+	&sw_attr_mac_802_1x.attr,
+	&sw_attr_src_addr_filter.attr,
+	&sw_attr_vlan_lookup_0.attr,
+	&sw_attr_prio_queue.attr,
+	&sw_attr_rx_prio_rate.attr,
+	&sw_attr_tx_prio_rate.attr,
+	&sw_attr_limit.attr,
+	&sw_attr_limit_port_based.attr,
+	&sw_attr_limit_packet_based.attr,
+	&sw_attr_limit_flow_ctrl.attr,
+	&sw_attr_limit_cnt_ifg.attr,
+	&sw_attr_limit_cnt_pre.attr,
+	&sw_attr_rx_p0_rate.attr,
+	&sw_attr_rx_p1_rate.attr,
+	&sw_attr_rx_p2_rate.attr,
+	&sw_attr_rx_p3_rate.attr,
+	&sw_attr_rx_p4_rate.attr,
+	&sw_attr_rx_p5_rate.attr,
+	&sw_attr_rx_p6_rate.attr,
+	&sw_attr_rx_p7_rate.attr,
+	&sw_attr_tx_q0_rate.attr,
+	&sw_attr_tx_q1_rate.attr,
+	&sw_attr_tx_q2_rate.attr,
+	&sw_attr_tx_q3_rate.attr,
+	&sw_attr_color_map.attr,
+	&sw_attr_tc_map.attr,
+	&sw_attr_back_pressure.attr,
+	&sw_attr_force_flow_ctrl.attr,
+	&sw_attr_pass_all.attr,
+	&sw_attr_tail_tag.attr,
+
+	&sw_attr_cust_vid.attr,
+	&sw_attr_sr_1_vid.attr,
+	&sw_attr_sr_2_vid.attr,
+	&sw_attr_sr_1_type.attr,
+	&sw_attr_sr_2_type.attr,
+
+	&sw_attr_pme_ctrl.attr,
+	&sw_attr_pme_status.attr,
+
+	&sw_attr_authen_mode.attr,
+	&sw_attr_acl.attr,
+	&sw_attr_acl_first_rule.attr,
+	&sw_attr_acl_ruleset.attr,
+	&sw_attr_acl_mode.attr,
+	&sw_attr_acl_enable.attr,
+	&sw_attr_acl_src.attr,
+	&sw_attr_acl_equal.attr,
+	&sw_attr_acl_addr.attr,
+	&sw_attr_acl_type.attr,
+	&sw_attr_acl_cnt.attr,
+	&sw_attr_acl_msec.attr,
+	&sw_attr_acl_intr_mode.attr,
+	&sw_attr_acl_ip_addr.attr,
+	&sw_attr_acl_ip_mask.attr,
+	&sw_attr_acl_protocol.attr,
+	&sw_attr_acl_seqnum.attr,
+	&sw_attr_acl_port_mode.attr,
+	&sw_attr_acl_max_port.attr,
+	&sw_attr_acl_min_port.attr,
+	&sw_attr_acl_tcp_flag_enable.attr,
+	&sw_attr_acl_tcp_flag.attr,
+	&sw_attr_acl_tcp_flag_mask.attr,
+	&sw_attr_acl_prio_mode.attr,
+	&sw_attr_acl_prio.attr,
+	&sw_attr_acl_vlan_prio_replace.attr,
+	&sw_attr_acl_vlan_prio.attr,
+	&sw_attr_acl_map_mode.attr,
+	&sw_attr_acl_ports.attr,
+	&sw_attr_acl_index.attr,
+	&sw_attr_acl_act_index.attr,
+	&sw_attr_acl_act.attr,
+	&sw_attr_acl_rule_index.attr,
+	&sw_attr_acl_info.attr,
+	&sw_attr_acl_table.attr,
+
+	&sw_attr_p_index.attr,
+	&sw_attr_q_index.attr,
+	&sw_attr_police_type.attr,
+	&sw_attr_non_dscp_color.attr,
+	&sw_attr_police_drop_all.attr,
+	&sw_attr_police_port_based.attr,
+	&sw_attr_color_mark.attr,
+	&sw_attr_color_remap.attr,
+	&sw_attr_drop_srp.attr,
+	&sw_attr_color_aware.attr,
+	&sw_attr_police.attr,
+
+	&sw_attr_q_cir.attr,
+	&sw_attr_q_pir.attr,
+	&sw_attr_q_cbs.attr,
+	&sw_attr_q_pbs.attr,
+
+	&sw_attr_wred_max.attr,
+	&sw_attr_wred_min.attr,
+	&sw_attr_wred_multiplier.attr,
+	&sw_attr_wred_avg_size.attr,
+	&sw_attr_wred_q_max.attr,
+	&sw_attr_wred_q_min.attr,
+	&sw_attr_wred_q_multiplier.attr,
+	&sw_attr_wred_q_avg_size.attr,
+	&sw_attr_wred_random_drop.attr,
+	&sw_attr_wred_drop_gyr.attr,
+	&sw_attr_wred_drop_yr.attr,
+	&sw_attr_wred_drop_r.attr,
+	&sw_attr_wred_drop_all.attr,
+	&sw_attr_wred_q_pmon.attr,
+
+	&sw_attr_q_scheduling.attr,
+	&sw_attr_q_shaping.attr,
+#ifdef MTI_PREEMPT_ENABLE
+	&sw_attr_preempt.attr,
+#endif
+	&sw_attr_q_tx_ratio.attr,
+	&sw_attr_q_credit_hi.attr,
+	&sw_attr_q_credit_lo.attr,
+	&sw_attr_q_credit_incr.attr,
+	&sw_attr_srp.attr,
+
+	&sw_attr_qm_drop.attr,
+	&sw_attr_qm_burst.attr,
+	&sw_attr_qm_resv_space.attr,
+	&sw_attr_qm_hi.attr,
+	&sw_attr_qm_lo.attr,
+	&sw_attr_qm_tx_used.attr,
+	&sw_attr_qm_tx_avail.attr,
+	&sw_attr_qm_tx_calc.attr,
+
+	&sw_attr_mmd_id.attr,
+	&sw_attr_mmd_reg.attr,
+	&sw_attr_mmd_val.attr,
+
+	&sw_attr_rx_flow_ctrl.attr,
+	&sw_attr_tx_flow_ctrl.attr,
+
+	&sw_attr_duplex.attr,
+	&sw_attr_speed.attr,
+	&sw_attr_mac_oper.attr,
+	&sw_attr_vlan_restricted.attr,
+	&sw_attr_vlan_untagged.attr,
+
+#ifdef CONFIG_KSZ_STP
+	&sw_attr_stp_info.attr,
+	&sw_attr_stp_on.attr,
+	&sw_attr_stp_prio.attr,
+	&sw_attr_stp_admin_path_cost.attr,
+	&sw_attr_stp_path_cost.attr,
+	&sw_attr_stp_admin_edge.attr,
+	&sw_attr_stp_auto_edge.attr,
+	&sw_attr_stp_mcheck.attr,
+	&sw_attr_stp_admin_p2p.attr,
+#ifdef CONFIG_KSZ_MSTP
+	&sw_attr_stp_auto_isolate.attr,
+#endif
+#endif
+
+#ifdef CONFIG_KSZ_MRP
+	&sw_attr_mmrpEnabled.attr,
+	&sw_attr_mmrp_mac.attr,
+	&sw_attr_mmrp_svc.attr,
+	&sw_attr_mmrp_reg.attr,
+	&sw_attr_mvrpEnabled.attr,
+	&sw_attr_mvrp_vid.attr,
+	&sw_attr_mvrp_reg.attr,
+#endif
+
+#ifdef CONFIG_KSZ_MSRP
+	&sw_attr_msrpEnabled.attr,
+#endif
+
+#ifdef CONFIG_KSZ_AVB
+	&sw_attr_asCapable.attr,
+	&sw_attr_q_delta.attr,
+	&sw_attr_q_admin_mbps.attr,
+	&sw_attr_q_admin_slope.attr,
+	&sw_attr_q_oper_slope.attr,
+	&sw_attr_q_alg.attr,
+	&sw_attr_sr_a_rx_prio.attr,
+	&sw_attr_sr_a_tx_prio.attr,
+	&sw_attr_sr_a_boundary.attr,
+	&sw_attr_sr_a_latency.attr,
+	&sw_attr_sr_b_rx_prio.attr,
+	&sw_attr_sr_b_tx_prio.attr,
+	&sw_attr_sr_b_boundary.attr,
+	&sw_attr_sr_b_latency.attr,
+	&sw_attr_max_frame_size.attr,
+	&sw_attr_max_int_frames.attr,
+	&sw_attr_class_prio.attr,
+#endif
+
+	&sw_attr_linkmd.attr,
+	&sw_attr_sqi.attr,
+	&sw_attr_mac_loopback.attr,
+	&sw_attr_phy_loopback.attr,
+	&sw_attr_remote_loopback.attr,
+
+	NULL
+};
+
+static struct attribute_group lan_group = {
+	.name  = "sw",
+	.attrs  = lan_attrs,
+};
+
+static struct attribute_group sw_group = {
+	.name  = "sw0",
+	.attrs  = sw_attrs,
+};
+
+/* Kernel checking requires the attributes are in data segment. */
+#define SW_ATTRS_SIZE		(sizeof(sw_attrs) / sizeof(void *) - 1)
+
+#define MAX_SWITCHES		2
+
+static struct ksz_dev_attr ksz_sw_dev_attrs[(
+	SW_ATTRS_SIZE * TOTAL_PORT_NUM) * MAX_SWITCHES];
+static struct ksz_dev_attr *ksz_sw_dev_attrs_ptr = ksz_sw_dev_attrs;
+
+static void exit_sw_sysfs(struct ksz_sw *sw, struct ksz_sw_sysfs *info,
+	struct device *dev)
+{
+	int i;
+	int j;
+	uint n;
+
+	if (sw->overrides & SYSFS_PHY_PORT)
+		n = sw->port_cnt;
+	else
+		n = sw->mib_port_cnt + 1;
+	j = 0;
+	if (sw->overrides & SYSFS_1_BASE)
+		j = 1;
+	for (i = 0; i < n; i++) {
+		sw_group.name = sw_name[i + j];
+		sw_group.attrs = info->port_attrs[i];
+		sysfs_remove_group(&dev->kobj, &sw_group);
+		kfree(info->port_attrs[i]);
+		info->port_attrs[i] = NULL;
+		info->ksz_port_attrs[i] = NULL;
+	}
+	ksz_sw_dev_attrs_ptr = ksz_sw_dev_attrs;
+
+	sysfs_remove_group(&dev->kobj, &lan_group);
+}
+
+static int init_sw_sysfs(struct ksz_sw *sw, struct ksz_sw_sysfs *info,
+	struct device *dev)
+{
+	int err;
+	int i;
+	int j;
+	uint n;
+	uint p;
+	char *file;
+
+	err = sysfs_create_group(&dev->kobj, &lan_group);
+	if (err)
+		return err;
+	if (sw->overrides & SYSFS_PHY_PORT)
+		n = sw->port_cnt;
+	else
+		n = sw->mib_port_cnt + 1;
+	j = 0;
+	if (sw->overrides & SYSFS_1_BASE)
+		j = 1;
+	for (i = 0; i < n; i++) {
+		p = i;
+		if (!(sw->overrides & SYSFS_PHY_PORT))
+			p = sw->ops->get_phy_port(sw, i + 1);
+		file = NULL;
+		if (p >= sw->phy_port_cnt || p == sw->HOST_PORT)
+			file = "0_linkmd";
+		err = alloc_dev_attr(sw_attrs,
+			sizeof(sw_attrs) / sizeof(void *), i + j,
+			&info->ksz_port_attrs[i], &info->port_attrs[i],
+			file, &ksz_sw_dev_attrs_ptr);
+		if (err)
+			return err;
+		sw_group.name = sw_name[i + j];
+		sw_group.attrs = info->port_attrs[i];
+		err = sysfs_create_group(&dev->kobj, &sw_group);
+		if (err)
+			return err;
+	}
+	return err;
+}
+
diff -Napur linux-linux4microchip-2022.10-orig/drivers/net/ethernet/micrel/ksz_sw_sysfs.c linux-linux4microchip-2022.10/drivers/net/ethernet/micrel/ksz_sw_sysfs.c
--- linux-linux4microchip-2022.10-orig/drivers/net/ethernet/micrel/ksz_sw_sysfs.c	1969-12-31 16:00:00.000000000 -0800
+++ linux-linux4microchip-2022.10/drivers/net/ethernet/micrel/ksz_sw_sysfs.c	2023-04-25 16:13:55.392163494 -0700
@@ -0,0 +1,909 @@
+/**
+ * Microchip switch common sysfs code
+ *
+ * Copyright (c) 2015-2021 Microchip Technology Inc.
+ *	Tristram Ha <Tristram.Ha@microchip.com>
+ *
+ * Copyright (c) 2011-2013 Micrel, Inc.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ */
+
+
+#include "ksz_sysfs.h"
+
+
+static char *sw_name[TOTAL_PORT_NUM] = {
+	"sw0",
+	"sw1",
+	"sw2",
+};
+
+static char *mac_name[SWITCH_MAC_TABLE_ENTRIES] = {
+	"mac0",
+	"mac1",
+	"mac2",
+	"mac3",
+	"mac4",
+	"mac5",
+	"mac6",
+	"mac7",
+	"mac8",
+	"mac9",
+	"maca",
+	"macb",
+	"macc",
+	"macd",
+	"mace",
+	"macf",
+};
+
+static char *vlan_name[VLAN_TABLE_ENTRIES] = {
+	"vlan0",
+	"vlan1",
+	"vlan2",
+	"vlan3",
+	"vlan4",
+	"vlan5",
+	"vlan6",
+	"vlan7",
+	"vlan8",
+	"vlan9",
+	"vlana",
+	"vlanb",
+	"vlanc",
+	"vland",
+	"vlane",
+	"vlanf",
+};
+
+static ssize_t netlan_show(struct device *d, struct device_attribute *attr,
+	char *buf, unsigned long offset)
+{
+	struct ksz_port *port;
+	struct ksz_sw *sw;
+	struct semaphore *proc_sem;
+	ssize_t len = -EINVAL;
+	int proc_num;
+
+	get_private_data(d, &proc_sem, &sw, &port);
+	if (down_interruptible(proc_sem))
+		return -ERESTARTSYS;
+
+	len = 0;
+	proc_num = offset / sizeof(int);
+	len = sw->ops->sysfs_read(sw, proc_num, port, len, buf);
+	if (len)
+		goto netlan_show_done;
+
+#ifdef CONFIG_KSZ_STP
+	len = sw->ops->sysfs_stp_read(sw, proc_num, len, buf);
+	if (len)
+		goto netlan_show_done;
+#endif
+
+	/* Require hardware to be acquired first. */
+	sw->ops->acquire(sw);
+	len = sw->ops->sysfs_read_hw(sw, proc_num, len, buf);
+	sw->ops->release(sw);
+
+netlan_show_done:
+	up(proc_sem);
+	return len;
+}
+
+static ssize_t netlan_store(struct device *d, struct device_attribute *attr,
+	const char *buf, size_t count, unsigned long offset)
+{
+	struct ksz_port *port;
+	struct ksz_sw *sw;
+	struct semaphore *proc_sem;
+	ssize_t ret = -EINVAL;
+	int num;
+	int proc_num;
+
+	num = get_num_val(buf);
+	get_private_data(d, &proc_sem, &sw, &port);
+	if (down_interruptible(proc_sem))
+		return -ERESTARTSYS;
+
+	proc_num = offset / sizeof(int);
+	ret = count;
+
+#ifdef CONFIG_KSZ_STP
+	if (sw->ops->sysfs_stp_write(sw, proc_num, num, buf))
+		goto netlan_store_done;
+#endif
+
+	sw->ops->acquire(sw);
+	sw->ops->sysfs_write(sw, proc_num, port, num, buf);
+	sw->ops->release(sw);
+
+#ifdef CONFIG_KSZ_STP
+netlan_store_done:
+#endif
+	up(proc_sem);
+	return ret;
+}
+
+static ssize_t netsw_show(struct device *d, struct device_attribute *attr,
+	char *buf, unsigned long offset)
+{
+	struct ksz_sw *sw;
+	struct semaphore *proc_sem;
+	ssize_t len = -EINVAL;
+	int num;
+	uint port;
+
+	if (attr->attr.name[1] != '_')
+		return len;
+	port = attr->attr.name[0] - '0';
+	if (port >= TOTAL_PORT_NUM)
+		return len;
+
+	get_private_data(d, &proc_sem, &sw, NULL);
+	if (down_interruptible(proc_sem))
+		return -ERESTARTSYS;
+
+	len = 0;
+	num = offset / sizeof(int);
+	len = sw->ops->sysfs_port_read(sw, num, port, len, buf);
+	if (len)
+		goto netsw_show_done;
+
+#ifdef CONFIG_KSZ_STP
+	len = sw->ops->sysfs_stp_port_read(sw, num, port, len, buf);
+	if (len)
+		goto netsw_show_done;
+#endif
+
+	/* Require hardware to be acquired first. */
+	sw->ops->acquire(sw);
+	len = sw->ops->sysfs_port_read_hw(sw, num, port, len, buf);
+	sw->ops->release(sw);
+
+netsw_show_done:
+	up(proc_sem);
+	return len;
+}
+
+static ssize_t netsw_store(struct device *d, struct device_attribute *attr,
+	const char *buf, size_t count, unsigned long offset)
+{
+	struct ksz_sw *sw;
+	struct semaphore *proc_sem;
+	ssize_t ret = -EINVAL;
+	int num;
+	uint port;
+	int proc_num;
+
+	if (attr->attr.name[1] != '_')
+		return ret;
+	port = attr->attr.name[0] - '0';
+	if (port >= TOTAL_PORT_NUM)
+		return ret;
+	num = get_num_val(buf);
+	get_private_data(d, &proc_sem, &sw, NULL);
+	if (down_interruptible(proc_sem))
+		return -ERESTARTSYS;
+
+	proc_num = offset / sizeof(int);
+	ret = count;
+
+#ifdef CONFIG_KSZ_STP
+	if (sw->ops->sysfs_stp_port_write(sw, proc_num, port, num, buf))
+		goto netsw_store_done;
+#endif
+
+	sw->ops->acquire(sw);
+	sw->ops->sysfs_port_write(sw, proc_num, port, num, buf);
+	sw->ops->release(sw);
+
+#ifdef CONFIG_KSZ_STP
+netsw_store_done:
+#endif
+	up(proc_sem);
+	return ret;
+}
+
+static ssize_t netmac_show(struct device *d, struct device_attribute *attr,
+	char *buf, unsigned long offset)
+{
+	struct ksz_sw *sw;
+	struct semaphore *proc_sem;
+	ssize_t len = -EINVAL;
+	int num;
+	int index;
+
+	if (attr->attr.name[1] != '_')
+		return len;
+	if ('a' <= attr->attr.name[0] && attr->attr.name[0] <= 'f')
+		index = attr->attr.name[0] - 'a' + 10;
+	else
+		index = attr->attr.name[0] - '0';
+	if (index >= SWITCH_MAC_TABLE_ENTRIES)
+		return len;
+
+	get_private_data(d, &proc_sem, &sw, NULL);
+	if (down_interruptible(proc_sem))
+		return -ERESTARTSYS;
+
+	len = 0;
+	num = offset / sizeof(int);
+	len = sw->ops->sysfs_mac_read(sw, num, index, len, buf);
+	up(proc_sem);
+	return len;
+}
+
+static ssize_t netmac_store(struct device *d, struct device_attribute *attr,
+	const char *buf, size_t count, unsigned long offset)
+{
+	struct ksz_sw *sw;
+	struct semaphore *proc_sem;
+	ssize_t ret = -EINVAL;
+	int num;
+	int index;
+	int proc_num;
+
+	if (attr->attr.name[1] != '_')
+		return ret;
+	if ('a' <= attr->attr.name[0] && attr->attr.name[0] <= 'f')
+		index = attr->attr.name[0] - 'a' + 10;
+	else
+		index = attr->attr.name[0] - '0';
+	if (index >= SWITCH_MAC_TABLE_ENTRIES)
+		return ret;
+	num = get_num_val(buf);
+	get_private_data(d, &proc_sem, &sw, NULL);
+	if (down_interruptible(proc_sem))
+		return -ERESTARTSYS;
+
+	proc_num = offset / sizeof(int);
+	ret = count;
+	sw->ops->sysfs_mac_write(sw, proc_num, index, num, buf);
+	up(proc_sem);
+	return ret;
+}
+
+static ssize_t netvlan_show(struct device *d, struct device_attribute *attr,
+	char *buf, unsigned long offset)
+{
+	struct ksz_sw *sw;
+	struct semaphore *proc_sem;
+	ssize_t len = -EINVAL;
+	int num;
+	int index;
+
+	if (attr->attr.name[1] != '_')
+		return len;
+	if ('a' <= attr->attr.name[0] && attr->attr.name[0] <= 'f')
+		index = attr->attr.name[0] - 'a' + 10;
+	else
+		index = attr->attr.name[0] - '0';
+	if (index >= VLAN_TABLE_ENTRIES)
+		return len;
+
+	get_private_data(d, &proc_sem, &sw, NULL);
+	if (down_interruptible(proc_sem))
+		return -ERESTARTSYS;
+
+	len = 0;
+	num = offset / sizeof(int);
+	len = sw->ops->sysfs_vlan_read(sw, num, index, len, buf);
+	up(proc_sem);
+	return len;
+}
+
+static ssize_t netvlan_store(struct device *d, struct device_attribute *attr,
+	const char *buf, size_t count, unsigned long offset)
+{
+	struct ksz_sw *sw;
+	struct semaphore *proc_sem;
+	ssize_t ret = -EINVAL;
+	int num;
+	int index;
+	int proc_num;
+
+	if (attr->attr.name[1] != '_')
+		return ret;
+	if ('a' <= attr->attr.name[0] && attr->attr.name[0] <= 'f')
+		index = attr->attr.name[0] - 'a' + 10;
+	else
+		index = attr->attr.name[0] - '0';
+	if (index >= VLAN_TABLE_ENTRIES)
+		return ret;
+	num = get_num_val(buf);
+	get_private_data(d, &proc_sem, &sw, NULL);
+	if (down_interruptible(proc_sem))
+		return -ERESTARTSYS;
+
+	proc_num = offset / sizeof(int);
+	ret = count;
+	sw->ops->sysfs_vlan_write(sw, proc_num, index, num);
+	up(proc_sem);
+	return ret;
+}
+
+#define LAN_ATTR(_name, _mode, _show, _store) \
+struct device_attribute lan_attr_##_name = \
+	__ATTR(_name, _mode, _show, _store)
+
+/* generate a read-only attribute */
+#define NETLAN_RD_ENTRY(name)						\
+static ssize_t show_lan_##name(struct device *d,			\
+	struct device_attribute *attr, char *buf)			\
+{									\
+	return netlan_show(d, attr, buf,				\
+		offsetof(struct lan_attributes, name));			\
+}									\
+static LAN_ATTR(name, S_IRUGO, show_lan_##name, NULL)
+
+/* generate a write-able attribute */
+#define NETLAN_WR_ENTRY(name)						\
+static ssize_t show_lan_##name(struct device *d,			\
+	struct device_attribute *attr, char *buf)			\
+{									\
+	return netlan_show(d, attr, buf,				\
+		offsetof(struct lan_attributes, name));			\
+}									\
+static ssize_t store_lan_##name(struct device *d,			\
+	struct device_attribute *attr, const char *buf, size_t count)	\
+{									\
+	return netlan_store(d, attr, buf, count,			\
+		offsetof(struct lan_attributes, name));			\
+}									\
+static LAN_ATTR(name, S_IRUGO | S_IWUSR, show_lan_##name, store_lan_##name)
+
+#define SW_ATTR(_name, _mode, _show, _store) \
+struct device_attribute sw_attr_##_name = \
+	__ATTR(0_##_name, _mode, _show, _store)
+
+/* generate a read-only attribute */
+#define NETSW_RD_ENTRY(name)						\
+static ssize_t show_sw_##name(struct device *d,				\
+	struct device_attribute *attr, char *buf)			\
+{									\
+	return netsw_show(d, attr, buf,					\
+		offsetof(struct sw_attributes, name));			\
+}									\
+static SW_ATTR(name, S_IRUGO, show_sw_##name, NULL)
+
+/* generate a write-able attribute */
+#define NETSW_WR_ENTRY(name)						\
+static ssize_t show_sw_##name(struct device *d,				\
+	struct device_attribute *attr, char *buf)			\
+{									\
+	return netsw_show(d, attr, buf,					\
+		offsetof(struct sw_attributes, name));			\
+}									\
+static ssize_t store_sw_##name(struct device *d,			\
+	struct device_attribute *attr, const char *buf, size_t count)	\
+{									\
+	return netsw_store(d, attr, buf, count,				\
+		offsetof(struct sw_attributes, name));			\
+}									\
+static SW_ATTR(name, S_IRUGO | S_IWUSR, show_sw_##name, store_sw_##name)
+
+#define MAC_ATTR(_name, _mode, _show, _store) \
+struct device_attribute mac_attr_##_name = \
+	__ATTR(0_##_name, _mode, _show, _store)
+
+/* generate a read-only attribute */
+#define NETMAC_RD_ENTRY(name)						\
+static ssize_t show_mac_##name(struct device *d,			\
+	struct device_attribute *attr, char *buf)			\
+{									\
+	return netmac_show(d, attr, buf,				\
+		offsetof(struct static_mac_attributes, name));		\
+}									\
+static MAC_ATTR(name, S_IRUGO, show_mac_##name, NULL)
+
+/* generate a write-able attribute */
+#define NETMAC_WR_ENTRY(name)						\
+static ssize_t show_mac_##name(struct device *d,			\
+	struct device_attribute *attr, char *buf)			\
+{									\
+	return netmac_show(d, attr, buf,				\
+		offsetof(struct static_mac_attributes, name));		\
+}									\
+static ssize_t store_mac_##name(struct device *d,			\
+	struct device_attribute *attr, const char *buf, size_t count)	\
+{									\
+	return netmac_store(d, attr, buf, count,			\
+		offsetof(struct static_mac_attributes, name));		\
+}									\
+static MAC_ATTR(name, S_IRUGO | S_IWUSR, show_mac_##name, store_mac_##name)
+
+#define VLAN_ATTR(_name, _mode, _show, _store) \
+struct device_attribute vlan_attr_##_name = \
+	__ATTR(0_##_name, _mode, _show, _store)
+
+/* generate a read-only attribute */
+#define NETVLAN_RD_ENTRY(name)						\
+static ssize_t show_vlan_##name(struct device *d,			\
+	struct device_attribute *attr, char *buf)			\
+{									\
+	return netvlan_show(d, attr, buf,				\
+		offsetof(struct vlan_attributes, name));		\
+}									\
+static VLAN_ATTR(name, S_IRUGO, show_vlan_##name, NULL)
+
+/* generate a write-able attribute */
+#define NETVLAN_WR_ENTRY(name)						\
+static ssize_t show_vlan_##name(struct device *d,			\
+	struct device_attribute *attr, char *buf)			\
+{									\
+	return netvlan_show(d, attr, buf,				\
+		offsetof(struct vlan_attributes, name));		\
+}									\
+static ssize_t store_vlan_##name(struct device *d,			\
+	struct device_attribute *attr, const char *buf, size_t count)	\
+{									\
+	return netvlan_store(d, attr, buf, count,			\
+		offsetof(struct vlan_attributes, name));		\
+}									\
+static VLAN_ATTR(name, S_IRUGO | S_IWUSR, show_vlan_##name, store_vlan_##name)
+
+NETLAN_WR_ENTRY(info);
+NETLAN_RD_ENTRY(version);
+NETLAN_WR_ENTRY(duplex);
+NETLAN_WR_ENTRY(speed);
+NETLAN_WR_ENTRY(force);
+NETLAN_WR_ENTRY(flow_ctrl);
+NETLAN_WR_ENTRY(mib);
+NETLAN_WR_ENTRY(reg);
+NETLAN_WR_ENTRY(vid);
+NETLAN_WR_ENTRY(features);
+NETLAN_WR_ENTRY(overrides);
+
+NETLAN_WR_ENTRY(dynamic_table);
+NETLAN_WR_ENTRY(static_table);
+NETLAN_RD_ENTRY(vlan_table);
+NETLAN_WR_ENTRY(aging);
+NETLAN_WR_ENTRY(fast_aging);
+NETLAN_WR_ENTRY(link_aging);
+NETLAN_WR_ENTRY(bcast_per);
+NETLAN_WR_ENTRY(mcast_storm);
+NETLAN_WR_ENTRY(diffserv_map);
+NETLAN_WR_ENTRY(p_802_1p_map);
+NETLAN_WR_ENTRY(vlan);
+NETLAN_WR_ENTRY(null_vid);
+NETLAN_WR_ENTRY(macaddr);
+NETLAN_WR_ENTRY(mirror_mode);
+NETLAN_WR_ENTRY(tail_tag);
+NETLAN_WR_ENTRY(igmp_snoop);
+NETLAN_WR_ENTRY(ipv6_mld_snoop);
+NETLAN_WR_ENTRY(ipv6_mld_option);
+NETLAN_WR_ENTRY(aggr_backoff);
+NETLAN_WR_ENTRY(no_exc_drop);
+#ifdef SWITCH_BUF_RESERVE
+NETLAN_WR_ENTRY(buf_reserve);
+#endif
+NETLAN_WR_ENTRY(huge_packet);
+NETLAN_WR_ENTRY(legal_packet);
+NETLAN_WR_ENTRY(length_check);
+NETLAN_WR_ENTRY(back_pressure);
+NETLAN_WR_ENTRY(sw_flow_ctrl);
+NETLAN_WR_ENTRY(sw_half_duplex);
+#ifdef SWITCH_10_MBIT
+NETLAN_WR_ENTRY(sw_10_mbit);
+#endif
+NETLAN_WR_ENTRY(rx_flow_ctrl);
+NETLAN_WR_ENTRY(tx_flow_ctrl);
+NETLAN_WR_ENTRY(fair_flow_ctrl);
+NETLAN_WR_ENTRY(vlan_bound);
+NETLAN_WR_ENTRY(fw_unk_dest);
+NETLAN_WR_ENTRY(ins_tag_0_1);
+NETLAN_WR_ENTRY(ins_tag_0_2);
+NETLAN_WR_ENTRY(ins_tag_1_0);
+NETLAN_WR_ENTRY(ins_tag_1_2);
+NETLAN_WR_ENTRY(ins_tag_2_0);
+NETLAN_WR_ENTRY(ins_tag_2_1);
+NETLAN_WR_ENTRY(pass_all);
+NETLAN_WR_ENTRY(pass_pause);
+#ifdef SWITCH_PORT_PHY_ADDR_MASK
+NETLAN_WR_ENTRY(phy_addr);
+#endif
+NETLAN_RD_ENTRY(ports);
+NETLAN_RD_ENTRY(dev_start);
+NETLAN_RD_ENTRY(vlan_start);
+NETLAN_RD_ENTRY(stp);
+
+#ifdef CONFIG_KSZ_STP
+NETLAN_RD_ENTRY(stp_br_info);
+NETLAN_WR_ENTRY(stp_br_on);
+NETLAN_WR_ENTRY(stp_br_prio);
+NETLAN_WR_ENTRY(stp_br_fwd_delay);
+NETLAN_WR_ENTRY(stp_br_hello_time);
+NETLAN_WR_ENTRY(stp_br_max_age);
+NETLAN_WR_ENTRY(stp_br_tx_hold);
+NETLAN_WR_ENTRY(stp_version);
+#endif
+
+NETSW_WR_ENTRY(mib);
+NETSW_WR_ENTRY(vid);
+NETSW_WR_ENTRY(member);
+NETSW_WR_ENTRY(bcast_storm);
+NETSW_WR_ENTRY(rx);
+NETSW_WR_ENTRY(tx);
+NETSW_WR_ENTRY(learn);
+NETSW_WR_ENTRY(mirror_port);
+NETSW_WR_ENTRY(mirror_rx);
+NETSW_WR_ENTRY(mirror_tx);
+NETSW_WR_ENTRY(diffserv);
+NETSW_WR_ENTRY(p_802_1p);
+NETSW_WR_ENTRY(port_based);
+NETSW_WR_ENTRY(non_vid);
+NETSW_WR_ENTRY(drop_tagged);
+NETSW_WR_ENTRY(ingress);
+NETSW_WR_ENTRY(ins_tag);
+NETSW_WR_ENTRY(rmv_tag);
+#ifdef PORT_DOUBLE_TAG
+NETSW_WR_ENTRY(double_tag);
+#endif
+NETSW_WR_ENTRY(replace_prio);
+NETSW_WR_ENTRY(prio_queue);
+NETSW_WR_ENTRY(tx_p0_ctrl);
+NETSW_WR_ENTRY(tx_p1_ctrl);
+NETSW_WR_ENTRY(tx_p2_ctrl);
+NETSW_WR_ENTRY(tx_p3_ctrl);
+#ifdef RATE_RATIO_MASK
+NETSW_WR_ENTRY(tx_p0_ratio);
+NETSW_WR_ENTRY(tx_p1_ratio);
+NETSW_WR_ENTRY(tx_p2_ratio);
+NETSW_WR_ENTRY(tx_p3_ratio);
+#endif
+NETSW_WR_ENTRY(rx_prio_rate);
+NETSW_WR_ENTRY(tx_prio_rate);
+NETSW_WR_ENTRY(rx_limit);
+NETSW_WR_ENTRY(cnt_ifg);
+NETSW_WR_ENTRY(cnt_pre);
+NETSW_WR_ENTRY(rx_p0_rate);
+NETSW_WR_ENTRY(rx_p1_rate);
+NETSW_WR_ENTRY(rx_p2_rate);
+NETSW_WR_ENTRY(rx_p3_rate);
+NETSW_WR_ENTRY(tx_p0_rate);
+NETSW_WR_ENTRY(tx_p1_rate);
+NETSW_WR_ENTRY(tx_p2_rate);
+NETSW_WR_ENTRY(tx_p3_rate);
+NETSW_WR_ENTRY(back_pressure);
+NETSW_WR_ENTRY(force_flow_ctrl);
+NETSW_WR_ENTRY(fw_unk_dest);
+NETSW_WR_ENTRY(fw_inv_vid);
+
+NETSW_RD_ENTRY(duplex);
+NETSW_RD_ENTRY(speed);
+NETSW_WR_ENTRY(linkmd);
+NETSW_WR_ENTRY(macaddr);
+NETSW_WR_ENTRY(src_filter_0);
+NETSW_WR_ENTRY(src_filter_1);
+
+#ifdef CONFIG_KSZ_STP
+NETSW_RD_ENTRY(stp_info);
+NETSW_WR_ENTRY(stp_on);
+NETSW_WR_ENTRY(stp_prio);
+NETSW_WR_ENTRY(stp_admin_path_cost);
+NETSW_WR_ENTRY(stp_path_cost);
+NETSW_WR_ENTRY(stp_admin_edge);
+NETSW_WR_ENTRY(stp_auto_edge);
+NETSW_WR_ENTRY(stp_mcheck);
+NETSW_WR_ENTRY(stp_admin_p2p);
+#endif
+
+NETMAC_WR_ENTRY(fid);
+NETMAC_WR_ENTRY(use_fid);
+NETMAC_WR_ENTRY(override);
+NETMAC_WR_ENTRY(valid);
+NETMAC_WR_ENTRY(ports);
+NETMAC_WR_ENTRY(addr);
+
+NETVLAN_WR_ENTRY(valid);
+NETVLAN_WR_ENTRY(member);
+NETVLAN_WR_ENTRY(fid);
+NETVLAN_WR_ENTRY(vid);
+
+static struct attribute *lan_attrs[] = {
+	&lan_attr_info.attr,
+	&lan_attr_version.attr,
+#ifdef USE_SPEED_LINK
+	&lan_attr_duplex.attr,
+	&lan_attr_speed.attr,
+	&lan_attr_force.attr,
+	&lan_attr_flow_ctrl.attr,
+#endif
+#ifdef USE_MIB
+	&lan_attr_mib.attr,
+#endif
+	&lan_attr_reg.attr,
+	&lan_attr_vid.attr,
+	&lan_attr_features.attr,
+	&lan_attr_overrides.attr,
+
+	&lan_attr_dynamic_table.attr,
+	&lan_attr_static_table.attr,
+	&lan_attr_vlan_table.attr,
+	&lan_attr_aging.attr,
+	&lan_attr_fast_aging.attr,
+	&lan_attr_link_aging.attr,
+	&lan_attr_bcast_per.attr,
+	&lan_attr_mcast_storm.attr,
+	&lan_attr_diffserv_map.attr,
+	&lan_attr_p_802_1p_map.attr,
+	&lan_attr_vlan.attr,
+	&lan_attr_null_vid.attr,
+	&lan_attr_macaddr.attr,
+	&lan_attr_mirror_mode.attr,
+	&lan_attr_tail_tag.attr,
+	&lan_attr_igmp_snoop.attr,
+	&lan_attr_ipv6_mld_snoop.attr,
+	&lan_attr_ipv6_mld_option.attr,
+	&lan_attr_aggr_backoff.attr,
+	&lan_attr_no_exc_drop.attr,
+#ifdef SWITCH_BUF_RESERVE
+	&lan_attr_buf_reserve.attr,
+#endif
+	&lan_attr_huge_packet.attr,
+	&lan_attr_legal_packet.attr,
+	&lan_attr_length_check.attr,
+	&lan_attr_back_pressure.attr,
+	&lan_attr_sw_flow_ctrl.attr,
+	&lan_attr_sw_half_duplex.attr,
+#ifdef SWITCH_10_MBIT
+	&lan_attr_sw_10_mbit.attr,
+#endif
+	&lan_attr_rx_flow_ctrl.attr,
+	&lan_attr_tx_flow_ctrl.attr,
+	&lan_attr_fair_flow_ctrl.attr,
+	&lan_attr_vlan_bound.attr,
+	&lan_attr_fw_unk_dest.attr,
+	&lan_attr_ins_tag_0_1.attr,
+	&lan_attr_ins_tag_0_2.attr,
+	&lan_attr_ins_tag_1_0.attr,
+	&lan_attr_ins_tag_1_2.attr,
+	&lan_attr_ins_tag_2_0.attr,
+	&lan_attr_ins_tag_2_1.attr,
+	&lan_attr_pass_all.attr,
+	&lan_attr_pass_pause.attr,
+#ifdef SWITCH_PORT_PHY_ADDR_MASK
+	&lan_attr_phy_addr.attr,
+#endif
+	&lan_attr_ports.attr,
+	&lan_attr_dev_start.attr,
+	&lan_attr_vlan_start.attr,
+	&lan_attr_stp.attr,
+
+#ifdef CONFIG_KSZ_STP
+	&lan_attr_stp_br_info.attr,
+	&lan_attr_stp_br_on.attr,
+	&lan_attr_stp_br_prio.attr,
+	&lan_attr_stp_br_fwd_delay.attr,
+	&lan_attr_stp_br_hello_time.attr,
+	&lan_attr_stp_br_max_age.attr,
+	&lan_attr_stp_br_tx_hold.attr,
+	&lan_attr_stp_version.attr,
+#endif
+
+	NULL
+};
+
+static struct attribute *sw_attrs[] = {
+	&sw_attr_vid.attr,
+	&sw_attr_member.attr,
+	&sw_attr_bcast_storm.attr,
+	&sw_attr_rx.attr,
+	&sw_attr_tx.attr,
+	&sw_attr_learn.attr,
+	&sw_attr_mirror_port.attr,
+	&sw_attr_mirror_rx.attr,
+	&sw_attr_mirror_tx.attr,
+	&sw_attr_diffserv.attr,
+	&sw_attr_p_802_1p.attr,
+	&sw_attr_port_based.attr,
+	&sw_attr_non_vid.attr,
+	&sw_attr_drop_tagged.attr,
+	&sw_attr_ingress.attr,
+	&sw_attr_ins_tag.attr,
+	&sw_attr_rmv_tag.attr,
+#ifdef PORT_DOUBLE_TAG
+	&sw_attr_double_tag.attr,
+#endif
+	&sw_attr_replace_prio.attr,
+	&sw_attr_prio_queue.attr,
+	&sw_attr_tx_p0_ctrl.attr,
+	&sw_attr_tx_p1_ctrl.attr,
+	&sw_attr_tx_p2_ctrl.attr,
+	&sw_attr_tx_p3_ctrl.attr,
+#ifdef RATE_RATIO_MASK
+	&sw_attr_tx_p0_ratio.attr,
+	&sw_attr_tx_p1_ratio.attr,
+	&sw_attr_tx_p2_ratio.attr,
+	&sw_attr_tx_p3_ratio.attr,
+#endif
+	&sw_attr_rx_prio_rate.attr,
+	&sw_attr_tx_prio_rate.attr,
+	&sw_attr_rx_limit.attr,
+	&sw_attr_cnt_ifg.attr,
+	&sw_attr_cnt_pre.attr,
+	&sw_attr_rx_p0_rate.attr,
+	&sw_attr_rx_p1_rate.attr,
+	&sw_attr_rx_p2_rate.attr,
+	&sw_attr_rx_p3_rate.attr,
+	&sw_attr_tx_p0_rate.attr,
+	&sw_attr_tx_p1_rate.attr,
+	&sw_attr_tx_p2_rate.attr,
+	&sw_attr_tx_p3_rate.attr,
+	&sw_attr_back_pressure.attr,
+	&sw_attr_force_flow_ctrl.attr,
+	&sw_attr_fw_unk_dest.attr,
+	&sw_attr_fw_inv_vid.attr,
+	&sw_attr_mib.attr,
+
+	&sw_attr_duplex.attr,
+	&sw_attr_speed.attr,
+	&sw_attr_linkmd.attr,
+	&sw_attr_macaddr.attr,
+	&sw_attr_src_filter_0.attr,
+	&sw_attr_src_filter_1.attr,
+
+#ifdef CONFIG_KSZ_STP
+	&sw_attr_stp_info.attr,
+	&sw_attr_stp_on.attr,
+	&sw_attr_stp_prio.attr,
+	&sw_attr_stp_admin_path_cost.attr,
+	&sw_attr_stp_path_cost.attr,
+	&sw_attr_stp_admin_edge.attr,
+	&sw_attr_stp_auto_edge.attr,
+	&sw_attr_stp_mcheck.attr,
+	&sw_attr_stp_admin_p2p.attr,
+#endif
+
+	NULL
+};
+
+static struct attribute *mac_attrs[] = {
+	&mac_attr_fid.attr,
+	&mac_attr_use_fid.attr,
+	&mac_attr_override.attr,
+	&mac_attr_valid.attr,
+	&mac_attr_ports.attr,
+	&mac_attr_addr.attr,
+	NULL
+};
+
+static struct attribute *vlan_attrs[] = {
+	&vlan_attr_valid.attr,
+	&vlan_attr_member.attr,
+	&vlan_attr_fid.attr,
+	&vlan_attr_vid.attr,
+	NULL
+};
+
+static struct attribute_group lan_group = {
+	.name  = "sw",
+	.attrs  = lan_attrs,
+};
+
+static struct attribute_group sw_group = {
+	.name  = "sw0",
+	.attrs  = sw_attrs,
+};
+
+static struct attribute_group mac_group = {
+	.name  = "mac0",
+	.attrs  = mac_attrs,
+};
+
+static struct attribute_group vlan_group = {
+	.name  = "vlan0",
+	.attrs  = vlan_attrs,
+};
+
+/* Kernel checking requires the attributes are in data segment. */
+#define MAC_ATTRS_SIZE		(sizeof(mac_attrs) / sizeof(void *) - 1)
+#define VLAN_ATTRS_SIZE		(sizeof(vlan_attrs) / sizeof(void *) - 1)
+#define SW_ATTRS_SIZE		(sizeof(sw_attrs) / sizeof(void *) - 1)
+
+#define MAX_SWITCHES		2
+
+static struct ksz_dev_attr ksz_sw_dev_attrs[(
+	MAC_ATTRS_SIZE * SWITCH_MAC_TABLE_ENTRIES +
+	VLAN_ATTRS_SIZE * VLAN_TABLE_ENTRIES +
+	SW_ATTRS_SIZE * TOTAL_PORT_NUM) * MAX_SWITCHES];
+static struct ksz_dev_attr *ksz_sw_dev_attrs_ptr = ksz_sw_dev_attrs;
+
+static void exit_sw_sysfs(struct ksz_sw *sw, struct ksz_sw_sysfs *info,
+	struct device *dev)
+{
+	int i;
+
+	for (i = 0; i < VLAN_TABLE_ENTRIES; i++) {
+		vlan_group.name = vlan_name[i];
+		vlan_group.attrs = info->vlan_attrs[i];
+		sysfs_remove_group(&dev->kobj, &vlan_group);
+		kfree(info->vlan_attrs[i]);
+		info->vlan_attrs[i] = NULL;
+		info->ksz_vlan_attrs[i] = NULL;
+	}
+	for (i = 0; i < SWITCH_MAC_TABLE_ENTRIES; i++) {
+		mac_group.name = mac_name[i];
+		mac_group.attrs = info->mac_attrs[i];
+		sysfs_remove_group(&dev->kobj, &mac_group);
+		kfree(info->mac_attrs[i]);
+		info->mac_attrs[i] = NULL;
+		info->ksz_mac_attrs[i] = NULL;
+	}
+	for (i = 0; i < TOTAL_PORT_NUM; i++) {
+		sw_group.name = sw_name[i];
+		sw_group.attrs = info->port_attrs[i];
+		sysfs_remove_group(&dev->kobj, &sw_group);
+		kfree(info->port_attrs[i]);
+		info->port_attrs[i] = NULL;
+		info->ksz_port_attrs[i] = NULL;
+	}
+	ksz_sw_dev_attrs_ptr = ksz_sw_dev_attrs;
+
+	sysfs_remove_group(&dev->kobj, &lan_group);
+}
+
+static int init_sw_sysfs(struct ksz_sw *sw, struct ksz_sw_sysfs *info,
+	struct device *dev)
+{
+	int err;
+	int i;
+	char *file;
+
+	err = sysfs_create_group(&dev->kobj, &lan_group);
+	if (err)
+		return err;
+	for (i = 0; i < SWITCH_MAC_TABLE_ENTRIES; i++) {
+		err = alloc_dev_attr(mac_attrs,
+			sizeof(mac_attrs) / sizeof(void *), i,
+			&info->ksz_mac_attrs[i],
+			&info->mac_attrs[i], NULL, &ksz_sw_dev_attrs_ptr);
+		if (err)
+			return err;
+		mac_group.name = mac_name[i];
+		mac_group.attrs = info->mac_attrs[i];
+		err = sysfs_create_group(&dev->kobj, &mac_group);
+		if (err)
+			return err;
+	}
+	for (i = 0; i < VLAN_TABLE_ENTRIES; i++) {
+		err = alloc_dev_attr(vlan_attrs,
+			sizeof(vlan_attrs) / sizeof(void *), i,
+			&info->ksz_vlan_attrs[i],
+			&info->vlan_attrs[i], NULL, &ksz_sw_dev_attrs_ptr);
+		if (err)
+			return err;
+		vlan_group.name = vlan_name[i];
+		vlan_group.attrs = info->vlan_attrs[i];
+		err = sysfs_create_group(&dev->kobj, &vlan_group);
+		if (err)
+			return err;
+	}
+	for (i = 0; i < TOTAL_PORT_NUM; i++) {
+		file = NULL;
+		if (i == sw->HOST_PORT)
+			file = "0_duplex";
+		err = alloc_dev_attr(sw_attrs,
+			sizeof(sw_attrs) / sizeof(void *), i,
+			&info->ksz_port_attrs[i], &info->port_attrs[i],
+			file, &ksz_sw_dev_attrs_ptr);
+		if (err)
+			return err;
+		sw_group.name = sw_name[i];
+		sw_group.attrs = info->port_attrs[i];
+		err = sysfs_create_group(&dev->kobj, &sw_group);
+		if (err)
+			return err;
+	}
+	return err;
+}
+
diff -Napur linux-linux4microchip-2022.10-orig/drivers/net/ethernet/micrel/ksz_sysfs.h linux-linux4microchip-2022.10/drivers/net/ethernet/micrel/ksz_sysfs.h
--- linux-linux4microchip-2022.10-orig/drivers/net/ethernet/micrel/ksz_sysfs.h	1969-12-31 16:00:00.000000000 -0800
+++ linux-linux4microchip-2022.10/drivers/net/ethernet/micrel/ksz_sysfs.h	2023-04-25 16:13:55.396163494 -0700
@@ -0,0 +1,138 @@
+/**
+ * Microchip switch common sysfs header
+ *
+ * Copyright (c) 2016-2019 Microchip Technology Inc.
+ *	Tristram Ha <Tristram.Ha@microchip.com>
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ */
+
+
+#ifndef KSZ_SYSFS_H
+#define KSZ_SYSFS_H
+
+
+#ifndef get_private_data
+
+#ifndef get_sysfs_data
+static void get_sysfs_data_(struct net_device *dev,
+	struct semaphore **proc_sem, struct ksz_port **port)
+{
+	*proc_sem = NULL;
+	*port = NULL;
+}
+
+#define get_sysfs_data		get_sysfs_data_
+#endif
+
+static void get_private_data_(struct device *d, struct semaphore **proc_sem,
+	struct ksz_sw **sw, struct ksz_port **port)
+{
+	if (d->bus && (
+#if defined(__LINUX_SPI_H)
+	    d->bus == &spi_bus_type ||
+#endif
+#if defined(_LINUX_I2C_H)
+	    d->bus == &i2c_bus_type ||
+#endif
+	    d->bus == &platform_bus_type
+	    )) {
+		struct sw_priv *hw_priv;
+
+		hw_priv = dev_get_drvdata(d);
+		if (port && hw_priv->phydev) {
+			struct phy_priv *phydata;
+
+			phydata = hw_priv->phydev->priv;
+			*port = phydata->port;
+		}
+		*proc_sem = &hw_priv->proc_sem;
+		*sw = &hw_priv->sw;
+	} else {
+		struct net_device *dev;
+		struct ksz_port *p = NULL;
+
+		dev = to_net_dev(d);
+		get_sysfs_data(dev, proc_sem, &p);
+		*sw = p->sw;
+		if (port)
+			*port = p;
+	}
+}
+
+#define get_private_data	get_private_data_
+#endif
+
+#ifndef get_num_val
+static int get_num_val_(const char *buf)
+{
+	int num = -1;
+
+	if ('0' == buf[0] && 'x' == buf[1])
+		sscanf(&buf[2], "%x", (unsigned int *) &num);
+	else if ('0' == buf[0] && 'b' == buf[1]) {
+		int i = 2;
+
+		num = 0;
+		while (buf[i]) {
+			num <<= 1;
+			num |= buf[i] - '0';
+			i++;
+		}
+	} else if ('0' == buf[0] && 'd' == buf[1])
+		sscanf(&buf[2], "%u", &num);
+	else
+		sscanf(buf, "%d", &num);
+	return num;
+}  /* get_num_val */
+
+#define get_num_val		get_num_val_
+#endif
+
+static int alloc_dev_attr(struct attribute **attrs, size_t attr_size, int item,
+	struct ksz_dev_attr **ksz_attrs, struct attribute ***item_attrs,
+	char *item_name, struct ksz_dev_attr **attrs_ptr)
+{
+	struct attribute **attr_ptr;
+	struct device_attribute *dev_attr;
+	struct ksz_dev_attr *new_attr;
+
+	*item_attrs = kmalloc(attr_size * sizeof(void *), GFP_KERNEL);
+	if (!*item_attrs)
+		return -ENOMEM;
+
+	attr_size--;
+	attr_size *= sizeof(struct ksz_dev_attr);
+	*ksz_attrs = *attrs_ptr;
+	*attrs_ptr += attr_size / sizeof(struct ksz_dev_attr);
+
+	new_attr = *ksz_attrs;
+	attr_ptr = *item_attrs;
+	while (*attrs != NULL) {
+		if (item_name && !strcmp((*attrs)->name, item_name))
+			break;
+		dev_attr = container_of(*attrs, struct device_attribute, attr);
+		memcpy(new_attr, dev_attr, sizeof(struct device_attribute));
+		strncpy(new_attr->dev_name, (*attrs)->name, DEV_NAME_SIZE);
+		if (10 <= item && item <= 15)
+			new_attr->dev_name[0] = item - 10 + 'a';
+		else
+			new_attr->dev_name[0] = item + '0';
+		new_attr->dev_attr.attr.name = new_attr->dev_name;
+		*attr_ptr = &new_attr->dev_attr.attr;
+		new_attr++;
+		attr_ptr++;
+		attrs++;
+	}
+	*attr_ptr = NULL;
+	return 0;
+}
+
+#endif
diff -Napur linux-linux4microchip-2022.10-orig/drivers/net/ethernet/micrel/Makefile linux-linux4microchip-2022.10/drivers/net/ethernet/micrel/Makefile
--- linux-linux4microchip-2022.10-orig/drivers/net/ethernet/micrel/Makefile	2022-11-09 04:48:42.000000000 -0800
+++ linux-linux4microchip-2022.10/drivers/net/ethernet/micrel/Makefile	2023-04-25 16:13:55.396163494 -0700
@@ -7,3 +7,11 @@ obj-$(CONFIG_KS8842) += ks8842.o
 obj-$(CONFIG_KS8851) += ks8851_common.o ks8851_spi.o
 obj-$(CONFIG_KS8851_MLL) += ks8851_common.o ks8851_par.o
 obj-$(CONFIG_KSZ884X_PCI) += ksz884x.o
+obj-$(CONFIG_SPI_KSZ8463) += spi-ksz8463.o
+obj-$(CONFIG_I2C_KSZ8863) += i2c-ksz8863.o
+obj-$(CONFIG_SPI_KSZ8863) += spi-ksz8863.o
+obj-$(CONFIG_SPI_KSZ8795) += spi-ksz8795.o
+obj-$(CONFIG_SPI_KSZ8895) += spi-ksz8895.o
+obj-$(CONFIG_I2C_KSZ9897) += i2c-ksz9897.o
+obj-$(CONFIG_SPI_KSZ9897) += spi-ksz9897.o
+obj-$(CONFIG_KSZ8462_HLI) += ksz8462_h.o
diff -Napur linux-linux4microchip-2022.10-orig/drivers/net/ethernet/micrel/micrel_ptp.c linux-linux4microchip-2022.10/drivers/net/ethernet/micrel/micrel_ptp.c
--- linux-linux4microchip-2022.10-orig/drivers/net/ethernet/micrel/micrel_ptp.c	1969-12-31 16:00:00.000000000 -0800
+++ linux-linux4microchip-2022.10/drivers/net/ethernet/micrel/micrel_ptp.c	2023-04-25 16:13:55.396163494 -0700
@@ -0,0 +1,288 @@
+/*
+ * PTP 1588 clock using Microchip PTP switch
+ *
+ * Copyright (C) 2010 OMICRON electronics GmbH
+ * Copyright (C) 2013-2015 Micrel, Inc.
+ * Copyright (C) 2015-2019 Microchip Technology Inc.
+ *
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of the GNU General Public License as published by
+ *  the Free Software Foundation; either version 2 of the License, or
+ *  (at your option) any later version.
+ *
+ *  This program is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  You should have received a copy of the GNU General Public License
+ *  along with this program; if not, write to the Free Software
+ *  Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+ */
+#include <linux/device.h>
+#include <linux/hrtimer.h>
+#include <linux/init.h>
+#include <linux/interrupt.h>
+#include <linux/kernel.h>
+#include <linux/module.h>
+#include <linux/of.h>
+#include <linux/of_platform.h>
+#include <linux/timex.h>
+#include <linux/io.h>
+
+#include <linux/ptp_clock_kernel.h>
+
+#define N_ALARM		0
+#define N_EXT_TS	2
+
+struct micrel_ptp_info {
+	struct ptp_clock *clock;
+	struct ptp_clock_info caps;
+	struct ptp_info *ptp;
+
+	u32 clock_events;
+	u64 alarm_interval;
+	u64 alarm_value;
+};
+
+#ifndef NO_PPS_DETECT
+static void ptp_event_pps(struct micrel_ptp_info *info)
+{
+	struct ptp_clock_event event;
+
+	event.type = PTP_CLOCK_PPS;
+	ptp_clock_event(info->clock, &event);
+}
+
+#if 0
+static void ptp_event_alarm(struct micrel_ptp_info *info)
+{
+	struct ptp_clock_event event;
+
+	event.type = PTP_CLOCK_ALARM;
+	event.index = 0;
+	event.timestamp = info->alarm_value;
+	ptp_clock_event(info->clock, &event);
+}
+#endif
+
+static void ptp_event_trigger(struct micrel_ptp_info *info, int index,
+	u32 sec, u32 nsec)
+{
+	struct ptp_clock_event event;
+
+	event.type = PTP_CLOCK_EXTTS;
+	event.index = index;
+	event.timestamp = sec;
+	event.timestamp *= NSEC_PER_SEC;
+	event.timestamp += nsec;
+	ptp_clock_event(info->clock, &event);
+}
+#endif
+
+/*
+ * PTP clock operations
+ */
+
+static int ptp_adjfreq(struct ptp_clock_info *clock, s32 ppb)
+{
+	struct ptp_clk_options clk_opt;
+	int output;
+	struct micrel_ptp_info *info =
+		container_of(clock, struct micrel_ptp_info, caps);
+	int err = 0;
+
+	output = 1;
+	clk_opt.sec = clk_opt.nsec = 0;
+	clk_opt.drift = -ppb;
+	clk_opt.interval = NANOSEC_IN_SEC;
+	err = proc_ptp_hw_access(info->ptp,
+		DEV_CMD_PUT, DEV_PTP_CLK, output,
+		&clk_opt, sizeof(clk_opt), NULL, &output,
+		true);
+	return err;
+}
+
+static int ptp_adjtime(struct ptp_clock_info *clock, s64 delta)
+{
+	struct ptp_clk_options clk_opt;
+	int output;
+	struct micrel_ptp_info *info =
+		container_of(clock, struct micrel_ptp_info, caps);
+	int neg_adj = 0;
+	int err = 0;
+
+	if (delta < 0) {
+		neg_adj = 1;
+		delta = -delta;
+	}
+	clk_opt.sec = div_u64_rem(delta, NSEC_PER_SEC, &clk_opt.nsec);
+	if (!neg_adj)
+		output = 2;
+	else
+		output = 1;
+	clk_opt.interval = 0;
+	err = proc_ptp_hw_access(info->ptp,
+		DEV_CMD_PUT, DEV_PTP_CLK, output,
+		&clk_opt, sizeof(clk_opt), NULL, &output,
+		true);
+	return err;
+}
+
+static int ptp_gettime(struct ptp_clock_info *clock,
+		       struct timespec64 *ts)
+{
+	struct ptp_clk_options clk_opt;
+	int output;
+	struct micrel_ptp_info *info =
+		container_of(clock, struct micrel_ptp_info, caps);
+	int err = 0;
+
+	if (!ts)
+		return -info->ptp->drift;
+	err = proc_ptp_hw_access(info->ptp,
+		DEV_CMD_GET, DEV_PTP_CLK, 0,
+		&clk_opt, sizeof(clk_opt), NULL, &output,
+		true);
+	if (err)
+		return err;
+	ts->tv_sec = clk_opt.sec;
+	ts->tv_nsec = clk_opt.nsec;
+	return 0;
+}
+
+static int ptp_settime(struct ptp_clock_info *clock,
+		       const struct timespec64 *ts)
+{
+	struct ptp_clk_options clk_opt;
+	int output;
+	struct micrel_ptp_info *info =
+		container_of(clock, struct micrel_ptp_info, caps);
+	int err = 0;
+
+	output = 0;
+	clk_opt.sec = ts->tv_sec;
+	clk_opt.nsec = ts->tv_nsec;
+	err = proc_ptp_hw_access(info->ptp,
+		DEV_CMD_PUT, DEV_PTP_CLK, output,
+		&clk_opt, sizeof(clk_opt), NULL, &output,
+		true);
+	return err;
+}
+
+static int ptp_enable(struct ptp_clock_info *clock,
+	struct ptp_clock_request *rq, int on)
+{
+	struct micrel_ptp_info *info =
+		container_of(clock, struct micrel_ptp_info, caps);
+	struct ptp_info *ptp = info->ptp;
+	u32 bit;
+
+	switch (rq->type) {
+	case PTP_CLK_REQ_EXTTS:
+		if (rq->extts.index >= 2)
+			return -EINVAL;
+		bit = 1 << rq->extts.index;
+		if (on)
+			ptp->clock_events |= bit;
+		else
+			ptp->clock_events &= ~bit;
+		return 0;
+
+	case PTP_CLK_REQ_PPS:
+		bit = 1 << 31;
+		if (on)
+			ptp->clock_events |= bit;
+		else
+			ptp->clock_events &= ~bit;
+		return 0;
+
+	default:
+		break;
+	}
+
+	return -EOPNOTSUPP;
+}
+
+static struct ptp_clock_info ptp_caps = {
+	.owner		= THIS_MODULE,
+	/* Only 16 characters. */
+	.name		= "Microchip clock",
+	.max_adj	= MAX_DRIFT_CORR,
+	.n_alarm	= N_ALARM,
+	.n_ext_ts	= N_EXT_TS,
+	.n_per_out	= 0,
+	.pps		= 1,
+	.adjfreq	= ptp_adjfreq,
+	.adjtime	= ptp_adjtime,
+	.gettime64	= ptp_gettime,
+	.settime64	= ptp_settime,
+	.enable		= ptp_enable,
+};
+
+static int micrel_ptp_get_ts_info(struct ptp_info *ptp,
+	struct ethtool_ts_info *info)
+{
+	struct micrel_ptp_info *clock_info = ptp->clock_info;
+
+	info->so_timestamping = SOF_TIMESTAMPING_TX_HARDWARE |
+		SOF_TIMESTAMPING_RX_HARDWARE |
+		SOF_TIMESTAMPING_RAW_HARDWARE;
+	if (clock_info && clock_info->clock)
+		info->phc_index = ptp_clock_index(clock_info->clock);
+	else
+		info->phc_index = -1;
+	info->tx_types = (1 << HWTSTAMP_TX_OFF) | (1 << HWTSTAMP_TX_ON);
+	info->rx_filters = (1 << HWTSTAMP_FILTER_NONE) |
+		(1 << HWTSTAMP_FILTER_ALL);
+	return 0;
+}
+
+static int micrel_ptp_probe(struct ptp_info *ptp)
+{
+	struct micrel_ptp_info *info;
+#if 0
+	struct timespec now;
+#endif
+	int err = -ENOMEM;
+
+	info = kzalloc(sizeof(*info), GFP_KERNEL);
+	if (!info)
+		goto no_memory;
+
+	err = -ENODEV;
+
+	info->caps = ptp_caps;
+
+#if 0
+	getnstimeofday(&now);
+	ptp_settime(&info->caps, &now);
+#endif
+
+	info->clock = ptp_clock_register(&info->caps, ptp->dev_parent);
+	if (IS_ERR(info->clock)) {
+		err = PTR_ERR(info->clock);
+		goto no_clock;
+	}
+	info->ptp = ptp;
+	ptp->clock_info = info;
+
+	return 0;
+
+no_clock:
+	kfree(info);
+no_memory:
+	return err;
+}
+
+static int micrel_ptp_remove(struct ptp_info *ptp)
+{
+	struct micrel_ptp_info *info = ptp->clock_info;
+
+	ptp_clock_unregister(info->clock);
+	kfree(info);
+	ptp->clock_info = NULL;
+
+	return 0;
+}
+
diff -Napur linux-linux4microchip-2022.10-orig/drivers/net/ethernet/micrel/mmrp.c linux-linux4microchip-2022.10/drivers/net/ethernet/micrel/mmrp.c
--- linux-linux4microchip-2022.10-orig/drivers/net/ethernet/micrel/mmrp.c	1969-12-31 16:00:00.000000000 -0800
+++ linux-linux4microchip-2022.10/drivers/net/ethernet/micrel/mmrp.c	2023-04-25 16:13:55.396163494 -0700
@@ -0,0 +1,177 @@
+/*
+ *	IEEE 802.1Q Multiple MAC Registration Protocol (MMRP)
+ *
+ *	Copyright (c) 2016-2017 Microchip Technology Inc.
+ *
+ *	Copyright (c) 2012 Massachusetts Institute of Technology
+ *
+ *	Adapted from code in net/8021q/vlan_gvrp.c
+ *	Copyright (c) 2008 Patrick McHardy <kaber@trash.net>
+ *
+ *	This program is free software; you can redistribute it and/or
+ *	modify it under the terms of the GNU General Public License
+ *	version 2 as published by the Free Software Foundation.
+ */
+
+
+#ifndef ETH_P_MMRP
+#define ETH_P_MMRP		0x88F6
+#endif
+
+#define MRP_MMRP_ADDRESS	{ 0x01, 0x80, 0xc2, 0x00, 0x00, 0x20 }
+
+enum mmrp_attributes {
+	MMRP_ATTR_INVALID,
+	MMRP_ATTR_SVC,
+	MMRP_ATTR_MAC,
+	__MMRP_ATTR_MAX
+};
+#define MMRP_ATTR_MAX	(__MMRP_ATTR_MAX - 1)
+
+static struct mrp_application mac_mrp_app __read_mostly = {
+	.type		= MRP_APPLICATION_MMRP,
+	.maxattr	= MMRP_ATTR_MAX,
+	.pkttype.type	= htons(ETH_P_MMRP),
+	.group_address	= MRP_MMRP_ADDRESS,
+	.version	= 0,
+};
+
+static int mmrp_attr_chk(u8 attrtype, u8 attrlen)
+{
+	if (MMRP_ATTR_MAC == attrtype && ETH_ALEN == attrlen)
+		return 0;
+	else if (MMRP_ATTR_SVC == attrtype && 1 == attrlen)
+		return 0;
+	if (MMRP_ATTR_MAC != attrtype &&
+	    MMRP_ATTR_SVC != attrtype)
+		return 1;
+	return -1;
+}
+
+static int mmrp_rxpdu(struct mrp_applicant *app, u8 *data, int len)
+{
+	int rc;
+
+	app->dry_run = 1;
+	rc = mrp_rx(app, data, len);
+
+	/* Discard entire PDU if malformed. */
+	if (rc < 0)
+		return 0;
+	app->dry_run = 0;
+	return mrp_rx(app, data, len);
+}
+
+static int mmrp_txpdu(struct mrp_applicant *app)
+{
+	int bytes;
+	int err;
+	u8 *msg_buf;
+	u8 *msg_eof;
+	u8 *msg_ptr;
+
+	if (!app->normal)
+		return 0;
+
+	err = mrp_pdu_init(app);
+	if (err < 0)
+		return err;
+
+	msg_buf = app->pdu->data;
+	msg_eof = msg_buf + app->dev->mtu;
+	msg_buf++;
+	msg_ptr = msg_buf;
+
+	if (app->normal & (1 << MMRP_ATTR_MAC)) {
+		err = mrp_tx(app, msg_ptr, msg_eof, &bytes,
+			     MMRP_ATTR_MAC, ETH_ALEN, MMRP_MAC_OPT_MAX,
+			     MMRP_MAC_MIN);
+
+		/* LeaveAll for MAC attribute type sent. */
+		if (bytes) {
+			msg_ptr += bytes;
+			app->lva.tx &= ~(1 << (MMRP_ATTR_MAC - 1));
+
+			/* No more space. */
+			if (-1 == err)
+				goto send;
+		}
+	}
+
+#if 1
+	if (app->normal & (1 << MMRP_ATTR_SVC)) {
+		err = mrp_tx(app, msg_ptr, msg_eof, &bytes,
+			     MMRP_ATTR_SVC, 1, MMRP_SVC_OPT_MAX, MMRP_SVC_MIN);
+
+		/* LeaveAll for SVC attribute type sent. */
+		if (bytes) {
+			msg_ptr += bytes;
+			app->lva.tx &= ~(1 << (MMRP_ATTR_SVC - 1));
+
+			/* No more space. */
+			if (-1 == err)
+				goto send;
+		}
+	}
+#endif
+	app->lva.tx = 0;
+
+send:
+	return mrp_txpdu(app, msg_buf, msg_ptr, msg_eof);
+}
+
+static int mmrp_req_join_mac(struct mrp_applicant *app, u8 *addr, u8 new_decl)
+{
+	if (new_decl)
+		new_decl = MRP_EVENT_NEW;
+	else
+		new_decl = MRP_EVENT_JOIN;
+	return mrp_req_new(app, addr, ETH_ALEN, MMRP_ATTR_MAC,
+		new_decl);
+}
+
+#if 0
+static int mmrp_req_join_svc(struct mrp_applicant *app, u8 svc)
+{
+	return mrp_req_new(app, &svc, 1, MMRP_ATTR_SVC,
+		MRP_EVENT_JOIN);
+}
+#endif
+
+static int mmrp_req_leave_mac(struct mrp_applicant *app, u8 *addr)
+{
+	return mrp_req_leave(app, addr, ETH_ALEN, MMRP_ATTR_MAC);
+}
+
+#if 0
+static int mmrp_req_leave_svc(struct mrp_applicant *app, u8 svc)
+{
+	return mrp_req_leave(app, &svc, 1, MMRP_ATTR_SVC);
+}
+#endif
+
+static void mmrp_req_set_mac(struct mrp_applicant *app, u8 *addr,
+			enum mrp_registrar_state state)
+{
+	mrp_req_set(app, addr, ETH_ALEN, MMRP_ATTR_MAC, state);
+}
+
+static void mmrp_init_application(struct mrp_applicant *app,
+				  void (*acton)(struct mrp_applicant *app,
+				  struct mrp_attr *attr),
+				  void (*cleanup)(struct mrp_applicant *app))
+{
+	app->attrval_inc = mrp_attrvalue_inc;
+	app->attr_chk = mmrp_attr_chk;
+	app->attr_cmp = mrp_attr_cmp;
+	app->attr_valid = mrp_attr_valid;
+	app->attr_size = mrp_attr_size;
+	app->attr_len = mrp_attr_len;
+	app->attr_type = mrp_attr_type;
+	app->rxpdu = mmrp_rxpdu;
+	app->txpdu = mmrp_txpdu;
+	app->acton = acton;
+	app->cleanup = cleanup;
+	app->normal = (1 << MMRP_ATTR_MAC);
+}
+
diff -Napur linux-linux4microchip-2022.10-orig/drivers/net/ethernet/micrel/mrp.c linux-linux4microchip-2022.10/drivers/net/ethernet/micrel/mrp.c
--- linux-linux4microchip-2022.10-orig/drivers/net/ethernet/micrel/mrp.c	1969-12-31 16:00:00.000000000 -0800
+++ linux-linux4microchip-2022.10/drivers/net/ethernet/micrel/mrp.c	2023-04-25 16:13:55.396163494 -0700
@@ -0,0 +1,2063 @@
+/*
+ *	IEEE 802.1Q Multiple Registration Protocol (MRP)
+ *
+ *	Copyright (c) 2016-2019 Microchip Technology Inc.
+ *
+ *	Copyright (c) 2012 Massachusetts Institute of Technology
+ *
+ *	Adapted from code in net/802/garp.c
+ *	Copyright (c) 2008 Patrick McHardy <kaber@trash.net>
+ *
+ *	This program is free software; you can redistribute it and/or
+ *	modify it under the terms of the GNU General Public License
+ *	version 2 as published by the Free Software Foundation.
+ */
+#include <linux/kernel.h>
+#include <linux/timer.h>
+#include <linux/skbuff.h>
+#include <linux/netdevice.h>
+#include <linux/etherdevice.h>
+#include <linux/rtnetlink.h>
+#include <linux/slab.h>
+#include <linux/module.h>
+#include "mrp.h"
+#include <asm/unaligned.h>
+
+#ifndef NETIF_F_HW_VLAN_CTAG_FILTER
+#define prandom_u32()		prandom32(app->rnd)
+#endif
+
+
+static unsigned int mrp_join_time __read_mostly = 200;
+module_param(mrp_join_time, uint, 0644);
+MODULE_PARM_DESC(mrp_join_time, "Join time in ms (default 200ms)");
+
+static unsigned int mrp_periodic_time __read_mostly = 1000;
+module_param(mrp_periodic_time, uint, 0644);
+MODULE_PARM_DESC(mrp_periodic_time, "Periodic time in ms (default 1s)");
+
+static unsigned int mrp_leave_time __read_mostly = 1000;
+static unsigned int mrp_lva_time __read_mostly = 10000;
+
+MODULE_LICENSE("GPL");
+
+static const u8
+mrp_applicant_state_table[MRP_APPLICANT_MAX + 1][MRP_EVENT_MAX + 1] = {
+	[MRP_APPLICANT_VO] = {
+		[MRP_EVENT_NEW]		= MRP_APPLICANT_VN,
+		[MRP_EVENT_JOIN]	= MRP_APPLICANT_VP,
+		[MRP_EVENT_LV]		= MRP_APPLICANT_VO,
+		[MRP_EVENT_TX]		= MRP_APPLICANT_VO,
+		[MRP_EVENT_TX_LA]	= MRP_APPLICANT_LO,
+		[MRP_EVENT_TX_LAF]	= MRP_APPLICANT_LO,
+		[MRP_EVENT_R_NEW]	= MRP_APPLICANT_VO,
+		[MRP_EVENT_R_JOIN_IN]	= MRP_APPLICANT_AO,
+		[MRP_EVENT_R_IN]	= MRP_APPLICANT_VO,
+		[MRP_EVENT_R_JOIN_MT]	= MRP_APPLICANT_VO,
+		[MRP_EVENT_R_MT]	= MRP_APPLICANT_VO,
+		[MRP_EVENT_R_LV]	= MRP_APPLICANT_LO,
+		[MRP_EVENT_R_LA]	= MRP_APPLICANT_LO,
+		[MRP_EVENT_REDECLARE]	= MRP_APPLICANT_LO,
+		[MRP_EVENT_PERIODIC]	= MRP_APPLICANT_VO,
+		[MRP_EVENT_LV_TIMER]	= MRP_APPLICANT_VO,
+		[MRP_EVENT_FLUSH]	= MRP_APPLICANT_VO,
+	},
+	[MRP_APPLICANT_VP] = {
+		[MRP_EVENT_NEW]		= MRP_APPLICANT_VN,
+		[MRP_EVENT_JOIN]	= MRP_APPLICANT_VP,
+		[MRP_EVENT_LV]		= MRP_APPLICANT_VO,
+		[MRP_EVENT_TX]		= MRP_APPLICANT_AA,
+		[MRP_EVENT_TX_LA]	= MRP_APPLICANT_AA,
+		[MRP_EVENT_TX_LAF]	= MRP_APPLICANT_VP,
+		[MRP_EVENT_R_NEW]	= MRP_APPLICANT_VP,
+		[MRP_EVENT_R_JOIN_IN]	= MRP_APPLICANT_AP,
+		[MRP_EVENT_R_IN]	= MRP_APPLICANT_VP,
+		[MRP_EVENT_R_JOIN_MT]	= MRP_APPLICANT_VP,
+		[MRP_EVENT_R_MT]	= MRP_APPLICANT_VP,
+		[MRP_EVENT_R_LV]	= MRP_APPLICANT_VP,
+		[MRP_EVENT_R_LA]	= MRP_APPLICANT_VP,
+		[MRP_EVENT_REDECLARE]	= MRP_APPLICANT_VP,
+		[MRP_EVENT_PERIODIC]	= MRP_APPLICANT_VP,
+		[MRP_EVENT_LV_TIMER]	= MRP_APPLICANT_VP,
+		[MRP_EVENT_FLUSH]	= MRP_APPLICANT_VP,
+	},
+	[MRP_APPLICANT_VN] = {
+		[MRP_EVENT_NEW]		= MRP_APPLICANT_VN,
+		[MRP_EVENT_JOIN]	= MRP_APPLICANT_VN,
+		[MRP_EVENT_LV]		= MRP_APPLICANT_LA,
+		[MRP_EVENT_TX]		= MRP_APPLICANT_AN,
+		[MRP_EVENT_TX_LA]	= MRP_APPLICANT_AN,
+		[MRP_EVENT_TX_LAF]	= MRP_APPLICANT_VN,
+		[MRP_EVENT_R_NEW]	= MRP_APPLICANT_VN,
+		[MRP_EVENT_R_JOIN_IN]	= MRP_APPLICANT_VN,
+		[MRP_EVENT_R_IN]	= MRP_APPLICANT_VN,
+		[MRP_EVENT_R_JOIN_MT]	= MRP_APPLICANT_VN,
+		[MRP_EVENT_R_MT]	= MRP_APPLICANT_VN,
+		[MRP_EVENT_R_LV]	= MRP_APPLICANT_VN,
+		[MRP_EVENT_R_LA]	= MRP_APPLICANT_VN,
+		[MRP_EVENT_REDECLARE]	= MRP_APPLICANT_VN,
+		[MRP_EVENT_PERIODIC]	= MRP_APPLICANT_VN,
+		[MRP_EVENT_LV_TIMER]	= MRP_APPLICANT_VN,
+		[MRP_EVENT_FLUSH]	= MRP_APPLICANT_VN,
+	},
+	[MRP_APPLICANT_AN] = {
+		[MRP_EVENT_NEW]		= MRP_APPLICANT_AN,
+		[MRP_EVENT_JOIN]	= MRP_APPLICANT_AN,
+		[MRP_EVENT_LV]		= MRP_APPLICANT_LA,
+		[MRP_EVENT_TX]		= MRP_APPLICANT_QA,
+		[MRP_EVENT_TX_LA]	= MRP_APPLICANT_QA,
+		[MRP_EVENT_TX_LAF]	= MRP_APPLICANT_VN,
+		[MRP_EVENT_R_NEW]	= MRP_APPLICANT_AN,
+		[MRP_EVENT_R_JOIN_IN]	= MRP_APPLICANT_AN,
+		[MRP_EVENT_R_IN]	= MRP_APPLICANT_AN,
+		[MRP_EVENT_R_JOIN_MT]	= MRP_APPLICANT_AN,
+		[MRP_EVENT_R_MT]	= MRP_APPLICANT_AN,
+		[MRP_EVENT_R_LV]	= MRP_APPLICANT_VN,
+		[MRP_EVENT_R_LA]	= MRP_APPLICANT_VN,
+		[MRP_EVENT_REDECLARE]	= MRP_APPLICANT_VN,
+		[MRP_EVENT_PERIODIC]	= MRP_APPLICANT_AN,
+		[MRP_EVENT_LV_TIMER]	= MRP_APPLICANT_AN,
+		[MRP_EVENT_FLUSH]	= MRP_APPLICANT_AN,
+	},
+	[MRP_APPLICANT_AA] = {
+		[MRP_EVENT_NEW]		= MRP_APPLICANT_VN,
+		[MRP_EVENT_JOIN]	= MRP_APPLICANT_AA,
+		[MRP_EVENT_LV]		= MRP_APPLICANT_LA,
+		[MRP_EVENT_TX]		= MRP_APPLICANT_QA,
+		[MRP_EVENT_TX_LA]	= MRP_APPLICANT_QA,
+		[MRP_EVENT_TX_LAF]	= MRP_APPLICANT_VP,
+		[MRP_EVENT_R_NEW]	= MRP_APPLICANT_AA,
+		[MRP_EVENT_R_JOIN_IN]	= MRP_APPLICANT_QA,
+		[MRP_EVENT_R_IN]	= MRP_APPLICANT_QA,
+		[MRP_EVENT_R_JOIN_MT]	= MRP_APPLICANT_AA,
+		[MRP_EVENT_R_MT]	= MRP_APPLICANT_AA,
+		[MRP_EVENT_R_LV]	= MRP_APPLICANT_VP,
+		[MRP_EVENT_R_LA]	= MRP_APPLICANT_VP,
+		[MRP_EVENT_REDECLARE]	= MRP_APPLICANT_VP,
+		[MRP_EVENT_PERIODIC]	= MRP_APPLICANT_AA,
+		[MRP_EVENT_LV_TIMER]	= MRP_APPLICANT_AA,
+		[MRP_EVENT_FLUSH]	= MRP_APPLICANT_AA,
+	},
+	[MRP_APPLICANT_QA] = {
+		[MRP_EVENT_NEW]		= MRP_APPLICANT_VN,
+		[MRP_EVENT_JOIN]	= MRP_APPLICANT_QA,
+		[MRP_EVENT_LV]		= MRP_APPLICANT_LA,
+		[MRP_EVENT_TX]		= MRP_APPLICANT_QA,
+		[MRP_EVENT_TX_LA]	= MRP_APPLICANT_QA,
+		[MRP_EVENT_TX_LAF]	= MRP_APPLICANT_VP,
+		[MRP_EVENT_R_NEW]	= MRP_APPLICANT_QA,
+		[MRP_EVENT_R_JOIN_IN]	= MRP_APPLICANT_QA,
+		[MRP_EVENT_R_IN]	= MRP_APPLICANT_QA,
+		[MRP_EVENT_R_JOIN_MT]	= MRP_APPLICANT_AA,
+		[MRP_EVENT_R_MT]	= MRP_APPLICANT_AA,
+		[MRP_EVENT_R_LV]	= MRP_APPLICANT_VP,
+		[MRP_EVENT_R_LA]	= MRP_APPLICANT_VP,
+		[MRP_EVENT_REDECLARE]	= MRP_APPLICANT_VP,
+		[MRP_EVENT_PERIODIC]	= MRP_APPLICANT_AA,
+		[MRP_EVENT_LV_TIMER]	= MRP_APPLICANT_QA,
+		[MRP_EVENT_FLUSH]	= MRP_APPLICANT_QA,
+	},
+	[MRP_APPLICANT_LA] = {
+		[MRP_EVENT_NEW]		= MRP_APPLICANT_VN,
+		[MRP_EVENT_JOIN]	= MRP_APPLICANT_AA,
+		[MRP_EVENT_LV]		= MRP_APPLICANT_LA,
+		[MRP_EVENT_TX]		= MRP_APPLICANT_VO,
+		[MRP_EVENT_TX_LA]	= MRP_APPLICANT_LO,
+		[MRP_EVENT_TX_LAF]	= MRP_APPLICANT_LO,
+		[MRP_EVENT_R_NEW]	= MRP_APPLICANT_LA,
+		[MRP_EVENT_R_JOIN_IN]	= MRP_APPLICANT_LA,
+		[MRP_EVENT_R_IN]	= MRP_APPLICANT_LA,
+		[MRP_EVENT_R_JOIN_MT]	= MRP_APPLICANT_LA,
+		[MRP_EVENT_R_MT]	= MRP_APPLICANT_LA,
+		[MRP_EVENT_R_LV]	= MRP_APPLICANT_LA,
+		[MRP_EVENT_R_LA]	= MRP_APPLICANT_LA,
+		[MRP_EVENT_REDECLARE]	= MRP_APPLICANT_LA,
+		[MRP_EVENT_PERIODIC]	= MRP_APPLICANT_LA,
+		[MRP_EVENT_LV_TIMER]	= MRP_APPLICANT_LA,
+		[MRP_EVENT_FLUSH]	= MRP_APPLICANT_LA,
+	},
+	[MRP_APPLICANT_AO] = {
+		[MRP_EVENT_NEW]		= MRP_APPLICANT_VN,
+		[MRP_EVENT_JOIN]	= MRP_APPLICANT_AP,
+		[MRP_EVENT_LV]		= MRP_APPLICANT_AO,
+		[MRP_EVENT_TX]		= MRP_APPLICANT_AO,
+		[MRP_EVENT_TX_LA]	= MRP_APPLICANT_LO,
+		[MRP_EVENT_TX_LAF]	= MRP_APPLICANT_LO,
+		[MRP_EVENT_R_NEW]	= MRP_APPLICANT_AO,
+		[MRP_EVENT_R_JOIN_IN]	= MRP_APPLICANT_QO,
+		[MRP_EVENT_R_IN]	= MRP_APPLICANT_AO,
+		[MRP_EVENT_R_JOIN_MT]	= MRP_APPLICANT_AO,
+		[MRP_EVENT_R_MT]	= MRP_APPLICANT_AO,
+		[MRP_EVENT_R_LV]	= MRP_APPLICANT_LO,
+		[MRP_EVENT_R_LA]	= MRP_APPLICANT_LO,
+		[MRP_EVENT_REDECLARE]	= MRP_APPLICANT_LO,
+		[MRP_EVENT_PERIODIC]	= MRP_APPLICANT_AO,
+		[MRP_EVENT_LV_TIMER]	= MRP_APPLICANT_AO,
+		[MRP_EVENT_FLUSH]	= MRP_APPLICANT_AO,
+	},
+	[MRP_APPLICANT_QO] = {
+		[MRP_EVENT_NEW]		= MRP_APPLICANT_VN,
+		[MRP_EVENT_JOIN]	= MRP_APPLICANT_QP,
+		[MRP_EVENT_LV]		= MRP_APPLICANT_QO,
+		[MRP_EVENT_TX]		= MRP_APPLICANT_QO,
+		[MRP_EVENT_TX_LA]	= MRP_APPLICANT_LO,
+		[MRP_EVENT_TX_LAF]	= MRP_APPLICANT_LO,
+		[MRP_EVENT_R_NEW]	= MRP_APPLICANT_QO,
+		[MRP_EVENT_R_JOIN_IN]	= MRP_APPLICANT_QO,
+		[MRP_EVENT_R_IN]	= MRP_APPLICANT_QO,
+		[MRP_EVENT_R_JOIN_MT]	= MRP_APPLICANT_AO,
+		[MRP_EVENT_R_MT]	= MRP_APPLICANT_AO,
+		[MRP_EVENT_R_LV]	= MRP_APPLICANT_LO,
+		[MRP_EVENT_R_LA]	= MRP_APPLICANT_LO,
+		[MRP_EVENT_REDECLARE]	= MRP_APPLICANT_LO,
+		[MRP_EVENT_PERIODIC]	= MRP_APPLICANT_QO,
+		[MRP_EVENT_LV_TIMER]	= MRP_APPLICANT_QO,
+		[MRP_EVENT_FLUSH]	= MRP_APPLICANT_QO,
+	},
+	[MRP_APPLICANT_AP] = {
+		[MRP_EVENT_NEW]		= MRP_APPLICANT_VN,
+		[MRP_EVENT_JOIN]	= MRP_APPLICANT_AP,
+		[MRP_EVENT_LV]		= MRP_APPLICANT_AO,
+		[MRP_EVENT_TX]		= MRP_APPLICANT_QA,
+		[MRP_EVENT_TX_LA]	= MRP_APPLICANT_QA,
+		[MRP_EVENT_TX_LAF]	= MRP_APPLICANT_VP,
+		[MRP_EVENT_R_NEW]	= MRP_APPLICANT_AP,
+		[MRP_EVENT_R_JOIN_IN]	= MRP_APPLICANT_QP,
+		[MRP_EVENT_R_IN]	= MRP_APPLICANT_AP,
+		[MRP_EVENT_R_JOIN_MT]	= MRP_APPLICANT_AP,
+		[MRP_EVENT_R_MT]	= MRP_APPLICANT_AP,
+		[MRP_EVENT_R_LV]	= MRP_APPLICANT_VP,
+		[MRP_EVENT_R_LA]	= MRP_APPLICANT_VP,
+		[MRP_EVENT_REDECLARE]	= MRP_APPLICANT_VP,
+		[MRP_EVENT_PERIODIC]	= MRP_APPLICANT_AP,
+		[MRP_EVENT_LV_TIMER]	= MRP_APPLICANT_AP,
+		[MRP_EVENT_FLUSH]	= MRP_APPLICANT_AP,
+	},
+	[MRP_APPLICANT_QP] = {
+		[MRP_EVENT_NEW]		= MRP_APPLICANT_VN,
+		[MRP_EVENT_JOIN]	= MRP_APPLICANT_QP,
+		[MRP_EVENT_LV]		= MRP_APPLICANT_QO,
+		[MRP_EVENT_TX]		= MRP_APPLICANT_QP,
+		[MRP_EVENT_TX_LA]	= MRP_APPLICANT_QA,
+		[MRP_EVENT_TX_LAF]	= MRP_APPLICANT_VP,
+		[MRP_EVENT_R_NEW]	= MRP_APPLICANT_QP,
+		[MRP_EVENT_R_JOIN_IN]	= MRP_APPLICANT_QP,
+		[MRP_EVENT_R_IN]	= MRP_APPLICANT_QP,
+		[MRP_EVENT_R_JOIN_MT]	= MRP_APPLICANT_AP,
+		[MRP_EVENT_R_MT]	= MRP_APPLICANT_AP,
+		[MRP_EVENT_R_LV]	= MRP_APPLICANT_VP,
+		[MRP_EVENT_R_LA]	= MRP_APPLICANT_VP,
+		[MRP_EVENT_REDECLARE]	= MRP_APPLICANT_VP,
+		[MRP_EVENT_PERIODIC]	= MRP_APPLICANT_AP,
+		[MRP_EVENT_LV_TIMER]	= MRP_APPLICANT_VP,
+		[MRP_EVENT_FLUSH]	= MRP_APPLICANT_VP,
+	},
+	[MRP_APPLICANT_LO] = {
+		[MRP_EVENT_NEW]		= MRP_APPLICANT_VN,
+		[MRP_EVENT_JOIN]	= MRP_APPLICANT_VP,
+		[MRP_EVENT_LV]		= MRP_APPLICANT_LO,
+		[MRP_EVENT_TX]		= MRP_APPLICANT_VO,
+		[MRP_EVENT_TX_LA]	= MRP_APPLICANT_LO,
+		[MRP_EVENT_TX_LAF]	= MRP_APPLICANT_LO,
+		[MRP_EVENT_R_NEW]	= MRP_APPLICANT_LO,
+		[MRP_EVENT_R_JOIN_IN]	= MRP_APPLICANT_LO,
+		[MRP_EVENT_R_IN]	= MRP_APPLICANT_LO,
+		[MRP_EVENT_R_JOIN_MT]	= MRP_APPLICANT_VO,
+		[MRP_EVENT_R_MT]	= MRP_APPLICANT_VO,
+		[MRP_EVENT_R_LV]	= MRP_APPLICANT_LO,
+		[MRP_EVENT_R_LA]	= MRP_APPLICANT_LO,
+		[MRP_EVENT_REDECLARE]	= MRP_APPLICANT_LO,
+		[MRP_EVENT_PERIODIC]	= MRP_APPLICANT_LO,
+		[MRP_EVENT_LV_TIMER]	= MRP_APPLICANT_LO,
+		[MRP_EVENT_FLUSH]	= MRP_APPLICANT_LO,
+	},
+};
+
+static const u8
+mrp_tx_action_table[MRP_APPLICANT_MAX + 1] = {
+	[MRP_APPLICANT_VO] = MRP_TX_ACTION_S_IN_OPTIONAL,
+	[MRP_APPLICANT_VP] = MRP_TX_ACTION_S_JOIN_IN,
+	[MRP_APPLICANT_VN] = MRP_TX_ACTION_S_NEW,
+	[MRP_APPLICANT_AN] = MRP_TX_ACTION_S_NEW,
+	[MRP_APPLICANT_AA] = MRP_TX_ACTION_S_JOIN_IN,
+	[MRP_APPLICANT_QA] = MRP_TX_ACTION_S_JOIN_IN_OPTIONAL,
+	[MRP_APPLICANT_LA] = MRP_TX_ACTION_S_LV,
+	[MRP_APPLICANT_AO] = MRP_TX_ACTION_S_IN_OPTIONAL,
+	[MRP_APPLICANT_QO] = MRP_TX_ACTION_S_IN_OPTIONAL,
+	[MRP_APPLICANT_AP] = MRP_TX_ACTION_S_JOIN_IN,
+	[MRP_APPLICANT_QP] = MRP_TX_ACTION_S_IN_OPTIONAL,
+	[MRP_APPLICANT_LO] = MRP_TX_ACTION_S_IN,
+};
+
+static const u8
+mrp_tx_la_action_table[MRP_APPLICANT_MAX + 1] = {
+	[MRP_APPLICANT_VO] = MRP_TX_ACTION_S_IN_OPTIONAL,
+	[MRP_APPLICANT_VP] = MRP_TX_ACTION_S_IN,
+	[MRP_APPLICANT_VN] = MRP_TX_ACTION_S_NEW,
+	[MRP_APPLICANT_AN] = MRP_TX_ACTION_S_NEW,
+	[MRP_APPLICANT_AA] = MRP_TX_ACTION_S_JOIN_IN,
+	[MRP_APPLICANT_QA] = MRP_TX_ACTION_S_JOIN_IN,
+	[MRP_APPLICANT_LA] = MRP_TX_ACTION_S_IN_OPTIONAL,
+	[MRP_APPLICANT_AO] = MRP_TX_ACTION_S_IN_OPTIONAL,
+	[MRP_APPLICANT_QO] = MRP_TX_ACTION_S_IN_OPTIONAL,
+	[MRP_APPLICANT_AP] = MRP_TX_ACTION_S_JOIN_IN,
+	[MRP_APPLICANT_QP] = MRP_TX_ACTION_S_JOIN_IN,
+	[MRP_APPLICANT_LO] = MRP_TX_ACTION_S_IN_OPTIONAL,
+};
+
+#ifdef DBG_MRP_APP
+static char *format_event(char *str, enum mrp_event event)
+{
+	switch (event) {
+	case MRP_EVENT_NEW:
+		strcpy(str, "New!");
+		break;
+	case MRP_EVENT_JOIN:
+		strcpy(str, "Join!");
+		break;
+	case MRP_EVENT_LV:
+		strcpy(str, "Lv!");
+		break;
+	case MRP_EVENT_TX:
+		strcpy(str, "tx!");
+		break;
+	case MRP_EVENT_TX_LA:
+		strcpy(str, "txLA!");
+		break;
+	case MRP_EVENT_TX_LAF:
+		strcpy(str, "txLAF!");
+		break;
+	case MRP_EVENT_R_NEW:
+		strcpy(str, "rNew!");
+		break;
+	case MRP_EVENT_R_JOIN_IN:
+		strcpy(str, "rJoinIn!");
+		break;
+	case MRP_EVENT_R_IN:
+		strcpy(str, "rIn!");
+		break;
+	case MRP_EVENT_R_JOIN_MT:
+		strcpy(str, "rJoinMt!");
+		break;
+	case MRP_EVENT_R_MT:
+		strcpy(str, "rMt!");
+		break;
+	case MRP_EVENT_R_LV:
+		strcpy(str, "rLv!");
+		break;
+	case MRP_EVENT_R_LA:
+		strcpy(str, "rLA!");
+		break;
+	case MRP_EVENT_REDECLARE:
+		strcpy(str, "Redec!");
+		break;
+	case MRP_EVENT_PERIODIC:
+		strcpy(str, "periodic!");
+		break;
+	case MRP_EVENT_PERIODIC_DISABLE:
+		strcpy(str, "peoridicDisable!");
+		break;
+	case MRP_EVENT_PERIODIC_ENABLE:
+		strcpy(str, "periodicEnable!");
+		break;
+	case MRP_EVENT_LV_TIMER:
+		strcpy(str, "lv!");
+		break;
+	case MRP_EVENT_LVA_TIMER:
+		strcpy(str, "lva!");
+		break;
+	case MRP_EVENT_FLUSH:
+		strcpy(str, "Flush!");
+		break;
+	default:
+		strcpy(str, "?");
+		break;
+	}
+	return str;
+}  /* format_event */
+
+static char *format_action(char *str, enum mrp_tx_action action)
+{
+	switch (action) {
+	case MRP_TX_ACTION_NONE:
+		strcpy(str, "-");
+		break;
+	case MRP_TX_ACTION_S_NEW:
+		strcpy(str, "sN");
+		break;
+	case MRP_TX_ACTION_S_JOIN_IN:
+		strcpy(str, "sJ");
+		break;
+	case MRP_TX_ACTION_S_JOIN_IN_OPTIONAL:
+		strcpy(str, "[sJ]");
+		break;
+	case MRP_TX_ACTION_S_IN:
+		strcpy(str, "s");
+		break;
+	case MRP_TX_ACTION_S_IN_OPTIONAL:
+		strcpy(str, "[s]");
+		break;
+	case MRP_TX_ACTION_S_LV:
+		strcpy(str, "sL");
+		break;
+	default:
+		strcpy(str, "?");
+		break;
+	}
+	return str;
+}  /* format_action */
+
+static char *format_app_state(char *str, enum mrp_applicant_state state)
+{
+	switch (state) {
+	case MRP_APPLICANT_VO:
+		strcpy(str, "VO");
+		break;
+	case MRP_APPLICANT_VP:
+		strcpy(str, "VP");
+		break;
+	case MRP_APPLICANT_VN:
+		strcpy(str, "VN");
+		break;
+	case MRP_APPLICANT_AN:
+		strcpy(str, "AN");
+		break;
+	case MRP_APPLICANT_AA:
+		strcpy(str, "AA");
+		break;
+	case MRP_APPLICANT_QA:
+		strcpy(str, "QA");
+		break;
+	case MRP_APPLICANT_LA:
+		strcpy(str, "LA");
+		break;
+	case MRP_APPLICANT_AO:
+		strcpy(str, "AO");
+		break;
+	case MRP_APPLICANT_QO:
+		strcpy(str, "QO");
+		break;
+	case MRP_APPLICANT_AP:
+		strcpy(str, "AP");
+		break;
+	case MRP_APPLICANT_QP:
+		strcpy(str, "QP");
+		break;
+	case MRP_APPLICANT_LO:
+		strcpy(str, "LO");
+		break;
+	default:
+		strcpy(str, "INV");
+		break;
+	}
+	return str;
+}  /* format_app_state */
+#endif
+
+#ifdef DBG_MRP_REG
+static char *format_reg_state(char *str, enum mrp_registrar_state state)
+{
+	switch (state) {
+	case MRP_REGISTRAR_MT:
+		strcpy(str, "MT");
+		break;
+	case MRP_REGISTRAR_IN:
+		strcpy(str, "IN");
+		break;
+	case MRP_REGISTRAR_LV:
+		strcpy(str, "LV");
+		break;
+	default:
+		strcpy(str, "INV");
+		break;
+	}
+	return str;
+}  /* format_reg_state */
+#endif
+
+static void mrp_attrvalue_inc(void *value, u8 len)
+{
+	u8 *v = (u8 *)value;
+
+	/* Add 1 to the last byte. If it becomes zero,
+	 * go to the previous byte and repeat.
+	 */
+	while (len > 0 && !++v[--len])
+		;
+}
+
+static int mrp_attr_cmp(const struct mrp_attr *attr,
+			 const void *value, u8 len, u8 type)
+{
+	if (attr->type != type)
+		return attr->type - type;
+	if (attr->len != len)
+		return attr->len - len;
+	return memcmp(attr->value, value, len);
+}
+
+static int mrp_attr_valid(u8 attrtype, const void *value)
+{
+	return true;
+}
+
+static u8 mrp_attr_size(u8 attrtype, u8 attrlen)
+{
+	return attrlen;
+}
+
+static u8 mrp_attr_len(u8 attrtype, u8 attrlen)
+{
+	return attrlen;
+}
+
+static u8 mrp_attr_type(struct mrp_attr *attr)
+{
+	return attr->type;
+}
+
+#if 0
+static int dbg_listener;
+static int dbg_talker;
+#endif
+
+static struct mrp_attr *mrp_attr_lookup(const struct mrp_applicant *app,
+					const void *value, u8 len, u8 type)
+{
+	struct rb_node *parent = app->mad.rb_node;
+	struct mrp_attr *attr;
+	int d;
+
+	while (parent) {
+		attr = rb_entry(parent, struct mrp_attr, node);
+		d = app->attr_cmp(attr, value, len, type);
+		if (d > 0)
+			parent = parent->rb_left;
+		else if (d < 0)
+			parent = parent->rb_right;
+		else
+			return attr;
+	}
+#if 0
+{
+const u8 *data = value;
+
+if ((len == 34 && !dbg_talker) || (len == 10 && !dbg_listener) ||
+(len != 34 && len != 10)) {
+dbg_msg("p:%d %d %d = ", app->port, app->app->type, type);
+for (d = 0; d < len; d++)
+dbg_msg("%02x ", data[d]);
+dbg_msg("\n");
+
+/* XMOS sends a talker with irrevant data. */
+if (34 == len && 0x20 == data[17] && 0x01 == data[19])
+++dbg_talker;
+if (10 == len)
+++dbg_listener;
+}
+}
+#endif
+	return NULL;
+}
+
+static struct mrp_attr *mrp_attr_create(struct mrp_applicant *app,
+					const void *value, u8 len, u8 type,
+					u8 size)
+{
+	struct rb_node *parent = NULL, **p = &app->mad.rb_node;
+	struct mrp_attr *attr;
+	int d;
+
+	while (*p) {
+		parent = *p;
+		attr = rb_entry(parent, struct mrp_attr, node);
+		d = app->attr_cmp(attr, value, len, type);
+		if (d > 0)
+			p = &parent->rb_left;
+		else if (d < 0)
+			p = &parent->rb_right;
+		else {
+			/* The attribute already exists; re-use it. */
+			return attr;
+		}
+	}
+	attr = kmalloc(sizeof(*attr) + size, GFP_ATOMIC);
+	if (!attr)
+		return attr;
+	attr->state = MRP_APPLICANT_VO;
+	attr->type  = type;
+	attr->len   = len;
+	memcpy(attr->value, value, len);
+
+	attr->new_state = MRP_APPLICANT_VO;
+	attr->action = MRP_TX_ACTION_NONE;
+	attr->fix_state = MRP_REGISTRAR_LV;
+	attr->reg_state = MRP_REGISTRAR_MT;
+	attr->notify = MRP_NOTIFY_NONE;
+	attr->aging = 0;
+
+	rb_link_node(&attr->node, parent, p);
+	rb_insert_color(&attr->node, &app->mad);
+	return attr;
+}
+
+static void mrp_attr_destroy(struct mrp_applicant *app, struct mrp_attr *attr)
+{
+#if 0
+	u8 len = app->attr_len(attr->type, attr->len);
+	u8 type = app->attr_type(attr);
+dbg_msg("%s p:%d [%d=%d:%02x]\n", __func__, app->port, app->app->type,
+type, attr->value[len - 1]);
+#endif
+	rb_erase(&attr->node, &app->mad);
+	kfree(attr);
+}
+
+static int mrp_pdu_init(struct mrp_applicant *app)
+{
+	struct sk_buff *skb;
+	struct mrp_pdu_hdr *ph;
+
+	skb = alloc_skb(app->dev->mtu + LL_RESERVED_SPACE(app->dev) + 8,
+			GFP_ATOMIC);
+	if (!skb)
+		return -ENOMEM;
+
+	skb->dev = app->dev;
+	skb->protocol = app->app->pkttype.type;
+	skb_reserve(skb, LL_RESERVED_SPACE(app->dev));
+	skb_reset_network_header(skb);
+	skb_reset_transport_header(skb);
+
+	ph = (struct mrp_pdu_hdr *)__skb_put(skb, sizeof(*ph));
+	ph->version = app->app->version;
+
+	app->pdu = skb;
+	return 0;
+}
+
+static int mrp_applicant_chk(struct mrp_attr *attr)
+{
+	int ret = 0;
+#if 0
+	u8 len = attr->len;
+
+if (25 == len || 34 == len || 10 == len)
+	len = 8;
+dbg_msg("aging [%02x] %d\n", attr->value[len - 1], attr->aging);
+#endif
+	if (attr->new_state != attr->state) {
+		if (MRP_APPLICANT_VO == attr->new_state) {
+			if (!attr->aging && attr->fix_state == MRP_REGISTRAR_LV)
+				attr->aging = 4;
+		} else if (MRP_APPLICANT_LO != attr->new_state)
+			attr->aging = 0;
+		attr->state = attr->new_state;
+	}
+	if (MRP_REGISTRAR_MT != attr->reg_state)
+		attr->aging = 0;
+	if (attr->action != MRP_TX_ACTION_NONE) {
+		if (attr->aging > 1)
+			--attr->aging;
+		attr->action = MRP_TX_ACTION_NONE;
+	}
+	if (1 == attr->aging)
+		return 1;
+	return ret;
+}
+
+#define MRP_OPT_MAX(attrlen)		\
+	((((2 + (attrlen)) + 1) * 2 - (2 + (attrlen))) * 3)
+
+#define MMRP_SVC_OPT_MAX		MRP_OPT_MAX(1)
+#define MMRP_MAC_OPT_MAX		MRP_OPT_MAX(6)
+#define MVRP_VID_OPT_MAX		MRP_OPT_MAX(2)
+
+static void mrp_prepare_tx(struct mrp_applicant *app, u8 attrtype,
+	int firstval_max)
+{
+	u8 firstval[20];
+	int firstval_cnt = 0;
+	int firstval_opt = 0;
+	int opt_cnt = 0;
+	struct rb_node *node, *next;
+	struct rb_node *opt = NULL;
+	struct mrp_attr *attr;
+	struct mrp_attr *first = NULL;
+	struct mrp_attr dummy;
+	struct mrp_attr *last = &dummy;
+	enum mrp_tx_action last_action = MRP_TX_ACTION_NONE;
+
+	memset(firstval, 0, sizeof(firstval));
+	for (node = rb_first(&app->mad);
+	     next = node ? rb_next(node) : NULL, node != NULL;
+	     node = next) {
+		attr = rb_entry(node, struct mrp_attr, node);
+		if (attrtype != attr->type)
+			continue;
+		if (MRP_TX_ACTION_NONE == attr->action)
+{
+dbg_msg(" no send\n");
+			continue;
+}
+
+		/* At least one attribute that is trying to send something. */
+		if (last) {
+			last = attr;
+
+			/* Rememeber action so that it can be restored. */
+			last_action = attr->action;
+		}
+		if (firstval_cnt) {
+			int firstval_next = 1;
+
+			app->attrval_inc(firstval, attr->len);
+			if (memcmp(firstval, attr->value, attr->len))
+				firstval_next = 0;
+			if (!firstval_next)
+				first = NULL;
+			else {
+				++firstval_cnt;
+				if (MRP_TX_ACTION_S_JOIN_IN_OPTIONAL ==
+				    attr->action ||
+				    MRP_TX_ACTION_S_IN_OPTIONAL ==
+				    attr->action) {
+					if (!opt) {
+						opt = node;
+						opt_cnt = 1;
+						firstval_opt =
+							firstval_cnt % 3;
+						if (!firstval_opt)
+							firstval_opt = 3;
+					} else {
+						++opt_cnt;
+						++firstval_opt;
+					}
+
+					/* Exceed allowed encoding. */
+					if (firstval_opt >= firstval_max)
+						first = NULL;
+				} else {
+					opt = NULL;
+					opt_cnt = 0;
+					firstval_opt = 0;
+				}
+			}
+		}
+		if (!first) {
+			while (opt_cnt) {
+				first = rb_entry(opt, struct mrp_attr, node);
+				first->action = MRP_TX_ACTION_NONE;
+				opt = rb_next(opt);
+				--opt_cnt;
+			}
+			opt = NULL;
+			firstval_opt = 0;
+
+			/* Skip if not needed to send. */
+			if (MRP_TX_ACTION_S_JOIN_IN_OPTIONAL == attr->action ||
+			    MRP_TX_ACTION_S_IN_OPTIONAL == attr->action) {
+				attr->action = MRP_TX_ACTION_NONE;
+				continue;
+			}
+			first = attr;
+			memcpy(firstval, attr->value, attr->len);
+			firstval_cnt = 1;
+
+			/* There is at least one attribute sending. */
+			last = NULL;
+		}
+	}
+
+	/* Need to send at least one for LeaveAll. */
+	if ((app->lva.tx & (1 << (attrtype - 1))) && last &&
+	    MRP_TX_ACTION_NONE == last->action)
+		last->action = last_action;
+}
+
+static u8 mrp_3pack_encode(u8 vect[], int *i)
+{
+	u8 val;
+
+	val = vect[0] * __MRP_VECATTR_EVENT_MAX * __MRP_VECATTR_EVENT_MAX;
+	val += vect[1] * __MRP_VECATTR_EVENT_MAX;
+	val += vect[2];
+	memset(vect, 0, 3);
+	*i = 0;
+	return val;
+}
+
+#ifdef CONFIG_KSZ_MSRP
+static u8 mrp_4pack_encode(u8 vect[])
+{
+	u8 val;
+
+	val = vect[0] * 64;
+	val += vect[1] * 16;
+	val += vect[2] * 4;
+	val += vect[3];
+	return val;
+}
+
+static void mrp_4pack_decode(u8 val, u8 vect[], int *i)
+{
+	vect[3] = val & 3;
+	val >>= 2;
+	vect[2] = val & 3;
+	val >>= 2;
+	vect[1] = val & 3;
+	val >>= 2;
+	vect[0] = val & 3;
+	*i = 0;
+}
+#endif
+
+static void mrp_encode_action(struct mrp_attr *attr, u8 *vectevt)
+{
+	switch (attr->action) {
+	case MRP_TX_ACTION_S_JOIN_IN:
+	case MRP_TX_ACTION_S_JOIN_IN_OPTIONAL:
+		*vectevt = MRP_REGISTRAR_IN == attr->reg_state ?
+			MRP_VECATTR_EVENT_JOIN_IN :
+			MRP_VECATTR_EVENT_JOIN_MT;
+		if (attr->fix_state != MRP_REGISTRAR_LV)
+			*vectevt = MRP_VECATTR_EVENT_JOIN_IN;
+		break;
+	case MRP_TX_ACTION_S_IN:
+	case MRP_TX_ACTION_S_IN_OPTIONAL:
+		*vectevt = MRP_REGISTRAR_IN == attr->reg_state ?
+			MRP_VECATTR_EVENT_IN :
+			MRP_VECATTR_EVENT_MT;
+		if (attr->fix_state != MRP_REGISTRAR_LV)
+			*vectevt = MRP_VECATTR_EVENT_IN;
+		break;
+	case MRP_TX_ACTION_S_NEW:
+		*vectevt = MRP_VECATTR_EVENT_NEW;
+		break;
+	case MRP_TX_ACTION_S_LV:
+		*vectevt = MRP_VECATTR_EVENT_LV;
+		break;
+	default:
+		break;
+	}
+}
+
+/* mrp_vecattr_hdr + attrlen + vaevents + 4 endmarks */
+#define MRP_MIN			(2 + 1 + 4)
+
+#define MVRP_VID_MIN		(2 + MRP_MIN)
+#define MMRP_SVC_MIN		(1 + MRP_MIN)
+#define MMRP_MAC_MIN		(6 + MRP_MIN)
+
+static int mrp_tx(struct mrp_applicant *app, u8 *msg_buf, u8 *msg_eof,
+	int *bytes, u8 attrtype, u8 attrlen, u8 attrmax, u8 msg_min)
+{
+	struct rb_node *node, *next;
+	struct mrp_attr *attr;
+	struct mrp_msg_hdr *mh;
+	struct mrp_vecattr_hdr *vah;
+	u8 firstval[20];
+	u8 vectevt[3];
+	int i = 0;
+	int len = 0;
+	int total = 0;
+	int num_break = false;
+	int node_break = false;
+	int need_vah = false;
+	u8 *msg_ptr = msg_buf;
+
+	*bytes = 0;
+
+	/* mrp_msg_hdr + mrp_vecattr_hdr + attrlen + vaevents + 4 endmarks */
+	if (msg_ptr > (msg_eof - (sizeof(struct mrp_msg_hdr) + msg_min)))
+		return -1;
+	memset(vectevt, 0, 3);
+
+	mrp_prepare_tx(app, attrtype, attrmax);
+
+	mh = (struct mrp_msg_hdr *) msg_ptr;
+	mh->attrtype = attrtype;
+	mh->attrlen = attrlen;
+	msg_ptr += sizeof(struct mrp_msg_hdr);
+
+	vah = (struct mrp_vecattr_hdr *) msg_ptr;
+	msg_ptr += sizeof(struct mrp_vecattr_hdr);
+
+	node = app->last_node;
+	if (node) {
+		attr = rb_entry(node, struct mrp_attr, node);
+		if (attrtype != attr->type)
+			return 0;
+	} else
+		node = rb_first(&app->mad);
+	for (;
+	     next = node ? rb_next(node) : NULL, node != NULL;
+	     node = next) {
+
+		/* Check if there is enough space for all required data. */
+		if (!len && msg_ptr > (msg_eof - msg_min)) {
+dbg_msg(" no space\n");
+			node_break = true;
+			break;
+		}
+
+		attr = rb_entry(node, struct mrp_attr, node);
+		if (attrtype != attr->type)
+			continue;
+
+		if (MRP_TX_ACTION_NONE == attr->action) {
+			if (mrp_applicant_chk(attr))
+				mrp_attr_destroy(app, attr);
+
+			/* In case there are events waiting. */
+			if (i)
+				i += 2;
+			goto val_end;
+		}
+		if (len) {
+			app->attrval_inc(firstval, attr->len);
+			if (memcmp(firstval, attr->value, attr->len)) {
+				/* Process with current node. */
+				next = node;
+				num_break = true;
+				if (i)
+					i += 2;
+				goto val_end;
+			}
+		}
+		if (need_vah) {
+			need_vah = false;
+			vah = (struct mrp_vecattr_hdr *) msg_ptr;
+			msg_ptr += sizeof(struct mrp_vecattr_hdr);
+		}
+		if (!len) {
+			memcpy(firstval, attr->value, attr->len);
+			memcpy(vah->firstattrvalue, firstval, attr->len);
+			msg_ptr += attr->len;
+		}
+		mrp_encode_action(attr, &vectevt[i]);
+		if (mrp_applicant_chk(attr))
+			mrp_attr_destroy(app, attr);
+
+		++i;
+		++len;
+		++total;
+
+val_end:
+		if (len && i >= 3) {
+			*msg_ptr++ = mrp_3pack_encode(vectevt, &i);
+			if (msg_ptr > msg_eof - (1 + 4)) {
+				num_break = true;
+				node_break = true;
+			} else if (len % 3) {
+				num_break = true;
+			}
+		}
+		if (num_break) {
+			put_unaligned(cpu_to_be16(len), &vah->lenflags);
+			if (app->lva.tx & (1 << (attrtype - 1)))
+				vah->lenflags |= MRP_VECATTR_HDR_FLAG_LA;
+			len = 0;
+			num_break = false;
+			need_vah = true;
+
+			if (msg_ptr > (msg_eof - msg_min)) {
+				node_break = true;
+				break;
+			}
+		}
+		if (node_break)
+			break;
+	}
+	if (len) {
+		if (i)
+			*msg_ptr++ = mrp_3pack_encode(vectevt, &i);
+		put_unaligned(cpu_to_be16(len), &vah->lenflags);
+		if (app->lva.tx & (1 << (attrtype - 1)))
+			vah->lenflags |= MRP_VECATTR_HDR_FLAG_LA;
+	}
+
+	/* Need to send LeaveAll with zero number of attributes. */
+	if (app->lva.tx & (1 << (attrtype - 1)) && !total) {
+		memset(vah->firstattrvalue, 0, attrlen);
+		msg_ptr += attrlen;
+		put_unaligned(0, &vah->lenflags);
+		vah->lenflags |= MRP_VECATTR_HDR_FLAG_LA;
+	}
+	app->last_node = node;
+
+	if (msg_ptr == msg_buf +
+	    sizeof(struct mrp_msg_hdr) +
+	    sizeof(struct mrp_vecattr_hdr)) {
+		return 0;
+	}
+	*msg_ptr++ = MRP_END_MARK;
+	*msg_ptr++ = MRP_END_MARK;
+
+	*bytes = msg_ptr - msg_buf;
+
+	/* Not all attributes sent. */
+	if (node)
+		return -1;
+	return 0;
+}
+
+static void mrp_leave_timer_arm(struct mrp_applicant *app)
+{
+	unsigned long delay;
+
+	delay = mrp_leave_time;
+#if 1
+	if (mrp_10_1_2f_hack)
+		delay -= 200;
+	if (fqtss_hack)
+		delay = 10000;
+#endif
+
+	/* Extend the timer in case one is already scheduled. */
+	mod_timer(&app->leave_timer,
+		  jiffies + msecs_to_jiffies(delay));
+	app->timer_arm.leave = 1;
+}
+
+static int mrp_registrar_event(struct mrp_applicant *app, struct mrp_attr *attr,
+	enum mrp_event event)
+{
+	enum mrp_registrar_state state = attr->reg_state;
+	enum mrp_notification notify = MRP_NOTIFY_NONE;
+
+	switch (event) {
+	case MRP_EVENT_R_LV:
+	case MRP_EVENT_R_LA:
+	case MRP_EVENT_REDECLARE:
+		if (MRP_REGISTRAR_IN == attr->reg_state) {
+			mrp_leave_timer_arm(app);
+			state = MRP_REGISTRAR_LV;
+		}
+		break;
+	case MRP_EVENT_R_NEW:
+		state = MRP_REGISTRAR_IN;
+		notify = MRP_NOTIFY_NEW;
+		break;
+	case MRP_EVENT_R_JOIN_IN:
+	case MRP_EVENT_R_JOIN_MT:
+		if (MRP_REGISTRAR_MT == attr->reg_state) {
+			state = MRP_REGISTRAR_IN;
+			notify = MRP_NOTIFY_JOIN;
+		} else if (MRP_REGISTRAR_LV == attr->reg_state)
+			state = MRP_REGISTRAR_IN;
+		if (attr->changed)
+			notify = MRP_NOTIFY_JOIN;
+		break;
+	case MRP_EVENT_LV_TIMER:
+		if (MRP_REGISTRAR_LV == attr->reg_state) {
+			state = MRP_REGISTRAR_MT;
+			notify = MRP_NOTIFY_LV;
+		}
+		break;
+	case MRP_EVENT_FLUSH:
+		if (MRP_REGISTRAR_MT != attr->reg_state)
+			notify = MRP_NOTIFY_LV;
+		state = MRP_REGISTRAR_MT;
+		break;
+	case MRP_EVENT_R_MT:
+		break;
+	default:
+		break;
+	}
+#ifdef DBG_MRP_REG
+	if (state != attr->reg_state || (notify && notify != attr->notify)) {
+		char last_reg_str[10];
+		char reg_str[10];
+		u8 len = app->attr_len(attr->type, attr->len);
+		u8 type = app->attr_type(attr);
+
+		format_reg_state(reg_str, state);
+		format_reg_state(last_reg_str, attr->reg_state);
+		if (notify != attr->notify && notify)
+dbg_msg(" ==>");
+dbg_msg(" reg_event p:%d [%d=%d:%02x] %s:%s %d\n", app->port, app->app->type,
+type, attr->value[len - 1],
+	last_reg_str, reg_str, notify);
+	}
+#endif
+
+	/* Either fixed IN or MT */
+	if (attr->fix_state != MRP_REGISTRAR_LV) {
+		state = attr->fix_state;
+		notify = MRP_NOTIFY_NONE;
+	}
+	attr->reg_state = state;
+	attr->notify = notify;
+	attr->changed = 0;
+	return notify != MRP_NOTIFY_NONE;
+}
+
+static int mrp_attr_event(struct mrp_applicant *app,
+			   struct mrp_attr *attr, enum mrp_event event)
+{
+	enum mrp_applicant_state state;
+	int tx = false;
+	int tx_req = false;
+#ifdef DBG_MRP_APP
+	char ev_str[10];
+	char last_app_str[10];
+	char last_new_str[10];
+	char app_str[10];
+	char new_str[10];
+	char act_str[10];
+	enum mrp_applicant_state last_state = attr->state;
+	enum mrp_tx_action last_action = attr->action;
+
+	format_event(ev_str, event);
+	format_app_state(last_new_str, attr->new_state);
+	format_app_state(last_app_str, attr->state);
+#endif
+
+	/* Do not change state if already sent. */
+	if (MRP_EVENT_TX_LAF == event && MRP_TX_ACTION_NONE == attr->action)
+		return tx_req;
+
+	/* Note #4. */
+	if (MRP_EVENT_R_JOIN_IN == event) {
+		if ((MRP_APPLICANT_VO == attr->state ||
+		    MRP_APPLICANT_VP == attr->state) && app->p2p_mac)
+			goto registrar;
+
+	/* Note #5. */
+	} else if (MRP_EVENT_R_IN == event) {
+		if (MRP_APPLICANT_AA == attr->state && !app->p2p_mac)
+			goto registrar;
+	}
+
+	state = mrp_applicant_state_table[attr->state][event];
+	if (state == MRP_APPLICANT_INVALID) {
+		WARN_ON(1);
+		return tx_req;
+	}
+
+	/* Note #8. */
+	if (MRP_APPLICANT_AN == attr->state && MRP_EVENT_TX == event &&
+	    MRP_REGISTRAR_IN != attr->reg_state)
+		state = MRP_APPLICANT_AA;
+
+	if (event == MRP_EVENT_TX) {
+		attr->action = mrp_tx_action_table[attr->state];
+		tx = true;
+	}
+	if (event == MRP_EVENT_TX_LA) {
+		attr->action = mrp_tx_la_action_table[attr->state];
+		tx = true;
+	}
+#ifdef MRP_BASIC
+	if (MRP_TX_ACTION_S_JOIN_IN_OPTIONAL == attr->action)
+		attr->action = MRP_TX_ACTION_S_JOIN_IN;
+	else if (MRP_TX_ACTION_S_IN_OPTIONAL == attr->action)
+		attr->action = MRP_TX_ACTION_S_IN;
+#endif
+	if (state != attr->state) {
+		switch (state) {
+		case MRP_APPLICANT_VN:
+		case MRP_APPLICANT_AN:
+		case MRP_APPLICANT_AA:
+		case MRP_APPLICANT_LA:
+		case MRP_APPLICANT_VP:
+		case MRP_APPLICANT_AP:
+		case MRP_APPLICANT_LO:
+#if 0
+if (MRP_EVENT_R_MT == event)
+dbg_msg("<%d:%d>", attr->state, state);
+#endif
+
+			/* Note #6. */
+			tx_req = true;
+			break;
+		case MRP_APPLICANT_VO:
+
+			/* Attribute is not being sent. */
+			if (!tx) {
+				if (!attr->aging)
+					attr->aging = 3;
+			}
+			break;
+		default:
+			break;
+		}
+	}
+	if (tx && !tx_req &&
+	    (MRP_TX_ACTION_S_JOIN_IN_OPTIONAL == attr->action ||
+	    MRP_TX_ACTION_S_IN_OPTIONAL == attr->action))
+		tx = false;
+
+	/* Update state after attribute is sent. */
+	attr->new_state = state;
+	if (!tx)
+		attr->state = state;
+#ifdef DBG_MRP_APP
+	if (last_state != state || last_action != attr->action) {
+		u8 len = app->attr_len(attr->type, attr->len);
+		u8 type = app->attr_type(attr);
+
+		format_app_state(new_str, attr->new_state);
+		format_app_state(app_str, attr->state);
+		format_action(act_str, attr->action);
+if (MRP_EVENT_R_LV == event)
+dbg_msg(" --> ");
+else if (app->rx)
+dbg_msg(" -> ");
+dbg_msg("attr_event p:%d [%d=%d:%02x] %s:%s %s ", app->port, app->app->type,
+type, attr->value[len - 1],
+last_app_str, last_new_str, ev_str);
+dbg_msg(" %s:%s %s %d\n", app_str, new_str, act_str, tx_req);
+	}
+#endif
+
+registrar:
+	switch (event) {
+	case MRP_EVENT_R_NEW:
+	case MRP_EVENT_R_JOIN_IN:
+	case MRP_EVENT_R_IN:
+	case MRP_EVENT_R_JOIN_MT:
+	case MRP_EVENT_R_MT:
+	case MRP_EVENT_R_LV:
+	case MRP_EVENT_R_LA:
+	case MRP_EVENT_REDECLARE:
+	case MRP_EVENT_LV_TIMER:
+	case MRP_EVENT_LVA_TIMER:
+	case MRP_EVENT_FLUSH:
+		if (mrp_registrar_event(app, attr, event))
+			app->acton(app, attr);
+		break;
+	default:
+		break;
+	}
+
+	/* Attribute has been changed.  Need to send. */
+	if (!app->rx && attr->changed) {
+		attr->changed = 0;
+		tx_req |= 1;
+	}
+
+	return tx_req;
+}
+
+#if 1
+static int max_lva;
+#endif
+
+static void mrp_lva_timer_arm(struct mrp_applicant *app)
+{
+	unsigned long delay;
+	unsigned long fixed;
+	unsigned long random;
+
+	/* Extend the timer in case one is already scheduled. */
+	/* LeaveAll timer can be 1.5 longer. */
+	app->timer_arm.lva = 1;
+	random = mrp_lva_time / 2;
+	fixed = msecs_to_jiffies(mrp_lva_time);
+	delay = (u64)msecs_to_jiffies(random) * prandom_u32() >> 32;
+	delay += fixed;
+#if 1
+	if (max_lva) {
+		delay = 3000;
+		max_lva = 0;
+	}
+#endif
+	mod_timer(&app->lva_timer, jiffies + delay);
+#if 0
+if (app->port < 3 && app->app->type == 2)
+dbg_msg("  s lva: %d=%lu %lu\n", app->port, delay, jiffies);
+#endif
+}
+
+static int mrp_lva_timer_event(struct mrp_applicant *app, enum mrp_event event)
+{
+	enum mrp_timer_state state = app->lva.state;
+	int tx_lva = 0;
+	int tx_req = false;
+
+	switch (event) {
+	case MRP_EVENT_TX:
+		if (MRP_TIMER_ACTIVE == state) {
+			tx_lva = 0xf;
+			state = MRP_TIMER_PASSIVE;
+		}
+		break;
+	case MRP_EVENT_R_LA:
+		state = MRP_TIMER_PASSIVE;
+#if 0
+if (app->port < 3 && app->app->type == 2) {
+dbg_msg("  rla  ");
+}
+#endif
+#if 1
+		if (fqtss_34_2_3_hack)
+			max_lva = 1;
+#endif
+		mrp_lva_timer_arm(app);
+		break;
+	case MRP_EVENT_LVA_TIMER:
+		state = MRP_TIMER_ACTIVE;
+#if 0
+if (app->port < 3 && app->app->type == 2)
+dbg_msg("  timer  ");
+#endif
+		mrp_lva_timer_arm(app);
+		break;
+	default:
+		return tx_req;
+	}
+	if (state != app->lva.state && MRP_TIMER_ACTIVE == state)
+		tx_req = true;
+	app->lva.state = state;
+#if 0
+if (app->lva.tx && app->port < 2)
+dbg_msg(" lva: %d=%x %x\n", app->port, app->lva.tx, tx_lva);
+#endif
+#if 0
+if (!app->lva.tx)
+#endif
+	app->lva.tx = tx_lva;
+	return tx_req;
+}
+
+static void mrp_periodic_timer_arm(struct mrp_applicant *app)
+{
+	/* Ignore if one is already scheduled. */
+	if (app->timer_arm.periodic)
+		return;
+	mod_timer(&app->periodic_timer,
+		  jiffies + msecs_to_jiffies(mrp_periodic_time));
+	app->timer_arm.periodic = 1;
+}
+
+static void mrp_periodic_event(struct mrp_applicant *app, enum mrp_event event)
+{
+	enum mrp_timer_state state = app->periodic.state;
+
+	switch (event) {
+	case MRP_EVENT_PERIODIC:
+		if (MRP_TIMER_ACTIVE == state)
+			mrp_periodic_timer_arm(app);
+		break;
+	case MRP_EVENT_PERIODIC_DISABLE:
+		state = MRP_TIMER_PASSIVE;
+		break;
+	case MRP_EVENT_PERIODIC_ENABLE:
+		if (MRP_TIMER_PASSIVE == state) {
+			state = MRP_TIMER_ACTIVE;
+			mrp_periodic_timer_arm(app);
+		}
+	default:
+		break;
+	}
+	app->periodic.state = state;
+}
+
+static void mrp_join_timer_arm(struct mrp_applicant *app)
+{
+	unsigned long delay;
+	unsigned long fixed;
+	unsigned long random;
+
+	/* Ignore if one is already scheduled. */
+	if (app->timer_arm.join)
+		return;
+	app->timer_arm.join = 1;
+	random = mrp_join_time / 10;
+	fixed = msecs_to_jiffies(mrp_join_time);
+	delay = (u64)msecs_to_jiffies(random) * prandom_u32() >> 32;
+	delay += fixed;
+	mod_timer(&app->join_timer, jiffies + delay);
+}
+
+static void mrp_mad_event(struct mrp_applicant *app, enum mrp_event event)
+{
+	struct rb_node *node, *next;
+	struct mrp_attr *attr;
+	enum mrp_event tx_event;
+	int tx_req = 0;
+
+	/* Flush! means leavealltimer!. */
+	if (MRP_EVENT_FLUSH == event)
+		tx_req |= mrp_lva_timer_event(app, MRP_EVENT_LVA_TIMER);
+
+	/* Do this first so that tx! becomes txLA! if LeaveAll is indicated. */
+	if (MRP_EVENT_TX == event ||
+	    (MRP_EVENT_R_LA == event && !app->lva.rx) ||
+	    MRP_EVENT_LVA_TIMER == event)
+		tx_req |= mrp_lva_timer_event(app, event);
+#if 1
+	if (MRP_EVENT_TX == event && app->lva.tx)
+		event = MRP_EVENT_TX_LA;
+#endif
+#if 0
+/*
+ * Do this first or after txLAF! ?
+ * If first the registrar values will always be empty, prompting the other to
+ * send back the values.  On the other hand, the receiving side can flush the
+ * registrar value and get the current one immediately without waiting for
+ * another transmit.
+ */
+
+	if (MRP_EVENT_TX_LA == event) {
+		for (node = rb_first(&app->mad);
+		     next = node ? rb_next(node) : NULL, node != NULL;
+		     node = next) {
+			attr = rb_entry(node, struct mrp_attr, node);
+			tx_req |= mrp_attr_event(app, attr, MRP_EVENT_R_LA);
+		}
+	}
+#endif
+	for (node = rb_first(&app->mad);
+	     next = node ? rb_next(node) : NULL, node != NULL;
+	     node = next) {
+		attr = rb_entry(node, struct mrp_attr, node);
+
+		/* Act on specific attribute type. */
+		if (app->lva_type) {
+			u8 attrtype = app->attr_type(attr);
+
+			if (attrtype != app->lva_type)
+				continue;
+		}
+		tx_event = event;
+#if 0
+		if (MRP_EVENT_TX == event && app->lva.tx) {
+			u8 attrtype = app->attr_type(attr);
+
+			if (app->lva.tx & (attrtype - 1))
+				tx_event = MRP_EVENT_TX_LA;
+		}
+#endif
+		tx_req |= mrp_attr_event(app, attr, tx_event);
+	}
+	if (MRP_EVENT_PERIODIC == event)
+		mrp_periodic_event(app, MRP_EVENT_PERIODIC);
+	if (MRP_EVENT_TX == event || MRP_EVENT_TX_LA == event) {
+		int rc;
+
+		event = 0;
+#if 1
+		app->lva.rx = app->lva.tx;
+#endif
+		rc = app->txpdu(app);
+
+		/* Not all attributes sent. */
+		if (app->last_node) {
+			if (app->lva.tx)
+				event = MRP_EVENT_TX_LAF;
+			else
+				tx_req |= 2;
+		} else if (app->lva.rx) {
+#if 1
+			event = MRP_EVENT_R_LA;
+#endif
+		}
+		if (event) {
+			for (node = rb_first(&app->mad);
+			     next = node ? rb_next(node) : NULL, node != NULL;
+			     node = next) {
+				attr = rb_entry(node, struct mrp_attr, node);
+				tx_req |= mrp_attr_event(app, attr, event);
+			}
+		}
+	}
+	if (tx_req) {
+		mrp_join_timer_arm(app);
+
+		/* Not continuing previous tx!. */
+		if (!(tx_req & 2))
+			app->last_node = NULL;
+	}
+}
+
+static void mrp_join_timer_exec(struct mrp_applicant *app)
+{
+	spin_lock(&app->lock);
+	app->timer_arm.join = 0;
+	mrp_mad_event(app, MRP_EVENT_TX);
+	spin_unlock(&app->lock);
+}
+
+static void mrp_join_timer_work(struct work_struct *work)
+{
+	struct mrp_applicant *app =
+		container_of(work, struct mrp_applicant, join_work);
+
+	mrp_join_timer_exec(app);
+}
+
+static void mrp_join_timer(struct timer_list *t)
+{
+	struct mrp_applicant *app = from_timer(app, t, join_timer);
+
+	/* Timer is running in interrupt context. */
+	schedule_work(&app->join_work);
+}
+
+static void mrp_periodic_timer_exec(struct mrp_applicant *app)
+{
+	spin_lock(&app->lock);
+	app->timer_arm.periodic = 0;
+	mrp_mad_event(app, MRP_EVENT_PERIODIC);
+	spin_unlock(&app->lock);
+}
+
+static void mrp_periodic_timer_work(struct work_struct *work)
+{
+	struct mrp_applicant *app =
+		container_of(work, struct mrp_applicant, periodic_work);
+
+	mrp_periodic_timer_exec(app);
+}
+
+static void mrp_periodic_timer(struct timer_list *t)
+{
+	struct mrp_applicant *app = from_timer(app, t, periodic_timer);
+
+	/* Periodic timer is disabled. */
+	if (MRP_TIMER_PASSIVE == app->periodic.state) {
+		app->timer_arm.periodic = 0;
+		return;
+	}
+
+	/* Timer is running in interrupt context.  Not necessary. */
+	schedule_work(&app->periodic_work);
+}
+
+static void mrp_leave_timer_exec(struct mrp_applicant *app)
+{
+	spin_lock(&app->lock);
+	app->timer_arm.leave = 0;
+	mrp_mad_event(app, MRP_EVENT_LV_TIMER);
+	spin_unlock(&app->lock);
+	app->cleanup(app);
+}
+
+static void mrp_leave_timer_work(struct work_struct *work)
+{
+	struct mrp_applicant *app =
+		container_of(work, struct mrp_applicant, leave_work);
+
+	mrp_leave_timer_exec(app);
+}
+
+static void mrp_leave_timer(struct timer_list *t)
+{
+	struct mrp_applicant *app = from_timer(app, t, leave_timer);
+
+	/* Timer is running in interrupt context.. */
+	schedule_work(&app->leave_work);
+}
+
+static void mrp_lva_timer_exec(struct mrp_applicant *app)
+{
+	int tx_req;
+
+	spin_lock(&app->lock);
+	app->timer_arm.lva = 0;
+#if 0
+if (app->port < 3 && app->app->type == 2) {
+dbg_msg(" lva: %d=%lu %lu\n", app->port, jiffies - app->rla_jiffies, jiffies);
+app->rla_jiffies = jiffies;
+}
+#endif
+	tx_req = mrp_lva_timer_event(app, MRP_EVENT_LVA_TIMER);
+	if (tx_req)
+		mrp_join_timer_arm(app);
+	spin_unlock(&app->lock);
+}
+
+static void mrp_lva_timer_work(struct work_struct *work)
+{
+	struct mrp_applicant *app =
+		container_of(work, struct mrp_applicant, lva_work);
+
+	mrp_lva_timer_exec(app);
+}
+
+static void mrp_lva_timer(struct timer_list *t)
+{
+	struct mrp_applicant *app = from_timer(app, t, lva_timer);
+
+	/* Timer is running in interrupt context.  Not necessary. */
+	schedule_work(&app->lva_work);
+}
+
+static void mrp_update_attr(struct mrp_applicant *app, u8 *attrvalue,
+	u8 attrlen, u8 attrtype, u8 vaevent)
+{
+	enum mrp_event event;
+	struct mrp_attr *attr;
+
+	switch (vaevent) {
+	case MRP_VECATTR_EVENT_NEW:
+		event = MRP_EVENT_R_NEW;
+		break;
+	case MRP_VECATTR_EVENT_JOIN_IN:
+		event = MRP_EVENT_R_JOIN_IN;
+		break;
+	case MRP_VECATTR_EVENT_IN:
+		event = MRP_EVENT_R_IN;
+		break;
+	case MRP_VECATTR_EVENT_JOIN_MT:
+		event = MRP_EVENT_R_JOIN_MT;
+		break;
+	case MRP_VECATTR_EVENT_MT:
+		event = MRP_EVENT_R_MT;
+		break;
+	case MRP_VECATTR_EVENT_LV:
+		event = MRP_EVENT_R_LV;
+		break;
+	default:
+		return;
+	}
+#ifdef DBG_MRP_
+{
+	char ev_str[10];
+	u8 len = app->attr_len(attrtype, attrlen);
+
+	format_event(ev_str, event);
+dbg_msg("> ");
+#if 1
+dbg_msg("%s p:%d [%d=%d:%02x] %s\n", __func__, app->port, app->app->type,
+attrtype, attrvalue[len - 1], ev_str);
+#endif
+}
+#endif
+	attr = mrp_attr_lookup(app, attrvalue, attrlen, attrtype);
+	if (attr == NULL) {
+
+		/* No need to create an attribute. */
+		if (MRP_VECATTR_EVENT_MT == vaevent ||
+		    MRP_VECATTR_EVENT_LV == vaevent)
+{
+#if 1
+if (MRP_VECATTR_EVENT_LV == vaevent)
+dbg_msg(" not create\n");
+#endif
+			return;
+}
+		attr = mrp_attr_create(app, attrvalue, attrlen, attrtype,
+				       app->attr_size(attrtype, attrlen));
+		if (!attr)
+			return;
+	}
+
+#if 1
+	/* AVnu test tool sends Lv and does not expect receiving talker
+	 * declaration in MSRP.c.35.4.5b.
+	 */
+	/* MSRP does not follow standard MRP operation.
+	 * Ignore rLv! if not registered.
+	 */
+	if (event == MRP_EVENT_R_LV &&
+	    app->app->type == MRP_APPLICATION_MSRP &&
+	    (attr->fix_state != MRP_REGISTRAR_LV ||
+	    attr->reg_state != MRP_REGISTRAR_IN))
+		return;
+#endif
+
+	if (app->attr_upd) {
+		attr->changed = app->attr_upd(attrvalue, attr, false);
+	}
+
+	app->rx = 1;
+	if (mrp_attr_event(app, attr, event))
+		mrp_join_timer_arm(app);
+	app->rx = 0;
+}
+
+static int mrp_parse_end_mark(u8 **msg_ptr, u8 *msg_eof)
+{
+	u8 *data = *msg_ptr;
+
+	if (*msg_ptr > (msg_eof - 2))
+{
+dbg_msg(" e1 %02x %02x; %p %p\n", data[0], data[1], *msg_ptr, msg_eof);
+		return -2;
+}
+
+	/* End mark can terminte list. */
+	if (MRP_END_MARK == data[0] && MRP_END_MARK == data[1]) {
+		*msg_ptr += 2;
+		return 0;
+	}
+
+	/* Keep parsing. */
+	return 1;
+}
+
+static int mrp_parse_events(struct mrp_applicant *app,
+	u8 **msg_ptr, u8 *msg_eof, u8 attrlen, u8 attrtype)
+{
+	struct mrp_vecattr_hdr *vah;
+	u16 valen;
+	u16 LA;
+	u8 firstval[20];
+	u8 vaevent;
+	u8 vaevents;
+
+	vah = (struct mrp_vecattr_hdr *) *msg_ptr;
+	valen = get_unaligned(&vah->lenflags);
+	LA = valen & ~MRP_VECATTR_HDR_LEN_MASK;
+
+	/* Only LeaveAll is understood in this version. */
+	if (app->ver_diff <= 0 && LA && LA != MRP_VECATTR_HDR_FLAG_LA)
+		return -1;
+	LA &= MRP_VECATTR_HDR_FLAG_LA;
+	valen = be16_to_cpu(valen & MRP_VECATTR_HDR_LEN_MASK);
+
+	*msg_ptr += sizeof(struct mrp_vecattr_hdr);
+
+	memcpy(firstval, vah->firstattrvalue, attrlen);
+	*msg_ptr += attrlen;
+
+	if (*msg_ptr + (valen + 2) / 3 > msg_eof)
+		return -1;
+
+	/* Do it once for each attribute type. */
+	if (!app->dry_run && LA && !(app->lva.rx & (1 << attrtype))) {
+		app->lva_type = attrtype;
+		mrp_mad_event(app, MRP_EVENT_R_LA);
+		app->lva_type = 0;
+		app->lva.rx |= (1 << attrtype);
+	}
+
+	/* In a VectorAttribute, the Vector contains events which are packed
+	 * three to a byte. We process one byte of the Vector at a time.
+	 */
+	while (valen > 0) {
+		vaevents = **msg_ptr;
+		*msg_ptr += sizeof(vaevents);
+
+		/* Extract and process the first event. */
+		vaevent = vaevents / (__MRP_VECATTR_EVENT_MAX *
+				      __MRP_VECATTR_EVENT_MAX);
+		if (vaevent >= __MRP_VECATTR_EVENT_MAX) {
+			/* The byte is malformed; stop processing. */
+			return -2;
+		}
+		if (!app->dry_run && app->attr_valid(attrtype, firstval))
+			mrp_update_attr(app, firstval, attrlen, attrtype,
+				vaevent);
+
+		/* If present, extract and process the second event. */
+		if (!--valen)
+			break;
+		vaevents %= (__MRP_VECATTR_EVENT_MAX *
+			     __MRP_VECATTR_EVENT_MAX);
+		vaevent = vaevents / __MRP_VECATTR_EVENT_MAX;
+		app->attrval_inc(firstval, attrlen);
+		if (!app->dry_run && app->attr_valid(attrtype, firstval))
+			mrp_update_attr(app, firstval, attrlen, attrtype,
+				vaevent);
+
+		/* If present, extract and process the third event. */
+		if (!--valen)
+			break;
+		vaevents %= __MRP_VECATTR_EVENT_MAX;
+		vaevent = vaevents;
+		app->attrval_inc(firstval, attrlen);
+		if (!app->dry_run && app->attr_valid(attrtype, firstval))
+			mrp_update_attr(app, firstval, attrlen, attrtype,
+				vaevent);
+		app->attrval_inc(firstval, attrlen);
+		--valen;
+	}
+	return 0;
+}
+
+static int mrp_parse_msg(struct mrp_applicant *app, u8 **msg_ptr, u8 *msg_eof)
+{
+	struct mrp_msg_hdr *mh;
+	int rc;
+
+	/*
+	 * End mark terminates the message.  Happen when there are filler
+	 * bytes in the frame.
+	 */
+	mh = (struct mrp_msg_hdr *) *msg_ptr;
+	if (MRP_END_MARK == mh->attrtype &&
+	    MRP_END_MARK == mh->attrlen) {
+		return -1;
+	}
+
+	/* Check whether the attribute type is understood. */
+	rc = app->attr_chk(mh->attrtype, mh->attrlen);
+	if (rc < 0)
+		return -1;
+
+	/* Attribute not understood but not in future protocol. */
+	if (rc && (app->ver_diff <= 0 || !app->msg_cnt))
+		return -1;
+
+	*msg_ptr += sizeof(struct mrp_msg_hdr);
+	while (*msg_ptr < (msg_eof - 3)) {
+		rc = mrp_parse_events(app, msg_ptr, msg_eof,
+			mh->attrlen, mh->attrtype);
+		if (rc < 0)
+			return rc;
+
+		/* End mark terminates the vector list. */
+		rc = mrp_parse_end_mark(msg_ptr, msg_eof);
+		if (rc <= 0) {
+			if (app->dry_run)
+				rc = 0;
+			return rc;
+		}
+	}
+	return 0;
+}
+
+static int mrp_rx(struct mrp_applicant *app, u8 *data, int len)
+{
+	struct mrp_pdu_hdr *ph;
+	u8 *msg_ptr;
+	u8 *msg_eof;
+	int rc;
+
+	ph = (struct mrp_pdu_hdr *) data;
+	app->ver_diff = ph->version - app->app->version;
+	app->msg_cnt = 0;
+
+	msg_ptr = (u8 *)(ph + 1);
+	msg_eof = msg_ptr + len - sizeof(struct mrp_pdu_hdr);
+
+	while (msg_ptr < (msg_eof - 2)) {
+
+		/* rLA! applies to the attribute type declared. */
+		app->lva.rx = 0;
+
+		rc = mrp_parse_msg(app, &msg_ptr, msg_eof);
+		if (rc < 0) {
+			if (app->dry_run && rc == -2)
+				rc = 0;
+			return rc;
+		}
+		app->msg_cnt++;
+
+		/* End mark terminates the frame. */
+		rc = mrp_parse_end_mark(&msg_ptr, msg_eof);
+		if (rc <= 0) {
+			if (app->dry_run)
+				rc = 0;
+			return rc;
+		}
+	}
+	return 0;
+}
+
+static int mrp_txpdu(struct mrp_applicant *app, u8 *msg_buf, u8 *msg_ptr,
+	u8 *msg_eof)
+{
+	/* Have something to send. */
+	if (msg_ptr != msg_buf) {
+		int rc;
+#ifndef DBG_MRP_RX
+		struct mrp_info *mrp = app->parent;
+#endif
+
+		/* Reset txLA flags if not txLAF. */
+		if (!app->last_node)
+			app->lva.tx = 0;
+		if (msg_ptr <= (msg_eof - 2)) {
+			*msg_ptr++ = MRP_END_MARK;
+			*msg_ptr++ = MRP_END_MARK;
+		}
+		skb_put(app->pdu, msg_ptr - msg_buf);
+		dev_hard_header(app->pdu, app->dev,
+			ntohs(app->app->pkttype.type),
+			app->group_address, app->src_addr,
+			app->pdu->len);
+#if 0
+		if (app->app->type == 2 && 1 == app->port && app->rla_jiffies) {
+if (jiffies - app->rla_jiffies > 20)
+dbg_msg("p:%d %d %lu: \n", app->port, app->app->type, jiffies - app->rla_jiffies);
+app->rla_jiffies = 0;
+		}
+#endif
+#ifdef DBG_MRP_TX
+		for (rc = 0; rc < app->pdu->len; rc++) {
+			dbg_msg("%02x ", app->pdu->data[rc]);
+			if ((rc % 16) == 15)
+				dbg_msg("\n");
+		}
+		if ((rc % 16))
+			dbg_msg("\n");
+#endif
+#ifdef DBG_MRP_RX
+		app->rxpdu(app, &app->pdu->data[14], app->pdu->len - 14);
+		kfree_skb(app->pdu);
+#else
+#ifdef MRP_PASSTHRU
+		kfree_skb(app->pdu);
+#else
+		rc = proc_mrp_xmit(mrp, app->port, app->pdu);
+#endif
+#endif
+		app->pdu = NULL;
+	} else {
+		kfree_skb(app->pdu);
+		app->pdu = NULL;
+	}
+	return 0;
+}
+
+static int mrp_req_new(struct mrp_applicant *app,
+	const void *value, u8 len, u8 type, u8 event)
+{
+	struct mrp_attr *attr;
+
+	spin_lock_bh(&app->lock);
+
+	/* Create automatically returns found attribute. */
+	attr = mrp_attr_create(app, value, len, type,
+			       app->attr_size(type, len));
+	if (!attr) {
+		spin_unlock_bh(&app->lock);
+		return -ENOMEM;
+	}
+
+	/* Some data in the attribute can be changed. */
+	if (app->attr_upd)
+		attr->changed = app->attr_upd(value, attr, true);
+	if (mrp_attr_event(app, attr, event))
+		mrp_join_timer_arm(app);
+	spin_unlock_bh(&app->lock);
+	return 0;
+}
+
+static int mrp_req_leave(struct mrp_applicant *app,
+	const void *value, u8 len, u8 type)
+{
+	struct mrp_attr *attr;
+	int rc;
+
+	spin_lock_bh(&app->lock);
+	attr = mrp_attr_lookup(app, value, len, type);
+	if (!attr) {
+		spin_unlock_bh(&app->lock);
+		return 0;
+	}
+	rc = mrp_attr_event(app, attr, MRP_EVENT_LV);
+	if (rc)
+		mrp_join_timer_arm(app);
+	spin_unlock_bh(&app->lock);
+	return rc;
+}
+
+static void mrp_req_set(struct mrp_applicant *app,
+	const void *value, u8 len, u8 type, enum mrp_registrar_state state)
+{
+	struct mrp_attr *attr;
+
+	spin_lock_bh(&app->lock);
+
+	/* Create automatically returns found attribute. */
+	attr = mrp_attr_create(app, value, len, type,
+			       app->attr_size(type, len));
+	if (!attr) {
+		spin_unlock_bh(&app->lock);
+		return;
+	}
+	if (attr->fix_state == state) {
+		spin_unlock_bh(&app->lock);
+		return;
+	}
+	attr->fix_state = state;
+	if (state != MRP_REGISTRAR_LV) {
+		attr->reg_state = state;
+		attr->notify = (state == MRP_REGISTRAR_IN) ?
+			MRP_NOTIFY_NEW : MRP_NOTIFY_LV;
+	} else if (MRP_APPLICANT_VO == attr->state)
+		attr->aging = 2;
+	spin_unlock_bh(&app->lock);
+	if (state != MRP_REGISTRAR_LV)
+		app->acton(app, attr);
+}
+
+#ifndef NETIF_F_HW_VLAN_CTAG_FILTER
+static struct rnd_state rnd;
+#endif
+
+static int mrp_init_applicant(struct mrp_port *port, void *mrp, u8 num,
+			      struct net_device *dev,
+			      struct mrp_application *appl)
+{
+	struct mrp_applicant *app;
+	int err;
+
+	err = -ENOMEM;
+	app = kzalloc(sizeof(*app), GFP_KERNEL);
+	if (!app)
+		goto err2;
+
+#ifndef NETIF_F_HW_VLAN_CTAG_FILTER
+	app->rnd = &rnd;
+#endif
+	app->parent = mrp;
+	app->port = num;
+	app->dev = dev;
+	app->app = appl;
+	app->mad = RB_ROOT;
+	app->p2p_mac = 1;
+	app->group_address = appl->group_address;
+	inc_mac_addr(app->src_addr, dev->dev_addr, 0);
+#if 0
+	inc_mac_addr(app->src_addr, dev->dev_addr, num + 1);
+#endif
+	spin_lock_init(&app->lock);
+	skb_queue_head_init(&app->queue);
+	rcu_assign_pointer(port->applicants[appl->type], app);
+	INIT_WORK(&app->join_work, mrp_join_timer_work);
+	timer_setup(&app->join_timer, mrp_join_timer, 0);
+
+	INIT_WORK(&app->periodic_work, mrp_periodic_timer_work);
+	timer_setup(&app->periodic_timer, mrp_periodic_timer, 0);
+	app->periodic.state = MRP_TIMER_PASSIVE;
+
+	INIT_WORK(&app->leave_work, mrp_leave_timer_work);
+	timer_setup(&app->leave_timer, mrp_leave_timer, 0);
+
+	INIT_WORK(&app->lva_work, mrp_lva_timer_work);
+	timer_setup(&app->lva_timer, mrp_lva_timer, 0);
+	app->lva.state = MRP_TIMER_PASSIVE;
+	mrp_lva_timer_arm(app);
+	return 0;
+
+err2:
+	return err;
+}
+
+static void mrp_uninit_applicant(struct mrp_port *port,
+	struct mrp_application *appl)
+{
+	struct mrp_applicant *app = rtnl_dereference(
+		port->applicants[appl->type]);
+
+	RCU_INIT_POINTER(port->applicants[appl->type], NULL);
+
+	/* Delete timer and generate a final TX event to flush out
+	 * all pending messages before the applicant is gone.
+	 */
+
+	/* TX event actually may arm several timers. */
+	spin_lock_bh(&app->lock);
+	mrp_mad_event(app, MRP_EVENT_TX);
+	spin_unlock_bh(&app->lock);
+
+	del_timer_sync(&app->join_timer);
+	del_timer_sync(&app->periodic_timer);
+	del_timer_sync(&app->leave_timer);
+	del_timer_sync(&app->lva_timer);
+	flush_work(&app->join_work);
+	flush_work(&app->periodic_work);
+	flush_work(&app->leave_work);
+	flush_work(&app->lva_work);
+
+	kfree_rcu(app, rcu);
+}
+
diff -Napur linux-linux4microchip-2022.10-orig/drivers/net/ethernet/micrel/mrp.h linux-linux4microchip-2022.10/drivers/net/ethernet/micrel/mrp.h
--- linux-linux4microchip-2022.10-orig/drivers/net/ethernet/micrel/mrp.h	1969-12-31 16:00:00.000000000 -0800
+++ linux-linux4microchip-2022.10/drivers/net/ethernet/micrel/mrp.h	2023-04-25 16:13:55.396163494 -0700
@@ -0,0 +1,226 @@
+#ifndef _NET_MRP_H
+#define _NET_MRP_H
+
+#define MRP_END_MARK		0x0
+
+struct mrp_pdu_hdr {
+	u8	version;
+};
+
+struct mrp_msg_hdr {
+	u8	attrtype;
+	u8	attrlen;
+};
+
+struct mrp_vecattr_hdr {
+	__be16	lenflags;
+	unsigned char	firstattrvalue[];
+#define MRP_VECATTR_HDR_LEN_MASK cpu_to_be16(0x1FFF)
+#define MRP_VECATTR_HDR_FLAG_LA cpu_to_be16(0x2000)
+};
+
+enum mrp_vecattr_event {
+	MRP_VECATTR_EVENT_NEW,
+	MRP_VECATTR_EVENT_JOIN_IN,
+	MRP_VECATTR_EVENT_IN,
+	MRP_VECATTR_EVENT_JOIN_MT,
+	MRP_VECATTR_EVENT_MT,
+	MRP_VECATTR_EVENT_LV,
+	__MRP_VECATTR_EVENT_MAX
+};
+
+struct mrp_skb_cb {
+	struct mrp_msg_hdr	*mh;
+	struct mrp_vecattr_hdr	*vah;
+	unsigned char		attrvalue[];
+};
+
+static inline struct mrp_skb_cb *mrp_cb(struct sk_buff *skb)
+{
+	BUILD_BUG_ON(sizeof(struct mrp_skb_cb) >
+		     sizeof_field(struct sk_buff, cb));
+	return (struct mrp_skb_cb *)skb->cb;
+}
+
+enum mrp_applicant_state {
+	MRP_APPLICANT_INVALID,
+	MRP_APPLICANT_VO,
+	MRP_APPLICANT_VP,
+	MRP_APPLICANT_VN,
+	MRP_APPLICANT_AN,
+	MRP_APPLICANT_AA,
+	MRP_APPLICANT_QA,
+	MRP_APPLICANT_LA,
+	MRP_APPLICANT_AO,
+	MRP_APPLICANT_QO,
+	MRP_APPLICANT_AP,
+	MRP_APPLICANT_QP,
+	MRP_APPLICANT_LO,
+	__MRP_APPLICANT_MAX
+};
+#define MRP_APPLICANT_MAX	(__MRP_APPLICANT_MAX - 1)
+
+enum mrp_registrar_state {
+	MRP_REGISTRAR_INVALID,
+	MRP_REGISTRAR_MT,
+	MRP_REGISTRAR_IN,
+	MRP_REGISTRAR_LV,
+	__MRP_REGISTRAR_MAX
+};
+#define MRP_REGISTRAR_MAX	(__MRP_REGISTRAR_MAX - 1)
+
+enum mrp_event {
+	MRP_EVENT_NEW,
+	MRP_EVENT_JOIN,
+	MRP_EVENT_LV,
+	MRP_EVENT_TX,
+	MRP_EVENT_TX_LA,
+	MRP_EVENT_TX_LAF,
+	MRP_EVENT_R_NEW,
+	MRP_EVENT_R_JOIN_IN,
+	MRP_EVENT_R_IN,
+	MRP_EVENT_R_JOIN_MT,
+	MRP_EVENT_R_MT,
+	MRP_EVENT_R_LV,
+	MRP_EVENT_R_LA,
+	MRP_EVENT_REDECLARE,
+	MRP_EVENT_PERIODIC,
+	MRP_EVENT_PERIODIC_DISABLE,
+	MRP_EVENT_PERIODIC_ENABLE,
+	MRP_EVENT_LV_TIMER,
+	MRP_EVENT_LVA_TIMER,
+	MRP_EVENT_FLUSH,
+	__MRP_EVENT_MAX
+};
+#define MRP_EVENT_MAX		(__MRP_EVENT_MAX - 1)
+
+enum mrp_tx_action {
+	MRP_TX_ACTION_NONE,
+	MRP_TX_ACTION_S_NEW,
+	MRP_TX_ACTION_S_JOIN_IN,
+	MRP_TX_ACTION_S_JOIN_IN_OPTIONAL,
+	MRP_TX_ACTION_S_IN,
+	MRP_TX_ACTION_S_IN_OPTIONAL,
+	MRP_TX_ACTION_S_LV,
+};
+
+enum mrp_notification {
+	MRP_NOTIFY_NONE,
+	MRP_NOTIFY_NEW,
+	MRP_NOTIFY_JOIN,
+	MRP_NOTIFY_LV,
+};
+
+enum mrp_timer_state {
+	MRP_TIMER_PASSIVE,
+	MRP_TIMER_ACTIVE,
+};
+
+struct mrp_attr {
+	struct rb_node			node;
+	enum mrp_applicant_state	state;
+	enum mrp_applicant_state	new_state;
+	enum mrp_tx_action		action;
+	enum mrp_registrar_state	fix_state;
+	enum mrp_registrar_state	reg_state;
+	enum mrp_notification		notify;
+	u8				aging;
+	u8				type;
+	u8				len;
+	u8				changed;
+	unsigned char			value[];
+};
+
+struct mrp_lva {
+	enum mrp_timer_state		state;
+	u8				rx;
+	u8				tx;
+};
+
+struct mrp_periodic {
+	enum mrp_timer_state		state;
+};
+
+enum mrp_applications {
+	MRP_APPLICATION_MVRP,
+	MRP_APPLICATION_MMRP,
+	MRP_APPLICATION_MSRP,
+	__MRP_APPLICATION_MAX
+};
+#define MRP_APPLICATION_MAX	(__MRP_APPLICATION_MAX - 1)
+
+struct mrp_application {
+	enum mrp_applications	type;
+	unsigned int		maxattr;
+	struct packet_type	pkttype;
+	unsigned char		group_address[ETH_ALEN];
+	u8			version;
+};
+
+struct mrp_applicant {
+	struct mrp_application	*app;
+	struct net_device	*dev;
+	struct mrp_lva		lva;
+	struct mrp_periodic	periodic;
+	struct work_struct	join_work;
+	struct work_struct	periodic_work;
+	struct work_struct	leave_work;
+	struct work_struct	lva_work;
+	struct timer_list	join_timer;
+	struct timer_list	periodic_timer;
+	struct timer_list	leave_timer;
+	struct timer_list	lva_timer;
+	struct {
+		u8 join:1;
+		u8 periodic:1;
+		u8 leave:1;
+		u8 lva:1;
+	} timer_arm;
+	u8 p2p_mac:1;
+	u8 rx:1;
+	u8 dry_run:1;
+	struct rb_node		*last_node;
+	void *parent;
+	u8 lva_type;
+	u8 msg_cnt;
+	u8 port;
+	int ver_diff;
+	unsigned long rla_jiffies;
+	u16 normal;
+	u8 *group_address;
+	u8 src_addr[ETH_ALEN];
+
+	void (*attrval_inc)(void *value, u8 len);
+	int (*attr_chk)(u8 attrtype, u8 attrlen);
+	int (*attr_cmp)(const struct mrp_attr *attr, const void *value,
+		u8 len, u8 type);
+	int (*attr_valid)(u8 attrtype, const void *value);
+	u8 (*attr_size)(u8 attrtype, u8 attrlen);
+	u8 (*attr_len)(u8 attrtype, u8 attrlen);
+	u8 (*attr_type)(struct mrp_attr *attr);
+	int (*attr_upd)(const void *value, struct mrp_attr *attr, int tx);
+	int (*rxpdu)(struct mrp_applicant *app, u8 *data, int len);
+	int (*txpdu)(struct mrp_applicant *app);
+	void (*acton)(struct mrp_applicant *app, struct mrp_attr *attr);
+	void (*cleanup)(struct mrp_applicant *app);
+
+	spinlock_t		lock;
+	struct sk_buff_head	queue;
+	struct sk_buff		*pdu;
+	struct rb_root		mad;
+	struct rcu_head		rcu;
+#ifndef NETIF_F_HW_VLAN_CTAG_FILTER
+	struct rnd_state	*rnd;
+#endif
+};
+
+struct mrp_port {
+	struct mrp_applicant __rcu	*applicants[MRP_APPLICATION_MAX + 1];
+	struct rcu_head			rcu;
+};
+
+int mrp_register_application(struct mrp_application *app);
+void mrp_unregister_application(struct mrp_application *app);
+
+
+#endif /* _NET_MRP_H */
diff -Napur linux-linux4microchip-2022.10-orig/drivers/net/ethernet/micrel/msrp.c linux-linux4microchip-2022.10/drivers/net/ethernet/micrel/msrp.c
--- linux-linux4microchip-2022.10-orig/drivers/net/ethernet/micrel/msrp.c	1969-12-31 16:00:00.000000000 -0800
+++ linux-linux4microchip-2022.10/drivers/net/ethernet/micrel/msrp.c	2023-04-25 16:13:55.396163494 -0700
@@ -0,0 +1,1237 @@
+/*
+ *	IEEE 802.1Qat Multiple Stream Registration Protocol (MSRP)
+ *
+ *	Copyright (c) 2016-2017 Microchip Technology Inc.
+ *
+ *	Copyright (c) 2012 Massachusetts Institute of Technology
+ *
+ *	Adapted from code in net/8021q/vlan_gvrp.c
+ *	Copyright (c) 2008 Patrick McHardy <kaber@trash.net>
+ *
+ *	This program is free software; you can redistribute it and/or
+ *	modify it under the terms of the GNU General Public License
+ *	version 2 as published by the Free Software Foundation.
+ */
+
+
+#ifndef ETH_P_MSRP
+#define ETH_P_MSRP		0x22EA
+#endif
+
+
+#define MRP_MSRP_ADDRESS	{ 0x01, 0x80, 0xc2, 0x00, 0x00, 0x0E }
+
+enum msrp_attributes {
+	MSRP_ATTR_INVALID,
+
+	/* Used in PDU but not in storage. */
+	MSRP_ATTR_TALKER,
+	MSRP_ATTR_TALKER_FAILED,
+	MSRP_ATTR_LISTENER,
+	MSRP_ATTR_DOMAIN,
+	__MSRP_ATTR_MAX
+};
+#define MSRP_ATTR_MAX	(__MSRP_ATTR_MAX - 1)
+
+enum msrp_listener_attributes {
+	MSRP_LISTENER_IGNORE,
+	MSRP_LISTENER_ASKFAILED,
+	MSRP_LISTENER_READY,
+	MSRP_LISTENER_READYFAIL,
+};
+
+/* Attribute length is 25. */
+struct srp_talker {
+	u8 id[8];
+	u8 dest[ETH_ALEN];
+	u16 vlan_id;
+	u16 max_frame_size;
+	u16 max_interval_frames;
+	u8 reserved:4;
+	u8 rank:1;
+	u8 priority:3;
+	u32 accumulated_latency;
+} __packed;
+
+/* Attribute length is 34. */
+struct srp_talker_failed {
+	u8 id[8];
+	u8 dest[ETH_ALEN];
+	u16 vlan_id;
+	u16 max_frame_size;
+	u16 max_interval_frames;
+	u8 reserved:4;
+	u8 rank:1;
+	u8 priority:3;
+	u32 accumulated_latency;
+	u8 bridge_id[8];
+	u8 failure_code;
+} __packed;
+
+/* Attribute length is 8. */
+struct srp_listener {
+	u8 id[8];
+	u8 substate;
+	u8 nextval;
+} __packed;
+
+/* Attribute length is 4. */
+struct srp_domain {
+	u8 class_id;
+	u8 class_priority;
+	u16 class_vid;
+} __packed;
+
+
+static struct mrp_application srp_mrp_app __read_mostly = {
+	.type		= MRP_APPLICATION_MSRP,
+	.maxattr	= MSRP_ATTR_MAX,
+	.pkttype.type	= htons(ETH_P_MSRP),
+	.group_address	= MRP_MSRP_ADDRESS,
+	.version	= 0,
+};
+
+static void msrp_attrvalue_inc(void *value, u8 len)
+{
+	if (sizeof(struct srp_talker) == len ||
+	    sizeof(struct srp_talker_failed) == len) {
+		u8 dest_len;
+		struct srp_talker *talker = value;
+
+		len = 8;
+		dest_len = 6;
+		while (len > 0 && !++talker->id[--len])
+			;
+		while (dest_len > 0 && !++talker->dest[--dest_len])
+			;
+	} else if (sizeof(struct srp_listener) == len) {
+		struct srp_listener *listener = value;
+
+		len = 8;
+		while (len > 0 && !++listener->id[--len])
+			;
+	} else if (sizeof(struct srp_domain) == len) {
+		struct srp_domain *domain = value;
+
+		domain->class_id++;
+		domain->class_priority++;
+	}
+else
+dbg_msg(" %s ?\n", __func__);
+}
+
+/* XMOS uses same stream id and a different destination address for sync? */
+#if 0
+#define TALKER_DEST_ADDR_LEN	6
+#else
+#define TALKER_DEST_ADDR_LEN	0
+#endif
+
+static int msrp_attrvalue_cmp(const void *value, const void *attr, u8 len)
+{
+	if (sizeof(struct srp_talker) == len ||
+	    sizeof(struct srp_talker_failed) == len)
+		return memcmp(value, attr, 8 + TALKER_DEST_ADDR_LEN);
+	else if (sizeof(struct srp_listener) == len)
+		return memcmp(value, attr, 8);
+	else if (sizeof(struct srp_domain) == len)
+		return memcmp(value, attr, 2);
+	return 0;
+}
+
+static int msrp_attr_cmp(const struct mrp_attr *attr,
+			 const void *value, u8 len, u8 type)
+{
+	if (MSRP_ATTR_TALKER == type && 25 == len) {
+		type = MSRP_ATTR_TALKER_FAILED;
+		len = 34;
+	}
+	if (attr->type != type)
+		return attr->type - type;
+	if (attr->len != len)
+		return attr->len - len;
+	return msrp_attrvalue_cmp(attr->value, value, len);
+}
+
+static int msrp_attr_chk(u8 attrtype, u8 attrlen)
+{
+	if ((MSRP_ATTR_LISTENER == attrtype &&
+	    sizeof(struct srp_listener) - 2 == attrlen) ||
+	    (MSRP_ATTR_TALKER == attrtype &&
+	    sizeof(struct srp_talker) == attrlen) ||
+	    (MSRP_ATTR_TALKER_FAILED == attrtype &&
+	    sizeof(struct srp_talker_failed) == attrlen) ||
+	    (MSRP_ATTR_DOMAIN == attrtype &&
+	    sizeof(struct srp_domain) == attrlen))
+		return 0;
+	if (MSRP_ATTR_LISTENER != attrtype &&
+	    MSRP_ATTR_TALKER != attrtype &&
+	    MSRP_ATTR_TALKER_FAILED != attrtype &&
+	    MSRP_ATTR_DOMAIN != attrtype)
+		return 1;
+	return -1;
+}
+
+static u8 msrp_attr_size(u8 attrtype, u8 attrlen)
+{
+	if (attrtype == MSRP_ATTR_LISTENER ||
+	    attrtype == MSRP_ATTR_TALKER ||
+	    attrtype == MSRP_ATTR_TALKER_FAILED)
+		return attrlen * 2;
+	return attrlen;
+}
+
+static u8 msrp_attr_len(u8 attrtype, u8 attrlen)
+{
+	if (MSRP_ATTR_LISTENER == attrtype)
+		return 8;
+	if (MSRP_ATTR_TALKER == attrtype)
+		return 8;
+	if (MSRP_ATTR_TALKER_FAILED == attrtype)
+		return 8;
+	if (MSRP_ATTR_DOMAIN == attrtype)
+		return 1;
+	return attrlen;
+}
+
+static u8 msrp_attr_type(struct mrp_attr *attr)
+{
+	u8 type = attr->type;
+
+	if (MSRP_ATTR_TALKER_FAILED == attr->type) {
+		struct srp_talker_failed *talker =
+			(struct srp_talker_failed *) attr->value;
+
+		if (!talker->failure_code)
+			type = MSRP_ATTR_TALKER;
+	}
+	return type;
+}
+
+static int msrp_attr_upd(const void *value, struct mrp_attr *attr, int tx)
+{
+	int ret = false;
+
+	/* Update talker failure code. */
+	if (MSRP_ATTR_TALKER_FAILED == attr->type) {
+		struct srp_talker_failed *old;
+		struct srp_talker_failed *old_rx = (struct srp_talker_failed *)
+			attr->value;
+		struct srp_talker_failed *old_tx = old_rx + 1;
+		const struct srp_talker_failed *new =
+			(struct srp_talker_failed *) value;
+
+		/* Initialize the second half after creation. */
+		if (memcmp(old_tx, old_rx, 8))
+			memcpy(old_tx, old_rx,
+				sizeof(struct srp_talker_failed));
+		old = tx ? old_tx : old_rx;
+		if (memcmp(old->dest, new->dest,
+			sizeof(struct srp_talker_failed) - 8)) {
+#if 0
+u8 *src = (u8 *) old->dest;
+u8 *dst = (u8 *) new->dest;
+int n = sizeof(struct srp_talker_failed) - 8;
+int i;
+for (i = 0; i < n; i++) {
+dbg_msg("%02x:%02x ", src[i], dst[i]);
+if ((i % 10) == 9)
+dbg_msg("\n");
+}
+dbg_msg("\n %p %d\n", value, n);
+#endif
+			if (attr->state == MRP_APPLICANT_QA)
+				attr->state = MRP_APPLICANT_AA;
+			memcpy(old->dest, new->dest,
+				sizeof(struct srp_talker_failed) - 8);
+			ret = true;
+		}
+
+	/* Update listener substate. */
+	} else if (MSRP_ATTR_LISTENER == attr->type) {
+		struct srp_listener *old;
+		struct srp_listener *old_rx = (struct srp_listener *)
+			attr->value;
+		struct srp_listener *old_tx = old_rx + 1;
+		const struct srp_listener *new = (struct srp_listener *)
+			value;
+
+		/* Initialize the second half after creation. */
+		if (memcmp(old_tx, old_rx, 8))
+			memcpy(old_tx, old_rx,
+				sizeof(struct srp_listener));
+		old = tx ? old_tx : old_rx;
+		if (old->substate != new->substate) {
+
+			/* Does changing require a NEW indication? */
+			if (attr->state == MRP_APPLICANT_QA)
+				attr->state = MRP_APPLICANT_AA;
+			old->substate = new->substate;
+			ret = true;
+		}
+
+	/* Update domain VID. */
+	} else if (MSRP_ATTR_DOMAIN == attr->type ) {
+		struct srp_domain *old = (struct srp_domain *) attr->value;
+		const struct srp_domain *new = (struct srp_domain *) value;
+
+		if (old->class_vid != new->class_vid) {
+			if (attr->state == MRP_APPLICANT_QA)
+				attr->state = MRP_APPLICANT_AA;
+			old->class_vid = new->class_vid;
+			ret = true;
+		}
+	}
+	return ret;
+}
+
+static int chk_listen_id(u8 *first, u8 *next)
+{
+	if (!memcmp(first, next, 6) && (first[6] < next[6] ||
+	    (first[6] == next[6] && first[7] <= next[7]))) {
+		int diff;
+
+		diff = next[6];
+		diff -= first[6];
+		diff <<= 8;
+		diff |= next[7];
+		diff -= first[7];
+		return diff;
+	} else
+		return 0x10000;
+}
+
+static int find_used_space(int cnt, int left, int cnt_per_byte)
+{
+	int num;
+
+	num = cnt - left;
+	num += (cnt_per_byte - 1);
+	num /= cnt_per_byte;
+	return num;
+}
+
+static int find_largest_cnt(int cnt)
+{
+	int num;
+	int attr_cnt;
+	int decl_cnt;
+	int attr_left;
+	int decl_left;
+
+	num = 21;
+	attr_left = (3 - (cnt % 3)) % 3;
+	decl_left = (4 - (cnt % 4)) % 4;
+	do {
+		--num;
+		attr_cnt = find_used_space(num, attr_left, 3);
+		decl_cnt = find_used_space(num, decl_left, 4);
+#ifdef DBG_MRP
+dbg_msg(" f: c=%d a=%d d=%d n=%d %d\n",
+cnt, attr_left, decl_left, num, attr_cnt + decl_cnt);
+#endif
+	} while (attr_cnt + decl_cnt > 11);
+	return num;
+}
+
+static void msrp_prepare_tx(struct mrp_applicant *app, u8 attrtype,
+	int firstval_max)
+{
+	u8 firstval[40];
+	int firstval_cnt = 0;
+	int firstval_opt = 0;
+	int opt_cnt = 0;
+	int listen_cnt = 0;
+	struct rb_node *node, *next;
+	struct rb_node *opt = NULL;
+	struct mrp_attr *attr;
+	struct mrp_attr *first = NULL;
+	struct mrp_attr dummy;
+	struct mrp_attr *last = &dummy;
+	enum mrp_tx_action last_action = MRP_TX_ACTION_NONE;
+	u8 attr_type = attrtype;
+	u8 attr_len;
+	struct srp_listener *listener;
+	void *value;
+
+	/* MSRP_ATTR_TALKER_FAILED type is used internally in storage. */
+	if (MSRP_ATTR_TALKER == attrtype)
+		attr_type = MSRP_ATTR_TALKER_FAILED;
+	memset(firstval, 0, sizeof(firstval));
+	for (node = rb_first(&app->mad);
+	     next = node ? rb_next(node) : NULL, node != NULL;
+	     node = next) {
+		attr = rb_entry(node, struct mrp_attr, node);
+		if (attr_type != attr->type)
+			continue;
+		attr_len = attr->len;
+		value = attr->value;
+		if (MSRP_ATTR_TALKER_FAILED == attr_type) {
+			struct srp_talker_failed *talker =
+				(struct srp_talker_failed *) attr->value;
+
+			talker++;
+			if ((MSRP_ATTR_TALKER == attrtype &&
+			    talker->failure_code) ||
+			    (MSRP_ATTR_TALKER_FAILED == attrtype &&
+			    !talker->failure_code))
+				continue;
+			value = talker;
+		}
+		if (mrp_10_5_1_hack && attr->action == MRP_TX_ACTION_S_LV) {
+dbg_msg(" no Lv sent\n");
+			attr->action = MRP_TX_ACTION_NONE;
+		}
+		if (MRP_TX_ACTION_NONE == attr->action)
+{
+dbg_msg(" no send %d\n", app->port);
+			continue;
+}
+
+		/* Used only for listener. */
+		listener = NULL;
+		if (MSRP_ATTR_LISTENER == attr_type) {
+			listener = (struct srp_listener *) attr->value;
+
+			listener++;
+			value = listener;
+
+			/* Actual attribute length. */
+			attr_len = 8;
+			listener->nextval = 0;
+		}
+
+		/* At least one attribute that is trying to send something. */
+		if (last) {
+			last = attr;
+
+			/* Rememeber action so that it can be restored. */
+			last_action = attr->action;
+		}
+		if (firstval_cnt) {
+			int firstval_next = 1;
+			int diff = 0;
+
+			if (MSRP_ATTR_LISTENER == attrtype)
+				diff = chk_listen_id(firstval, value);
+
+			app->attrval_inc(firstval, attr->len);
+			if (memcmp(firstval, value, attr_len))
+				firstval_next = 0;
+
+			if (MSRP_ATTR_LISTENER == attrtype) {
+				if (!firstval_next) {
+					int max;
+
+					max = find_largest_cnt(listen_cnt);
+					if (diff <= max) {
+						listen_cnt += diff;
+						listener->nextval = diff;
+						opt = NULL;
+						opt_cnt = 0;
+						firstval_opt = 0;
+						first = attr;
+						memcpy(firstval, value,
+							attr_len);
+						firstval_cnt = 1;
+						continue;
+					}
+				} else
+					++listen_cnt;
+			}
+			if (!firstval_next)
+				first = NULL;
+			else {
+				++firstval_cnt;
+				if (MRP_TX_ACTION_S_JOIN_IN_OPTIONAL ==
+				    attr->action ||
+				    MRP_TX_ACTION_S_IN_OPTIONAL ==
+				    attr->action) {
+					if (!opt) {
+						opt = node;
+						opt_cnt = 1;
+						firstval_opt =
+							firstval_cnt % 3;
+						if (!firstval_opt)
+							firstval_opt = 3;
+					} else {
+						++opt_cnt;
+						++firstval_opt;
+					}
+
+					/* Exceed allowed encoding. */
+					if (firstval_opt >= firstval_max)
+						first = NULL;
+				} else {
+					opt = NULL;
+					opt_cnt = 0;
+					firstval_opt = 0;
+				}
+			}
+		}
+		if (!first) {
+			while (opt_cnt) {
+				first = rb_entry(opt, struct mrp_attr, node);
+				first->action = MRP_TX_ACTION_NONE;
+				opt = rb_next(opt);
+				--opt_cnt;
+			}
+			opt = NULL;
+			firstval_opt = 0;
+
+			/* Skip if not needed to send. */
+			if (MRP_TX_ACTION_S_JOIN_IN_OPTIONAL == attr->action ||
+			    MRP_TX_ACTION_S_IN_OPTIONAL == attr->action) {
+				attr->action = MRP_TX_ACTION_NONE;
+				continue;
+			}
+			first = attr;
+			memcpy(firstval, value, attr_len);
+			firstval_cnt = 1;
+			if (MSRP_ATTR_LISTENER == attrtype)
+				listen_cnt = 1;
+
+			/* There is at least one attribute sending. */
+			last = NULL;
+		}
+	}
+
+	/* Need to send at least one for LeaveAll. */
+	if ((app->lva.tx & (1 << (attrtype - 1))) && last &&
+	    MRP_TX_ACTION_NONE == last->action)
+		last->action = last_action;
+}
+
+/* attrlist + mrp_vecattr_hdr + attrlen + vaevents + 4 endmarks */
+#define MSRP_MIN			(2 + 2 + 1 + 4)
+
+#define MSRP_TALKER_MIN			(25 + MRP_MIN)
+#define MSRP_TALKER_FAILED_MIN		(34 + MRP_MIN)
+#define MSRP_LISTENER_MIN		(9 + MRP_MIN)
+#define MSRP_DOMAIN_MIN			(4 + MRP_MIN)
+
+#define MSRP_TALKER_OPT_MAX		MRP_OPT_MAX(25)
+#define MSRP_TALKER_FAILED_OPT_MAX	MRP_OPT_MAX(34)
+#define MSRP_DOMAIN_OPT_MAX		MRP_OPT_MAX(4)
+#define MSRP_LISTENER_OPT_MAX		\
+	((((2 + 8) + 2) * 2 - (2 + 8)) / 2 * 3)
+
+static u8 *msrp_encode_substate(u8 *msg_ptr, u8 *list_decl, int len)
+{
+	int j;
+
+	memset(&list_decl[len], 0, len & 3);
+	for (j = 0; j < len; j += 4)
+		*msg_ptr++ = mrp_4pack_encode(&list_decl[j]);
+	return msg_ptr;
+}
+
+static int msrp_tx(struct mrp_applicant *app, u8 *msg_buf, u8 *msg_eof,
+	int *bytes, u8 attrtype, u8 attrlen, u8 attrmax, u8 msg_min)
+{
+	struct rb_node *node, *next;
+	struct mrp_attr *attr;
+	struct mrp_msg_hdr *mh;
+	struct mrp_vecattr_hdr *vah;
+	u8 firstval[40];
+	u8 vectevt[3];
+	int i = 0;
+	int len = 0;
+	int total = 0;
+	int num_break = false;
+	int node_break = false;
+	int need_vah = false;
+	u8 *msg_ptr = msg_buf;
+	u16 *attr_list;
+	u8 attr_type = attrtype;
+	int nextval;
+	u8 *list_decl;
+	void *value;
+
+	/* MSRP_ATTR_TALKER_FAILED type is used internally in storage. */
+	if (MSRP_ATTR_TALKER == attrtype)
+		attr_type = MSRP_ATTR_TALKER_FAILED;
+	*bytes = 0;
+
+	/* Check if previous attribute types are all sent. */
+	node = app->last_node;
+	if (node) {
+		attr = rb_entry(node, struct mrp_attr, node);
+		if (attr_type != attr->type)
+			return 0;
+	} else
+		node = rb_first(&app->mad);
+
+	/* Need one more byte for listener substate. */
+	if (MSRP_ATTR_LISTENER == attrtype)
+		msg_min++;
+
+	/* mrp_msg_hdr + mrp_vecattr_hdr + attrlen + vaevents + 4 endmarks */
+	if (msg_ptr > (msg_eof - (sizeof(struct mrp_msg_hdr) + 2 + msg_min)))
+		return -1;
+	memset(vectevt, 0, 3);
+
+	msrp_prepare_tx(app, attrtype, attrmax);
+
+	mh = (struct mrp_msg_hdr *) msg_ptr;
+	mh->attrtype = attrtype;
+	mh->attrlen = attrlen;
+	msg_ptr += sizeof(struct mrp_msg_hdr);
+
+	attr_list = (u16 *) msg_ptr;
+	msg_ptr += 2;
+
+	vah = (struct mrp_vecattr_hdr *) msg_ptr;
+	msg_ptr += sizeof(struct mrp_vecattr_hdr);
+
+	list_decl = kzalloc(1600 / 2, GFP_KERNEL);
+	for (;
+	     next = node ? rb_next(node) : NULL, node != NULL;
+	     node = next) {
+
+		/* Check if there is enough space for all required data. */
+		if (!len && msg_ptr > (msg_eof - msg_min)) {
+dbg_msg(" no space\n");
+			node_break = true;
+			break;
+		}
+
+		attr = rb_entry(node, struct mrp_attr, node);
+		if (attr_type != attr->type)
+			continue;
+
+		value = attr->value;
+		if (MSRP_ATTR_TALKER_FAILED == attr_type) {
+			struct srp_talker_failed *talker =
+				(struct srp_talker_failed *) attr->value;
+
+			talker++;
+			if ((MSRP_ATTR_TALKER == attrtype &&
+			    talker->failure_code) ||
+			    (MSRP_ATTR_TALKER_FAILED == attrtype &&
+			    !talker->failure_code))
+				continue;
+			value = talker;
+		}
+
+		/* See if next value is close enough for efficient encoding. */
+		nextval = 0;
+		if (MSRP_ATTR_LISTENER == attr_type) {
+			struct srp_listener *listener =
+				(struct srp_listener *) attr->value;
+
+			listener++;
+			value = listener;
+			nextval = listener->nextval;
+		}
+
+		if (MRP_TX_ACTION_NONE == attr->action) {
+			if (mrp_applicant_chk(attr))
+				mrp_attr_destroy(app, attr);
+
+#if 0
+if (i)
+dbg_msg(" no snd\n");
+#endif
+			/* In case there are events waiting. */
+			if (i)
+				i += 2;
+			goto val_end;
+		}
+
+		do {
+			if (need_vah) {
+				need_vah = false;
+				vah = (struct mrp_vecattr_hdr *) msg_ptr;
+				msg_ptr += sizeof(struct mrp_vecattr_hdr);
+			}
+			if (!len) {
+				memcpy(firstval, value, attrlen);
+				memcpy(vah->firstattrvalue, firstval, attrlen);
+				msg_ptr += attrlen;
+			} else {
+				app->attrval_inc(firstval, attr->len);
+			}
+			if (nextval)
+				--nextval;
+
+			/* Special case for listener. */
+			if (nextval > 0) {
+				/* Fill in dummy attribute. */
+				vectevt[i] = MRP_VECATTR_EVENT_NEW;
+				list_decl[len] = MSRP_LISTENER_IGNORE;
+#if 0
+dbg_msg(" iinc [%02x:%02x] ", firstval[6], firstval[7]);
+#endif
+			} else {
+#if 0
+if (MSRP_ATTR_LISTENER == attr_type)
+dbg_msg(" ne [%02x:%02x] ", firstval[6], firstval[7]);
+#endif
+				if (len && memcmp(firstval, value, attrlen)) {
+					/* Process with current node. */
+					next = node;
+					num_break = true;
+					if (i)
+						i += 2;
+					goto val_end;
+				}
+				mrp_encode_action(attr, &vectevt[i]);
+				if (MSRP_ATTR_LISTENER == attrtype) {
+					struct srp_listener *listener =
+						(struct srp_listener *) value;
+
+					list_decl[len] = listener->substate;
+				}
+				if (mrp_applicant_chk(attr))
+					mrp_attr_destroy(app, attr);
+			}
+
+			++i;
+			++len;
+			++total;
+
+val_end:
+			if (len && i >= 3) {
+				int left = (1 + 4);
+
+				if (MSRP_ATTR_LISTENER == attrtype)
+					left = (1 + (len + 3) / 4 + 4);
+#if 0
+dbg_msg(" vect:%d ", nextval);
+#endif
+				*msg_ptr++ = mrp_3pack_encode(vectevt, &i);
+				if (msg_ptr > msg_eof - left) {
+dbg_msg(" node break\n");
+					num_break = true;
+					node_break = true;
+					nextval = -1;
+				} else if (len % 3) {
+					num_break = true;
+				}
+			}
+			if (num_break) {
+#if 0
+dbg_msg(" num: %d ", nextval);
+#endif
+				if (MSRP_ATTR_LISTENER == attrtype)
+					msg_ptr =
+						msrp_encode_substate(msg_ptr,
+								     list_decl,
+								     len);
+				put_unaligned(cpu_to_be16(len), &vah->lenflags);
+				if (app->lva.tx & (1 << (attrtype - 1)))
+					vah->lenflags |= MRP_VECATTR_HDR_FLAG_LA;
+				len = 0;
+				num_break = false;
+				need_vah = true;
+
+				if (msg_ptr > (msg_eof - msg_min)) {
+					node_break = true;
+					break;
+				}
+if (nextval > 0)
+dbg_msg(" ?? next");
+nextval = -1;
+			}
+		} while (nextval > 0);
+		if (node_break)
+			break;
+	}
+	if (len) {
+		if (i)
+			*msg_ptr++ = mrp_3pack_encode(vectevt, &i);
+		if (MSRP_ATTR_LISTENER == attrtype)
+			msg_ptr = msrp_encode_substate(msg_ptr, list_decl, len);
+		put_unaligned(cpu_to_be16(len), &vah->lenflags);
+		if (app->lva.tx & (1 << (attrtype - 1)))
+			vah->lenflags |= MRP_VECATTR_HDR_FLAG_LA;
+	}
+
+	/* Need to send LeaveAll with zero number of attributes. */
+	if (app->lva.tx & (1 << (attrtype - 1)) && !total) {
+		memset(vah->firstattrvalue, 0, attrlen);
+		msg_ptr += attrlen;
+		put_unaligned(0, &vah->lenflags);
+		vah->lenflags |= MRP_VECATTR_HDR_FLAG_LA;
+	}
+	app->last_node = node;
+	kfree(list_decl);
+
+	if (msg_ptr == msg_buf +
+	    sizeof(struct mrp_msg_hdr) + 2 +
+	    sizeof(struct mrp_vecattr_hdr)) {
+		return 0;
+	}
+	*msg_ptr++ = MRP_END_MARK;
+	*msg_ptr++ = MRP_END_MARK;
+
+	*bytes = msg_ptr - msg_buf;
+	len = msg_ptr - (u8 *) attr_list;
+	*attr_list = htons(len - 2);
+
+	/* Not all attributes sent. */
+	if (node)
+		return -1;
+	return 0;
+}
+
+static int msrp_parse_events(struct mrp_applicant *app,
+	u8 **msg_ptr, u8 *msg_eof, u8 attrlen, u8 attrtype)
+{
+	struct mrp_vecattr_hdr *vah;
+	u16 valen;
+	u16 LA;
+	u8 firstval[40];
+	u8 vaevent;
+	u8 vaevents;
+	int add_len = 0;
+	u8 *decl_ptr = NULL;
+	u8 decl[4];
+	int i = 4;
+	int ignore;
+
+	vah = (struct mrp_vecattr_hdr *) *msg_ptr;
+	valen = get_unaligned(&vah->lenflags);
+	LA = valen & ~MRP_VECATTR_HDR_LEN_MASK;
+
+	/* Only LeaveAll is understood in this version. */
+	if (app->ver_diff <= 0 && LA && LA != MRP_VECATTR_HDR_FLAG_LA)
+		return -1;
+#if 0
+if (app->port < 3)
+dbg_msg("  rx: %d=%d %x %lu\n", app->port, attrtype, LA, jiffies);
+#endif
+
+	LA &= MRP_VECATTR_HDR_FLAG_LA;
+	valen &= MRP_VECATTR_HDR_LEN_MASK;
+	valen = be16_to_cpu(valen);
+
+	*msg_ptr += sizeof(struct mrp_vecattr_hdr);
+
+	if (MSRP_ATTR_LISTENER == attrtype)
+		add_len = (valen + 3) / 4;
+
+	/* Listener storage has extra stuff. */
+	if (attrlen < sizeof(struct srp_talker_failed))
+		memset(firstval, 0, sizeof(struct srp_talker_failed));
+	memcpy(firstval, vah->firstattrvalue, attrlen);
+	*msg_ptr += attrlen;
+
+	if (*msg_ptr + (valen + 2) / 3 + add_len > msg_eof)
+		return -1;
+
+	/* Do it once for each attribute type. */
+	if (!app->dry_run && LA && !(app->lva.rx & (1 << attrtype))) {
+#if 0
+app->rla_jiffies = jiffies;
+#endif
+		app->lva_type = attrtype;
+		mrp_mad_event(app, MRP_EVENT_R_LA);
+		app->lva_type = 0;
+		app->lva.rx |= (1 << attrtype);
+	}
+
+	if (!valen)
+		return 0;
+
+	if (MSRP_ATTR_LISTENER == attrtype) {
+		/* For substate. */
+		attrlen += 2;
+		decl_ptr = *msg_ptr + (valen + 2) / 3;
+	} else if (MSRP_ATTR_TALKER == attrtype) {
+		attrtype = MSRP_ATTR_TALKER_FAILED;
+		memset(&firstval[25], 0, 34 - 25);
+		attrlen = 34;
+	}
+
+	/* In a VectorAttribute, the Vector contains events which are packed
+	 * three to a byte. We process one byte of the Vector at a time.
+	 */
+	while (valen > 0) {
+		vaevents = **msg_ptr;
+		*msg_ptr += sizeof(vaevents);
+
+		/* Extract and process the first event. */
+		vaevent = vaevents / (__MRP_VECATTR_EVENT_MAX *
+				      __MRP_VECATTR_EVENT_MAX);
+		if (vaevent >= __MRP_VECATTR_EVENT_MAX) {
+			/* The byte is malformed; stop processing. */
+			return -2;
+		}
+		ignore = false;
+		if (decl_ptr) {
+			if (4 == i)
+				mrp_4pack_decode(*decl_ptr++, decl, &i);
+			if (MSRP_LISTENER_IGNORE == decl[i])
+				ignore = true;
+			firstval[8] = decl[i++];
+		}
+		if (!app->dry_run && !ignore)
+			mrp_update_attr(app, firstval, attrlen, attrtype,
+				vaevent);
+
+		/* If present, extract and process the second event. */
+		if (!--valen)
+			break;
+		vaevents %= (__MRP_VECATTR_EVENT_MAX *
+			     __MRP_VECATTR_EVENT_MAX);
+		vaevent = vaevents / __MRP_VECATTR_EVENT_MAX;
+		app->attrval_inc(firstval, attrlen);
+		ignore = false;
+		if (decl_ptr) {
+			if (4 == i)
+				mrp_4pack_decode(*decl_ptr++, decl, &i);
+			if (MSRP_LISTENER_IGNORE == decl[i])
+				ignore = true;
+			firstval[8] = decl[i++];
+		}
+		if (!app->dry_run && !ignore)
+			mrp_update_attr(app, firstval, attrlen, attrtype,
+				vaevent);
+
+		/* If present, extract and process the third event. */
+		if (!--valen)
+			break;
+		vaevents %= __MRP_VECATTR_EVENT_MAX;
+		vaevent = vaevents;
+		app->attrval_inc(firstval, attrlen);
+		ignore = false;
+		if (decl_ptr) {
+			if (4 == i)
+				mrp_4pack_decode(*decl_ptr++, decl, &i);
+			if (MSRP_LISTENER_IGNORE == decl[i])
+				ignore = true;
+			firstval[8] = decl[i++];
+		}
+		if (!app->dry_run && !ignore)
+			mrp_update_attr(app, firstval, attrlen, attrtype,
+				vaevent);
+		app->attrval_inc(firstval, attrlen);
+		--valen;
+	}
+#if 0
+dbg_msg(" decl_ptr: %p %p\n", *msg_ptr, decl_ptr);
+#endif
+	if (decl_ptr)
+		*msg_ptr = decl_ptr;
+	return 0;
+}
+
+static int msrp_parse_msg(struct mrp_applicant *app, u8 **msg_ptr, u8 *msg_eof)
+{
+	struct mrp_msg_hdr *mh;
+	u16 *attr_list;
+	int rc;
+	int add_len = 0;
+
+	/*
+	 * End mark terminates the message.  Happen when there are filler
+	 * bytes in the frame.
+	 */
+	mh = (struct mrp_msg_hdr *) *msg_ptr;
+	if (MRP_END_MARK == mh->attrtype &&
+	    MRP_END_MARK == mh->attrlen) {
+		return -1;
+	}
+
+	/* Check whether the attribute type is understood. */
+	rc = app->attr_chk(mh->attrtype, mh->attrlen);
+	if (rc < 0)
+		return -1;
+
+	/* Attribute not understood but not in future protocol. */
+	if (rc && (app->ver_diff <= 0 || !app->msg_cnt))
+		return -1;
+
+	if (MSRP_ATTR_LISTENER == mh->attrtype)
+		add_len++;
+
+	*msg_ptr += sizeof(struct mrp_msg_hdr);
+	attr_list = (u16 *) *msg_ptr;
+	if (ntohs(*attr_list) < mh->attrlen + 4)
+{
+int i;
+u8 *data = *msg_ptr;
+
+for (i = 0; i < msg_eof - *msg_ptr; i++) {
+dbg_msg("%02x ", data[i]);
+if ((i % 16) == 15)
+dbg_msg("\n");
+}
+if ((i % 16))
+dbg_msg("\n");
+dbg_msg(" bad attrlist: %d %d\n", ntohs(*attr_list), mh->attrlen);
+		return -1;
+}
+	*msg_ptr += 2;
+	while (*msg_ptr <= (msg_eof - (2 + 1 + add_len))) {
+		rc = msrp_parse_events(app, msg_ptr, msg_eof,
+			mh->attrlen, mh->attrtype);
+		if (rc < 0)
+			return rc;
+		rc = mrp_parse_end_mark(msg_ptr, msg_eof);
+		if (rc <= 0) {
+			u16 len = *msg_ptr - (u8 *) attr_list;
+			u16 list_len = ntohs(*attr_list) + 2;
+
+			if (len != list_len) {
+if (app->dry_run)
+dbg_msg("attrlist %d:%u %u %d\n", app->port, list_len, len, rc);
+				if (len < list_len)
+					*msg_ptr += list_len - len;
+			}
+		}
+
+		/* End mark terminates the vector list. */
+		if (rc <= 0) {
+			if (app->dry_run)
+				rc = 0;
+			return rc;
+		}
+	}
+	return 0;
+}
+
+static int msrp_rx(struct mrp_applicant *app, u8 *data, int len)
+{
+	struct mrp_pdu_hdr *ph;
+	u8 *msg_ptr;
+	u8 *msg_eof;
+	int rc;
+
+	ph = (struct mrp_pdu_hdr *) data;
+	app->ver_diff = ph->version - app->app->version;
+	app->msg_cnt = 0;
+
+	app->lva.rx = 0;
+
+	msg_ptr = (u8 *)(ph + 1);
+	msg_eof = msg_ptr + len - sizeof(struct mrp_pdu_hdr);
+
+	while (msg_ptr <= (msg_eof - (2 + 2))) {
+		rc = msrp_parse_msg(app, &msg_ptr, msg_eof);
+		if (rc < 0) {
+			if (app->dry_run && rc == -2)
+				rc = 0;
+			return rc;
+		}
+		app->msg_cnt++;
+
+		/* End mark terminates the frame. */
+		rc = mrp_parse_end_mark(&msg_ptr, msg_eof);
+		if (rc <= 0) {
+			if (app->dry_run)
+				rc = 0;
+			return rc;
+		}
+	}
+	return 0;
+}
+
+static int msrp_rxpdu(struct mrp_applicant *app, u8 *data, int len)
+{
+	int rc;
+
+	app->dry_run = 1;
+	rc = msrp_rx(app, data, len);
+if (rc < 0)
+dbg_msg(" !! %s %d\n", __func__, app->port);
+
+	/* Discard entire PDU if malformed. */
+	if (rc < 0)
+		return rc;
+	app->dry_run = 0;
+	return msrp_rx(app, data, len);
+}
+
+static int msrp_txpdu(struct mrp_applicant *app)
+{
+	int bytes;
+	int err;
+	u8 *msg_buf;
+	u8 *msg_eof;
+	u8 *msg_ptr;
+#if 1
+	int tx_0 = 0;
+	int tx_1 = 0;
+#endif
+
+	if (!app->normal)
+		return 0;
+
+	err = mrp_pdu_init(app);
+	if (err < 0)
+		return err;
+
+	msg_buf = app->pdu->data;
+	msg_eof = msg_buf + app->dev->mtu;
+	msg_buf++;
+	msg_ptr = msg_buf;
+
+	if (app->normal & (1 << MSRP_ATTR_TALKER)) {
+		err = msrp_tx(app, msg_ptr, msg_eof, &bytes,
+			      MSRP_ATTR_TALKER, sizeof(struct srp_talker),
+			      MSRP_TALKER_OPT_MAX, MSRP_TALKER_MIN);
+
+		/* LeaveAll for MAC attribute type sent. */
+		if (bytes) {
+			msg_ptr += bytes;
+			app->lva.tx &= ~(1 << (MSRP_ATTR_TALKER - 1));
+#if 1
+			tx_0 = 1;
+#endif
+
+			/* No more space. */
+			if (-1 == err)
+				goto send;
+		}
+	}
+
+	if (app->normal & (1 << MSRP_ATTR_TALKER_FAILED)) {
+		err = msrp_tx(app, msg_ptr, msg_eof, &bytes,
+			      MSRP_ATTR_TALKER_FAILED,
+			      sizeof(struct srp_talker_failed),
+			      MSRP_TALKER_FAILED_OPT_MAX,
+			      MSRP_TALKER_FAILED_MIN);
+
+		/* LeaveAll for MAC attribute type sent. */
+		if (bytes) {
+			msg_ptr += bytes;
+			app->lva.tx &= ~(1 << (MSRP_ATTR_TALKER_FAILED - 1));
+#if 1
+			tx_1 = 1;
+#endif
+
+			/* No more space. */
+			if (-1 == err)
+				goto send;
+		}
+	}
+
+	if (app->normal & (1 << MSRP_ATTR_LISTENER)) {
+		err = msrp_tx(app, msg_ptr, msg_eof, &bytes,
+			      MSRP_ATTR_LISTENER,
+			      sizeof(struct srp_listener) - 2,
+			      MSRP_LISTENER_OPT_MAX, MSRP_LISTENER_MIN);
+
+		/* LeaveAll for SVC attribute type sent. */
+		if (bytes) {
+			msg_ptr += bytes;
+			app->lva.tx &= ~(1 << (MSRP_ATTR_LISTENER - 1));
+
+			/* No more space. */
+			if (-1 == err)
+				goto send;
+		}
+	}
+#if 0
+if (tx_0 && !tx_1)
+goto send;
+#endif
+#if 1
+	if (mrp_10_5_1d_hack && !tx_0 && tx_1)
+		goto send;
+#endif
+
+	if (app->normal & (1 << MSRP_ATTR_DOMAIN)) {
+		err = msrp_tx(app, msg_ptr, msg_eof, &bytes,
+			      MSRP_ATTR_DOMAIN, sizeof(struct srp_domain),
+			      MSRP_DOMAIN_OPT_MAX, MSRP_DOMAIN_MIN);
+
+		/* LeaveAll for SVC attribute type sent. */
+		if (bytes) {
+			msg_ptr += bytes;
+			app->lva.tx &= ~(1 << (MSRP_ATTR_DOMAIN - 1));
+
+			/* No more space. */
+			if (-1 == err)
+				goto send;
+		}
+	}
+	app->lva.tx = 0;
+
+send:
+
+	return mrp_txpdu(app, msg_buf, msg_ptr, msg_eof);
+}
+
+static int msrp_req_join_domain(struct mrp_applicant *app,
+	struct srp_domain *domain, u8 new_decl)
+{
+	if (new_decl)
+		new_decl = MRP_EVENT_NEW;
+	else
+		new_decl = MRP_EVENT_JOIN;
+	return mrp_req_new(app, domain, sizeof(struct srp_domain),
+		MSRP_ATTR_DOMAIN, new_decl);
+}
+
+static int msrp_req_new_listener(struct mrp_applicant *app,
+	struct srp_listener *listener, u8 new_decl)
+{
+	if (new_decl)
+		new_decl = MRP_EVENT_NEW;
+	else
+		new_decl = MRP_EVENT_JOIN;
+	listener->nextval = 0;
+	return mrp_req_new(app, listener, sizeof(struct srp_listener),
+		MSRP_ATTR_LISTENER, new_decl);
+}
+
+static int msrp_req_new_talker(struct mrp_applicant *app,
+	struct srp_talker_failed *talker, u8 new_decl)
+{
+	if (new_decl)
+		new_decl = MRP_EVENT_NEW;
+	else
+		new_decl = MRP_EVENT_JOIN;
+	return mrp_req_new(app, talker, sizeof(struct srp_talker_failed),
+		MSRP_ATTR_TALKER_FAILED, new_decl);
+}
+
+static int msrp_req_leave_domain(struct mrp_applicant *app,
+	struct srp_domain *domain)
+{
+	return mrp_req_leave(app, domain, sizeof(struct srp_domain),
+		MSRP_ATTR_DOMAIN);
+}
+
+static int msrp_req_leave_listener(struct mrp_applicant *app,
+	struct srp_listener *listener)
+{
+	listener->nextval = 0;
+	return mrp_req_leave(app, listener, sizeof(struct srp_listener),
+		MSRP_ATTR_LISTENER);
+}
+
+static int msrp_req_leave_talker(struct mrp_applicant *app,
+	struct srp_talker_failed *talker)
+{
+	return mrp_req_leave(app, talker, sizeof(struct srp_talker_failed),
+		MSRP_ATTR_TALKER_FAILED);
+}
+
+static void msrp_init_application(struct mrp_applicant *app,
+				  void (*acton)(struct mrp_applicant *app,
+				  struct mrp_attr *attr),
+				  void (*cleanup)(struct mrp_applicant *app))
+{
+	app->attrval_inc = msrp_attrvalue_inc;
+	app->attr_chk = msrp_attr_chk;
+	app->attr_cmp = msrp_attr_cmp;
+	app->attr_valid = mrp_attr_valid;
+	app->attr_size = msrp_attr_size;
+	app->attr_len = msrp_attr_len;
+	app->attr_type = msrp_attr_type;
+	app->attr_upd = msrp_attr_upd;
+	app->rxpdu = msrp_rxpdu;
+	app->txpdu = msrp_txpdu;
+	app->acton = acton;
+	app->cleanup = cleanup;
+	app->normal = (1 << MSRP_ATTR_TALKER) |
+		      (1 << MSRP_ATTR_TALKER_FAILED) |
+		      (1 << MSRP_ATTR_LISTENER) |
+		      (1 << MSRP_ATTR_DOMAIN);
+}
+
diff -Napur linux-linux4microchip-2022.10-orig/drivers/net/ethernet/micrel/mvrp.c linux-linux4microchip-2022.10/drivers/net/ethernet/micrel/mvrp.c
--- linux-linux4microchip-2022.10-orig/drivers/net/ethernet/micrel/mvrp.c	1969-12-31 16:00:00.000000000 -0800
+++ linux-linux4microchip-2022.10/drivers/net/ethernet/micrel/mvrp.c	2023-04-25 16:13:55.396163494 -0700
@@ -0,0 +1,160 @@
+/*
+ *	IEEE 802.1Q Multiple VLAN Registration Protocol (MVRP)
+ *
+ *	Copyright (c) 2016-2017 Microchip Technology Inc.
+ *
+ *	Copyright (c) 2012 Massachusetts Institute of Technology
+ *
+ *	Adapted from code in net/8021q/vlan_gvrp.c
+ *	Copyright (c) 2008 Patrick McHardy <kaber@trash.net>
+ *
+ *	This program is free software; you can redistribute it and/or
+ *	modify it under the terms of the GNU General Public License
+ *	version 2 as published by the Free Software Foundation.
+ */
+
+
+#ifndef ETH_P_MVRP
+#define ETH_P_MVRP		0x88F5
+#endif
+
+
+#if 1
+#define MRP_MVRP_ADDRESS	{ 0x01, 0x80, 0xc2, 0x00, 0x00, 0x21 }
+#else
+/* Switch does not forward frames so hard to debug. */
+#define MRP_MVRP_ADDRESS	{ 0x01, 0x80, 0xc2, 0x00, 0x00, 0x0E }
+#endif
+
+enum mvrp_attributes {
+	MVRP_ATTR_INVALID,
+	MVRP_ATTR_VID,
+	__MVRP_ATTR_MAX
+};
+#define MVRP_ATTR_MAX	(__MVRP_ATTR_MAX - 1)
+
+static struct mrp_application vlan_mrp_app __read_mostly = {
+	.type		= MRP_APPLICATION_MVRP,
+	.maxattr	= MVRP_ATTR_MAX,
+	.pkttype.type	= htons(ETH_P_MVRP),
+	.group_address	= MRP_MVRP_ADDRESS,
+	.version	= 0,
+};
+
+static int mvrp_attr_chk(u8 attrtype, u8 attrlen)
+{
+	if (MVRP_ATTR_VID == attrtype && 2 == attrlen)
+		return 0;
+	if (MVRP_ATTR_VID != attrtype)
+		return 1;
+	return -1;
+}
+
+static int mvrp_attr_valid(u8 attrtype, const void *value)
+{
+	if (MVRP_ATTR_VID == attrtype) {
+		u16 *ptr = (u16 *) value;
+		u16 vid = ntohs(*ptr);
+
+		if (1 <= vid && vid <= 4094)
+			return true;
+	}
+	return false;
+}
+
+static int mvrp_rxpdu(struct mrp_applicant *app, u8 *data, int len)
+{
+	int rc;
+
+	app->dry_run = 1;
+	rc = mrp_rx(app, data, len);
+
+	/* Discard entire PDU if malformed. */
+	if (rc < 0)
+		return rc;
+	app->dry_run = 0;
+	return mrp_rx(app, data, len);
+}
+
+static int mvrp_txpdu(struct mrp_applicant *app)
+{
+	int bytes;
+	int err;
+	u8 *msg_buf;
+	u8 *msg_eof;
+	u8 *msg_ptr;
+
+	if (!app->normal)
+		return 0;
+
+	err = mrp_pdu_init(app);
+	if (err < 0)
+		return err;
+
+	msg_buf = app->pdu->data;
+	msg_eof = msg_buf + app->dev->mtu;
+	msg_buf++;
+	msg_ptr = msg_buf;
+
+	if (app->normal & (1 << MVRP_ATTR_VID)) {
+		err = mrp_tx(app, msg_ptr, msg_eof, &bytes,
+			     MVRP_ATTR_VID, 2, MVRP_VID_OPT_MAX, MVRP_VID_MIN);
+
+		/* LeaveAll for VLAN attribute type sent. */
+		if (bytes) {
+			msg_ptr += bytes;
+			app->lva.tx &= ~(1 << (MVRP_ATTR_VID - 1));
+		}
+	}
+	if (!app->last_node)
+		app->lva.tx = 0;
+
+	return mrp_txpdu(app, msg_buf, msg_ptr, msg_eof);
+}
+
+static int mvrp_req_join(struct mrp_applicant *app, u16 vid, u8 new_decl)
+{
+	__be16 vlan_id = htons(vid);
+
+	if (new_decl)
+		new_decl = MRP_EVENT_NEW;
+	else
+		new_decl = MRP_EVENT_JOIN;
+	return mrp_req_new(app, &vlan_id, sizeof(vlan_id), MVRP_ATTR_VID,
+		new_decl);
+}
+
+static int mvrp_req_leave(struct mrp_applicant *app, u16 vid)
+{
+	__be16 vlan_id = htons(vid);
+
+	return mrp_req_leave(app, &vlan_id, sizeof(vlan_id), MVRP_ATTR_VID);
+}
+
+static void mvrp_req_set(struct mrp_applicant *app, u16 vid,
+			enum mrp_registrar_state state)
+{
+	__be16 vlan_id = htons(vid);
+
+	mrp_req_set(app, &vlan_id, sizeof(vlan_id), MVRP_ATTR_VID, state);
+}
+
+static void mvrp_init_application(struct mrp_applicant *app,
+				  void (*acton)(struct mrp_applicant *app,
+				  struct mrp_attr *attr),
+				  void (*cleanup)(struct mrp_applicant *app))
+{
+	app->attrval_inc = mrp_attrvalue_inc;
+	app->attr_chk = mvrp_attr_chk;
+	app->attr_cmp = mrp_attr_cmp;
+	app->attr_valid = mvrp_attr_valid;
+	app->attr_size = mrp_attr_size;
+	app->attr_len = mrp_attr_len;
+	app->attr_type = mrp_attr_type;
+	app->rxpdu = mvrp_rxpdu;
+	app->txpdu = mvrp_txpdu;
+	app->acton = acton;
+	app->cleanup = cleanup;
+	app->normal = (1 << MVRP_ATTR_VID);
+}
+
diff -Napur linux-linux4microchip-2022.10-orig/drivers/net/ethernet/micrel/smi-ksz8863.c linux-linux4microchip-2022.10/drivers/net/ethernet/micrel/smi-ksz8863.c
--- linux-linux4microchip-2022.10-orig/drivers/net/ethernet/micrel/smi-ksz8863.c	1969-12-31 16:00:00.000000000 -0800
+++ linux-linux4microchip-2022.10/drivers/net/ethernet/micrel/smi-ksz8863.c	2023-03-25 13:55:42.416649253 -0700
@@ -0,0 +1,254 @@
+/**
+ * Microchip KSZ8863 SMI driver
+ *
+ * Copyright (c) 2018-2023 Microchip Technology Inc.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+ */
+
+#if 0
+#define DEBUG
+#define DBG
+#define DEBUG_MSG
+#endif
+
+#include <linux/debugfs.h>
+#include <linux/seq_file.h>
+
+#include "ksz_cfg_8863.h"
+#ifndef PHY_RESET_NOT
+#define PHY_RESET_NOT			PHY_RESET
+#endif
+
+#include "ksz_spi_net.h"
+
+#define USE_SHOW_HELP
+#include "ksz_common.c"
+#include "ksz_req.c"
+
+#define SW_DRV_RELDATE			"Mar 25, 2023"
+#define SW_DRV_VERSION			"1.2.3"
+
+/* -------------------------------------------------------------------------- */
+
+#define MIB_READ_INTERVAL		(HZ / 2)
+
+static void smi_addr(unsigned reg, int *phyid, int *regnum)
+{
+	*regnum = reg & 0x1f;
+	*phyid = reg >> 5;
+}
+
+static u8 mdio_r8(struct sw_priv *priv, unsigned reg)
+{
+	struct smi_hw_priv *smi = priv->hw_dev;
+	int phyid;
+	int regnum;
+	int ret;
+
+	smi_addr(reg, &phyid, &regnum);
+	phyid |= 0x10;
+	ret = smi->read(smi->bus, phyid, regnum);
+	return (u8)ret;
+}
+
+static void mdio_w8(struct sw_priv *priv, unsigned reg, unsigned val)
+{
+	struct smi_hw_priv *smi = priv->hw_dev;
+	int phyid;
+	int regnum;
+	int ret;
+
+	smi_addr(reg, &phyid, &regnum);
+	ret = smi->write(smi->bus, phyid, regnum, val & 0xff);
+}
+
+static uint mdio_r_c(struct sw_priv *priv, unsigned reg, int cnt)
+{
+	int i;
+	int ret;
+	uint val;
+
+	val = 0;
+	for (i = 0; i < cnt; i++) {
+		ret = mdio_r8(priv, reg++);
+		val <<= 8;
+		val |= ret;
+	}
+	return val;
+}
+
+static void mdio_w_c(struct sw_priv *priv, unsigned reg, unsigned val, int cnt)
+{
+	int i;
+
+	for (i = 0; i < cnt; i++) {
+		mdio_w8(priv, reg++, val);
+		val >>= 8;
+	}
+}
+
+static u16 mdio_r16(struct sw_priv *priv, unsigned reg)
+{
+	uint val;
+
+	val = mdio_r_c(priv, reg, 2);
+	return (u16)val;
+}
+
+static u32 mdio_r32(struct sw_priv *priv, unsigned reg)
+{
+	uint val;
+
+	val = mdio_r_c(priv, reg, 4);
+	return (u32)val;
+}
+
+static void mdio_w16(struct sw_priv *priv, unsigned reg, unsigned val)
+{
+	val = cpu_to_be16(val);
+	mdio_w_c(priv, reg, val, 2);
+}
+
+static void mdio_w32(struct sw_priv *priv, unsigned reg, unsigned val)
+{
+	val = cpu_to_be32(val);
+	mdio_w_c(priv, reg, val, 4);
+}
+
+static u8 smi_r8(struct ksz_sw *sw, unsigned reg)
+{
+	return mdio_r8(sw->dev, reg);
+}
+
+static u16 smi_r16(struct ksz_sw *sw, unsigned reg)
+{
+	return mdio_r16(sw->dev, reg);
+}
+
+static u32 smi_r32(struct ksz_sw *sw, unsigned reg)
+{
+	return mdio_r32(sw->dev, reg);
+}
+
+static void smi_w8(struct ksz_sw *sw, unsigned reg, unsigned val)
+{
+	mdio_w8(sw->dev, reg, val);
+}
+
+static void smi_w16(struct ksz_sw *sw, unsigned reg, unsigned val)
+{
+	mdio_w16(sw->dev, reg, val);
+}
+
+static void smi_w32(struct ksz_sw *sw, unsigned reg, unsigned val)
+{
+	mdio_w32(sw->dev, reg, val);
+}
+
+#include "ksz_sw.c"
+
+static struct ksz_sw_reg_ops smi_reg_ops = {
+	.r8			= smi_r8,
+	.r16			= smi_r16,
+	.r32			= smi_r32,
+	.w8			= smi_w8,
+	.w16			= smi_w16,
+	.w32			= smi_w32,
+
+	.r			= smi_r8,
+	.w			= smi_w8,
+
+	.get			= sw_reg_get,
+	.set			= sw_reg_set,
+};
+
+/* -------------------------------------------------------------------------- */
+
+static int smi_probe(struct platform_device **pdev, struct mii_bus *bus,
+		     int irq,
+		     int (*read)(struct mii_bus *b, int id, int reg),
+		     int (*write)(struct mii_bus *b, int id, int reg, u16 val))
+{
+	struct smi_hw_priv *smi;
+	struct sw_priv *priv = NULL;
+	int ret = -ENOMEM;
+
+	*pdev = platform_device_register_data(NULL, "smi", sw_device_present,
+					      NULL, 0);
+	if (IS_ERR(*pdev))
+		return PTR_ERR(*pdev);
+
+#ifdef DEBUG_MSG
+	if (init_dbg())
+		goto probe_err;
+#endif
+
+	priv = kzalloc(sizeof(struct sw_priv), GFP_KERNEL);
+	if (!priv)
+		goto probe_err;
+
+	priv->hw_dev = kzalloc(sizeof(struct smi_hw_priv), GFP_KERNEL);
+	if (!priv->hw_dev)
+		goto probe_err;
+
+	smi = priv->hw_dev;
+
+	smi->bus = bus;
+	smi->read = read;
+	smi->write = write;
+
+	priv->dev = &(*pdev)->dev;
+	priv->of_dev = &(*pdev)->dev;
+
+	priv->sw.reg = &smi_reg_ops;
+
+	priv->irq = irq;
+
+	ret = ksz_probe(priv);
+
+	/* It will be deleted when probing fails. */
+	priv = NULL;
+	if (!ret)
+		return 0;
+
+probe_err:
+	if (priv) {
+		if (priv->hw_dev)
+			kfree(priv->hw_dev);
+		kfree(priv);
+	}
+	if (*pdev)
+		platform_device_unregister(*pdev);
+
+#ifdef DEBUG_MSG
+	exit_dbg();
+#endif
+	return ret;
+}
+
+static int smi_remove(struct platform_device *pdev)
+{
+	struct sw_priv *priv = platform_get_drvdata(pdev);
+	int ret;
+
+	ret = ksz_remove(priv);
+	platform_device_unregister(pdev);
+
+#ifdef DEBUG_MSG
+	exit_dbg();
+#endif
+	return ret;
+}
+
diff -Napur linux-linux4microchip-2022.10-orig/drivers/net/ethernet/micrel/smi-ksz8895.c linux-linux4microchip-2022.10/drivers/net/ethernet/micrel/smi-ksz8895.c
--- linux-linux4microchip-2022.10-orig/drivers/net/ethernet/micrel/smi-ksz8895.c	1969-12-31 16:00:00.000000000 -0800
+++ linux-linux4microchip-2022.10/drivers/net/ethernet/micrel/smi-ksz8895.c	2023-03-25 13:55:24.676648510 -0700
@@ -0,0 +1,292 @@
+/**
+ * Microchip KSZ8895 SMI driver
+ *
+ * Copyright (c) 2018-2023 Microchip Technology Inc.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+ */
+
+#if 0
+#define DEBUG
+#define DBG
+#define DEBUG_MSG
+#if 0
+#define DBG_LINK
+#endif
+#endif
+
+#include <linux/debugfs.h>
+#include <linux/seq_file.h>
+
+#include "ksz_cfg_8895.h"
+#include "ksz_spi_net.h"
+
+#define USE_SHOW_HELP
+#include "ksz_common.c"
+#include "ksz_req.c"
+
+#define SW_DRV_RELDATE			"Mar 25, 2023"
+#define SW_DRV_VERSION			"1.2.3"
+
+/* -------------------------------------------------------------------------- */
+
+#define MIB_READ_INTERVAL		(HZ / 2)
+
+static void smi_addr(unsigned reg, int *phyid, int *regnum)
+{
+	int phy_hi;
+
+	*regnum = reg & 0x1f;
+	*phyid = (reg & 0xe0) >> 5;
+	phy_hi = (*phyid >> 1) << 3;
+	*phyid &= 1;
+	*phyid |= 6;
+	*phyid |= phy_hi;
+}
+
+static u8 mdio_r8(struct sw_priv *priv, unsigned reg)
+{
+	struct smi_hw_priv *smi = priv->hw_dev;
+	int phyid;
+	int regnum;
+	int ret;
+
+	smi_addr(reg, &phyid, &regnum);
+	ret = smi->read(smi->bus, phyid, regnum);
+	return (u8)ret;
+}
+
+static void mdio_w8(struct sw_priv *priv, unsigned reg, unsigned val)
+{
+	struct smi_hw_priv *smi = priv->hw_dev;
+	int phyid;
+	int regnum;
+	int ret;
+
+	smi_addr(reg, &phyid, &regnum);
+	ret = smi->write(smi->bus, phyid, regnum, val & 0xff);
+}
+
+static uint mdio_r_c(struct sw_priv *priv, unsigned reg, int cnt)
+{
+	int i;
+	int ret;
+	uint val;
+
+	val = 0;
+	for (i = 0; i < cnt; i++) {
+		ret = mdio_r8(priv, reg++);
+		val <<= 8;
+		val |= ret;
+	}
+	return val;
+}
+
+static void mdio_w_c(struct sw_priv *priv, unsigned reg, unsigned val, int cnt)
+{
+	int i;
+
+	for (i = 0; i < cnt; i++) {
+		mdio_w8(priv, reg++, val);
+		val >>= 8;
+	}
+}
+
+static u16 mdio_r16(struct sw_priv *priv, unsigned reg)
+{
+	uint val;
+
+	val = mdio_r_c(priv, reg, 2);
+	return (u16)val;
+}
+
+static u32 mdio_r32(struct sw_priv *priv, unsigned reg)
+{
+	uint val;
+
+	val = mdio_r_c(priv, reg, 4);
+	return (u32)val;
+}
+
+static void mdio_w16(struct sw_priv *priv, unsigned reg, unsigned val)
+{
+	val = cpu_to_be16(val);
+	mdio_w_c(priv, reg, val, 2);
+}
+
+static void mdio_w32(struct sw_priv *priv, unsigned reg, unsigned val)
+{
+	val = cpu_to_be32(val);
+	mdio_w_c(priv, reg, val, 4);
+}
+
+static int mdio_r(struct sw_priv *priv, u32 reg, void *buf, unsigned len)
+{
+	int i;
+	int ret;
+	u8 *data = (u8 *)buf;
+
+	for (i = 0; i < len; i++) {
+		ret = mdio_r8(priv, reg++);
+		data[i] = ret & 0xff;
+	}
+	return 0;
+}
+
+static int mdio_w(struct sw_priv *priv, u32 reg, void *buf, unsigned len)
+{
+	int i;
+	u8 *data = (u8 *)buf;
+
+	for (i = 0; i < len; i++) {
+		mdio_w8(priv, reg++, data[i]);
+	}
+	return 0;
+}
+
+static u8 smi_r8(struct ksz_sw *sw, unsigned reg)
+{
+	return mdio_r8(sw->dev, reg);
+}
+
+static u16 smi_r16(struct ksz_sw *sw, unsigned reg)
+{
+	return mdio_r16(sw->dev, reg);
+}
+
+static u32 smi_r32(struct ksz_sw *sw, unsigned reg)
+{
+	return mdio_r32(sw->dev, reg);
+}
+
+static void smi_w8(struct ksz_sw *sw, unsigned reg, unsigned val)
+{
+	mdio_w8(sw->dev, reg, val);
+}
+
+static void smi_w16(struct ksz_sw *sw, unsigned reg, unsigned val)
+{
+	mdio_w16(sw->dev, reg, val);
+}
+
+static void smi_w32(struct ksz_sw *sw, unsigned reg, unsigned val)
+{
+	mdio_w32(sw->dev, reg, val);
+}
+
+static void smi_r(struct ksz_sw *sw, unsigned reg, void *buf, size_t cnt)
+{
+	mdio_r(sw->dev, reg, buf, cnt);
+}
+
+static void smi_w(struct ksz_sw *sw, unsigned reg, void *buf, size_t cnt)
+{
+	mdio_w(sw->dev, reg, buf, cnt);
+}
+
+#include "ksz_sw_8895.c"
+
+static struct ksz_sw_reg_ops smi_reg_ops = {
+	.r8			= smi_r8,
+	.r16			= smi_r16,
+	.r32			= smi_r32,
+	.w8			= smi_w8,
+	.w16			= smi_w16,
+	.w32			= smi_w32,
+
+	.r			= smi_r,
+	.w			= smi_w,
+
+	.get			= sw_reg_get,
+	.set			= sw_reg_set,
+};
+
+/* -------------------------------------------------------------------------- */
+
+static int smi_probe(struct platform_device **pdev, struct mii_bus *bus,
+		     int irq,
+		     int (*read)(struct mii_bus *b, int id, int reg),
+		     int (*write)(struct mii_bus *b, int id, int reg, u16 val))
+{
+	struct smi_hw_priv *smi;
+	struct sw_priv *priv = NULL;
+	int ret = -ENOMEM;
+
+	*pdev = platform_device_register_data(NULL, "smi", sw_device_present,
+					      NULL, 0);
+	if (IS_ERR(*pdev))
+		return PTR_ERR(*pdev);
+
+#ifdef DEBUG_MSG
+	if (init_dbg())
+		goto probe_err;
+#endif
+
+	priv = kzalloc(sizeof(struct sw_priv), GFP_KERNEL);
+	if (!priv)
+		goto probe_err;
+
+	priv->hw_dev = kzalloc(sizeof(struct smi_hw_priv), GFP_KERNEL);
+	if (!priv->hw_dev)
+		goto probe_err;
+
+	smi = priv->hw_dev;
+
+	smi->bus = bus;
+	smi->read = read;
+	smi->write = write;
+
+	priv->dev = &(*pdev)->dev;
+	priv->of_dev = &(*pdev)->dev;
+
+	priv->sw.reg = &smi_reg_ops;
+
+	priv->irq = irq;
+
+	ret = ksz_probe(priv);
+
+	/* It will be deleted when probing fails. */
+	priv = NULL;
+	if (!ret)
+		return 0;
+
+probe_err:
+	if (priv) {
+		if (priv->hw_dev)
+			kfree(priv->hw_dev);
+		kfree(priv);
+	}
+	if (*pdev)
+		platform_device_unregister(*pdev);
+
+#ifdef DEBUG_MSG
+	exit_dbg();
+#endif
+	return ret;
+}
+
+static int smi_remove(struct platform_device *pdev)
+{
+	struct sw_priv *priv = platform_get_drvdata(pdev);
+	int ret;
+
+	ret = ksz_remove(priv);
+	platform_device_unregister(pdev);
+
+#ifdef DEBUG_MSG
+	exit_dbg();
+#endif
+	return ret;
+}
+
diff -Napur linux-linux4microchip-2022.10-orig/drivers/net/ethernet/micrel/spi-ksz8463.c linux-linux4microchip-2022.10/drivers/net/ethernet/micrel/spi-ksz8463.c
--- linux-linux4microchip-2022.10-orig/drivers/net/ethernet/micrel/spi-ksz8463.c	1969-12-31 16:00:00.000000000 -0800
+++ linux-linux4microchip-2022.10/drivers/net/ethernet/micrel/spi-ksz8463.c	2023-03-25 13:53:42.948644252 -0700
@@ -0,0 +1,507 @@
+/**
+ * Microchip KSZ8463 SPI driver
+ *
+ * Copyright (c) 2015-2023 Microchip Technology Inc.
+ * Copyright (c) 2010-2015 Micrel, Inc.
+ *
+ * Copyright 2009 Simtec Electronics
+ *	http://www.simtec.co.uk/
+ *	Ben Dooks <ben@simtec.co.uk>
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+ */
+
+#if 0
+#define DEBUG
+#define DBG
+#define DEBUG_MSG
+#endif
+
+#ifndef CONFIG_KSZ_SWITCH_EMBEDDED
+#define pr_fmt(fmt) KBUILD_MODNAME ": " fmt
+
+#include <linux/kernel.h>
+#include <linux/module.h>
+#include <linux/interrupt.h>
+#include <linux/of_net.h>
+#include <linux/of_mdio.h>
+#include <linux/phy.h>
+#include <linux/phylink.h>
+#include <linux/platform_device.h>
+#include <linux/sched.h>
+#include <linux/netdevice.h>
+#include <linux/etherdevice.h>
+#include <linux/ethtool.h>
+#include <linux/cache.h>
+#include <linux/crc32.h>
+#include <linux/if_vlan.h>
+#include <linux/ip.h>
+#include <net/ip.h>
+#include <net/ipv6.h>
+#endif
+
+/* -------------------------------------------------------------------------- */
+
+#include <linux/net_tstamp.h>
+#include <linux/spi/spi.h>
+#include <linux/debugfs.h>
+#include <linux/seq_file.h>
+
+#include "ksz_cfg_8463.h"
+
+
+#define KS8463MLI_DEV0			"ksz8463"
+#define KS8463MLI_DEV2			"ksz8463_2"
+
+#define SW_DRV_RELDATE			"Mar 25, 2023"
+#define SW_DRV_VERSION			"1.2.3"
+
+/* -------------------------------------------------------------------------- */
+
+#define HW_R(ks, reg)		spi_rdreg16(ks, reg)
+#define HW_W(ks, reg, val)	spi_wrreg16(ks, reg, val)
+#define HW_R8(ks, reg)		spi_rdreg8(ks, reg)
+#define HW_W8(ks, reg, val)	spi_wrreg8(ks, reg, val)
+#define HW_R16(ks, reg)		spi_rdreg16(ks, reg)
+#define HW_W16(ks, reg, val)	spi_wrreg16(ks, reg, val)
+#define HW_R32(ks, reg)		spi_rdreg32(ks, reg)
+#define HW_W32(ks, reg, val)	spi_wrreg32(ks, reg, val)
+
+#include "ksz_spi_net.h"
+
+/* -------------------------------------------------------------------------- */
+
+#define SPI_BYTE_ENABLE_S		4
+#define SPI_ADDR_ENABLE_S		2
+#define SPI_ADDR_S			11
+#define SPI_ADDR_M			((1 << SPI_ADDR_S) - 1)
+#define SPI_TURNAROUND_S		2
+
+#define MK_BYTE(reg)			(1 << ((reg) & 3))
+#define MK_WORD(reg)			(3 << ((reg) & 2))
+#define MK_LONG(reg)			(0xf)
+
+#define MK_OP(_byteen, _reg)		\
+	((((_reg) >> SPI_ADDR_ENABLE_S) << SPI_BYTE_ENABLE_S) | _byteen)
+
+#define KS_SPIOP_RD			0
+#define KS_SPIOP_WR			\
+	(1 << (SPI_ADDR_S - SPI_ADDR_ENABLE_S + SPI_BYTE_ENABLE_S))
+
+/*
+ * SPI register read/write calls.
+ *
+ * All these calls issue SPI transactions to access the chip's registers. They
+ * all require that the necessary lock is held to prevent accesses when the
+ * chip is busy transfering packet data (RX/TX FIFO accesses).
+ */
+
+/**
+ * spi_wrreg - issue write register command
+ * @priv:	The switch device structure.
+ * @op:		The register address and byte enables in message format.
+ * @val:	The value to write.
+ * @txl:	The length of data.
+ *
+ * This is the low level write call that issues the necessary spi message(s)
+ * to write data to the register specified in @op.
+ */
+static void spi_wrreg(struct sw_priv *priv, unsigned op, unsigned val,
+	unsigned txl)
+{
+	struct spi_hw_priv *hw_priv = priv->hw_dev;
+	struct spi_transfer *xfer = &hw_priv->spi_xfer1;
+	struct spi_message *msg = &hw_priv->spi_msg1;
+	struct spi_device *spi = hw_priv->spidev;
+	__le16 txb[4];
+	int ret;
+
+	op |= KS_SPIOP_WR;
+	op <<= SPI_TURNAROUND_S;
+	txb[0] = cpu_to_be16(op);
+	txb[1] = cpu_to_le16(val);
+	txb[2] = cpu_to_le16(val >> 16);
+
+	xfer->tx_buf = txb;
+	xfer->rx_buf = NULL;
+	xfer->len = txl + 2;
+
+	ret = spi_sync(spi, msg);
+	if (ret < 0)
+		pr_alert("spi_sync() failed\n");
+}
+
+/**
+ * spi_wrreg32 - write 32bit register value to chip
+ * @priv:	The switch device structure.
+ * @reg:	The register address.
+ * @val:	The value to write.
+ *
+ * Issue a write to put the value @val into the register specified in @reg.
+ */
+static void spi_wrreg32(struct sw_priv *priv, unsigned reg, unsigned val)
+{
+	spi_wrreg(priv, MK_OP(MK_LONG(reg), reg), val, 4);
+}
+
+/**
+ * spi_wrreg16 - write 16bit register value to chip
+ * @priv:	The switch device structure.
+ * @reg:	The register address.
+ * @val:	The value to write.
+ *
+ * Issue a write to put the value @val into the register specified in @reg.
+ */
+static void spi_wrreg16(struct sw_priv *priv, unsigned reg, unsigned val)
+{
+	spi_wrreg(priv, MK_OP(MK_WORD(reg), reg), val, 2);
+}
+
+/**
+ * spi_wrreg8 - write 8bit register value to chip
+ * @priv:	The switch device structure.
+ * @reg:	The register address.
+ * @val:	The value to write.
+ *
+ * Issue a write to put the value @val into the register specified in @reg.
+ */
+static void spi_wrreg8(struct sw_priv *priv, unsigned reg, unsigned val)
+{
+	spi_wrreg(priv, MK_OP(MK_BYTE(reg), reg), val, 1);
+}
+
+/**
+ * ksz_rx_1msg - select whether to use one or two messages for spi read
+ * @hw_priv:	The hardware device structure.
+ *
+ * Return whether to generate a single message with a tx and rx buffer
+ * supplied to spi_sync(), or alternatively send the tx and rx buffers
+ * as separate messages.
+ *
+ * Depending on the hardware in use, a single message may be more efficient
+ * on interrupts or work done by the driver.
+ *
+ * This currently always returns false until we add some per-device data passed
+ * from the platform code to specify which mode is better.
+ */
+static inline bool ksz_rx_1msg(struct spi_hw_priv *hw_priv)
+{
+	return hw_priv->rx_1msg;
+}
+
+/**
+ * spi_rdreg - issue read register command and return the data
+ * @priv:	The switch device structure.
+ * @op:		The register address and byte enables in message format.
+ * @rxb:	The RX buffer to return the result into.
+ * @rxl:	The length of data expected.
+ *
+ * This is the low level read call that issues the necessary spi message(s)
+ * to read data from the register specified in @op.
+ */
+static void spi_rdreg(struct sw_priv *priv, unsigned op, u8 *rxb, unsigned rxl)
+{
+	struct spi_hw_priv *hw_priv = priv->hw_dev;
+	struct spi_transfer *xfer;
+	struct spi_message *msg;
+	struct spi_device *spi = hw_priv->spidev;
+	__le16 *txb = (__le16 *) hw_priv->txd;
+	u8 *trx = hw_priv->rxd;
+	int ret;
+
+	op |= KS_SPIOP_RD;
+	op <<= SPI_TURNAROUND_S;
+	txb[0] = cpu_to_be16(op);
+
+	if (ksz_rx_1msg(hw_priv)) {
+#if defined(CONFIG_SPI_PEGASUS) || defined(CONFIG_SPI_PEGASUS_MODULE)
+		/*
+		 * A hack to tell KSZ8692 SPI host controller the read command.
+		 */
+		txb[1] = 0;
+		memcpy(trx, txb, 2 + 2);
+		txb[1] ^= 0xffff;
+#endif
+		msg = &hw_priv->spi_msg1;
+		xfer = &hw_priv->spi_xfer1;
+
+		xfer->tx_buf = txb;
+		xfer->rx_buf = trx;
+		xfer->len = rxl + 2;
+	} else {
+		msg = &hw_priv->spi_msg2;
+		xfer = hw_priv->spi_xfer2;
+
+		xfer->tx_buf = txb;
+		xfer->rx_buf = NULL;
+		xfer->len = 2;
+
+		xfer++;
+		xfer->tx_buf = NULL;
+		xfer->rx_buf = trx;
+		xfer->len = rxl;
+	}
+
+	ret = spi_sync(spi, msg);
+	if (ret < 0)
+		pr_alert("read: spi_sync() failed\n");
+	else if (ksz_rx_1msg(hw_priv))
+		memcpy(rxb, trx + 2, rxl);
+	else
+		memcpy(rxb, trx, rxl);
+}
+
+/**
+ * spi_rdreg8 - read 8 bit register from device
+ * @priv:	The switch device structure.
+ * @reg:	The register address.
+ *
+ * Read a 8bit register from the chip, returning the result.
+ */
+static u8 spi_rdreg8(struct sw_priv *priv, unsigned reg)
+{
+	u8 rxb[1];
+
+	spi_rdreg(priv, MK_OP(MK_BYTE(reg), reg), rxb, 1);
+	return rxb[0];
+}
+
+/**
+ * spi_rdreg16 - read 16 bit register from device
+ * @priv:	The switch device structure.
+ * @reg:	The register address.
+ *
+ * Read a 16bit register from the chip, returning the result.
+ */
+static u16 spi_rdreg16(struct sw_priv *priv, unsigned reg)
+{
+	__le16 rx = 0;
+
+	spi_rdreg(priv, MK_OP(MK_WORD(reg), reg), (u8 *) &rx, 2);
+	return le16_to_cpu(rx);
+}
+
+/**
+ * spi_rdreg32 - read 32 bit register from device
+ * @priv:	The switch device structure.
+ * @reg:	The register address.
+ *
+ * Read a 32bit register from the chip.
+ *
+ * Note, this read requires the address be aligned to 4 bytes.
+ */
+static u32 spi_rdreg32(struct sw_priv *priv, unsigned reg)
+{
+	__le32 rx = 0;
+
+	WARN_ON(reg & 3);
+
+	spi_rdreg(priv, MK_OP(MK_LONG(reg), reg), (u8 *) &rx, 4);
+	return le32_to_cpu(rx);
+}
+
+/* -------------------------------------------------------------------------- */
+
+#define USE_SHOW_HELP
+#include "ksz_common.c"
+
+/* For ksz_request used by PTP or MRP driver. */
+#include "ksz_req.c"
+
+/* -------------------------------------------------------------------------- */
+
+#define MIB_READ_INTERVAL		(HZ / 2)
+
+static u8 sw_r8(struct ksz_sw *sw, unsigned reg)
+{
+	return HW_R8(sw->dev, reg);
+}
+
+static u16 sw_r16(struct ksz_sw *sw, unsigned reg)
+{
+	return HW_R16(sw->dev, reg);
+}
+
+static u32 sw_r32(struct ksz_sw *sw, unsigned reg)
+{
+	return HW_R32(sw->dev, reg);
+}
+
+static void sw_w8(struct ksz_sw *sw, unsigned reg, unsigned val)
+{
+	HW_W8(sw->dev, reg, val);
+}
+
+static void sw_w16(struct ksz_sw *sw, unsigned reg, unsigned val)
+{
+	HW_W16(sw->dev, reg, val);
+}
+
+static void sw_w32(struct ksz_sw *sw, unsigned reg, unsigned val)
+{
+	HW_W32(sw->dev, reg, val);
+}
+
+#define USE_DIFF_PORT_PRIORITY
+#include "ksz_sw.c"
+
+static struct ksz_sw_reg_ops sw_reg_ops = {
+	.r8			= sw_r8,
+	.r16			= sw_r16,
+	.r32			= sw_r32,
+	.w8			= sw_w8,
+	.w16			= sw_w16,
+	.w32			= sw_w32,
+
+	.r			= sw_r16,
+	.w			= sw_w16,
+
+	.get			= sw_reg_get,
+	.set			= sw_reg_set,
+};
+
+/* -------------------------------------------------------------------------- */
+
+static int rx_1msg;
+static int spi_bus;
+
+static int ksz8463_probe(struct spi_device *spi)
+{
+	struct spi_hw_priv *hw_priv;
+	struct sw_priv *priv;
+
+	spi->bits_per_word = 8;
+
+	priv = kzalloc(sizeof(struct sw_priv), GFP_KERNEL);
+	if (!priv)
+		return -ENOMEM;
+
+	priv->hw_dev = kzalloc(sizeof(struct spi_hw_priv), GFP_KERNEL);
+	if (!priv->hw_dev) {
+		kfree(priv);
+		return -ENOMEM;
+	}
+	hw_priv = priv->hw_dev;
+
+	hw_priv->rx_1msg = rx_1msg;
+	hw_priv->spidev = spi;
+
+	priv->dev = &spi->dev;
+	priv->of_dev = &spi->dev;
+
+	priv->sw.reg = &sw_reg_ops;
+
+	/* initialise pre-made spi transfer messages */
+
+	spi_message_init(&hw_priv->spi_msg1);
+	spi_message_add_tail(&hw_priv->spi_xfer1, &hw_priv->spi_msg1);
+
+	spi_message_init(&hw_priv->spi_msg2);
+	spi_message_add_tail(&hw_priv->spi_xfer2[0], &hw_priv->spi_msg2);
+	spi_message_add_tail(&hw_priv->spi_xfer2[1], &hw_priv->spi_msg2);
+
+	priv->irq = spi->irq;
+	priv->spi_mode = (spi->mode & 3) | 4;
+
+	return ksz_probe(priv);
+}
+
+static int ksz8463_remove(struct spi_device *spi)
+{
+	struct sw_priv *priv = dev_get_drvdata(&spi->dev);
+
+	return ksz_remove(priv);
+}
+
+static const struct spi_device_id ksz8463_spi_ids[] = {
+	{ .name = "ksz8463" },
+	{},
+};
+MODULE_DEVICE_TABLE(spi, ksz8463_spi_ids);
+
+static const struct of_device_id ksz8463_dt_ids[] = {
+	{ .compatible = "microchip,ksz8463" },
+	{},
+};
+MODULE_DEVICE_TABLE(of, ksz8463_dt_ids);
+
+static struct spi_driver ksz8463_driver = {
+	.driver = {
+		.name = KS8463MLI_DEV0,
+		.owner = THIS_MODULE,
+		.of_match_table = of_match_ptr(ksz8463_dt_ids),
+	},
+	.probe = ksz8463_probe,
+	.remove = ksz8463_remove,
+	.id_table = ksz8463_spi_ids,
+};
+
+#if defined(CONFIG_SPI_FTDI) && defined(CONFIG_ARCH_MICREL_PEGASUS)
+static void ksz8463_late_init(void)
+{
+	spi_register_driver(&ksz8463_driver);
+}
+#endif
+
+static int __init ksz8463_init(void)
+{
+	if (spi_bus == 0)
+		sprintf((char *) ksz8463_driver.driver.name, KS8463MLI_DEV0);
+	else
+		sprintf((char *) ksz8463_driver.driver.name, KS8463MLI_DEV2);
+#ifdef DEBUG_MSG
+	if (init_dbg())
+		return -ENOMEM;
+#endif
+
+#if defined(CONFIG_SPI_FTDI) && defined(CONFIG_ARCH_MICREL_PEGASUS)
+	pegasus_register_late_call(ksz8463_late_init);
+	return 0;
+#else
+	return spi_register_driver(&ksz8463_driver);
+#endif
+}
+
+static void
+#ifndef CONFIG_KSZ8463_EMBEDDED
+__exit
+#endif
+ksz8463_exit(void)
+{
+	spi_unregister_driver(&ksz8463_driver);
+#ifdef DEBUG_MSG
+	exit_dbg();
+#endif
+}
+
+#ifndef CONFIG_KSZ8463_EMBEDDED
+subsys_initcall(ksz8463_init);
+module_exit(ksz8463_exit);
+#endif
+
+module_param(rx_1msg, int, 0);
+MODULE_PARM_DESC(rx_1msg,
+	"Configure whether receive one message is used");
+
+module_param(spi_bus, int, 0);
+MODULE_PARM_DESC(spi_bus,
+	"Configure which spi master to use(0=KSZ8692, 2=FTDI)");
+
+#ifndef CONFIG_KSZ8463_EMBEDDED
+MODULE_DESCRIPTION("Microchip KSZ8463 MLI Switch Driver");
+MODULE_AUTHOR("Tristram Ha <Tristram.Ha@microchip.com>");
+MODULE_LICENSE("GPL");
+
+MODULE_ALIAS("spi:ksz8463");
+#endif
diff -Napur linux-linux4microchip-2022.10-orig/drivers/net/ethernet/micrel/spi-ksz8795.c linux-linux4microchip-2022.10/drivers/net/ethernet/micrel/spi-ksz8795.c
--- linux-linux4microchip-2022.10-orig/drivers/net/ethernet/micrel/spi-ksz8795.c	1969-12-31 16:00:00.000000000 -0800
+++ linux-linux4microchip-2022.10/drivers/net/ethernet/micrel/spi-ksz8795.c	2023-03-28 13:16:04.000000000 -0700
@@ -0,0 +1,577 @@
+/**
+ * Microchip KSZ8795 SPI driver
+ *
+ * Copyright (c) 2015-2023 Microchip Technology Inc.
+ * Copyright (c) 2010-2015 Micrel, Inc.
+ *
+ * Copyright 2009 Simtec Electronics
+ *	http://www.simtec.co.uk/
+ *	Ben Dooks <ben@simtec.co.uk>
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+ */
+
+#if 0
+#define DEBUG
+#define DBG
+#define DEBUG_MSG
+#if 0
+#define DBG_LINK
+#endif
+#endif
+
+#ifndef CONFIG_KSZ_SWITCH_EMBEDDED
+#define pr_fmt(fmt) KBUILD_MODNAME ": " fmt
+
+#include <linux/kernel.h>
+#include <linux/module.h>
+#include <linux/interrupt.h>
+#include <linux/of_net.h>
+#include <linux/of_mdio.h>
+#include <linux/phy.h>
+#include <linux/phylink.h>
+#include <linux/platform_device.h>
+#include <linux/sched.h>
+#include <linux/netdevice.h>
+#include <linux/etherdevice.h>
+#include <linux/ethtool.h>
+#include <linux/cache.h>
+#include <linux/crc32.h>
+#include <linux/if_vlan.h>
+#include <linux/ip.h>
+#include <net/ip.h>
+#endif
+
+/* -------------------------------------------------------------------------- */
+
+#include <linux/spi/spi.h>
+#include <linux/debugfs.h>
+#include <linux/seq_file.h>
+
+#include "ksz_cfg_8795.h"
+
+#if 0
+#define USE_MII_MODE
+#endif
+#if 0
+#define USE_GMII_MODE
+#endif
+#if 0
+#define USE_RMII_MODE
+#endif
+#if 0
+#define USE_GMII_100_MODE
+#endif
+#if 0
+#define USE_10_MBIT_MODE
+#ifndef USE_GMII_100_MODE
+#define USE_GMII_100_MODE
+#endif
+#endif
+#if 0
+#define USE_HALF_DUPLEX
+#endif
+
+#define KS8795_DEV			"ksz8795"
+
+#define SW_DRV_RELDATE			"Mar 25, 2023"
+#define SW_DRV_VERSION			"1.2.4"
+
+/* -------------------------------------------------------------------------- */
+
+#define HW_R(ks, reg)		spi_rdreg8(ks, reg)
+#define HW_W(ks, reg, val)	spi_wrreg8(ks, reg, val)
+#define HW_R8(ks, reg)		spi_rdreg8(ks, reg)
+#define HW_W8(ks, reg, val)	spi_wrreg8(ks, reg, val)
+#define HW_R16(ks, reg)		spi_rdreg16(ks, reg)
+#define HW_W16(ks, reg, val)	spi_wrreg16(ks, reg, val)
+#define HW_R32(ks, reg)		spi_rdreg32(ks, reg)
+#define HW_W32(ks, reg, val)	spi_wrreg32(ks, reg, val)
+
+#include "ksz_spi_net.h"
+
+/* -------------------------------------------------------------------------- */
+
+/* SPI frame opcodes */
+#define KS_SPIOP_RD			3
+#define KS_SPIOP_WR			2
+
+#define SPI_ADDR_S			12
+#define SPI_ADDR_M			((1 << SPI_ADDR_S) - 1)
+#define SPI_TURNAROUND_S		1
+
+#define SPI_CMD_LEN			2
+
+/*
+ * SPI register read/write calls.
+ *
+ * All these calls issue SPI transactions to access the chip's registers. They
+ * all require that the necessary lock is held to prevent accesses when the
+ * chip is busy transfering packet data (RX/TX FIFO accesses).
+ */
+
+/**
+ * spi_wrreg - issue write register command
+ * @priv:	The switch device structure.
+ * @reg:	The register address.
+ * @buf:	The data buffer to write.
+ * @txl:	The length of data.
+ *
+ * This is the low level write call that issues the necessary spi message(s)
+ * to write data to the register specified in @reg.
+ */
+static void spi_wrreg(struct sw_priv *priv, u8 reg, void *buf, size_t txl)
+{
+	struct spi_hw_priv *hw_priv = priv->hw_dev;
+	struct spi_device *spi = hw_priv->spidev;
+	u16 *txw = (u16 *) hw_priv->txd;
+	struct spi_transfer *xfer;
+	struct spi_message *msg;
+	int ret;
+
+#ifdef DEBUG
+	if (!mutex_is_locked(&priv->lock))
+		pr_alert("W not locked: %02X\n", reg);
+#endif
+	*txw = ((KS_SPIOP_WR << SPI_ADDR_S) | reg) << SPI_TURNAROUND_S;
+	*txw = cpu_to_be16(*txw);
+
+	/* Own transmit buffer is being used. */
+	if (buf == hw_priv->txd) {
+		xfer = &hw_priv->spi_xfer1;
+		msg = &hw_priv->spi_msg1;
+
+		xfer->tx_buf = hw_priv->txd;
+		xfer->rx_buf = NULL;
+		xfer->len = txl + SPI_CMD_LEN;
+	} else {
+		xfer = hw_priv->spi_xfer2;
+		msg = &hw_priv->spi_msg2;
+
+		xfer->tx_buf = hw_priv->txd;
+		xfer->rx_buf = NULL;
+		xfer->len = SPI_CMD_LEN;
+
+		xfer++;
+		xfer->tx_buf = buf;
+		xfer->rx_buf = NULL;
+		xfer->len = txl;
+	}
+
+	ret = spi_sync(spi, msg);
+	if (ret < 0)
+		pr_alert("spi_sync() failed: %x %u\n", reg, txl);
+}
+
+static void spi_wrreg_size(struct sw_priv *priv, u8 reg, unsigned val,
+			   size_t size)
+{
+	struct spi_hw_priv *hw_priv = priv->hw_dev;
+	u8 *txb = hw_priv->txd;
+	int i = SPI_CMD_LEN;
+	size_t cnt = size;
+
+	do {
+		txb[i++] = (u8)(val >> (8 * (cnt - 1)));
+		cnt--;
+	} while (cnt);
+	spi_wrreg(priv, reg, txb, size);
+}
+
+/**
+ * spi_wrreg32 - write 32bit register value to chip
+ * @priv:	The switch device structure.
+ * @reg:	The register address.
+ * @val:	The value to write.
+ *
+ * Issue a write to put the value @val into the register specified in @reg.
+ */
+static void spi_wrreg32(struct sw_priv *priv, u8 reg, unsigned val)
+{
+	spi_wrreg_size(priv, reg, val, 4);
+}
+
+/**
+ * spi_wrreg16 - write 16bit register value to chip
+ * @priv:	The switch device structure.
+ * @reg:	The register address.
+ * @val:	The value to write.
+ *
+ * Issue a write to put the value @val into the register specified in @reg.
+ */
+static void spi_wrreg16(struct sw_priv *priv, u8 reg, unsigned val)
+{
+	spi_wrreg_size(priv, reg, val, 2);
+}
+
+/**
+ * spi_wrreg8 - write 8bit register value to chip
+ * @priv:	The switch device structure.
+ * @reg:	The register address.
+ * @val:	The value to write.
+ *
+ * Issue a write to put the value @val into the register specified in @reg.
+ */
+static void spi_wrreg8(struct sw_priv *priv, u8 reg, unsigned val)
+{
+	spi_wrreg_size(priv, reg, val, 1);
+}
+
+/**
+ * ksz_rx_1msg - select whether to use one or two messages for spi read
+ * @hw_priv:	The hardware device structure.
+ *
+ * Return whether to generate a single message with a tx and rx buffer
+ * supplied to spi_sync(), or alternatively send the tx and rx buffers
+ * as separate messages.
+ *
+ * Depending on the hardware in use, a single message may be more efficient
+ * on interrupts or work done by the driver.
+ *
+ * This currently always returns false until we add some per-device data passed
+ * from the platform code to specify which mode is better.
+ */
+static inline bool ksz_rx_1msg(struct spi_hw_priv *hw_priv)
+{
+	return hw_priv->rx_1msg;
+}
+
+/**
+ * spi_rdreg - issue read register command and return the data
+ * @priv:	The switch device structure.
+ * @reg:	The register address.
+ * @rxb:	The RX buffer to return the result into.
+ * @rxl:	The length of data expected.
+ *
+ * This is the low level read call that issues the necessary spi message(s)
+ * to read data from the register specified in @reg.
+ */
+static void spi_rdreg(struct sw_priv *priv, u8 reg, void *rxb, void **rx,
+		      size_t rxl)
+{
+	struct spi_hw_priv *hw_priv = priv->hw_dev;
+	struct spi_device *spi = hw_priv->spidev;
+	u16 *txw = (u16 *) hw_priv->txd;
+	struct spi_transfer *xfer;
+	struct spi_message *msg;
+	int ret;
+
+#ifdef DEBUG
+	if (!mutex_is_locked(&priv->lock))
+		pr_alert("R not locked: %02X\n", reg);
+#endif
+	*txw = ((KS_SPIOP_RD << SPI_ADDR_S) | reg) << SPI_TURNAROUND_S;
+	*txw = cpu_to_be16(*txw);
+
+	if (rx)
+		*rx = rxb;
+	if (ksz_rx_1msg(hw_priv)) {
+		msg = &hw_priv->spi_msg1;
+		xfer = &hw_priv->spi_xfer1;
+
+		xfer->tx_buf = hw_priv->txd;
+		xfer->rx_buf = hw_priv->rxd;
+		xfer->len = rxl + SPI_CMD_LEN;
+		++txw;
+		memset(txw, 0, rxl);
+		if (rx && rxb == hw_priv->rxd)
+			*rx = (u8 *) rxb + SPI_CMD_LEN;
+#if defined(CONFIG_SPI_PEGASUS) || defined(CONFIG_SPI_PEGASUS_MODULE)
+		/*
+		 * A hack to tell KSZ8692 SPI host controller the read command.
+		 */
+		memcpy(hw_priv->rxd, hw_priv->txd, SPI_CMD_LEN + 1);
+		hw_priv->rxd[SPI_CMD_LEN] ^= 0xff;
+#endif
+	} else {
+		msg = &hw_priv->spi_msg2;
+		xfer = hw_priv->spi_xfer2;
+
+		xfer->tx_buf = hw_priv->txd;
+		xfer->rx_buf = NULL;
+		xfer->len = SPI_CMD_LEN;
+
+		xfer++;
+		xfer->tx_buf = NULL;
+		xfer->rx_buf = rxb;
+		xfer->len = rxl;
+	}
+
+	ret = spi_sync(spi, msg);
+	if (ret < 0)
+		pr_alert("read: spi_sync() failed: %x %u\n", reg, rxl);
+	else if (ksz_rx_1msg(hw_priv) && rxb != hw_priv->rxd)
+		memcpy(rxb, hw_priv->rxd + SPI_CMD_LEN, rxl);
+}
+
+static void *spi_rdreg_size(struct sw_priv *priv, u8 reg, size_t size)
+{
+	struct spi_hw_priv *hw_priv = priv->hw_dev;
+	void *r;
+
+	spi_rdreg(priv, reg, hw_priv->rxd, &r, size);
+	return r;
+}
+
+/**
+ * spi_rdreg8 - read 8 bit register from device
+ * @priv:	The switch device structure.
+ * @reg:	The register address.
+ *
+ * Read a 8bit register from the chip, returning the result.
+ */
+static u8 spi_rdreg8(struct sw_priv *priv, u8 reg)
+{
+	u8 *rx;
+
+	rx = spi_rdreg_size(priv, reg, 1);
+	return rx[0];
+}
+
+/**
+ * spi_rdreg16 - read 16 bit register from device
+ * @priv:	The switch device structure.
+ * @reg:	The register address.
+ *
+ * Read a 16bit register from the chip, returning the result.
+ */
+static u16 spi_rdreg16(struct sw_priv *priv, u8 reg)
+{
+	u16 *rx;
+
+	rx = spi_rdreg_size(priv, reg, 2);
+	return be16_to_cpu(*rx);
+}
+
+/**
+ * spi_rdreg32 - read 32 bit register from device
+ * @priv:	The switch device structure.
+ * @reg:	The register address.
+ *
+ * Read a 32bit register from the chip.
+ *
+ * Note, this read requires the address be aligned to 4 bytes.
+ */
+static u32 spi_rdreg32(struct sw_priv *priv, u8 reg)
+{
+	u32 *rx;
+
+	rx = spi_rdreg_size(priv, reg, 4);
+	return be32_to_cpu(*rx);
+}
+
+/* -------------------------------------------------------------------------- */
+
+#define USE_SHOW_HELP
+#include "ksz_common.c"
+
+#include "ksz_req.c"
+
+/* -------------------------------------------------------------------------- */
+
+#define MIB_READ_INTERVAL		(HZ / 2)
+
+static void sw_lock(struct ksz_sw *sw)
+{
+	mutex_lock(sw->reglock);
+}  /* sw_lock */
+
+static void sw_unlock(struct ksz_sw *sw)
+{
+	mutex_unlock(sw->reglock);
+}  /* sw_unlock */
+
+static void sw_r(struct ksz_sw *sw, unsigned reg, void *buf, size_t cnt)
+{
+	if (cnt + SPI_CMD_LEN > 128)
+		cnt = 128 - SPI_CMD_LEN;
+	spi_rdreg(sw->dev, reg, buf, NULL, cnt);
+}
+
+static void sw_w(struct ksz_sw *sw, unsigned reg, void *buf, size_t cnt)
+{
+	spi_wrreg(sw->dev, reg, buf, cnt);
+}
+
+static u8 sw_r8(struct ksz_sw *sw, unsigned reg)
+{
+	return HW_R8(sw->dev, reg);
+}
+
+static u16 sw_r16(struct ksz_sw *sw, unsigned reg)
+{
+	return HW_R16(sw->dev, reg);
+}
+
+static u32 sw_r32(struct ksz_sw *sw, unsigned reg)
+{
+	return HW_R32(sw->dev, reg);
+}
+
+static void sw_w8(struct ksz_sw *sw, unsigned reg, unsigned val)
+{
+	HW_W8(sw->dev, reg, val);
+}
+
+static void sw_w16(struct ksz_sw *sw, unsigned reg, unsigned val)
+{
+	HW_W16(sw->dev, reg, val);
+}
+
+static void sw_w32(struct ksz_sw *sw, unsigned reg, unsigned val)
+{
+	HW_W32(sw->dev, reg, val);
+}
+
+#include "ksz_sw_8795.c"
+
+static struct ksz_sw_reg_ops sw_reg_ops = {
+	.lock			= sw_lock,
+	.unlock			= sw_unlock,
+
+	.r8			= sw_r8,
+	.r16			= sw_r16,
+	.r32			= sw_r32,
+	.w8			= sw_w8,
+	.w16			= sw_w16,
+	.w32			= sw_w32,
+
+	.r			= sw_r,
+	.w			= sw_w,
+
+	.get			= sw_reg_get,
+	.set			= sw_reg_set,
+};
+
+/* -------------------------------------------------------------------------- */
+
+static int rx_1msg;
+static int spi_bus;
+
+static int ksz8795_probe(struct spi_device *spi)
+{
+	struct spi_hw_priv *hw_priv;
+	struct sw_priv *priv;
+
+	spi->bits_per_word = 8;
+
+	priv = kzalloc(sizeof(struct sw_priv), GFP_KERNEL);
+	if (!priv)
+		return -ENOMEM;
+
+	priv->hw_dev = kzalloc(sizeof(struct spi_hw_priv), GFP_KERNEL);
+	if (!priv->hw_dev) {
+		kfree(priv);
+		return -ENOMEM;
+	}
+	hw_priv = priv->hw_dev;
+
+	hw_priv->rx_1msg = rx_1msg;
+	hw_priv->spidev = spi;
+
+	priv->dev = &spi->dev;
+	priv->of_dev = &spi->dev;
+
+	priv->sw.reg = &sw_reg_ops;
+
+	/* initialise pre-made spi transfer messages */
+
+	spi_message_init(&hw_priv->spi_msg1);
+	spi_message_add_tail(&hw_priv->spi_xfer1, &hw_priv->spi_msg1);
+
+	spi_message_init(&hw_priv->spi_msg2);
+	spi_message_add_tail(&hw_priv->spi_xfer2[0], &hw_priv->spi_msg2);
+	spi_message_add_tail(&hw_priv->spi_xfer2[1], &hw_priv->spi_msg2);
+
+	priv->irq = spi->irq;
+	priv->spi_mode = (spi->mode & 3) | 4;
+
+	return ksz_probe(priv);
+}
+
+static int ksz8795_remove(struct spi_device *spi)
+{
+	struct sw_priv *priv = dev_get_drvdata(&spi->dev);
+
+	return ksz_remove(priv);
+}
+
+static const struct spi_device_id ksz8795_spi_ids[] = {
+	{ .name = "ksz8795" },
+	{ .name = "ksz8794" },
+	{ .name = "ksz8765" },
+	{},
+};
+MODULE_DEVICE_TABLE(spi, ksz8795_spi_ids);
+
+static const struct of_device_id ksz8795_dt_ids[] = {
+	{ .compatible = "microchip,ksz8795" },
+	{ .compatible = "microchip,ksz8794" },
+	{ .compatible = "microchip,ksz8765" },
+	{},
+};
+MODULE_DEVICE_TABLE(of, ksz8795_dt_ids);
+
+static struct spi_driver ksz8795_driver = {
+	.driver = {
+		.name = KS8795_DEV,
+		.owner = THIS_MODULE,
+		.of_match_table = of_match_ptr(ksz8795_dt_ids),
+	},
+	.probe = ksz8795_probe,
+	.remove = ksz8795_remove,
+	.id_table = ksz8795_spi_ids,
+};
+
+static int __init ksz8795_init(void)
+{
+#ifdef DEBUG_MSG
+	if (init_dbg())
+		return -ENOMEM;
+#endif
+	return spi_register_driver(&ksz8795_driver);
+}
+
+#ifndef CONFIG_KSZ_SWITCH_EMBEDDED
+static void __exit ksz8795_exit(void)
+#else
+static void ksz8795_exit(void)
+#endif
+{
+	spi_unregister_driver(&ksz8795_driver);
+#ifdef DEBUG_MSG
+	exit_dbg();
+#endif
+}
+
+#ifndef CONFIG_KSZ_SWITCH_EMBEDDED
+subsys_initcall(ksz8795_init);
+module_exit(ksz8795_exit);
+#endif
+
+module_param(rx_1msg, int, 0);
+MODULE_PARM_DESC(rx_1msg,
+	"Configure whether receive one message is used");
+
+module_param(spi_bus, int, 0);
+MODULE_PARM_DESC(spi_bus,
+	"Configure which spi master to use(0=KSZ8692, 2=FTDI)");
+
+#ifndef CONFIG_KSZ_SWITCH_EMBEDDED
+MODULE_DESCRIPTION("KSZ8795 SPI switch driver");
+MODULE_AUTHOR("Tristram Ha <Tristram.Ha@microchip.com>");
+MODULE_LICENSE("GPL");
+
+MODULE_ALIAS("spi:ksz8795");
+#endif
diff -Napur linux-linux4microchip-2022.10-orig/drivers/net/ethernet/micrel/spi-ksz8863.c linux-linux4microchip-2022.10/drivers/net/ethernet/micrel/spi-ksz8863.c
--- linux-linux4microchip-2022.10-orig/drivers/net/ethernet/micrel/spi-ksz8863.c	1969-12-31 16:00:00.000000000 -0800
+++ linux-linux4microchip-2022.10/drivers/net/ethernet/micrel/spi-ksz8863.c	2023-03-25 13:53:22.248643385 -0700
@@ -0,0 +1,525 @@
+/**
+ * Microchip KSZ8863 SPI driver
+ *
+ * Copyright (c) 2015-2023 Microchip Technology Inc.
+ * Copyright (c) 2010-2015 Micrel, Inc.
+ *
+ * Copyright 2009 Simtec Electronics
+ *	http://www.simtec.co.uk/
+ *	Ben Dooks <ben@simtec.co.uk>
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+ */
+
+#if 0
+#define DEBUG
+#define DBG
+#define DEBUG_MSG
+#endif
+
+#ifndef CONFIG_KSZ_SWITCH_EMBEDDED
+#define pr_fmt(fmt) KBUILD_MODNAME ": " fmt
+
+#include <linux/kernel.h>
+#include <linux/module.h>
+#include <linux/interrupt.h>
+#include <linux/of_net.h>
+#include <linux/of_mdio.h>
+#include <linux/phy.h>
+#include <linux/phylink.h>
+#include <linux/platform_device.h>
+#include <linux/sched.h>
+#include <linux/netdevice.h>
+#include <linux/etherdevice.h>
+#include <linux/ethtool.h>
+#include <linux/cache.h>
+#include <linux/crc32.h>
+#include <linux/if_vlan.h>
+#include <linux/ip.h>
+#include <net/ip.h>
+#endif
+
+/* -------------------------------------------------------------------------- */
+
+#include <linux/spi/spi.h>
+#include <linux/debugfs.h>
+#include <linux/seq_file.h>
+
+#include "ksz_cfg_8863.h"
+#ifndef PHY_RESET_NOT
+#define PHY_RESET_NOT			PHY_RESET
+#endif
+
+#define KS8863MLI_DEV0			"ksz8863"
+#define KS8863MLI_DEV2			"ksz8863_2"
+
+#define SW_DRV_RELDATE			"Mar 25, 2023"
+#define SW_DRV_VERSION			"1.2.3"
+
+/* -------------------------------------------------------------------------- */
+
+#define HW_R(ks, reg)		spi_rdreg8(ks, reg)
+#define HW_W(ks, reg, val)	spi_wrreg8(ks, reg, val)
+#define HW_R8(ks, reg)		spi_rdreg8(ks, reg)
+#define HW_W8(ks, reg, val)	spi_wrreg8(ks, reg, val)
+#define HW_R16(ks, reg)		spi_rdreg16(ks, reg)
+#define HW_W16(ks, reg, val)	spi_wrreg16(ks, reg, val)
+#define HW_R32(ks, reg)		spi_rdreg32(ks, reg)
+#define HW_W32(ks, reg, val)	spi_wrreg32(ks, reg, val)
+
+#include "ksz_spi_net.h"
+
+/* -------------------------------------------------------------------------- */
+
+/* SPI frame opcodes */
+#define KS_SPIOP_RD			3
+#define KS_SPIOP_WR			2
+
+#define SPI_CMD_LEN			2
+
+/*
+ * SPI register read/write calls.
+ *
+ * All these calls issue SPI transactions to access the chip's registers. They
+ * all require that the necessary lock is held to prevent accesses when the
+ * chip is busy transfering packet data (RX/TX FIFO accesses).
+ */
+
+/**
+ * spi_wrreg - issue write register command
+ * @priv:	The switch device structure.
+ * @reg:	The register address.
+ * @buf:	The data buffer to write.
+ * @txl:	The length of data.
+ *
+ * This is the low level write call that issues the necessary spi message(s)
+ * to write data to the register specified in @reg.
+ */
+static void spi_wrreg(struct sw_priv *priv, u8 reg, void *buf, size_t txl)
+{
+	struct spi_hw_priv *hw_priv = priv->hw_dev;
+	struct spi_device *spi = hw_priv->spidev;
+	u8 *txb = (u8 *) hw_priv->txd;
+	struct spi_transfer *xfer;
+	struct spi_message *msg;
+	int ret;
+
+#ifdef DEBUG
+	if (!mutex_is_locked(&priv->lock))
+		pr_alert("W not locked: %02X\n", reg);
+#endif
+	txb[0] = KS_SPIOP_WR;
+	txb[1] = reg;
+
+	/* Own transmit buffer is being used. */
+	if (buf == hw_priv->txd) {
+		xfer = &hw_priv->spi_xfer1;
+		msg = &hw_priv->spi_msg1;
+
+		xfer->tx_buf = hw_priv->txd;
+		xfer->rx_buf = NULL;
+		xfer->len = txl + SPI_CMD_LEN;
+	} else {
+		xfer = hw_priv->spi_xfer2;
+		msg = &hw_priv->spi_msg2;
+
+		xfer->tx_buf = hw_priv->txd;
+		xfer->rx_buf = NULL;
+		xfer->len = SPI_CMD_LEN;
+
+		xfer++;
+		xfer->tx_buf = buf;
+		xfer->rx_buf = NULL;
+		xfer->len = txl;
+	}
+
+	ret = spi_sync(spi, msg);
+	if (ret < 0)
+		pr_alert("spi_sync() failed: %x %u\n", reg, txl);
+}
+
+static void spi_wrreg_size(struct sw_priv *priv, u8 reg, unsigned val,
+			   size_t size)
+{
+	struct spi_hw_priv *hw_priv = priv->hw_dev;
+	u8 *txb = hw_priv->txd;
+	int i = SPI_CMD_LEN;
+	size_t cnt = size;
+
+	do {
+		txb[i++] = (u8)(val >> (8 * (cnt - 1)));
+		cnt--;
+	} while (cnt);
+	spi_wrreg(priv, reg, txb, size);
+}
+
+/**
+ * spi_wrreg32 - write 32bit register value to chip
+ * @priv:	The switch device structure.
+ * @reg:	The register address.
+ * @val:	The value to write.
+ *
+ * Issue a write to put the value @val into the register specified in @reg.
+ */
+static void spi_wrreg32(struct sw_priv *priv, u8 reg, unsigned val)
+{
+	spi_wrreg_size(priv, reg, val, 4);
+}
+
+/**
+ * spi_wrreg16 - write 16bit register value to chip
+ * @priv:	The switch device structure.
+ * @reg:	The register address.
+ * @val:	The value to write.
+ *
+ * Issue a write to put the value @val into the register specified in @reg.
+ */
+static void spi_wrreg16(struct sw_priv *priv, u8 reg, unsigned val)
+{
+	spi_wrreg_size(priv, reg, val, 2);
+}
+
+/**
+ * spi_wrreg8 - write 8bit register value to chip
+ * @priv:	The switch device structure.
+ * @reg:	The register address.
+ * @val:	The value to write.
+ *
+ * Issue a write to put the value @val into the register specified in @reg.
+ */
+static void spi_wrreg8(struct sw_priv *priv, u8 reg, unsigned val)
+{
+	spi_wrreg_size(priv, reg, val, 1);
+}
+
+/**
+ * ksz_rx_1msg - select whether to use one or two messages for spi read
+ * @hw_priv:	The hardware device structure.
+ *
+ * Return whether to generate a single message with a tx and rx buffer
+ * supplied to spi_sync(), or alternatively send the tx and rx buffers
+ * as separate messages.
+ *
+ * Depending on the hardware in use, a single message may be more efficient
+ * on interrupts or work done by the driver.
+ *
+ * This currently always returns false until we add some per-device data passed
+ * from the platform code to specify which mode is better.
+ */
+static inline bool ksz_rx_1msg(struct spi_hw_priv *hw_priv)
+{
+	return hw_priv->rx_1msg;
+}
+
+/**
+ * spi_rdreg - issue read register command and return the data
+ * @priv:	The switch device structure.
+ * @reg:	The register address.
+ * @rxb:	The RX buffer to return the result into.
+ * @rxl:	The length of data expected.
+ *
+ * This is the low level read call that issues the necessary spi message(s)
+ * to read data from the register specified in @reg.
+ */
+static void spi_rdreg(struct sw_priv *priv, u8 reg, void *rxb, void **rx,
+		      size_t rxl)
+{
+	struct spi_hw_priv *hw_priv = priv->hw_dev;
+	struct spi_device *spi = hw_priv->spidev;
+	u8 *txb = (u8 *) hw_priv->txd;
+	struct spi_transfer *xfer;
+	struct spi_message *msg;
+	int ret;
+
+#ifdef DEBUG
+	if (!mutex_is_locked(&priv->lock))
+		pr_alert("R not locked: %02X\n", reg);
+#endif
+	txb[0] = KS_SPIOP_RD;
+	txb[1] = reg;
+
+	if (rx)
+		*rx = rxb;
+	if (ksz_rx_1msg(hw_priv)) {
+		msg = &hw_priv->spi_msg1;
+		xfer = &hw_priv->spi_xfer1;
+
+		xfer->tx_buf = hw_priv->txd;
+		xfer->rx_buf = hw_priv->rxd;
+		xfer->len = rxl + SPI_CMD_LEN;
+		txb += SPI_CMD_LEN;
+		memset(txb, 0, rxl);
+		if (rx && rxb == hw_priv->rxd)
+			*rx = (u8 *) rxb + SPI_CMD_LEN;
+#if defined(CONFIG_SPI_PEGASUS) || defined(CONFIG_SPI_PEGASUS_MODULE)
+		/*
+		 * A hack to tell KSZ8692 SPI host controller the read command.
+		 */
+		memcpy(hw_priv->rxd, hw_priv->txd, SPI_CMD_LEN + 1);
+		hw_priv->rxd[SPI_CMD_LEN] ^= 0xff;
+#endif
+	} else {
+		msg = &hw_priv->spi_msg2;
+		xfer = hw_priv->spi_xfer2;
+
+		xfer->tx_buf = hw_priv->txd;
+		xfer->rx_buf = NULL;
+		xfer->len = SPI_CMD_LEN;
+
+		xfer++;
+		xfer->tx_buf = NULL;
+		xfer->rx_buf = rxb;
+		xfer->len = rxl;
+	}
+
+	ret = spi_sync(spi, msg);
+	if (ret < 0)
+		pr_alert("read: spi_sync() failed: %x %u\n", reg, rxl);
+	else if (ksz_rx_1msg(hw_priv) && rxb != hw_priv->rxd)
+		memcpy(rxb, hw_priv->rxd + SPI_CMD_LEN, rxl);
+}
+
+static void *spi_rdreg_size(struct sw_priv *priv, u8 reg, size_t size)
+{
+	struct spi_hw_priv *hw_priv = priv->hw_dev;
+	void *r;
+
+	spi_rdreg(priv, reg, hw_priv->rxd, &r, size);
+	return r;
+}
+
+/**
+ * spi_rdreg8 - read 8 bit register from device
+ * @priv:	The switch device structure.
+ * @reg:	The register address.
+ *
+ * Read a 8bit register from the chip, returning the result.
+ */
+static u8 spi_rdreg8(struct sw_priv *priv, u8 reg)
+{
+	u8 *rx;
+
+	rx = spi_rdreg_size(priv, reg, 1);
+	return rx[0];
+}
+
+/**
+ * spi_rdreg16 - read 16 bit register from device
+ * @priv:	The switch device structure.
+ * @reg:	The register address.
+ *
+ * Read a 16bit register from the chip, returning the result.
+ */
+static u16 spi_rdreg16(struct sw_priv *priv, u8 reg)
+{
+	u16 *rx;
+
+	rx = spi_rdreg_size(priv, reg, 2);
+	return be16_to_cpu(*rx);
+}
+
+/**
+ * spi_rdreg32 - read 32 bit register from device
+ * @priv:	The switch device structure.
+ * @reg:	The register address.
+ *
+ * Read a 32bit register from the chip.
+ *
+ * Note, this read requires the address be aligned to 4 bytes.
+ */
+static u32 spi_rdreg32(struct sw_priv *priv, u8 reg)
+{
+	u32 *rx;
+
+	rx = spi_rdreg_size(priv, reg, 4);
+	return be32_to_cpu(*rx);
+}
+
+/* -------------------------------------------------------------------------- */
+
+#define USE_SHOW_HELP
+#include "ksz_common.c"
+
+#include "ksz_req.c"
+
+/* -------------------------------------------------------------------------- */
+
+#define MIB_READ_INTERVAL		(HZ / 2)
+
+static u8 sw_r8(struct ksz_sw *sw, unsigned reg)
+{
+	return HW_R8(sw->dev, reg);
+}
+
+static u16 sw_r16(struct ksz_sw *sw, unsigned reg)
+{
+	return HW_R16(sw->dev, reg);
+}
+
+static u32 sw_r32(struct ksz_sw *sw, unsigned reg)
+{
+	return HW_R32(sw->dev, reg);
+}
+
+static void sw_w8(struct ksz_sw *sw, unsigned reg, unsigned val)
+{
+	HW_W8(sw->dev, reg, val);
+}
+
+static void sw_w16(struct ksz_sw *sw, unsigned reg, unsigned val)
+{
+	HW_W16(sw->dev, reg, val);
+}
+
+static void sw_w32(struct ksz_sw *sw, unsigned reg, unsigned val)
+{
+	HW_W32(sw->dev, reg, val);
+}
+
+#include "ksz_sw.c"
+
+static struct ksz_sw_reg_ops sw_reg_ops = {
+	.r8			= sw_r8,
+	.r16			= sw_r16,
+	.r32			= sw_r32,
+	.w8			= sw_w8,
+	.w16			= sw_w16,
+	.w32			= sw_w32,
+
+	.r			= sw_r8,
+	.w			= sw_w8,
+
+	.get			= sw_reg_get,
+	.set			= sw_reg_set,
+};
+
+/* -------------------------------------------------------------------------- */
+
+static int rx_1msg;
+static int spi_bus;
+
+static int ksz8863_probe(struct spi_device *spi)
+{
+	struct spi_hw_priv *hw_priv;
+	struct sw_priv *priv;
+
+	spi->bits_per_word = 8;
+
+	priv = kzalloc(sizeof(struct sw_priv), GFP_KERNEL);
+	if (!priv)
+		return -ENOMEM;
+
+	priv->hw_dev = kzalloc(sizeof(struct spi_hw_priv), GFP_KERNEL);
+	if (!priv->hw_dev) {
+		kfree(priv);
+		return -ENOMEM;
+	}
+	hw_priv = priv->hw_dev;
+
+	hw_priv->rx_1msg = rx_1msg;
+	hw_priv->spidev = spi;
+
+	priv->dev = &spi->dev;
+	priv->of_dev = &spi->dev;
+
+	priv->sw.reg = &sw_reg_ops;
+
+	/* initialise pre-made spi transfer messages */
+
+	spi_message_init(&hw_priv->spi_msg1);
+	spi_message_add_tail(&hw_priv->spi_xfer1, &hw_priv->spi_msg1);
+
+	spi_message_init(&hw_priv->spi_msg2);
+	spi_message_add_tail(&hw_priv->spi_xfer2[0], &hw_priv->spi_msg2);
+	spi_message_add_tail(&hw_priv->spi_xfer2[1], &hw_priv->spi_msg2);
+
+	priv->irq = spi->irq;
+	priv->spi_mode = (spi->mode & 3) | 4;
+
+	return ksz_probe(priv);
+}
+
+static int ksz8863_remove(struct spi_device *spi)
+{
+	struct sw_priv *priv = dev_get_drvdata(&spi->dev);
+
+	return ksz_remove(priv);
+}
+
+static const struct spi_device_id ksz8863_spi_ids[] = {
+	{ .name = "ksz8863" },
+	{ .name = "ksz8873" },
+	{},
+};
+MODULE_DEVICE_TABLE(spi, ksz8863_spi_ids);
+
+static const struct of_device_id ksz8863_dt_ids[] = {
+	{ .compatible = "microchip,ksz8863" },
+	{ .compatible = "microchip,ksz8873" },
+	{},
+};
+MODULE_DEVICE_TABLE(of, ksz8863_dt_ids);
+
+static struct spi_driver ksz8863_driver = {
+	.driver = {
+		.name = KS8863MLI_DEV0,
+		.owner = THIS_MODULE,
+		.of_match_table = of_match_ptr(ksz8863_dt_ids),
+	},
+	.probe = ksz8863_probe,
+	.remove = ksz8863_remove,
+	.id_table = ksz8863_spi_ids,
+};
+
+static int __init ksz8863_init(void)
+{
+#ifdef DEBUG_MSG
+	if (init_dbg())
+		return -ENOMEM;
+#endif
+	return spi_register_driver(&ksz8863_driver);
+}
+
+#ifndef CONFIG_KSZ_SWITCH_EMBEDDED
+static void __exit ksz8863_exit(void)
+#else
+static void ksz8863_exit(void)
+#endif
+{
+	spi_unregister_driver(&ksz8863_driver);
+#ifdef DEBUG_MSG
+	exit_dbg();
+#endif
+}
+
+#ifndef CONFIG_KSZ_SWITCH_EMBEDDED
+subsys_initcall(ksz8863_init);
+module_exit(ksz8863_exit);
+#endif
+
+module_param(rx_1msg, int, 0);
+MODULE_PARM_DESC(rx_1msg,
+	"Configure whether receive one message is used");
+
+module_param(spi_bus, int, 0);
+MODULE_PARM_DESC(spi_bus,
+	"Configure which spi master to use(0=KSZ8692, 2=FTDI)");
+
+#ifndef CONFIG_KSZ_SWITCH_EMBEDDED
+MODULE_DESCRIPTION("KSZ8863 SPI switch driver");
+MODULE_AUTHOR("Tristram Ha <Tristram.Ha@microchip.com>");
+MODULE_LICENSE("GPL");
+
+MODULE_ALIAS("spi:ksz8863");
+#endif
diff -Napur linux-linux4microchip-2022.10-orig/drivers/net/ethernet/micrel/spi-ksz8895.c linux-linux4microchip-2022.10/drivers/net/ethernet/micrel/spi-ksz8895.c
--- linux-linux4microchip-2022.10-orig/drivers/net/ethernet/micrel/spi-ksz8895.c	1969-12-31 16:00:00.000000000 -0800
+++ linux-linux4microchip-2022.10/drivers/net/ethernet/micrel/spi-ksz8895.c	2023-03-25 13:53:14.056643042 -0700
@@ -0,0 +1,531 @@
+/**
+ * Microchip KSZ8895 SPI driver
+ *
+ * Copyright (c) 2015-2023 Microchip Technology Inc.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+ */
+
+#if 0
+#define DEBUG
+#define DBG
+#define DEBUG_MSG
+#if 0
+#define DBG_LINK
+#endif
+#endif
+
+#ifndef CONFIG_KSZ_SWITCH_EMBEDDED
+#define pr_fmt(fmt) KBUILD_MODNAME ": " fmt
+
+#include <linux/kernel.h>
+#include <linux/module.h>
+#include <linux/interrupt.h>
+#include <linux/of_net.h>
+#include <linux/of_mdio.h>
+#include <linux/phy.h>
+#include <linux/phylink.h>
+#include <linux/platform_device.h>
+#include <linux/sched.h>
+#include <linux/netdevice.h>
+#include <linux/etherdevice.h>
+#include <linux/ethtool.h>
+#include <linux/cache.h>
+#include <linux/crc32.h>
+#include <linux/if_vlan.h>
+#include <linux/ip.h>
+#include <net/ip.h>
+#endif
+
+/* -------------------------------------------------------------------------- */
+
+#include <linux/spi/spi.h>
+#include <linux/debugfs.h>
+#include <linux/seq_file.h>
+
+#include "ksz_cfg_8895.h"
+
+#define KS8895_DEV			"ksz8895"
+
+#define SW_DRV_RELDATE			"Mar 25, 2023"
+#define SW_DRV_VERSION			"1.2.3"
+
+/* -------------------------------------------------------------------------- */
+
+#define HW_R(ks, reg)		spi_rdreg8(ks, reg)
+#define HW_W(ks, reg, val)	spi_wrreg8(ks, reg, val)
+#define HW_R8(ks, reg)		spi_rdreg8(ks, reg)
+#define HW_W8(ks, reg, val)	spi_wrreg8(ks, reg, val)
+#define HW_R16(ks, reg)		spi_rdreg16(ks, reg)
+#define HW_W16(ks, reg, val)	spi_wrreg16(ks, reg, val)
+#define HW_R32(ks, reg)		spi_rdreg32(ks, reg)
+#define HW_W32(ks, reg, val)	spi_wrreg32(ks, reg, val)
+
+#include "ksz_spi_net.h"
+
+/* -------------------------------------------------------------------------- */
+
+/* SPI frame opcodes */
+#define KS_SPIOP_RD			3
+#define KS_SPIOP_WR			2
+
+#define SPI_CMD_LEN			2
+
+/*
+ * SPI register read/write calls.
+ *
+ * All these calls issue SPI transactions to access the chip's registers. They
+ * all require that the necessary lock is held to prevent accesses when the
+ * chip is busy transfering packet data (RX/TX FIFO accesses).
+ */
+
+/**
+ * spi_wrreg - issue write register command
+ * @priv:	The switch device structure.
+ * @reg:	The register address.
+ * @buf:	The data buffer to write.
+ * @txl:	The length of data.
+ *
+ * This is the low level write call that issues the necessary spi message(s)
+ * to write data to the register specified in @reg.
+ */
+static void spi_wrreg(struct sw_priv *priv, u8 reg, void *buf, size_t txl)
+{
+	struct spi_hw_priv *hw_priv = priv->hw_dev;
+	struct spi_device *spi = hw_priv->spidev;
+	u8 *txb = (u8 *) hw_priv->txd;
+	struct spi_transfer *xfer;
+	struct spi_message *msg;
+	int ret;
+
+#ifdef DEBUG
+	if (!mutex_is_locked(&priv->lock))
+		pr_alert("W not locked: %02X\n", reg);
+#endif
+	txb[0] = KS_SPIOP_WR;
+	txb[1] = reg;
+
+	/* Own transmit buffer is being used. */
+	if (buf == hw_priv->txd) {
+		xfer = &hw_priv->spi_xfer1;
+		msg = &hw_priv->spi_msg1;
+
+		xfer->tx_buf = hw_priv->txd;
+		xfer->rx_buf = NULL;
+		xfer->len = txl + SPI_CMD_LEN;
+	} else {
+		xfer = hw_priv->spi_xfer2;
+		msg = &hw_priv->spi_msg2;
+
+		xfer->tx_buf = hw_priv->txd;
+		xfer->rx_buf = NULL;
+		xfer->len = SPI_CMD_LEN;
+
+		xfer++;
+		xfer->tx_buf = buf;
+		xfer->rx_buf = NULL;
+		xfer->len = txl;
+	}
+
+	ret = spi_sync(spi, msg);
+	if (ret < 0)
+		pr_alert("spi_sync() failed: %x %u\n", reg, txl);
+}
+
+static void spi_wrreg_size(struct sw_priv *priv, u8 reg, unsigned val,
+			   size_t size)
+{
+	struct spi_hw_priv *hw_priv = priv->hw_dev;
+	u8 *txb = hw_priv->txd;
+	int i = SPI_CMD_LEN;
+	size_t cnt = size;
+
+	do {
+		txb[i++] = (u8)(val >> (8 * (cnt - 1)));
+		cnt--;
+	} while (cnt);
+	spi_wrreg(priv, reg, txb, size);
+}
+
+/**
+ * spi_wrreg32 - write 32bit register value to chip
+ * @priv:	The switch device structure.
+ * @reg:	The register address.
+ * @val:	The value to write.
+ *
+ * Issue a write to put the value @val into the register specified in @reg.
+ */
+static void spi_wrreg32(struct sw_priv *priv, u8 reg, unsigned val)
+{
+	spi_wrreg_size(priv, reg, val, 4);
+}
+
+/**
+ * spi_wrreg16 - write 16bit register value to chip
+ * @priv:	The switch device structure.
+ * @reg:	The register address.
+ * @val:	The value to write.
+ *
+ * Issue a write to put the value @val into the register specified in @reg.
+ */
+static void spi_wrreg16(struct sw_priv *priv, u8 reg, unsigned val)
+{
+	spi_wrreg_size(priv, reg, val, 2);
+}
+
+/**
+ * spi_wrreg8 - write 8bit register value to chip
+ * @priv:	The switch device structure.
+ * @reg:	The register address.
+ * @val:	The value to write.
+ *
+ * Issue a write to put the value @val into the register specified in @reg.
+ */
+static void spi_wrreg8(struct sw_priv *priv, u8 reg, unsigned val)
+{
+	spi_wrreg_size(priv, reg, val, 1);
+}
+
+/**
+ * ksz_rx_1msg - select whether to use one or two messages for spi read
+ * @hw_priv:	The hardware device structure.
+ *
+ * Return whether to generate a single message with a tx and rx buffer
+ * supplied to spi_sync(), or alternatively send the tx and rx buffers
+ * as separate messages.
+ *
+ * Depending on the hardware in use, a single message may be more efficient
+ * on interrupts or work done by the driver.
+ *
+ * This currently always returns false until we add some per-device data passed
+ * from the platform code to specify which mode is better.
+ */
+static inline bool ksz_rx_1msg(struct spi_hw_priv *hw_priv)
+{
+	return hw_priv->rx_1msg;
+}
+
+/**
+ * spi_rdreg - issue read register command and return the data
+ * @priv:	The switch device structure.
+ * @reg:	The register address.
+ * @rxb:	The RX buffer to return the result into.
+ * @rxl:	The length of data expected.
+ *
+ * This is the low level read call that issues the necessary spi message(s)
+ * to read data from the register specified in @reg.
+ */
+static void spi_rdreg(struct sw_priv *priv, u8 reg, void *rxb, void **rx,
+		      size_t rxl)
+{
+	struct spi_hw_priv *hw_priv = priv->hw_dev;
+	struct spi_device *spi = hw_priv->spidev;
+	u8 *txb = (u8 *) hw_priv->txd;
+	struct spi_transfer *xfer;
+	struct spi_message *msg;
+	int ret;
+
+#ifdef DEBUG
+	if (!mutex_is_locked(&priv->lock))
+		pr_alert("R not locked: %02X\n", reg);
+#endif
+	txb[0] = KS_SPIOP_RD;
+	txb[1] = reg;
+
+	if (rx)
+		*rx = rxb;
+	if (ksz_rx_1msg(hw_priv)) {
+		msg = &hw_priv->spi_msg1;
+		xfer = &hw_priv->spi_xfer1;
+
+		xfer->tx_buf = hw_priv->txd;
+		xfer->rx_buf = hw_priv->rxd;
+		xfer->len = rxl + SPI_CMD_LEN;
+		txb += SPI_CMD_LEN;
+		memset(txb, 0, rxl);
+		if (rx && rxb == hw_priv->rxd)
+			*rx = (u8 *) rxb + SPI_CMD_LEN;
+#if defined(CONFIG_SPI_PEGASUS) || defined(CONFIG_SPI_PEGASUS_MODULE)
+		/*
+		 * A hack to tell KSZ8692 SPI host controller the read command.
+		 */
+		memcpy(hw_priv->rxd, hw_priv->txd, SPI_CMD_LEN + 1);
+		hw_priv->rxd[SPI_CMD_LEN] ^= 0xff;
+#endif
+	} else {
+		msg = &hw_priv->spi_msg2;
+		xfer = hw_priv->spi_xfer2;
+
+		xfer->tx_buf = hw_priv->txd;
+		xfer->rx_buf = NULL;
+		xfer->len = SPI_CMD_LEN;
+
+		xfer++;
+		xfer->tx_buf = NULL;
+		xfer->rx_buf = rxb;
+		xfer->len = rxl;
+	}
+
+	ret = spi_sync(spi, msg);
+	if (ret < 0)
+		pr_alert("read: spi_sync() failed: %x %u\n", reg, rxl);
+	else if (ksz_rx_1msg(hw_priv) && rxb != hw_priv->rxd)
+		memcpy(rxb, hw_priv->rxd + SPI_CMD_LEN, rxl);
+}
+
+static void *spi_rdreg_size(struct sw_priv *priv, u8 reg, size_t size)
+{
+	struct spi_hw_priv *hw_priv = priv->hw_dev;
+	void *r;
+
+	spi_rdreg(priv, reg, hw_priv->rxd, &r, size);
+	return r;
+}
+
+/**
+ * spi_rdreg8 - read 8 bit register from device
+ * @priv:	The switch device structure.
+ * @reg:	The register address.
+ *
+ * Read a 8bit register from the chip, returning the result.
+ */
+static u8 spi_rdreg8(struct sw_priv *priv, u8 reg)
+{
+	u8 *rx;
+
+	rx = spi_rdreg_size(priv, reg, 1);
+	return rx[0];
+}
+
+/**
+ * spi_rdreg16 - read 16 bit register from device
+ * @priv:	The switch device structure.
+ * @reg:	The register address.
+ *
+ * Read a 16bit register from the chip, returning the result.
+ */
+static u16 spi_rdreg16(struct sw_priv *priv, u8 reg)
+{
+	u16 *rx;
+
+	rx = spi_rdreg_size(priv, reg, 2);
+	return be16_to_cpu(*rx);
+}
+
+/**
+ * spi_rdreg32 - read 32 bit register from device
+ * @priv:	The switch device structure.
+ * @reg:	The register address.
+ *
+ * Read a 32bit register from the chip.
+ *
+ * Note, this read requires the address be aligned to 4 bytes.
+ */
+static u32 spi_rdreg32(struct sw_priv *priv, u8 reg)
+{
+	u32 *rx;
+
+	rx = spi_rdreg_size(priv, reg, 4);
+	return be32_to_cpu(*rx);
+}
+
+/* -------------------------------------------------------------------------- */
+
+#define USE_SHOW_HELP
+#include "ksz_common.c"
+
+#include "ksz_req.c"
+
+/* -------------------------------------------------------------------------- */
+
+#define MIB_READ_INTERVAL		(HZ / 2)
+
+static void sw_r(struct ksz_sw *sw, unsigned reg, void *buf, size_t cnt)
+{
+	if (cnt + SPI_CMD_LEN > 128)
+		cnt = 128 - SPI_CMD_LEN;
+	spi_rdreg(sw->dev, reg, buf, NULL, cnt);
+}
+
+static void sw_w(struct ksz_sw *sw, unsigned reg, void *buf, size_t cnt)
+{
+	spi_wrreg(sw->dev, reg, buf, cnt);
+}
+
+static u8 sw_r8(struct ksz_sw *sw, unsigned reg)
+{
+	return HW_R8(sw->dev, reg);
+}
+
+static u16 sw_r16(struct ksz_sw *sw, unsigned reg)
+{
+	return HW_R16(sw->dev, reg);
+}
+
+static u32 sw_r32(struct ksz_sw *sw, unsigned reg)
+{
+	return HW_R32(sw->dev, reg);
+}
+
+static void sw_w8(struct ksz_sw *sw, unsigned reg, unsigned val)
+{
+	HW_W8(sw->dev, reg, val);
+}
+
+static void sw_w16(struct ksz_sw *sw, unsigned reg, unsigned val)
+{
+	HW_W16(sw->dev, reg, val);
+}
+
+static void sw_w32(struct ksz_sw *sw, unsigned reg, unsigned val)
+{
+	HW_W32(sw->dev, reg, val);
+}
+
+#include "ksz_sw_8895.c"
+
+static struct ksz_sw_reg_ops sw_reg_ops = {
+	.r8			= sw_r8,
+	.r16			= sw_r16,
+	.r32			= sw_r32,
+	.w8			= sw_w8,
+	.w16			= sw_w16,
+	.w32			= sw_w32,
+
+	.r			= sw_r,
+	.w			= sw_w,
+
+	.get			= sw_reg_get,
+	.set			= sw_reg_set,
+};
+
+/* -------------------------------------------------------------------------- */
+
+static int rx_1msg;
+static int spi_bus;
+
+static int ksz8895_probe(struct spi_device *spi)
+{
+	struct spi_hw_priv *hw_priv;
+	struct sw_priv *priv;
+
+	spi->bits_per_word = 8;
+
+	priv = kzalloc(sizeof(struct sw_priv), GFP_KERNEL);
+	if (!priv)
+		return -ENOMEM;
+
+	priv->hw_dev = kzalloc(sizeof(struct spi_hw_priv), GFP_KERNEL);
+	if (!priv->hw_dev) {
+		kfree(priv);
+		return -ENOMEM;
+	}
+	hw_priv = priv->hw_dev;
+
+	hw_priv->rx_1msg = rx_1msg;
+	hw_priv->spidev = spi;
+
+	priv->dev = &spi->dev;
+	priv->of_dev = &spi->dev;
+
+	priv->sw.reg = &sw_reg_ops;
+
+	/* initialise pre-made spi transfer messages */
+
+	spi_message_init(&hw_priv->spi_msg1);
+	spi_message_add_tail(&hw_priv->spi_xfer1, &hw_priv->spi_msg1);
+
+	spi_message_init(&hw_priv->spi_msg2);
+	spi_message_add_tail(&hw_priv->spi_xfer2[0], &hw_priv->spi_msg2);
+	spi_message_add_tail(&hw_priv->spi_xfer2[1], &hw_priv->spi_msg2);
+
+	priv->irq = spi->irq;
+	priv->spi_mode = (spi->mode & 3) | 4;
+
+	return ksz_probe(priv);
+}
+
+static int ksz8895_remove(struct spi_device *spi)
+{
+	struct sw_priv *priv = dev_get_drvdata(&spi->dev);
+
+	return ksz_remove(priv);
+}
+
+static const struct spi_device_id ksz8895_spi_ids[] = {
+	{ .name = "ksz8895" },
+	{ .name = "ksz8864" },
+	{},
+};
+MODULE_DEVICE_TABLE(spi, ksz8895_spi_ids);
+
+static const struct of_device_id ksz8895_dt_ids[] = {
+	{ .compatible = "microchip,ksz8895" },
+	{ .compatible = "microchip,ksz8864" },
+	{},
+};
+MODULE_DEVICE_TABLE(of, ksz8895_dt_ids);
+
+static struct spi_driver ksz8895_driver = {
+	.driver = {
+		.name = KS8895_DEV,
+		.owner = THIS_MODULE,
+		.of_match_table = of_match_ptr(ksz8895_dt_ids),
+	},
+	.probe = ksz8895_probe,
+	.remove = ksz8895_remove,
+	.id_table = ksz8895_spi_ids,
+};
+
+static int __init ksz8895_init(void)
+{
+#ifdef DEBUG_MSG
+	if (init_dbg())
+		return -ENOMEM;
+#endif
+	return spi_register_driver(&ksz8895_driver);
+}
+
+#ifndef CONFIG_KSZ_SWITCH_EMBEDDED
+static void __exit ksz8895_exit(void)
+#else
+static void ksz8895_exit(void)
+#endif
+{
+	spi_unregister_driver(&ksz8895_driver);
+#ifdef DEBUG_MSG
+	exit_dbg();
+#endif
+}
+
+#ifndef CONFIG_KSZ_SWITCH_EMBEDDED
+subsys_initcall(ksz8895_init);
+module_exit(ksz8895_exit);
+#endif
+
+module_param(rx_1msg, int, 0);
+MODULE_PARM_DESC(rx_1msg,
+	"Configure whether receive one message is used");
+
+module_param(spi_bus, int, 0);
+MODULE_PARM_DESC(spi_bus,
+	"Configure which spi master to use(0=KSZ8692, 2=FTDI)");
+
+#ifndef CONFIG_KSZ_SWITCH_EMBEDDED
+MODULE_DESCRIPTION("KSZ8895 SPI switch driver");
+MODULE_AUTHOR("Tristram Ha <Tristram.Ha@microchip.com>");
+MODULE_LICENSE("GPL");
+
+MODULE_ALIAS("spi:ksz8895");
+#endif
diff -Napur linux-linux4microchip-2022.10-orig/drivers/net/ethernet/micrel/spi-ksz9897.c linux-linux4microchip-2022.10/drivers/net/ethernet/micrel/spi-ksz9897.c
--- linux-linux4microchip-2022.10-orig/drivers/net/ethernet/micrel/spi-ksz9897.c	1969-12-31 16:00:00.000000000 -0800
+++ linux-linux4microchip-2022.10/drivers/net/ethernet/micrel/spi-ksz9897.c	2024-02-20 17:07:33.368431258 -0800
@@ -0,0 +1,700 @@
+/**
+ * Microchip KSZ9897 SPI driver
+ *
+ * Copyright (c) 2015-2023 Microchip Technology Inc.
+ * Copyright (c) 2013-2015 Micrel, Inc.
+ *
+ * Copyright 2009 Simtec Electronics
+ *	http://www.simtec.co.uk/
+ *	Ben Dooks <ben@simtec.co.uk>
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+ */
+
+#if 0
+#define DEBUG
+#define DBG
+#define DEBUG_MSG
+#if 0
+#define DEBUG_PHY
+#define DBG_LINK
+#endif
+#endif
+
+#ifndef CONFIG_KSZ_SWITCH_EMBEDDED
+#define pr_fmt(fmt) KBUILD_MODNAME ": " fmt
+
+#include <linux/kernel.h>
+#include <linux/module.h>
+#include <linux/interrupt.h>
+#include <linux/of_net.h>
+#include <linux/of_mdio.h>
+#include <linux/phy.h>
+#include <linux/phylink.h>
+#include <linux/platform_device.h>
+#include <linux/sched.h>
+#include <linux/netdevice.h>
+#include <linux/etherdevice.h>
+#include <linux/ethtool.h>
+#include <linux/cache.h>
+#include <linux/crc32.h>
+#include <linux/if_vlan.h>
+#include <linux/ip.h>
+#include <net/ip.h>
+#include <net/ipv6.h>
+#endif
+
+/* -------------------------------------------------------------------------- */
+
+#include <linux/net_tstamp.h>
+#include <linux/spi/spi.h>
+#include <linux/debugfs.h>
+#include <linux/seq_file.h>
+
+#undef MAX_REQUEST_SIZE
+#define MAX_REQUEST_SIZE		80
+
+#include "ksz_cfg_9897.h"
+
+
+#ifdef CONFIG_KSZ_DLR
+/* Have ACL to handle beacon timeout. */
+#define CONFIG_HAVE_ACL_HW
+
+/* Have DLR to transmit beacons. */
+#define CONFIG_HAVE_DLR_HW
+#endif
+
+#ifdef CONFIG_KSZ_HSR
+#define CONFIG_HAVE_HSR_HW
+#endif
+
+#if 0
+#define USE_MII_PHY
+#endif
+#if 0
+#define USE_RGMII_PHY
+#endif
+#if 0
+#define USE_MII_MODE
+#endif
+#if 0
+#define USE_GMII_MODE
+#endif
+#if 0
+#define USE_RMII_MODE
+#endif
+#if 0
+#define USE_RGMII_MODE
+#endif
+#if 0
+#define USE_GMII_100_MODE
+#endif
+#if 0
+#define USE_10_MBIT_MODE
+#ifndef USE_GMII_100_MODE
+#define USE_GMII_100_MODE
+#endif
+#endif
+#if 0
+#define USE_HALF_DUPLEX
+#endif
+
+#define KS9897MLI_DEV0			"ksz9897"
+
+#define SW_DRV_RELDATE			"Nov 17, 2023"
+#define SW_DRV_VERSION			"1.2.8"
+
+/* -------------------------------------------------------------------------- */
+
+#define HW_R(ks, reg)		spi_rdreg8(ks, reg)
+#define HW_W(ks, reg, val)	spi_wrreg8(ks, reg, val)
+#define HW_R8(ks, reg)		spi_rdreg8(ks, reg)
+#define HW_W8(ks, reg, val)	spi_wrreg8(ks, reg, val)
+#define HW_R16(ks, reg)		spi_rdreg16(ks, reg)
+#define HW_W16(ks, reg, val)	spi_wrreg16(ks, reg, val)
+#define HW_R24(ks, reg)		spi_rdreg24(ks, reg)
+#define HW_W24(ks, reg, val)	spi_wrreg24(ks, reg, val)
+#define HW_R32(ks, reg)		spi_rdreg32(ks, reg)
+#define HW_W32(ks, reg, val)	spi_wrreg32(ks, reg, val)
+
+#include "ksz_spi_net.h"
+
+/* -------------------------------------------------------------------------- */
+
+/* SPI frame opcodes */
+#define KS_SPIOP_RD			3
+#define KS_SPIOP_WR			2
+
+#define SPI_ADDR_S			24
+#define SPI_ADDR_M			((1 << SPI_ADDR_S) - 1)
+#define SPI_TURNAROUND_S		5
+
+#define SPI_CMD_LEN			4
+
+/*
+ * SPI register read/write calls.
+ *
+ * All these calls issue SPI transactions to access the chip's registers. They
+ * all require that the necessary lock is held to prevent accesses when the
+ * chip is busy transfering packet data (RX/TX FIFO accesses).
+ */
+
+/**
+ * spi_wrreg - issue write register command
+ * @priv:	The switch device structure.
+ * @addr:	The register address.
+ * @txb:	The data buffer to write.
+ * @txl:	The length of data.
+ *
+ * This is the low level write call that issues the necessary spi message(s)
+ * to write data to the register specified in @addr.
+ */
+static void spi_wrreg(struct sw_priv *priv, u32 addr, void *txb, size_t txl)
+{
+	struct spi_hw_priv *hw_priv = priv->hw_dev;
+	struct spi_device *spi = hw_priv->spidev;
+	u32 *txc = (u32 *) hw_priv->txd;
+	struct spi_transfer *xfer;
+	struct spi_message *msg;
+	u8 *tx;
+	int ret;
+	struct ksz_sw *sw = &priv->sw;
+
+	if (!(sw->features & NEW_CAP)) {
+		u32 len = (addr & 3) + txl;
+
+		if (len >= 4 && (len & 3))
+			pr_alert("W may not be correct: %x %x\n", addr, txl);
+	}
+#ifdef DEBUG
+	if (!mutex_is_locked(&priv->lock))
+		pr_alert("W not locked: %04x\n", addr);
+#endif
+	*txc = addr & SPI_ADDR_M;
+	*txc |= KS_SPIOP_WR << SPI_ADDR_S;
+	*txc <<= SPI_TURNAROUND_S;
+	*txc = cpu_to_be32(*txc);
+
+	/* Own transmit buffer is being used. */
+	if (txb == hw_priv->txd) {
+		xfer = &hw_priv->spi_xfer1;
+		msg = &hw_priv->spi_msg1;
+
+		xfer->tx_buf = hw_priv->txd;
+		xfer->rx_buf = NULL;
+		xfer->len = txl + SPI_CMD_LEN;
+		tx = &hw_priv->txd[SPI_CMD_LEN];
+	} else {
+		xfer = hw_priv->spi_xfer2;
+		msg = &hw_priv->spi_msg2;
+
+		xfer->tx_buf = hw_priv->txd;
+		xfer->rx_buf = NULL;
+		xfer->len = SPI_CMD_LEN;
+
+		xfer++;
+		xfer->tx_buf = txb;
+		xfer->rx_buf = NULL;
+		xfer->len = txl;
+		tx = txb;
+	}
+
+	ret = spi_sync(spi, msg);
+	if (ret < 0)
+		pr_alert("spi_sync() failed: %x %u\n", addr, txl);
+}
+
+static void spi_wrreg_size(struct sw_priv *priv, u32 reg, u32 val, size_t size)
+{
+	struct spi_hw_priv *hw_priv = priv->hw_dev;
+	u8 *txb = hw_priv->txd;
+	int i = SPI_CMD_LEN;
+	size_t cnt = size;
+
+	do {
+		txb[i++] = (u8)(val >> (8 * (cnt - 1)));
+		cnt--;
+	} while (cnt);
+	spi_wrreg(priv, reg, txb, size);
+}
+
+/**
+ * spi_wrreg32 - write 32bit register value to chip
+ * @priv:	The switch device structure.
+ * @reg:	The register address.
+ * @val:	The value to write.
+ *
+ * Issue a write to put the value @val into the register specified in @reg.
+ */
+static void spi_wrreg32(struct sw_priv *priv, u32 reg, u32 val)
+{
+	spi_wrreg_size(priv, reg, val, 4);
+}
+
+/**
+ * spi_wrreg24 - write 24bit register value to chip
+ * @priv:	The switch device structure.
+ * @reg:	The register address.
+ * @val:	The value to write.
+ *
+ * Issue a write to put the value @val into the register specified in @reg.
+ */
+static void spi_wrreg24(struct sw_priv *priv, u32 reg, u32 val)
+{
+	spi_wrreg_size(priv, reg, val, 3);
+}
+
+/**
+ * spi_wrreg16 - write 16bit register value to chip
+ * @priv:	The switch device structure.
+ * @reg:	The register address.
+ * @val:	The value to write.
+ *
+ * Issue a write to put the value @val into the register specified in @reg.
+ */
+static void spi_wrreg16(struct sw_priv *priv, u32 reg, u32 val)
+{
+	spi_wrreg_size(priv, reg, val, 2);
+}
+
+/**
+ * spi_wrreg8 - write 8bit register value to chip
+ * @priv:	The switch device structure.
+ * @reg:	The register address.
+ * @val:	The value to write.
+ *
+ * Issue a write to put the value @val into the register specified in @reg.
+ */
+static void spi_wrreg8(struct sw_priv *priv, u32 reg, u32 val)
+{
+	spi_wrreg_size(priv, reg, val, 1);
+}
+
+/**
+ * ksz_rx_1msg - select whether to use one or two messages for spi read
+ * @hw_priv:	The hardware device structure.
+ *
+ * Return whether to generate a single message with a tx and rx buffer
+ * supplied to spi_sync(), or alternatively send the tx and rx buffers
+ * as separate messages.
+ *
+ * Depending on the hardware in use, a single message may be more efficient
+ * on interrupts or work done by the driver.
+ *
+ * This currently always returns false until we add some per-device data passed
+ * from the platform code to specify which mode is better.
+ */
+static inline bool ksz_rx_1msg(struct spi_hw_priv *hw_priv)
+{
+	return hw_priv->rx_1msg;
+}
+
+/**
+ * spi_rdreg - issue read register command and return the data
+ * @priv:	The switch device structure.
+ * @addr:	The register address.
+ * @rxb:	The RX buffer to return the result into.
+ * @rxl:	The length of data expected.
+ *
+ * This is the low level read call that issues the necessary spi message(s)
+ * to read data from the register specified in @addr.
+ */
+static void spi_rdreg(struct sw_priv *priv, u32 addr, void *rxb, void **rx,
+		      size_t rxl)
+{
+	struct spi_hw_priv *hw_priv = priv->hw_dev;
+	struct spi_device *spi = hw_priv->spidev;
+	u32 *txc = (u32 *) hw_priv->txd;
+	struct spi_transfer *xfer;
+	struct spi_message *msg;
+	int ret;
+
+#ifdef DEBUG
+	if (!mutex_is_locked(&priv->lock))
+		pr_alert("R not locked: %04x\n", addr);
+#endif
+	*txc = addr & SPI_ADDR_M;
+	*txc |= KS_SPIOP_RD << SPI_ADDR_S;
+	*txc <<= SPI_TURNAROUND_S;
+	*txc = cpu_to_be32(*txc);
+
+	if (rx)
+		*rx = rxb;
+	if (ksz_rx_1msg(hw_priv)) {
+		msg = &hw_priv->spi_msg1;
+		xfer = &hw_priv->spi_xfer1;
+
+		xfer->tx_buf = hw_priv->txd;
+		xfer->rx_buf = hw_priv->rxd;
+		xfer->len = rxl + SPI_CMD_LEN;
+		++txc;
+		memset(txc, 0, rxl);
+		if (rx && rxb == hw_priv->rxd)
+			*rx = (u8 *) rxb + SPI_CMD_LEN;
+#if defined(CONFIG_SPI_PEGASUS) || defined(CONFIG_SPI_PEGASUS_MODULE)
+		/*
+		 * A hack to tell KSZ8692 SPI host controller the read command.
+		 */
+		memcpy(hw_priv->rxd, hw_priv->txd, SPI_CMD_LEN + 1);
+		hw_priv->rxd[SPI_CMD_LEN] ^= 0xff;
+#endif
+	} else {
+		msg = &hw_priv->spi_msg2;
+		xfer = hw_priv->spi_xfer2;
+
+		xfer->tx_buf = hw_priv->txd;
+		xfer->rx_buf = NULL;
+		xfer->len = SPI_CMD_LEN;
+
+		xfer++;
+		xfer->tx_buf = NULL;
+		xfer->rx_buf = rxb;
+		xfer->len = rxl;
+	}
+
+	ret = spi_sync(spi, msg);
+	if (ret < 0)
+		pr_alert("read: spi_sync() failed: %x %u\n", addr, rxl);
+	else if (ksz_rx_1msg(hw_priv) && rxb != hw_priv->rxd)
+		memcpy(rxb, hw_priv->rxd + SPI_CMD_LEN, rxl);
+}
+
+static void *spi_rdreg_size(struct sw_priv *priv, u32 reg, size_t size)
+{
+	struct spi_hw_priv *hw_priv = priv->hw_dev;
+	void *r;
+
+	spi_rdreg(priv, reg, hw_priv->rxd, &r, size);
+	return r;
+}
+
+/**
+ * spi_rdreg8 - read 8 bit register from device
+ * @priv:	The switch device structure.
+ * @reg:	The register address.
+ *
+ * Read a 8bit register from the chip, returning the result.
+ */
+static u8 spi_rdreg8(struct sw_priv *priv, u32 reg)
+{
+	u8 *rx;
+
+	rx = spi_rdreg_size(priv, reg, 1);
+	return rx[0];
+}
+
+/**
+ * spi_rdreg16 - read 16 bit register from device
+ * @priv:	The switch device structure.
+ * @reg:	The register address.
+ *
+ * Read a 16bit register from the chip, returning the result.
+ */
+static u16 spi_rdreg16(struct sw_priv *priv, u32 reg)
+{
+	u16 *rx;
+
+	rx = spi_rdreg_size(priv, reg, 2);
+	return be16_to_cpu(*rx);
+}
+
+/**
+ * spi_rdreg24 - read 24 bit register from device
+ * @priv:	The switch device structure.
+ * @reg:	The register address.
+ *
+ * Read a 24bit register from the chip.
+ */
+static u32 spi_rdreg24(struct sw_priv *priv, u32 reg)
+{
+	u32 *rx;
+
+	rx = spi_rdreg_size(priv, reg, 3);
+	return be32_to_cpu(*rx) >> 8;
+}
+
+/**
+ * spi_rdreg32 - read 32 bit register from device
+ * @priv:	The switch device structure.
+ * @reg:	The register address.
+ *
+ * Read a 32bit register from the chip.
+ */
+static u32 spi_rdreg32(struct sw_priv *priv, u32 reg)
+{
+	u32 *rx;
+
+	rx = spi_rdreg_size(priv, reg, 4);
+	return be32_to_cpu(*rx);
+}
+
+/* -------------------------------------------------------------------------- */
+
+#define USE_SHOW_HELP
+#include "ksz_common.c"
+
+/* For ksz_request used by PTP or MRP driver. */
+#include "ksz_req.c"
+
+/* -------------------------------------------------------------------------- */
+
+#define MIB_READ_INTERVAL		(HZ / 2)
+
+static void sw_lock(struct ksz_sw *sw)
+{
+	const struct ksz_sw_reg_ops *reg_ops = sw->reg;
+
+	mutex_lock(sw->reglock);
+	if (sw->reg != reg_ops) {
+		mutex_unlock(sw->reglock);
+		sw->reg->lock(sw);
+	}
+}  /* sw_lock */
+
+static void sw_unlock(struct ksz_sw *sw)
+{
+	mutex_unlock(sw->reglock);
+}  /* sw_unlock */
+
+static void sw_r(struct ksz_sw *sw, unsigned reg, void *buf, size_t cnt)
+{
+	if (cnt + SPI_CMD_LEN > 128)
+		cnt = 128 - SPI_CMD_LEN;
+	spi_rdreg(sw->dev, reg, buf, NULL, cnt);
+}
+
+static void sw_w(struct ksz_sw *sw, unsigned reg, void *buf, size_t cnt)
+{
+	spi_wrreg(sw->dev, reg, buf, cnt);
+}
+
+static u8 sw_r8(struct ksz_sw *sw, unsigned reg)
+{
+	return HW_R8(sw->dev, reg);
+}
+
+static u16 sw_r16(struct ksz_sw *sw, unsigned reg)
+{
+	return HW_R16(sw->dev, reg);
+}
+
+static u32 sw_r24(struct ksz_sw *sw, unsigned reg)
+{
+	return HW_R24(sw->dev, reg);
+}
+
+static u32 sw_r32(struct ksz_sw *sw, unsigned reg)
+{
+	return HW_R32(sw->dev, reg);
+}
+
+static void sw_w8(struct ksz_sw *sw, unsigned reg, unsigned val)
+{
+	HW_W8(sw->dev, reg, val);
+}
+
+static void sw_w16(struct ksz_sw *sw, unsigned reg, unsigned val)
+{
+	HW_W16(sw->dev, reg, val);
+}
+
+static void sw_w24(struct ksz_sw *sw, unsigned reg, unsigned val)
+{
+	HW_W24(sw->dev, reg, val);
+}
+
+static void sw_w32(struct ksz_sw *sw, unsigned reg, unsigned val)
+{
+	HW_W32(sw->dev, reg, val);
+}
+
+#include "ksz_sw_9897.c"
+
+static struct ksz_sw_reg_ops sw_reg_ops = {
+	.lock			= sw_lock,
+	.unlock			= sw_unlock,
+
+	.r8			= sw_r8,
+	.r16			= sw_r16,
+	.r24			= sw_r24,
+	.r32			= sw_r32,
+	.w8			= sw_w8,
+	.w16			= sw_w16,
+	.w24			= sw_w24,
+	.w32			= sw_w32,
+
+	.r			= sw_r,
+	.w			= sw_w,
+
+	.get			= sw_reg_get,
+	.set			= sw_reg_set,
+
+	.r_dyn_mac_hw		= sw_r_dyn_mac_hw,
+	.w_dyn_mac_hw		= sw_w_dyn_mac_hw,
+	.start_dyn_mac_hw	= sw_start_dyn_mac_hw,
+	.g_dyn_mac_hw		= sw_g_dyn_mac_hw,
+	.stop_dyn_mac_hw	= sw_stop_dyn_mac_hw,
+	.r_sta_mac_hw		= sw_r_sta_mac_hw,
+	.w_sta_mac_hw		= sw_w_sta_mac_hw,
+	.r_vlan_hw		= sw_r_vlan_hw,
+	.w_vlan_hw		= sw_w_vlan_hw,
+	.r_mib_cnt_hw		= sw_r_mib_cnt_hw,
+	.r_acl_hw		= sw_r_acl_hw,
+	.w_acl_hw		= sw_w_acl_hw,
+
+#ifdef CONFIG_KSZ_HSR
+	.r_hsr_hw		= sw_r_hsr_hw,
+	.w_hsr_hw		= sw_w_hsr_hw,
+	.start_hsr_hw		= sw_start_hsr_hw,
+	.g_hsr_hw		= sw_g_hsr_hw,
+	.stop_hsr_hw		= sw_stop_hsr_hw,
+#endif
+};
+
+/* -------------------------------------------------------------------------- */
+
+static int rx_1msg = 1;
+static int spi_bus;
+
+static int ksz9897_probe(struct spi_device *spi)
+{
+	struct spi_hw_priv *hw_priv;
+	struct sw_priv *priv;
+
+	spi->bits_per_word = 8;
+
+	priv = kzalloc(sizeof(struct sw_priv), GFP_KERNEL);
+	if (!priv)
+		return -ENOMEM;
+
+	priv->hw_dev = kzalloc(sizeof(struct spi_hw_priv), GFP_KERNEL);
+	if (!priv->hw_dev) {
+		kfree(priv);
+		return -ENOMEM;
+	}
+	hw_priv = priv->hw_dev;
+
+	hw_priv->rx_1msg = rx_1msg;
+	hw_priv->spidev = spi;
+
+	priv->dev = &spi->dev;
+	priv->of_dev = &spi->dev;
+
+	priv->sw.reg = &sw_reg_ops;
+
+	/* initialise pre-made spi transfer messages */
+
+	spi_message_init(&hw_priv->spi_msg1);
+	spi_message_add_tail(&hw_priv->spi_xfer1, &hw_priv->spi_msg1);
+
+	spi_message_init(&hw_priv->spi_msg2);
+	spi_message_add_tail(&hw_priv->spi_xfer2[0], &hw_priv->spi_msg2);
+	spi_message_add_tail(&hw_priv->spi_xfer2[1], &hw_priv->spi_msg2);
+
+	priv->irq = spi->irq;
+	priv->spi_mode = (spi->mode & 3) | 4;
+
+	return ksz_probe(priv);
+}
+
+static int ksz9897_remove(struct spi_device *spi)
+{
+	struct sw_priv *priv = dev_get_drvdata(&spi->dev);
+
+	return ksz_remove(priv);
+}
+
+static const struct spi_device_id ksz9897_spi_ids[] = {
+	{ .name = "ksz9897" },
+	{ .name = "ksz9567" },
+	{ .name = "ksz9477" },
+	{ .name = "ksz9896" },
+	{ .name = "ksz9566" },
+	{ .name = "ksz8565" },
+	{ .name = "ksz9893" },
+	{ .name = "ksz9563" },
+	{ .name = "ksz8563" },
+	{ .name = "lan9646" },
+	{},
+};
+MODULE_DEVICE_TABLE(spi, ksz9897_spi_ids);
+
+static const struct of_device_id ksz9897_dt_ids[] = {
+	{ .compatible = "microchip,ksz9897" },
+	{ .compatible = "microchip,ksz9567" },
+	{ .compatible = "microchip,ksz9477" },
+	{ .compatible = "microchip,ksz9896" },
+	{ .compatible = "microchip,ksz9566" },
+	{ .compatible = "microchip,ksz8565" },
+	{ .compatible = "microchip,ksz9893" },
+	{ .compatible = "microchip,ksz9563" },
+	{ .compatible = "microchip,ksz8563" },
+	{ .compatible = "microchip,lan9646" },
+	{},
+};
+MODULE_DEVICE_TABLE(of, ksz9897_dt_ids);
+
+static struct spi_driver ksz9897_driver = {
+	.driver = {
+		.name = KS9897MLI_DEV0,
+		.owner = THIS_MODULE,
+		.of_match_table = of_match_ptr(ksz9897_dt_ids),
+	},
+	.probe = ksz9897_probe,
+	.remove = ksz9897_remove,
+	.id_table = ksz9897_spi_ids,
+};
+
+static int __init ksz9897_init(void)
+{
+#ifdef DEBUG_MSG
+	if (init_dbg())
+		return -ENOMEM;
+#endif
+	return spi_register_driver(&ksz9897_driver);
+}
+
+#ifndef CONFIG_KSZ_SWITCH_EMBEDDED
+static void __exit ksz9897_exit(void)
+#else
+static void ksz9897_exit(void)
+#endif
+{
+	spi_unregister_driver(&ksz9897_driver);
+#ifdef DEBUG_MSG
+	exit_dbg();
+#endif
+}
+
+#ifndef CONFIG_KSZ_SWITCH_EMBEDDED
+subsys_initcall(ksz9897_init);
+module_exit(ksz9897_exit);
+#endif
+
+module_param(rx_1msg, int, 0);
+MODULE_PARM_DESC(rx_1msg,
+	"Configure whether receive one message is used");
+
+module_param(spi_bus, int, 0);
+MODULE_PARM_DESC(spi_bus,
+	"Configure which spi master to use(0=KSZ8692, 2=FTDI)");
+
+#ifndef CONFIG_KSZ_SWITCH_EMBEDDED
+MODULE_DESCRIPTION("Microchip KSZ9897 SPI Switch Driver");
+MODULE_AUTHOR("Tristram Ha <Tristram.Ha@microchip.com>");
+MODULE_LICENSE("GPL");
+
+MODULE_ALIAS("spi:ksz9897");
+#endif
